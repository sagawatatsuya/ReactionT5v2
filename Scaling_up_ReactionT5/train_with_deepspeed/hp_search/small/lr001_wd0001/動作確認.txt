qrsh -g gca50095 -l rt_AF=1 -l h_rt=00:30:00

source /etc/profile.d/modules.sh
module load python/3.11 cuda/12.2 cudnn/8.9 nccl/2.18/2.18.5-1 hpcx/2.12
source /home/acf15718oa/third/bin/activate
export TRANSFORMERS_CACHE='/groups/gca50095/sagawa/cache/'
export HF_DATASETS_CACHE="/groups/gca50095/sagawa/cache/"

wd=/home/acf15718oa/train_with_deepspeed/hp_search/small/lr001_wd0001

cd $wd


export OMP_NUM_THREADS=8
export NGPU_PER_NODE=4


# launch on master node
node_rank=0
torchrun --nproc_per_node $NGPU_PER_NODE --nnodes $NHOSTS --node_rank $node_rank --master_addr `hostname` /home/acf15718oa/train_with_deepspeed/train_with_deepspeed.py \


CUDA_VISIBLE_DEVICES=1 python /data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py \
    --do_train \
    --do_eval \
    --num_train_epochs="10" \
    --output_dir="./output" \
    --overwrite_output_dir \
    --save_total_limit="2" \
    --deepspeed="/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json" \
    --per_device_train_batch_size="128" \
    --per_device_eval_batch_size="512" \
    --learning_rate="0.01" \
    --weight_decay="0.001" \
    --warmup_steps="10000" \
    --logging_steps="16" \
    --save_steps="32" \
    --eval_steps="32" \
    --config_name="/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/small" \
    --tokenizer_name="/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/small" \
    --train_files_dir="/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22//" \
    --max_seq_length="512" \
    --num_workers="8" \
    --local_rank=1