{
  "best_metric": 2.5234375,
  "best_model_checkpoint": "./output/checkpoint-2048",
  "epoch": 1.8581496083295992e-06,
  "eval_steps": 1024,
  "global_step": 2048,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.9033587630149987e-08,
      "grad_norm": 228838.12046073092,
      "learning_rate": 0.001,
      "loss": 94.5918,
      "step": 32
    },
    {
      "epoch": 5.8067175260299974e-08,
      "grad_norm": 199569.23722858692,
      "learning_rate": 0.001,
      "loss": 41.3843,
      "step": 64
    },
    {
      "epoch": 8.710076289044995e-08,
      "grad_norm": 347882.28850575304,
      "learning_rate": 0.001,
      "loss": 26.4458,
      "step": 96
    },
    {
      "epoch": 1.1613435052059995e-07,
      "grad_norm": 156029.16363295677,
      "learning_rate": 0.001,
      "loss": 22.5046,
      "step": 128
    },
    {
      "epoch": 1.4516793815074993e-07,
      "grad_norm": 129226.76478191352,
      "learning_rate": 0.001,
      "loss": 20.1504,
      "step": 160
    },
    {
      "epoch": 1.742015257808999e-07,
      "grad_norm": 157049.55854761263,
      "learning_rate": 0.001,
      "loss": 18.3494,
      "step": 192
    },
    {
      "epoch": 2.0323511341104991e-07,
      "grad_norm": 162870.75808750937,
      "learning_rate": 0.001,
      "loss": 17.2405,
      "step": 224
    },
    {
      "epoch": 2.322687010411999e-07,
      "grad_norm": 127318.5766807028,
      "learning_rate": 0.001,
      "loss": 17.2012,
      "step": 256
    },
    {
      "epoch": 2.613022886713499e-07,
      "grad_norm": 116727.62497369677,
      "learning_rate": 0.001,
      "loss": 14.8938,
      "step": 288
    },
    {
      "epoch": 2.9033587630149986e-07,
      "grad_norm": 96292.76957279815,
      "learning_rate": 0.001,
      "loss": 14.679,
      "step": 320
    },
    {
      "epoch": 3.1936946393164984e-07,
      "grad_norm": 197427.9276698208,
      "learning_rate": 0.001,
      "loss": 13.9636,
      "step": 352
    },
    {
      "epoch": 3.484030515617998e-07,
      "grad_norm": 115830.11366652456,
      "learning_rate": 0.001,
      "loss": 13.8083,
      "step": 384
    },
    {
      "epoch": 3.774366391919498e-07,
      "grad_norm": 95732.24025374105,
      "learning_rate": 0.001,
      "loss": 13.0327,
      "step": 416
    },
    {
      "epoch": 4.0647022682209983e-07,
      "grad_norm": 102237.66945700592,
      "learning_rate": 0.001,
      "loss": 11.8198,
      "step": 448
    },
    {
      "epoch": 4.355038144522498e-07,
      "grad_norm": 60949.802821666286,
      "learning_rate": 0.001,
      "loss": 11.3992,
      "step": 480
    },
    {
      "epoch": 4.645374020823998e-07,
      "grad_norm": 95341.68548961153,
      "learning_rate": 0.001,
      "loss": 11.3218,
      "step": 512
    },
    {
      "epoch": 4.935709897125498e-07,
      "grad_norm": 116087.86465432122,
      "learning_rate": 0.001,
      "loss": 10.6416,
      "step": 544
    },
    {
      "epoch": 5.226045773426998e-07,
      "grad_norm": 74499.10002140966,
      "learning_rate": 0.001,
      "loss": 9.8201,
      "step": 576
    },
    {
      "epoch": 5.516381649728497e-07,
      "grad_norm": 61439.122129145035,
      "learning_rate": 0.001,
      "loss": 9.4067,
      "step": 608
    },
    {
      "epoch": 5.806717526029997e-07,
      "grad_norm": 84489.02925232364,
      "learning_rate": 0.001,
      "loss": 9.2338,
      "step": 640
    },
    {
      "epoch": 6.097053402331497e-07,
      "grad_norm": 143151.34011248374,
      "learning_rate": 0.001,
      "loss": 9.0345,
      "step": 672
    },
    {
      "epoch": 6.387389278632997e-07,
      "grad_norm": 71524.54191394727,
      "learning_rate": 0.001,
      "loss": 8.3297,
      "step": 704
    },
    {
      "epoch": 6.677725154934497e-07,
      "grad_norm": 59850.29991570635,
      "learning_rate": 0.001,
      "loss": 7.713,
      "step": 736
    },
    {
      "epoch": 6.968061031235996e-07,
      "grad_norm": 118532.33523389303,
      "learning_rate": 0.001,
      "loss": 7.202,
      "step": 768
    },
    {
      "epoch": 7.258396907537496e-07,
      "grad_norm": 64046.49910806991,
      "learning_rate": 0.001,
      "loss": 7.2407,
      "step": 800
    },
    {
      "epoch": 7.548732783838996e-07,
      "grad_norm": 92933.20801521919,
      "learning_rate": 0.001,
      "loss": 6.5248,
      "step": 832
    },
    {
      "epoch": 7.839068660140496e-07,
      "grad_norm": 137766.56430353486,
      "learning_rate": 0.001,
      "loss": 6.2839,
      "step": 864
    },
    {
      "epoch": 8.129404536441997e-07,
      "grad_norm": 62298.594013027294,
      "learning_rate": 0.001,
      "loss": 6.084,
      "step": 896
    },
    {
      "epoch": 8.419740412743496e-07,
      "grad_norm": 89960.16184956538,
      "learning_rate": 0.001,
      "loss": 6.1453,
      "step": 928
    },
    {
      "epoch": 8.710076289044996e-07,
      "grad_norm": 74087.67174098536,
      "learning_rate": 0.001,
      "loss": 5.3455,
      "step": 960
    },
    {
      "epoch": 9.000412165346496e-07,
      "grad_norm": 50765.94937554108,
      "learning_rate": 0.001,
      "loss": 5.0284,
      "step": 992
    },
    {
      "epoch": 9.290748041647996e-07,
      "grad_norm": 104028.11004723675,
      "learning_rate": 0.001,
      "loss": 4.8666,
      "step": 1024
    },
    {
      "epoch": 9.290748041647996e-07,
      "eval_loss": 3.900390625,
      "eval_runtime": 92967.3552,
      "eval_samples_per_second": 229.294,
      "eval_steps_per_second": 1.791,
      "step": 1024
    },
    {
      "epoch": 9.581083917949496e-07,
      "grad_norm": 85673.29460222712,
      "learning_rate": 0.001,
      "loss": 4.7325,
      "step": 1056
    },
    {
      "epoch": 9.871419794250995e-07,
      "grad_norm": 98727.51219391685,
      "learning_rate": 0.001,
      "loss": 4.6762,
      "step": 1088
    },
    {
      "epoch": 1.0161755670552495e-06,
      "grad_norm": 135771.49132273684,
      "learning_rate": 0.001,
      "loss": 4.4052,
      "step": 1120
    },
    {
      "epoch": 1.0452091546853995e-06,
      "grad_norm": 147804.78888046896,
      "learning_rate": 0.001,
      "loss": 4.4097,
      "step": 1152
    },
    {
      "epoch": 1.0742427423155495e-06,
      "grad_norm": 89245.99782623307,
      "learning_rate": 0.001,
      "loss": 4.2807,
      "step": 1184
    },
    {
      "epoch": 1.1032763299456995e-06,
      "grad_norm": 54311.01015447973,
      "learning_rate": 0.001,
      "loss": 4.0439,
      "step": 1216
    },
    {
      "epoch": 1.1323099175758494e-06,
      "grad_norm": 42182.01322838918,
      "learning_rate": 0.001,
      "loss": 3.838,
      "step": 1248
    },
    {
      "epoch": 1.1613435052059994e-06,
      "grad_norm": 43193.97596887789,
      "learning_rate": 0.001,
      "loss": 3.5625,
      "step": 1280
    },
    {
      "epoch": 1.1903770928361494e-06,
      "grad_norm": 35337.592164718866,
      "learning_rate": 0.001,
      "loss": 3.5033,
      "step": 1312
    },
    {
      "epoch": 1.2194106804662994e-06,
      "grad_norm": 34943.988953752836,
      "learning_rate": 0.001,
      "loss": 3.4567,
      "step": 1344
    },
    {
      "epoch": 1.2484442680964494e-06,
      "grad_norm": 30254.360479111107,
      "learning_rate": 0.001,
      "loss": 3.5396,
      "step": 1376
    },
    {
      "epoch": 1.2774778557265993e-06,
      "grad_norm": 63014.134033564245,
      "learning_rate": 0.001,
      "loss": 3.2749,
      "step": 1408
    },
    {
      "epoch": 1.3065114433567493e-06,
      "grad_norm": 38762.73303057977,
      "learning_rate": 0.001,
      "loss": 3.1953,
      "step": 1440
    },
    {
      "epoch": 1.3355450309868993e-06,
      "grad_norm": 73164.4357321233,
      "learning_rate": 0.001,
      "loss": 3.1761,
      "step": 1472
    },
    {
      "epoch": 1.3645786186170493e-06,
      "grad_norm": 37150.84149787189,
      "learning_rate": 0.001,
      "loss": 3.1317,
      "step": 1504
    },
    {
      "epoch": 1.3936122062471993e-06,
      "grad_norm": 36790.07659682159,
      "learning_rate": 0.001,
      "loss": 3.0029,
      "step": 1536
    },
    {
      "epoch": 1.4226457938773492e-06,
      "grad_norm": 37243.52292681239,
      "learning_rate": 0.001,
      "loss": 2.9432,
      "step": 1568
    },
    {
      "epoch": 1.4516793815074992e-06,
      "grad_norm": 35069.23706042092,
      "learning_rate": 0.001,
      "loss": 2.8476,
      "step": 1600
    },
    {
      "epoch": 1.4807129691376492e-06,
      "grad_norm": 66659.58607732275,
      "learning_rate": 0.001,
      "loss": 2.9178,
      "step": 1632
    },
    {
      "epoch": 1.5097465567677992e-06,
      "grad_norm": 33321.831882416074,
      "learning_rate": 0.001,
      "loss": 2.806,
      "step": 1664
    },
    {
      "epoch": 1.5387801443979492e-06,
      "grad_norm": 25289.43558089029,
      "learning_rate": 0.001,
      "loss": 2.8203,
      "step": 1696
    },
    {
      "epoch": 1.5678137320280991e-06,
      "grad_norm": 23911.472811184172,
      "learning_rate": 0.001,
      "loss": 2.9467,
      "step": 1728
    },
    {
      "epoch": 1.5968473196582493e-06,
      "grad_norm": 15992.564522302231,
      "learning_rate": 0.001,
      "loss": 3.1945,
      "step": 1760
    },
    {
      "epoch": 1.6258809072883993e-06,
      "grad_norm": 39322.40765772106,
      "learning_rate": 0.001,
      "loss": 2.9077,
      "step": 1792
    },
    {
      "epoch": 1.6549144949185493e-06,
      "grad_norm": 34614.08499440654,
      "learning_rate": 0.001,
      "loss": 2.7701,
      "step": 1824
    },
    {
      "epoch": 1.6839480825486993e-06,
      "grad_norm": 17402.551077356446,
      "learning_rate": 0.001,
      "loss": 2.6499,
      "step": 1856
    },
    {
      "epoch": 1.7129816701788493e-06,
      "grad_norm": 31327.74999900248,
      "learning_rate": 0.001,
      "loss": 2.6458,
      "step": 1888
    },
    {
      "epoch": 1.7420152578089992e-06,
      "grad_norm": 30487.848070993798,
      "learning_rate": 0.001,
      "loss": 2.6434,
      "step": 1920
    },
    {
      "epoch": 1.7710488454391492e-06,
      "grad_norm": 23749.536416528215,
      "learning_rate": 0.001,
      "loss": 2.4745,
      "step": 1952
    },
    {
      "epoch": 1.8000824330692992e-06,
      "grad_norm": 15104.569242451105,
      "learning_rate": 0.001,
      "loss": 2.5208,
      "step": 1984
    },
    {
      "epoch": 1.8291160206994492e-06,
      "grad_norm": 26800.296267019137,
      "learning_rate": 0.001,
      "loss": 2.4775,
      "step": 2016
    },
    {
      "epoch": 1.8581496083295992e-06,
      "grad_norm": 21926.99281707366,
      "learning_rate": 0.001,
      "loss": 2.4528,
      "step": 2048
    },
    {
      "epoch": 1.8581496083295992e-06,
      "eval_loss": 2.5234375,
      "eval_runtime": 92996.8014,
      "eval_samples_per_second": 229.222,
      "eval_steps_per_second": 1.791,
      "step": 2048
    }
  ],
  "logging_steps": 32,
  "max_steps": 1102171747,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 1024,
  "total_flos": 3.996711023070413e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
