{
  "best_metric": 2.23828125,
  "best_model_checkpoint": "./output/checkpoint-3072",
  "epoch": 2.7872244124943985e-06,
  "eval_steps": 1024,
  "global_step": 3072,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.9033587630149987e-08,
      "grad_norm": 228838.12046073092,
      "learning_rate": 0.001,
      "loss": 94.5918,
      "step": 32
    },
    {
      "epoch": 5.8067175260299974e-08,
      "grad_norm": 199569.23722858692,
      "learning_rate": 0.001,
      "loss": 41.3843,
      "step": 64
    },
    {
      "epoch": 8.710076289044995e-08,
      "grad_norm": 347882.28850575304,
      "learning_rate": 0.001,
      "loss": 26.4458,
      "step": 96
    },
    {
      "epoch": 1.1613435052059995e-07,
      "grad_norm": 156029.16363295677,
      "learning_rate": 0.001,
      "loss": 22.5046,
      "step": 128
    },
    {
      "epoch": 1.4516793815074993e-07,
      "grad_norm": 129226.76478191352,
      "learning_rate": 0.001,
      "loss": 20.1504,
      "step": 160
    },
    {
      "epoch": 1.742015257808999e-07,
      "grad_norm": 157049.55854761263,
      "learning_rate": 0.001,
      "loss": 18.3494,
      "step": 192
    },
    {
      "epoch": 2.0323511341104991e-07,
      "grad_norm": 162870.75808750937,
      "learning_rate": 0.001,
      "loss": 17.2405,
      "step": 224
    },
    {
      "epoch": 2.322687010411999e-07,
      "grad_norm": 127318.5766807028,
      "learning_rate": 0.001,
      "loss": 17.2012,
      "step": 256
    },
    {
      "epoch": 2.613022886713499e-07,
      "grad_norm": 116727.62497369677,
      "learning_rate": 0.001,
      "loss": 14.8938,
      "step": 288
    },
    {
      "epoch": 2.9033587630149986e-07,
      "grad_norm": 96292.76957279815,
      "learning_rate": 0.001,
      "loss": 14.679,
      "step": 320
    },
    {
      "epoch": 3.1936946393164984e-07,
      "grad_norm": 197427.9276698208,
      "learning_rate": 0.001,
      "loss": 13.9636,
      "step": 352
    },
    {
      "epoch": 3.484030515617998e-07,
      "grad_norm": 115830.11366652456,
      "learning_rate": 0.001,
      "loss": 13.8083,
      "step": 384
    },
    {
      "epoch": 3.774366391919498e-07,
      "grad_norm": 95732.24025374105,
      "learning_rate": 0.001,
      "loss": 13.0327,
      "step": 416
    },
    {
      "epoch": 4.0647022682209983e-07,
      "grad_norm": 102237.66945700592,
      "learning_rate": 0.001,
      "loss": 11.8198,
      "step": 448
    },
    {
      "epoch": 4.355038144522498e-07,
      "grad_norm": 60949.802821666286,
      "learning_rate": 0.001,
      "loss": 11.3992,
      "step": 480
    },
    {
      "epoch": 4.645374020823998e-07,
      "grad_norm": 95341.68548961153,
      "learning_rate": 0.001,
      "loss": 11.3218,
      "step": 512
    },
    {
      "epoch": 4.935709897125498e-07,
      "grad_norm": 116087.86465432122,
      "learning_rate": 0.001,
      "loss": 10.6416,
      "step": 544
    },
    {
      "epoch": 5.226045773426998e-07,
      "grad_norm": 74499.10002140966,
      "learning_rate": 0.001,
      "loss": 9.8201,
      "step": 576
    },
    {
      "epoch": 5.516381649728497e-07,
      "grad_norm": 61439.122129145035,
      "learning_rate": 0.001,
      "loss": 9.4067,
      "step": 608
    },
    {
      "epoch": 5.806717526029997e-07,
      "grad_norm": 84489.02925232364,
      "learning_rate": 0.001,
      "loss": 9.2338,
      "step": 640
    },
    {
      "epoch": 6.097053402331497e-07,
      "grad_norm": 143151.34011248374,
      "learning_rate": 0.001,
      "loss": 9.0345,
      "step": 672
    },
    {
      "epoch": 6.387389278632997e-07,
      "grad_norm": 71524.54191394727,
      "learning_rate": 0.001,
      "loss": 8.3297,
      "step": 704
    },
    {
      "epoch": 6.677725154934497e-07,
      "grad_norm": 59850.29991570635,
      "learning_rate": 0.001,
      "loss": 7.713,
      "step": 736
    },
    {
      "epoch": 6.968061031235996e-07,
      "grad_norm": 118532.33523389303,
      "learning_rate": 0.001,
      "loss": 7.202,
      "step": 768
    },
    {
      "epoch": 7.258396907537496e-07,
      "grad_norm": 64046.49910806991,
      "learning_rate": 0.001,
      "loss": 7.2407,
      "step": 800
    },
    {
      "epoch": 7.548732783838996e-07,
      "grad_norm": 92933.20801521919,
      "learning_rate": 0.001,
      "loss": 6.5248,
      "step": 832
    },
    {
      "epoch": 7.839068660140496e-07,
      "grad_norm": 137766.56430353486,
      "learning_rate": 0.001,
      "loss": 6.2839,
      "step": 864
    },
    {
      "epoch": 8.129404536441997e-07,
      "grad_norm": 62298.594013027294,
      "learning_rate": 0.001,
      "loss": 6.084,
      "step": 896
    },
    {
      "epoch": 8.419740412743496e-07,
      "grad_norm": 89960.16184956538,
      "learning_rate": 0.001,
      "loss": 6.1453,
      "step": 928
    },
    {
      "epoch": 8.710076289044996e-07,
      "grad_norm": 74087.67174098536,
      "learning_rate": 0.001,
      "loss": 5.3455,
      "step": 960
    },
    {
      "epoch": 9.000412165346496e-07,
      "grad_norm": 50765.94937554108,
      "learning_rate": 0.001,
      "loss": 5.0284,
      "step": 992
    },
    {
      "epoch": 9.290748041647996e-07,
      "grad_norm": 104028.11004723675,
      "learning_rate": 0.001,
      "loss": 4.8666,
      "step": 1024
    },
    {
      "epoch": 9.290748041647996e-07,
      "eval_loss": 3.900390625,
      "eval_runtime": 92967.3552,
      "eval_samples_per_second": 229.294,
      "eval_steps_per_second": 1.791,
      "step": 1024
    },
    {
      "epoch": 9.581083917949496e-07,
      "grad_norm": 85673.29460222712,
      "learning_rate": 0.001,
      "loss": 4.7325,
      "step": 1056
    },
    {
      "epoch": 9.871419794250995e-07,
      "grad_norm": 98727.51219391685,
      "learning_rate": 0.001,
      "loss": 4.6762,
      "step": 1088
    },
    {
      "epoch": 1.0161755670552495e-06,
      "grad_norm": 135771.49132273684,
      "learning_rate": 0.001,
      "loss": 4.4052,
      "step": 1120
    },
    {
      "epoch": 1.0452091546853995e-06,
      "grad_norm": 147804.78888046896,
      "learning_rate": 0.001,
      "loss": 4.4097,
      "step": 1152
    },
    {
      "epoch": 1.0742427423155495e-06,
      "grad_norm": 89245.99782623307,
      "learning_rate": 0.001,
      "loss": 4.2807,
      "step": 1184
    },
    {
      "epoch": 1.1032763299456995e-06,
      "grad_norm": 54311.01015447973,
      "learning_rate": 0.001,
      "loss": 4.0439,
      "step": 1216
    },
    {
      "epoch": 1.1323099175758494e-06,
      "grad_norm": 42182.01322838918,
      "learning_rate": 0.001,
      "loss": 3.838,
      "step": 1248
    },
    {
      "epoch": 1.1613435052059994e-06,
      "grad_norm": 43193.97596887789,
      "learning_rate": 0.001,
      "loss": 3.5625,
      "step": 1280
    },
    {
      "epoch": 1.1903770928361494e-06,
      "grad_norm": 35337.592164718866,
      "learning_rate": 0.001,
      "loss": 3.5033,
      "step": 1312
    },
    {
      "epoch": 1.2194106804662994e-06,
      "grad_norm": 34943.988953752836,
      "learning_rate": 0.001,
      "loss": 3.4567,
      "step": 1344
    },
    {
      "epoch": 1.2484442680964494e-06,
      "grad_norm": 30254.360479111107,
      "learning_rate": 0.001,
      "loss": 3.5396,
      "step": 1376
    },
    {
      "epoch": 1.2774778557265993e-06,
      "grad_norm": 63014.134033564245,
      "learning_rate": 0.001,
      "loss": 3.2749,
      "step": 1408
    },
    {
      "epoch": 1.3065114433567493e-06,
      "grad_norm": 38762.73303057977,
      "learning_rate": 0.001,
      "loss": 3.1953,
      "step": 1440
    },
    {
      "epoch": 1.3355450309868993e-06,
      "grad_norm": 73164.4357321233,
      "learning_rate": 0.001,
      "loss": 3.1761,
      "step": 1472
    },
    {
      "epoch": 1.3645786186170493e-06,
      "grad_norm": 37150.84149787189,
      "learning_rate": 0.001,
      "loss": 3.1317,
      "step": 1504
    },
    {
      "epoch": 1.3936122062471993e-06,
      "grad_norm": 36790.07659682159,
      "learning_rate": 0.001,
      "loss": 3.0029,
      "step": 1536
    },
    {
      "epoch": 1.4226457938773492e-06,
      "grad_norm": 37243.52292681239,
      "learning_rate": 0.001,
      "loss": 2.9432,
      "step": 1568
    },
    {
      "epoch": 1.4516793815074992e-06,
      "grad_norm": 35069.23706042092,
      "learning_rate": 0.001,
      "loss": 2.8476,
      "step": 1600
    },
    {
      "epoch": 1.4807129691376492e-06,
      "grad_norm": 66659.58607732275,
      "learning_rate": 0.001,
      "loss": 2.9178,
      "step": 1632
    },
    {
      "epoch": 1.5097465567677992e-06,
      "grad_norm": 33321.831882416074,
      "learning_rate": 0.001,
      "loss": 2.806,
      "step": 1664
    },
    {
      "epoch": 1.5387801443979492e-06,
      "grad_norm": 25289.43558089029,
      "learning_rate": 0.001,
      "loss": 2.8203,
      "step": 1696
    },
    {
      "epoch": 1.5678137320280991e-06,
      "grad_norm": 23911.472811184172,
      "learning_rate": 0.001,
      "loss": 2.9467,
      "step": 1728
    },
    {
      "epoch": 1.5968473196582493e-06,
      "grad_norm": 15992.564522302231,
      "learning_rate": 0.001,
      "loss": 3.1945,
      "step": 1760
    },
    {
      "epoch": 1.6258809072883993e-06,
      "grad_norm": 39322.40765772106,
      "learning_rate": 0.001,
      "loss": 2.9077,
      "step": 1792
    },
    {
      "epoch": 1.6549144949185493e-06,
      "grad_norm": 34614.08499440654,
      "learning_rate": 0.001,
      "loss": 2.7701,
      "step": 1824
    },
    {
      "epoch": 1.6839480825486993e-06,
      "grad_norm": 17402.551077356446,
      "learning_rate": 0.001,
      "loss": 2.6499,
      "step": 1856
    },
    {
      "epoch": 1.7129816701788493e-06,
      "grad_norm": 31327.74999900248,
      "learning_rate": 0.001,
      "loss": 2.6458,
      "step": 1888
    },
    {
      "epoch": 1.7420152578089992e-06,
      "grad_norm": 30487.848070993798,
      "learning_rate": 0.001,
      "loss": 2.6434,
      "step": 1920
    },
    {
      "epoch": 1.7710488454391492e-06,
      "grad_norm": 23749.536416528215,
      "learning_rate": 0.001,
      "loss": 2.4745,
      "step": 1952
    },
    {
      "epoch": 1.8000824330692992e-06,
      "grad_norm": 15104.569242451105,
      "learning_rate": 0.001,
      "loss": 2.5208,
      "step": 1984
    },
    {
      "epoch": 1.8291160206994492e-06,
      "grad_norm": 26800.296267019137,
      "learning_rate": 0.001,
      "loss": 2.4775,
      "step": 2016
    },
    {
      "epoch": 1.8581496083295992e-06,
      "grad_norm": 21926.99281707366,
      "learning_rate": 0.001,
      "loss": 2.4528,
      "step": 2048
    },
    {
      "epoch": 1.8581496083295992e-06,
      "eval_loss": 2.5234375,
      "eval_runtime": 92996.8014,
      "eval_samples_per_second": 229.222,
      "eval_steps_per_second": 1.791,
      "step": 2048
    },
    {
      "epoch": 1.8871831959597491e-06,
      "grad_norm": 15146.168723475914,
      "learning_rate": 0.001,
      "loss": 2.5472,
      "step": 2080
    },
    {
      "epoch": 1.916216783589899e-06,
      "grad_norm": 18356.476568230624,
      "learning_rate": 0.001,
      "loss": 2.4733,
      "step": 2112
    },
    {
      "epoch": 1.945250371220049e-06,
      "grad_norm": 30659.215384611525,
      "learning_rate": 0.001,
      "loss": 2.424,
      "step": 2144
    },
    {
      "epoch": 1.974283958850199e-06,
      "grad_norm": 17211.764813638372,
      "learning_rate": 0.001,
      "loss": 2.4397,
      "step": 2176
    },
    {
      "epoch": 2.003317546480349e-06,
      "grad_norm": 35558.42111230475,
      "learning_rate": 0.001,
      "loss": 2.4664,
      "step": 2208
    },
    {
      "epoch": 2.032351134110499e-06,
      "grad_norm": 79888.19967930182,
      "learning_rate": 0.001,
      "loss": 2.4292,
      "step": 2240
    },
    {
      "epoch": 2.061384721740649e-06,
      "grad_norm": 33960.20588865739,
      "learning_rate": 0.001,
      "loss": 2.4078,
      "step": 2272
    },
    {
      "epoch": 2.090418309370799e-06,
      "grad_norm": 201921.00475185836,
      "learning_rate": 0.001,
      "loss": 2.5416,
      "step": 2304
    },
    {
      "epoch": 2.1194518970009488e-06,
      "grad_norm": 11413.826965571188,
      "learning_rate": 0.001,
      "loss": 2.6804,
      "step": 2336
    },
    {
      "epoch": 2.148485484631099e-06,
      "grad_norm": 14753.853937192142,
      "learning_rate": 0.001,
      "loss": 2.5419,
      "step": 2368
    },
    {
      "epoch": 2.177519072261249e-06,
      "grad_norm": 12224.238708402254,
      "learning_rate": 0.001,
      "loss": 2.387,
      "step": 2400
    },
    {
      "epoch": 2.206552659891399e-06,
      "grad_norm": 7282.3214018608105,
      "learning_rate": 0.001,
      "loss": 2.3356,
      "step": 2432
    },
    {
      "epoch": 2.235586247521549e-06,
      "grad_norm": 9148.65607616769,
      "learning_rate": 0.001,
      "loss": 2.3223,
      "step": 2464
    },
    {
      "epoch": 2.264619835151699e-06,
      "grad_norm": 11665.724923895643,
      "learning_rate": 0.001,
      "loss": 2.3244,
      "step": 2496
    },
    {
      "epoch": 2.293653422781849e-06,
      "grad_norm": 8827.505083544274,
      "learning_rate": 0.001,
      "loss": 2.2969,
      "step": 2528
    },
    {
      "epoch": 2.322687010411999e-06,
      "grad_norm": 12260.926494355961,
      "learning_rate": 0.001,
      "loss": 2.3314,
      "step": 2560
    },
    {
      "epoch": 2.351720598042149e-06,
      "grad_norm": 5051.146775733209,
      "learning_rate": 0.001,
      "loss": 2.3524,
      "step": 2592
    },
    {
      "epoch": 2.380754185672299e-06,
      "grad_norm": 4644.961894353924,
      "learning_rate": 0.001,
      "loss": 2.4875,
      "step": 2624
    },
    {
      "epoch": 2.409787773302449e-06,
      "grad_norm": 6521.366382898603,
      "learning_rate": 0.001,
      "loss": 2.4814,
      "step": 2656
    },
    {
      "epoch": 2.4388213609325988e-06,
      "grad_norm": 4141.347184190188,
      "learning_rate": 0.001,
      "loss": 2.4597,
      "step": 2688
    },
    {
      "epoch": 2.467854948562749e-06,
      "grad_norm": 6109.801643670604,
      "learning_rate": 0.001,
      "loss": 2.3276,
      "step": 2720
    },
    {
      "epoch": 2.4968885361928987e-06,
      "grad_norm": 5459.483308885558,
      "learning_rate": 0.001,
      "loss": 2.3556,
      "step": 2752
    },
    {
      "epoch": 2.525922123823049e-06,
      "grad_norm": 3942.089927944313,
      "learning_rate": 0.001,
      "loss": 2.3498,
      "step": 2784
    },
    {
      "epoch": 2.5549557114531987e-06,
      "grad_norm": 4980.340475308893,
      "learning_rate": 0.001,
      "loss": 2.366,
      "step": 2816
    },
    {
      "epoch": 2.583989299083349e-06,
      "grad_norm": 125560.159031438,
      "learning_rate": 0.001,
      "loss": 2.3533,
      "step": 2848
    },
    {
      "epoch": 2.6130228867134986e-06,
      "grad_norm": 5761.182029323497,
      "learning_rate": 0.001,
      "loss": 2.3909,
      "step": 2880
    },
    {
      "epoch": 2.642056474343649e-06,
      "grad_norm": 9050.063135138893,
      "learning_rate": 0.001,
      "loss": 2.4551,
      "step": 2912
    },
    {
      "epoch": 2.6710900619737986e-06,
      "grad_norm": 22282.137756508015,
      "learning_rate": 0.001,
      "loss": 2.3929,
      "step": 2944
    },
    {
      "epoch": 2.700123649603949e-06,
      "grad_norm": 4749.121155540254,
      "learning_rate": 0.001,
      "loss": 2.3619,
      "step": 2976
    },
    {
      "epoch": 2.7291572372340986e-06,
      "grad_norm": 4525.613922441905,
      "learning_rate": 0.001,
      "loss": 2.3572,
      "step": 3008
    },
    {
      "epoch": 2.7581908248642488e-06,
      "grad_norm": 3097.436863763328,
      "learning_rate": 0.001,
      "loss": 2.3414,
      "step": 3040
    },
    {
      "epoch": 2.7872244124943985e-06,
      "grad_norm": 3557.5253337116237,
      "learning_rate": 0.001,
      "loss": 2.3015,
      "step": 3072
    },
    {
      "epoch": 2.7872244124943985e-06,
      "eval_loss": 2.23828125,
      "eval_runtime": 93139.2559,
      "eval_samples_per_second": 228.871,
      "eval_steps_per_second": 1.788,
      "step": 3072
    }
  ],
  "logging_steps": 32,
  "max_steps": 1102171747,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 1024,
  "total_flos": 5.995066534605619e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
