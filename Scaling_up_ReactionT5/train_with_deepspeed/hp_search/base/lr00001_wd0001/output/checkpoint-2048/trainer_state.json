{
  "best_metric": 3.640625,
  "best_model_checkpoint": "./output/checkpoint-2048",
  "epoch": 1.8581496083295992e-06,
  "eval_steps": 1024,
  "global_step": 2048,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.9033587630149987e-08,
      "grad_norm": 451138.0238286283,
      "learning_rate": 0.0001,
      "loss": 85.7974,
      "step": 32
    },
    {
      "epoch": 5.8067175260299974e-08,
      "grad_norm": 262949.15207317175,
      "learning_rate": 0.0001,
      "loss": 30.813,
      "step": 64
    },
    {
      "epoch": 8.710076289044995e-08,
      "grad_norm": 152398.97196503656,
      "learning_rate": 0.0001,
      "loss": 26.0215,
      "step": 96
    },
    {
      "epoch": 1.1613435052059995e-07,
      "grad_norm": 196934.03240679353,
      "learning_rate": 0.0001,
      "loss": 21.0659,
      "step": 128
    },
    {
      "epoch": 1.4516793815074993e-07,
      "grad_norm": 172867.89443965585,
      "learning_rate": 0.0001,
      "loss": 20.8569,
      "step": 160
    },
    {
      "epoch": 1.742015257808999e-07,
      "grad_norm": 187451.6003239236,
      "learning_rate": 0.0001,
      "loss": 18.7622,
      "step": 192
    },
    {
      "epoch": 2.0323511341104991e-07,
      "grad_norm": 148661.96726802722,
      "learning_rate": 0.0001,
      "loss": 18.3557,
      "step": 224
    },
    {
      "epoch": 2.322687010411999e-07,
      "grad_norm": 82456.24517281877,
      "learning_rate": 0.0001,
      "loss": 15.8132,
      "step": 256
    },
    {
      "epoch": 2.613022886713499e-07,
      "grad_norm": 89193.18565899527,
      "learning_rate": 0.0001,
      "loss": 15.5491,
      "step": 288
    },
    {
      "epoch": 2.9033587630149986e-07,
      "grad_norm": 75385.50237280375,
      "learning_rate": 0.0001,
      "loss": 14.6208,
      "step": 320
    },
    {
      "epoch": 3.1936946393164984e-07,
      "grad_norm": 114281.05004767851,
      "learning_rate": 0.0001,
      "loss": 13.2466,
      "step": 352
    },
    {
      "epoch": 3.484030515617998e-07,
      "grad_norm": 132568.02462132415,
      "learning_rate": 0.0001,
      "loss": 13.8298,
      "step": 384
    },
    {
      "epoch": 3.774366391919498e-07,
      "grad_norm": 88494.16270014651,
      "learning_rate": 0.0001,
      "loss": 13.3208,
      "step": 416
    },
    {
      "epoch": 4.0647022682209983e-07,
      "grad_norm": 111915.49769357235,
      "learning_rate": 0.0001,
      "loss": 12.5298,
      "step": 448
    },
    {
      "epoch": 4.355038144522498e-07,
      "grad_norm": 118019.23924513324,
      "learning_rate": 0.0001,
      "loss": 11.8328,
      "step": 480
    },
    {
      "epoch": 4.645374020823998e-07,
      "grad_norm": 89126.45290821351,
      "learning_rate": 0.0001,
      "loss": 12.0154,
      "step": 512
    },
    {
      "epoch": 4.935709897125498e-07,
      "grad_norm": 78225.27591514139,
      "learning_rate": 0.0001,
      "loss": 11.6797,
      "step": 544
    },
    {
      "epoch": 5.226045773426998e-07,
      "grad_norm": 168700.2167633462,
      "learning_rate": 0.0001,
      "loss": 11.2327,
      "step": 576
    },
    {
      "epoch": 5.516381649728497e-07,
      "grad_norm": 52601.6365524876,
      "learning_rate": 0.0001,
      "loss": 11.4082,
      "step": 608
    },
    {
      "epoch": 5.806717526029997e-07,
      "grad_norm": 63327.364574881845,
      "learning_rate": 0.0001,
      "loss": 11.3889,
      "step": 640
    },
    {
      "epoch": 6.097053402331497e-07,
      "grad_norm": 130675.33410709153,
      "learning_rate": 0.0001,
      "loss": 11.365,
      "step": 672
    },
    {
      "epoch": 6.387389278632997e-07,
      "grad_norm": 89631.52927402276,
      "learning_rate": 0.0001,
      "loss": 10.3994,
      "step": 704
    },
    {
      "epoch": 6.677725154934497e-07,
      "grad_norm": 57931.863494971396,
      "learning_rate": 0.0001,
      "loss": 10.0906,
      "step": 736
    },
    {
      "epoch": 6.968061031235996e-07,
      "grad_norm": 37847.19582743219,
      "learning_rate": 0.0001,
      "loss": 10.1682,
      "step": 768
    },
    {
      "epoch": 7.258396907537496e-07,
      "grad_norm": 56392.51148867197,
      "learning_rate": 0.0001,
      "loss": 10.073,
      "step": 800
    },
    {
      "epoch": 7.548732783838996e-07,
      "grad_norm": 52542.17262352214,
      "learning_rate": 0.0001,
      "loss": 9.5305,
      "step": 832
    },
    {
      "epoch": 7.839068660140496e-07,
      "grad_norm": 93042.89316223997,
      "learning_rate": 0.0001,
      "loss": 9.7058,
      "step": 864
    },
    {
      "epoch": 8.129404536441997e-07,
      "grad_norm": 59924.65722889035,
      "learning_rate": 0.0001,
      "loss": 9.6492,
      "step": 896
    },
    {
      "epoch": 8.419740412743496e-07,
      "grad_norm": 114375.43456529466,
      "learning_rate": 0.0001,
      "loss": 9.438,
      "step": 928
    },
    {
      "epoch": 8.710076289044996e-07,
      "grad_norm": 110460.56121530435,
      "learning_rate": 0.0001,
      "loss": 9.1423,
      "step": 960
    },
    {
      "epoch": 9.000412165346496e-07,
      "grad_norm": 67300.38478344682,
      "learning_rate": 0.0001,
      "loss": 8.8143,
      "step": 992
    },
    {
      "epoch": 9.290748041647996e-07,
      "grad_norm": 137239.99463713192,
      "learning_rate": 0.0001,
      "loss": 8.8447,
      "step": 1024
    },
    {
      "epoch": 9.290748041647996e-07,
      "eval_loss": 5.34375,
      "eval_runtime": 93203.1888,
      "eval_samples_per_second": 228.714,
      "eval_steps_per_second": 1.787,
      "step": 1024
    },
    {
      "epoch": 9.581083917949496e-07,
      "grad_norm": 138279.12386184692,
      "learning_rate": 0.0001,
      "loss": 8.4609,
      "step": 1056
    },
    {
      "epoch": 9.871419794250995e-07,
      "grad_norm": 141077.0603606412,
      "learning_rate": 0.0001,
      "loss": 8.675,
      "step": 1088
    },
    {
      "epoch": 1.0161755670552495e-06,
      "grad_norm": 121901.33610424458,
      "learning_rate": 0.0001,
      "loss": 8.5092,
      "step": 1120
    },
    {
      "epoch": 1.0452091546853995e-06,
      "grad_norm": 178005.79729885203,
      "learning_rate": 0.0001,
      "loss": 8.1788,
      "step": 1152
    },
    {
      "epoch": 1.0742427423155495e-06,
      "grad_norm": 185945.73444959687,
      "learning_rate": 0.0001,
      "loss": 8.0242,
      "step": 1184
    },
    {
      "epoch": 1.1032763299456995e-06,
      "grad_norm": 82704.04217449109,
      "learning_rate": 0.0001,
      "loss": 7.765,
      "step": 1216
    },
    {
      "epoch": 1.1323099175758494e-06,
      "grad_norm": 135860.98353832125,
      "learning_rate": 0.0001,
      "loss": 7.7346,
      "step": 1248
    },
    {
      "epoch": 1.1613435052059994e-06,
      "grad_norm": 159960.77359152774,
      "learning_rate": 0.0001,
      "loss": 7.342,
      "step": 1280
    },
    {
      "epoch": 1.1903770928361494e-06,
      "grad_norm": 79486.53561453035,
      "learning_rate": 0.0001,
      "loss": 7.3174,
      "step": 1312
    },
    {
      "epoch": 1.2194106804662994e-06,
      "grad_norm": 105578.90880284755,
      "learning_rate": 0.0001,
      "loss": 7.3243,
      "step": 1344
    },
    {
      "epoch": 1.2484442680964494e-06,
      "grad_norm": 128477.15961991066,
      "learning_rate": 0.0001,
      "loss": 6.931,
      "step": 1376
    },
    {
      "epoch": 1.2774778557265993e-06,
      "grad_norm": 134794.61259264036,
      "learning_rate": 0.0001,
      "loss": 7.0537,
      "step": 1408
    },
    {
      "epoch": 1.3065114433567493e-06,
      "grad_norm": 110054.23695614813,
      "learning_rate": 0.0001,
      "loss": 6.7357,
      "step": 1440
    },
    {
      "epoch": 1.3355450309868993e-06,
      "grad_norm": 164786.77636266814,
      "learning_rate": 0.0001,
      "loss": 6.7771,
      "step": 1472
    },
    {
      "epoch": 1.3645786186170493e-06,
      "grad_norm": 128029.33088944892,
      "learning_rate": 0.0001,
      "loss": 6.6121,
      "step": 1504
    },
    {
      "epoch": 1.3936122062471993e-06,
      "grad_norm": 94089.54645442819,
      "learning_rate": 0.0001,
      "loss": 6.5745,
      "step": 1536
    },
    {
      "epoch": 1.4226457938773492e-06,
      "grad_norm": 169391.05619837192,
      "learning_rate": 0.0001,
      "loss": 6.3392,
      "step": 1568
    },
    {
      "epoch": 1.4516793815074992e-06,
      "grad_norm": 86932.35071019303,
      "learning_rate": 0.0001,
      "loss": 6.6266,
      "step": 1600
    },
    {
      "epoch": 1.4807129691376492e-06,
      "grad_norm": 100247.17803509484,
      "learning_rate": 0.0001,
      "loss": 6.1584,
      "step": 1632
    },
    {
      "epoch": 1.5097465567677992e-06,
      "grad_norm": 41040.40993947307,
      "learning_rate": 0.0001,
      "loss": 6.3629,
      "step": 1664
    },
    {
      "epoch": 1.5387801443979492e-06,
      "grad_norm": 54438.01289540242,
      "learning_rate": 0.0001,
      "loss": 6.0178,
      "step": 1696
    },
    {
      "epoch": 1.5678137320280991e-06,
      "grad_norm": 59820.0835840272,
      "learning_rate": 0.0001,
      "loss": 6.1539,
      "step": 1728
    },
    {
      "epoch": 1.5968473196582493e-06,
      "grad_norm": 98096.87848244714,
      "learning_rate": 0.0001,
      "loss": 6.3234,
      "step": 1760
    },
    {
      "epoch": 1.6258809072883993e-06,
      "grad_norm": 32951.30613496224,
      "learning_rate": 0.0001,
      "loss": 6.1445,
      "step": 1792
    },
    {
      "epoch": 1.6549144949185493e-06,
      "grad_norm": 62409.10529722406,
      "learning_rate": 0.0001,
      "loss": 5.9102,
      "step": 1824
    },
    {
      "epoch": 1.6839480825486993e-06,
      "grad_norm": 47860.343500647796,
      "learning_rate": 0.0001,
      "loss": 5.7457,
      "step": 1856
    },
    {
      "epoch": 1.7129816701788493e-06,
      "grad_norm": 55861.429269219385,
      "learning_rate": 0.0001,
      "loss": 5.583,
      "step": 1888
    },
    {
      "epoch": 1.7420152578089992e-06,
      "grad_norm": 63991.590572511945,
      "learning_rate": 0.0001,
      "loss": 5.6187,
      "step": 1920
    },
    {
      "epoch": 1.7710488454391492e-06,
      "grad_norm": 50421.95458329635,
      "learning_rate": 0.0001,
      "loss": 5.5286,
      "step": 1952
    },
    {
      "epoch": 1.8000824330692992e-06,
      "grad_norm": 56012.106405669125,
      "learning_rate": 0.0001,
      "loss": 5.3546,
      "step": 1984
    },
    {
      "epoch": 1.8291160206994492e-06,
      "grad_norm": 55259.70886640645,
      "learning_rate": 0.0001,
      "loss": 5.343,
      "step": 2016
    },
    {
      "epoch": 1.8581496083295992e-06,
      "grad_norm": 40083.676078922705,
      "learning_rate": 0.0001,
      "loss": 5.2913,
      "step": 2048
    },
    {
      "epoch": 1.8581496083295992e-06,
      "eval_loss": 3.640625,
      "eval_runtime": 93201.8616,
      "eval_samples_per_second": 228.717,
      "eval_steps_per_second": 1.787,
      "step": 2048
    }
  ],
  "logging_steps": 32,
  "max_steps": 1102171747,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 1024,
  "total_flos": 3.996711023070413e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
