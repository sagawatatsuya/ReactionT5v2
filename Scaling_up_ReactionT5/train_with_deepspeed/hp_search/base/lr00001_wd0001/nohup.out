[2024-04-22 12:00:11,985] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-22 12:00:12,665] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-22 12:00:12,666] [INFO] [runner.py:568:main] cmd = /home/sagawa/miniconda3/envs/deepspeed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=12347 --enable_each_rank_log=None /data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py --do_train --do_eval --num_train_epochs=10 --output_dir=./output --overwrite_output_dir --save_total_limit=2 --deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json --per_device_train_batch_size=32 --per_device_eval_batch_size=128 --learning_rate=0.0001 --weight_decay=0.001 --warmup_steps=10000 --logging_steps=32 --save_steps=512 --eval_steps=512 --config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/ --max_seq_length=512 --num_workers=2 --local_rank=1
[2024-04-22 12:00:14,014] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-22 12:00:14,621] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}
[2024-04-22 12:00:14,621] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-22 12:00:14,621] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-22 12:00:14,621] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-22 12:00:14,621] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2
[2024-04-22 12:00:14,622] [INFO] [launch.py:253:main] process 306299 spawned with command: ['/home/sagawa/miniconda3/envs/deepspeed/bin/python', '-u', '/data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py', '--local_rank=0', '--do_train', '--do_eval', '--num_train_epochs=10', '--output_dir=./output', '--overwrite_output_dir', '--save_total_limit=2', '--deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json', '--per_device_train_batch_size=32', '--per_device_eval_batch_size=128', '--learning_rate=0.0001', '--weight_decay=0.001', '--warmup_steps=10000', '--logging_steps=32', '--save_steps=512', '--eval_steps=512', '--config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/', '--max_seq_length=512', '--num_workers=2', '--local_rank=1']
[2024-04-22 12:00:19,018] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-22 12:00:19,491] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-22 12:00:19,491] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
max_steps is given, it will override any value given in num_train_epochs
training started
{'loss': 85.7974, 'grad_norm': 451138.0238286283, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 30.813, 'grad_norm': 262949.15207317175, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 26.0215, 'grad_norm': 152398.97196503656, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 21.0659, 'grad_norm': 196934.03240679353, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 20.8569, 'grad_norm': 172867.89443965585, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 18.7622, 'grad_norm': 187451.6003239236, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 18.3557, 'grad_norm': 148661.96726802722, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 15.8132, 'grad_norm': 82456.24517281877, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 15.5491, 'grad_norm': 89193.18565899527, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 14.6208, 'grad_norm': 75385.50237280375, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 13.2466, 'grad_norm': 114281.05004767851, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 13.8298, 'grad_norm': 132568.02462132415, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 13.3208, 'grad_norm': 88494.16270014651, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 12.5298, 'grad_norm': 111915.49769357235, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 11.8328, 'grad_norm': 118019.23924513324, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 12.0154, 'grad_norm': 89126.45290821351, 'learning_rate': 0.0001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
[2024-04-23 12:00:09,904] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 306299
[2024-04-23 12:00:09,973] [INFO] [launch.py:325:sigkill_handler] Main process received SIGTERM, exiting
[2024-04-23 15:02:08,988] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-23 15:02:09,762] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-23 15:02:09,763] [INFO] [runner.py:568:main] cmd = /home/sagawa/miniconda3/envs/deepspeed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=12347 --enable_each_rank_log=None /data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py --do_train --do_eval --num_train_epochs=10 --output_dir=./output --overwrite_output_dir --save_total_limit=2 --deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json --per_device_train_batch_size=32 --per_device_eval_batch_size=128 --learning_rate=0.0001 --weight_decay=0.001 --warmup_steps=10000 --logging_steps=32 --save_steps=1024 --eval_steps=1024 --config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/ --max_seq_length=512 --num_workers=2 --local_rank=1
[2024-04-23 15:02:11,114] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-23 15:02:11,805] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}
[2024-04-23 15:02:11,805] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-23 15:02:11,805] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-23 15:02:11,805] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-23 15:02:11,805] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2
[2024-04-23 15:02:11,806] [INFO] [launch.py:253:main] process 377149 spawned with command: ['/home/sagawa/miniconda3/envs/deepspeed/bin/python', '-u', '/data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py', '--local_rank=0', '--do_train', '--do_eval', '--num_train_epochs=10', '--output_dir=./output', '--overwrite_output_dir', '--save_total_limit=2', '--deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json', '--per_device_train_batch_size=32', '--per_device_eval_batch_size=128', '--learning_rate=0.0001', '--weight_decay=0.001', '--warmup_steps=10000', '--logging_steps=32', '--save_steps=1024', '--eval_steps=1024', '--config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/', '--max_seq_length=512', '--num_workers=2', '--local_rank=1']
output_dir already exists
[2024-04-23 15:02:16,250] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-23 15:02:16,694] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-23 15:02:16,694] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
max_steps is given, it will override any value given in num_train_epochs
training started
{'loss': 85.7974, 'grad_norm': 451138.0238286283, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 30.813, 'grad_norm': 262949.15207317175, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 26.0215, 'grad_norm': 152398.97196503656, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 21.0659, 'grad_norm': 196934.03240679353, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 20.8569, 'grad_norm': 172867.89443965585, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 18.7622, 'grad_norm': 187451.6003239236, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 18.3557, 'grad_norm': 148661.96726802722, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 15.8132, 'grad_norm': 82456.24517281877, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 15.5491, 'grad_norm': 89193.18565899527, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 14.6208, 'grad_norm': 75385.50237280375, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 13.2466, 'grad_norm': 114281.05004767851, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 13.8298, 'grad_norm': 132568.02462132415, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 13.3208, 'grad_norm': 88494.16270014651, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 12.5298, 'grad_norm': 111915.49769357235, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 11.8328, 'grad_norm': 118019.23924513324, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 12.0154, 'grad_norm': 89126.45290821351, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 11.6797, 'grad_norm': 78225.27591514139, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 11.2327, 'grad_norm': 168700.2167633462, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 11.4082, 'grad_norm': 52601.6365524876, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 11.3889, 'grad_norm': 63327.364574881845, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 11.365, 'grad_norm': 130675.33410709153, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 10.3994, 'grad_norm': 89631.52927402276, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 10.0906, 'grad_norm': 57931.863494971396, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 10.1682, 'grad_norm': 37847.19582743219, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 10.073, 'grad_norm': 56392.51148867197, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 9.5305, 'grad_norm': 52542.17262352214, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 9.7058, 'grad_norm': 93042.89316223997, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 9.6492, 'grad_norm': 59924.65722889035, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 9.438, 'grad_norm': 114375.43456529466, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 9.1423, 'grad_norm': 110460.56121530435, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 8.8143, 'grad_norm': 67300.38478344682, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 8.8447, 'grad_norm': 137239.99463713192, 'learning_rate': 0.0001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 5.34375, 'eval_runtime': 93203.1888, 'eval_samples_per_second': 228.714, 'eval_steps_per_second': 1.787, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 8.4609, 'grad_norm': 138279.12386184692, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 8.675, 'grad_norm': 141077.0603606412, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 8.5092, 'grad_norm': 121901.33610424458, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 8.1788, 'grad_norm': 178005.79729885203, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 8.0242, 'grad_norm': 185945.73444959687, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 7.765, 'grad_norm': 82704.04217449109, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 7.7346, 'grad_norm': 135860.98353832125, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 7.342, 'grad_norm': 159960.77359152774, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 7.3174, 'grad_norm': 79486.53561453035, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 7.3243, 'grad_norm': 105578.90880284755, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.931, 'grad_norm': 128477.15961991066, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 7.0537, 'grad_norm': 134794.61259264036, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.7357, 'grad_norm': 110054.23695614813, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.7771, 'grad_norm': 164786.77636266814, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.6121, 'grad_norm': 128029.33088944892, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.5745, 'grad_norm': 94089.54645442819, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.3392, 'grad_norm': 169391.05619837192, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.6266, 'grad_norm': 86932.35071019303, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.1584, 'grad_norm': 100247.17803509484, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.3629, 'grad_norm': 41040.40993947307, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.0178, 'grad_norm': 54438.01289540242, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.1539, 'grad_norm': 59820.0835840272, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.3234, 'grad_norm': 98096.87848244714, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 6.1445, 'grad_norm': 32951.30613496224, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.9102, 'grad_norm': 62409.10529722406, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.7457, 'grad_norm': 47860.343500647796, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.583, 'grad_norm': 55861.429269219385, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.6187, 'grad_norm': 63991.590572511945, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.5286, 'grad_norm': 50421.95458329635, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.3546, 'grad_norm': 56012.106405669125, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.343, 'grad_norm': 55259.70886640645, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.2913, 'grad_norm': 40083.676078922705, 'learning_rate': 0.0001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 3.640625, 'eval_runtime': 93201.8616, 'eval_samples_per_second': 228.717, 'eval_steps_per_second': 1.787, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 5.3947, 'grad_norm': 43244.82119283186, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.2179, 'grad_norm': 50109.813809272935, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.113, 'grad_norm': 35924.974488508684, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.0001, 'grad_norm': 38578.05116902615, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.988, 'grad_norm': 37573.45398017063, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.1058, 'grad_norm': 34257.52168502561, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.1521, 'grad_norm': 50108.761070295885, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.0828, 'grad_norm': 49356.079909166205, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 5.0106, 'grad_norm': 50515.70781450063, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.7682, 'grad_norm': 52738.65163236542, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.6168, 'grad_norm': 32168.775419651898, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.5505, 'grad_norm': 51891.42360737466, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.5685, 'grad_norm': 44790.07247147519, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.5402, 'grad_norm': 41996.23906970718, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.4452, 'grad_norm': 33733.83535858323, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.4037, 'grad_norm': 39701.223356465984, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.2596, 'grad_norm': 38064.6094949101, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.3903, 'grad_norm': 43083.98681644957, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.5453, 'grad_norm': 45481.35688389254, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.2599, 'grad_norm': 104159.47956859233, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.198, 'grad_norm': 60984.68476593119, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.2841, 'grad_norm': 67389.0113297413, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.1653, 'grad_norm': 59733.90219967217, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.2251, 'grad_norm': 62778.30351323616, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.197, 'grad_norm': 114846.00332619328, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.0815, 'grad_norm': 69208.39162991726, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.1083, 'grad_norm': 66633.12329464979, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.983, 'grad_norm': 63777.026083065364, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 4.0231, 'grad_norm': 64913.48063384061, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.9854, 'grad_norm': 66431.80443131136, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.9175, 'grad_norm': 87560.49118181098, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.9168, 'grad_norm': 104445.48097452565, 'learning_rate': 0.0001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 2.943359375, 'eval_runtime': 93275.2241, 'eval_samples_per_second': 228.537, 'eval_steps_per_second': 1.785, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 3.8669, 'grad_norm': 86116.7884213061, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.81, 'grad_norm': 80455.01122987928, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.894, 'grad_norm': 64332.47024636937, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.9099, 'grad_norm': 82414.74586504529, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.7416, 'grad_norm': 70981.91544330148, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.7985, 'grad_norm': 85039.16537690148, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.7238, 'grad_norm': 96813.9813869877, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.721, 'grad_norm': 117309.30547914773, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.7915, 'grad_norm': 95259.82254864849, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.7471, 'grad_norm': 110962.87984727144, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.6633, 'grad_norm': 92358.4065691911, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.903, 'grad_norm': 112120.773168936, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.8969, 'grad_norm': 85806.83385372054, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.8817, 'grad_norm': 61272.12224168509, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.6194, 'grad_norm': 32660.121249009473, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.5621, 'grad_norm': 20528.103273317774, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.5624, 'grad_norm': 35491.67699616348, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.5905, 'grad_norm': 31756.730814112463, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.5406, 'grad_norm': 35275.72400390954, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4795, 'grad_norm': 30224.343566072697, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4669, 'grad_norm': 34446.566621363, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4547, 'grad_norm': 23973.815716318502, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4816, 'grad_norm': 34848.44053899686, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4836, 'grad_norm': 47166.98366442357, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4365, 'grad_norm': 30836.922544248802, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.3988, 'grad_norm': 34170.1249631897, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4061, 'grad_norm': 21606.66082484751, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4348, 'grad_norm': 44039.19790368576, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4299, 'grad_norm': 27839.165217369577, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.5228, 'grad_norm': 24198.077444292965, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4318, 'grad_norm': 31801.25255394825, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 3.4036, 'grad_norm': 24970.645806626628, 'learning_rate': 0.0001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
[2024-04-27 15:02:06,798] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 377149
[2024-04-27 15:02:06,908] [INFO] [launch.py:325:sigkill_handler] Main process received SIGTERM, exiting
