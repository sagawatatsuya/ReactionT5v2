{
  "best_metric": 3.9375,
  "best_model_checkpoint": "./output/checkpoint-3072",
  "epoch": 2.7872244124943985e-06,
  "eval_steps": 1024,
  "global_step": 3072,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.9033587630149987e-08,
      "grad_norm": 1246302.545561069,
      "learning_rate": 1e-05,
      "loss": 52.2119,
      "step": 32
    },
    {
      "epoch": 5.8067175260299974e-08,
      "grad_norm": 778724.2331506064,
      "learning_rate": 1e-05,
      "loss": 26.9819,
      "step": 64
    },
    {
      "epoch": 8.710076289044995e-08,
      "grad_norm": 440532.03971561475,
      "learning_rate": 1e-05,
      "loss": 23.0005,
      "step": 96
    },
    {
      "epoch": 1.1613435052059995e-07,
      "grad_norm": 390024.9208037866,
      "learning_rate": 1e-05,
      "loss": 20.7959,
      "step": 128
    },
    {
      "epoch": 1.4516793815074993e-07,
      "grad_norm": 482810.2640831075,
      "learning_rate": 1e-05,
      "loss": 19.5601,
      "step": 160
    },
    {
      "epoch": 1.742015257808999e-07,
      "grad_norm": 279617.26607632794,
      "learning_rate": 1e-05,
      "loss": 18.5884,
      "step": 192
    },
    {
      "epoch": 2.0323511341104991e-07,
      "grad_norm": 419022.58085215406,
      "learning_rate": 1e-05,
      "loss": 18.0576,
      "step": 224
    },
    {
      "epoch": 2.322687010411999e-07,
      "grad_norm": 477799.2792292596,
      "learning_rate": 1e-05,
      "loss": 17.1477,
      "step": 256
    },
    {
      "epoch": 2.613022886713499e-07,
      "grad_norm": 316659.87696580694,
      "learning_rate": 1e-05,
      "loss": 16.7324,
      "step": 288
    },
    {
      "epoch": 2.9033587630149986e-07,
      "grad_norm": 266074.60674029007,
      "learning_rate": 1e-05,
      "loss": 16.3032,
      "step": 320
    },
    {
      "epoch": 3.1936946393164984e-07,
      "grad_norm": 213796.05281669722,
      "learning_rate": 1e-05,
      "loss": 15.5691,
      "step": 352
    },
    {
      "epoch": 3.484030515617998e-07,
      "grad_norm": 304936.56158617645,
      "learning_rate": 1e-05,
      "loss": 15.4065,
      "step": 384
    },
    {
      "epoch": 3.774366391919498e-07,
      "grad_norm": 243443.49872609045,
      "learning_rate": 1e-05,
      "loss": 15.2937,
      "step": 416
    },
    {
      "epoch": 4.0647022682209983e-07,
      "grad_norm": 171806.77832961074,
      "learning_rate": 1e-05,
      "loss": 14.2649,
      "step": 448
    },
    {
      "epoch": 4.355038144522498e-07,
      "grad_norm": 162063.9942491854,
      "learning_rate": 1e-05,
      "loss": 13.9092,
      "step": 480
    },
    {
      "epoch": 4.645374020823998e-07,
      "grad_norm": 310931.8797164421,
      "learning_rate": 1e-05,
      "loss": 13.7544,
      "step": 512
    },
    {
      "epoch": 4.935709897125498e-07,
      "grad_norm": 196538.94950365435,
      "learning_rate": 1e-05,
      "loss": 13.4575,
      "step": 544
    },
    {
      "epoch": 5.226045773426998e-07,
      "grad_norm": 157683.16577238042,
      "learning_rate": 1e-05,
      "loss": 12.9373,
      "step": 576
    },
    {
      "epoch": 5.516381649728497e-07,
      "grad_norm": 308634.2816473893,
      "learning_rate": 1e-05,
      "loss": 13.0173,
      "step": 608
    },
    {
      "epoch": 5.806717526029997e-07,
      "grad_norm": 258984.11154354623,
      "learning_rate": 1e-05,
      "loss": 12.6973,
      "step": 640
    },
    {
      "epoch": 6.097053402331497e-07,
      "grad_norm": 122156.98639046398,
      "learning_rate": 1e-05,
      "loss": 12.2688,
      "step": 672
    },
    {
      "epoch": 6.387389278632997e-07,
      "grad_norm": 216731.37079804574,
      "learning_rate": 1e-05,
      "loss": 11.9121,
      "step": 704
    },
    {
      "epoch": 6.677725154934497e-07,
      "grad_norm": 190579.6832823478,
      "learning_rate": 1e-05,
      "loss": 11.5139,
      "step": 736
    },
    {
      "epoch": 6.968061031235996e-07,
      "grad_norm": 157205.74737585135,
      "learning_rate": 1e-05,
      "loss": 11.3853,
      "step": 768
    },
    {
      "epoch": 7.258396907537496e-07,
      "grad_norm": 239204.62677799523,
      "learning_rate": 1e-05,
      "loss": 11.3923,
      "step": 800
    },
    {
      "epoch": 7.548732783838996e-07,
      "grad_norm": 88806.07524263191,
      "learning_rate": 1e-05,
      "loss": 11.1506,
      "step": 832
    },
    {
      "epoch": 7.839068660140496e-07,
      "grad_norm": 186804.14654926694,
      "learning_rate": 1e-05,
      "loss": 11.0132,
      "step": 864
    },
    {
      "epoch": 8.129404536441997e-07,
      "grad_norm": 136771.89907287245,
      "learning_rate": 1e-05,
      "loss": 11.0569,
      "step": 896
    },
    {
      "epoch": 8.419740412743496e-07,
      "grad_norm": 136870.51704439492,
      "learning_rate": 1e-05,
      "loss": 10.9331,
      "step": 928
    },
    {
      "epoch": 8.710076289044996e-07,
      "grad_norm": 141455.57934560234,
      "learning_rate": 1e-05,
      "loss": 10.6448,
      "step": 960
    },
    {
      "epoch": 9.000412165346496e-07,
      "grad_norm": 213476.6913365485,
      "learning_rate": 1e-05,
      "loss": 10.6196,
      "step": 992
    },
    {
      "epoch": 9.290748041647996e-07,
      "grad_norm": 373464.3630441866,
      "learning_rate": 1e-05,
      "loss": 10.615,
      "step": 1024
    },
    {
      "epoch": 9.290748041647996e-07,
      "eval_loss": 6.28125,
      "eval_runtime": 100317.4671,
      "eval_samples_per_second": 212.494,
      "eval_steps_per_second": 1.66,
      "step": 1024
    },
    {
      "epoch": 9.581083917949496e-07,
      "grad_norm": 206872.49112436385,
      "learning_rate": 1e-05,
      "loss": 10.3628,
      "step": 1056
    },
    {
      "epoch": 9.871419794250995e-07,
      "grad_norm": 315348.34225028043,
      "learning_rate": 1e-05,
      "loss": 10.4783,
      "step": 1088
    },
    {
      "epoch": 1.0161755670552495e-06,
      "grad_norm": 323318.71893844934,
      "learning_rate": 1e-05,
      "loss": 10.4634,
      "step": 1120
    },
    {
      "epoch": 1.0452091546853995e-06,
      "grad_norm": 206598.7745946234,
      "learning_rate": 1e-05,
      "loss": 10.2288,
      "step": 1152
    },
    {
      "epoch": 1.0742427423155495e-06,
      "grad_norm": 446974.18269962753,
      "learning_rate": 1e-05,
      "loss": 10.1672,
      "step": 1184
    },
    {
      "epoch": 1.1032763299456995e-06,
      "grad_norm": 186497.22121254247,
      "learning_rate": 1e-05,
      "loss": 9.9802,
      "step": 1216
    },
    {
      "epoch": 1.1323099175758494e-06,
      "grad_norm": 196493.35296645534,
      "learning_rate": 1e-05,
      "loss": 9.8591,
      "step": 1248
    },
    {
      "epoch": 1.1613435052059994e-06,
      "grad_norm": 326164.22003647184,
      "learning_rate": 1e-05,
      "loss": 9.7666,
      "step": 1280
    },
    {
      "epoch": 1.1903770928361494e-06,
      "grad_norm": 339703.6029482172,
      "learning_rate": 1e-05,
      "loss": 9.7749,
      "step": 1312
    },
    {
      "epoch": 1.2194106804662994e-06,
      "grad_norm": 380927.31384346803,
      "learning_rate": 1e-05,
      "loss": 9.6511,
      "step": 1344
    },
    {
      "epoch": 1.2484442680964494e-06,
      "grad_norm": 259725.7164317773,
      "learning_rate": 1e-05,
      "loss": 9.5784,
      "step": 1376
    },
    {
      "epoch": 1.2774778557265993e-06,
      "grad_norm": 369254.015030304,
      "learning_rate": 1e-05,
      "loss": 9.6531,
      "step": 1408
    },
    {
      "epoch": 1.3065114433567493e-06,
      "grad_norm": 276529.4281048583,
      "learning_rate": 1e-05,
      "loss": 9.3508,
      "step": 1440
    },
    {
      "epoch": 1.3355450309868993e-06,
      "grad_norm": 308209.6996267314,
      "learning_rate": 1e-05,
      "loss": 9.375,
      "step": 1472
    },
    {
      "epoch": 1.3645786186170493e-06,
      "grad_norm": 505897.9813045314,
      "learning_rate": 1e-05,
      "loss": 9.2542,
      "step": 1504
    },
    {
      "epoch": 1.3936122062471993e-06,
      "grad_norm": 298813.18707178906,
      "learning_rate": 1e-05,
      "loss": 9.2268,
      "step": 1536
    },
    {
      "epoch": 1.4226457938773492e-06,
      "grad_norm": 349584.6576038485,
      "learning_rate": 1e-05,
      "loss": 9.2224,
      "step": 1568
    },
    {
      "epoch": 1.4516793815074992e-06,
      "grad_norm": 205580.27827590855,
      "learning_rate": 1e-05,
      "loss": 9.1831,
      "step": 1600
    },
    {
      "epoch": 1.4807129691376492e-06,
      "grad_norm": 259166.20903196465,
      "learning_rate": 1e-05,
      "loss": 9.0945,
      "step": 1632
    },
    {
      "epoch": 1.5097465567677992e-06,
      "grad_norm": 276243.218284178,
      "learning_rate": 1e-05,
      "loss": 8.9712,
      "step": 1664
    },
    {
      "epoch": 1.5387801443979492e-06,
      "grad_norm": 237678.92258254622,
      "learning_rate": 1e-05,
      "loss": 8.8113,
      "step": 1696
    },
    {
      "epoch": 1.5678137320280991e-06,
      "grad_norm": 271135.11152928905,
      "learning_rate": 1e-05,
      "loss": 9.072,
      "step": 1728
    },
    {
      "epoch": 1.5968473196582493e-06,
      "grad_norm": 264462.1007252268,
      "learning_rate": 1e-05,
      "loss": 9.0815,
      "step": 1760
    },
    {
      "epoch": 1.6258809072883993e-06,
      "grad_norm": 376781.2649270131,
      "learning_rate": 1e-05,
      "loss": 9.033,
      "step": 1792
    },
    {
      "epoch": 1.6549144949185493e-06,
      "grad_norm": 247748.42623919935,
      "learning_rate": 1e-05,
      "loss": 8.8414,
      "step": 1824
    },
    {
      "epoch": 1.6839480825486993e-06,
      "grad_norm": 285250.1408939179,
      "learning_rate": 1e-05,
      "loss": 8.7708,
      "step": 1856
    },
    {
      "epoch": 1.7129816701788493e-06,
      "grad_norm": 242599.11729435457,
      "learning_rate": 1e-05,
      "loss": 8.6187,
      "step": 1888
    },
    {
      "epoch": 1.7420152578089992e-06,
      "grad_norm": 167662.3496435619,
      "learning_rate": 1e-05,
      "loss": 8.6787,
      "step": 1920
    },
    {
      "epoch": 1.7710488454391492e-06,
      "grad_norm": 314246.6006944228,
      "learning_rate": 1e-05,
      "loss": 8.4882,
      "step": 1952
    },
    {
      "epoch": 1.8000824330692992e-06,
      "grad_norm": 190087.05222607875,
      "learning_rate": 1e-05,
      "loss": 8.4097,
      "step": 1984
    },
    {
      "epoch": 1.8291160206994492e-06,
      "grad_norm": 317932.0702288462,
      "learning_rate": 1e-05,
      "loss": 8.3647,
      "step": 2016
    },
    {
      "epoch": 1.8581496083295992e-06,
      "grad_norm": 291941.885943076,
      "learning_rate": 1e-05,
      "loss": 8.1943,
      "step": 2048
    },
    {
      "epoch": 1.8581496083295992e-06,
      "eval_loss": 5.5390625,
      "eval_runtime": 100620.1826,
      "eval_samples_per_second": 211.855,
      "eval_steps_per_second": 1.655,
      "step": 2048
    },
    {
      "epoch": 1.8871831959597491e-06,
      "grad_norm": 166048.58334836827,
      "learning_rate": 1e-05,
      "loss": 8.4299,
      "step": 2080
    },
    {
      "epoch": 1.916216783589899e-06,
      "grad_norm": 301387.67157267727,
      "learning_rate": 1e-05,
      "loss": 8.245,
      "step": 2112
    },
    {
      "epoch": 1.945250371220049e-06,
      "grad_norm": 207298.8073289376,
      "learning_rate": 1e-05,
      "loss": 7.9896,
      "step": 2144
    },
    {
      "epoch": 1.974283958850199e-06,
      "grad_norm": 183676.51015848486,
      "learning_rate": 1e-05,
      "loss": 7.9275,
      "step": 2176
    },
    {
      "epoch": 2.003317546480349e-06,
      "grad_norm": 277421.8695344691,
      "learning_rate": 1e-05,
      "loss": 8.0903,
      "step": 2208
    },
    {
      "epoch": 2.032351134110499e-06,
      "grad_norm": 299579.2031233143,
      "learning_rate": 1e-05,
      "loss": 8.2445,
      "step": 2240
    },
    {
      "epoch": 2.061384721740649e-06,
      "grad_norm": 263142.15048144606,
      "learning_rate": 1e-05,
      "loss": 8.1388,
      "step": 2272
    },
    {
      "epoch": 2.090418309370799e-06,
      "grad_norm": 304351.2669531704,
      "learning_rate": 1e-05,
      "loss": 8.1173,
      "step": 2304
    },
    {
      "epoch": 2.1194518970009488e-06,
      "grad_norm": 246059.89495242818,
      "learning_rate": 1e-05,
      "loss": 8.0836,
      "step": 2336
    },
    {
      "epoch": 2.148485484631099e-06,
      "grad_norm": 282155.2178783869,
      "learning_rate": 1e-05,
      "loss": 7.7767,
      "step": 2368
    },
    {
      "epoch": 2.177519072261249e-06,
      "grad_norm": 247616.36546884375,
      "learning_rate": 1e-05,
      "loss": 7.6296,
      "step": 2400
    },
    {
      "epoch": 2.206552659891399e-06,
      "grad_norm": 395345.2715993958,
      "learning_rate": 1e-05,
      "loss": 7.5415,
      "step": 2432
    },
    {
      "epoch": 2.235586247521549e-06,
      "grad_norm": 322448.0563191535,
      "learning_rate": 1e-05,
      "loss": 7.5293,
      "step": 2464
    },
    {
      "epoch": 2.264619835151699e-06,
      "grad_norm": 283136.40099429106,
      "learning_rate": 1e-05,
      "loss": 7.4209,
      "step": 2496
    },
    {
      "epoch": 2.293653422781849e-06,
      "grad_norm": 192806.8921589682,
      "learning_rate": 1e-05,
      "loss": 7.3522,
      "step": 2528
    },
    {
      "epoch": 2.322687010411999e-06,
      "grad_norm": 230937.11753635446,
      "learning_rate": 1e-05,
      "loss": 7.2562,
      "step": 2560
    },
    {
      "epoch": 2.351720598042149e-06,
      "grad_norm": 229731.29569999818,
      "learning_rate": 1e-05,
      "loss": 7.2296,
      "step": 2592
    },
    {
      "epoch": 2.380754185672299e-06,
      "grad_norm": 156423.93249116326,
      "learning_rate": 1e-05,
      "loss": 7.3562,
      "step": 2624
    },
    {
      "epoch": 2.409787773302449e-06,
      "grad_norm": 170336.51549799883,
      "learning_rate": 1e-05,
      "loss": 7.3322,
      "step": 2656
    },
    {
      "epoch": 2.4388213609325988e-06,
      "grad_norm": 242913.48985184004,
      "learning_rate": 1e-05,
      "loss": 7.1117,
      "step": 2688
    },
    {
      "epoch": 2.467854948562749e-06,
      "grad_norm": 250595.74585375548,
      "learning_rate": 1e-05,
      "loss": 7.0967,
      "step": 2720
    },
    {
      "epoch": 2.4968885361928987e-06,
      "grad_norm": 236676.2116985989,
      "learning_rate": 1e-05,
      "loss": 7.1918,
      "step": 2752
    },
    {
      "epoch": 2.525922123823049e-06,
      "grad_norm": 177048.94654303935,
      "learning_rate": 1e-05,
      "loss": 7.1305,
      "step": 2784
    },
    {
      "epoch": 2.5549557114531987e-06,
      "grad_norm": 258123.02208055754,
      "learning_rate": 1e-05,
      "loss": 7.1006,
      "step": 2816
    },
    {
      "epoch": 2.583989299083349e-06,
      "grad_norm": 322345.1685383232,
      "learning_rate": 1e-05,
      "loss": 7.0571,
      "step": 2848
    },
    {
      "epoch": 2.6130228867134986e-06,
      "grad_norm": 198691.23753200594,
      "learning_rate": 1e-05,
      "loss": 6.9454,
      "step": 2880
    },
    {
      "epoch": 2.642056474343649e-06,
      "grad_norm": 299003.39208778215,
      "learning_rate": 1e-05,
      "loss": 6.9677,
      "step": 2912
    },
    {
      "epoch": 2.6710900619737986e-06,
      "grad_norm": 190490.4420489385,
      "learning_rate": 1e-05,
      "loss": 6.8182,
      "step": 2944
    },
    {
      "epoch": 2.700123649603949e-06,
      "grad_norm": 132750.01709981056,
      "learning_rate": 1e-05,
      "loss": 6.8403,
      "step": 2976
    },
    {
      "epoch": 2.7291572372340986e-06,
      "grad_norm": 284936.835442524,
      "learning_rate": 1e-05,
      "loss": 6.8381,
      "step": 3008
    },
    {
      "epoch": 2.7581908248642488e-06,
      "grad_norm": 302938.32211854606,
      "learning_rate": 1e-05,
      "loss": 6.7524,
      "step": 3040
    },
    {
      "epoch": 2.7872244124943985e-06,
      "grad_norm": 171489.495794932,
      "learning_rate": 1e-05,
      "loss": 6.6564,
      "step": 3072
    },
    {
      "epoch": 2.7872244124943985e-06,
      "eval_loss": 3.9375,
      "eval_runtime": 100424.7466,
      "eval_samples_per_second": 212.267,
      "eval_steps_per_second": 1.658,
      "step": 3072
    }
  ],
  "logging_steps": 32,
  "max_steps": 1102171747,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 1024,
  "total_flos": 5.995066534605619e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
