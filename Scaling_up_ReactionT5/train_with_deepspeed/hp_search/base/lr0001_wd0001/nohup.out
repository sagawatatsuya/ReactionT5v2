[2024-04-22 11:59:50,925] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-22 11:59:51,594] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-22 11:59:51,594] [INFO] [runner.py:568:main] cmd = /home/sagawa/miniconda3/envs/deepspeed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=12346 --enable_each_rank_log=None /data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py --do_train --do_eval --num_train_epochs=10 --output_dir=./output --overwrite_output_dir --save_total_limit=2 --deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json --per_device_train_batch_size=32 --per_device_eval_batch_size=128 --learning_rate=0.001 --weight_decay=0.001 --warmup_steps=10000 --logging_steps=32 --save_steps=512 --eval_steps=512 --config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/ --max_seq_length=512 --num_workers=2 --local_rank=1
[2024-04-22 11:59:52,995] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-22 11:59:53,556] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2024-04-22 11:59:53,556] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-22 11:59:53,556] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-22 11:59:53,556] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-22 11:59:53,556] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2024-04-22 11:59:53,557] [INFO] [launch.py:253:main] process 305888 spawned with command: ['/home/sagawa/miniconda3/envs/deepspeed/bin/python', '-u', '/data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py', '--local_rank=0', '--do_train', '--do_eval', '--num_train_epochs=10', '--output_dir=./output', '--overwrite_output_dir', '--save_total_limit=2', '--deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json', '--per_device_train_batch_size=32', '--per_device_eval_batch_size=128', '--learning_rate=0.001', '--weight_decay=0.001', '--warmup_steps=10000', '--logging_steps=32', '--save_steps=512', '--eval_steps=512', '--config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/', '--max_seq_length=512', '--num_workers=2', '--local_rank=1']
[2024-04-22 11:59:57,823] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-22 11:59:58,214] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-22 11:59:58,214] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
max_steps is given, it will override any value given in num_train_epochs
training started
{'loss': 95.0264, 'grad_norm': 253703.74054790757, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 40.1245, 'grad_norm': 584558.6684533897, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 27.8765, 'grad_norm': 149548.96156108874, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 22.0229, 'grad_norm': 248226.61543033615, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 19.499, 'grad_norm': 145739.86117737315, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 19.4678, 'grad_norm': 144363.53711377401, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 17.6426, 'grad_norm': 192955.12033631033, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 16.6389, 'grad_norm': 171987.55006104364, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 15.4644, 'grad_norm': 127597.65715717511, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.7295, 'grad_norm': 58581.84018960142, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.2002, 'grad_norm': 111658.47586278437, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.0547, 'grad_norm': 80409.3435366811, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.05, 'grad_norm': 96166.20545701073, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 12.2749, 'grad_norm': 116390.32949519475, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 11.5759, 'grad_norm': 153224.29008482955, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 11.9983, 'grad_norm': 117152.34717238917, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
[2024-04-23 11:59:48,994] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 305888
[2024-04-23 11:59:49,037] [INFO] [launch.py:325:sigkill_handler] Main process received SIGTERM, exiting
[2024-04-23 15:02:05,653] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-23 15:02:06,324] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-23 15:02:06,325] [INFO] [runner.py:568:main] cmd = /home/sagawa/miniconda3/envs/deepspeed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=12346 --enable_each_rank_log=None /data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py --do_train --do_eval --num_train_epochs=10 --output_dir=./output --overwrite_output_dir --save_total_limit=2 --deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json --per_device_train_batch_size=32 --per_device_eval_batch_size=128 --learning_rate=0.001 --weight_decay=0.001 --warmup_steps=10000 --logging_steps=32 --save_steps=1024 --eval_steps=1024 --config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/ --max_seq_length=512 --num_workers=2 --local_rank=1
[2024-04-23 15:02:07,701] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-23 15:02:08,451] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2024-04-23 15:02:08,451] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-23 15:02:08,451] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-23 15:02:08,451] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-23 15:02:08,451] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2024-04-23 15:02:08,452] [INFO] [launch.py:253:main] process 376965 spawned with command: ['/home/sagawa/miniconda3/envs/deepspeed/bin/python', '-u', '/data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py', '--local_rank=0', '--do_train', '--do_eval', '--num_train_epochs=10', '--output_dir=./output', '--overwrite_output_dir', '--save_total_limit=2', '--deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json', '--per_device_train_batch_size=32', '--per_device_eval_batch_size=128', '--learning_rate=0.001', '--weight_decay=0.001', '--warmup_steps=10000', '--logging_steps=32', '--save_steps=1024', '--eval_steps=1024', '--config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/', '--max_seq_length=512', '--num_workers=2', '--local_rank=1']
output_dir already exists
[2024-04-23 15:02:12,929] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-23 15:02:13,384] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-23 15:02:13,384] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
max_steps is given, it will override any value given in num_train_epochs
training started
{'loss': 95.0264, 'grad_norm': 253703.74054790757, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 40.1245, 'grad_norm': 584558.6684533897, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 27.8765, 'grad_norm': 149548.96156108874, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 22.0229, 'grad_norm': 248226.61543033615, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 19.499, 'grad_norm': 145739.86117737315, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 19.4678, 'grad_norm': 144363.53711377401, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 17.6426, 'grad_norm': 192955.12033631033, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 16.6389, 'grad_norm': 171987.55006104364, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 15.4644, 'grad_norm': 127597.65715717511, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.7295, 'grad_norm': 58581.84018960142, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.2002, 'grad_norm': 111658.47586278437, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.0547, 'grad_norm': 80409.3435366811, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.05, 'grad_norm': 96166.20545701073, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 12.2749, 'grad_norm': 116390.32949519475, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 11.5759, 'grad_norm': 153224.29008482955, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 11.9983, 'grad_norm': 117152.34717238917, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 11.2136, 'grad_norm': 82600.82033490951, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 10.4482, 'grad_norm': 125787.05357865729, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 10.1584, 'grad_norm': 148390.26728192114, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 10.0332, 'grad_norm': 86839.93440808209, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 9.1588, 'grad_norm': 60278.57091869381, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 8.5682, 'grad_norm': 135047.05479202425, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 8.2205, 'grad_norm': 60991.97481636416, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 8.0651, 'grad_norm': 30705.529925405943, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 7.7932, 'grad_norm': 62787.861247218796, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 7.1312, 'grad_norm': 123315.34600365034, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.6401, 'grad_norm': 127801.55517050643, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.4066, 'grad_norm': 63497.4378065761, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.2635, 'grad_norm': 64687.31733500779, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.8324, 'grad_norm': 95367.0299422185, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.4875, 'grad_norm': 94430.30903264058, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.2036, 'grad_norm': 124141.9466256269, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 3.625, 'eval_runtime': 92783.4509, 'eval_samples_per_second': 229.749, 'eval_steps_per_second': 1.795, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 5.1418, 'grad_norm': 50401.06982991532, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.7784, 'grad_norm': 38777.96838412244, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.8757, 'grad_norm': 28539.984022420198, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.5171, 'grad_norm': 80362.51544096912, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.5229, 'grad_norm': 109123.41783503667, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.0877, 'grad_norm': 48907.651098780036, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.3359, 'grad_norm': 70210.48856118294, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.8156, 'grad_norm': 43216.932052148266, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.6435, 'grad_norm': 31069.164391724473, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.4596, 'grad_norm': 25117.966239327576, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.5166, 'grad_norm': 48268.129112282775, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.4086, 'grad_norm': 86942.33932900587, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.3594, 'grad_norm': 95693.16330856661, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.3232, 'grad_norm': 47406.87815074939, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.3099, 'grad_norm': 80633.01239566832, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.2234, 'grad_norm': 22508.089923403095, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0538, 'grad_norm': 32765.241094794343, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8675, 'grad_norm': 25648.08764800994, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.001, 'grad_norm': 41751.12855959705, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.9169, 'grad_norm': 29191.379549449182, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8617, 'grad_norm': 27186.210622298946, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.9307, 'grad_norm': 24061.080939974414, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0551, 'grad_norm': 76678.00978116215, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8782, 'grad_norm': 35622.27123584346, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0095, 'grad_norm': 25074.28483526499, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.6744, 'grad_norm': 21152.28063353926, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.6815, 'grad_norm': 28115.112235237477, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.6674, 'grad_norm': 31629.63521762463, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.674, 'grad_norm': 23811.418101406725, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5157, 'grad_norm': 38704.06118225838, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5375, 'grad_norm': 25606.601648793618, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4356, 'grad_norm': 53643.73737912004, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 2.47265625, 'eval_runtime': 92932.4821, 'eval_samples_per_second': 229.38, 'eval_steps_per_second': 1.792, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 2.5934, 'grad_norm': 41772.9966365833, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5623, 'grad_norm': 35460.681550133806, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4492, 'grad_norm': 37483.92594166198, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3986, 'grad_norm': 45697.5566961736, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4777, 'grad_norm': 46780.88840541616, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.472, 'grad_norm': 52926.4939326232, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4858, 'grad_norm': 40191.46257602477, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.485, 'grad_norm': 36635.79277155061, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5221, 'grad_norm': 40542.391838666845, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4591, 'grad_norm': 46710.46615053205, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3707, 'grad_norm': 22489.36513110141, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4091, 'grad_norm': 60844.34593288024, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3877, 'grad_norm': 39678.60884658131, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3932, 'grad_norm': 51287.955701119536, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4108, 'grad_norm': 34747.940831076594, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3603, 'grad_norm': 27932.318772346847, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3014, 'grad_norm': 33755.64189879967, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3889, 'grad_norm': 43259.58927220646, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4138, 'grad_norm': 50528.47814846595, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2994, 'grad_norm': 32044.345772694436, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.252, 'grad_norm': 32477.73064732202, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2592, 'grad_norm': 30715.506442186495, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.261, 'grad_norm': 22305.92531145032, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2673, 'grad_norm': 28230.760528189814, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2713, 'grad_norm': 24297.594284208466, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.234, 'grad_norm': 48612.038344426575, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2428, 'grad_norm': 22089.724670081338, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1937, 'grad_norm': 21074.737103935604, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1976, 'grad_norm': 16566.83264839722, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1917, 'grad_norm': 17727.27435337988, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1833, 'grad_norm': 13148.66974260134, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1502, 'grad_norm': 215101.19367404727, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 2.16796875, 'eval_runtime': 92531.1174, 'eval_samples_per_second': 230.375, 'eval_steps_per_second': 1.8, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 2.152, 'grad_norm': 14819.40403660012, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1421, 'grad_norm': 15918.969564642053, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1591, 'grad_norm': 15185.307372588808, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1939, 'grad_norm': 12753.373671307525, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1468, 'grad_norm': 7229.374869240079, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1026, 'grad_norm': 9646.333202310607, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0821, 'grad_norm': 6204.359132900029, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.081, 'grad_norm': 12592.818270744638, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0892, 'grad_norm': 4963.7976389051155, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1133, 'grad_norm': 14222.453234234943, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1078, 'grad_norm': 33580.49922201872, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1685, 'grad_norm': 9116.999067675722, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1657, 'grad_norm': 31720.745041691564, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1603, 'grad_norm': 4726.406774707399, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0776, 'grad_norm': 4857.471744642474, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1286, 'grad_norm': 8896.652769440874, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.08, 'grad_norm': 2126.035946015495, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1127, 'grad_norm': 3676.7029217765203, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0675, 'grad_norm': 2793.78654338516, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0668, 'grad_norm': 6387.30831414924, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1016, 'grad_norm': 8490.431437801028, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.071, 'grad_norm': 6394.908247973539, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1102, 'grad_norm': 3161.2056244414093, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0588, 'grad_norm': 3897.8029067668367, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0747, 'grad_norm': 3570.714931914896, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0616, 'grad_norm': 3742.0930139428656, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0651, 'grad_norm': 4058.1442187285556, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0227, 'grad_norm': 2227.312407925749, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0166, 'grad_norm': 4255.105286593976, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0898, 'grad_norm': 4243.204213798812, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0785, 'grad_norm': 35179.319777391946, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0912, 'grad_norm': 8067.1271218445545, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
[2024-04-27 15:02:03,693] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 376965
[2024-04-27 15:02:04,495] [INFO] [launch.py:325:sigkill_handler] Main process received SIGTERM, exiting
