{
  "best_metric": 2.37890625,
  "best_model_checkpoint": "./output/checkpoint-2048",
  "epoch": 1.8581496083295992e-06,
  "eval_steps": 1024,
  "global_step": 2048,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.9033587630149987e-08,
      "grad_norm": 276429.8667510441,
      "learning_rate": 0.001,
      "loss": 94.2227,
      "step": 32
    },
    {
      "epoch": 5.8067175260299974e-08,
      "grad_norm": 184762.5756044768,
      "learning_rate": 0.001,
      "loss": 41.5059,
      "step": 64
    },
    {
      "epoch": 8.710076289044995e-08,
      "grad_norm": 218296.25257433992,
      "learning_rate": 0.001,
      "loss": 28.1021,
      "step": 96
    },
    {
      "epoch": 1.1613435052059995e-07,
      "grad_norm": 180495.34544691173,
      "learning_rate": 0.001,
      "loss": 21.8184,
      "step": 128
    },
    {
      "epoch": 1.4516793815074993e-07,
      "grad_norm": 145624.36103894157,
      "learning_rate": 0.001,
      "loss": 19.8958,
      "step": 160
    },
    {
      "epoch": 1.742015257808999e-07,
      "grad_norm": 273959.895692782,
      "learning_rate": 0.001,
      "loss": 18.4817,
      "step": 192
    },
    {
      "epoch": 2.0323511341104991e-07,
      "grad_norm": 128326.90156003924,
      "learning_rate": 0.001,
      "loss": 17.8398,
      "step": 224
    },
    {
      "epoch": 2.322687010411999e-07,
      "grad_norm": 127245.92780910515,
      "learning_rate": 0.001,
      "loss": 17.0203,
      "step": 256
    },
    {
      "epoch": 2.613022886713499e-07,
      "grad_norm": 130738.64983240419,
      "learning_rate": 0.001,
      "loss": 15.5479,
      "step": 288
    },
    {
      "epoch": 2.9033587630149986e-07,
      "grad_norm": 81342.55466851284,
      "learning_rate": 0.001,
      "loss": 14.9814,
      "step": 320
    },
    {
      "epoch": 3.1936946393164984e-07,
      "grad_norm": 217226.93961845525,
      "learning_rate": 0.001,
      "loss": 14.5786,
      "step": 352
    },
    {
      "epoch": 3.484030515617998e-07,
      "grad_norm": 72921.97561777932,
      "learning_rate": 0.001,
      "loss": 14.0774,
      "step": 384
    },
    {
      "epoch": 3.774366391919498e-07,
      "grad_norm": 123260.17465507665,
      "learning_rate": 0.001,
      "loss": 13.7695,
      "step": 416
    },
    {
      "epoch": 4.0647022682209983e-07,
      "grad_norm": 71937.7453080092,
      "learning_rate": 0.001,
      "loss": 12.8018,
      "step": 448
    },
    {
      "epoch": 4.355038144522498e-07,
      "grad_norm": 108041.0724863466,
      "learning_rate": 0.001,
      "loss": 11.9287,
      "step": 480
    },
    {
      "epoch": 4.645374020823998e-07,
      "grad_norm": 74652.16449641631,
      "learning_rate": 0.001,
      "loss": 12.0608,
      "step": 512
    },
    {
      "epoch": 4.935709897125498e-07,
      "grad_norm": 114834.78943247121,
      "learning_rate": 0.001,
      "loss": 11.5957,
      "step": 544
    },
    {
      "epoch": 5.226045773426998e-07,
      "grad_norm": 73832.21584105409,
      "learning_rate": 0.001,
      "loss": 10.6877,
      "step": 576
    },
    {
      "epoch": 5.516381649728497e-07,
      "grad_norm": 43469.87996302727,
      "learning_rate": 0.001,
      "loss": 10.2344,
      "step": 608
    },
    {
      "epoch": 5.806717526029997e-07,
      "grad_norm": 141947.81057839532,
      "learning_rate": 0.001,
      "loss": 9.9121,
      "step": 640
    },
    {
      "epoch": 6.097053402331497e-07,
      "grad_norm": 107774.22445093261,
      "learning_rate": 0.001,
      "loss": 9.6652,
      "step": 672
    },
    {
      "epoch": 6.387389278632997e-07,
      "grad_norm": 75702.2102715634,
      "learning_rate": 0.001,
      "loss": 8.6216,
      "step": 704
    },
    {
      "epoch": 6.677725154934497e-07,
      "grad_norm": 54632.33328350529,
      "learning_rate": 0.001,
      "loss": 8.0563,
      "step": 736
    },
    {
      "epoch": 6.968061031235996e-07,
      "grad_norm": 51417.84686273823,
      "learning_rate": 0.001,
      "loss": 7.8513,
      "step": 768
    },
    {
      "epoch": 7.258396907537496e-07,
      "grad_norm": 91371.29089599205,
      "learning_rate": 0.001,
      "loss": 7.7797,
      "step": 800
    },
    {
      "epoch": 7.548732783838996e-07,
      "grad_norm": 54478.41583599876,
      "learning_rate": 0.001,
      "loss": 7.3337,
      "step": 832
    },
    {
      "epoch": 7.839068660140496e-07,
      "grad_norm": 63618.49089690827,
      "learning_rate": 0.001,
      "loss": 6.705,
      "step": 864
    },
    {
      "epoch": 8.129404536441997e-07,
      "grad_norm": 44163.079965056786,
      "learning_rate": 0.001,
      "loss": 6.4646,
      "step": 896
    },
    {
      "epoch": 8.419740412743496e-07,
      "grad_norm": 63421.064261016625,
      "learning_rate": 0.001,
      "loss": 6.5341,
      "step": 928
    },
    {
      "epoch": 8.710076289044996e-07,
      "grad_norm": 130500.08723368733,
      "learning_rate": 0.001,
      "loss": 5.8187,
      "step": 960
    },
    {
      "epoch": 9.000412165346496e-07,
      "grad_norm": 51820.705668680355,
      "learning_rate": 0.001,
      "loss": 5.3379,
      "step": 992
    },
    {
      "epoch": 9.290748041647996e-07,
      "grad_norm": 247814.82635225845,
      "learning_rate": 0.001,
      "loss": 5.1445,
      "step": 1024
    },
    {
      "epoch": 9.290748041647996e-07,
      "eval_loss": 5.72265625,
      "eval_runtime": 100029.6129,
      "eval_samples_per_second": 213.106,
      "eval_steps_per_second": 1.665,
      "step": 1024
    },
    {
      "epoch": 9.581083917949496e-07,
      "grad_norm": 75893.83205504912,
      "learning_rate": 0.001,
      "loss": 5.4302,
      "step": 1056
    },
    {
      "epoch": 9.871419794250995e-07,
      "grad_norm": 59447.99421342995,
      "learning_rate": 0.001,
      "loss": 4.7625,
      "step": 1088
    },
    {
      "epoch": 1.0161755670552495e-06,
      "grad_norm": 63233.542997368095,
      "learning_rate": 0.001,
      "loss": 4.7711,
      "step": 1120
    },
    {
      "epoch": 1.0452091546853995e-06,
      "grad_norm": 94123.07925264664,
      "learning_rate": 0.001,
      "loss": 4.567,
      "step": 1152
    },
    {
      "epoch": 1.0742427423155495e-06,
      "grad_norm": 77332.09341534729,
      "learning_rate": 0.001,
      "loss": 4.4905,
      "step": 1184
    },
    {
      "epoch": 1.1032763299456995e-06,
      "grad_norm": 51069.20167772353,
      "learning_rate": 0.001,
      "loss": 4.3145,
      "step": 1216
    },
    {
      "epoch": 1.1323099175758494e-06,
      "grad_norm": 49213.818222121314,
      "learning_rate": 0.001,
      "loss": 4.1549,
      "step": 1248
    },
    {
      "epoch": 1.1613435052059994e-06,
      "grad_norm": 63384.632837936355,
      "learning_rate": 0.001,
      "loss": 3.8228,
      "step": 1280
    },
    {
      "epoch": 1.1903770928361494e-06,
      "grad_norm": 53401.96071306746,
      "learning_rate": 0.001,
      "loss": 3.5809,
      "step": 1312
    },
    {
      "epoch": 1.2194106804662994e-06,
      "grad_norm": 49709.59504964811,
      "learning_rate": 0.001,
      "loss": 3.606,
      "step": 1344
    },
    {
      "epoch": 1.2484442680964494e-06,
      "grad_norm": 43197.71771749058,
      "learning_rate": 0.001,
      "loss": 3.3657,
      "step": 1376
    },
    {
      "epoch": 1.2774778557265993e-06,
      "grad_norm": 86758.75019846701,
      "learning_rate": 0.001,
      "loss": 3.3216,
      "step": 1408
    },
    {
      "epoch": 1.3065114433567493e-06,
      "grad_norm": 70981.3595812309,
      "learning_rate": 0.001,
      "loss": 3.3814,
      "step": 1440
    },
    {
      "epoch": 1.3355450309868993e-06,
      "grad_norm": 51492.109220734004,
      "learning_rate": 0.001,
      "loss": 3.291,
      "step": 1472
    },
    {
      "epoch": 1.3645786186170493e-06,
      "grad_norm": 27990.055376865548,
      "learning_rate": 0.001,
      "loss": 3.2132,
      "step": 1504
    },
    {
      "epoch": 1.3936122062471993e-06,
      "grad_norm": 26365.4091566962,
      "learning_rate": 0.001,
      "loss": 3.0261,
      "step": 1536
    },
    {
      "epoch": 1.4226457938773492e-06,
      "grad_norm": 32134.493616673033,
      "learning_rate": 0.001,
      "loss": 3.0278,
      "step": 1568
    },
    {
      "epoch": 1.4516793815074992e-06,
      "grad_norm": 26114.069464562584,
      "learning_rate": 0.001,
      "loss": 2.9786,
      "step": 1600
    },
    {
      "epoch": 1.4807129691376492e-06,
      "grad_norm": 53485.946715001686,
      "learning_rate": 0.001,
      "loss": 2.955,
      "step": 1632
    },
    {
      "epoch": 1.5097465567677992e-06,
      "grad_norm": 31461.261131747404,
      "learning_rate": 0.001,
      "loss": 2.9849,
      "step": 1664
    },
    {
      "epoch": 1.5387801443979492e-06,
      "grad_norm": 44432.55347152581,
      "learning_rate": 0.001,
      "loss": 2.8414,
      "step": 1696
    },
    {
      "epoch": 1.5678137320280991e-06,
      "grad_norm": 25060.519707300566,
      "learning_rate": 0.001,
      "loss": 2.8226,
      "step": 1728
    },
    {
      "epoch": 1.5968473196582493e-06,
      "grad_norm": 24702.31762406111,
      "learning_rate": 0.001,
      "loss": 2.9449,
      "step": 1760
    },
    {
      "epoch": 1.6258809072883993e-06,
      "grad_norm": 34179.456988079844,
      "learning_rate": 0.001,
      "loss": 3.0681,
      "step": 1792
    },
    {
      "epoch": 1.6549144949185493e-06,
      "grad_norm": 14261.198827588092,
      "learning_rate": 0.001,
      "loss": 2.8573,
      "step": 1824
    },
    {
      "epoch": 1.6839480825486993e-06,
      "grad_norm": 17581.142624983167,
      "learning_rate": 0.001,
      "loss": 2.5629,
      "step": 1856
    },
    {
      "epoch": 1.7129816701788493e-06,
      "grad_norm": 26266.95536220367,
      "learning_rate": 0.001,
      "loss": 2.5464,
      "step": 1888
    },
    {
      "epoch": 1.7420152578089992e-06,
      "grad_norm": 19356.682567010288,
      "learning_rate": 0.001,
      "loss": 2.5692,
      "step": 1920
    },
    {
      "epoch": 1.7710488454391492e-06,
      "grad_norm": 12023.104091706102,
      "learning_rate": 0.001,
      "loss": 2.4727,
      "step": 1952
    },
    {
      "epoch": 1.8000824330692992e-06,
      "grad_norm": 43849.66243883755,
      "learning_rate": 0.001,
      "loss": 2.4838,
      "step": 1984
    },
    {
      "epoch": 1.8291160206994492e-06,
      "grad_norm": 19415.57766330943,
      "learning_rate": 0.001,
      "loss": 2.5178,
      "step": 2016
    },
    {
      "epoch": 1.8581496083295992e-06,
      "grad_norm": 43292.42926886871,
      "learning_rate": 0.001,
      "loss": 2.4461,
      "step": 2048
    },
    {
      "epoch": 1.8581496083295992e-06,
      "eval_loss": 2.37890625,
      "eval_runtime": 100236.7205,
      "eval_samples_per_second": 212.665,
      "eval_steps_per_second": 1.661,
      "step": 2048
    }
  ],
  "logging_steps": 32,
  "max_steps": 1102171747,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 1024,
  "total_flos": 3.996711023070413e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
