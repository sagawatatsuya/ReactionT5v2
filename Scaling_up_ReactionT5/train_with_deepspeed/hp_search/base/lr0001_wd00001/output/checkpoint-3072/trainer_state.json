{
  "best_metric": 2.34765625,
  "best_model_checkpoint": "./output/checkpoint-3072",
  "epoch": 2.7872244124943985e-06,
  "eval_steps": 1024,
  "global_step": 3072,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.9033587630149987e-08,
      "grad_norm": 251484.33131310585,
      "learning_rate": 0.001,
      "loss": 95.1543,
      "step": 32
    },
    {
      "epoch": 5.8067175260299974e-08,
      "grad_norm": 548985.3317384718,
      "learning_rate": 0.001,
      "loss": 39.2808,
      "step": 64
    },
    {
      "epoch": 8.710076289044995e-08,
      "grad_norm": 209480.7911384717,
      "learning_rate": 0.001,
      "loss": 27.9155,
      "step": 96
    },
    {
      "epoch": 1.1613435052059995e-07,
      "grad_norm": 159585.38479447298,
      "learning_rate": 0.001,
      "loss": 22.2439,
      "step": 128
    },
    {
      "epoch": 1.4516793815074993e-07,
      "grad_norm": 136808.87269471964,
      "learning_rate": 0.001,
      "loss": 20.4871,
      "step": 160
    },
    {
      "epoch": 1.742015257808999e-07,
      "grad_norm": 79762.91990643271,
      "learning_rate": 0.001,
      "loss": 18.3066,
      "step": 192
    },
    {
      "epoch": 2.0323511341104991e-07,
      "grad_norm": 186715.07002917572,
      "learning_rate": 0.001,
      "loss": 17.571,
      "step": 224
    },
    {
      "epoch": 2.322687010411999e-07,
      "grad_norm": 80184.66766159226,
      "learning_rate": 0.001,
      "loss": 16.916,
      "step": 256
    },
    {
      "epoch": 2.613022886713499e-07,
      "grad_norm": 118177.92687299944,
      "learning_rate": 0.001,
      "loss": 15.4651,
      "step": 288
    },
    {
      "epoch": 2.9033587630149986e-07,
      "grad_norm": 54936.850146327095,
      "learning_rate": 0.001,
      "loss": 14.793,
      "step": 320
    },
    {
      "epoch": 3.1936946393164984e-07,
      "grad_norm": 131514.3536500864,
      "learning_rate": 0.001,
      "loss": 14.3647,
      "step": 352
    },
    {
      "epoch": 3.484030515617998e-07,
      "grad_norm": 115396.36791511248,
      "learning_rate": 0.001,
      "loss": 13.812,
      "step": 384
    },
    {
      "epoch": 3.774366391919498e-07,
      "grad_norm": 102749.9028126061,
      "learning_rate": 0.001,
      "loss": 13.7073,
      "step": 416
    },
    {
      "epoch": 4.0647022682209983e-07,
      "grad_norm": 136116.10596839743,
      "learning_rate": 0.001,
      "loss": 12.9146,
      "step": 448
    },
    {
      "epoch": 4.355038144522498e-07,
      "grad_norm": 67432.04128602367,
      "learning_rate": 0.001,
      "loss": 11.8679,
      "step": 480
    },
    {
      "epoch": 4.645374020823998e-07,
      "grad_norm": 90661.16646061862,
      "learning_rate": 0.001,
      "loss": 12.2324,
      "step": 512
    },
    {
      "epoch": 4.935709897125498e-07,
      "grad_norm": 101716.44130621167,
      "learning_rate": 0.001,
      "loss": 11.302,
      "step": 544
    },
    {
      "epoch": 5.226045773426998e-07,
      "grad_norm": 84930.83002067034,
      "learning_rate": 0.001,
      "loss": 10.8062,
      "step": 576
    },
    {
      "epoch": 5.516381649728497e-07,
      "grad_norm": 97563.08810200711,
      "learning_rate": 0.001,
      "loss": 10.5029,
      "step": 608
    },
    {
      "epoch": 5.806717526029997e-07,
      "grad_norm": 76391.60665937063,
      "learning_rate": 0.001,
      "loss": 9.9272,
      "step": 640
    },
    {
      "epoch": 6.097053402331497e-07,
      "grad_norm": 96200.12740116304,
      "learning_rate": 0.001,
      "loss": 9.1934,
      "step": 672
    },
    {
      "epoch": 6.387389278632997e-07,
      "grad_norm": 74223.84975194967,
      "learning_rate": 0.001,
      "loss": 8.5337,
      "step": 704
    },
    {
      "epoch": 6.677725154934497e-07,
      "grad_norm": 23360.72421822577,
      "learning_rate": 0.001,
      "loss": 8.3817,
      "step": 736
    },
    {
      "epoch": 6.968061031235996e-07,
      "grad_norm": 55331.67237667772,
      "learning_rate": 0.001,
      "loss": 7.89,
      "step": 768
    },
    {
      "epoch": 7.258396907537496e-07,
      "grad_norm": 41202.157225077426,
      "learning_rate": 0.001,
      "loss": 7.5684,
      "step": 800
    },
    {
      "epoch": 7.548732783838996e-07,
      "grad_norm": 167532.71833286775,
      "learning_rate": 0.001,
      "loss": 6.9851,
      "step": 832
    },
    {
      "epoch": 7.839068660140496e-07,
      "grad_norm": 26946.34268319172,
      "learning_rate": 0.001,
      "loss": 6.5815,
      "step": 864
    },
    {
      "epoch": 8.129404536441997e-07,
      "grad_norm": 28738.752930494393,
      "learning_rate": 0.001,
      "loss": 6.4563,
      "step": 896
    },
    {
      "epoch": 8.419740412743496e-07,
      "grad_norm": 46455.946616122244,
      "learning_rate": 0.001,
      "loss": 6.5568,
      "step": 928
    },
    {
      "epoch": 8.710076289044996e-07,
      "grad_norm": 23772.00437489443,
      "learning_rate": 0.001,
      "loss": 5.6162,
      "step": 960
    },
    {
      "epoch": 9.000412165346496e-07,
      "grad_norm": 25954.143098935092,
      "learning_rate": 0.001,
      "loss": 5.3188,
      "step": 992
    },
    {
      "epoch": 9.290748041647996e-07,
      "grad_norm": 26474.482960012647,
      "learning_rate": 0.001,
      "loss": 5.1554,
      "step": 1024
    },
    {
      "epoch": 9.290748041647996e-07,
      "eval_loss": 3.541015625,
      "eval_runtime": 93072.9374,
      "eval_samples_per_second": 229.034,
      "eval_steps_per_second": 1.789,
      "step": 1024
    },
    {
      "epoch": 9.581083917949496e-07,
      "grad_norm": 27837.31272950031,
      "learning_rate": 0.001,
      "loss": 5.4464,
      "step": 1056
    },
    {
      "epoch": 9.871419794250995e-07,
      "grad_norm": 43054.75137542894,
      "learning_rate": 0.001,
      "loss": 4.7642,
      "step": 1088
    },
    {
      "epoch": 1.0161755670552495e-06,
      "grad_norm": 26335.43301333775,
      "learning_rate": 0.001,
      "loss": 4.6772,
      "step": 1120
    },
    {
      "epoch": 1.0452091546853995e-06,
      "grad_norm": 59064.54259536765,
      "learning_rate": 0.001,
      "loss": 4.374,
      "step": 1152
    },
    {
      "epoch": 1.0742427423155495e-06,
      "grad_norm": 34388.10131426276,
      "learning_rate": 0.001,
      "loss": 4.3741,
      "step": 1184
    },
    {
      "epoch": 1.1032763299456995e-06,
      "grad_norm": 35270.02648141903,
      "learning_rate": 0.001,
      "loss": 4.1086,
      "step": 1216
    },
    {
      "epoch": 1.1323099175758494e-06,
      "grad_norm": 29809.89822189938,
      "learning_rate": 0.001,
      "loss": 4.3196,
      "step": 1248
    },
    {
      "epoch": 1.1613435052059994e-06,
      "grad_norm": 28844.470041933517,
      "learning_rate": 0.001,
      "loss": 3.6094,
      "step": 1280
    },
    {
      "epoch": 1.1903770928361494e-06,
      "grad_norm": 22207.224410087812,
      "learning_rate": 0.001,
      "loss": 3.649,
      "step": 1312
    },
    {
      "epoch": 1.2194106804662994e-06,
      "grad_norm": 24225.197955847543,
      "learning_rate": 0.001,
      "loss": 3.5085,
      "step": 1344
    },
    {
      "epoch": 1.2484442680964494e-06,
      "grad_norm": 31180.37049170519,
      "learning_rate": 0.001,
      "loss": 3.3001,
      "step": 1376
    },
    {
      "epoch": 1.2774778557265993e-06,
      "grad_norm": 25556.00657379787,
      "learning_rate": 0.001,
      "loss": 3.3562,
      "step": 1408
    },
    {
      "epoch": 1.3065114433567493e-06,
      "grad_norm": 44102.3685531741,
      "learning_rate": 0.001,
      "loss": 3.3891,
      "step": 1440
    },
    {
      "epoch": 1.3355450309868993e-06,
      "grad_norm": 29512.53970772424,
      "learning_rate": 0.001,
      "loss": 3.287,
      "step": 1472
    },
    {
      "epoch": 1.3645786186170493e-06,
      "grad_norm": 22507.662695180057,
      "learning_rate": 0.001,
      "loss": 3.1601,
      "step": 1504
    },
    {
      "epoch": 1.3936122062471993e-06,
      "grad_norm": 11447.824072722293,
      "learning_rate": 0.001,
      "loss": 3.0491,
      "step": 1536
    },
    {
      "epoch": 1.4226457938773492e-06,
      "grad_norm": 6937.5970984195965,
      "learning_rate": 0.001,
      "loss": 3.0525,
      "step": 1568
    },
    {
      "epoch": 1.4516793815074992e-06,
      "grad_norm": 15432.079315503794,
      "learning_rate": 0.001,
      "loss": 3.0868,
      "step": 1600
    },
    {
      "epoch": 1.4807129691376492e-06,
      "grad_norm": 17700.780321782426,
      "learning_rate": 0.001,
      "loss": 2.941,
      "step": 1632
    },
    {
      "epoch": 1.5097465567677992e-06,
      "grad_norm": 14185.124320921548,
      "learning_rate": 0.001,
      "loss": 2.9619,
      "step": 1664
    },
    {
      "epoch": 1.5387801443979492e-06,
      "grad_norm": 19873.615473788355,
      "learning_rate": 0.001,
      "loss": 2.8544,
      "step": 1696
    },
    {
      "epoch": 1.5678137320280991e-06,
      "grad_norm": 20609.602616256336,
      "learning_rate": 0.001,
      "loss": 2.8411,
      "step": 1728
    },
    {
      "epoch": 1.5968473196582493e-06,
      "grad_norm": 76404.4353686355,
      "learning_rate": 0.001,
      "loss": 3.0479,
      "step": 1760
    },
    {
      "epoch": 1.6258809072883993e-06,
      "grad_norm": 32882.97918376618,
      "learning_rate": 0.001,
      "loss": 2.8907,
      "step": 1792
    },
    {
      "epoch": 1.6549144949185493e-06,
      "grad_norm": 56425.02111652241,
      "learning_rate": 0.001,
      "loss": 2.8869,
      "step": 1824
    },
    {
      "epoch": 1.6839480825486993e-06,
      "grad_norm": 25564.3160675188,
      "learning_rate": 0.001,
      "loss": 2.7009,
      "step": 1856
    },
    {
      "epoch": 1.7129816701788493e-06,
      "grad_norm": 42025.024354543806,
      "learning_rate": 0.001,
      "loss": 2.6359,
      "step": 1888
    },
    {
      "epoch": 1.7420152578089992e-06,
      "grad_norm": 38558.37237228771,
      "learning_rate": 0.001,
      "loss": 2.7049,
      "step": 1920
    },
    {
      "epoch": 1.7710488454391492e-06,
      "grad_norm": 27889.88949422353,
      "learning_rate": 0.001,
      "loss": 2.4968,
      "step": 1952
    },
    {
      "epoch": 1.8000824330692992e-06,
      "grad_norm": 19733.526902203772,
      "learning_rate": 0.001,
      "loss": 2.5519,
      "step": 1984
    },
    {
      "epoch": 1.8291160206994492e-06,
      "grad_norm": 31792.731747995484,
      "learning_rate": 0.001,
      "loss": 2.5964,
      "step": 2016
    },
    {
      "epoch": 1.8581496083295992e-06,
      "grad_norm": 36585.151632868765,
      "learning_rate": 0.001,
      "loss": 2.4842,
      "step": 2048
    },
    {
      "epoch": 1.8581496083295992e-06,
      "eval_loss": 2.681640625,
      "eval_runtime": 93374.6359,
      "eval_samples_per_second": 228.294,
      "eval_steps_per_second": 1.784,
      "step": 2048
    },
    {
      "epoch": 1.8871831959597491e-06,
      "grad_norm": 37250.84911783891,
      "learning_rate": 0.001,
      "loss": 2.5933,
      "step": 2080
    },
    {
      "epoch": 1.916216783589899e-06,
      "grad_norm": 31109.27450134445,
      "learning_rate": 0.001,
      "loss": 2.5596,
      "step": 2112
    },
    {
      "epoch": 1.945250371220049e-06,
      "grad_norm": 16259.915620937274,
      "learning_rate": 0.001,
      "loss": 2.5057,
      "step": 2144
    },
    {
      "epoch": 1.974283958850199e-06,
      "grad_norm": 34859.50579110381,
      "learning_rate": 0.001,
      "loss": 2.5275,
      "step": 2176
    },
    {
      "epoch": 2.003317546480349e-06,
      "grad_norm": 239862.88603283334,
      "learning_rate": 0.001,
      "loss": 2.5623,
      "step": 2208
    },
    {
      "epoch": 2.032351134110499e-06,
      "grad_norm": 17329.633752621547,
      "learning_rate": 0.001,
      "loss": 2.5034,
      "step": 2240
    },
    {
      "epoch": 2.061384721740649e-06,
      "grad_norm": 16871.912517554138,
      "learning_rate": 0.001,
      "loss": 2.4594,
      "step": 2272
    },
    {
      "epoch": 2.090418309370799e-06,
      "grad_norm": 35953.62586721957,
      "learning_rate": 0.001,
      "loss": 2.5385,
      "step": 2304
    },
    {
      "epoch": 2.1194518970009488e-06,
      "grad_norm": 8520.970976068396,
      "learning_rate": 0.001,
      "loss": 2.5938,
      "step": 2336
    },
    {
      "epoch": 2.148485484631099e-06,
      "grad_norm": 6514.71337627374,
      "learning_rate": 0.001,
      "loss": 2.5233,
      "step": 2368
    },
    {
      "epoch": 2.177519072261249e-06,
      "grad_norm": 3335.988421742498,
      "learning_rate": 0.001,
      "loss": 2.4058,
      "step": 2400
    },
    {
      "epoch": 2.206552659891399e-06,
      "grad_norm": 4678.78317514287,
      "learning_rate": 0.001,
      "loss": 2.3794,
      "step": 2432
    },
    {
      "epoch": 2.235586247521549e-06,
      "grad_norm": 3401.0393815714633,
      "learning_rate": 0.001,
      "loss": 2.3318,
      "step": 2464
    },
    {
      "epoch": 2.264619835151699e-06,
      "grad_norm": 3934.349676248922,
      "learning_rate": 0.001,
      "loss": 2.3227,
      "step": 2496
    },
    {
      "epoch": 2.293653422781849e-06,
      "grad_norm": 3462.456858937018,
      "learning_rate": 0.001,
      "loss": 2.2805,
      "step": 2528
    },
    {
      "epoch": 2.322687010411999e-06,
      "grad_norm": 10874.456285258588,
      "learning_rate": 0.001,
      "loss": 2.2375,
      "step": 2560
    },
    {
      "epoch": 2.351720598042149e-06,
      "grad_norm": 2778.4676757162392,
      "learning_rate": 0.001,
      "loss": 2.254,
      "step": 2592
    },
    {
      "epoch": 2.380754185672299e-06,
      "grad_norm": 11066.94542319605,
      "learning_rate": 0.001,
      "loss": 2.4211,
      "step": 2624
    },
    {
      "epoch": 2.409787773302449e-06,
      "grad_norm": 2190.196452375905,
      "learning_rate": 0.001,
      "loss": 2.3562,
      "step": 2656
    },
    {
      "epoch": 2.4388213609325988e-06,
      "grad_norm": 1558.9929241340387,
      "learning_rate": 0.001,
      "loss": 2.2817,
      "step": 2688
    },
    {
      "epoch": 2.467854948562749e-06,
      "grad_norm": 2434.417627996478,
      "learning_rate": 0.001,
      "loss": 2.1871,
      "step": 2720
    },
    {
      "epoch": 2.4968885361928987e-06,
      "grad_norm": 4927.994984017334,
      "learning_rate": 0.001,
      "loss": 2.2366,
      "step": 2752
    },
    {
      "epoch": 2.525922123823049e-06,
      "grad_norm": 9181.139716832546,
      "learning_rate": 0.001,
      "loss": 2.2931,
      "step": 2784
    },
    {
      "epoch": 2.5549557114531987e-06,
      "grad_norm": 894.0962965475251,
      "learning_rate": 0.001,
      "loss": 2.379,
      "step": 2816
    },
    {
      "epoch": 2.583989299083349e-06,
      "grad_norm": 2974.794992936488,
      "learning_rate": 0.001,
      "loss": 2.3494,
      "step": 2848
    },
    {
      "epoch": 2.6130228867134986e-06,
      "grad_norm": 1355.4730262900846,
      "learning_rate": 0.001,
      "loss": 2.2729,
      "step": 2880
    },
    {
      "epoch": 2.642056474343649e-06,
      "grad_norm": 2423.195475503369,
      "learning_rate": 0.001,
      "loss": 2.2559,
      "step": 2912
    },
    {
      "epoch": 2.6710900619737986e-06,
      "grad_norm": 1748.4509751491462,
      "learning_rate": 0.001,
      "loss": 2.2584,
      "step": 2944
    },
    {
      "epoch": 2.700123649603949e-06,
      "grad_norm": 2564.7636952553735,
      "learning_rate": 0.001,
      "loss": 2.2363,
      "step": 2976
    },
    {
      "epoch": 2.7291572372340986e-06,
      "grad_norm": 1773.6243172526702,
      "learning_rate": 0.001,
      "loss": 2.258,
      "step": 3008
    },
    {
      "epoch": 2.7581908248642488e-06,
      "grad_norm": 929.4081281923459,
      "learning_rate": 0.001,
      "loss": 2.2053,
      "step": 3040
    },
    {
      "epoch": 2.7872244124943985e-06,
      "grad_norm": 1182.431551930174,
      "learning_rate": 0.001,
      "loss": 2.1801,
      "step": 3072
    },
    {
      "epoch": 2.7872244124943985e-06,
      "eval_loss": 2.34765625,
      "eval_runtime": 93332.6191,
      "eval_samples_per_second": 228.397,
      "eval_steps_per_second": 1.784,
      "step": 3072
    }
  ],
  "logging_steps": 32,
  "max_steps": 1102171747,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 1024,
  "total_flos": 5.995066534605619e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
