[2024-04-27 17:41:52,247] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-27 17:41:52,895] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-27 17:41:52,895] [INFO] [runner.py:568:main] cmd = /home/sagawa/miniconda3/envs/deepspeed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=12348 --enable_each_rank_log=None /data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py --do_train --do_eval --num_train_epochs=10 --output_dir=./output --overwrite_output_dir --save_total_limit=2 --deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json --per_device_train_batch_size=32 --per_device_eval_batch_size=128 --learning_rate=0.001 --weight_decay=0.0001 --warmup_steps=10000 --logging_steps=32 --save_steps=1024 --eval_steps=1024 --config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/ --max_seq_length=512 --num_workers=2 --local_rank=1
[2024-04-27 17:41:54,194] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-27 17:41:54,849] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}
[2024-04-27 17:41:54,850] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-27 17:41:54,850] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-27 17:41:54,850] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-27 17:41:54,850] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2
[2024-04-27 17:41:54,851] [INFO] [launch.py:253:main] process 456750 spawned with command: ['/home/sagawa/miniconda3/envs/deepspeed/bin/python', '-u', '/data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py', '--local_rank=0', '--do_train', '--do_eval', '--num_train_epochs=10', '--output_dir=./output', '--overwrite_output_dir', '--save_total_limit=2', '--deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json', '--per_device_train_batch_size=32', '--per_device_eval_batch_size=128', '--learning_rate=0.001', '--weight_decay=0.0001', '--warmup_steps=10000', '--logging_steps=32', '--save_steps=1024', '--eval_steps=1024', '--config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/', '--max_seq_length=512', '--num_workers=2', '--local_rank=1']
[2024-04-27 17:41:58,997] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-27 17:41:59,465] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-27 17:41:59,465] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
max_steps is given, it will override any value given in num_train_epochs
training started
{'loss': 95.1543, 'grad_norm': 251484.33131310585, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 39.2808, 'grad_norm': 548985.3317384718, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 27.9155, 'grad_norm': 209480.7911384717, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 22.2439, 'grad_norm': 159585.38479447298, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 20.4871, 'grad_norm': 136808.87269471964, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 18.3066, 'grad_norm': 79762.91990643271, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 17.571, 'grad_norm': 186715.07002917572, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 16.916, 'grad_norm': 80184.66766159226, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 15.4651, 'grad_norm': 118177.92687299944, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.793, 'grad_norm': 54936.850146327095, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.3647, 'grad_norm': 131514.3536500864, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 13.812, 'grad_norm': 115396.36791511248, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 13.7073, 'grad_norm': 102749.9028126061, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 12.9146, 'grad_norm': 136116.10596839743, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 11.8679, 'grad_norm': 67432.04128602367, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 12.2324, 'grad_norm': 90661.16646061862, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 11.302, 'grad_norm': 101716.44130621167, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 10.8062, 'grad_norm': 84930.83002067034, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 10.5029, 'grad_norm': 97563.08810200711, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 9.9272, 'grad_norm': 76391.60665937063, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 9.1934, 'grad_norm': 96200.12740116304, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 8.5337, 'grad_norm': 74223.84975194967, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 8.3817, 'grad_norm': 23360.72421822577, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 7.89, 'grad_norm': 55331.67237667772, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 7.5684, 'grad_norm': 41202.157225077426, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.9851, 'grad_norm': 167532.71833286775, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.5815, 'grad_norm': 26946.34268319172, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.4563, 'grad_norm': 28738.752930494393, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.5568, 'grad_norm': 46455.946616122244, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.6162, 'grad_norm': 23772.00437489443, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.3188, 'grad_norm': 25954.143098935092, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.1554, 'grad_norm': 26474.482960012647, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 3.541015625, 'eval_runtime': 93072.9374, 'eval_samples_per_second': 229.034, 'eval_steps_per_second': 1.789, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 5.4464, 'grad_norm': 27837.31272950031, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.7642, 'grad_norm': 43054.75137542894, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.6772, 'grad_norm': 26335.43301333775, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.374, 'grad_norm': 59064.54259536765, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.3741, 'grad_norm': 34388.10131426276, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.1086, 'grad_norm': 35270.02648141903, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.3196, 'grad_norm': 29809.89822189938, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.6094, 'grad_norm': 28844.470041933517, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.649, 'grad_norm': 22207.224410087812, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.5085, 'grad_norm': 24225.197955847543, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.3001, 'grad_norm': 31180.37049170519, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.3562, 'grad_norm': 25556.00657379787, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.3891, 'grad_norm': 44102.3685531741, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.287, 'grad_norm': 29512.53970772424, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.1601, 'grad_norm': 22507.662695180057, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0491, 'grad_norm': 11447.824072722293, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0525, 'grad_norm': 6937.5970984195965, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0868, 'grad_norm': 15432.079315503794, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.941, 'grad_norm': 17700.780321782426, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.9619, 'grad_norm': 14185.124320921548, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8544, 'grad_norm': 19873.615473788355, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8411, 'grad_norm': 20609.602616256336, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0479, 'grad_norm': 76404.4353686355, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8907, 'grad_norm': 32882.97918376618, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8869, 'grad_norm': 56425.02111652241, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.7009, 'grad_norm': 25564.3160675188, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.6359, 'grad_norm': 42025.024354543806, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.7049, 'grad_norm': 38558.37237228771, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4968, 'grad_norm': 27889.88949422353, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5519, 'grad_norm': 19733.526902203772, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5964, 'grad_norm': 31792.731747995484, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4842, 'grad_norm': 36585.151632868765, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 2.681640625, 'eval_runtime': 93374.6359, 'eval_samples_per_second': 228.294, 'eval_steps_per_second': 1.784, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 2.5933, 'grad_norm': 37250.84911783891, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5596, 'grad_norm': 31109.27450134445, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5057, 'grad_norm': 16259.915620937274, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5275, 'grad_norm': 34859.50579110381, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5623, 'grad_norm': 239862.88603283334, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5034, 'grad_norm': 17329.633752621547, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4594, 'grad_norm': 16871.912517554138, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5385, 'grad_norm': 35953.62586721957, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5938, 'grad_norm': 8520.970976068396, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5233, 'grad_norm': 6514.71337627374, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4058, 'grad_norm': 3335.988421742498, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3794, 'grad_norm': 4678.78317514287, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3318, 'grad_norm': 3401.0393815714633, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3227, 'grad_norm': 3934.349676248922, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2805, 'grad_norm': 3462.456858937018, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2375, 'grad_norm': 10874.456285258588, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.254, 'grad_norm': 2778.4676757162392, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4211, 'grad_norm': 11066.94542319605, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3562, 'grad_norm': 2190.196452375905, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2817, 'grad_norm': 1558.9929241340387, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1871, 'grad_norm': 2434.417627996478, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2366, 'grad_norm': 4927.994984017334, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2931, 'grad_norm': 9181.139716832546, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.379, 'grad_norm': 894.0962965475251, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3494, 'grad_norm': 2974.794992936488, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2729, 'grad_norm': 1355.4730262900846, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2559, 'grad_norm': 2423.195475503369, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2584, 'grad_norm': 1748.4509751491462, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2363, 'grad_norm': 2564.7636952553735, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.258, 'grad_norm': 1773.6243172526702, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2053, 'grad_norm': 929.4081281923459, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1801, 'grad_norm': 1182.431551930174, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 2.34765625, 'eval_runtime': 93332.6191, 'eval_samples_per_second': 228.397, 'eval_steps_per_second': 1.784, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 2.2085, 'grad_norm': 2956.9002984544472, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1777, 'grad_norm': 6291.365928794796, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1924, 'grad_norm': 1396.2390330813705, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2222, 'grad_norm': 16610.743421051327, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1605, 'grad_norm': 2945.6324872597397, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3307, 'grad_norm': 924.8577170976084, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2515, 'grad_norm': 811.5629885443274, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2355, 'grad_norm': 1593.6642525952573, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2617, 'grad_norm': 1891.3316489315139, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2109, 'grad_norm': 979.7143556925151, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1857, 'grad_norm': 962.206233578592, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2863, 'grad_norm': 914.9259276711967, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2515, 'grad_norm': 3126.2906134906907, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2864, 'grad_norm': 1111.6979060765566, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1824, 'grad_norm': 1181.1587332467218, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1622, 'grad_norm': 801.5610940533479, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1507, 'grad_norm': 1387.1548219286844, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1576, 'grad_norm': 659.8409520719747, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0809, 'grad_norm': 3447.8528735286836, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0678, 'grad_norm': 820.117445857604, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.091, 'grad_norm': 3168.7567652787743, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0746, 'grad_norm': 2343.475270565064, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1081, 'grad_norm': 601.5657142771271, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0658, 'grad_norm': 739.2015963186226, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0486, 'grad_norm': 532.42562567461, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0381, 'grad_norm': 938.1294969912203, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0844, 'grad_norm': 1031.344753222704, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0618, 'grad_norm': 1607.2341100085575, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0645, 'grad_norm': 1399.3352327980597, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1408, 'grad_norm': 1271.388869012939, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0894, 'grad_norm': 879.3837330767496, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0869, 'grad_norm': 880.1682145689539, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
[2024-05-01 17:41:50,367] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 456750
[2024-05-01 17:41:50,983] [INFO] [launch.py:325:sigkill_handler] Main process received SIGTERM, exiting
