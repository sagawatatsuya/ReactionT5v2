[2024-04-27 17:41:33,124] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-27 17:41:33,825] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-27 17:41:33,825] [INFO] [runner.py:568:main] cmd = /home/sagawa/miniconda3/envs/deepspeed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=12346 --enable_each_rank_log=None /data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py --do_train --do_eval --num_train_epochs=10 --output_dir=./output --overwrite_output_dir --save_total_limit=2 --deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json --per_device_train_batch_size=32 --per_device_eval_batch_size=128 --learning_rate=0.001 --weight_decay=0.01 --warmup_steps=10000 --logging_steps=32 --save_steps=1024 --eval_steps=1024 --config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base --train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/ --max_seq_length=512 --num_workers=2 --local_rank=1
[2024-04-27 17:41:35,108] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-27 17:41:35,774] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-27 17:41:35,774] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-27 17:41:35,774] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-27 17:41:35,774] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-27 17:41:35,774] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-27 17:41:35,775] [INFO] [launch.py:253:main] process 456432 spawned with command: ['/home/sagawa/miniconda3/envs/deepspeed/bin/python', '-u', '/data1/Scaling_up_ReactionT5/train_with_deepspeed/train_with_deepspeed.py', '--local_rank=0', '--do_train', '--do_eval', '--num_train_epochs=10', '--output_dir=./output', '--overwrite_output_dir', '--save_total_limit=2', '--deepspeed=/data1/Scaling_up_ReactionT5/train_with_deepspeed/deepspeed_configs/ds_config_zero0.json', '--per_device_train_batch_size=32', '--per_device_eval_batch_size=128', '--learning_rate=0.001', '--weight_decay=0.01', '--warmup_steps=10000', '--logging_steps=32', '--save_steps=1024', '--eval_steps=1024', '--config_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--tokenizer_name=/data1/Scaling_up_ReactionT5/train_with_deepspeed/T5configs/base', '--train_files_dir=/media/sagawa//7182ee6c-8215-4bea-a609-999c7c2c02cf/preprocessed_ZINC22/', '--max_seq_length=512', '--num_workers=2', '--local_rank=1']
[2024-04-27 17:41:40,186] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-04-27 17:41:40,632] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-27 17:41:40,632] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
max_steps is given, it will override any value given in num_train_epochs
training started
{'loss': 94.7998, 'grad_norm': 694552.6969438676, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 41.3667, 'grad_norm': 391411.5655317303, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 28.1509, 'grad_norm': 224589.9687875663, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 21.4873, 'grad_norm': 214577.9947711321, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 20.0762, 'grad_norm': 137246.95686243832, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 18.0886, 'grad_norm': 84333.66151187793, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 18.0168, 'grad_norm': 239205.19914082138, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 16.8359, 'grad_norm': 82171.37214383122, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 15.6658, 'grad_norm': 74322.57261424688, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 15.0076, 'grad_norm': 60320.62943968672, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.1187, 'grad_norm': 129146.39333717375, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.0303, 'grad_norm': 50048.21771052392, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 14.1074, 'grad_norm': 52759.945109903216, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 12.4639, 'grad_norm': 109900.33361186853, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 12.3909, 'grad_norm': 100430.73027714176, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 12.1763, 'grad_norm': 141386.61119073475, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 11.2341, 'grad_norm': 79272.47330568159, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 10.8271, 'grad_norm': 146182.29496077832, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 10.4875, 'grad_norm': 60964.39564204668, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 10.1448, 'grad_norm': 133368.63391367553, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 9.7312, 'grad_norm': 76102.53336124889, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 8.8966, 'grad_norm': 71280.48350004367, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 8.1759, 'grad_norm': 52634.80947053955, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 7.8865, 'grad_norm': 66323.19172054372, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 7.8051, 'grad_norm': 88513.98190116632, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 7.126, 'grad_norm': 48352.69192092618, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.4425, 'grad_norm': 54274.56774586049, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.7588, 'grad_norm': 54375.93379428072, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 6.511, 'grad_norm': 89772.74704496904, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.9033, 'grad_norm': 112546.94394784783, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.5498, 'grad_norm': 54051.01486558786, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 5.2078, 'grad_norm': 124476.11304985387, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 4.34375, 'eval_runtime': 93121.909, 'eval_samples_per_second': 228.914, 'eval_steps_per_second': 1.788, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 5.053, 'grad_norm': 152085.6238833901, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.7762, 'grad_norm': 183970.822034365, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.7966, 'grad_norm': 52477.21029170663, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.4298, 'grad_norm': 105921.97581238749, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.5706, 'grad_norm': 105248.01824262536, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.0227, 'grad_norm': 45047.78689347569, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 4.0091, 'grad_norm': 62144.04956228714, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.6544, 'grad_norm': 37880.141921592636, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.6684, 'grad_norm': 34943.68108828834, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.4738, 'grad_norm': 27543.858843669674, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.42, 'grad_norm': 48946.11371702558, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.5349, 'grad_norm': 76684.68989309405, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.3548, 'grad_norm': 85149.14820478241, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.1625, 'grad_norm': 42600.111173563855, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.1615, 'grad_norm': 40449.52766102467, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.1418, 'grad_norm': 32316.798851371404, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0198, 'grad_norm': 34574.33637830233, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.9355, 'grad_norm': 27827.516310299772, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8612, 'grad_norm': 37993.95572982629, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.9601, 'grad_norm': 23904.19009295232, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8508, 'grad_norm': 22622.012642556805, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.8777, 'grad_norm': 26571.65617721259, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 3.0137, 'grad_norm': 50807.725396833106, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.9686, 'grad_norm': 28338.676045291882, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.9758, 'grad_norm': 14065.920943898413, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.6875, 'grad_norm': 11985.101251136763, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.722, 'grad_norm': 33193.57907788794, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5889, 'grad_norm': 25940.96837051385, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.566, 'grad_norm': 28231.93808437529, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5574, 'grad_norm': 32637.813652265373, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5784, 'grad_norm': 25668.040517343743, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4954, 'grad_norm': 27558.289787285423, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 2.62109375, 'eval_runtime': 93510.9498, 'eval_samples_per_second': 227.961, 'eval_steps_per_second': 1.781, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 2.5574, 'grad_norm': 27230.15387396847, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.6038, 'grad_norm': 31841.281381250974, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5796, 'grad_norm': 45107.318075895404, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.547, 'grad_norm': 33063.394139138225, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5688, 'grad_norm': 23568.69262390258, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5469, 'grad_norm': 14224.185319377697, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4888, 'grad_norm': 14395.541393084179, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5662, 'grad_norm': 17464.143609120943, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5648, 'grad_norm': 24707.312763633363, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5395, 'grad_norm': 24900.485457115086, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5234, 'grad_norm': 9083.252391076667, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4161, 'grad_norm': 20434.571490491304, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4855, 'grad_norm': 18082.555350392267, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4358, 'grad_norm': 22908.19207183317, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4402, 'grad_norm': 23901.491334224316, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4161, 'grad_norm': 26476.019224951477, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3899, 'grad_norm': 25193.903389510724, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5571, 'grad_norm': 16575.921573173542, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4996, 'grad_norm': 43060.57863057579, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.5651, 'grad_norm': 9323.089616645331, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3812, 'grad_norm': 15469.129257976998, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3671, 'grad_norm': 125055.70694694425, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3427, 'grad_norm': 8025.16859635983, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3387, 'grad_norm': 10293.598593300596, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3901, 'grad_norm': 9159.260341315778, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.331, 'grad_norm': 7281.831019736726, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3259, 'grad_norm': 10539.70919902442, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3666, 'grad_norm': 11158.268682909551, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3669, 'grad_norm': 28348.939803809244, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2737, 'grad_norm': 6447.712307477746, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2865, 'grad_norm': 7530.921590350015, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3324, 'grad_norm': 17420.97525972642, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
{'eval_loss': 2.474609375, 'eval_runtime': 93328.0053, 'eval_samples_per_second': 228.408, 'eval_steps_per_second': 1.784, 'epoch': 0.0}
/home/sagawa/miniconda3/envs/deepspeed/lib/python3.11/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 2.4318, 'grad_norm': 5322.557514954629, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3769, 'grad_norm': 7506.511340163285, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.4026, 'grad_norm': 6386.083972200805, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.357, 'grad_norm': 2612.915995588071, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3663, 'grad_norm': 3541.654556842042, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3184, 'grad_norm': 1468.3344837774532, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2244, 'grad_norm': 3287.4879657574415, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2611, 'grad_norm': 4213.008277941072, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2676, 'grad_norm': 2287.3011388971063, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1941, 'grad_norm': 2604.4087140846386, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1693, 'grad_norm': 2960.8067734994124, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3079, 'grad_norm': 2394.685965841868, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3136, 'grad_norm': 3244.4640158275756, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.3287, 'grad_norm': 3524.854553736934, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2467, 'grad_norm': 2266.741087166772, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2787, 'grad_norm': 3112.0357485093255, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1892, 'grad_norm': 15774.752446552053, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.2466, 'grad_norm': 2689.3476207065537, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1691, 'grad_norm': 6989.18755113926, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1266, 'grad_norm': 5215.62069652309, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1284, 'grad_norm': 3861.2889039801203, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0946, 'grad_norm': 2253.4772990869023, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.126, 'grad_norm': 2028.0031126701952, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0915, 'grad_norm': 913.6127376383279, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0868, 'grad_norm': 3828.020261832479, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0658, 'grad_norm': 733.4816853967793, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0881, 'grad_norm': 5983.396328591312, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0827, 'grad_norm': 2133.957477464113, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0715, 'grad_norm': 1206.0659898508873, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.1396, 'grad_norm': 963.984310038291, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0687, 'grad_norm': 1265.663314234872, 'learning_rate': 0.001, 'epoch': 0.0}
{'loss': 2.0922, 'grad_norm': 972.1145524705408, 'learning_rate': 0.001, 'epoch': 0.0}
Too many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.
[2024-05-01 17:41:31,121] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 456432
[2024-05-01 17:41:31,220] [INFO] [launch.py:325:sigkill_handler] Main process received SIGTERM, exiting
