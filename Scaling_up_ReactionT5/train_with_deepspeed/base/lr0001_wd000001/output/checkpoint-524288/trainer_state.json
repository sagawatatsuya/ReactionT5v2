{
  "best_metric": 2.5859375,
  "best_model_checkpoint": "./output/checkpoint-262144",
  "epoch": 0.0019027452041085894,
  "eval_steps": 262144,
  "global_step": 524288,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.161343508367059e-07,
      "grad_norm": 25606.21861970252,
      "learning_rate": 0.002,
      "loss": 121.1763,
      "step": 32
    },
    {
      "epoch": 2.322687016734118e-07,
      "grad_norm": 26252.957776220188,
      "learning_rate": 0.002,
      "loss": 30.9237,
      "step": 64
    },
    {
      "epoch": 3.484030525101177e-07,
      "grad_norm": 31495.85102834975,
      "learning_rate": 0.002,
      "loss": 25.4659,
      "step": 96
    },
    {
      "epoch": 4.645374033468236e-07,
      "grad_norm": 12962.518119563036,
      "learning_rate": 0.002,
      "loss": 23.5299,
      "step": 128
    },
    {
      "epoch": 5.806717541835295e-07,
      "grad_norm": 12426.120633568627,
      "learning_rate": 0.002,
      "loss": 19.9345,
      "step": 160
    },
    {
      "epoch": 6.968061050202354e-07,
      "grad_norm": 12445.267212880566,
      "learning_rate": 0.002,
      "loss": 17.2281,
      "step": 192
    },
    {
      "epoch": 8.129404558569413e-07,
      "grad_norm": 10421.491064142407,
      "learning_rate": 0.002,
      "loss": 15.8623,
      "step": 224
    },
    {
      "epoch": 9.290748066936471e-07,
      "grad_norm": 13429.952792173173,
      "learning_rate": 0.002,
      "loss": 15.2696,
      "step": 256
    },
    {
      "epoch": 1.045209157530353e-06,
      "grad_norm": 8847.379894635475,
      "learning_rate": 0.002,
      "loss": 13.6005,
      "step": 288
    },
    {
      "epoch": 1.161343508367059e-06,
      "grad_norm": 12037.405866713974,
      "learning_rate": 0.002,
      "loss": 12.4542,
      "step": 320
    },
    {
      "epoch": 1.2774778592037649e-06,
      "grad_norm": 9362.097361168597,
      "learning_rate": 0.002,
      "loss": 11.9981,
      "step": 352
    },
    {
      "epoch": 1.3936122100404707e-06,
      "grad_norm": 8652.927654846075,
      "learning_rate": 0.002,
      "loss": 10.2553,
      "step": 384
    },
    {
      "epoch": 1.5097465608771768e-06,
      "grad_norm": 8445.383058215892,
      "learning_rate": 0.002,
      "loss": 9.502,
      "step": 416
    },
    {
      "epoch": 1.6258809117138826e-06,
      "grad_norm": 7352.88582802698,
      "learning_rate": 0.002,
      "loss": 8.8499,
      "step": 448
    },
    {
      "epoch": 1.7420152625505885e-06,
      "grad_norm": 7638.959222302473,
      "learning_rate": 0.002,
      "loss": 7.8651,
      "step": 480
    },
    {
      "epoch": 1.8581496133872943e-06,
      "grad_norm": 5757.693895996903,
      "learning_rate": 0.002,
      "loss": 7.0638,
      "step": 512
    },
    {
      "epoch": 1.974283964224e-06,
      "grad_norm": 7366.733672395113,
      "learning_rate": 0.002,
      "loss": 6.7561,
      "step": 544
    },
    {
      "epoch": 2.090418315060706e-06,
      "grad_norm": 6237.095958857776,
      "learning_rate": 0.002,
      "loss": 6.2991,
      "step": 576
    },
    {
      "epoch": 2.2065526658974122e-06,
      "grad_norm": 6806.069093096249,
      "learning_rate": 0.002,
      "loss": 5.6183,
      "step": 608
    },
    {
      "epoch": 2.322687016734118e-06,
      "grad_norm": 3817.958944252806,
      "learning_rate": 0.002,
      "loss": 4.9676,
      "step": 640
    },
    {
      "epoch": 2.438821367570824e-06,
      "grad_norm": 5384.017737712238,
      "learning_rate": 0.002,
      "loss": 4.6814,
      "step": 672
    },
    {
      "epoch": 2.5549557184075298e-06,
      "grad_norm": 5141.492293099349,
      "learning_rate": 0.002,
      "loss": 4.3281,
      "step": 704
    },
    {
      "epoch": 2.6710900692442356e-06,
      "grad_norm": 4229.500738857957,
      "learning_rate": 0.002,
      "loss": 4.08,
      "step": 736
    },
    {
      "epoch": 2.7872244200809414e-06,
      "grad_norm": 7080.830495076125,
      "learning_rate": 0.002,
      "loss": 4.0565,
      "step": 768
    },
    {
      "epoch": 2.9033587709176473e-06,
      "grad_norm": 5399.129281652737,
      "learning_rate": 0.002,
      "loss": 3.6685,
      "step": 800
    },
    {
      "epoch": 3.0194931217543536e-06,
      "grad_norm": 4261.404433986524,
      "learning_rate": 0.002,
      "loss": 3.5524,
      "step": 832
    },
    {
      "epoch": 3.1356274725910594e-06,
      "grad_norm": 3588.6112564611954,
      "learning_rate": 0.002,
      "loss": 3.281,
      "step": 864
    },
    {
      "epoch": 3.2517618234277652e-06,
      "grad_norm": 5658.2693909003665,
      "learning_rate": 0.002,
      "loss": 3.4934,
      "step": 896
    },
    {
      "epoch": 3.367896174264471e-06,
      "grad_norm": 3937.2998679297975,
      "learning_rate": 0.002,
      "loss": 3.2193,
      "step": 928
    },
    {
      "epoch": 3.484030525101177e-06,
      "grad_norm": 3751.292810485473,
      "learning_rate": 0.002,
      "loss": 2.9689,
      "step": 960
    },
    {
      "epoch": 3.6001648759378828e-06,
      "grad_norm": 3798.7740785679794,
      "learning_rate": 0.002,
      "loss": 2.9843,
      "step": 992
    },
    {
      "epoch": 3.7162992267745886e-06,
      "grad_norm": 5974.447840595815,
      "learning_rate": 0.002,
      "loss": 2.8554,
      "step": 1024
    },
    {
      "epoch": 3.8324335776112944e-06,
      "grad_norm": 8235.092227777415,
      "learning_rate": 0.002,
      "loss": 2.9834,
      "step": 1056
    },
    {
      "epoch": 3.948567928448e-06,
      "grad_norm": 7271.106380737391,
      "learning_rate": 0.002,
      "loss": 2.7836,
      "step": 1088
    },
    {
      "epoch": 4.064702279284706e-06,
      "grad_norm": 5466.077752831548,
      "learning_rate": 0.002,
      "loss": 2.8092,
      "step": 1120
    },
    {
      "epoch": 4.180836630121412e-06,
      "grad_norm": 8236.143636435683,
      "learning_rate": 0.002,
      "loss": 2.7501,
      "step": 1152
    },
    {
      "epoch": 4.296970980958119e-06,
      "grad_norm": 7018.669247086658,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 1184
    },
    {
      "epoch": 4.4131053317948245e-06,
      "grad_norm": 6594.205714109926,
      "learning_rate": 0.002,
      "loss": 2.6161,
      "step": 1216
    },
    {
      "epoch": 4.52923968263153e-06,
      "grad_norm": 6783.993550999293,
      "learning_rate": 0.002,
      "loss": 2.6676,
      "step": 1248
    },
    {
      "epoch": 4.645374033468236e-06,
      "grad_norm": 7339.988061979392,
      "learning_rate": 0.002,
      "loss": 2.5955,
      "step": 1280
    },
    {
      "epoch": 4.761508384304942e-06,
      "grad_norm": 4498.653756958853,
      "learning_rate": 0.002,
      "loss": 2.6165,
      "step": 1312
    },
    {
      "epoch": 4.877642735141648e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5293,
      "step": 1344
    },
    {
      "epoch": 4.993777085978354e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.51,
      "step": 1376
    },
    {
      "epoch": 5.1099114368150595e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5095,
      "step": 1408
    },
    {
      "epoch": 5.226045787651765e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5377,
      "step": 1440
    },
    {
      "epoch": 5.342180138488471e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5509,
      "step": 1472
    },
    {
      "epoch": 5.458314489325177e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5526,
      "step": 1504
    },
    {
      "epoch": 5.574448840161883e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5431,
      "step": 1536
    },
    {
      "epoch": 5.690583190998589e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.52,
      "step": 1568
    },
    {
      "epoch": 5.806717541835295e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5166,
      "step": 1600
    },
    {
      "epoch": 5.922851892672001e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5487,
      "step": 1632
    },
    {
      "epoch": 6.038986243508707e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5548,
      "step": 1664
    },
    {
      "epoch": 6.155120594345413e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.562,
      "step": 1696
    },
    {
      "epoch": 6.271254945182119e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5504,
      "step": 1728
    },
    {
      "epoch": 6.387389296018825e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5516,
      "step": 1760
    },
    {
      "epoch": 6.5035236468555305e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5553,
      "step": 1792
    },
    {
      "epoch": 6.619657997692236e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5442,
      "step": 1824
    },
    {
      "epoch": 6.735792348528942e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5426,
      "step": 1856
    },
    {
      "epoch": 6.851926699365648e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4981,
      "step": 1888
    },
    {
      "epoch": 6.968061050202354e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5369,
      "step": 1920
    },
    {
      "epoch": 7.08419540103906e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5353,
      "step": 1952
    },
    {
      "epoch": 7.2003297518757655e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5345,
      "step": 1984
    },
    {
      "epoch": 7.316464102712471e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5576,
      "step": 2016
    },
    {
      "epoch": 7.432598453549177e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5426,
      "step": 2048
    },
    {
      "epoch": 7.548732804385883e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5414,
      "step": 2080
    },
    {
      "epoch": 7.664867155222589e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5565,
      "step": 2112
    },
    {
      "epoch": 7.781001506059296e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5321,
      "step": 2144
    },
    {
      "epoch": 7.897135856896e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5323,
      "step": 2176
    },
    {
      "epoch": 8.013270207732707e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5585,
      "step": 2208
    },
    {
      "epoch": 8.129404558569412e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5486,
      "step": 2240
    },
    {
      "epoch": 8.245538909406119e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5373,
      "step": 2272
    },
    {
      "epoch": 8.361673260242824e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5286,
      "step": 2304
    },
    {
      "epoch": 8.47780761107953e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5284,
      "step": 2336
    },
    {
      "epoch": 8.593941961916237e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5471,
      "step": 2368
    },
    {
      "epoch": 8.710076312752942e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5492,
      "step": 2400
    },
    {
      "epoch": 8.826210663589649e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5306,
      "step": 2432
    },
    {
      "epoch": 8.942345014426354e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5395,
      "step": 2464
    },
    {
      "epoch": 9.05847936526306e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 2496
    },
    {
      "epoch": 9.174613716099766e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5569,
      "step": 2528
    },
    {
      "epoch": 9.290748066936472e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5602,
      "step": 2560
    },
    {
      "epoch": 9.406882417773177e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5577,
      "step": 2592
    },
    {
      "epoch": 9.523016768609884e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5488,
      "step": 2624
    },
    {
      "epoch": 9.639151119446589e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5331,
      "step": 2656
    },
    {
      "epoch": 9.755285470283296e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.552,
      "step": 2688
    },
    {
      "epoch": 9.87141982112e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5316,
      "step": 2720
    },
    {
      "epoch": 9.987554171956707e-06,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5128,
      "step": 2752
    },
    {
      "epoch": 1.0103688522793412e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5304,
      "step": 2784
    },
    {
      "epoch": 1.0219822873630119e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5117,
      "step": 2816
    },
    {
      "epoch": 1.0335957224466826e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5178,
      "step": 2848
    },
    {
      "epoch": 1.045209157530353e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5113,
      "step": 2880
    },
    {
      "epoch": 1.0568225926140237e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5152,
      "step": 2912
    },
    {
      "epoch": 1.0684360276976942e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5047,
      "step": 2944
    },
    {
      "epoch": 1.0800494627813649e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5175,
      "step": 2976
    },
    {
      "epoch": 1.0916628978650354e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5071,
      "step": 3008
    },
    {
      "epoch": 1.103276332948706e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.507,
      "step": 3040
    },
    {
      "epoch": 1.1148897680323766e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4938,
      "step": 3072
    },
    {
      "epoch": 1.1265032031160472e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5135,
      "step": 3104
    },
    {
      "epoch": 1.1381166381997177e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5132,
      "step": 3136
    },
    {
      "epoch": 1.1497300732833884e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.501,
      "step": 3168
    },
    {
      "epoch": 1.161343508367059e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4977,
      "step": 3200
    },
    {
      "epoch": 1.1729569434507296e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4848,
      "step": 3232
    },
    {
      "epoch": 1.1845703785344003e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5134,
      "step": 3264
    },
    {
      "epoch": 1.1961838136180708e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5038,
      "step": 3296
    },
    {
      "epoch": 1.2077972487017414e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4903,
      "step": 3328
    },
    {
      "epoch": 1.219410683785412e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4988,
      "step": 3360
    },
    {
      "epoch": 1.2310241188690826e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5051,
      "step": 3392
    },
    {
      "epoch": 1.2426375539527531e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5169,
      "step": 3424
    },
    {
      "epoch": 1.2542509890364238e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5068,
      "step": 3456
    },
    {
      "epoch": 1.2658644241200943e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5103,
      "step": 3488
    },
    {
      "epoch": 1.277477859203765e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5211,
      "step": 3520
    },
    {
      "epoch": 1.2890912942874354e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5246,
      "step": 3552
    },
    {
      "epoch": 1.3007047293711061e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.506,
      "step": 3584
    },
    {
      "epoch": 1.3123181644547766e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4917,
      "step": 3616
    },
    {
      "epoch": 1.3239315995384473e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.483,
      "step": 3648
    },
    {
      "epoch": 1.3355450346221178e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4807,
      "step": 3680
    },
    {
      "epoch": 1.3471584697057884e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5283,
      "step": 3712
    },
    {
      "epoch": 1.3587719047894591e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5187,
      "step": 3744
    },
    {
      "epoch": 1.3703853398731296e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5001,
      "step": 3776
    },
    {
      "epoch": 1.3819987749568003e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5019,
      "step": 3808
    },
    {
      "epoch": 1.3936122100404708e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5046,
      "step": 3840
    },
    {
      "epoch": 1.4052256451241414e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5075,
      "step": 3872
    },
    {
      "epoch": 1.416839080207812e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4989,
      "step": 3904
    },
    {
      "epoch": 1.4284525152914826e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4739,
      "step": 3936
    },
    {
      "epoch": 1.4400659503751531e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5135,
      "step": 3968
    },
    {
      "epoch": 1.4516793854588238e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.513,
      "step": 4000
    },
    {
      "epoch": 1.4632928205424943e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5133,
      "step": 4032
    },
    {
      "epoch": 1.474906255626165e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5014,
      "step": 4064
    },
    {
      "epoch": 1.4865196907098354e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4978,
      "step": 4096
    },
    {
      "epoch": 1.4981331257935061e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4942,
      "step": 4128
    },
    {
      "epoch": 1.5097465608771766e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.516,
      "step": 4160
    },
    {
      "epoch": 1.5213599959608473e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5051,
      "step": 4192
    },
    {
      "epoch": 1.5329734310445178e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5032,
      "step": 4224
    },
    {
      "epoch": 1.5445868661281884e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4935,
      "step": 4256
    },
    {
      "epoch": 1.556200301211859e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5169,
      "step": 4288
    },
    {
      "epoch": 1.5678137362955298e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5113,
      "step": 4320
    },
    {
      "epoch": 1.5794271713792e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5117,
      "step": 4352
    },
    {
      "epoch": 1.5910406064628708e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5128,
      "step": 4384
    },
    {
      "epoch": 1.6026540415465414e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5241,
      "step": 4416
    },
    {
      "epoch": 1.614267476630212e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4999,
      "step": 4448
    },
    {
      "epoch": 1.6258809117138824e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4996,
      "step": 4480
    },
    {
      "epoch": 1.637494346797553e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4819,
      "step": 4512
    },
    {
      "epoch": 1.6491077818812238e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4988,
      "step": 4544
    },
    {
      "epoch": 1.6607212169648945e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5241,
      "step": 4576
    },
    {
      "epoch": 1.6723346520485648e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5323,
      "step": 4608
    },
    {
      "epoch": 1.6839480871322355e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5395,
      "step": 4640
    },
    {
      "epoch": 1.695561522215906e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5273,
      "step": 4672
    },
    {
      "epoch": 1.7071749572995768e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5156,
      "step": 4704
    },
    {
      "epoch": 1.7187883923832475e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5224,
      "step": 4736
    },
    {
      "epoch": 1.7304018274669178e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5263,
      "step": 4768
    },
    {
      "epoch": 1.7420152625505885e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5275,
      "step": 4800
    },
    {
      "epoch": 1.753628697634259e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5078,
      "step": 4832
    },
    {
      "epoch": 1.7652421327179298e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5443,
      "step": 4864
    },
    {
      "epoch": 1.7768555678016e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5298,
      "step": 4896
    },
    {
      "epoch": 1.7884690028852708e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5237,
      "step": 4928
    },
    {
      "epoch": 1.8000824379689415e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5212,
      "step": 4960
    },
    {
      "epoch": 1.811695873052612e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5088,
      "step": 4992
    },
    {
      "epoch": 1.8233093081362825e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5197,
      "step": 5024
    },
    {
      "epoch": 1.834922743219953e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5221,
      "step": 5056
    },
    {
      "epoch": 1.8465361783036238e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5163,
      "step": 5088
    },
    {
      "epoch": 1.8581496133872945e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5135,
      "step": 5120
    },
    {
      "epoch": 1.869763048470965e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5159,
      "step": 5152
    },
    {
      "epoch": 1.8813764835546355e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5323,
      "step": 5184
    },
    {
      "epoch": 1.892989918638306e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5328,
      "step": 5216
    },
    {
      "epoch": 1.9046033537219768e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5471,
      "step": 5248
    },
    {
      "epoch": 1.9162167888056475e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5294,
      "step": 5280
    },
    {
      "epoch": 1.9278302238893178e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5302,
      "step": 5312
    },
    {
      "epoch": 1.9394436589729885e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5281,
      "step": 5344
    },
    {
      "epoch": 1.951057094056659e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5097,
      "step": 5376
    },
    {
      "epoch": 1.9626705291403298e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5177,
      "step": 5408
    },
    {
      "epoch": 1.974283964224e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.507,
      "step": 5440
    },
    {
      "epoch": 1.9858973993076708e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5407,
      "step": 5472
    },
    {
      "epoch": 1.9975108343913415e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5319,
      "step": 5504
    },
    {
      "epoch": 2.009124269475012e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5225,
      "step": 5536
    },
    {
      "epoch": 2.0207377045586825e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5258,
      "step": 5568
    },
    {
      "epoch": 2.032351139642353e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5133,
      "step": 5600
    },
    {
      "epoch": 2.0439645747260238e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5308,
      "step": 5632
    },
    {
      "epoch": 2.0555780098096945e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5046,
      "step": 5664
    },
    {
      "epoch": 2.067191444893365e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.522,
      "step": 5696
    },
    {
      "epoch": 2.0788048799770355e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 5728
    },
    {
      "epoch": 2.090418315060706e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5384,
      "step": 5760
    },
    {
      "epoch": 2.1020317501443768e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5294,
      "step": 5792
    },
    {
      "epoch": 2.1136451852280475e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5191,
      "step": 5824
    },
    {
      "epoch": 2.1252586203117178e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5236,
      "step": 5856
    },
    {
      "epoch": 2.1368720553953885e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.508,
      "step": 5888
    },
    {
      "epoch": 2.148485490479059e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5268,
      "step": 5920
    },
    {
      "epoch": 2.1600989255627298e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5288,
      "step": 5952
    },
    {
      "epoch": 2.1717123606464e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5173,
      "step": 5984
    },
    {
      "epoch": 2.1833257957300708e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5121,
      "step": 6016
    },
    {
      "epoch": 2.1949392308137415e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.523,
      "step": 6048
    },
    {
      "epoch": 2.206552665897412e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5473,
      "step": 6080
    },
    {
      "epoch": 2.2181661009810828e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5338,
      "step": 6112
    },
    {
      "epoch": 2.229779536064753e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5278,
      "step": 6144
    },
    {
      "epoch": 2.2413929711484238e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5145,
      "step": 6176
    },
    {
      "epoch": 2.2530064062320945e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5394,
      "step": 6208
    },
    {
      "epoch": 2.264619841315765e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5109,
      "step": 6240
    },
    {
      "epoch": 2.2762332763994355e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5045,
      "step": 6272
    },
    {
      "epoch": 2.287846711483106e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5245,
      "step": 6304
    },
    {
      "epoch": 2.299460146566777e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5171,
      "step": 6336
    },
    {
      "epoch": 2.3110735816504475e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 6368
    },
    {
      "epoch": 2.322687016734118e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5356,
      "step": 6400
    },
    {
      "epoch": 2.3343004518177885e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5168,
      "step": 6432
    },
    {
      "epoch": 2.345913886901459e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5185,
      "step": 6464
    },
    {
      "epoch": 2.35752732198513e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5139,
      "step": 6496
    },
    {
      "epoch": 2.3691407570688005e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5273,
      "step": 6528
    },
    {
      "epoch": 2.380754192152471e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5339,
      "step": 6560
    },
    {
      "epoch": 2.3923676272361415e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5098,
      "step": 6592
    },
    {
      "epoch": 2.4039810623198122e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5302,
      "step": 6624
    },
    {
      "epoch": 2.415594497403483e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5459,
      "step": 6656
    },
    {
      "epoch": 2.4272079324871532e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5284,
      "step": 6688
    },
    {
      "epoch": 2.438821367570824e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5242,
      "step": 6720
    },
    {
      "epoch": 2.4504348026544945e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5043,
      "step": 6752
    },
    {
      "epoch": 2.4620482377381652e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5133,
      "step": 6784
    },
    {
      "epoch": 2.4736616728218355e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5279,
      "step": 6816
    },
    {
      "epoch": 2.4852751079055062e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5225,
      "step": 6848
    },
    {
      "epoch": 2.496888542989177e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5011,
      "step": 6880
    },
    {
      "epoch": 2.5085019780728475e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5169,
      "step": 6912
    },
    {
      "epoch": 2.520115413156518e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5364,
      "step": 6944
    },
    {
      "epoch": 2.5317288482401885e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5433,
      "step": 6976
    },
    {
      "epoch": 2.5433422833238592e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5389,
      "step": 7008
    },
    {
      "epoch": 2.55495571840753e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5157,
      "step": 7040
    },
    {
      "epoch": 2.5665691534912005e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5332,
      "step": 7072
    },
    {
      "epoch": 2.578182588574871e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5195,
      "step": 7104
    },
    {
      "epoch": 2.5897960236585415e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5304,
      "step": 7136
    },
    {
      "epoch": 2.6014094587422122e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5291,
      "step": 7168
    },
    {
      "epoch": 2.613022893825883e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5116,
      "step": 7200
    },
    {
      "epoch": 2.6246363289095532e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5204,
      "step": 7232
    },
    {
      "epoch": 2.636249763993224e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5367,
      "step": 7264
    },
    {
      "epoch": 2.6478631990768945e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5234,
      "step": 7296
    },
    {
      "epoch": 2.6594766341605652e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5217,
      "step": 7328
    },
    {
      "epoch": 2.6710900692442355e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5011,
      "step": 7360
    },
    {
      "epoch": 2.6827035043279062e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5181,
      "step": 7392
    },
    {
      "epoch": 2.694316939411577e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5316,
      "step": 7424
    },
    {
      "epoch": 2.7059303744952475e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5209,
      "step": 7456
    },
    {
      "epoch": 2.7175438095789182e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5167,
      "step": 7488
    },
    {
      "epoch": 2.7291572446625885e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5299,
      "step": 7520
    },
    {
      "epoch": 2.7407706797462592e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5375,
      "step": 7552
    },
    {
      "epoch": 2.75238411482993e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5338,
      "step": 7584
    },
    {
      "epoch": 2.7639975499136005e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5249,
      "step": 7616
    },
    {
      "epoch": 2.775610984997271e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5182,
      "step": 7648
    },
    {
      "epoch": 2.7872244200809415e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.507,
      "step": 7680
    },
    {
      "epoch": 2.7988378551646122e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5311,
      "step": 7712
    },
    {
      "epoch": 2.810451290248283e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.531,
      "step": 7744
    },
    {
      "epoch": 2.8220647253319532e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5046,
      "step": 7776
    },
    {
      "epoch": 2.833678160415624e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5239,
      "step": 7808
    },
    {
      "epoch": 2.8452915954992945e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5383,
      "step": 7840
    },
    {
      "epoch": 2.8569050305829652e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5349,
      "step": 7872
    },
    {
      "epoch": 2.868518465666636e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.524,
      "step": 7904
    },
    {
      "epoch": 2.8801319007503062e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5177,
      "step": 7936
    },
    {
      "epoch": 2.891745335833977e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5172,
      "step": 7968
    },
    {
      "epoch": 2.9033587709176475e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5351,
      "step": 8000
    },
    {
      "epoch": 2.9149722060013182e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5307,
      "step": 8032
    },
    {
      "epoch": 2.9265856410849885e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5193,
      "step": 8064
    },
    {
      "epoch": 2.9381990761686592e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5123,
      "step": 8096
    },
    {
      "epoch": 2.94981251125233e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5051,
      "step": 8128
    },
    {
      "epoch": 2.9614259463360005e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5339,
      "step": 8160
    },
    {
      "epoch": 2.973039381419671e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5321,
      "step": 8192
    },
    {
      "epoch": 2.9846528165033415e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5102,
      "step": 8224
    },
    {
      "epoch": 2.9962662515870122e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5213,
      "step": 8256
    },
    {
      "epoch": 3.007879686670683e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5251,
      "step": 8288
    },
    {
      "epoch": 3.0194931217543532e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.536,
      "step": 8320
    },
    {
      "epoch": 3.031106556838024e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5209,
      "step": 8352
    },
    {
      "epoch": 3.0427199919216946e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5189,
      "step": 8384
    },
    {
      "epoch": 3.054333427005365e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5387,
      "step": 8416
    },
    {
      "epoch": 3.0659468620890356e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5258,
      "step": 8448
    },
    {
      "epoch": 3.077560297172706e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5332,
      "step": 8480
    },
    {
      "epoch": 3.089173732256377e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5185,
      "step": 8512
    },
    {
      "epoch": 3.1007871673400476e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5164,
      "step": 8544
    },
    {
      "epoch": 3.112400602423718e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5121,
      "step": 8576
    },
    {
      "epoch": 3.124014037507389e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5228,
      "step": 8608
    },
    {
      "epoch": 3.1356274725910596e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5165,
      "step": 8640
    },
    {
      "epoch": 3.1472409076747296e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5169,
      "step": 8672
    },
    {
      "epoch": 3.1588543427584e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5374,
      "step": 8704
    },
    {
      "epoch": 3.170467777842071e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5199,
      "step": 8736
    },
    {
      "epoch": 3.1820812129257416e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5324,
      "step": 8768
    },
    {
      "epoch": 3.193694648009412e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5244,
      "step": 8800
    },
    {
      "epoch": 3.205308083093083e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5221,
      "step": 8832
    },
    {
      "epoch": 3.2169215181767536e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5477,
      "step": 8864
    },
    {
      "epoch": 3.228534953260424e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5347,
      "step": 8896
    },
    {
      "epoch": 3.240148388344095e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5361,
      "step": 8928
    },
    {
      "epoch": 3.251761823427765e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5093,
      "step": 8960
    },
    {
      "epoch": 3.2633752585114356e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5233,
      "step": 8992
    },
    {
      "epoch": 3.274988693595106e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5091,
      "step": 9024
    },
    {
      "epoch": 3.286602128678777e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5237,
      "step": 9056
    },
    {
      "epoch": 3.2982155637624476e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.519,
      "step": 9088
    },
    {
      "epoch": 3.309828998846118e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5104,
      "step": 9120
    },
    {
      "epoch": 3.321442433929789e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5271,
      "step": 9152
    },
    {
      "epoch": 3.3330558690134596e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 9184
    },
    {
      "epoch": 3.3446693040971296e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5171,
      "step": 9216
    },
    {
      "epoch": 3.3562827391808e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5208,
      "step": 9248
    },
    {
      "epoch": 3.367896174264471e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5242,
      "step": 9280
    },
    {
      "epoch": 3.3795096093481416e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5439,
      "step": 9312
    },
    {
      "epoch": 3.391123044431812e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5236,
      "step": 9344
    },
    {
      "epoch": 3.402736479515483e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5287,
      "step": 9376
    },
    {
      "epoch": 3.4143499145991536e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.523,
      "step": 9408
    },
    {
      "epoch": 3.425963349682824e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5171,
      "step": 9440
    },
    {
      "epoch": 3.437576784766495e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5104,
      "step": 9472
    },
    {
      "epoch": 3.449190219850165e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5298,
      "step": 9504
    },
    {
      "epoch": 3.4608036549338356e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5276,
      "step": 9536
    },
    {
      "epoch": 3.472417090017506e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5286,
      "step": 9568
    },
    {
      "epoch": 3.484030525101177e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.522,
      "step": 9600
    },
    {
      "epoch": 3.4956439601848476e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5251,
      "step": 9632
    },
    {
      "epoch": 3.507257395268518e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5167,
      "step": 9664
    },
    {
      "epoch": 3.518870830352189e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5287,
      "step": 9696
    },
    {
      "epoch": 3.5304842654358596e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5286,
      "step": 9728
    },
    {
      "epoch": 3.54209770051953e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.553,
      "step": 9760
    },
    {
      "epoch": 3.5537111356032e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5258,
      "step": 9792
    },
    {
      "epoch": 3.565324570686871e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.51,
      "step": 9824
    },
    {
      "epoch": 3.5769380057705416e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5224,
      "step": 9856
    },
    {
      "epoch": 3.588551440854212e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5088,
      "step": 9888
    },
    {
      "epoch": 3.600164875937883e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5188,
      "step": 9920
    },
    {
      "epoch": 3.6117783110215536e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5083,
      "step": 9952
    },
    {
      "epoch": 3.623391746105224e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.526,
      "step": 9984
    },
    {
      "epoch": 3.635005181188895e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5276,
      "step": 10016
    },
    {
      "epoch": 3.646618616272565e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5215,
      "step": 10048
    },
    {
      "epoch": 3.6582320513562356e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5337,
      "step": 10080
    },
    {
      "epoch": 3.669845486439906e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5178,
      "step": 10112
    },
    {
      "epoch": 3.681458921523577e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5234,
      "step": 10144
    },
    {
      "epoch": 3.6930723566072476e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5351,
      "step": 10176
    },
    {
      "epoch": 3.704685791690918e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5385,
      "step": 10208
    },
    {
      "epoch": 3.716299226774589e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5243,
      "step": 10240
    },
    {
      "epoch": 3.7279126618582596e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5202,
      "step": 10272
    },
    {
      "epoch": 3.73952609694193e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5185,
      "step": 10304
    },
    {
      "epoch": 3.7511395320256e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5042,
      "step": 10336
    },
    {
      "epoch": 3.762752967109271e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5226,
      "step": 10368
    },
    {
      "epoch": 3.7743664021929416e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5217,
      "step": 10400
    },
    {
      "epoch": 3.785979837276612e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5101,
      "step": 10432
    },
    {
      "epoch": 3.797593272360283e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5239,
      "step": 10464
    },
    {
      "epoch": 3.8092067074439536e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5231,
      "step": 10496
    },
    {
      "epoch": 3.820820142527624e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5252,
      "step": 10528
    },
    {
      "epoch": 3.832433577611295e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5252,
      "step": 10560
    },
    {
      "epoch": 3.844047012694965e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.537,
      "step": 10592
    },
    {
      "epoch": 3.8556604477786356e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5373,
      "step": 10624
    },
    {
      "epoch": 3.867273882862306e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5436,
      "step": 10656
    },
    {
      "epoch": 3.878887317945977e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5316,
      "step": 10688
    },
    {
      "epoch": 3.8905007530296476e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5082,
      "step": 10720
    },
    {
      "epoch": 3.902114188113318e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5176,
      "step": 10752
    },
    {
      "epoch": 3.913727623196989e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5086,
      "step": 10784
    },
    {
      "epoch": 3.9253410582806596e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5114,
      "step": 10816
    },
    {
      "epoch": 3.93695449336433e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5285,
      "step": 10848
    },
    {
      "epoch": 3.948567928448e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5071,
      "step": 10880
    },
    {
      "epoch": 3.960181363531671e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5254,
      "step": 10912
    },
    {
      "epoch": 3.9717947986153416e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5224,
      "step": 10944
    },
    {
      "epoch": 3.983408233699012e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5281,
      "step": 10976
    },
    {
      "epoch": 3.995021668782683e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5223,
      "step": 11008
    },
    {
      "epoch": 4.0066351038663536e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5182,
      "step": 11040
    },
    {
      "epoch": 4.018248538950024e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5325,
      "step": 11072
    },
    {
      "epoch": 4.029861974033695e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.54,
      "step": 11104
    },
    {
      "epoch": 4.041475409117365e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5283,
      "step": 11136
    },
    {
      "epoch": 4.0530888442010356e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5229,
      "step": 11168
    },
    {
      "epoch": 4.064702279284706e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5115,
      "step": 11200
    },
    {
      "epoch": 4.076315714368377e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.522,
      "step": 11232
    },
    {
      "epoch": 4.0879291494520476e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 11264
    },
    {
      "epoch": 4.099542584535718e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5254,
      "step": 11296
    },
    {
      "epoch": 4.111156019619389e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5097,
      "step": 11328
    },
    {
      "epoch": 4.1227694547030596e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5281,
      "step": 11360
    },
    {
      "epoch": 4.13438288978673e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5095,
      "step": 11392
    },
    {
      "epoch": 4.1459963248704e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5376,
      "step": 11424
    },
    {
      "epoch": 4.157609759954071e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5347,
      "step": 11456
    },
    {
      "epoch": 4.1692231950377416e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5299,
      "step": 11488
    },
    {
      "epoch": 4.180836630121412e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.532,
      "step": 11520
    },
    {
      "epoch": 4.192450065205083e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5356,
      "step": 11552
    },
    {
      "epoch": 4.2040635002887536e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5124,
      "step": 11584
    },
    {
      "epoch": 4.215676935372424e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5195,
      "step": 11616
    },
    {
      "epoch": 4.227290370456095e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5033,
      "step": 11648
    },
    {
      "epoch": 4.2389038055397656e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5124,
      "step": 11680
    },
    {
      "epoch": 4.2505172406234356e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5292,
      "step": 11712
    },
    {
      "epoch": 4.262130675707106e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5332,
      "step": 11744
    },
    {
      "epoch": 4.273744110790777e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5172,
      "step": 11776
    },
    {
      "epoch": 4.2853575458744476e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5186,
      "step": 11808
    },
    {
      "epoch": 4.296970980958118e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5275,
      "step": 11840
    },
    {
      "epoch": 4.308584416041789e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5374,
      "step": 11872
    },
    {
      "epoch": 4.3201978511254596e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5195,
      "step": 11904
    },
    {
      "epoch": 4.33181128620913e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5366,
      "step": 11936
    },
    {
      "epoch": 4.3434247212928e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5255,
      "step": 11968
    },
    {
      "epoch": 4.355038156376471e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5346,
      "step": 12000
    },
    {
      "epoch": 4.3666515914601416e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5335,
      "step": 12032
    },
    {
      "epoch": 4.378265026543812e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.518,
      "step": 12064
    },
    {
      "epoch": 4.389878461627483e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5126,
      "step": 12096
    },
    {
      "epoch": 4.4014918967111537e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5149,
      "step": 12128
    },
    {
      "epoch": 4.413105331794824e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5159,
      "step": 12160
    },
    {
      "epoch": 4.424718766878495e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5192,
      "step": 12192
    },
    {
      "epoch": 4.4363322019621657e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5246,
      "step": 12224
    },
    {
      "epoch": 4.4479456370458356e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5015,
      "step": 12256
    },
    {
      "epoch": 4.459559072129506e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5354,
      "step": 12288
    },
    {
      "epoch": 4.471172507213177e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5482,
      "step": 12320
    },
    {
      "epoch": 4.4827859422968477e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5386,
      "step": 12352
    },
    {
      "epoch": 4.494399377380518e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5342,
      "step": 12384
    },
    {
      "epoch": 4.506012812464189e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5173,
      "step": 12416
    },
    {
      "epoch": 4.51762624754786e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5424,
      "step": 12448
    },
    {
      "epoch": 4.52923968263153e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5152,
      "step": 12480
    },
    {
      "epoch": 4.5408531177152e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5167,
      "step": 12512
    },
    {
      "epoch": 4.552466552798871e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5166,
      "step": 12544
    },
    {
      "epoch": 4.5640799878825417e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4968,
      "step": 12576
    },
    {
      "epoch": 4.575693422966212e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5327,
      "step": 12608
    },
    {
      "epoch": 4.587306858049883e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.525,
      "step": 12640
    },
    {
      "epoch": 4.598920293133554e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5207,
      "step": 12672
    },
    {
      "epoch": 4.610533728217224e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5107,
      "step": 12704
    },
    {
      "epoch": 4.622147163300895e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5102,
      "step": 12736
    },
    {
      "epoch": 4.633760598384566e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5302,
      "step": 12768
    },
    {
      "epoch": 4.645374033468236e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 12800
    },
    {
      "epoch": 4.656987468551906e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5334,
      "step": 12832
    },
    {
      "epoch": 4.668600903635577e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5255,
      "step": 12864
    },
    {
      "epoch": 4.680214338719248e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5404,
      "step": 12896
    },
    {
      "epoch": 4.691827773802918e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5325,
      "step": 12928
    },
    {
      "epoch": 4.703441208886589e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 12960
    },
    {
      "epoch": 4.71505464397026e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5287,
      "step": 12992
    },
    {
      "epoch": 4.7266680790539303e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4975,
      "step": 13024
    },
    {
      "epoch": 4.738281514137601e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5173,
      "step": 13056
    },
    {
      "epoch": 4.749894949221271e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5219,
      "step": 13088
    },
    {
      "epoch": 4.761508384304942e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.521,
      "step": 13120
    },
    {
      "epoch": 4.7731218193886123e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5248,
      "step": 13152
    },
    {
      "epoch": 4.784735254472283e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 13184
    },
    {
      "epoch": 4.796348689555954e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5479,
      "step": 13216
    },
    {
      "epoch": 4.8079621246396243e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5313,
      "step": 13248
    },
    {
      "epoch": 4.819575559723295e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5269,
      "step": 13280
    },
    {
      "epoch": 4.831188994806966e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5107,
      "step": 13312
    },
    {
      "epoch": 4.842802429890636e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5119,
      "step": 13344
    },
    {
      "epoch": 4.8544158649743063e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5163,
      "step": 13376
    },
    {
      "epoch": 4.866029300057977e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.518,
      "step": 13408
    },
    {
      "epoch": 4.877642735141648e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5267,
      "step": 13440
    },
    {
      "epoch": 4.8892561702253184e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5084,
      "step": 13472
    },
    {
      "epoch": 4.900869605308989e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5301,
      "step": 13504
    },
    {
      "epoch": 4.91248304039266e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5324,
      "step": 13536
    },
    {
      "epoch": 4.9240964754763304e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5267,
      "step": 13568
    },
    {
      "epoch": 4.935709910560001e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5269,
      "step": 13600
    },
    {
      "epoch": 4.947323345643671e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5165,
      "step": 13632
    },
    {
      "epoch": 4.958936780727342e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5197,
      "step": 13664
    },
    {
      "epoch": 4.9705502158110124e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5403,
      "step": 13696
    },
    {
      "epoch": 4.982163650894683e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5312,
      "step": 13728
    },
    {
      "epoch": 4.993777085978354e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5248,
      "step": 13760
    },
    {
      "epoch": 5.0053905210620244e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5262,
      "step": 13792
    },
    {
      "epoch": 5.017003956145695e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5324,
      "step": 13824
    },
    {
      "epoch": 5.028617391229366e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5323,
      "step": 13856
    },
    {
      "epoch": 5.040230826313036e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.513,
      "step": 13888
    },
    {
      "epoch": 5.0518442613967064e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4844,
      "step": 13920
    },
    {
      "epoch": 5.063457696480377e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5128,
      "step": 13952
    },
    {
      "epoch": 5.075071131564048e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5204,
      "step": 13984
    },
    {
      "epoch": 5.0866845666477184e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5294,
      "step": 14016
    },
    {
      "epoch": 5.098298001731389e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.549,
      "step": 14048
    },
    {
      "epoch": 5.10991143681506e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 14080
    },
    {
      "epoch": 5.1215248718987304e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 14112
    },
    {
      "epoch": 5.133138306982401e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5364,
      "step": 14144
    },
    {
      "epoch": 5.144751742066071e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5202,
      "step": 14176
    },
    {
      "epoch": 5.156365177149742e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5379,
      "step": 14208
    },
    {
      "epoch": 5.1679786122334124e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4922,
      "step": 14240
    },
    {
      "epoch": 5.179592047317083e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5223,
      "step": 14272
    },
    {
      "epoch": 5.191205482400754e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5323,
      "step": 14304
    },
    {
      "epoch": 5.2028189174844244e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5154,
      "step": 14336
    },
    {
      "epoch": 5.214432352568095e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5111,
      "step": 14368
    },
    {
      "epoch": 5.226045787651766e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5157,
      "step": 14400
    },
    {
      "epoch": 5.2376592227354364e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.53,
      "step": 14432
    },
    {
      "epoch": 5.2492726578191064e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5219,
      "step": 14464
    },
    {
      "epoch": 5.260886092902777e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5176,
      "step": 14496
    },
    {
      "epoch": 5.272499527986448e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.512,
      "step": 14528
    },
    {
      "epoch": 5.2841129630701184e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5174,
      "step": 14560
    },
    {
      "epoch": 5.295726398153789e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5382,
      "step": 14592
    },
    {
      "epoch": 5.30733983323746e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.528,
      "step": 14624
    },
    {
      "epoch": 5.3189532683211304e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5486,
      "step": 14656
    },
    {
      "epoch": 5.330566703404801e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5369,
      "step": 14688
    },
    {
      "epoch": 5.342180138488471e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.528,
      "step": 14720
    },
    {
      "epoch": 5.353793573572142e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5197,
      "step": 14752
    },
    {
      "epoch": 5.3654070086558124e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5135,
      "step": 14784
    },
    {
      "epoch": 5.377020443739483e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4975,
      "step": 14816
    },
    {
      "epoch": 5.388633878823154e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5087,
      "step": 14848
    },
    {
      "epoch": 5.4002473139068244e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5463,
      "step": 14880
    },
    {
      "epoch": 5.411860748990495e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5358,
      "step": 14912
    },
    {
      "epoch": 5.423474184074166e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5321,
      "step": 14944
    },
    {
      "epoch": 5.4350876191578364e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5283,
      "step": 14976
    },
    {
      "epoch": 5.4467010542415064e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5185,
      "step": 15008
    },
    {
      "epoch": 5.458314489325177e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5262,
      "step": 15040
    },
    {
      "epoch": 5.469927924408848e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5192,
      "step": 15072
    },
    {
      "epoch": 5.4815413594925184e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5099,
      "step": 15104
    },
    {
      "epoch": 5.493154794576189e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.519,
      "step": 15136
    },
    {
      "epoch": 5.50476822965986e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 15168
    },
    {
      "epoch": 5.5163816647435304e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 15200
    },
    {
      "epoch": 5.527995099827201e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5189,
      "step": 15232
    },
    {
      "epoch": 5.539608534910871e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5156,
      "step": 15264
    },
    {
      "epoch": 5.551221969994542e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5302,
      "step": 15296
    },
    {
      "epoch": 5.5628354050782124e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5179,
      "step": 15328
    },
    {
      "epoch": 5.574448840161883e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5236,
      "step": 15360
    },
    {
      "epoch": 5.586062275245554e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5285,
      "step": 15392
    },
    {
      "epoch": 5.5976757103292244e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.516,
      "step": 15424
    },
    {
      "epoch": 5.609289145412895e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5321,
      "step": 15456
    },
    {
      "epoch": 5.620902580496566e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5243,
      "step": 15488
    },
    {
      "epoch": 5.6325160155802364e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5326,
      "step": 15520
    },
    {
      "epoch": 5.6441294506639064e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5504,
      "step": 15552
    },
    {
      "epoch": 5.655742885747577e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5287,
      "step": 15584
    },
    {
      "epoch": 5.667356320831248e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5154,
      "step": 15616
    },
    {
      "epoch": 5.6789697559149184e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5204,
      "step": 15648
    },
    {
      "epoch": 5.690583190998589e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.493,
      "step": 15680
    },
    {
      "epoch": 5.70219662608226e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5067,
      "step": 15712
    },
    {
      "epoch": 5.7138100611659304e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5358,
      "step": 15744
    },
    {
      "epoch": 5.725423496249601e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.531,
      "step": 15776
    },
    {
      "epoch": 5.737036931333272e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5381,
      "step": 15808
    },
    {
      "epoch": 5.748650366416942e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 15840
    },
    {
      "epoch": 5.7602638015006124e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5299,
      "step": 15872
    },
    {
      "epoch": 5.771877236584283e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5198,
      "step": 15904
    },
    {
      "epoch": 5.783490671667954e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5195,
      "step": 15936
    },
    {
      "epoch": 5.7951041067516244e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5142,
      "step": 15968
    },
    {
      "epoch": 5.806717541835295e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5316,
      "step": 16000
    },
    {
      "epoch": 5.818330976918966e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5269,
      "step": 16032
    },
    {
      "epoch": 5.8299444120026364e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.526,
      "step": 16064
    },
    {
      "epoch": 5.8415578470863064e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.516,
      "step": 16096
    },
    {
      "epoch": 5.853171282169977e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5215,
      "step": 16128
    },
    {
      "epoch": 5.864784717253648e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5166,
      "step": 16160
    },
    {
      "epoch": 5.8763981523373184e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5271,
      "step": 16192
    },
    {
      "epoch": 5.888011587420989e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5176,
      "step": 16224
    },
    {
      "epoch": 5.89962502250466e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5228,
      "step": 16256
    },
    {
      "epoch": 5.9112384575883304e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5062,
      "step": 16288
    },
    {
      "epoch": 5.922851892672001e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5314,
      "step": 16320
    },
    {
      "epoch": 5.934465327755672e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5244,
      "step": 16352
    },
    {
      "epoch": 5.946078762839342e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5368,
      "step": 16384
    },
    {
      "epoch": 5.9576921979230124e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5323,
      "step": 16416
    },
    {
      "epoch": 5.969305633006683e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5423,
      "step": 16448
    },
    {
      "epoch": 5.980919068090354e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 16480
    },
    {
      "epoch": 5.9925325031740244e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5323,
      "step": 16512
    },
    {
      "epoch": 6.004145938257695e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.498,
      "step": 16544
    },
    {
      "epoch": 6.015759373341366e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5134,
      "step": 16576
    },
    {
      "epoch": 6.0273728084250364e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5214,
      "step": 16608
    },
    {
      "epoch": 6.0389862435087064e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5408,
      "step": 16640
    },
    {
      "epoch": 6.050599678592377e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5299,
      "step": 16672
    },
    {
      "epoch": 6.062213113676048e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5137,
      "step": 16704
    },
    {
      "epoch": 6.0738265487597184e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5259,
      "step": 16736
    },
    {
      "epoch": 6.085439983843389e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.523,
      "step": 16768
    },
    {
      "epoch": 6.09705341892706e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5239,
      "step": 16800
    },
    {
      "epoch": 6.10866685401073e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5239,
      "step": 16832
    },
    {
      "epoch": 6.120280289094401e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5103,
      "step": 16864
    },
    {
      "epoch": 6.131893724178071e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5325,
      "step": 16896
    },
    {
      "epoch": 6.143507159261742e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5311,
      "step": 16928
    },
    {
      "epoch": 6.155120594345412e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5277,
      "step": 16960
    },
    {
      "epoch": 6.166734029429084e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.513,
      "step": 16992
    },
    {
      "epoch": 6.178347464512754e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5103,
      "step": 17024
    },
    {
      "epoch": 6.189960899596424e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.531,
      "step": 17056
    },
    {
      "epoch": 6.201574334680095e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5319,
      "step": 17088
    },
    {
      "epoch": 6.213187769763765e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.524,
      "step": 17120
    },
    {
      "epoch": 6.224801204847436e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5157,
      "step": 17152
    },
    {
      "epoch": 6.236414639931106e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5145,
      "step": 17184
    },
    {
      "epoch": 6.248028075014778e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5322,
      "step": 17216
    },
    {
      "epoch": 6.259641510098448e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5424,
      "step": 17248
    },
    {
      "epoch": 6.271254945182119e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5346,
      "step": 17280
    },
    {
      "epoch": 6.282868380265789e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5154,
      "step": 17312
    },
    {
      "epoch": 6.294481815349459e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5307,
      "step": 17344
    },
    {
      "epoch": 6.30609525043313e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5258,
      "step": 17376
    },
    {
      "epoch": 6.3177086855168e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5079,
      "step": 17408
    },
    {
      "epoch": 6.329322120600472e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5162,
      "step": 17440
    },
    {
      "epoch": 6.340935555684142e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4966,
      "step": 17472
    },
    {
      "epoch": 6.352548990767813e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.537,
      "step": 17504
    },
    {
      "epoch": 6.364162425851483e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5306,
      "step": 17536
    },
    {
      "epoch": 6.375775860935154e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5315,
      "step": 17568
    },
    {
      "epoch": 6.387389296018824e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.512,
      "step": 17600
    },
    {
      "epoch": 6.399002731102494e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.515,
      "step": 17632
    },
    {
      "epoch": 6.410616166186166e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5254,
      "step": 17664
    },
    {
      "epoch": 6.422229601269836e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5278,
      "step": 17696
    },
    {
      "epoch": 6.433843036353507e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5255,
      "step": 17728
    },
    {
      "epoch": 6.445456471437177e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 17760
    },
    {
      "epoch": 6.457069906520848e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5274,
      "step": 17792
    },
    {
      "epoch": 6.468683341604518e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.539,
      "step": 17824
    },
    {
      "epoch": 6.48029677668819e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5245,
      "step": 17856
    },
    {
      "epoch": 6.49191021177186e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5217,
      "step": 17888
    },
    {
      "epoch": 6.50352364685553e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4999,
      "step": 17920
    },
    {
      "epoch": 6.515137081939201e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5258,
      "step": 17952
    },
    {
      "epoch": 6.526750517022871e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5329,
      "step": 17984
    },
    {
      "epoch": 6.538363952106542e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5185,
      "step": 18016
    },
    {
      "epoch": 6.549977387190212e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5103,
      "step": 18048
    },
    {
      "epoch": 6.561590822273884e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5072,
      "step": 18080
    },
    {
      "epoch": 6.573204257357554e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.544,
      "step": 18112
    },
    {
      "epoch": 6.584817692441225e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5493,
      "step": 18144
    },
    {
      "epoch": 6.596431127524895e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5293,
      "step": 18176
    },
    {
      "epoch": 6.608044562608565e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5152,
      "step": 18208
    },
    {
      "epoch": 6.619657997692236e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5288,
      "step": 18240
    },
    {
      "epoch": 6.631271432775906e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5219,
      "step": 18272
    },
    {
      "epoch": 6.642884867859578e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5247,
      "step": 18304
    },
    {
      "epoch": 6.654498302943248e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5382,
      "step": 18336
    },
    {
      "epoch": 6.666111738026919e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4962,
      "step": 18368
    },
    {
      "epoch": 6.677725173110589e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.525,
      "step": 18400
    },
    {
      "epoch": 6.689338608194259e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5388,
      "step": 18432
    },
    {
      "epoch": 6.70095204327793e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5179,
      "step": 18464
    },
    {
      "epoch": 6.7125654783616e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5226,
      "step": 18496
    },
    {
      "epoch": 6.724178913445272e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.498,
      "step": 18528
    },
    {
      "epoch": 6.735792348528942e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5284,
      "step": 18560
    },
    {
      "epoch": 6.747405783612613e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5339,
      "step": 18592
    },
    {
      "epoch": 6.759019218696283e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5178,
      "step": 18624
    },
    {
      "epoch": 6.770632653779954e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.522,
      "step": 18656
    },
    {
      "epoch": 6.782246088863624e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 18688
    },
    {
      "epoch": 6.793859523947294e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5352,
      "step": 18720
    },
    {
      "epoch": 6.805472959030966e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.531,
      "step": 18752
    },
    {
      "epoch": 6.817086394114636e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5162,
      "step": 18784
    },
    {
      "epoch": 6.828699829198307e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.505,
      "step": 18816
    },
    {
      "epoch": 6.840313264281977e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.521,
      "step": 18848
    },
    {
      "epoch": 6.851926699365649e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.539,
      "step": 18880
    },
    {
      "epoch": 6.863540134449318e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.515,
      "step": 18912
    },
    {
      "epoch": 6.87515356953299e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5262,
      "step": 18944
    },
    {
      "epoch": 6.88676700461666e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5157,
      "step": 18976
    },
    {
      "epoch": 6.89838043970033e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5428,
      "step": 19008
    },
    {
      "epoch": 6.909993874784001e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5335,
      "step": 19040
    },
    {
      "epoch": 6.921607309867671e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5207,
      "step": 19072
    },
    {
      "epoch": 6.933220744951343e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5295,
      "step": 19104
    },
    {
      "epoch": 6.944834180035012e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5103,
      "step": 19136
    },
    {
      "epoch": 6.956447615118684e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 19168
    },
    {
      "epoch": 6.968061050202354e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5323,
      "step": 19200
    },
    {
      "epoch": 6.979674485286025e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5123,
      "step": 19232
    },
    {
      "epoch": 6.991287920369695e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5058,
      "step": 19264
    },
    {
      "epoch": 7.002901355453365e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5255,
      "step": 19296
    },
    {
      "epoch": 7.014514790537037e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5267,
      "step": 19328
    },
    {
      "epoch": 7.026128225620706e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5232,
      "step": 19360
    },
    {
      "epoch": 7.037741660704378e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5112,
      "step": 19392
    },
    {
      "epoch": 7.049355095788048e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5208,
      "step": 19424
    },
    {
      "epoch": 7.060968530871719e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5262,
      "step": 19456
    },
    {
      "epoch": 7.072581965955389e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5357,
      "step": 19488
    },
    {
      "epoch": 7.08419540103906e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5285,
      "step": 19520
    },
    {
      "epoch": 7.09580883612273e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5233,
      "step": 19552
    },
    {
      "epoch": 7.1074222712064e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5362,
      "step": 19584
    },
    {
      "epoch": 7.119035706290072e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5275,
      "step": 19616
    },
    {
      "epoch": 7.130649141373742e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.527,
      "step": 19648
    },
    {
      "epoch": 7.142262576457413e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.518,
      "step": 19680
    },
    {
      "epoch": 7.153876011541083e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5013,
      "step": 19712
    },
    {
      "epoch": 7.165489446624755e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5244,
      "step": 19744
    },
    {
      "epoch": 7.177102881708425e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5147,
      "step": 19776
    },
    {
      "epoch": 7.188716316792095e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.519,
      "step": 19808
    },
    {
      "epoch": 7.200329751875766e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5274,
      "step": 19840
    },
    {
      "epoch": 7.211943186959436e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5335,
      "step": 19872
    },
    {
      "epoch": 7.223556622043107e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.525,
      "step": 19904
    },
    {
      "epoch": 7.235170057126777e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 19936
    },
    {
      "epoch": 7.246783492210449e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5208,
      "step": 19968
    },
    {
      "epoch": 7.258396927294119e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5347,
      "step": 20000
    },
    {
      "epoch": 7.27001036237779e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5342,
      "step": 20032
    },
    {
      "epoch": 7.28162379746146e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5287,
      "step": 20064
    },
    {
      "epoch": 7.29323723254513e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5282,
      "step": 20096
    },
    {
      "epoch": 7.304850667628801e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5128,
      "step": 20128
    },
    {
      "epoch": 7.316464102712471e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5062,
      "step": 20160
    },
    {
      "epoch": 7.328077537796143e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5326,
      "step": 20192
    },
    {
      "epoch": 7.339690972879813e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5157,
      "step": 20224
    },
    {
      "epoch": 7.351304407963484e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.513,
      "step": 20256
    },
    {
      "epoch": 7.362917843047154e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5228,
      "step": 20288
    },
    {
      "epoch": 7.374531278130825e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5262,
      "step": 20320
    },
    {
      "epoch": 7.386144713214495e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5298,
      "step": 20352
    },
    {
      "epoch": 7.397758148298165e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5162,
      "step": 20384
    },
    {
      "epoch": 7.409371583381837e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5166,
      "step": 20416
    },
    {
      "epoch": 7.420985018465507e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5349,
      "step": 20448
    },
    {
      "epoch": 7.432598453549178e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 20480
    },
    {
      "epoch": 7.444211888632848e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5267,
      "step": 20512
    },
    {
      "epoch": 7.455825323716519e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5174,
      "step": 20544
    },
    {
      "epoch": 7.467438758800189e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5186,
      "step": 20576
    },
    {
      "epoch": 7.47905219388386e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.516,
      "step": 20608
    },
    {
      "epoch": 7.49066562896753e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5276,
      "step": 20640
    },
    {
      "epoch": 7.5022790640512e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5217,
      "step": 20672
    },
    {
      "epoch": 7.513892499134872e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5239,
      "step": 20704
    },
    {
      "epoch": 7.525505934218542e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.526,
      "step": 20736
    },
    {
      "epoch": 7.537119369302213e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.537,
      "step": 20768
    },
    {
      "epoch": 7.548732804385883e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5337,
      "step": 20800
    },
    {
      "epoch": 7.560346239469555e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5072,
      "step": 20832
    },
    {
      "epoch": 7.571959674553225e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5283,
      "step": 20864
    },
    {
      "epoch": 7.583573109636895e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5473,
      "step": 20896
    },
    {
      "epoch": 7.595186544720566e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.537,
      "step": 20928
    },
    {
      "epoch": 7.606799979804236e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5222,
      "step": 20960
    },
    {
      "epoch": 7.618413414887907e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4991,
      "step": 20992
    },
    {
      "epoch": 7.630026849971577e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5098,
      "step": 21024
    },
    {
      "epoch": 7.641640285055249e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5071,
      "step": 21056
    },
    {
      "epoch": 7.653253720138919e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5322,
      "step": 21088
    },
    {
      "epoch": 7.66486715522259e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5161,
      "step": 21120
    },
    {
      "epoch": 7.67648059030626e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5106,
      "step": 21152
    },
    {
      "epoch": 7.68809402538993e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5309,
      "step": 21184
    },
    {
      "epoch": 7.699707460473601e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5327,
      "step": 21216
    },
    {
      "epoch": 7.711320895557271e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5326,
      "step": 21248
    },
    {
      "epoch": 7.722934330640943e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5304,
      "step": 21280
    },
    {
      "epoch": 7.734547765724613e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5065,
      "step": 21312
    },
    {
      "epoch": 7.746161200808284e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5539,
      "step": 21344
    },
    {
      "epoch": 7.757774635891954e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5358,
      "step": 21376
    },
    {
      "epoch": 7.769388070975625e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.526,
      "step": 21408
    },
    {
      "epoch": 7.781001506059295e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5165,
      "step": 21440
    },
    {
      "epoch": 7.792614941142965e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5052,
      "step": 21472
    },
    {
      "epoch": 7.804228376226637e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5159,
      "step": 21504
    },
    {
      "epoch": 7.815841811310307e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5349,
      "step": 21536
    },
    {
      "epoch": 7.827455246393978e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5212,
      "step": 21568
    },
    {
      "epoch": 7.839068681477648e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5085,
      "step": 21600
    },
    {
      "epoch": 7.850682116561319e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5098,
      "step": 21632
    },
    {
      "epoch": 7.862295551644989e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5375,
      "step": 21664
    },
    {
      "epoch": 7.87390898672866e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5215,
      "step": 21696
    },
    {
      "epoch": 7.88552242181233e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5338,
      "step": 21728
    },
    {
      "epoch": 7.897135856896e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5212,
      "step": 21760
    },
    {
      "epoch": 7.908749291979672e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5536,
      "step": 21792
    },
    {
      "epoch": 7.920362727063342e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5348,
      "step": 21824
    },
    {
      "epoch": 7.931976162147013e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5255,
      "step": 21856
    },
    {
      "epoch": 7.943589597230683e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5077,
      "step": 21888
    },
    {
      "epoch": 7.955203032314355e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4897,
      "step": 21920
    },
    {
      "epoch": 7.966816467398025e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5293,
      "step": 21952
    },
    {
      "epoch": 7.978429902481696e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5216,
      "step": 21984
    },
    {
      "epoch": 7.990043337565366e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5266,
      "step": 22016
    },
    {
      "epoch": 8.001656772649036e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5161,
      "step": 22048
    },
    {
      "epoch": 8.013270207732707e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5111,
      "step": 22080
    },
    {
      "epoch": 8.024883642816377e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5363,
      "step": 22112
    },
    {
      "epoch": 8.036497077900049e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.527,
      "step": 22144
    },
    {
      "epoch": 8.048110512983719e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5201,
      "step": 22176
    },
    {
      "epoch": 8.05972394806739e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5087,
      "step": 22208
    },
    {
      "epoch": 8.07133738315106e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5393,
      "step": 22240
    },
    {
      "epoch": 8.08295081823473e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.539,
      "step": 22272
    },
    {
      "epoch": 8.094564253318401e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5342,
      "step": 22304
    },
    {
      "epoch": 8.106177688402071e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5157,
      "step": 22336
    },
    {
      "epoch": 8.117791123485743e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4952,
      "step": 22368
    },
    {
      "epoch": 8.129404558569413e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5391,
      "step": 22400
    },
    {
      "epoch": 8.141017993653084e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.535,
      "step": 22432
    },
    {
      "epoch": 8.152631428736754e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5228,
      "step": 22464
    },
    {
      "epoch": 8.164244863820425e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4995,
      "step": 22496
    },
    {
      "epoch": 8.175858298904095e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5205,
      "step": 22528
    },
    {
      "epoch": 8.187471733987765e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5284,
      "step": 22560
    },
    {
      "epoch": 8.199085169071437e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5297,
      "step": 22592
    },
    {
      "epoch": 8.210698604155107e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5458,
      "step": 22624
    },
    {
      "epoch": 8.222312039238778e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 22656
    },
    {
      "epoch": 8.233925474322448e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5259,
      "step": 22688
    },
    {
      "epoch": 8.245538909406119e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5353,
      "step": 22720
    },
    {
      "epoch": 8.257152344489789e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5152,
      "step": 22752
    },
    {
      "epoch": 8.26876577957346e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5045,
      "step": 22784
    },
    {
      "epoch": 8.28037921465713e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4878,
      "step": 22816
    },
    {
      "epoch": 8.2919926497408e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5184,
      "step": 22848
    },
    {
      "epoch": 8.303606084824472e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5378,
      "step": 22880
    },
    {
      "epoch": 8.315219519908142e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5244,
      "step": 22912
    },
    {
      "epoch": 8.326832954991813e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5232,
      "step": 22944
    },
    {
      "epoch": 8.338446390075483e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5157,
      "step": 22976
    },
    {
      "epoch": 8.350059825159155e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.535,
      "step": 23008
    },
    {
      "epoch": 8.361673260242825e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5392,
      "step": 23040
    },
    {
      "epoch": 8.373286695326496e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5198,
      "step": 23072
    },
    {
      "epoch": 8.384900130410166e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5238,
      "step": 23104
    },
    {
      "epoch": 8.396513565493836e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.533,
      "step": 23136
    },
    {
      "epoch": 8.408127000577507e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.531,
      "step": 23168
    },
    {
      "epoch": 8.419740435661177e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.531,
      "step": 23200
    },
    {
      "epoch": 8.431353870744849e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5172,
      "step": 23232
    },
    {
      "epoch": 8.442967305828519e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5098,
      "step": 23264
    },
    {
      "epoch": 8.45458074091219e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5257,
      "step": 23296
    },
    {
      "epoch": 8.46619417599586e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5237,
      "step": 23328
    },
    {
      "epoch": 8.477807611079531e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5002,
      "step": 23360
    },
    {
      "epoch": 8.489421046163201e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5151,
      "step": 23392
    },
    {
      "epoch": 8.501034481246871e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5056,
      "step": 23424
    },
    {
      "epoch": 8.512647916330543e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5411,
      "step": 23456
    },
    {
      "epoch": 8.524261351414213e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5482,
      "step": 23488
    },
    {
      "epoch": 8.535874786497884e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5403,
      "step": 23520
    },
    {
      "epoch": 8.547488221581554e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5249,
      "step": 23552
    },
    {
      "epoch": 8.559101656665225e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5387,
      "step": 23584
    },
    {
      "epoch": 8.570715091748895e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.523,
      "step": 23616
    },
    {
      "epoch": 8.582328526832565e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5158,
      "step": 23648
    },
    {
      "epoch": 8.593941961916237e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4967,
      "step": 23680
    },
    {
      "epoch": 8.605555396999907e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5114,
      "step": 23712
    },
    {
      "epoch": 8.617168832083578e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5191,
      "step": 23744
    },
    {
      "epoch": 8.628782267167248e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5285,
      "step": 23776
    },
    {
      "epoch": 8.640395702250919e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5267,
      "step": 23808
    },
    {
      "epoch": 8.652009137334589e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5188,
      "step": 23840
    },
    {
      "epoch": 8.66362257241826e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5235,
      "step": 23872
    },
    {
      "epoch": 8.67523600750193e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5212,
      "step": 23904
    },
    {
      "epoch": 8.6868494425856e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5245,
      "step": 23936
    },
    {
      "epoch": 8.698462877669272e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5273,
      "step": 23968
    },
    {
      "epoch": 8.710076312752942e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5283,
      "step": 24000
    },
    {
      "epoch": 8.721689747836613e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5354,
      "step": 24032
    },
    {
      "epoch": 8.733303182920283e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5236,
      "step": 24064
    },
    {
      "epoch": 8.744916618003955e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5296,
      "step": 24096
    },
    {
      "epoch": 8.756530053087625e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5267,
      "step": 24128
    },
    {
      "epoch": 8.768143488171296e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.522,
      "step": 24160
    },
    {
      "epoch": 8.779756923254966e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.52,
      "step": 24192
    },
    {
      "epoch": 8.791370358338636e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5162,
      "step": 24224
    },
    {
      "epoch": 8.802983793422307e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5061,
      "step": 24256
    },
    {
      "epoch": 8.814597228505977e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5203,
      "step": 24288
    },
    {
      "epoch": 8.826210663589649e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5339,
      "step": 24320
    },
    {
      "epoch": 8.837824098673319e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5352,
      "step": 24352
    },
    {
      "epoch": 8.84943753375699e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5394,
      "step": 24384
    },
    {
      "epoch": 8.86105096884066e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.529,
      "step": 24416
    },
    {
      "epoch": 8.872664403924331e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5379,
      "step": 24448
    },
    {
      "epoch": 8.884277839008001e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5232,
      "step": 24480
    },
    {
      "epoch": 8.895891274091671e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5073,
      "step": 24512
    },
    {
      "epoch": 8.907504709175343e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5028,
      "step": 24544
    },
    {
      "epoch": 8.919118144259013e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.511,
      "step": 24576
    },
    {
      "epoch": 8.930731579342684e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5137,
      "step": 24608
    },
    {
      "epoch": 8.942345014426354e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5336,
      "step": 24640
    },
    {
      "epoch": 8.953958449510025e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5207,
      "step": 24672
    },
    {
      "epoch": 8.965571884593695e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 24704
    },
    {
      "epoch": 8.977185319677367e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5259,
      "step": 24736
    },
    {
      "epoch": 8.988798754761037e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5252,
      "step": 24768
    },
    {
      "epoch": 9.000412189844707e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5326,
      "step": 24800
    },
    {
      "epoch": 9.012025624928378e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5138,
      "step": 24832
    },
    {
      "epoch": 9.023639060012048e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5285,
      "step": 24864
    },
    {
      "epoch": 9.03525249509572e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5469,
      "step": 24896
    },
    {
      "epoch": 9.046865930179389e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5266,
      "step": 24928
    },
    {
      "epoch": 9.05847936526306e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5246,
      "step": 24960
    },
    {
      "epoch": 9.07009280034673e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5184,
      "step": 24992
    },
    {
      "epoch": 9.0817062354304e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5302,
      "step": 25024
    },
    {
      "epoch": 9.093319670514072e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5055,
      "step": 25056
    },
    {
      "epoch": 9.104933105597742e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5203,
      "step": 25088
    },
    {
      "epoch": 9.116546540681413e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5063,
      "step": 25120
    },
    {
      "epoch": 9.128159975765083e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5077,
      "step": 25152
    },
    {
      "epoch": 9.139773410848755e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5365,
      "step": 25184
    },
    {
      "epoch": 9.151386845932425e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5426,
      "step": 25216
    },
    {
      "epoch": 9.163000281016096e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5347,
      "step": 25248
    },
    {
      "epoch": 9.174613716099766e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5229,
      "step": 25280
    },
    {
      "epoch": 9.186227151183436e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5284,
      "step": 25312
    },
    {
      "epoch": 9.197840586267107e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5346,
      "step": 25344
    },
    {
      "epoch": 9.209454021350777e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5306,
      "step": 25376
    },
    {
      "epoch": 9.221067456434449e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5091,
      "step": 25408
    },
    {
      "epoch": 9.232680891518119e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4926,
      "step": 25440
    },
    {
      "epoch": 9.24429432660179e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5283,
      "step": 25472
    },
    {
      "epoch": 9.25590776168546e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5163,
      "step": 25504
    },
    {
      "epoch": 9.267521196769131e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5261,
      "step": 25536
    },
    {
      "epoch": 9.279134631852801e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5288,
      "step": 25568
    },
    {
      "epoch": 9.290748066936471e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5099,
      "step": 25600
    },
    {
      "epoch": 9.302361502020143e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5316,
      "step": 25632
    },
    {
      "epoch": 9.313974937103813e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5246,
      "step": 25664
    },
    {
      "epoch": 9.325588372187484e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5232,
      "step": 25696
    },
    {
      "epoch": 9.337201807271154e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5218,
      "step": 25728
    },
    {
      "epoch": 9.348815242354825e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5212,
      "step": 25760
    },
    {
      "epoch": 9.360428677438495e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5368,
      "step": 25792
    },
    {
      "epoch": 9.372042112522167e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5444,
      "step": 25824
    },
    {
      "epoch": 9.383655547605837e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5344,
      "step": 25856
    },
    {
      "epoch": 9.395268982689507e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.517,
      "step": 25888
    },
    {
      "epoch": 9.406882417773178e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5143,
      "step": 25920
    },
    {
      "epoch": 9.418495852856848e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5149,
      "step": 25952
    },
    {
      "epoch": 9.43010928794052e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.527,
      "step": 25984
    },
    {
      "epoch": 9.44172272302419e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5108,
      "step": 26016
    },
    {
      "epoch": 9.453336158107861e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5119,
      "step": 26048
    },
    {
      "epoch": 9.464949593191531e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5469,
      "step": 26080
    },
    {
      "epoch": 9.476563028275202e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5334,
      "step": 26112
    },
    {
      "epoch": 9.488176463358872e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5343,
      "step": 26144
    },
    {
      "epoch": 9.499789898442542e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5253,
      "step": 26176
    },
    {
      "epoch": 9.511403333526213e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.504,
      "step": 26208
    },
    {
      "epoch": 9.523016768609883e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5444,
      "step": 26240
    },
    {
      "epoch": 9.534630203693555e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.505,
      "step": 26272
    },
    {
      "epoch": 9.546243638777225e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5068,
      "step": 26304
    },
    {
      "epoch": 9.557857073860896e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5192,
      "step": 26336
    },
    {
      "epoch": 9.569470508944566e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.505,
      "step": 26368
    },
    {
      "epoch": 9.581083944028236e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5262,
      "step": 26400
    },
    {
      "epoch": 9.592697379111907e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5349,
      "step": 26432
    },
    {
      "epoch": 9.604310814195577e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5255,
      "step": 26464
    },
    {
      "epoch": 9.615924249279249e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5133,
      "step": 26496
    },
    {
      "epoch": 9.627537684362919e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 26528
    },
    {
      "epoch": 9.63915111944659e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5318,
      "step": 26560
    },
    {
      "epoch": 9.65076455453026e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5315,
      "step": 26592
    },
    {
      "epoch": 9.662377989613931e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5158,
      "step": 26624
    },
    {
      "epoch": 9.673991424697601e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5182,
      "step": 26656
    },
    {
      "epoch": 9.685604859781271e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5458,
      "step": 26688
    },
    {
      "epoch": 9.697218294864943e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5377,
      "step": 26720
    },
    {
      "epoch": 9.708831729948613e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5296,
      "step": 26752
    },
    {
      "epoch": 9.720445165032284e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5101,
      "step": 26784
    },
    {
      "epoch": 9.732058600115954e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5039,
      "step": 26816
    },
    {
      "epoch": 9.743672035199625e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5093,
      "step": 26848
    },
    {
      "epoch": 9.755285470283295e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5232,
      "step": 26880
    },
    {
      "epoch": 9.766898905366967e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5246,
      "step": 26912
    },
    {
      "epoch": 9.778512340450637e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5325,
      "step": 26944
    },
    {
      "epoch": 9.790125775534307e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5161,
      "step": 26976
    },
    {
      "epoch": 9.801739210617978e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5441,
      "step": 27008
    },
    {
      "epoch": 9.813352645701648e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5312,
      "step": 27040
    },
    {
      "epoch": 9.82496608078532e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5231,
      "step": 27072
    },
    {
      "epoch": 9.83657951586899e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.513,
      "step": 27104
    },
    {
      "epoch": 9.848192950952661e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5233,
      "step": 27136
    },
    {
      "epoch": 9.859806386036331e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5354,
      "step": 27168
    },
    {
      "epoch": 9.871419821120002e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5179,
      "step": 27200
    },
    {
      "epoch": 9.883033256203672e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5263,
      "step": 27232
    },
    {
      "epoch": 9.894646691287342e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5137,
      "step": 27264
    },
    {
      "epoch": 9.906260126371013e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 27296
    },
    {
      "epoch": 9.917873561454683e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5343,
      "step": 27328
    },
    {
      "epoch": 9.929486996538355e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.524,
      "step": 27360
    },
    {
      "epoch": 9.941100431622025e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5251,
      "step": 27392
    },
    {
      "epoch": 9.952713866705696e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5055,
      "step": 27424
    },
    {
      "epoch": 9.964327301789366e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5273,
      "step": 27456
    },
    {
      "epoch": 9.975940736873037e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5237,
      "step": 27488
    },
    {
      "epoch": 9.987554171956707e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5237,
      "step": 27520
    },
    {
      "epoch": 9.999167607040377e-05,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5331,
      "step": 27552
    },
    {
      "epoch": 0.00010010781042124049,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5428,
      "step": 27584
    },
    {
      "epoch": 0.00010022394477207719,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5351,
      "step": 27616
    },
    {
      "epoch": 0.0001003400791229139,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5256,
      "step": 27648
    },
    {
      "epoch": 0.0001004562134737506,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5185,
      "step": 27680
    },
    {
      "epoch": 0.00010057234782458731,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.492,
      "step": 27712
    },
    {
      "epoch": 0.00010068848217542401,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5129,
      "step": 27744
    },
    {
      "epoch": 0.00010080461652626071,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5391,
      "step": 27776
    },
    {
      "epoch": 0.00010092075087709743,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5268,
      "step": 27808
    },
    {
      "epoch": 0.00010103688522793413,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5365,
      "step": 27840
    },
    {
      "epoch": 0.00010115301957877084,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5185,
      "step": 27872
    },
    {
      "epoch": 0.00010126915392960754,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5267,
      "step": 27904
    },
    {
      "epoch": 0.00010138528828044425,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.523,
      "step": 27936
    },
    {
      "epoch": 0.00010150142263128095,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5239,
      "step": 27968
    },
    {
      "epoch": 0.00010161755698211767,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5115,
      "step": 28000
    },
    {
      "epoch": 0.00010173369133295437,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5233,
      "step": 28032
    },
    {
      "epoch": 0.00010184982568379107,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5166,
      "step": 28064
    },
    {
      "epoch": 0.00010196596003462778,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5279,
      "step": 28096
    },
    {
      "epoch": 0.00010208209438546448,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.522,
      "step": 28128
    },
    {
      "epoch": 0.0001021982287363012,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5139,
      "step": 28160
    },
    {
      "epoch": 0.0001023143630871379,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5245,
      "step": 28192
    },
    {
      "epoch": 0.00010243049743797461,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5303,
      "step": 28224
    },
    {
      "epoch": 0.00010254663178881131,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5278,
      "step": 28256
    },
    {
      "epoch": 0.00010266276613964802,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5228,
      "step": 28288
    },
    {
      "epoch": 0.00010277890049048472,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 28320
    },
    {
      "epoch": 0.00010289503484132142,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5212,
      "step": 28352
    },
    {
      "epoch": 0.00010301116919215813,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5155,
      "step": 28384
    },
    {
      "epoch": 0.00010312730354299483,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5495,
      "step": 28416
    },
    {
      "epoch": 0.00010324343789383155,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5369,
      "step": 28448
    },
    {
      "epoch": 0.00010335957224466825,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5314,
      "step": 28480
    },
    {
      "epoch": 0.00010347570659550496,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5123,
      "step": 28512
    },
    {
      "epoch": 0.00010359184094634166,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5302,
      "step": 28544
    },
    {
      "epoch": 0.00010370797529717837,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5059,
      "step": 28576
    },
    {
      "epoch": 0.00010382410964801507,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5066,
      "step": 28608
    },
    {
      "epoch": 0.00010394024399885177,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.52,
      "step": 28640
    },
    {
      "epoch": 0.00010405637834968849,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.528,
      "step": 28672
    },
    {
      "epoch": 0.00010417251270052519,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5282,
      "step": 28704
    },
    {
      "epoch": 0.0001042886470513619,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5278,
      "step": 28736
    },
    {
      "epoch": 0.0001044047814021986,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5204,
      "step": 28768
    },
    {
      "epoch": 0.00010452091575303531,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5228,
      "step": 28800
    },
    {
      "epoch": 0.00010463705010387201,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5222,
      "step": 28832
    },
    {
      "epoch": 0.00010475318445470873,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.528,
      "step": 28864
    },
    {
      "epoch": 0.00010486931880554543,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5238,
      "step": 28896
    },
    {
      "epoch": 0.00010498545315638213,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5403,
      "step": 28928
    },
    {
      "epoch": 0.00010510158750721884,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5095,
      "step": 28960
    },
    {
      "epoch": 0.00010521772185805554,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5212,
      "step": 28992
    },
    {
      "epoch": 0.00010533385620889225,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5331,
      "step": 29024
    },
    {
      "epoch": 0.00010544999055972895,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5037,
      "step": 29056
    },
    {
      "epoch": 0.00010556612491056567,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5275,
      "step": 29088
    },
    {
      "epoch": 0.00010568225926140237,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5175,
      "step": 29120
    },
    {
      "epoch": 0.00010579839361223907,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 29152
    },
    {
      "epoch": 0.00010591452796307578,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5228,
      "step": 29184
    },
    {
      "epoch": 0.00010603066231391248,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5083,
      "step": 29216
    },
    {
      "epoch": 0.0001061467966647492,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5198,
      "step": 29248
    },
    {
      "epoch": 0.0001062629310155859,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5239,
      "step": 29280
    },
    {
      "epoch": 0.00010637906536642261,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5451,
      "step": 29312
    },
    {
      "epoch": 0.00010649519971725931,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5314,
      "step": 29344
    },
    {
      "epoch": 0.00010661133406809602,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5355,
      "step": 29376
    },
    {
      "epoch": 0.00010672746841893272,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5122,
      "step": 29408
    },
    {
      "epoch": 0.00010684360276976942,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4986,
      "step": 29440
    },
    {
      "epoch": 0.00010695973712060613,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5123,
      "step": 29472
    },
    {
      "epoch": 0.00010707587147144283,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5237,
      "step": 29504
    },
    {
      "epoch": 0.00010719200582227955,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5085,
      "step": 29536
    },
    {
      "epoch": 0.00010730814017311625,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4856,
      "step": 29568
    },
    {
      "epoch": 0.00010742427452395296,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5101,
      "step": 29600
    },
    {
      "epoch": 0.00010754040887478966,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5028,
      "step": 29632
    },
    {
      "epoch": 0.00010765654322562637,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5162,
      "step": 29664
    },
    {
      "epoch": 0.00010777267757646307,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4978,
      "step": 29696
    },
    {
      "epoch": 0.00010788881192729977,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4932,
      "step": 29728
    },
    {
      "epoch": 0.00010800494627813649,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5178,
      "step": 29760
    },
    {
      "epoch": 0.00010812108062897319,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5061,
      "step": 29792
    },
    {
      "epoch": 0.0001082372149798099,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4944,
      "step": 29824
    },
    {
      "epoch": 0.0001083533493306466,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4888,
      "step": 29856
    },
    {
      "epoch": 0.00010846948368148331,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5149,
      "step": 29888
    },
    {
      "epoch": 0.00010858561803232001,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5275,
      "step": 29920
    },
    {
      "epoch": 0.00010870175238315673,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5514,
      "step": 29952
    },
    {
      "epoch": 0.00010881788673399343,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5479,
      "step": 29984
    },
    {
      "epoch": 0.00010893402108483013,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5448,
      "step": 30016
    },
    {
      "epoch": 0.00010905015543566684,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5383,
      "step": 30048
    },
    {
      "epoch": 0.00010916628978650354,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5469,
      "step": 30080
    },
    {
      "epoch": 0.00010928242413734025,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5555,
      "step": 30112
    },
    {
      "epoch": 0.00010939855848817695,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5345,
      "step": 30144
    },
    {
      "epoch": 0.00010951469283901367,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5286,
      "step": 30176
    },
    {
      "epoch": 0.00010963082718985037,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5121,
      "step": 30208
    },
    {
      "epoch": 0.00010974696154068708,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5014,
      "step": 30240
    },
    {
      "epoch": 0.00010986309589152378,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5021,
      "step": 30272
    },
    {
      "epoch": 0.00010997923024236048,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5001,
      "step": 30304
    },
    {
      "epoch": 0.0001100953645931972,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.506,
      "step": 30336
    },
    {
      "epoch": 0.0001102114989440339,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5105,
      "step": 30368
    },
    {
      "epoch": 0.00011032763329487061,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4994,
      "step": 30400
    },
    {
      "epoch": 0.00011044376764570731,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5159,
      "step": 30432
    },
    {
      "epoch": 0.00011055990199654402,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5095,
      "step": 30464
    },
    {
      "epoch": 0.00011067603634738072,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5014,
      "step": 30496
    },
    {
      "epoch": 0.00011079217069821742,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5189,
      "step": 30528
    },
    {
      "epoch": 0.00011090830504905413,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4998,
      "step": 30560
    },
    {
      "epoch": 0.00011102443939989083,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4961,
      "step": 30592
    },
    {
      "epoch": 0.00011114057375072755,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5081,
      "step": 30624
    },
    {
      "epoch": 0.00011125670810156425,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5132,
      "step": 30656
    },
    {
      "epoch": 0.00011137284245240096,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5393,
      "step": 30688
    },
    {
      "epoch": 0.00011148897680323766,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5379,
      "step": 30720
    },
    {
      "epoch": 0.00011160511115407437,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5439,
      "step": 30752
    },
    {
      "epoch": 0.00011172124550491107,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5466,
      "step": 30784
    },
    {
      "epoch": 0.00011183737985574777,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5382,
      "step": 30816
    },
    {
      "epoch": 0.00011195351420658449,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5526,
      "step": 30848
    },
    {
      "epoch": 0.00011206964855742119,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5443,
      "step": 30880
    },
    {
      "epoch": 0.0001121857829082579,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5428,
      "step": 30912
    },
    {
      "epoch": 0.0001123019172590946,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5006,
      "step": 30944
    },
    {
      "epoch": 0.00011241805160993131,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5082,
      "step": 30976
    },
    {
      "epoch": 0.00011253418596076801,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5074,
      "step": 31008
    },
    {
      "epoch": 0.00011265032031160473,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5109,
      "step": 31040
    },
    {
      "epoch": 0.00011276645466244143,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5072,
      "step": 31072
    },
    {
      "epoch": 0.00011288258901327813,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4944,
      "step": 31104
    },
    {
      "epoch": 0.00011299872336411484,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.49,
      "step": 31136
    },
    {
      "epoch": 0.00011311485771495154,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5046,
      "step": 31168
    },
    {
      "epoch": 0.00011323099206578825,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5313,
      "step": 31200
    },
    {
      "epoch": 0.00011334712641662495,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5197,
      "step": 31232
    },
    {
      "epoch": 0.00011346326076746167,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.509,
      "step": 31264
    },
    {
      "epoch": 0.00011357939511829837,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5117,
      "step": 31296
    },
    {
      "epoch": 0.00011369552946913508,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.502,
      "step": 31328
    },
    {
      "epoch": 0.00011381166381997178,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4963,
      "step": 31360
    },
    {
      "epoch": 0.00011392779817080848,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5137,
      "step": 31392
    },
    {
      "epoch": 0.0001140439325216452,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4924,
      "step": 31424
    },
    {
      "epoch": 0.0001141600668724819,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5415,
      "step": 31456
    },
    {
      "epoch": 0.00011427620122331861,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5416,
      "step": 31488
    },
    {
      "epoch": 0.00011439233557415531,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5505,
      "step": 31520
    },
    {
      "epoch": 0.00011450846992499202,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5441,
      "step": 31552
    },
    {
      "epoch": 0.00011462460427582872,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5305,
      "step": 31584
    },
    {
      "epoch": 0.00011474073862666544,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5407,
      "step": 31616
    },
    {
      "epoch": 0.00011485687297750213,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5512,
      "step": 31648
    },
    {
      "epoch": 0.00011497300732833883,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5326,
      "step": 31680
    },
    {
      "epoch": 0.00011508914167917555,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5099,
      "step": 31712
    },
    {
      "epoch": 0.00011520527603001225,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5048,
      "step": 31744
    },
    {
      "epoch": 0.00011532141038084896,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5154,
      "step": 31776
    },
    {
      "epoch": 0.00011543754473168566,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5076,
      "step": 31808
    },
    {
      "epoch": 0.00011555367908252238,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5125,
      "step": 31840
    },
    {
      "epoch": 0.00011566981343335907,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4997,
      "step": 31872
    },
    {
      "epoch": 0.00011578594778419577,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4955,
      "step": 31904
    },
    {
      "epoch": 0.00011590208213503249,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5151,
      "step": 31936
    },
    {
      "epoch": 0.00011601821648586919,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5218,
      "step": 31968
    },
    {
      "epoch": 0.0001161343508367059,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4951,
      "step": 32000
    },
    {
      "epoch": 0.0001162504851875426,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5051,
      "step": 32032
    },
    {
      "epoch": 0.00011636661953837932,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5168,
      "step": 32064
    },
    {
      "epoch": 0.00011648275388921602,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5177,
      "step": 32096
    },
    {
      "epoch": 0.00011659888824005273,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.499,
      "step": 32128
    },
    {
      "epoch": 0.00011671502259088943,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5144,
      "step": 32160
    },
    {
      "epoch": 0.00011683115694172613,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4975,
      "step": 32192
    },
    {
      "epoch": 0.00011694729129256284,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5416,
      "step": 32224
    },
    {
      "epoch": 0.00011706342564339954,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5461,
      "step": 32256
    },
    {
      "epoch": 0.00011717955999423626,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5368,
      "step": 32288
    },
    {
      "epoch": 0.00011729569434507296,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5455,
      "step": 32320
    },
    {
      "epoch": 0.00011741182869590967,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5414,
      "step": 32352
    },
    {
      "epoch": 0.00011752796304674637,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.551,
      "step": 32384
    },
    {
      "epoch": 0.00011764409739758308,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5474,
      "step": 32416
    },
    {
      "epoch": 0.00011776023174841978,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5197,
      "step": 32448
    },
    {
      "epoch": 0.00011787636609925648,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5007,
      "step": 32480
    },
    {
      "epoch": 0.0001179925004500932,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5025,
      "step": 32512
    },
    {
      "epoch": 0.0001181086348009299,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5174,
      "step": 32544
    },
    {
      "epoch": 0.00011822476915176661,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5123,
      "step": 32576
    },
    {
      "epoch": 0.00011834090350260331,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5037,
      "step": 32608
    },
    {
      "epoch": 0.00011845703785344002,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4904,
      "step": 32640
    },
    {
      "epoch": 0.00011857317220427672,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4954,
      "step": 32672
    },
    {
      "epoch": 0.00011868930655511344,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5286,
      "step": 32704
    },
    {
      "epoch": 0.00011880544090595014,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5137,
      "step": 32736
    },
    {
      "epoch": 0.00011892157525678684,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.502,
      "step": 32768
    },
    {
      "epoch": 0.00011903770960762355,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4868,
      "step": 32800
    },
    {
      "epoch": 0.00011915384395846025,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5152,
      "step": 32832
    },
    {
      "epoch": 0.00011926997830929696,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4934,
      "step": 32864
    },
    {
      "epoch": 0.00011938611266013366,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5053,
      "step": 32896
    },
    {
      "epoch": 0.00011950224701097038,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5246,
      "step": 32928
    },
    {
      "epoch": 0.00011961838136180708,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5274,
      "step": 32960
    },
    {
      "epoch": 0.00011973451571264378,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5557,
      "step": 32992
    },
    {
      "epoch": 0.00011985065006348049,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5544,
      "step": 33024
    },
    {
      "epoch": 0.00011996678441431719,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5486,
      "step": 33056
    },
    {
      "epoch": 0.0001200829187651539,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5197,
      "step": 33088
    },
    {
      "epoch": 0.0001201990531159906,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5393,
      "step": 33120
    },
    {
      "epoch": 0.00012031518746682732,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5418,
      "step": 33152
    },
    {
      "epoch": 0.00012043132181766402,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5453,
      "step": 33184
    },
    {
      "epoch": 0.00012054745616850073,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5154,
      "step": 33216
    },
    {
      "epoch": 0.00012066359051933743,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4926,
      "step": 33248
    },
    {
      "epoch": 0.00012077972487017413,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5115,
      "step": 33280
    },
    {
      "epoch": 0.00012089585922101084,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5062,
      "step": 33312
    },
    {
      "epoch": 0.00012101199357184754,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5055,
      "step": 33344
    },
    {
      "epoch": 0.00012112812792268426,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4947,
      "step": 33376
    },
    {
      "epoch": 0.00012124426227352096,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.482,
      "step": 33408
    },
    {
      "epoch": 0.00012136039662435767,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5192,
      "step": 33440
    },
    {
      "epoch": 0.00012147653097519437,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5282,
      "step": 33472
    },
    {
      "epoch": 0.00012159266532603108,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5101,
      "step": 33504
    },
    {
      "epoch": 0.00012170879967686778,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5089,
      "step": 33536
    },
    {
      "epoch": 0.00012182493402770448,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5059,
      "step": 33568
    },
    {
      "epoch": 0.0001219410683785412,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5282,
      "step": 33600
    },
    {
      "epoch": 0.0001220572027293779,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5019,
      "step": 33632
    },
    {
      "epoch": 0.0001221733370802146,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4945,
      "step": 33664
    },
    {
      "epoch": 0.00012228947143105132,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5148,
      "step": 33696
    },
    {
      "epoch": 0.00012240560578188802,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5241,
      "step": 33728
    },
    {
      "epoch": 0.00012252174013272472,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5538,
      "step": 33760
    },
    {
      "epoch": 0.00012263787448356142,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5603,
      "step": 33792
    },
    {
      "epoch": 0.00012275400883439812,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5478,
      "step": 33824
    },
    {
      "epoch": 0.00012287014318523485,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5366,
      "step": 33856
    },
    {
      "epoch": 0.00012298627753607155,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.547,
      "step": 33888
    },
    {
      "epoch": 0.00012310241188690825,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5363,
      "step": 33920
    },
    {
      "epoch": 0.00012321854623774495,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5191,
      "step": 33952
    },
    {
      "epoch": 0.00012333468058858168,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4997,
      "step": 33984
    },
    {
      "epoch": 0.00012345081493941838,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4962,
      "step": 34016
    },
    {
      "epoch": 0.00012356694929025508,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5166,
      "step": 34048
    },
    {
      "epoch": 0.00012368308364109178,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.514,
      "step": 34080
    },
    {
      "epoch": 0.00012379921799192848,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5122,
      "step": 34112
    },
    {
      "epoch": 0.0001239153523427652,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5026,
      "step": 34144
    },
    {
      "epoch": 0.0001240314866936019,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4914,
      "step": 34176
    },
    {
      "epoch": 0.0001241476210444386,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5142,
      "step": 34208
    },
    {
      "epoch": 0.0001242637553952753,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5183,
      "step": 34240
    },
    {
      "epoch": 0.00012437988974611203,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4982,
      "step": 34272
    },
    {
      "epoch": 0.00012449602409694873,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.521,
      "step": 34304
    },
    {
      "epoch": 0.00012461215844778543,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.502,
      "step": 34336
    },
    {
      "epoch": 0.00012472829279862213,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5161,
      "step": 34368
    },
    {
      "epoch": 0.00012484442714945883,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5016,
      "step": 34400
    },
    {
      "epoch": 0.00012496056150029556,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5164,
      "step": 34432
    },
    {
      "epoch": 0.00012507669585113226,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5119,
      "step": 34464
    },
    {
      "epoch": 0.00012519283020196896,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5432,
      "step": 34496
    },
    {
      "epoch": 0.00012530896455280566,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5288,
      "step": 34528
    },
    {
      "epoch": 0.00012542509890364238,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.542,
      "step": 34560
    },
    {
      "epoch": 0.00012554123325447908,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.533,
      "step": 34592
    },
    {
      "epoch": 0.00012565736760531578,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5527,
      "step": 34624
    },
    {
      "epoch": 0.00012577350195615248,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5628,
      "step": 34656
    },
    {
      "epoch": 0.00012588963630698918,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5559,
      "step": 34688
    },
    {
      "epoch": 0.0001260057706578259,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5264,
      "step": 34720
    },
    {
      "epoch": 0.0001261219050086626,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5168,
      "step": 34752
    },
    {
      "epoch": 0.0001262380393594993,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4925,
      "step": 34784
    },
    {
      "epoch": 0.000126354173710336,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.513,
      "step": 34816
    },
    {
      "epoch": 0.00012647030806117274,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4843,
      "step": 34848
    },
    {
      "epoch": 0.00012658644241200944,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5045,
      "step": 34880
    },
    {
      "epoch": 0.00012670257676284614,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5055,
      "step": 34912
    },
    {
      "epoch": 0.00012681871111368284,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4898,
      "step": 34944
    },
    {
      "epoch": 0.00012693484546451954,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5248,
      "step": 34976
    },
    {
      "epoch": 0.00012705097981535626,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5123,
      "step": 35008
    },
    {
      "epoch": 0.00012716711416619296,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5092,
      "step": 35040
    },
    {
      "epoch": 0.00012728324851702966,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5058,
      "step": 35072
    },
    {
      "epoch": 0.00012739938286786636,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.495,
      "step": 35104
    },
    {
      "epoch": 0.0001275155172187031,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4992,
      "step": 35136
    },
    {
      "epoch": 0.0001276316515695398,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4954,
      "step": 35168
    },
    {
      "epoch": 0.0001277477859203765,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.527,
      "step": 35200
    },
    {
      "epoch": 0.0001278639202712132,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5184,
      "step": 35232
    },
    {
      "epoch": 0.0001279800546220499,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5511,
      "step": 35264
    },
    {
      "epoch": 0.00012809618897288662,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5504,
      "step": 35296
    },
    {
      "epoch": 0.00012821232332372332,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5526,
      "step": 35328
    },
    {
      "epoch": 0.00012832845767456002,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5428,
      "step": 35360
    },
    {
      "epoch": 0.00012844459202539672,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5295,
      "step": 35392
    },
    {
      "epoch": 0.00012856072637623344,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5402,
      "step": 35424
    },
    {
      "epoch": 0.00012867686072707014,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5318,
      "step": 35456
    },
    {
      "epoch": 0.00012879299507790684,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5318,
      "step": 35488
    },
    {
      "epoch": 0.00012890912942874354,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5259,
      "step": 35520
    },
    {
      "epoch": 0.00012902526377958024,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5005,
      "step": 35552
    },
    {
      "epoch": 0.00012914139813041697,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5147,
      "step": 35584
    },
    {
      "epoch": 0.00012925753248125367,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5136,
      "step": 35616
    },
    {
      "epoch": 0.00012937366683209037,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5004,
      "step": 35648
    },
    {
      "epoch": 0.00012948980118292707,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4881,
      "step": 35680
    },
    {
      "epoch": 0.0001296059355337638,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4724,
      "step": 35712
    },
    {
      "epoch": 0.0001297220698846005,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5105,
      "step": 35744
    },
    {
      "epoch": 0.0001298382042354372,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.52,
      "step": 35776
    },
    {
      "epoch": 0.0001299543385862739,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5194,
      "step": 35808
    },
    {
      "epoch": 0.0001300704729371106,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5087,
      "step": 35840
    },
    {
      "epoch": 0.00013018660728794732,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5095,
      "step": 35872
    },
    {
      "epoch": 0.00013030274163878402,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5045,
      "step": 35904
    },
    {
      "epoch": 0.00013041887598962072,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5093,
      "step": 35936
    },
    {
      "epoch": 0.00013053501034045742,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5234,
      "step": 35968
    },
    {
      "epoch": 0.00013065114469129415,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5199,
      "step": 36000
    },
    {
      "epoch": 0.00013076727904213085,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5327,
      "step": 36032
    },
    {
      "epoch": 0.00013088341339296755,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5537,
      "step": 36064
    },
    {
      "epoch": 0.00013099954774380425,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5507,
      "step": 36096
    },
    {
      "epoch": 0.00013111568209464095,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5472,
      "step": 36128
    },
    {
      "epoch": 0.00013123181644547768,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5486,
      "step": 36160
    },
    {
      "epoch": 0.00013134795079631438,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5507,
      "step": 36192
    },
    {
      "epoch": 0.00013146408514715108,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5382,
      "step": 36224
    },
    {
      "epoch": 0.00013158021949798778,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5064,
      "step": 36256
    },
    {
      "epoch": 0.0001316963538488245,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4837,
      "step": 36288
    },
    {
      "epoch": 0.0001318124881996612,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4933,
      "step": 36320
    },
    {
      "epoch": 0.0001319286225504979,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.517,
      "step": 36352
    },
    {
      "epoch": 0.0001320447569013346,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.525,
      "step": 36384
    },
    {
      "epoch": 0.0001321608912521713,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5144,
      "step": 36416
    },
    {
      "epoch": 0.00013227702560300803,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4999,
      "step": 36448
    },
    {
      "epoch": 0.00013239315995384473,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5021,
      "step": 36480
    },
    {
      "epoch": 0.00013250929430468143,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5178,
      "step": 36512
    },
    {
      "epoch": 0.00013262542865551813,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5084,
      "step": 36544
    },
    {
      "epoch": 0.00013274156300635483,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5011,
      "step": 36576
    },
    {
      "epoch": 0.00013285769735719156,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.484,
      "step": 36608
    },
    {
      "epoch": 0.00013297383170802826,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5097,
      "step": 36640
    },
    {
      "epoch": 0.00013308996605886496,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5138,
      "step": 36672
    },
    {
      "epoch": 0.00013320610040970166,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5144,
      "step": 36704
    },
    {
      "epoch": 0.00013332223476053838,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5219,
      "step": 36736
    },
    {
      "epoch": 0.00013343836911137508,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5267,
      "step": 36768
    },
    {
      "epoch": 0.00013355450346221178,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5469,
      "step": 36800
    },
    {
      "epoch": 0.00013367063781304848,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5481,
      "step": 36832
    },
    {
      "epoch": 0.00013378677216388518,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5355,
      "step": 36864
    },
    {
      "epoch": 0.0001339029065147219,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5312,
      "step": 36896
    },
    {
      "epoch": 0.0001340190408655586,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5314,
      "step": 36928
    },
    {
      "epoch": 0.0001341351752163953,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.546,
      "step": 36960
    },
    {
      "epoch": 0.000134251309567232,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5428,
      "step": 36992
    },
    {
      "epoch": 0.00013436744391806874,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5233,
      "step": 37024
    },
    {
      "epoch": 0.00013448357826890544,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5083,
      "step": 37056
    },
    {
      "epoch": 0.00013459971261974214,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4953,
      "step": 37088
    },
    {
      "epoch": 0.00013471584697057884,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.515,
      "step": 37120
    },
    {
      "epoch": 0.00013483198132141554,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.499,
      "step": 37152
    },
    {
      "epoch": 0.00013494811567225226,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.494,
      "step": 37184
    },
    {
      "epoch": 0.00013506425002308896,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4928,
      "step": 37216
    },
    {
      "epoch": 0.00013518038437392566,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.526,
      "step": 37248
    },
    {
      "epoch": 0.00013529651872476236,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5247,
      "step": 37280
    },
    {
      "epoch": 0.0001354126530755991,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5111,
      "step": 37312
    },
    {
      "epoch": 0.0001355287874264358,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5145,
      "step": 37344
    },
    {
      "epoch": 0.0001356449217772725,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4941,
      "step": 37376
    },
    {
      "epoch": 0.0001357610561281092,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5002,
      "step": 37408
    },
    {
      "epoch": 0.0001358771904789459,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4835,
      "step": 37440
    },
    {
      "epoch": 0.00013599332482978262,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5127,
      "step": 37472
    },
    {
      "epoch": 0.00013610945918061932,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5082,
      "step": 37504
    },
    {
      "epoch": 0.00013622559353145602,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5378,
      "step": 37536
    },
    {
      "epoch": 0.00013634172788229272,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.551,
      "step": 37568
    },
    {
      "epoch": 0.00013645786223312944,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5519,
      "step": 37600
    },
    {
      "epoch": 0.00013657399658396614,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5476,
      "step": 37632
    },
    {
      "epoch": 0.00013669013093480284,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5471,
      "step": 37664
    },
    {
      "epoch": 0.00013680626528563954,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5396,
      "step": 37696
    },
    {
      "epoch": 0.00013692239963647624,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5502,
      "step": 37728
    },
    {
      "epoch": 0.00013703853398731297,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5239,
      "step": 37760
    },
    {
      "epoch": 0.00013715466833814967,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4957,
      "step": 37792
    },
    {
      "epoch": 0.00013727080268898637,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5031,
      "step": 37824
    },
    {
      "epoch": 0.00013738693703982307,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5088,
      "step": 37856
    },
    {
      "epoch": 0.0001375030713906598,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5225,
      "step": 37888
    },
    {
      "epoch": 0.0001376192057414965,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5086,
      "step": 37920
    },
    {
      "epoch": 0.0001377353400923332,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4908,
      "step": 37952
    },
    {
      "epoch": 0.0001378514744431699,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5056,
      "step": 37984
    },
    {
      "epoch": 0.0001379676087940066,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4969,
      "step": 38016
    },
    {
      "epoch": 0.00013808374314484332,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.509,
      "step": 38048
    },
    {
      "epoch": 0.00013819987749568002,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5214,
      "step": 38080
    },
    {
      "epoch": 0.00013831601184651672,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5148,
      "step": 38112
    },
    {
      "epoch": 0.00013843214619735342,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5053,
      "step": 38144
    },
    {
      "epoch": 0.00013854828054819015,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5078,
      "step": 38176
    },
    {
      "epoch": 0.00013866441489902685,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5085,
      "step": 38208
    },
    {
      "epoch": 0.00013878054924986355,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5174,
      "step": 38240
    },
    {
      "epoch": 0.00013889668360070025,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5229,
      "step": 38272
    },
    {
      "epoch": 0.00013901281795153695,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5354,
      "step": 38304
    },
    {
      "epoch": 0.00013912895230237368,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5509,
      "step": 38336
    },
    {
      "epoch": 0.00013924508665321038,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5406,
      "step": 38368
    },
    {
      "epoch": 0.00013936122100404708,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5422,
      "step": 38400
    },
    {
      "epoch": 0.00013947735535488378,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5518,
      "step": 38432
    },
    {
      "epoch": 0.0001395934897057205,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5362,
      "step": 38464
    },
    {
      "epoch": 0.0001397096240565572,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5538,
      "step": 38496
    },
    {
      "epoch": 0.0001398257584073939,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5083,
      "step": 38528
    },
    {
      "epoch": 0.0001399418927582306,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5067,
      "step": 38560
    },
    {
      "epoch": 0.0001400580271090673,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5096,
      "step": 38592
    },
    {
      "epoch": 0.00014017416145990403,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4924,
      "step": 38624
    },
    {
      "epoch": 0.00014029029581074073,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5027,
      "step": 38656
    },
    {
      "epoch": 0.00014040643016157743,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4982,
      "step": 38688
    },
    {
      "epoch": 0.00014052256451241413,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5018,
      "step": 38720
    },
    {
      "epoch": 0.00014063869886325086,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5132,
      "step": 38752
    },
    {
      "epoch": 0.00014075483321408756,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5185,
      "step": 38784
    },
    {
      "epoch": 0.00014087096756492426,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5089,
      "step": 38816
    },
    {
      "epoch": 0.00014098710191576096,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.516,
      "step": 38848
    },
    {
      "epoch": 0.00014110323626659766,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4948,
      "step": 38880
    },
    {
      "epoch": 0.00014121937061743438,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.513,
      "step": 38912
    },
    {
      "epoch": 0.00014133550496827108,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.509,
      "step": 38944
    },
    {
      "epoch": 0.00014145163931910778,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5134,
      "step": 38976
    },
    {
      "epoch": 0.00014156777366994448,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5251,
      "step": 39008
    },
    {
      "epoch": 0.0001416839080207812,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5481,
      "step": 39040
    },
    {
      "epoch": 0.0001418000423716179,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5315,
      "step": 39072
    },
    {
      "epoch": 0.0001419161767224546,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5467,
      "step": 39104
    },
    {
      "epoch": 0.0001420323110732913,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5336,
      "step": 39136
    },
    {
      "epoch": 0.000142148445424128,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.542,
      "step": 39168
    },
    {
      "epoch": 0.00014226457977496474,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5448,
      "step": 39200
    },
    {
      "epoch": 0.00014238071412580144,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5234,
      "step": 39232
    },
    {
      "epoch": 0.00014249684847663814,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5292,
      "step": 39264
    },
    {
      "epoch": 0.00014261298282747484,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5128,
      "step": 39296
    },
    {
      "epoch": 0.00014272911717831154,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.508,
      "step": 39328
    },
    {
      "epoch": 0.00014284525152914826,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5101,
      "step": 39360
    },
    {
      "epoch": 0.00014296138587998496,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.506,
      "step": 39392
    },
    {
      "epoch": 0.00014307752023082166,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5144,
      "step": 39424
    },
    {
      "epoch": 0.00014319365458165836,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5049,
      "step": 39456
    },
    {
      "epoch": 0.0001433097889324951,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5025,
      "step": 39488
    },
    {
      "epoch": 0.0001434259232833318,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5125,
      "step": 39520
    },
    {
      "epoch": 0.0001435420576341685,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4971,
      "step": 39552
    },
    {
      "epoch": 0.0001436581919850052,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5188,
      "step": 39584
    },
    {
      "epoch": 0.0001437743263358419,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5289,
      "step": 39616
    },
    {
      "epoch": 0.00014389046068667862,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5098,
      "step": 39648
    },
    {
      "epoch": 0.00014400659503751532,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4915,
      "step": 39680
    },
    {
      "epoch": 0.00014412272938835202,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4921,
      "step": 39712
    },
    {
      "epoch": 0.00014423886373918872,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.51,
      "step": 39744
    },
    {
      "epoch": 0.00014435499809002544,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5219,
      "step": 39776
    },
    {
      "epoch": 0.00014447113244086214,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5477,
      "step": 39808
    },
    {
      "epoch": 0.00014458726679169884,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5319,
      "step": 39840
    },
    {
      "epoch": 0.00014470340114253554,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5435,
      "step": 39872
    },
    {
      "epoch": 0.00014481953549337224,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5525,
      "step": 39904
    },
    {
      "epoch": 0.00014493566984420897,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5432,
      "step": 39936
    },
    {
      "epoch": 0.00014505180419504567,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5452,
      "step": 39968
    },
    {
      "epoch": 0.00014516793854588237,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5232,
      "step": 40000
    },
    {
      "epoch": 0.00014528407289671907,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5223,
      "step": 40032
    },
    {
      "epoch": 0.0001454002072475558,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.517,
      "step": 40064
    },
    {
      "epoch": 0.0001455163415983925,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5074,
      "step": 40096
    },
    {
      "epoch": 0.0001456324759492292,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.501,
      "step": 40128
    },
    {
      "epoch": 0.0001457486103000659,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4961,
      "step": 40160
    },
    {
      "epoch": 0.0001458647446509026,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5198,
      "step": 40192
    },
    {
      "epoch": 0.00014598087900173932,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.501,
      "step": 40224
    },
    {
      "epoch": 0.00014609701335257602,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5023,
      "step": 40256
    },
    {
      "epoch": 0.00014621314770341272,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5087,
      "step": 40288
    },
    {
      "epoch": 0.00014632928205424942,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5091,
      "step": 40320
    },
    {
      "epoch": 0.00014644541640508615,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5142,
      "step": 40352
    },
    {
      "epoch": 0.00014656155075592285,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4986,
      "step": 40384
    },
    {
      "epoch": 0.00014667768510675955,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.497,
      "step": 40416
    },
    {
      "epoch": 0.00014679381945759625,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4982,
      "step": 40448
    },
    {
      "epoch": 0.00014690995380843295,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5152,
      "step": 40480
    },
    {
      "epoch": 0.00014702608815926968,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5238,
      "step": 40512
    },
    {
      "epoch": 0.00014714222251010638,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5203,
      "step": 40544
    },
    {
      "epoch": 0.00014725835686094308,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5429,
      "step": 40576
    },
    {
      "epoch": 0.00014737449121177978,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5303,
      "step": 40608
    },
    {
      "epoch": 0.0001474906255626165,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5557,
      "step": 40640
    },
    {
      "epoch": 0.0001476067599134532,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5576,
      "step": 40672
    },
    {
      "epoch": 0.0001477228942642899,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5527,
      "step": 40704
    },
    {
      "epoch": 0.0001478390286151266,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5355,
      "step": 40736
    },
    {
      "epoch": 0.0001479551629659633,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5448,
      "step": 40768
    },
    {
      "epoch": 0.00014807129731680003,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.517,
      "step": 40800
    },
    {
      "epoch": 0.00014818743166763673,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5031,
      "step": 40832
    },
    {
      "epoch": 0.00014830356601847343,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4923,
      "step": 40864
    },
    {
      "epoch": 0.00014841970036931013,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4959,
      "step": 40896
    },
    {
      "epoch": 0.00014853583472014686,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5074,
      "step": 40928
    },
    {
      "epoch": 0.00014865196907098356,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5105,
      "step": 40960
    },
    {
      "epoch": 0.00014876810342182026,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.4839,
      "step": 40992
    },
    {
      "epoch": 0.00014888423777265696,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5021,
      "step": 41024
    },
    {
      "epoch": 0.00014900037212349366,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.514,
      "step": 41056
    },
    {
      "epoch": 0.00014911650647433038,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5131,
      "step": 41088
    },
    {
      "epoch": 0.00014923264082516708,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5206,
      "step": 41120
    },
    {
      "epoch": 0.00014934877517600378,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5125,
      "step": 41152
    },
    {
      "epoch": 0.00014946490952684048,
      "grad_norm": 3.953936225974965,
      "learning_rate": 0.002,
      "loss": 2.5117,
      "step": 41184
    },
    {
      "epoch": 0.0001495810438776772,
      "grad_norm": 0.28403663967624937,
      "learning_rate": 0.002,
      "loss": 2.4871,
      "step": 41216
    },
    {
      "epoch": 0.0001496971782285139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 41248
    },
    {
      "epoch": 0.0001498133125793506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 41280
    },
    {
      "epoch": 0.0001499294469301873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 41312
    },
    {
      "epoch": 0.000150045581281024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 41344
    },
    {
      "epoch": 0.00015016171563186074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 41376
    },
    {
      "epoch": 0.00015027784998269744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 41408
    },
    {
      "epoch": 0.00015039398433353414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 41440
    },
    {
      "epoch": 0.00015051011868437084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 41472
    },
    {
      "epoch": 0.00015062625303520756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 41504
    },
    {
      "epoch": 0.00015074238738604426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 41536
    },
    {
      "epoch": 0.00015085852173688096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 41568
    },
    {
      "epoch": 0.00015097465608771766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 41600
    },
    {
      "epoch": 0.00015109079043855436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 41632
    },
    {
      "epoch": 0.0001512069247893911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 41664
    },
    {
      "epoch": 0.0001513230591402278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 41696
    },
    {
      "epoch": 0.0001514391934910645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 41728
    },
    {
      "epoch": 0.0001515553278419012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 41760
    },
    {
      "epoch": 0.0001516714621927379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 41792
    },
    {
      "epoch": 0.00015178759654357462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 41824
    },
    {
      "epoch": 0.00015190373089441132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 41856
    },
    {
      "epoch": 0.00015201986524524802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 41888
    },
    {
      "epoch": 0.00015213599959608472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 41920
    },
    {
      "epoch": 0.00015225213394692144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 41952
    },
    {
      "epoch": 0.00015236826829775814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 41984
    },
    {
      "epoch": 0.00015248440264859484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 42016
    },
    {
      "epoch": 0.00015260053699943154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 42048
    },
    {
      "epoch": 0.00015271667135026824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 42080
    },
    {
      "epoch": 0.00015283280570110497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6843,
      "step": 42112
    },
    {
      "epoch": 0.00015294894005194167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 42144
    },
    {
      "epoch": 0.00015306507440277837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 42176
    },
    {
      "epoch": 0.00015318120875361507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 42208
    },
    {
      "epoch": 0.0001532973431044518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 42240
    },
    {
      "epoch": 0.0001534134774552885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 42272
    },
    {
      "epoch": 0.0001535296118061252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 42304
    },
    {
      "epoch": 0.0001536457461569619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 42336
    },
    {
      "epoch": 0.0001537618805077986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 42368
    },
    {
      "epoch": 0.00015387801485863532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 42400
    },
    {
      "epoch": 0.00015399414920947202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 42432
    },
    {
      "epoch": 0.00015411028356030872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 42464
    },
    {
      "epoch": 0.00015422641791114542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 42496
    },
    {
      "epoch": 0.00015434255226198215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 42528
    },
    {
      "epoch": 0.00015445868661281885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 42560
    },
    {
      "epoch": 0.00015457482096365555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 42592
    },
    {
      "epoch": 0.00015469095531449225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 42624
    },
    {
      "epoch": 0.00015480708966532895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 42656
    },
    {
      "epoch": 0.00015492322401616568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6934,
      "step": 42688
    },
    {
      "epoch": 0.00015503935836700238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 42720
    },
    {
      "epoch": 0.00015515549271783908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 42752
    },
    {
      "epoch": 0.00015527162706867578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 42784
    },
    {
      "epoch": 0.0001553877614195125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 42816
    },
    {
      "epoch": 0.0001555038957703492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 42848
    },
    {
      "epoch": 0.0001556200301211859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 42880
    },
    {
      "epoch": 0.0001557361644720226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.691,
      "step": 42912
    },
    {
      "epoch": 0.0001558522988228593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6909,
      "step": 42944
    },
    {
      "epoch": 0.00015596843317369603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6741,
      "step": 42976
    },
    {
      "epoch": 0.00015608456752453273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6908,
      "step": 43008
    },
    {
      "epoch": 0.00015620070187536943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 43040
    },
    {
      "epoch": 0.00015631683622620613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 43072
    },
    {
      "epoch": 0.00015643297057704286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 43104
    },
    {
      "epoch": 0.00015654910492787956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 43136
    },
    {
      "epoch": 0.00015666523927871626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 43168
    },
    {
      "epoch": 0.00015678137362955296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 43200
    },
    {
      "epoch": 0.00015689750798038966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.748,
      "step": 43232
    },
    {
      "epoch": 0.00015701364233122638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 43264
    },
    {
      "epoch": 0.00015712977668206308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 43296
    },
    {
      "epoch": 0.00015724591103289978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 43328
    },
    {
      "epoch": 0.00015736204538373648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 43360
    },
    {
      "epoch": 0.0001574781797345732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 43392
    },
    {
      "epoch": 0.0001575943140854099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 43424
    },
    {
      "epoch": 0.0001577104484362466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 43456
    },
    {
      "epoch": 0.0001578265827870833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 43488
    },
    {
      "epoch": 0.00015794271713792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 43520
    },
    {
      "epoch": 0.00015805885148875674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 43552
    },
    {
      "epoch": 0.00015817498583959344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6902,
      "step": 43584
    },
    {
      "epoch": 0.00015829112019043014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6866,
      "step": 43616
    },
    {
      "epoch": 0.00015840725454126684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 43648
    },
    {
      "epoch": 0.00015852338889210356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6877,
      "step": 43680
    },
    {
      "epoch": 0.00015863952324294026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 43712
    },
    {
      "epoch": 0.00015875565759377696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 43744
    },
    {
      "epoch": 0.00015887179194461366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.674,
      "step": 43776
    },
    {
      "epoch": 0.00015898792629545036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 43808
    },
    {
      "epoch": 0.0001591040606462871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 43840
    },
    {
      "epoch": 0.0001592201949971238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 43872
    },
    {
      "epoch": 0.0001593363293479605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 43904
    },
    {
      "epoch": 0.0001594524636987972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 43936
    },
    {
      "epoch": 0.00015956859804963392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 43968
    },
    {
      "epoch": 0.00015968473240047062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 44000
    },
    {
      "epoch": 0.00015980086675130732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7509,
      "step": 44032
    },
    {
      "epoch": 0.00015991700110214402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 44064
    },
    {
      "epoch": 0.00016003313545298072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 44096
    },
    {
      "epoch": 0.00016014926980381745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 44128
    },
    {
      "epoch": 0.00016026540415465414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 44160
    },
    {
      "epoch": 0.00016038153850549084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6905,
      "step": 44192
    },
    {
      "epoch": 0.00016049767285632754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 44224
    },
    {
      "epoch": 0.00016061380720716427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 44256
    },
    {
      "epoch": 0.00016072994155800097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 44288
    },
    {
      "epoch": 0.00016084607590883767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 44320
    },
    {
      "epoch": 0.00016096221025967437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6936,
      "step": 44352
    },
    {
      "epoch": 0.00016107834461051107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.679,
      "step": 44384
    },
    {
      "epoch": 0.0001611944789613478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 44416
    },
    {
      "epoch": 0.0001613106133121845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6864,
      "step": 44448
    },
    {
      "epoch": 0.0001614267476630212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 44480
    },
    {
      "epoch": 0.0001615428820138579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.689,
      "step": 44512
    },
    {
      "epoch": 0.0001616590163646946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6899,
      "step": 44544
    },
    {
      "epoch": 0.00016177515071553133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 44576
    },
    {
      "epoch": 0.00016189128506636803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 44608
    },
    {
      "epoch": 0.00016200741941720472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 44640
    },
    {
      "epoch": 0.00016212355376804142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 44672
    },
    {
      "epoch": 0.00016223968811887815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 44704
    },
    {
      "epoch": 0.00016235582246971485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 44736
    },
    {
      "epoch": 0.00016247195682055155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 44768
    },
    {
      "epoch": 0.00016258809117138825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 44800
    },
    {
      "epoch": 0.00016270422552222495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 44832
    },
    {
      "epoch": 0.00016282035987306168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 44864
    },
    {
      "epoch": 0.00016293649422389838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 44896
    },
    {
      "epoch": 0.00016305262857473508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 44928
    },
    {
      "epoch": 0.00016316876292557178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 44960
    },
    {
      "epoch": 0.0001632848972764085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 44992
    },
    {
      "epoch": 0.0001634010316272452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 45024
    },
    {
      "epoch": 0.0001635171659780819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 45056
    },
    {
      "epoch": 0.0001636333003289186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 45088
    },
    {
      "epoch": 0.0001637494346797553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6869,
      "step": 45120
    },
    {
      "epoch": 0.00016386556903059203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6837,
      "step": 45152
    },
    {
      "epoch": 0.00016398170338142873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 45184
    },
    {
      "epoch": 0.00016409783773226543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 45216
    },
    {
      "epoch": 0.00016421397208310213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 45248
    },
    {
      "epoch": 0.00016433010643393886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6867,
      "step": 45280
    },
    {
      "epoch": 0.00016444624078477556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6881,
      "step": 45312
    },
    {
      "epoch": 0.00016456237513561226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 45344
    },
    {
      "epoch": 0.00016467850948644896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 45376
    },
    {
      "epoch": 0.00016479464383728566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 45408
    },
    {
      "epoch": 0.00016491077818812239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 45440
    },
    {
      "epoch": 0.00016502691253895909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 45472
    },
    {
      "epoch": 0.00016514304688979579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 45504
    },
    {
      "epoch": 0.00016525918124063249,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 45536
    },
    {
      "epoch": 0.0001653753155914692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 45568
    },
    {
      "epoch": 0.0001654914499423059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 45600
    },
    {
      "epoch": 0.0001656075842931426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 45632
    },
    {
      "epoch": 0.0001657237186439793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 45664
    },
    {
      "epoch": 0.000165839852994816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 45696
    },
    {
      "epoch": 0.00016595598734565274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 45728
    },
    {
      "epoch": 0.00016607212169648944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 45760
    },
    {
      "epoch": 0.00016618825604732614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 45792
    },
    {
      "epoch": 0.00016630439039816284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7542,
      "step": 45824
    },
    {
      "epoch": 0.00016642052474899957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 45856
    },
    {
      "epoch": 0.00016653665909983627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.679,
      "step": 45888
    },
    {
      "epoch": 0.00016665279345067297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6826,
      "step": 45920
    },
    {
      "epoch": 0.00016676892780150967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6887,
      "step": 45952
    },
    {
      "epoch": 0.00016688506215234637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 45984
    },
    {
      "epoch": 0.0001670011965031831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 46016
    },
    {
      "epoch": 0.0001671173308540198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 46048
    },
    {
      "epoch": 0.0001672334652048565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 46080
    },
    {
      "epoch": 0.0001673495995556932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 46112
    },
    {
      "epoch": 0.00016746573390652992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 46144
    },
    {
      "epoch": 0.00016758186825736662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 46176
    },
    {
      "epoch": 0.00016769800260820332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 46208
    },
    {
      "epoch": 0.00016781413695904002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 46240
    },
    {
      "epoch": 0.00016793027130987672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 46272
    },
    {
      "epoch": 0.00016804640566071345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 46304
    },
    {
      "epoch": 0.00016816254001155015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 46336
    },
    {
      "epoch": 0.00016827867436238685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 46368
    },
    {
      "epoch": 0.00016839480871322355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 46400
    },
    {
      "epoch": 0.00016851094306406027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 46432
    },
    {
      "epoch": 0.00016862707741489697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 46464
    },
    {
      "epoch": 0.00016874321176573367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 46496
    },
    {
      "epoch": 0.00016885934611657037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 46528
    },
    {
      "epoch": 0.00016897548046740707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 46560
    },
    {
      "epoch": 0.0001690916148182438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 46592
    },
    {
      "epoch": 0.0001692077491690805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 46624
    },
    {
      "epoch": 0.0001693238835199172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 46656
    },
    {
      "epoch": 0.0001694400178707539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 46688
    },
    {
      "epoch": 0.00016955615222159063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6878,
      "step": 46720
    },
    {
      "epoch": 0.00016967228657242733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6843,
      "step": 46752
    },
    {
      "epoch": 0.00016978842092326403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 46784
    },
    {
      "epoch": 0.00016990455527410073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6807,
      "step": 46816
    },
    {
      "epoch": 0.00017002068962493743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 46848
    },
    {
      "epoch": 0.00017013682397577415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 46880
    },
    {
      "epoch": 0.00017025295832661085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 46912
    },
    {
      "epoch": 0.00017036909267744755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 46944
    },
    {
      "epoch": 0.00017048522702828425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 46976
    },
    {
      "epoch": 0.00017060136137912098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 47008
    },
    {
      "epoch": 0.00017071749572995768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 47040
    },
    {
      "epoch": 0.00017083363008079438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 47072
    },
    {
      "epoch": 0.00017094976443163108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 47104
    },
    {
      "epoch": 0.00017106589878246778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 47136
    },
    {
      "epoch": 0.0001711820331333045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 47168
    },
    {
      "epoch": 0.0001712981674841412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 47200
    },
    {
      "epoch": 0.0001714143018349779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 47232
    },
    {
      "epoch": 0.0001715304361858146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 47264
    },
    {
      "epoch": 0.0001716465705366513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 47296
    },
    {
      "epoch": 0.00017176270488748803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 47328
    },
    {
      "epoch": 0.00017187883923832473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 47360
    },
    {
      "epoch": 0.00017199497358916143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 47392
    },
    {
      "epoch": 0.00017211110793999813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 47424
    },
    {
      "epoch": 0.00017222724229083486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 47456
    },
    {
      "epoch": 0.00017234337664167156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 47488
    },
    {
      "epoch": 0.00017245951099250826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 47520
    },
    {
      "epoch": 0.00017257564534334496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 47552
    },
    {
      "epoch": 0.00017269177969418166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 47584
    },
    {
      "epoch": 0.00017280791404501839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 47616
    },
    {
      "epoch": 0.00017292404839585509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 47648
    },
    {
      "epoch": 0.00017304018274669179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 47680
    },
    {
      "epoch": 0.00017315631709752849,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 47712
    },
    {
      "epoch": 0.0001732724514483652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 47744
    },
    {
      "epoch": 0.0001733885857992019,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 47776
    },
    {
      "epoch": 0.0001735047201500386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 47808
    },
    {
      "epoch": 0.0001736208545008753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 47840
    },
    {
      "epoch": 0.000173736988851712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 47872
    },
    {
      "epoch": 0.00017385312320254874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 47904
    },
    {
      "epoch": 0.00017396925755338544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 47936
    },
    {
      "epoch": 0.00017408539190422214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 47968
    },
    {
      "epoch": 0.00017420152625505884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 48000
    },
    {
      "epoch": 0.00017431766060589557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 48032
    },
    {
      "epoch": 0.00017443379495673227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 48064
    },
    {
      "epoch": 0.00017454992930756897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 48096
    },
    {
      "epoch": 0.00017466606365840567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 48128
    },
    {
      "epoch": 0.00017478219800924237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 48160
    },
    {
      "epoch": 0.0001748983323600791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 48192
    },
    {
      "epoch": 0.0001750144667109158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 48224
    },
    {
      "epoch": 0.0001751306010617525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 48256
    },
    {
      "epoch": 0.0001752467354125892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.692,
      "step": 48288
    },
    {
      "epoch": 0.00017536286976342592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 48320
    },
    {
      "epoch": 0.00017547900411426262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 48352
    },
    {
      "epoch": 0.00017559513846509932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 48384
    },
    {
      "epoch": 0.00017571127281593602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 48416
    },
    {
      "epoch": 0.00017582740716677272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 48448
    },
    {
      "epoch": 0.00017594354151760945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6913,
      "step": 48480
    },
    {
      "epoch": 0.00017605967586844615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 48512
    },
    {
      "epoch": 0.00017617581021928285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 48544
    },
    {
      "epoch": 0.00017629194457011955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 48576
    },
    {
      "epoch": 0.00017640807892095627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 48608
    },
    {
      "epoch": 0.00017652421327179297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 48640
    },
    {
      "epoch": 0.00017664034762262967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 48672
    },
    {
      "epoch": 0.00017675648197346637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 48704
    },
    {
      "epoch": 0.00017687261632430307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 48736
    },
    {
      "epoch": 0.0001769887506751398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 48768
    },
    {
      "epoch": 0.0001771048850259765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 48800
    },
    {
      "epoch": 0.0001772210193768132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 48832
    },
    {
      "epoch": 0.0001773371537276499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 48864
    },
    {
      "epoch": 0.00017745328807848663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 48896
    },
    {
      "epoch": 0.00017756942242932333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 48928
    },
    {
      "epoch": 0.00017768555678016003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 48960
    },
    {
      "epoch": 0.00017780169113099673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6781,
      "step": 48992
    },
    {
      "epoch": 0.00017791782548183343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 49024
    },
    {
      "epoch": 0.00017803395983267015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 49056
    },
    {
      "epoch": 0.00017815009418350685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 49088
    },
    {
      "epoch": 0.00017826622853434355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 49120
    },
    {
      "epoch": 0.00017838236288518025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 49152
    },
    {
      "epoch": 0.00017849849723601698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 49184
    },
    {
      "epoch": 0.00017861463158685368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 49216
    },
    {
      "epoch": 0.00017873076593769038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 49248
    },
    {
      "epoch": 0.00017884690028852708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 49280
    },
    {
      "epoch": 0.00017896303463936378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 49312
    },
    {
      "epoch": 0.0001790791689902005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 49344
    },
    {
      "epoch": 0.0001791953033410372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 49376
    },
    {
      "epoch": 0.0001793114376918739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 49408
    },
    {
      "epoch": 0.0001794275720427106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 49440
    },
    {
      "epoch": 0.00017954370639354733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 49472
    },
    {
      "epoch": 0.00017965984074438403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 49504
    },
    {
      "epoch": 0.00017977597509522073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 49536
    },
    {
      "epoch": 0.00017989210944605743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 49568
    },
    {
      "epoch": 0.00018000824379689413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 49600
    },
    {
      "epoch": 0.00018012437814773086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 49632
    },
    {
      "epoch": 0.00018024051249856756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 49664
    },
    {
      "epoch": 0.00018035664684940426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 49696
    },
    {
      "epoch": 0.00018047278120024096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6887,
      "step": 49728
    },
    {
      "epoch": 0.0001805889155510777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6838,
      "step": 49760
    },
    {
      "epoch": 0.0001807050499019144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 49792
    },
    {
      "epoch": 0.00018082118425275109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 49824
    },
    {
      "epoch": 0.00018093731860358779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 49856
    },
    {
      "epoch": 0.00018105345295442449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 49888
    },
    {
      "epoch": 0.0001811695873052612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 49920
    },
    {
      "epoch": 0.0001812857216560979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 49952
    },
    {
      "epoch": 0.0001814018560069346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 49984
    },
    {
      "epoch": 0.0001815179903577713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 50016
    },
    {
      "epoch": 0.000181634124708608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 50048
    },
    {
      "epoch": 0.00018175025905944474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 50080
    },
    {
      "epoch": 0.00018186639341028144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 50112
    },
    {
      "epoch": 0.00018198252776111814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 50144
    },
    {
      "epoch": 0.00018209866211195484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 50176
    },
    {
      "epoch": 0.00018221479646279157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 50208
    },
    {
      "epoch": 0.00018233093081362827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 50240
    },
    {
      "epoch": 0.00018244706516446497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 50272
    },
    {
      "epoch": 0.00018256319951530167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 50304
    },
    {
      "epoch": 0.00018267933386613837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 50336
    },
    {
      "epoch": 0.0001827954682169751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 50368
    },
    {
      "epoch": 0.0001829116025678118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 50400
    },
    {
      "epoch": 0.0001830277369186485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6877,
      "step": 50432
    },
    {
      "epoch": 0.0001831438712694852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 50464
    },
    {
      "epoch": 0.00018326000562032192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6918,
      "step": 50496
    },
    {
      "epoch": 0.00018337613997115862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 50528
    },
    {
      "epoch": 0.00018349227432199532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 50560
    },
    {
      "epoch": 0.00018360840867283202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 50592
    },
    {
      "epoch": 0.00018372454302366872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6803,
      "step": 50624
    },
    {
      "epoch": 0.00018384067737450545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 50656
    },
    {
      "epoch": 0.00018395681172534215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 50688
    },
    {
      "epoch": 0.00018407294607617885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 50720
    },
    {
      "epoch": 0.00018418908042701555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 50752
    },
    {
      "epoch": 0.00018430521477785227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 50784
    },
    {
      "epoch": 0.00018442134912868897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 50816
    },
    {
      "epoch": 0.00018453748347952567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 50848
    },
    {
      "epoch": 0.00018465361783036237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 50880
    },
    {
      "epoch": 0.00018476975218119907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7492,
      "step": 50912
    },
    {
      "epoch": 0.0001848858865320358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 50944
    },
    {
      "epoch": 0.0001850020208828725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 50976
    },
    {
      "epoch": 0.0001851181552337092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 51008
    },
    {
      "epoch": 0.0001852342895845459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6901,
      "step": 51040
    },
    {
      "epoch": 0.00018535042393538263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 51072
    },
    {
      "epoch": 0.00018546655828621933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 51104
    },
    {
      "epoch": 0.00018558269263705603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 51136
    },
    {
      "epoch": 0.00018569882698789273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 51168
    },
    {
      "epoch": 0.00018581496133872943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 51200
    },
    {
      "epoch": 0.00018593109568956615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 51232
    },
    {
      "epoch": 0.00018604723004040285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6886,
      "step": 51264
    },
    {
      "epoch": 0.00018616336439123955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 51296
    },
    {
      "epoch": 0.00018627949874207625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 51328
    },
    {
      "epoch": 0.00018639563309291298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 51360
    },
    {
      "epoch": 0.00018651176744374968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 51392
    },
    {
      "epoch": 0.00018662790179458638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 51424
    },
    {
      "epoch": 0.00018674403614542308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 51456
    },
    {
      "epoch": 0.00018686017049625978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 51488
    },
    {
      "epoch": 0.0001869763048470965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 51520
    },
    {
      "epoch": 0.0001870924391979332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 51552
    },
    {
      "epoch": 0.0001872085735487699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 51584
    },
    {
      "epoch": 0.0001873247078996066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 51616
    },
    {
      "epoch": 0.00018744084225044333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 51648
    },
    {
      "epoch": 0.00018755697660128003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7644,
      "step": 51680
    },
    {
      "epoch": 0.00018767311095211673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 51712
    },
    {
      "epoch": 0.00018778924530295343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 51744
    },
    {
      "epoch": 0.00018790537965379013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 51776
    },
    {
      "epoch": 0.00018802151400462686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 51808
    },
    {
      "epoch": 0.00018813764835546356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 51840
    },
    {
      "epoch": 0.00018825378270630026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6903,
      "step": 51872
    },
    {
      "epoch": 0.00018836991705713696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 51904
    },
    {
      "epoch": 0.0001884860514079737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 51936
    },
    {
      "epoch": 0.0001886021857588104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 51968
    },
    {
      "epoch": 0.0001887183201096471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 52000
    },
    {
      "epoch": 0.0001888344544604838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6807,
      "step": 52032
    },
    {
      "epoch": 0.0001889505888113205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6849,
      "step": 52064
    },
    {
      "epoch": 0.00018906672316215721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 52096
    },
    {
      "epoch": 0.00018918285751299391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 52128
    },
    {
      "epoch": 0.00018929899186383061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 52160
    },
    {
      "epoch": 0.0001894151262146673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 52192
    },
    {
      "epoch": 0.00018953126056550404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 52224
    },
    {
      "epoch": 0.00018964739491634074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 52256
    },
    {
      "epoch": 0.00018976352926717744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 52288
    },
    {
      "epoch": 0.00018987966361801414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 52320
    },
    {
      "epoch": 0.00018999579796885084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 52352
    },
    {
      "epoch": 0.00019011193231968757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 52384
    },
    {
      "epoch": 0.00019022806667052427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 52416
    },
    {
      "epoch": 0.00019034420102136097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 52448
    },
    {
      "epoch": 0.00019046033537219767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 52480
    },
    {
      "epoch": 0.00019057646972303437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 52512
    },
    {
      "epoch": 0.0001906926040738711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 52544
    },
    {
      "epoch": 0.0001908087384247078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 52576
    },
    {
      "epoch": 0.0001909248727755445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 52608
    },
    {
      "epoch": 0.0001910410071263812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 52640
    },
    {
      "epoch": 0.00019115714147721792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 52672
    },
    {
      "epoch": 0.00019127327582805462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 52704
    },
    {
      "epoch": 0.00019138941017889132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 52736
    },
    {
      "epoch": 0.00019150554452972802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6854,
      "step": 52768
    },
    {
      "epoch": 0.00019162167888056472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6884,
      "step": 52800
    },
    {
      "epoch": 0.00019173781323140145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 52832
    },
    {
      "epoch": 0.00019185394758223815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 52864
    },
    {
      "epoch": 0.00019197008193307485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 52896
    },
    {
      "epoch": 0.00019208621628391155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 52928
    },
    {
      "epoch": 0.00019220235063474827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 52960
    },
    {
      "epoch": 0.00019231848498558497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6911,
      "step": 52992
    },
    {
      "epoch": 0.00019243461933642167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 53024
    },
    {
      "epoch": 0.00019255075368725837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 53056
    },
    {
      "epoch": 0.00019266688803809507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 53088
    },
    {
      "epoch": 0.0001927830223889318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 53120
    },
    {
      "epoch": 0.0001928991567397685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 53152
    },
    {
      "epoch": 0.0001930152910906052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 53184
    },
    {
      "epoch": 0.0001931314254414419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 53216
    },
    {
      "epoch": 0.00019324755979227863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 53248
    },
    {
      "epoch": 0.00019336369414311533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6909,
      "step": 53280
    },
    {
      "epoch": 0.00019347982849395203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 53312
    },
    {
      "epoch": 0.00019359596284478873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 53344
    },
    {
      "epoch": 0.00019371209719562543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 53376
    },
    {
      "epoch": 0.00019382823154646215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 53408
    },
    {
      "epoch": 0.00019394436589729885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7286,
      "step": 53440
    },
    {
      "epoch": 0.00019406050024813555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 53472
    },
    {
      "epoch": 0.00019417663459897225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 53504
    },
    {
      "epoch": 0.00019429276894980898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 53536
    },
    {
      "epoch": 0.00019440890330064568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 53568
    },
    {
      "epoch": 0.00019452503765148238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6794,
      "step": 53600
    },
    {
      "epoch": 0.00019464117200231908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6915,
      "step": 53632
    },
    {
      "epoch": 0.00019475730635315578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 53664
    },
    {
      "epoch": 0.0001948734407039925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 53696
    },
    {
      "epoch": 0.0001949895750548292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 53728
    },
    {
      "epoch": 0.0001951057094056659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 53760
    },
    {
      "epoch": 0.0001952218437565026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 53792
    },
    {
      "epoch": 0.00019533797810733933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 53824
    },
    {
      "epoch": 0.00019545411245817603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 53856
    },
    {
      "epoch": 0.00019557024680901273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 53888
    },
    {
      "epoch": 0.00019568638115984943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 53920
    },
    {
      "epoch": 0.00019580251551068613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 53952
    },
    {
      "epoch": 0.00019591864986152286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 53984
    },
    {
      "epoch": 0.00019603478421235956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 54016
    },
    {
      "epoch": 0.00019615091856319626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 54048
    },
    {
      "epoch": 0.00019626705291403296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 54080
    },
    {
      "epoch": 0.0001963831872648697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 54112
    },
    {
      "epoch": 0.0001964993216157064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 54144
    },
    {
      "epoch": 0.0001966154559665431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 54176
    },
    {
      "epoch": 0.0001967315903173798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 54208
    },
    {
      "epoch": 0.0001968477246682165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 54240
    },
    {
      "epoch": 0.00019696385901905321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 54272
    },
    {
      "epoch": 0.00019707999336988991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 54304
    },
    {
      "epoch": 0.00019719612772072661,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 54336
    },
    {
      "epoch": 0.00019731226207156331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 54368
    },
    {
      "epoch": 0.00019742839642240004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 54400
    },
    {
      "epoch": 0.00019754453077323674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 54432
    },
    {
      "epoch": 0.00019766066512407344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 54464
    },
    {
      "epoch": 0.00019777679947491014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 54496
    },
    {
      "epoch": 0.00019789293382574684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 54528
    },
    {
      "epoch": 0.00019800906817658357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 54560
    },
    {
      "epoch": 0.00019812520252742027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 54592
    },
    {
      "epoch": 0.00019824133687825697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 54624
    },
    {
      "epoch": 0.00019835747122909367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 54656
    },
    {
      "epoch": 0.0001984736055799304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 54688
    },
    {
      "epoch": 0.0001985897399307671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 54720
    },
    {
      "epoch": 0.0001987058742816038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 54752
    },
    {
      "epoch": 0.0001988220086324405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 54784
    },
    {
      "epoch": 0.0001989381429832772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 54816
    },
    {
      "epoch": 0.00019905427733411392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 54848
    },
    {
      "epoch": 0.00019917041168495062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 54880
    },
    {
      "epoch": 0.00019928654603578732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 54912
    },
    {
      "epoch": 0.00019940268038662402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 54944
    },
    {
      "epoch": 0.00019951881473746075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 54976
    },
    {
      "epoch": 0.00019963494908829745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 55008
    },
    {
      "epoch": 0.00019975108343913415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 55040
    },
    {
      "epoch": 0.00019986721778997085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 55072
    },
    {
      "epoch": 0.00019998335214080755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 55104
    },
    {
      "epoch": 0.00020009948649164427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 55136
    },
    {
      "epoch": 0.00020021562084248097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 55168
    },
    {
      "epoch": 0.00020033175519331767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 55200
    },
    {
      "epoch": 0.00020044788954415437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 55232
    },
    {
      "epoch": 0.00020056402389499107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 55264
    },
    {
      "epoch": 0.0002006801582458278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 55296
    },
    {
      "epoch": 0.0002007962925966645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6887,
      "step": 55328
    },
    {
      "epoch": 0.0002009124269475012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 55360
    },
    {
      "epoch": 0.0002010285612983379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 55392
    },
    {
      "epoch": 0.00020114469564917463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 55424
    },
    {
      "epoch": 0.00020126083000001133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 55456
    },
    {
      "epoch": 0.00020137696435084803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.74,
      "step": 55488
    },
    {
      "epoch": 0.00020149309870168473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 55520
    },
    {
      "epoch": 0.00020160923305252143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 55552
    },
    {
      "epoch": 0.00020172536740335815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 55584
    },
    {
      "epoch": 0.00020184150175419485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 55616
    },
    {
      "epoch": 0.00020195763610503155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 55648
    },
    {
      "epoch": 0.00020207377045586825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 55680
    },
    {
      "epoch": 0.00020218990480670498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 55712
    },
    {
      "epoch": 0.00020230603915754168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6859,
      "step": 55744
    },
    {
      "epoch": 0.00020242217350837838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 55776
    },
    {
      "epoch": 0.00020253830785921508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 55808
    },
    {
      "epoch": 0.00020265444221005178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 55840
    },
    {
      "epoch": 0.0002027705765608885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6873,
      "step": 55872
    },
    {
      "epoch": 0.0002028867109117252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 55904
    },
    {
      "epoch": 0.0002030028452625619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 55936
    },
    {
      "epoch": 0.0002031189796133986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 55968
    },
    {
      "epoch": 0.00020323511396423533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 56000
    },
    {
      "epoch": 0.00020335124831507203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 56032
    },
    {
      "epoch": 0.00020346738266590873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 56064
    },
    {
      "epoch": 0.00020358351701674543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7381,
      "step": 56096
    },
    {
      "epoch": 0.00020369965136758213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7456,
      "step": 56128
    },
    {
      "epoch": 0.00020381578571841886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 56160
    },
    {
      "epoch": 0.00020393192006925556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 56192
    },
    {
      "epoch": 0.00020404805442009226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 56224
    },
    {
      "epoch": 0.00020416418877092896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 56256
    },
    {
      "epoch": 0.0002042803231217657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 56288
    },
    {
      "epoch": 0.0002043964574726024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 56320
    },
    {
      "epoch": 0.0002045125918234391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 56352
    },
    {
      "epoch": 0.0002046287261742758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 56384
    },
    {
      "epoch": 0.0002047448605251125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 56416
    },
    {
      "epoch": 0.00020486099487594921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 56448
    },
    {
      "epoch": 0.00020497712922678591,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 56480
    },
    {
      "epoch": 0.00020509326357762261,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6859,
      "step": 56512
    },
    {
      "epoch": 0.00020520939792845931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6915,
      "step": 56544
    },
    {
      "epoch": 0.00020532553227929604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 56576
    },
    {
      "epoch": 0.00020544166663013274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 56608
    },
    {
      "epoch": 0.00020555780098096944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6789,
      "step": 56640
    },
    {
      "epoch": 0.00020567393533180614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 56672
    },
    {
      "epoch": 0.00020579006968264284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.681,
      "step": 56704
    },
    {
      "epoch": 0.00020590620403347957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 56736
    },
    {
      "epoch": 0.00020602233838431627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 56768
    },
    {
      "epoch": 0.00020613847273515297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 56800
    },
    {
      "epoch": 0.00020625460708598967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 56832
    },
    {
      "epoch": 0.0002063707414368264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 56864
    },
    {
      "epoch": 0.0002064868757876631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 56896
    },
    {
      "epoch": 0.0002066030101384998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 56928
    },
    {
      "epoch": 0.0002067191444893365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 56960
    },
    {
      "epoch": 0.0002068352788401732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7783,
      "step": 56992
    },
    {
      "epoch": 0.00020695141319100992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 57024
    },
    {
      "epoch": 0.00020706754754184662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 57056
    },
    {
      "epoch": 0.00020718368189268332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6867,
      "step": 57088
    },
    {
      "epoch": 0.00020729981624352002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 57120
    },
    {
      "epoch": 0.00020741595059435675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 57152
    },
    {
      "epoch": 0.00020753208494519345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 57184
    },
    {
      "epoch": 0.00020764821929603015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 57216
    },
    {
      "epoch": 0.00020776435364686685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 57248
    },
    {
      "epoch": 0.00020788048799770355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 57280
    },
    {
      "epoch": 0.00020799662234854028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6803,
      "step": 57312
    },
    {
      "epoch": 0.00020811275669937698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 57344
    },
    {
      "epoch": 0.00020822889105021367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6883,
      "step": 57376
    },
    {
      "epoch": 0.00020834502540105037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6831,
      "step": 57408
    },
    {
      "epoch": 0.0002084611597518871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 57440
    },
    {
      "epoch": 0.0002085772941027238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 57472
    },
    {
      "epoch": 0.0002086934284535605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 57504
    },
    {
      "epoch": 0.0002088095628043972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 57536
    },
    {
      "epoch": 0.0002089256971552339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 57568
    },
    {
      "epoch": 0.00020904183150607063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 57600
    },
    {
      "epoch": 0.00020915796585690733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 57632
    },
    {
      "epoch": 0.00020927410020774403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 57664
    },
    {
      "epoch": 0.00020939023455858073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 57696
    },
    {
      "epoch": 0.00020950636890941746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 57728
    },
    {
      "epoch": 0.00020962250326025416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7508,
      "step": 57760
    },
    {
      "epoch": 0.00020973863761109086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 57792
    },
    {
      "epoch": 0.00020985477196192756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 57824
    },
    {
      "epoch": 0.00020997090631276425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 57856
    },
    {
      "epoch": 0.00021008704066360098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6908,
      "step": 57888
    },
    {
      "epoch": 0.00021020317501443768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 57920
    },
    {
      "epoch": 0.00021031930936527438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 57952
    },
    {
      "epoch": 0.00021043544371611108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6943,
      "step": 57984
    },
    {
      "epoch": 0.00021055157806694778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 58016
    },
    {
      "epoch": 0.0002106677124177845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 58048
    },
    {
      "epoch": 0.0002107838467686212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 58080
    },
    {
      "epoch": 0.0002108999811194579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 58112
    },
    {
      "epoch": 0.0002110161154702946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 58144
    },
    {
      "epoch": 0.00021113224982113134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6957,
      "step": 58176
    },
    {
      "epoch": 0.00021124838417196804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 58208
    },
    {
      "epoch": 0.00021136451852280474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 58240
    },
    {
      "epoch": 0.00021148065287364144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 58272
    },
    {
      "epoch": 0.00021159678722447814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6902,
      "step": 58304
    },
    {
      "epoch": 0.00021171292157531486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 58336
    },
    {
      "epoch": 0.00021182905592615156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 58368
    },
    {
      "epoch": 0.00021194519027698826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 58400
    },
    {
      "epoch": 0.00021206132462782496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 58432
    },
    {
      "epoch": 0.0002121774589786617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 58464
    },
    {
      "epoch": 0.0002122935933294984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 58496
    },
    {
      "epoch": 0.0002124097276803351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 58528
    },
    {
      "epoch": 0.0002125258620311718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 58560
    },
    {
      "epoch": 0.0002126419963820085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 58592
    },
    {
      "epoch": 0.00021275813073284522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 58624
    },
    {
      "epoch": 0.00021287426508368192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 58656
    },
    {
      "epoch": 0.00021299039943451862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 58688
    },
    {
      "epoch": 0.00021310653378535532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7474,
      "step": 58720
    },
    {
      "epoch": 0.00021322266813619204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 58752
    },
    {
      "epoch": 0.00021333880248702874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6869,
      "step": 58784
    },
    {
      "epoch": 0.00021345493683786544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6918,
      "step": 58816
    },
    {
      "epoch": 0.00021357107118870214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6883,
      "step": 58848
    },
    {
      "epoch": 0.00021368720553953884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 58880
    },
    {
      "epoch": 0.00021380333989037557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6813,
      "step": 58912
    },
    {
      "epoch": 0.00021391947424121227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 58944
    },
    {
      "epoch": 0.00021403560859204897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 58976
    },
    {
      "epoch": 0.00021415174294288567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 59008
    },
    {
      "epoch": 0.0002142678772937224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 59040
    },
    {
      "epoch": 0.0002143840116445591,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6888,
      "step": 59072
    },
    {
      "epoch": 0.0002145001459953958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 59104
    },
    {
      "epoch": 0.0002146162803462325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 59136
    },
    {
      "epoch": 0.0002147324146970692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 59168
    },
    {
      "epoch": 0.00021484854904790592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 59200
    },
    {
      "epoch": 0.00021496468339874262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 59232
    },
    {
      "epoch": 0.00021508081774957932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 59264
    },
    {
      "epoch": 0.00021519695210041602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 59296
    },
    {
      "epoch": 0.00021531308645125275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 59328
    },
    {
      "epoch": 0.00021542922080208945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 59360
    },
    {
      "epoch": 0.00021554535515292615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 59392
    },
    {
      "epoch": 0.00021566148950376285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 59424
    },
    {
      "epoch": 0.00021577762385459955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 59456
    },
    {
      "epoch": 0.00021589375820543628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 59488
    },
    {
      "epoch": 0.00021600989255627298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 59520
    },
    {
      "epoch": 0.00021612602690710968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 59552
    },
    {
      "epoch": 0.00021624216125794638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 59584
    },
    {
      "epoch": 0.0002163582956087831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 59616
    },
    {
      "epoch": 0.0002164744299596198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 59648
    },
    {
      "epoch": 0.0002165905643104565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6866,
      "step": 59680
    },
    {
      "epoch": 0.0002167066986612932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.68,
      "step": 59712
    },
    {
      "epoch": 0.0002168228330121299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6934,
      "step": 59744
    },
    {
      "epoch": 0.00021693896736296663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 59776
    },
    {
      "epoch": 0.00021705510171380333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 59808
    },
    {
      "epoch": 0.00021717123606464003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 59840
    },
    {
      "epoch": 0.00021728737041547673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 59872
    },
    {
      "epoch": 0.00021740350476631346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 59904
    },
    {
      "epoch": 0.00021751963911715016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 59936
    },
    {
      "epoch": 0.00021763577346798686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 59968
    },
    {
      "epoch": 0.00021775190781882356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 60000
    },
    {
      "epoch": 0.00021786804216966026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 60032
    },
    {
      "epoch": 0.00021798417652049698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 60064
    },
    {
      "epoch": 0.00021810031087133368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 60096
    },
    {
      "epoch": 0.00021821644522217038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6853,
      "step": 60128
    },
    {
      "epoch": 0.00021833257957300708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 60160
    },
    {
      "epoch": 0.0002184487139238438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 60192
    },
    {
      "epoch": 0.0002185648482746805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 60224
    },
    {
      "epoch": 0.0002186809826255172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 60256
    },
    {
      "epoch": 0.0002187971169763539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 60288
    },
    {
      "epoch": 0.0002189132513271906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 60320
    },
    {
      "epoch": 0.00021902938567802734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 60352
    },
    {
      "epoch": 0.00021914552002886404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 60384
    },
    {
      "epoch": 0.00021926165437970074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 60416
    },
    {
      "epoch": 0.00021937778873053744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6858,
      "step": 60448
    },
    {
      "epoch": 0.00021949392308137416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 60480
    },
    {
      "epoch": 0.00021961005743221086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 60512
    },
    {
      "epoch": 0.00021972619178304756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 60544
    },
    {
      "epoch": 0.00021984232613388426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 60576
    },
    {
      "epoch": 0.00021995846048472096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 60608
    },
    {
      "epoch": 0.0002200745948355577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 60640
    },
    {
      "epoch": 0.0002201907291863944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 60672
    },
    {
      "epoch": 0.0002203068635372311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 60704
    },
    {
      "epoch": 0.0002204229978880678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 60736
    },
    {
      "epoch": 0.0002205391322389045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 60768
    },
    {
      "epoch": 0.00022065526658974122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 60800
    },
    {
      "epoch": 0.00022077140094057792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 60832
    },
    {
      "epoch": 0.00022088753529141462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 60864
    },
    {
      "epoch": 0.00022100366964225132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 60896
    },
    {
      "epoch": 0.00022111980399308804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6908,
      "step": 60928
    },
    {
      "epoch": 0.00022123593834392474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 60960
    },
    {
      "epoch": 0.00022135207269476144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 60992
    },
    {
      "epoch": 0.00022146820704559814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 61024
    },
    {
      "epoch": 0.00022158434139643484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 61056
    },
    {
      "epoch": 0.00022170047574727157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 61088
    },
    {
      "epoch": 0.00022181661009810827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 61120
    },
    {
      "epoch": 0.00022193274444894497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 61152
    },
    {
      "epoch": 0.00022204887879978167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 61184
    },
    {
      "epoch": 0.0002221650131506184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 61216
    },
    {
      "epoch": 0.0002222811475014551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 61248
    },
    {
      "epoch": 0.0002223972818522918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7419,
      "step": 61280
    },
    {
      "epoch": 0.0002225134162031285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 61312
    },
    {
      "epoch": 0.0002226295505539652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 61344
    },
    {
      "epoch": 0.00022274568490480192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 61376
    },
    {
      "epoch": 0.00022286181925563862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 61408
    },
    {
      "epoch": 0.00022297795360647532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 61440
    },
    {
      "epoch": 0.00022309408795731202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 61472
    },
    {
      "epoch": 0.00022321022230814875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 61504
    },
    {
      "epoch": 0.00022332635665898545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 61536
    },
    {
      "epoch": 0.00022344249100982215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 61568
    },
    {
      "epoch": 0.00022355862536065885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 61600
    },
    {
      "epoch": 0.00022367475971149555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 61632
    },
    {
      "epoch": 0.00022379089406233228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 61664
    },
    {
      "epoch": 0.00022390702841316898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 61696
    },
    {
      "epoch": 0.00022402316276400568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 61728
    },
    {
      "epoch": 0.00022413929711484238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 61760
    },
    {
      "epoch": 0.0002242554314656791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 61792
    },
    {
      "epoch": 0.0002243715658165158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6848,
      "step": 61824
    },
    {
      "epoch": 0.0002244877001673525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6743,
      "step": 61856
    },
    {
      "epoch": 0.0002246038345181892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 61888
    },
    {
      "epoch": 0.0002247199688690259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 61920
    },
    {
      "epoch": 0.00022483610321986263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 61952
    },
    {
      "epoch": 0.00022495223757069933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 61984
    },
    {
      "epoch": 0.00022506837192153603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 62016
    },
    {
      "epoch": 0.00022518450627237273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 62048
    },
    {
      "epoch": 0.00022530064062320946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 62080
    },
    {
      "epoch": 0.00022541677497404616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 62112
    },
    {
      "epoch": 0.00022553290932488286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 62144
    },
    {
      "epoch": 0.00022564904367571956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 62176
    },
    {
      "epoch": 0.00022576517802655626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 62208
    },
    {
      "epoch": 0.00022588131237739298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 62240
    },
    {
      "epoch": 0.00022599744672822968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 62272
    },
    {
      "epoch": 0.00022611358107906638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 62304
    },
    {
      "epoch": 0.00022622971542990308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 62336
    },
    {
      "epoch": 0.0002263458497807398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 62368
    },
    {
      "epoch": 0.0002264619841315765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 62400
    },
    {
      "epoch": 0.0002265781184824132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 62432
    },
    {
      "epoch": 0.0002266942528332499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 62464
    },
    {
      "epoch": 0.0002268103871840866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 62496
    },
    {
      "epoch": 0.00022692652153492334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 62528
    },
    {
      "epoch": 0.00022704265588576004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 62560
    },
    {
      "epoch": 0.00022715879023659674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6806,
      "step": 62592
    },
    {
      "epoch": 0.00022727492458743344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6857,
      "step": 62624
    },
    {
      "epoch": 0.00022739105893827016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 62656
    },
    {
      "epoch": 0.00022750719328910686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6854,
      "step": 62688
    },
    {
      "epoch": 0.00022762332763994356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 62720
    },
    {
      "epoch": 0.00022773946199078026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6927,
      "step": 62752
    },
    {
      "epoch": 0.00022785559634161696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 62784
    },
    {
      "epoch": 0.0002279717306924537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 62816
    },
    {
      "epoch": 0.0002280878650432904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 62848
    },
    {
      "epoch": 0.0002282039993941271,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 62880
    },
    {
      "epoch": 0.0002283201337449638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 62912
    },
    {
      "epoch": 0.00022843626809580052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 62944
    },
    {
      "epoch": 0.00022855240244663722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 62976
    },
    {
      "epoch": 0.00022866853679747392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 63008
    },
    {
      "epoch": 0.00022878467114831062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 63040
    },
    {
      "epoch": 0.00022890080549914732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 63072
    },
    {
      "epoch": 0.00022901693984998404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 63104
    },
    {
      "epoch": 0.00022913307420082074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 63136
    },
    {
      "epoch": 0.00022924920855165744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 63168
    },
    {
      "epoch": 0.00022936534290249414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6919,
      "step": 63200
    },
    {
      "epoch": 0.00022948147725333087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 63232
    },
    {
      "epoch": 0.00022959761160416757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 63264
    },
    {
      "epoch": 0.00022971374595500427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 63296
    },
    {
      "epoch": 0.00022982988030584097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 63328
    },
    {
      "epoch": 0.00022994601465667767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 63360
    },
    {
      "epoch": 0.0002300621490075144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6801,
      "step": 63392
    },
    {
      "epoch": 0.0002301782833583511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 63424
    },
    {
      "epoch": 0.0002302944177091878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 63456
    },
    {
      "epoch": 0.0002304105520600245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6902,
      "step": 63488
    },
    {
      "epoch": 0.0002305266864108612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6825,
      "step": 63520
    },
    {
      "epoch": 0.00023064282076169792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 63552
    },
    {
      "epoch": 0.00023075895511253462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 63584
    },
    {
      "epoch": 0.00023087508946337132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 63616
    },
    {
      "epoch": 0.00023099122381420802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 63648
    },
    {
      "epoch": 0.00023110735816504475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 63680
    },
    {
      "epoch": 0.00023122349251588145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 63712
    },
    {
      "epoch": 0.00023133962686671815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 63744
    },
    {
      "epoch": 0.00023145576121755485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 63776
    },
    {
      "epoch": 0.00023157189556839155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 63808
    },
    {
      "epoch": 0.00023168802991922828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7452,
      "step": 63840
    },
    {
      "epoch": 0.00023180416427006498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 63872
    },
    {
      "epoch": 0.00023192029862090168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 63904
    },
    {
      "epoch": 0.00023203643297173838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 63936
    },
    {
      "epoch": 0.0002321525673225751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 63968
    },
    {
      "epoch": 0.0002322687016734118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 64000
    },
    {
      "epoch": 0.0002323848360242485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 64032
    },
    {
      "epoch": 0.0002325009703750852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 64064
    },
    {
      "epoch": 0.0002326171047259219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 64096
    },
    {
      "epoch": 0.00023273323907675863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 64128
    },
    {
      "epoch": 0.00023284937342759533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6787,
      "step": 64160
    },
    {
      "epoch": 0.00023296550777843203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 64192
    },
    {
      "epoch": 0.00023308164212926873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6872,
      "step": 64224
    },
    {
      "epoch": 0.00023319777648010546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 64256
    },
    {
      "epoch": 0.00023331391083094216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6834,
      "step": 64288
    },
    {
      "epoch": 0.00023343004518177886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 64320
    },
    {
      "epoch": 0.00023354617953261556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 64352
    },
    {
      "epoch": 0.00023366231388345226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 64384
    },
    {
      "epoch": 0.00023377844823428898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 64416
    },
    {
      "epoch": 0.00023389458258512568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 64448
    },
    {
      "epoch": 0.00023401071693596238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 64480
    },
    {
      "epoch": 0.00023412685128679908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 64512
    },
    {
      "epoch": 0.0002342429856376358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 64544
    },
    {
      "epoch": 0.0002343591199884725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 64576
    },
    {
      "epoch": 0.0002344752543393092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 64608
    },
    {
      "epoch": 0.0002345913886901459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 64640
    },
    {
      "epoch": 0.0002347075230409826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 64672
    },
    {
      "epoch": 0.00023482365739181934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 64704
    },
    {
      "epoch": 0.00023493979174265604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 64736
    },
    {
      "epoch": 0.00023505592609349274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 64768
    },
    {
      "epoch": 0.00023517206044432944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 64800
    },
    {
      "epoch": 0.00023528819479516616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 64832
    },
    {
      "epoch": 0.00023540432914600286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 64864
    },
    {
      "epoch": 0.00023552046349683956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 64896
    },
    {
      "epoch": 0.00023563659784767626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6859,
      "step": 64928
    },
    {
      "epoch": 0.00023575273219851296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 64960
    },
    {
      "epoch": 0.0002358688665493497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 64992
    },
    {
      "epoch": 0.0002359850009001864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 65024
    },
    {
      "epoch": 0.0002361011352510231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6814,
      "step": 65056
    },
    {
      "epoch": 0.0002362172696018598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 65088
    },
    {
      "epoch": 0.00023633340395269652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 65120
    },
    {
      "epoch": 0.00023644953830353322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 65152
    },
    {
      "epoch": 0.00023656567265436992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 65184
    },
    {
      "epoch": 0.00023668180700520662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 65216
    },
    {
      "epoch": 0.00023679794135604332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 65248
    },
    {
      "epoch": 0.00023691407570688004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 65280
    },
    {
      "epoch": 0.00023703021005771674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 65312
    },
    {
      "epoch": 0.00023714634440855344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 65344
    },
    {
      "epoch": 0.00023726247875939014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 65376
    },
    {
      "epoch": 0.00023737861311022687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 65408
    },
    {
      "epoch": 0.00023749474746106357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 65440
    },
    {
      "epoch": 0.00023761088181190027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 65472
    },
    {
      "epoch": 0.00023772701616273697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7478,
      "step": 65504
    },
    {
      "epoch": 0.00023784315051357367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 65536
    },
    {
      "epoch": 0.0002379592848644104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 65568
    },
    {
      "epoch": 0.0002380754192152471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 65600
    },
    {
      "epoch": 0.0002381915535660838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6904,
      "step": 65632
    },
    {
      "epoch": 0.0002383076879169205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 65664
    },
    {
      "epoch": 0.00023842382226775722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6857,
      "step": 65696
    },
    {
      "epoch": 0.00023853995661859392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 65728
    },
    {
      "epoch": 0.00023865609096943062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6894,
      "step": 65760
    },
    {
      "epoch": 0.00023877222532026732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 65792
    },
    {
      "epoch": 0.00023888835967110402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 65824
    },
    {
      "epoch": 0.00023900449402194075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 65856
    },
    {
      "epoch": 0.00023912062837277745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 65888
    },
    {
      "epoch": 0.00023923676272361415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 65920
    },
    {
      "epoch": 0.00023935289707445085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 65952
    },
    {
      "epoch": 0.00023946903142528755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 65984
    },
    {
      "epoch": 0.00023958516577612428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 66016
    },
    {
      "epoch": 0.00023970130012696098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 66048
    },
    {
      "epoch": 0.00023981743447779768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 66080
    },
    {
      "epoch": 0.00023993356882863438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 66112
    },
    {
      "epoch": 0.0002400497031794711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 66144
    },
    {
      "epoch": 0.0002401658375303078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 66176
    },
    {
      "epoch": 0.0002402819718811445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 66208
    },
    {
      "epoch": 0.0002403981062319812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 66240
    },
    {
      "epoch": 0.0002405142405828179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 66272
    },
    {
      "epoch": 0.00024063037493365463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 66304
    },
    {
      "epoch": 0.00024074650928449133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 66336
    },
    {
      "epoch": 0.00024086264363532803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 66368
    },
    {
      "epoch": 0.00024097877798616473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 66400
    },
    {
      "epoch": 0.00024109491233700146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 66432
    },
    {
      "epoch": 0.00024121104668783816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6874,
      "step": 66464
    },
    {
      "epoch": 0.00024132718103867486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6828,
      "step": 66496
    },
    {
      "epoch": 0.00024144331538951156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6927,
      "step": 66528
    },
    {
      "epoch": 0.00024155944974034826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 66560
    },
    {
      "epoch": 0.00024167558409118498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 66592
    },
    {
      "epoch": 0.00024179171844202168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 66624
    },
    {
      "epoch": 0.00024190785279285838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 66656
    },
    {
      "epoch": 0.00024202398714369508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 66688
    },
    {
      "epoch": 0.0002421401214945318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 66720
    },
    {
      "epoch": 0.0002422562558453685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 66752
    },
    {
      "epoch": 0.0002423723901962052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 66784
    },
    {
      "epoch": 0.0002424885245470419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 66816
    },
    {
      "epoch": 0.0002426046588978786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 66848
    },
    {
      "epoch": 0.00024272079324871534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 66880
    },
    {
      "epoch": 0.00024283692759955204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 66912
    },
    {
      "epoch": 0.00024295306195038874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 66944
    },
    {
      "epoch": 0.00024306919630122544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 66976
    },
    {
      "epoch": 0.00024318533065206216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 67008
    },
    {
      "epoch": 0.00024330146500289886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 67040
    },
    {
      "epoch": 0.00024341759935373556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 67072
    },
    {
      "epoch": 0.00024353373370457226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 67104
    },
    {
      "epoch": 0.00024364986805540896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 67136
    },
    {
      "epoch": 0.0002437660024062457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 67168
    },
    {
      "epoch": 0.0002438821367570824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 67200
    },
    {
      "epoch": 0.0002439982711079191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 67232
    },
    {
      "epoch": 0.0002441144054587558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 67264
    },
    {
      "epoch": 0.0002442305398095925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 67296
    },
    {
      "epoch": 0.0002443466741604292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 67328
    },
    {
      "epoch": 0.0002444628085112659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 67360
    },
    {
      "epoch": 0.00024457894286210264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 67392
    },
    {
      "epoch": 0.00024469507721293934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 67424
    },
    {
      "epoch": 0.00024481121156377604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 67456
    },
    {
      "epoch": 0.00024492734591461274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 67488
    },
    {
      "epoch": 0.00024504348026544944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 67520
    },
    {
      "epoch": 0.00024515961461628614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 67552
    },
    {
      "epoch": 0.00024527574896712284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 67584
    },
    {
      "epoch": 0.00024539188331795954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 67616
    },
    {
      "epoch": 0.00024550801766879624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 67648
    },
    {
      "epoch": 0.000245624152019633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 67680
    },
    {
      "epoch": 0.0002457402863704697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6895,
      "step": 67712
    },
    {
      "epoch": 0.0002458564207213064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 67744
    },
    {
      "epoch": 0.0002459725550721431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 67776
    },
    {
      "epoch": 0.0002460886894229798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 67808
    },
    {
      "epoch": 0.0002462048237738165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 67840
    },
    {
      "epoch": 0.0002463209581246532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 67872
    },
    {
      "epoch": 0.0002464370924754899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6876,
      "step": 67904
    },
    {
      "epoch": 0.0002465532268263266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 67936
    },
    {
      "epoch": 0.00024666936117716335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 67968
    },
    {
      "epoch": 0.00024678549552800005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 68000
    },
    {
      "epoch": 0.00024690162987883675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 68032
    },
    {
      "epoch": 0.00024701776422967345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 68064
    },
    {
      "epoch": 0.00024713389858051015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 68096
    },
    {
      "epoch": 0.00024725003293134685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 68128
    },
    {
      "epoch": 0.00024736616728218355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7536,
      "step": 68160
    },
    {
      "epoch": 0.00024748230163302025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 68192
    },
    {
      "epoch": 0.00024759843598385695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 68224
    },
    {
      "epoch": 0.0002477145703346937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 68256
    },
    {
      "epoch": 0.0002478307046855304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 68288
    },
    {
      "epoch": 0.0002479468390363671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 68320
    },
    {
      "epoch": 0.0002480629733872038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 68352
    },
    {
      "epoch": 0.0002481791077380405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7409,
      "step": 68384
    },
    {
      "epoch": 0.0002482952420888772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 68416
    },
    {
      "epoch": 0.0002484113764397139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 68448
    },
    {
      "epoch": 0.0002485275107905506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 68480
    },
    {
      "epoch": 0.0002486436451413873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 68512
    },
    {
      "epoch": 0.00024875977949222406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 68544
    },
    {
      "epoch": 0.00024887591384306076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 68576
    },
    {
      "epoch": 0.00024899204819389746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 68608
    },
    {
      "epoch": 0.00024910818254473416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 68640
    },
    {
      "epoch": 0.00024922431689557086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 68672
    },
    {
      "epoch": 0.00024934045124640756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 68704
    },
    {
      "epoch": 0.00024945658559724426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 68736
    },
    {
      "epoch": 0.00024957271994808096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 68768
    },
    {
      "epoch": 0.00024968885429891766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 68800
    },
    {
      "epoch": 0.0002498049886497544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 68832
    },
    {
      "epoch": 0.0002499211230005911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 68864
    },
    {
      "epoch": 0.0002500372573514278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 68896
    },
    {
      "epoch": 0.0002501533917022645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 68928
    },
    {
      "epoch": 0.0002502695260531012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 68960
    },
    {
      "epoch": 0.0002503856604039379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 68992
    },
    {
      "epoch": 0.0002505017947547746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 69024
    },
    {
      "epoch": 0.0002506179291056113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 69056
    },
    {
      "epoch": 0.000250734063456448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 69088
    },
    {
      "epoch": 0.00025085019780728477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 69120
    },
    {
      "epoch": 0.00025096633215812147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 69152
    },
    {
      "epoch": 0.00025108246650895816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 69184
    },
    {
      "epoch": 0.00025119860085979486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 69216
    },
    {
      "epoch": 0.00025131473521063156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 69248
    },
    {
      "epoch": 0.00025143086956146826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 69280
    },
    {
      "epoch": 0.00025154700391230496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 69312
    },
    {
      "epoch": 0.00025166313826314166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 69344
    },
    {
      "epoch": 0.00025177927261397836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 69376
    },
    {
      "epoch": 0.0002518954069648151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 69408
    },
    {
      "epoch": 0.0002520115413156518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6875,
      "step": 69440
    },
    {
      "epoch": 0.0002521276756664885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6914,
      "step": 69472
    },
    {
      "epoch": 0.0002522438100173252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6799,
      "step": 69504
    },
    {
      "epoch": 0.0002523599443681619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 69536
    },
    {
      "epoch": 0.0002524760787189986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 69568
    },
    {
      "epoch": 0.0002525922130698353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 69600
    },
    {
      "epoch": 0.000252708347420672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 69632
    },
    {
      "epoch": 0.0002528244817715087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 69664
    },
    {
      "epoch": 0.00025294061612234547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 69696
    },
    {
      "epoch": 0.00025305675047318217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 69728
    },
    {
      "epoch": 0.00025317288482401887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 69760
    },
    {
      "epoch": 0.00025328901917485557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7462,
      "step": 69792
    },
    {
      "epoch": 0.00025340515352569227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 69824
    },
    {
      "epoch": 0.00025352128787652897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 69856
    },
    {
      "epoch": 0.00025363742222736567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7569,
      "step": 69888
    },
    {
      "epoch": 0.00025375355657820237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 69920
    },
    {
      "epoch": 0.00025386969092903907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 69952
    },
    {
      "epoch": 0.0002539858252798758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6892,
      "step": 69984
    },
    {
      "epoch": 0.0002541019596307125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 70016
    },
    {
      "epoch": 0.0002542180939815492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 70048
    },
    {
      "epoch": 0.0002543342283323859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 70080
    },
    {
      "epoch": 0.0002544503626832226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 70112
    },
    {
      "epoch": 0.0002545664970340593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 70144
    },
    {
      "epoch": 0.000254682631384896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 70176
    },
    {
      "epoch": 0.0002547987657357327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 70208
    },
    {
      "epoch": 0.0002549149000865694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6776,
      "step": 70240
    },
    {
      "epoch": 0.0002550310344374062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6873,
      "step": 70272
    },
    {
      "epoch": 0.0002551471687882429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6904,
      "step": 70304
    },
    {
      "epoch": 0.0002552633031390796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6844,
      "step": 70336
    },
    {
      "epoch": 0.0002553794374899163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 70368
    },
    {
      "epoch": 0.000255495571840753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 70400
    },
    {
      "epoch": 0.0002556117061915897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6909,
      "step": 70432
    },
    {
      "epoch": 0.0002557278405424264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 70464
    },
    {
      "epoch": 0.0002558439748932631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 70496
    },
    {
      "epoch": 0.0002559601092440998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7453,
      "step": 70528
    },
    {
      "epoch": 0.00025607624359493653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 70560
    },
    {
      "epoch": 0.00025619237794577323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 70592
    },
    {
      "epoch": 0.00025630851229660993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 70624
    },
    {
      "epoch": 0.00025642464664744663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 70656
    },
    {
      "epoch": 0.00025654078099828333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 70688
    },
    {
      "epoch": 0.00025665691534912003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 70720
    },
    {
      "epoch": 0.00025677304969995673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 70752
    },
    {
      "epoch": 0.00025688918405079343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 70784
    },
    {
      "epoch": 0.00025700531840163013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 70816
    },
    {
      "epoch": 0.0002571214527524669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 70848
    },
    {
      "epoch": 0.0002572375871033036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 70880
    },
    {
      "epoch": 0.0002573537214541403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 70912
    },
    {
      "epoch": 0.000257469855804977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 70944
    },
    {
      "epoch": 0.0002575859901558137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 70976
    },
    {
      "epoch": 0.0002577021245066504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 71008
    },
    {
      "epoch": 0.0002578182588574871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.683,
      "step": 71040
    },
    {
      "epoch": 0.0002579343932083238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 71072
    },
    {
      "epoch": 0.0002580505275591605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 71104
    },
    {
      "epoch": 0.00025816666190999724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 71136
    },
    {
      "epoch": 0.00025828279626083394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 71168
    },
    {
      "epoch": 0.00025839893061167064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 71200
    },
    {
      "epoch": 0.00025851506496250734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 71232
    },
    {
      "epoch": 0.00025863119931334404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 71264
    },
    {
      "epoch": 0.00025874733366418074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 71296
    },
    {
      "epoch": 0.00025886346801501744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 71328
    },
    {
      "epoch": 0.00025897960236585414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 71360
    },
    {
      "epoch": 0.00025909573671669084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 71392
    },
    {
      "epoch": 0.0002592118710675276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 71424
    },
    {
      "epoch": 0.0002593280054183643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 71456
    },
    {
      "epoch": 0.000259444139769201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 71488
    },
    {
      "epoch": 0.0002595602741200377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 71520
    },
    {
      "epoch": 0.0002596764084708744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 71552
    },
    {
      "epoch": 0.0002597925428217111,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 71584
    },
    {
      "epoch": 0.0002599086771725478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 71616
    },
    {
      "epoch": 0.0002600248115233845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 71648
    },
    {
      "epoch": 0.0002601409458742212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 71680
    },
    {
      "epoch": 0.00026025708022505795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 71712
    },
    {
      "epoch": 0.00026037321457589465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 71744
    },
    {
      "epoch": 0.00026048934892673135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6851,
      "step": 71776
    },
    {
      "epoch": 0.00026060548327756805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6822,
      "step": 71808
    },
    {
      "epoch": 0.00026072161762840475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 71840
    },
    {
      "epoch": 0.00026083775197924145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6909,
      "step": 71872
    },
    {
      "epoch": 0.00026095388633007815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 71904
    },
    {
      "epoch": 0.00026107002068091485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 71936
    },
    {
      "epoch": 0.00026118615503175155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 71968
    },
    {
      "epoch": 0.0002613022893825883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 72000
    },
    {
      "epoch": 0.000261418423733425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 72032
    },
    {
      "epoch": 0.0002615345580842617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 72064
    },
    {
      "epoch": 0.0002616506924350984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 72096
    },
    {
      "epoch": 0.0002617668267859351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 72128
    },
    {
      "epoch": 0.0002618829611367718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 72160
    },
    {
      "epoch": 0.0002619990954876085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 72192
    },
    {
      "epoch": 0.0002621152298384452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 72224
    },
    {
      "epoch": 0.0002622313641892819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 72256
    },
    {
      "epoch": 0.00026234749854011865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 72288
    },
    {
      "epoch": 0.00026246363289095535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 72320
    },
    {
      "epoch": 0.00026257976724179205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 72352
    },
    {
      "epoch": 0.00026269590159262875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 72384
    },
    {
      "epoch": 0.00026281203594346545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 72416
    },
    {
      "epoch": 0.00026292817029430215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 72448
    },
    {
      "epoch": 0.00026304430464513885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 72480
    },
    {
      "epoch": 0.00026316043899597555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 72512
    },
    {
      "epoch": 0.00026327657334681225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 72544
    },
    {
      "epoch": 0.000263392707697649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6812,
      "step": 72576
    },
    {
      "epoch": 0.0002635088420484857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 72608
    },
    {
      "epoch": 0.0002636249763993224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 72640
    },
    {
      "epoch": 0.0002637411107501591,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 72672
    },
    {
      "epoch": 0.0002638572451009958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 72704
    },
    {
      "epoch": 0.0002639733794518325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 72736
    },
    {
      "epoch": 0.0002640895138026692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 72768
    },
    {
      "epoch": 0.0002642056481535059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 72800
    },
    {
      "epoch": 0.0002643217825043426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 72832
    },
    {
      "epoch": 0.0002644379168551793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 72864
    },
    {
      "epoch": 0.00026455405120601606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 72896
    },
    {
      "epoch": 0.00026467018555685276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 72928
    },
    {
      "epoch": 0.00026478631990768946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 72960
    },
    {
      "epoch": 0.00026490245425852616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 72992
    },
    {
      "epoch": 0.00026501858860936286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6837,
      "step": 73024
    },
    {
      "epoch": 0.00026513472296019956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 73056
    },
    {
      "epoch": 0.00026525085731103626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 73088
    },
    {
      "epoch": 0.00026536699166187296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 73120
    },
    {
      "epoch": 0.00026548312601270966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 73152
    },
    {
      "epoch": 0.0002655992603635464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 73184
    },
    {
      "epoch": 0.0002657153947143831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 73216
    },
    {
      "epoch": 0.0002658315290652198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 73248
    },
    {
      "epoch": 0.0002659476634160565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 73280
    },
    {
      "epoch": 0.0002660637977668932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 73312
    },
    {
      "epoch": 0.0002661799321177299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.687,
      "step": 73344
    },
    {
      "epoch": 0.0002662960664685666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6867,
      "step": 73376
    },
    {
      "epoch": 0.0002664122008194033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 73408
    },
    {
      "epoch": 0.00026652833517024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7406,
      "step": 73440
    },
    {
      "epoch": 0.00026664446952107677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 73472
    },
    {
      "epoch": 0.00026676060387191347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 73504
    },
    {
      "epoch": 0.00026687673822275017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 73536
    },
    {
      "epoch": 0.00026699287257358687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 73568
    },
    {
      "epoch": 0.00026710900692442357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 73600
    },
    {
      "epoch": 0.00026722514127526027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 73632
    },
    {
      "epoch": 0.00026734127562609697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 73664
    },
    {
      "epoch": 0.00026745740997693367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 73696
    },
    {
      "epoch": 0.00026757354432777037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 73728
    },
    {
      "epoch": 0.0002676896786786071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 73760
    },
    {
      "epoch": 0.0002678058130294438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 73792
    },
    {
      "epoch": 0.0002679219473802805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 73824
    },
    {
      "epoch": 0.0002680380817311172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 73856
    },
    {
      "epoch": 0.0002681542160819539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 73888
    },
    {
      "epoch": 0.0002682703504327906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 73920
    },
    {
      "epoch": 0.0002683864847836273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 73952
    },
    {
      "epoch": 0.000268502619134464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 73984
    },
    {
      "epoch": 0.0002686187534853007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 74016
    },
    {
      "epoch": 0.0002687348878361375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 74048
    },
    {
      "epoch": 0.0002688510221869742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 74080
    },
    {
      "epoch": 0.00026896715653781087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 74112
    },
    {
      "epoch": 0.00026908329088864757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 74144
    },
    {
      "epoch": 0.00026919942523948427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 74176
    },
    {
      "epoch": 0.00026931555959032097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 74208
    },
    {
      "epoch": 0.00026943169394115767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 74240
    },
    {
      "epoch": 0.00026954782829199437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 74272
    },
    {
      "epoch": 0.00026966396264283107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 74304
    },
    {
      "epoch": 0.0002697800969936678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 74336
    },
    {
      "epoch": 0.0002698962313445045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 74368
    },
    {
      "epoch": 0.0002700123656953412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 74400
    },
    {
      "epoch": 0.0002701285000461779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 74432
    },
    {
      "epoch": 0.0002702446343970146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 74464
    },
    {
      "epoch": 0.0002703607687478513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 74496
    },
    {
      "epoch": 0.000270476903098688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6891,
      "step": 74528
    },
    {
      "epoch": 0.0002705930374495247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 74560
    },
    {
      "epoch": 0.0002707091718003614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 74592
    },
    {
      "epoch": 0.0002708253061511982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 74624
    },
    {
      "epoch": 0.0002709414405020349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 74656
    },
    {
      "epoch": 0.0002710575748528716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 74688
    },
    {
      "epoch": 0.0002711737092037083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6772,
      "step": 74720
    },
    {
      "epoch": 0.000271289843554545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6831,
      "step": 74752
    },
    {
      "epoch": 0.0002714059779053817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 74784
    },
    {
      "epoch": 0.0002715221122562184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 74816
    },
    {
      "epoch": 0.0002716382466070551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 74848
    },
    {
      "epoch": 0.0002717543809578918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 74880
    },
    {
      "epoch": 0.00027187051530872853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 74912
    },
    {
      "epoch": 0.00027198664965956523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 74944
    },
    {
      "epoch": 0.00027210278401040193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 74976
    },
    {
      "epoch": 0.00027221891836123863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 75008
    },
    {
      "epoch": 0.00027233505271207533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 75040
    },
    {
      "epoch": 0.00027245118706291203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 75072
    },
    {
      "epoch": 0.00027256732141374873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 75104
    },
    {
      "epoch": 0.00027268345576458543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 75136
    },
    {
      "epoch": 0.00027279959011542213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 75168
    },
    {
      "epoch": 0.0002729157244662589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7494,
      "step": 75200
    },
    {
      "epoch": 0.0002730318588170956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 75232
    },
    {
      "epoch": 0.0002731479931679323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 75264
    },
    {
      "epoch": 0.000273264127518769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 75296
    },
    {
      "epoch": 0.0002733802618696057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6844,
      "step": 75328
    },
    {
      "epoch": 0.0002734963962204424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 75360
    },
    {
      "epoch": 0.0002736125305712791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 75392
    },
    {
      "epoch": 0.0002737286649221158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 75424
    },
    {
      "epoch": 0.0002738447992729525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 75456
    },
    {
      "epoch": 0.00027396093362378924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 75488
    },
    {
      "epoch": 0.00027407706797462594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 75520
    },
    {
      "epoch": 0.00027419320232546264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6848,
      "step": 75552
    },
    {
      "epoch": 0.00027430933667629934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 75584
    },
    {
      "epoch": 0.00027442547102713604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 75616
    },
    {
      "epoch": 0.00027454160537797274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 75648
    },
    {
      "epoch": 0.00027465773972880944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 75680
    },
    {
      "epoch": 0.00027477387407964614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7428,
      "step": 75712
    },
    {
      "epoch": 0.00027489000843048284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 75744
    },
    {
      "epoch": 0.0002750061427813196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 75776
    },
    {
      "epoch": 0.0002751222771321563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 75808
    },
    {
      "epoch": 0.000275238411482993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 75840
    },
    {
      "epoch": 0.0002753545458338297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.749,
      "step": 75872
    },
    {
      "epoch": 0.0002754706801846664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 75904
    },
    {
      "epoch": 0.0002755868145355031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6891,
      "step": 75936
    },
    {
      "epoch": 0.0002757029488863398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 75968
    },
    {
      "epoch": 0.0002758190832371765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 76000
    },
    {
      "epoch": 0.0002759352175880132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 76032
    },
    {
      "epoch": 0.00027605135193884995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 76064
    },
    {
      "epoch": 0.00027616748628968665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 76096
    },
    {
      "epoch": 0.00027628362064052335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 76128
    },
    {
      "epoch": 0.00027639975499136005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 76160
    },
    {
      "epoch": 0.00027651588934219675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 76192
    },
    {
      "epoch": 0.00027663202369303345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 76224
    },
    {
      "epoch": 0.00027674815804387015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 76256
    },
    {
      "epoch": 0.00027686429239470685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6923,
      "step": 76288
    },
    {
      "epoch": 0.00027698042674554355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6886,
      "step": 76320
    },
    {
      "epoch": 0.0002770965610963803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 76352
    },
    {
      "epoch": 0.000277212695447217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 76384
    },
    {
      "epoch": 0.0002773288297980537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6853,
      "step": 76416
    },
    {
      "epoch": 0.0002774449641488904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 76448
    },
    {
      "epoch": 0.0002775610984997271,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 76480
    },
    {
      "epoch": 0.0002776772328505638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 76512
    },
    {
      "epoch": 0.0002777933672014005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 76544
    },
    {
      "epoch": 0.0002779095015522372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 76576
    },
    {
      "epoch": 0.0002780256359030739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 76608
    },
    {
      "epoch": 0.00027814177025391065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7511,
      "step": 76640
    },
    {
      "epoch": 0.00027825790460474735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 76672
    },
    {
      "epoch": 0.00027837403895558405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 76704
    },
    {
      "epoch": 0.00027849017330642075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7512,
      "step": 76736
    },
    {
      "epoch": 0.00027860630765725745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 76768
    },
    {
      "epoch": 0.00027872244200809415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 76800
    },
    {
      "epoch": 0.00027883857635893085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 76832
    },
    {
      "epoch": 0.00027895471070976755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 76864
    },
    {
      "epoch": 0.00027907084506060425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 76896
    },
    {
      "epoch": 0.000279186979411441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 76928
    },
    {
      "epoch": 0.0002793031137622777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 76960
    },
    {
      "epoch": 0.0002794192481131144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 76992
    },
    {
      "epoch": 0.0002795353824639511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 77024
    },
    {
      "epoch": 0.0002796515168147878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 77056
    },
    {
      "epoch": 0.0002797676511656245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6792,
      "step": 77088
    },
    {
      "epoch": 0.0002798837855164612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 77120
    },
    {
      "epoch": 0.0002799999198672979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6858,
      "step": 77152
    },
    {
      "epoch": 0.0002801160542181346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 77184
    },
    {
      "epoch": 0.00028023218856897136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 77216
    },
    {
      "epoch": 0.00028034832291980806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 77248
    },
    {
      "epoch": 0.00028046445727064476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 77280
    },
    {
      "epoch": 0.00028058059162148146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 77312
    },
    {
      "epoch": 0.00028069672597231816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 77344
    },
    {
      "epoch": 0.00028081286032315486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 77376
    },
    {
      "epoch": 0.00028092899467399156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 77408
    },
    {
      "epoch": 0.00028104512902482826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 77440
    },
    {
      "epoch": 0.00028116126337566496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 77472
    },
    {
      "epoch": 0.0002812773977265017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 77504
    },
    {
      "epoch": 0.0002813935320773384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 77536
    },
    {
      "epoch": 0.0002815096664281751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7397,
      "step": 77568
    },
    {
      "epoch": 0.0002816258007790118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 77600
    },
    {
      "epoch": 0.0002817419351298485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 77632
    },
    {
      "epoch": 0.0002818580694806852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 77664
    },
    {
      "epoch": 0.0002819742038315219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 77696
    },
    {
      "epoch": 0.0002820903381823586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 77728
    },
    {
      "epoch": 0.0002822064725331953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.691,
      "step": 77760
    },
    {
      "epoch": 0.00028232260688403207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6859,
      "step": 77792
    },
    {
      "epoch": 0.00028243874123486877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 77824
    },
    {
      "epoch": 0.00028255487558570547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 77856
    },
    {
      "epoch": 0.00028267100993654217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6922,
      "step": 77888
    },
    {
      "epoch": 0.00028278714428737887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 77920
    },
    {
      "epoch": 0.00028290327863821557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 77952
    },
    {
      "epoch": 0.00028301941298905227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 77984
    },
    {
      "epoch": 0.00028313554733988897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 78016
    },
    {
      "epoch": 0.00028325168169072567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 78048
    },
    {
      "epoch": 0.0002833678160415624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 78080
    },
    {
      "epoch": 0.0002834839503923991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 78112
    },
    {
      "epoch": 0.0002836000847432358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 78144
    },
    {
      "epoch": 0.0002837162190940725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 78176
    },
    {
      "epoch": 0.0002838323534449092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 78208
    },
    {
      "epoch": 0.0002839484877957459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 78240
    },
    {
      "epoch": 0.0002840646221465826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 78272
    },
    {
      "epoch": 0.0002841807564974193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 78304
    },
    {
      "epoch": 0.000284296890848256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 78336
    },
    {
      "epoch": 0.0002844130251990927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 78368
    },
    {
      "epoch": 0.0002845291595499295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 78400
    },
    {
      "epoch": 0.0002846452939007662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 78432
    },
    {
      "epoch": 0.0002847614282516029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 78464
    },
    {
      "epoch": 0.0002848775626024396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 78496
    },
    {
      "epoch": 0.0002849936969532763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6813,
      "step": 78528
    },
    {
      "epoch": 0.000285109831304113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6855,
      "step": 78560
    },
    {
      "epoch": 0.0002852259656549497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 78592
    },
    {
      "epoch": 0.0002853421000057864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 78624
    },
    {
      "epoch": 0.0002854582343566231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 78656
    },
    {
      "epoch": 0.00028557436870745983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6895,
      "step": 78688
    },
    {
      "epoch": 0.00028569050305829653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 78720
    },
    {
      "epoch": 0.00028580663740913323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 78752
    },
    {
      "epoch": 0.00028592277175996993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 78784
    },
    {
      "epoch": 0.00028603890611080663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 78816
    },
    {
      "epoch": 0.00028615504046164333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6909,
      "step": 78848
    },
    {
      "epoch": 0.00028627117481248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 78880
    },
    {
      "epoch": 0.0002863873091633167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 78912
    },
    {
      "epoch": 0.0002865034435141534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 78944
    },
    {
      "epoch": 0.0002866195778649902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 78976
    },
    {
      "epoch": 0.0002867357122158269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 79008
    },
    {
      "epoch": 0.0002868518465666636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 79040
    },
    {
      "epoch": 0.0002869679809175003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 79072
    },
    {
      "epoch": 0.000287084115268337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 79104
    },
    {
      "epoch": 0.0002872002496191737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 79136
    },
    {
      "epoch": 0.0002873163839700104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 79168
    },
    {
      "epoch": 0.0002874325183208471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 79200
    },
    {
      "epoch": 0.0002875486526716838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 79232
    },
    {
      "epoch": 0.00028766478702252053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 79264
    },
    {
      "epoch": 0.00028778092137335723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 79296
    },
    {
      "epoch": 0.00028789705572419393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 79328
    },
    {
      "epoch": 0.00028801319007503063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6842,
      "step": 79360
    },
    {
      "epoch": 0.00028812932442586733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 79392
    },
    {
      "epoch": 0.00028824545877670403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6878,
      "step": 79424
    },
    {
      "epoch": 0.00028836159312754073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6707,
      "step": 79456
    },
    {
      "epoch": 0.00028847772747837743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 79488
    },
    {
      "epoch": 0.00028859386182921413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 79520
    },
    {
      "epoch": 0.0002887099961800509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 79552
    },
    {
      "epoch": 0.0002888261305308876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 79584
    },
    {
      "epoch": 0.0002889422648817243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 79616
    },
    {
      "epoch": 0.000289058399232561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 79648
    },
    {
      "epoch": 0.0002891745335833977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 79680
    },
    {
      "epoch": 0.0002892906679342344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 79712
    },
    {
      "epoch": 0.0002894068022850711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 79744
    },
    {
      "epoch": 0.0002895229366359078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 79776
    },
    {
      "epoch": 0.0002896390709867445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 79808
    },
    {
      "epoch": 0.00028975520533758124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 79840
    },
    {
      "epoch": 0.00028987133968841794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 79872
    },
    {
      "epoch": 0.00028998747403925464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 79904
    },
    {
      "epoch": 0.00029010360839009134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 79936
    },
    {
      "epoch": 0.00029021974274092804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 79968
    },
    {
      "epoch": 0.00029033587709176474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 80000
    },
    {
      "epoch": 0.00029045201144260144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 80032
    },
    {
      "epoch": 0.00029056814579343814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 80064
    },
    {
      "epoch": 0.00029068428014427484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 80096
    },
    {
      "epoch": 0.0002908004144951116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 80128
    },
    {
      "epoch": 0.0002909165488459483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 80160
    },
    {
      "epoch": 0.000291032683196785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 80192
    },
    {
      "epoch": 0.0002911488175476217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6792,
      "step": 80224
    },
    {
      "epoch": 0.0002912649518984584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 80256
    },
    {
      "epoch": 0.0002913810862492951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 80288
    },
    {
      "epoch": 0.0002914972206001318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 80320
    },
    {
      "epoch": 0.0002916133549509685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 80352
    },
    {
      "epoch": 0.0002917294893018052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 80384
    },
    {
      "epoch": 0.00029184562365264195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 80416
    },
    {
      "epoch": 0.00029196175800347865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 80448
    },
    {
      "epoch": 0.00029207789235431535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 80480
    },
    {
      "epoch": 0.00029219402670515205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 80512
    },
    {
      "epoch": 0.00029231016105598875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 80544
    },
    {
      "epoch": 0.00029242629540682545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 80576
    },
    {
      "epoch": 0.00029254242975766215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6937,
      "step": 80608
    },
    {
      "epoch": 0.00029265856410849885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 80640
    },
    {
      "epoch": 0.00029277469845933555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 80672
    },
    {
      "epoch": 0.0002928908328101723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 80704
    },
    {
      "epoch": 0.000293006967161009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 80736
    },
    {
      "epoch": 0.0002931231015118457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 80768
    },
    {
      "epoch": 0.0002932392358626824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 80800
    },
    {
      "epoch": 0.0002933553702135191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 80832
    },
    {
      "epoch": 0.0002934715045643558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 80864
    },
    {
      "epoch": 0.0002935876389151925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 80896
    },
    {
      "epoch": 0.0002937037732660292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 80928
    },
    {
      "epoch": 0.0002938199076168659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 80960
    },
    {
      "epoch": 0.00029393604196770265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 80992
    },
    {
      "epoch": 0.00029405217631853935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 81024
    },
    {
      "epoch": 0.00029416831066937605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 81056
    },
    {
      "epoch": 0.00029428444502021275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 81088
    },
    {
      "epoch": 0.00029440057937104945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 81120
    },
    {
      "epoch": 0.00029451671372188615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 81152
    },
    {
      "epoch": 0.00029463284807272285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 81184
    },
    {
      "epoch": 0.00029474898242355955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 81216
    },
    {
      "epoch": 0.00029486511677439625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 81248
    },
    {
      "epoch": 0.000294981251125233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 81280
    },
    {
      "epoch": 0.0002950973854760697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 81312
    },
    {
      "epoch": 0.0002952135198269064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 81344
    },
    {
      "epoch": 0.0002953296541777431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 81376
    },
    {
      "epoch": 0.0002954457885285798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 81408
    },
    {
      "epoch": 0.0002955619228794165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 81440
    },
    {
      "epoch": 0.0002956780572302532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 81472
    },
    {
      "epoch": 0.0002957941915810899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 81504
    },
    {
      "epoch": 0.0002959103259319266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 81536
    },
    {
      "epoch": 0.00029602646028276336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 81568
    },
    {
      "epoch": 0.00029614259463360006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6759,
      "step": 81600
    },
    {
      "epoch": 0.00029625872898443676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 81632
    },
    {
      "epoch": 0.00029637486333527346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 81664
    },
    {
      "epoch": 0.00029649099768611016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 81696
    },
    {
      "epoch": 0.00029660713203694686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 81728
    },
    {
      "epoch": 0.00029672326638778356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 81760
    },
    {
      "epoch": 0.00029683940073862026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 81792
    },
    {
      "epoch": 0.00029695553508945696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 81824
    },
    {
      "epoch": 0.0002970716694402937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 81856
    },
    {
      "epoch": 0.0002971878037911304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 81888
    },
    {
      "epoch": 0.0002973039381419671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 81920
    },
    {
      "epoch": 0.0002974200724928038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 81952
    },
    {
      "epoch": 0.0002975362068436405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 81984
    },
    {
      "epoch": 0.0002976523411944772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 82016
    },
    {
      "epoch": 0.0002977684755453139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 82048
    },
    {
      "epoch": 0.0002978846098961506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 82080
    },
    {
      "epoch": 0.0002980007442469873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 82112
    },
    {
      "epoch": 0.00029811687859782407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 82144
    },
    {
      "epoch": 0.00029823301294866077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 82176
    },
    {
      "epoch": 0.00029834914729949747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 82208
    },
    {
      "epoch": 0.00029846528165033417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 82240
    },
    {
      "epoch": 0.00029858141600117087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 82272
    },
    {
      "epoch": 0.00029869755035200757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6895,
      "step": 82304
    },
    {
      "epoch": 0.00029881368470284427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 82336
    },
    {
      "epoch": 0.00029892981905368097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6822,
      "step": 82368
    },
    {
      "epoch": 0.00029904595340451767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6839,
      "step": 82400
    },
    {
      "epoch": 0.0002991620877553544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6902,
      "step": 82432
    },
    {
      "epoch": 0.0002992782221061911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 82464
    },
    {
      "epoch": 0.0002993943564570278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 82496
    },
    {
      "epoch": 0.0002995104908078645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 82528
    },
    {
      "epoch": 0.0002996266251587012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 82560
    },
    {
      "epoch": 0.0002997427595095379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 82592
    },
    {
      "epoch": 0.0002998588938603746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 82624
    },
    {
      "epoch": 0.0002999750282112113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 82656
    },
    {
      "epoch": 0.000300091162562048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 82688
    },
    {
      "epoch": 0.0003002072969128848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 82720
    },
    {
      "epoch": 0.0003003234312637215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 82752
    },
    {
      "epoch": 0.0003004395656145582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 82784
    },
    {
      "epoch": 0.0003005556999653949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 82816
    },
    {
      "epoch": 0.0003006718343162316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 82848
    },
    {
      "epoch": 0.0003007879686670683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 82880
    },
    {
      "epoch": 0.000300904103017905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 82912
    },
    {
      "epoch": 0.0003010202373687417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 82944
    },
    {
      "epoch": 0.0003011363717195784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 82976
    },
    {
      "epoch": 0.00030125250607041513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 83008
    },
    {
      "epoch": 0.00030136864042125183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 83040
    },
    {
      "epoch": 0.00030148477477208853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 83072
    },
    {
      "epoch": 0.00030160090912292523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 83104
    },
    {
      "epoch": 0.00030171704347376193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 83136
    },
    {
      "epoch": 0.00030183317782459863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6895,
      "step": 83168
    },
    {
      "epoch": 0.00030194931217543533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 83200
    },
    {
      "epoch": 0.00030206544652627203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 83232
    },
    {
      "epoch": 0.00030218158087710873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 83264
    },
    {
      "epoch": 0.0003022977152279455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 83296
    },
    {
      "epoch": 0.0003024138495787822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 83328
    },
    {
      "epoch": 0.0003025299839296189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 83360
    },
    {
      "epoch": 0.0003026461182804556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 83392
    },
    {
      "epoch": 0.0003027622526312923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 83424
    },
    {
      "epoch": 0.000302878386982129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7526,
      "step": 83456
    },
    {
      "epoch": 0.0003029945213329657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 83488
    },
    {
      "epoch": 0.0003031106556838024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 83520
    },
    {
      "epoch": 0.0003032267900346391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7499,
      "step": 83552
    },
    {
      "epoch": 0.0003033429243854758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 83584
    },
    {
      "epoch": 0.00030345905873631254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 83616
    },
    {
      "epoch": 0.00030357519308714924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 83648
    },
    {
      "epoch": 0.00030369132743798594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 83680
    },
    {
      "epoch": 0.00030380746178882264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 83712
    },
    {
      "epoch": 0.00030392359613965934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 83744
    },
    {
      "epoch": 0.00030403973049049604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 83776
    },
    {
      "epoch": 0.00030415586484133273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 83808
    },
    {
      "epoch": 0.00030427199919216943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6902,
      "step": 83840
    },
    {
      "epoch": 0.00030438813354300613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 83872
    },
    {
      "epoch": 0.0003045042678938429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6923,
      "step": 83904
    },
    {
      "epoch": 0.0003046204022446796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6894,
      "step": 83936
    },
    {
      "epoch": 0.0003047365365955163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6923,
      "step": 83968
    },
    {
      "epoch": 0.000304852670946353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 84000
    },
    {
      "epoch": 0.0003049688052971897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 84032
    },
    {
      "epoch": 0.0003050849396480264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 84064
    },
    {
      "epoch": 0.0003052010739988631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 84096
    },
    {
      "epoch": 0.0003053172083496998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6895,
      "step": 84128
    },
    {
      "epoch": 0.0003054333427005365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 84160
    },
    {
      "epoch": 0.00030554947705137324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 84192
    },
    {
      "epoch": 0.00030566561140220994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 84224
    },
    {
      "epoch": 0.00030578174575304664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 84256
    },
    {
      "epoch": 0.00030589788010388334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 84288
    },
    {
      "epoch": 0.00030601401445472004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7593,
      "step": 84320
    },
    {
      "epoch": 0.00030613014880555674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 84352
    },
    {
      "epoch": 0.00030624628315639344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 84384
    },
    {
      "epoch": 0.00030636241750723014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 84416
    },
    {
      "epoch": 0.00030647855185806684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 84448
    },
    {
      "epoch": 0.0003065946862089036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7439,
      "step": 84480
    },
    {
      "epoch": 0.0003067108205597403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 84512
    },
    {
      "epoch": 0.000306826954910577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 84544
    },
    {
      "epoch": 0.0003069430892614137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6831,
      "step": 84576
    },
    {
      "epoch": 0.0003070592236122504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 84608
    },
    {
      "epoch": 0.0003071753579630871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 84640
    },
    {
      "epoch": 0.0003072914923139238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6861,
      "step": 84672
    },
    {
      "epoch": 0.0003074076266647605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 84704
    },
    {
      "epoch": 0.0003075237610155972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 84736
    },
    {
      "epoch": 0.00030763989536643395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6864,
      "step": 84768
    },
    {
      "epoch": 0.00030775602971727065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 84800
    },
    {
      "epoch": 0.00030787216406810735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 84832
    },
    {
      "epoch": 0.00030798829841894405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 84864
    },
    {
      "epoch": 0.00030810443276978075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 84896
    },
    {
      "epoch": 0.00030822056712061745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 84928
    },
    {
      "epoch": 0.00030833670147145415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 84960
    },
    {
      "epoch": 0.00030845283582229085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 84992
    },
    {
      "epoch": 0.00030856897017312755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 85024
    },
    {
      "epoch": 0.0003086851045239643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 85056
    },
    {
      "epoch": 0.000308801238874801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 85088
    },
    {
      "epoch": 0.0003089173732256377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 85120
    },
    {
      "epoch": 0.0003090335075764744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 85152
    },
    {
      "epoch": 0.0003091496419273111,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 85184
    },
    {
      "epoch": 0.0003092657762781478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7526,
      "step": 85216
    },
    {
      "epoch": 0.0003093819106289845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 85248
    },
    {
      "epoch": 0.0003094980449798212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 85280
    },
    {
      "epoch": 0.0003096141793306579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 85312
    },
    {
      "epoch": 0.00030973031368149466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 85344
    },
    {
      "epoch": 0.00030984644803233136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6875,
      "step": 85376
    },
    {
      "epoch": 0.00030996258238316806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 85408
    },
    {
      "epoch": 0.00031007871673400476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 85440
    },
    {
      "epoch": 0.00031019485108484146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 85472
    },
    {
      "epoch": 0.00031031098543567816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 85504
    },
    {
      "epoch": 0.00031042711978651486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6888,
      "step": 85536
    },
    {
      "epoch": 0.00031054325413735156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 85568
    },
    {
      "epoch": 0.00031065938848818826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 85600
    },
    {
      "epoch": 0.000310775522839025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 85632
    },
    {
      "epoch": 0.0003108916571898617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 85664
    },
    {
      "epoch": 0.0003110077915406984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 85696
    },
    {
      "epoch": 0.0003111239258915351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 85728
    },
    {
      "epoch": 0.0003112400602423718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 85760
    },
    {
      "epoch": 0.0003113561945932085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 85792
    },
    {
      "epoch": 0.0003114723289440452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7439,
      "step": 85824
    },
    {
      "epoch": 0.0003115884632948819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 85856
    },
    {
      "epoch": 0.0003117045976457186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 85888
    },
    {
      "epoch": 0.00031182073199655536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6909,
      "step": 85920
    },
    {
      "epoch": 0.00031193686634739206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 85952
    },
    {
      "epoch": 0.00031205300069822876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 85984
    },
    {
      "epoch": 0.00031216913504906546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 86016
    },
    {
      "epoch": 0.00031228526939990216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 86048
    },
    {
      "epoch": 0.00031240140375073886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 86080
    },
    {
      "epoch": 0.00031251753810157556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 86112
    },
    {
      "epoch": 0.00031263367245241226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 86144
    },
    {
      "epoch": 0.00031274980680324896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 86176
    },
    {
      "epoch": 0.0003128659411540857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6869,
      "step": 86208
    },
    {
      "epoch": 0.0003129820755049224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 86240
    },
    {
      "epoch": 0.0003130982098557591,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 86272
    },
    {
      "epoch": 0.0003132143442065958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 86304
    },
    {
      "epoch": 0.0003133304785574325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 86336
    },
    {
      "epoch": 0.0003134466129082692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 86368
    },
    {
      "epoch": 0.0003135627472591059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 86400
    },
    {
      "epoch": 0.0003136788816099426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 86432
    },
    {
      "epoch": 0.0003137950159607793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 86464
    },
    {
      "epoch": 0.00031391115031161607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 86496
    },
    {
      "epoch": 0.00031402728466245277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 86528
    },
    {
      "epoch": 0.00031414341901328947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 86560
    },
    {
      "epoch": 0.00031425955336412617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 86592
    },
    {
      "epoch": 0.00031437568771496287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 86624
    },
    {
      "epoch": 0.00031449182206579957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 86656
    },
    {
      "epoch": 0.00031460795641663627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 86688
    },
    {
      "epoch": 0.00031472409076747297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 86720
    },
    {
      "epoch": 0.00031484022511830967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 86752
    },
    {
      "epoch": 0.0003149563594691464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 86784
    },
    {
      "epoch": 0.0003150724938199831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 86816
    },
    {
      "epoch": 0.0003151886281708198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 86848
    },
    {
      "epoch": 0.0003153047625216565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 86880
    },
    {
      "epoch": 0.0003154208968724932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 86912
    },
    {
      "epoch": 0.0003155370312233299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 86944
    },
    {
      "epoch": 0.0003156531655741666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 86976
    },
    {
      "epoch": 0.0003157692999250033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 87008
    },
    {
      "epoch": 0.00031588543427584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 87040
    },
    {
      "epoch": 0.0003160015686266768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 87072
    },
    {
      "epoch": 0.0003161177029775135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 87104
    },
    {
      "epoch": 0.0003162338373283502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 87136
    },
    {
      "epoch": 0.0003163499716791869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 87168
    },
    {
      "epoch": 0.0003164661060300236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 87200
    },
    {
      "epoch": 0.0003165822403808603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 87232
    },
    {
      "epoch": 0.000316698374731697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 87264
    },
    {
      "epoch": 0.0003168145090825337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 87296
    },
    {
      "epoch": 0.0003169306434333704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 87328
    },
    {
      "epoch": 0.00031704677778420713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 87360
    },
    {
      "epoch": 0.00031716291213504383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 87392
    },
    {
      "epoch": 0.00031727904648588053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 87424
    },
    {
      "epoch": 0.00031739518083671723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6943,
      "step": 87456
    },
    {
      "epoch": 0.00031751131518755393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 87488
    },
    {
      "epoch": 0.00031762744953839063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 87520
    },
    {
      "epoch": 0.00031774358388922733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 87552
    },
    {
      "epoch": 0.00031785971824006403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 87584
    },
    {
      "epoch": 0.00031797585259090073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6834,
      "step": 87616
    },
    {
      "epoch": 0.0003180919869417375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 87648
    },
    {
      "epoch": 0.0003182081212925742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 87680
    },
    {
      "epoch": 0.0003183242556434109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 87712
    },
    {
      "epoch": 0.0003184403899942476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 87744
    },
    {
      "epoch": 0.0003185565243450843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 87776
    },
    {
      "epoch": 0.000318672658695921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 87808
    },
    {
      "epoch": 0.0003187887930467577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7516,
      "step": 87840
    },
    {
      "epoch": 0.0003189049273975944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 87872
    },
    {
      "epoch": 0.0003190210617484311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 87904
    },
    {
      "epoch": 0.00031913719609926784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 87936
    },
    {
      "epoch": 0.00031925333045010454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 87968
    },
    {
      "epoch": 0.00031936946480094124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 88000
    },
    {
      "epoch": 0.00031948559915177794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 88032
    },
    {
      "epoch": 0.00031960173350261464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 88064
    },
    {
      "epoch": 0.00031971786785345134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 88096
    },
    {
      "epoch": 0.00031983400220428804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 88128
    },
    {
      "epoch": 0.00031995013655512474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 88160
    },
    {
      "epoch": 0.00032006627090596144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 88192
    },
    {
      "epoch": 0.0003201824052567982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 88224
    },
    {
      "epoch": 0.0003202985396076349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 88256
    },
    {
      "epoch": 0.0003204146739584716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 88288
    },
    {
      "epoch": 0.0003205308083093083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 88320
    },
    {
      "epoch": 0.000320646942660145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 88352
    },
    {
      "epoch": 0.0003207630770109817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6936,
      "step": 88384
    },
    {
      "epoch": 0.0003208792113618184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 88416
    },
    {
      "epoch": 0.0003209953457126551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 88448
    },
    {
      "epoch": 0.0003211114800634918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6842,
      "step": 88480
    },
    {
      "epoch": 0.00032122761441432854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 88512
    },
    {
      "epoch": 0.00032134374876516524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 88544
    },
    {
      "epoch": 0.00032145988311600194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 88576
    },
    {
      "epoch": 0.00032157601746683864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 88608
    },
    {
      "epoch": 0.00032169215181767534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 88640
    },
    {
      "epoch": 0.00032180828616851204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 88672
    },
    {
      "epoch": 0.00032192442051934874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 88704
    },
    {
      "epoch": 0.00032204055487018544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7472,
      "step": 88736
    },
    {
      "epoch": 0.00032215668922102214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7595,
      "step": 88768
    },
    {
      "epoch": 0.0003222728235718589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 88800
    },
    {
      "epoch": 0.0003223889579226956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 88832
    },
    {
      "epoch": 0.0003225050922735323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 88864
    },
    {
      "epoch": 0.000322621226624369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 88896
    },
    {
      "epoch": 0.0003227373609752057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 88928
    },
    {
      "epoch": 0.0003228534953260424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 88960
    },
    {
      "epoch": 0.0003229696296768791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 88992
    },
    {
      "epoch": 0.0003230857640277158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 89024
    },
    {
      "epoch": 0.0003232018983785525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 89056
    },
    {
      "epoch": 0.0003233180327293892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 89088
    },
    {
      "epoch": 0.00032343416708022595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 89120
    },
    {
      "epoch": 0.00032355030143106265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6818,
      "step": 89152
    },
    {
      "epoch": 0.00032366643578189935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 89184
    },
    {
      "epoch": 0.00032378257013273605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 89216
    },
    {
      "epoch": 0.00032389870448357275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6843,
      "step": 89248
    },
    {
      "epoch": 0.00032401483883440945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 89280
    },
    {
      "epoch": 0.00032413097318524615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 89312
    },
    {
      "epoch": 0.00032424710753608285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 89344
    },
    {
      "epoch": 0.00032436324188691955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 89376
    },
    {
      "epoch": 0.0003244793762377563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 89408
    },
    {
      "epoch": 0.000324595510588593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 89440
    },
    {
      "epoch": 0.0003247116449394297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 89472
    },
    {
      "epoch": 0.0003248277792902664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 89504
    },
    {
      "epoch": 0.0003249439136411031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7494,
      "step": 89536
    },
    {
      "epoch": 0.0003250600479919398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 89568
    },
    {
      "epoch": 0.0003251761823427765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.755,
      "step": 89600
    },
    {
      "epoch": 0.0003252923166936132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 89632
    },
    {
      "epoch": 0.0003254084510444499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6919,
      "step": 89664
    },
    {
      "epoch": 0.00032552458539528666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 89696
    },
    {
      "epoch": 0.00032564071974612336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 89728
    },
    {
      "epoch": 0.00032575685409696006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 89760
    },
    {
      "epoch": 0.00032587298844779676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 89792
    },
    {
      "epoch": 0.00032598912279863346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 89824
    },
    {
      "epoch": 0.00032610525714947016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 89856
    },
    {
      "epoch": 0.00032622139150030686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 89888
    },
    {
      "epoch": 0.00032633752585114356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 89920
    },
    {
      "epoch": 0.00032645366020198026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 89952
    },
    {
      "epoch": 0.000326569794552817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 89984
    },
    {
      "epoch": 0.0003266859289036537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.693,
      "step": 90016
    },
    {
      "epoch": 0.0003268020632544904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6793,
      "step": 90048
    },
    {
      "epoch": 0.0003269181976053271,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6886,
      "step": 90080
    },
    {
      "epoch": 0.0003270343319561638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 90112
    },
    {
      "epoch": 0.0003271504663070005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 90144
    },
    {
      "epoch": 0.0003272666006578372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6923,
      "step": 90176
    },
    {
      "epoch": 0.0003273827350086739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 90208
    },
    {
      "epoch": 0.0003274988693595106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 90240
    },
    {
      "epoch": 0.00032761500371034736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 90272
    },
    {
      "epoch": 0.00032773113806118406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 90304
    },
    {
      "epoch": 0.00032784727241202076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 90336
    },
    {
      "epoch": 0.00032796340676285746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7493,
      "step": 90368
    },
    {
      "epoch": 0.00032807954111369416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 90400
    },
    {
      "epoch": 0.00032819567546453086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 90432
    },
    {
      "epoch": 0.00032831180981536756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 90464
    },
    {
      "epoch": 0.00032842794416620426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 90496
    },
    {
      "epoch": 0.00032854407851704096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 90528
    },
    {
      "epoch": 0.0003286602128678777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 90560
    },
    {
      "epoch": 0.0003287763472187144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 90592
    },
    {
      "epoch": 0.0003288924815695511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 90624
    },
    {
      "epoch": 0.0003290086159203878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6836,
      "step": 90656
    },
    {
      "epoch": 0.0003291247502712245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6874,
      "step": 90688
    },
    {
      "epoch": 0.0003292408846220612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 90720
    },
    {
      "epoch": 0.0003293570189728979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 90752
    },
    {
      "epoch": 0.0003294731533237346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 90784
    },
    {
      "epoch": 0.0003295892876745713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.686,
      "step": 90816
    },
    {
      "epoch": 0.00032970542202540807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 90848
    },
    {
      "epoch": 0.00032982155637624477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 90880
    },
    {
      "epoch": 0.00032993769072708147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 90912
    },
    {
      "epoch": 0.00033005382507791817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 90944
    },
    {
      "epoch": 0.00033016995942875487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 90976
    },
    {
      "epoch": 0.00033028609377959157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 91008
    },
    {
      "epoch": 0.00033040222813042827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 91040
    },
    {
      "epoch": 0.00033051836248126497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 91072
    },
    {
      "epoch": 0.00033063449683210167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 91104
    },
    {
      "epoch": 0.0003307506311829384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 91136
    },
    {
      "epoch": 0.0003308667655337751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 91168
    },
    {
      "epoch": 0.0003309828998846118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 91200
    },
    {
      "epoch": 0.0003310990342354485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 91232
    },
    {
      "epoch": 0.0003312151685862852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 91264
    },
    {
      "epoch": 0.0003313313029371219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 91296
    },
    {
      "epoch": 0.0003314474372879586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7519,
      "step": 91328
    },
    {
      "epoch": 0.0003315635716387953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 91360
    },
    {
      "epoch": 0.000331679705989632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 91392
    },
    {
      "epoch": 0.0003317958403404688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6816,
      "step": 91424
    },
    {
      "epoch": 0.0003319119746913055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 91456
    },
    {
      "epoch": 0.0003320281090421422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 91488
    },
    {
      "epoch": 0.0003321442433929789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 91520
    },
    {
      "epoch": 0.0003322603777438156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 91552
    },
    {
      "epoch": 0.0003323765120946523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 91584
    },
    {
      "epoch": 0.000332492646445489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 91616
    },
    {
      "epoch": 0.0003326087807963257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 91648
    },
    {
      "epoch": 0.0003327249151471624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 91680
    },
    {
      "epoch": 0.00033284104949799913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 91712
    },
    {
      "epoch": 0.00033295718384883583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 91744
    },
    {
      "epoch": 0.00033307331819967253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 91776
    },
    {
      "epoch": 0.00033318945255050923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 91808
    },
    {
      "epoch": 0.00033330558690134593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 91840
    },
    {
      "epoch": 0.00033342172125218263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 91872
    },
    {
      "epoch": 0.00033353785560301933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 91904
    },
    {
      "epoch": 0.00033365398995385603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 91936
    },
    {
      "epoch": 0.00033377012430469273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 91968
    },
    {
      "epoch": 0.0003338862586555295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 92000
    },
    {
      "epoch": 0.0003340023930063662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 92032
    },
    {
      "epoch": 0.0003341185273572029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 92064
    },
    {
      "epoch": 0.0003342346617080396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 92096
    },
    {
      "epoch": 0.0003343507960588763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 92128
    },
    {
      "epoch": 0.000334466930409713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 92160
    },
    {
      "epoch": 0.0003345830647605497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 92192
    },
    {
      "epoch": 0.0003346991991113864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6836,
      "step": 92224
    },
    {
      "epoch": 0.0003348153334622231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 92256
    },
    {
      "epoch": 0.00033493146781305984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 92288
    },
    {
      "epoch": 0.00033504760216389654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6839,
      "step": 92320
    },
    {
      "epoch": 0.00033516373651473324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6886,
      "step": 92352
    },
    {
      "epoch": 0.00033527987086556994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 92384
    },
    {
      "epoch": 0.00033539600521640664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 92416
    },
    {
      "epoch": 0.00033551213956724334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 92448
    },
    {
      "epoch": 0.00033562827391808004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 92480
    },
    {
      "epoch": 0.00033574440826891674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 92512
    },
    {
      "epoch": 0.00033586054261975344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 92544
    },
    {
      "epoch": 0.0003359766769705902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 92576
    },
    {
      "epoch": 0.0003360928113214269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 92608
    },
    {
      "epoch": 0.0003362089456722636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 92640
    },
    {
      "epoch": 0.0003363250800231003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 92672
    },
    {
      "epoch": 0.000336441214373937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 92704
    },
    {
      "epoch": 0.0003365573487247737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 92736
    },
    {
      "epoch": 0.0003366734830756104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 92768
    },
    {
      "epoch": 0.0003367896174264471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 92800
    },
    {
      "epoch": 0.0003369057517772838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 92832
    },
    {
      "epoch": 0.00033702188612812054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 92864
    },
    {
      "epoch": 0.00033713802047895724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 92896
    },
    {
      "epoch": 0.00033725415482979394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 92928
    },
    {
      "epoch": 0.00033737028918063064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 92960
    },
    {
      "epoch": 0.00033748642353146734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 92992
    },
    {
      "epoch": 0.00033760255788230404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 93024
    },
    {
      "epoch": 0.00033771869223314074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 93056
    },
    {
      "epoch": 0.00033783482658397744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6899,
      "step": 93088
    },
    {
      "epoch": 0.00033795096093481414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.686,
      "step": 93120
    },
    {
      "epoch": 0.0003380670952856509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 93152
    },
    {
      "epoch": 0.0003381832296364876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 93184
    },
    {
      "epoch": 0.0003382993639873243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 93216
    },
    {
      "epoch": 0.000338415498338161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 93248
    },
    {
      "epoch": 0.0003385316326889977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 93280
    },
    {
      "epoch": 0.0003386477670398344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 93312
    },
    {
      "epoch": 0.0003387639013906711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 93344
    },
    {
      "epoch": 0.0003388800357415078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 93376
    },
    {
      "epoch": 0.0003389961700923445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 93408
    },
    {
      "epoch": 0.00033911230444318125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 93440
    },
    {
      "epoch": 0.00033922843879401795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 93472
    },
    {
      "epoch": 0.00033934457314485465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 93504
    },
    {
      "epoch": 0.00033946070749569135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 93536
    },
    {
      "epoch": 0.00033957684184652805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 93568
    },
    {
      "epoch": 0.00033969297619736475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 93600
    },
    {
      "epoch": 0.00033980911054820145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 93632
    },
    {
      "epoch": 0.00033992524489903815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 93664
    },
    {
      "epoch": 0.00034004137924987485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 93696
    },
    {
      "epoch": 0.0003401575136007116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 93728
    },
    {
      "epoch": 0.0003402736479515483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 93760
    },
    {
      "epoch": 0.000340389782302385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 93792
    },
    {
      "epoch": 0.0003405059166532217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 93824
    },
    {
      "epoch": 0.0003406220510040584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 93856
    },
    {
      "epoch": 0.0003407381853548951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 93888
    },
    {
      "epoch": 0.0003408543197057318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 93920
    },
    {
      "epoch": 0.0003409704540565685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 93952
    },
    {
      "epoch": 0.0003410865884074052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 93984
    },
    {
      "epoch": 0.00034120272275824196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 94016
    },
    {
      "epoch": 0.00034131885710907866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 94048
    },
    {
      "epoch": 0.00034143499145991536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 94080
    },
    {
      "epoch": 0.00034155112581075206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 94112
    },
    {
      "epoch": 0.00034166726016158876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 94144
    },
    {
      "epoch": 0.00034178339451242546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 94176
    },
    {
      "epoch": 0.00034189952886326216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 94208
    },
    {
      "epoch": 0.00034201566321409886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 94240
    },
    {
      "epoch": 0.00034213179756493556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 94272
    },
    {
      "epoch": 0.00034224793191577226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 94304
    },
    {
      "epoch": 0.000342364066266609,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 94336
    },
    {
      "epoch": 0.0003424802006174457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 94368
    },
    {
      "epoch": 0.0003425963349682824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 94400
    },
    {
      "epoch": 0.0003427124693191191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 94432
    },
    {
      "epoch": 0.0003428286036699558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6837,
      "step": 94464
    },
    {
      "epoch": 0.0003429447380207925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 94496
    },
    {
      "epoch": 0.0003430608723716292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 94528
    },
    {
      "epoch": 0.0003431770067224659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 94560
    },
    {
      "epoch": 0.0003432931410733026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 94592
    },
    {
      "epoch": 0.00034340927542413937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 94624
    },
    {
      "epoch": 0.00034352540977497606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 94656
    },
    {
      "epoch": 0.00034364154412581276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 94688
    },
    {
      "epoch": 0.00034375767847664946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 94720
    },
    {
      "epoch": 0.00034387381282748616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 94752
    },
    {
      "epoch": 0.00034398994717832286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 94784
    },
    {
      "epoch": 0.00034410608152915956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 94816
    },
    {
      "epoch": 0.00034422221587999626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 94848
    },
    {
      "epoch": 0.00034433835023083296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 94880
    },
    {
      "epoch": 0.0003444544845816697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 94912
    },
    {
      "epoch": 0.0003445706189325064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 94944
    },
    {
      "epoch": 0.0003446867532833431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 94976
    },
    {
      "epoch": 0.0003448028876341798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 95008
    },
    {
      "epoch": 0.0003449190219850165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 95040
    },
    {
      "epoch": 0.0003450351563358532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 95072
    },
    {
      "epoch": 0.0003451512906866899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 95104
    },
    {
      "epoch": 0.0003452674250375266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 95136
    },
    {
      "epoch": 0.0003453835593883633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 95168
    },
    {
      "epoch": 0.00034549969373920007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 95200
    },
    {
      "epoch": 0.00034561582809003677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 95232
    },
    {
      "epoch": 0.00034573196244087347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6795,
      "step": 95264
    },
    {
      "epoch": 0.00034584809679171017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 95296
    },
    {
      "epoch": 0.00034596423114254687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6799,
      "step": 95328
    },
    {
      "epoch": 0.00034608036549338357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 95360
    },
    {
      "epoch": 0.00034619649984422027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 95392
    },
    {
      "epoch": 0.00034631263419505697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 95424
    },
    {
      "epoch": 0.00034642876854589367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 95456
    },
    {
      "epoch": 0.0003465449028967304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 95488
    },
    {
      "epoch": 0.0003466610372475671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 95520
    },
    {
      "epoch": 0.0003467771715984038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 95552
    },
    {
      "epoch": 0.0003468933059492405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 95584
    },
    {
      "epoch": 0.0003470094403000772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 95616
    },
    {
      "epoch": 0.0003471255746509139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 95648
    },
    {
      "epoch": 0.0003472417090017506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 95680
    },
    {
      "epoch": 0.0003473578433525873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 95712
    },
    {
      "epoch": 0.000347473977703424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 95744
    },
    {
      "epoch": 0.0003475901120542608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 95776
    },
    {
      "epoch": 0.0003477062464050975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 95808
    },
    {
      "epoch": 0.0003478223807559342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 95840
    },
    {
      "epoch": 0.0003479385151067709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 95872
    },
    {
      "epoch": 0.0003480546494576076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 95904
    },
    {
      "epoch": 0.0003481707838084443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 95936
    },
    {
      "epoch": 0.000348286918159281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 95968
    },
    {
      "epoch": 0.0003484030525101177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6904,
      "step": 96000
    },
    {
      "epoch": 0.0003485191868609544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6887,
      "step": 96032
    },
    {
      "epoch": 0.00034863532121179113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 96064
    },
    {
      "epoch": 0.00034875145556262783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 96096
    },
    {
      "epoch": 0.00034886758991346453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 96128
    },
    {
      "epoch": 0.00034898372426430123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 96160
    },
    {
      "epoch": 0.00034909985861513793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 96192
    },
    {
      "epoch": 0.00034921599296597463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 96224
    },
    {
      "epoch": 0.00034933212731681133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 96256
    },
    {
      "epoch": 0.00034944826166764803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 96288
    },
    {
      "epoch": 0.00034956439601848473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 96320
    },
    {
      "epoch": 0.0003496805303693215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 96352
    },
    {
      "epoch": 0.0003497966647201582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 96384
    },
    {
      "epoch": 0.0003499127990709949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7409,
      "step": 96416
    },
    {
      "epoch": 0.0003500289334218316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 96448
    },
    {
      "epoch": 0.0003501450677726683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 96480
    },
    {
      "epoch": 0.000350261202123505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 96512
    },
    {
      "epoch": 0.0003503773364743417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 96544
    },
    {
      "epoch": 0.0003504934708251784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 96576
    },
    {
      "epoch": 0.0003506096051760151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 96608
    },
    {
      "epoch": 0.00035072573952685184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 96640
    },
    {
      "epoch": 0.00035084187387768854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 96672
    },
    {
      "epoch": 0.00035095800822852524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 96704
    },
    {
      "epoch": 0.00035107414257936194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 96736
    },
    {
      "epoch": 0.00035119027693019864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 96768
    },
    {
      "epoch": 0.00035130641128103534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.68,
      "step": 96800
    },
    {
      "epoch": 0.00035142254563187204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 96832
    },
    {
      "epoch": 0.00035153867998270874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 96864
    },
    {
      "epoch": 0.00035165481433354544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 96896
    },
    {
      "epoch": 0.0003517709486843822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 96928
    },
    {
      "epoch": 0.0003518870830352189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 96960
    },
    {
      "epoch": 0.0003520032173860556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 96992
    },
    {
      "epoch": 0.0003521193517368923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 97024
    },
    {
      "epoch": 0.000352235486087729,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 97056
    },
    {
      "epoch": 0.0003523516204385657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 97088
    },
    {
      "epoch": 0.0003524677547894024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 97120
    },
    {
      "epoch": 0.0003525838891402391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 97152
    },
    {
      "epoch": 0.0003527000234910758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7452,
      "step": 97184
    },
    {
      "epoch": 0.00035281615784191255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 97216
    },
    {
      "epoch": 0.00035293229219274925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 97248
    },
    {
      "epoch": 0.00035304842654358595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 97280
    },
    {
      "epoch": 0.00035316456089442265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 97312
    },
    {
      "epoch": 0.00035328069524525935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 97344
    },
    {
      "epoch": 0.00035339682959609605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 97376
    },
    {
      "epoch": 0.00035351296394693275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 97408
    },
    {
      "epoch": 0.00035362909829776945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 97440
    },
    {
      "epoch": 0.00035374523264860615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.693,
      "step": 97472
    },
    {
      "epoch": 0.0003538613669994429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6915,
      "step": 97504
    },
    {
      "epoch": 0.0003539775013502796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6861,
      "step": 97536
    },
    {
      "epoch": 0.0003540936357011163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 97568
    },
    {
      "epoch": 0.000354209770051953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 97600
    },
    {
      "epoch": 0.0003543259044027897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6842,
      "step": 97632
    },
    {
      "epoch": 0.0003544420387536264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 97664
    },
    {
      "epoch": 0.0003545581731044631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 97696
    },
    {
      "epoch": 0.0003546743074552998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 97728
    },
    {
      "epoch": 0.0003547904418061365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 97760
    },
    {
      "epoch": 0.00035490657615697325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 97792
    },
    {
      "epoch": 0.00035502271050780995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 97824
    },
    {
      "epoch": 0.00035513884485864665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 97856
    },
    {
      "epoch": 0.00035525497920948335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 97888
    },
    {
      "epoch": 0.00035537111356032005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 97920
    },
    {
      "epoch": 0.00035548724791115675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 97952
    },
    {
      "epoch": 0.00035560338226199345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 97984
    },
    {
      "epoch": 0.00035571951661283015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 98016
    },
    {
      "epoch": 0.00035583565096366685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 98048
    },
    {
      "epoch": 0.0003559517853145036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 98080
    },
    {
      "epoch": 0.0003560679196653403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 98112
    },
    {
      "epoch": 0.000356184054016177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 98144
    },
    {
      "epoch": 0.0003563001883670137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 98176
    },
    {
      "epoch": 0.0003564163227178504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 98208
    },
    {
      "epoch": 0.0003565324570686871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.679,
      "step": 98240
    },
    {
      "epoch": 0.0003566485914195238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 98272
    },
    {
      "epoch": 0.0003567647257703605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6861,
      "step": 98304
    },
    {
      "epoch": 0.0003568808601211972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6783,
      "step": 98336
    },
    {
      "epoch": 0.00035699699447203396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 98368
    },
    {
      "epoch": 0.00035711312882287066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 98400
    },
    {
      "epoch": 0.00035722926317370736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 98432
    },
    {
      "epoch": 0.00035734539752454406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 98464
    },
    {
      "epoch": 0.00035746153187538076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 98496
    },
    {
      "epoch": 0.00035757766622621746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 98528
    },
    {
      "epoch": 0.00035769380057705416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 98560
    },
    {
      "epoch": 0.00035780993492789086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 98592
    },
    {
      "epoch": 0.00035792606927872756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 98624
    },
    {
      "epoch": 0.0003580422036295643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 98656
    },
    {
      "epoch": 0.000358158337980401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 98688
    },
    {
      "epoch": 0.0003582744723312377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 98720
    },
    {
      "epoch": 0.0003583906066820744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 98752
    },
    {
      "epoch": 0.0003585067410329111,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 98784
    },
    {
      "epoch": 0.0003586228753837478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 98816
    },
    {
      "epoch": 0.0003587390097345845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 98848
    },
    {
      "epoch": 0.0003588551440854212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 98880
    },
    {
      "epoch": 0.0003589712784362579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 98912
    },
    {
      "epoch": 0.00035908741278709467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 98944
    },
    {
      "epoch": 0.00035920354713793137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 98976
    },
    {
      "epoch": 0.00035931968148876807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 99008
    },
    {
      "epoch": 0.00035943581583960477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 99040
    },
    {
      "epoch": 0.00035955195019044147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 99072
    },
    {
      "epoch": 0.00035966808454127817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6811,
      "step": 99104
    },
    {
      "epoch": 0.00035978421889211487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 99136
    },
    {
      "epoch": 0.00035990035324295157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6882,
      "step": 99168
    },
    {
      "epoch": 0.00036001648759378827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 99200
    },
    {
      "epoch": 0.000360132621944625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 99232
    },
    {
      "epoch": 0.0003602487562954617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 99264
    },
    {
      "epoch": 0.0003603648906462984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 99296
    },
    {
      "epoch": 0.0003604810249971351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 99328
    },
    {
      "epoch": 0.0003605971593479718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 99360
    },
    {
      "epoch": 0.0003607132936988085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 99392
    },
    {
      "epoch": 0.0003608294280496452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 99424
    },
    {
      "epoch": 0.0003609455624004819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 99456
    },
    {
      "epoch": 0.0003610616967513186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 99488
    },
    {
      "epoch": 0.0003611778311021554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 99520
    },
    {
      "epoch": 0.0003612939654529921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 99552
    },
    {
      "epoch": 0.0003614100998038288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6914,
      "step": 99584
    },
    {
      "epoch": 0.00036152623415466547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 99616
    },
    {
      "epoch": 0.00036164236850550217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 99648
    },
    {
      "epoch": 0.00036175850285633887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 99680
    },
    {
      "epoch": 0.00036187463720717557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 99712
    },
    {
      "epoch": 0.00036199077155801227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 99744
    },
    {
      "epoch": 0.00036210690590884897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 99776
    },
    {
      "epoch": 0.00036222304025968567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 99808
    },
    {
      "epoch": 0.0003623391746105224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 99840
    },
    {
      "epoch": 0.0003624553089613591,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 99872
    },
    {
      "epoch": 0.0003625714433121958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 99904
    },
    {
      "epoch": 0.0003626875776630325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 99936
    },
    {
      "epoch": 0.0003628037120138692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 99968
    },
    {
      "epoch": 0.0003629198463647059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 100000
    },
    {
      "epoch": 0.0003630359807155426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 100032
    },
    {
      "epoch": 0.0003631521150663793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 100064
    },
    {
      "epoch": 0.000363268249417216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 100096
    },
    {
      "epoch": 0.0003633843837680528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 100128
    },
    {
      "epoch": 0.0003635005181188895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 100160
    },
    {
      "epoch": 0.0003636166524697262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 100192
    },
    {
      "epoch": 0.0003637327868205629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 100224
    },
    {
      "epoch": 0.0003638489211713996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 100256
    },
    {
      "epoch": 0.0003639650555222363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 100288
    },
    {
      "epoch": 0.000364081189873073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 100320
    },
    {
      "epoch": 0.0003641973242239097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 100352
    },
    {
      "epoch": 0.0003643134585747464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 100384
    },
    {
      "epoch": 0.00036442959292558313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 100416
    },
    {
      "epoch": 0.00036454572727641983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 100448
    },
    {
      "epoch": 0.00036466186162725653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 100480
    },
    {
      "epoch": 0.00036477799597809323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 100512
    },
    {
      "epoch": 0.00036489413032892993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 100544
    },
    {
      "epoch": 0.00036501026467976663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 100576
    },
    {
      "epoch": 0.00036512639903060333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 100608
    },
    {
      "epoch": 0.00036524253338144003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 100640
    },
    {
      "epoch": 0.00036535866773227673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 100672
    },
    {
      "epoch": 0.0003654748020831135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 100704
    },
    {
      "epoch": 0.0003655909364339502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 100736
    },
    {
      "epoch": 0.0003657070707847869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 100768
    },
    {
      "epoch": 0.0003658232051356236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 100800
    },
    {
      "epoch": 0.0003659393394864603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6936,
      "step": 100832
    },
    {
      "epoch": 0.000366055473837297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 100864
    },
    {
      "epoch": 0.0003661716081881337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 100896
    },
    {
      "epoch": 0.0003662877425389704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 100928
    },
    {
      "epoch": 0.0003664038768898071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 100960
    },
    {
      "epoch": 0.00036652001124064384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 100992
    },
    {
      "epoch": 0.00036663614559148054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 101024
    },
    {
      "epoch": 0.00036675227994231724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 101056
    },
    {
      "epoch": 0.00036686841429315394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 101088
    },
    {
      "epoch": 0.00036698454864399064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 101120
    },
    {
      "epoch": 0.00036710068299482734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 101152
    },
    {
      "epoch": 0.00036721681734566404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 101184
    },
    {
      "epoch": 0.00036733295169650074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 101216
    },
    {
      "epoch": 0.00036744908604733744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6899,
      "step": 101248
    },
    {
      "epoch": 0.0003675652203981742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 101280
    },
    {
      "epoch": 0.0003676813547490109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 101312
    },
    {
      "epoch": 0.0003677974890998476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 101344
    },
    {
      "epoch": 0.0003679136234506843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 101376
    },
    {
      "epoch": 0.000368029757801521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 101408
    },
    {
      "epoch": 0.0003681458921523577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 101440
    },
    {
      "epoch": 0.0003682620265031944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 101472
    },
    {
      "epoch": 0.0003683781608540311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 101504
    },
    {
      "epoch": 0.0003684942952048678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 101536
    },
    {
      "epoch": 0.00036861042955570455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 101568
    },
    {
      "epoch": 0.00036872656390654125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 101600
    },
    {
      "epoch": 0.00036884269825737795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 101632
    },
    {
      "epoch": 0.00036895883260821465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 101664
    },
    {
      "epoch": 0.00036907496695905135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 101696
    },
    {
      "epoch": 0.00036919110130988805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 101728
    },
    {
      "epoch": 0.00036930723566072475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 101760
    },
    {
      "epoch": 0.00036942337001156145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 101792
    },
    {
      "epoch": 0.00036953950436239815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 101824
    },
    {
      "epoch": 0.0003696556387132349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 101856
    },
    {
      "epoch": 0.0003697717730640716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 101888
    },
    {
      "epoch": 0.0003698879074149083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 101920
    },
    {
      "epoch": 0.000370004041765745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 101952
    },
    {
      "epoch": 0.0003701201761165817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 101984
    },
    {
      "epoch": 0.0003702363104674184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6728,
      "step": 102016
    },
    {
      "epoch": 0.0003703524448182551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6871,
      "step": 102048
    },
    {
      "epoch": 0.0003704685791690918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 102080
    },
    {
      "epoch": 0.0003705847135199285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 102112
    },
    {
      "epoch": 0.00037070084787076525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6956,
      "step": 102144
    },
    {
      "epoch": 0.00037081698222160195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6913,
      "step": 102176
    },
    {
      "epoch": 0.00037093311657243865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6846,
      "step": 102208
    },
    {
      "epoch": 0.00037104925092327535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 102240
    },
    {
      "epoch": 0.00037116538527411205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7471,
      "step": 102272
    },
    {
      "epoch": 0.00037128151962494875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 102304
    },
    {
      "epoch": 0.00037139765397578545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 102336
    },
    {
      "epoch": 0.00037151378832662215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 102368
    },
    {
      "epoch": 0.00037162992267745885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 102400
    },
    {
      "epoch": 0.0003717460570282956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 102432
    },
    {
      "epoch": 0.0003718621913791323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 102464
    },
    {
      "epoch": 0.000371978325729969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7512,
      "step": 102496
    },
    {
      "epoch": 0.0003720944600808057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 102528
    },
    {
      "epoch": 0.0003722105944316424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 102560
    },
    {
      "epoch": 0.0003723267287824791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 102592
    },
    {
      "epoch": 0.0003724428631333158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 102624
    },
    {
      "epoch": 0.0003725589974841525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 102656
    },
    {
      "epoch": 0.0003726751318349892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 102688
    },
    {
      "epoch": 0.00037279126618582596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 102720
    },
    {
      "epoch": 0.00037290740053666266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 102752
    },
    {
      "epoch": 0.00037302353488749936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 102784
    },
    {
      "epoch": 0.00037313966923833606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6885,
      "step": 102816
    },
    {
      "epoch": 0.00037325580358917276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 102848
    },
    {
      "epoch": 0.00037337193794000946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 102880
    },
    {
      "epoch": 0.00037348807229084616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.687,
      "step": 102912
    },
    {
      "epoch": 0.00037360420664168286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6826,
      "step": 102944
    },
    {
      "epoch": 0.00037372034099251956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.688,
      "step": 102976
    },
    {
      "epoch": 0.0003738364753433563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 103008
    },
    {
      "epoch": 0.000373952609694193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 103040
    },
    {
      "epoch": 0.0003740687440450297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 103072
    },
    {
      "epoch": 0.0003741848783958664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 103104
    },
    {
      "epoch": 0.0003743010127467031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 103136
    },
    {
      "epoch": 0.0003744171470975398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 103168
    },
    {
      "epoch": 0.0003745332814483765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 103200
    },
    {
      "epoch": 0.0003746494157992132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 103232
    },
    {
      "epoch": 0.0003747655501500499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7501,
      "step": 103264
    },
    {
      "epoch": 0.00037488168450088667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 103296
    },
    {
      "epoch": 0.00037499781885172337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 103328
    },
    {
      "epoch": 0.00037511395320256007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 103360
    },
    {
      "epoch": 0.00037523008755339677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 103392
    },
    {
      "epoch": 0.00037534622190423347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 103424
    },
    {
      "epoch": 0.00037546235625507017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 103456
    },
    {
      "epoch": 0.00037557849060590687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 103488
    },
    {
      "epoch": 0.00037569462495674357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 103520
    },
    {
      "epoch": 0.00037581075930758027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6731,
      "step": 103552
    },
    {
      "epoch": 0.000375926893658417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 103584
    },
    {
      "epoch": 0.0003760430280092537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 103616
    },
    {
      "epoch": 0.0003761591623600904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 103648
    },
    {
      "epoch": 0.0003762752967109271,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6895,
      "step": 103680
    },
    {
      "epoch": 0.0003763914310617638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 103712
    },
    {
      "epoch": 0.0003765075654126005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 103744
    },
    {
      "epoch": 0.0003766236997634372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 103776
    },
    {
      "epoch": 0.0003767398341142739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 103808
    },
    {
      "epoch": 0.0003768559684651106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 103840
    },
    {
      "epoch": 0.0003769721028159474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 103872
    },
    {
      "epoch": 0.0003770882371667841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 103904
    },
    {
      "epoch": 0.0003772043715176208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 103936
    },
    {
      "epoch": 0.0003773205058684575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 103968
    },
    {
      "epoch": 0.0003774366402192942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 104000
    },
    {
      "epoch": 0.0003775527745701309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7505,
      "step": 104032
    },
    {
      "epoch": 0.0003776689089209676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 104064
    },
    {
      "epoch": 0.0003777850432718043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 104096
    },
    {
      "epoch": 0.000377901177622641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 104128
    },
    {
      "epoch": 0.00037801731197347773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 104160
    },
    {
      "epoch": 0.00037813344632431443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 104192
    },
    {
      "epoch": 0.00037824958067515113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 104224
    },
    {
      "epoch": 0.00037836571502598783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 104256
    },
    {
      "epoch": 0.00037848184937682453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6836,
      "step": 104288
    },
    {
      "epoch": 0.00037859798372766123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 104320
    },
    {
      "epoch": 0.0003787141180784979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6919,
      "step": 104352
    },
    {
      "epoch": 0.0003788302524293346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.687,
      "step": 104384
    },
    {
      "epoch": 0.0003789463867801713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 104416
    },
    {
      "epoch": 0.0003790625211310081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6878,
      "step": 104448
    },
    {
      "epoch": 0.0003791786554818448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 104480
    },
    {
      "epoch": 0.0003792947898326815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 104512
    },
    {
      "epoch": 0.0003794109241835182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 104544
    },
    {
      "epoch": 0.0003795270585343549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 104576
    },
    {
      "epoch": 0.0003796431928851916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 104608
    },
    {
      "epoch": 0.0003797593272360283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 104640
    },
    {
      "epoch": 0.000379875461586865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 104672
    },
    {
      "epoch": 0.0003799915959377017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 104704
    },
    {
      "epoch": 0.00038010773028853843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 104736
    },
    {
      "epoch": 0.00038022386463937513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 104768
    },
    {
      "epoch": 0.00038033999899021183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 104800
    },
    {
      "epoch": 0.00038045613334104853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 104832
    },
    {
      "epoch": 0.00038057226769188523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 104864
    },
    {
      "epoch": 0.00038068840204272193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 104896
    },
    {
      "epoch": 0.00038080453639355863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 104928
    },
    {
      "epoch": 0.00038092067074439533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 104960
    },
    {
      "epoch": 0.00038103680509523203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 104992
    },
    {
      "epoch": 0.00038115293944606873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 105024
    },
    {
      "epoch": 0.0003812690737969055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 105056
    },
    {
      "epoch": 0.0003813852081477422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 105088
    },
    {
      "epoch": 0.0003815013424985789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 105120
    },
    {
      "epoch": 0.0003816174768494156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.692,
      "step": 105152
    },
    {
      "epoch": 0.0003817336112002523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 105184
    },
    {
      "epoch": 0.000381849745551089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 105216
    },
    {
      "epoch": 0.0003819658799019257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 105248
    },
    {
      "epoch": 0.0003820820142527624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 105280
    },
    {
      "epoch": 0.0003821981486035991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 105312
    },
    {
      "epoch": 0.00038231428295443584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 105344
    },
    {
      "epoch": 0.00038243041730527254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 105376
    },
    {
      "epoch": 0.00038254655165610924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 105408
    },
    {
      "epoch": 0.00038266268600694594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 105440
    },
    {
      "epoch": 0.00038277882035778264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 105472
    },
    {
      "epoch": 0.00038289495470861934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 105504
    },
    {
      "epoch": 0.00038301108905945604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 105536
    },
    {
      "epoch": 0.00038312722341029274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 105568
    },
    {
      "epoch": 0.00038324335776112944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 105600
    },
    {
      "epoch": 0.0003833594921119662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 105632
    },
    {
      "epoch": 0.0003834756264628029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 105664
    },
    {
      "epoch": 0.0003835917608136396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 105696
    },
    {
      "epoch": 0.0003837078951644763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 105728
    },
    {
      "epoch": 0.000383824029515313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 105760
    },
    {
      "epoch": 0.0003839401638661497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 105792
    },
    {
      "epoch": 0.0003840562982169864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 105824
    },
    {
      "epoch": 0.0003841724325678231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 105856
    },
    {
      "epoch": 0.0003842885669186598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 105888
    },
    {
      "epoch": 0.00038440470126949655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 105920
    },
    {
      "epoch": 0.00038452083562033325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.693,
      "step": 105952
    },
    {
      "epoch": 0.00038463696997116995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.683,
      "step": 105984
    },
    {
      "epoch": 0.00038475310432200665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.69,
      "step": 106016
    },
    {
      "epoch": 0.00038486923867284335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 106048
    },
    {
      "epoch": 0.00038498537302368005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 106080
    },
    {
      "epoch": 0.00038510150737451675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 106112
    },
    {
      "epoch": 0.00038521764172535345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 106144
    },
    {
      "epoch": 0.00038533377607619015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 106176
    },
    {
      "epoch": 0.0003854499104270269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 106208
    },
    {
      "epoch": 0.0003855660447778636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 106240
    },
    {
      "epoch": 0.0003856821791287003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 106272
    },
    {
      "epoch": 0.000385798313479537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 106304
    },
    {
      "epoch": 0.0003859144478303737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 106336
    },
    {
      "epoch": 0.0003860305821812104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 106368
    },
    {
      "epoch": 0.0003861467165320471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 106400
    },
    {
      "epoch": 0.0003862628508828838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 106432
    },
    {
      "epoch": 0.0003863789852337205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 106464
    },
    {
      "epoch": 0.00038649511958455725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 106496
    },
    {
      "epoch": 0.00038661125393539395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 106528
    },
    {
      "epoch": 0.00038672738828623065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 106560
    },
    {
      "epoch": 0.00038684352263706735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 106592
    },
    {
      "epoch": 0.00038695965698790405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 106624
    },
    {
      "epoch": 0.00038707579133874075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 106656
    },
    {
      "epoch": 0.00038719192568957745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 106688
    },
    {
      "epoch": 0.00038730806004041415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 106720
    },
    {
      "epoch": 0.00038742419439125085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 106752
    },
    {
      "epoch": 0.0003875403287420876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 106784
    },
    {
      "epoch": 0.0003876564630929243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 106816
    },
    {
      "epoch": 0.000387772597443761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 106848
    },
    {
      "epoch": 0.0003878887317945977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 106880
    },
    {
      "epoch": 0.0003880048661454344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 106912
    },
    {
      "epoch": 0.0003881210004962711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 106944
    },
    {
      "epoch": 0.0003882371348471078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 106976
    },
    {
      "epoch": 0.0003883532691979445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 107008
    },
    {
      "epoch": 0.0003884694035487812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 107040
    },
    {
      "epoch": 0.00038858553789961796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 107072
    },
    {
      "epoch": 0.00038870167225045466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 107104
    },
    {
      "epoch": 0.00038881780660129136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 107136
    },
    {
      "epoch": 0.00038893394095212806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 107168
    },
    {
      "epoch": 0.00038905007530296476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 107200
    },
    {
      "epoch": 0.00038916620965380146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 107232
    },
    {
      "epoch": 0.00038928234400463816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 107264
    },
    {
      "epoch": 0.00038939847835547486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 107296
    },
    {
      "epoch": 0.00038951461270631156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 107328
    },
    {
      "epoch": 0.0003896307470571483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.687,
      "step": 107360
    },
    {
      "epoch": 0.000389746881407985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 107392
    },
    {
      "epoch": 0.0003898630157588217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 107424
    },
    {
      "epoch": 0.0003899791501096584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 107456
    },
    {
      "epoch": 0.0003900952844604951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 107488
    },
    {
      "epoch": 0.0003902114188113318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 107520
    },
    {
      "epoch": 0.0003903275531621685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 107552
    },
    {
      "epoch": 0.0003904436875130052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7439,
      "step": 107584
    },
    {
      "epoch": 0.0003905598218638419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 107616
    },
    {
      "epoch": 0.00039067595621467867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 107648
    },
    {
      "epoch": 0.00039079209056551537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6934,
      "step": 107680
    },
    {
      "epoch": 0.00039090822491635207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 107712
    },
    {
      "epoch": 0.00039102435926718877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 107744
    },
    {
      "epoch": 0.00039114049361802547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 107776
    },
    {
      "epoch": 0.00039125662796886217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 107808
    },
    {
      "epoch": 0.00039137276231969887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 107840
    },
    {
      "epoch": 0.00039148889667053557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 107872
    },
    {
      "epoch": 0.00039160503102137227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 107904
    },
    {
      "epoch": 0.000391721165372209,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 107936
    },
    {
      "epoch": 0.0003918372997230457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 107968
    },
    {
      "epoch": 0.0003919534340738824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 108000
    },
    {
      "epoch": 0.0003920695684247191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 108032
    },
    {
      "epoch": 0.0003921857027755558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 108064
    },
    {
      "epoch": 0.0003923018371263925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 108096
    },
    {
      "epoch": 0.0003924179714772292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6833,
      "step": 108128
    },
    {
      "epoch": 0.0003925341058280659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 108160
    },
    {
      "epoch": 0.0003926502401789026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 108192
    },
    {
      "epoch": 0.0003927663745297394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 108224
    },
    {
      "epoch": 0.0003928825088805761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6913,
      "step": 108256
    },
    {
      "epoch": 0.0003929986432314128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 108288
    },
    {
      "epoch": 0.0003931147775822495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 108320
    },
    {
      "epoch": 0.0003932309119330862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 108352
    },
    {
      "epoch": 0.0003933470462839229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 108384
    },
    {
      "epoch": 0.0003934631806347596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 108416
    },
    {
      "epoch": 0.0003935793149855963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 108448
    },
    {
      "epoch": 0.000393695449336433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 108480
    },
    {
      "epoch": 0.00039381158368726973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 108512
    },
    {
      "epoch": 0.00039392771803810643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 108544
    },
    {
      "epoch": 0.00039404385238894313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 108576
    },
    {
      "epoch": 0.00039415998673977983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 108608
    },
    {
      "epoch": 0.00039427612109061653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 108640
    },
    {
      "epoch": 0.00039439225544145323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 108672
    },
    {
      "epoch": 0.00039450838979228993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 108704
    },
    {
      "epoch": 0.00039462452414312663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 108736
    },
    {
      "epoch": 0.00039474065849396333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 108768
    },
    {
      "epoch": 0.0003948567928448001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 108800
    },
    {
      "epoch": 0.0003949729271956368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 108832
    },
    {
      "epoch": 0.0003950890615464735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6842,
      "step": 108864
    },
    {
      "epoch": 0.0003952051958973102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6869,
      "step": 108896
    },
    {
      "epoch": 0.0003953213302481469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6922,
      "step": 108928
    },
    {
      "epoch": 0.0003954374645989836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.695,
      "step": 108960
    },
    {
      "epoch": 0.0003955535989498203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 108992
    },
    {
      "epoch": 0.000395669733300657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 109024
    },
    {
      "epoch": 0.0003957858676514937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.682,
      "step": 109056
    },
    {
      "epoch": 0.00039590200200233044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 109088
    },
    {
      "epoch": 0.00039601813635316714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 109120
    },
    {
      "epoch": 0.00039613427070400384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 109152
    },
    {
      "epoch": 0.00039625040505484054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 109184
    },
    {
      "epoch": 0.00039636653940567724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 109216
    },
    {
      "epoch": 0.00039648267375651394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 109248
    },
    {
      "epoch": 0.00039659880810735064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 109280
    },
    {
      "epoch": 0.00039671494245818733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 109312
    },
    {
      "epoch": 0.00039683107680902403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7671,
      "step": 109344
    },
    {
      "epoch": 0.0003969472111598608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 109376
    },
    {
      "epoch": 0.0003970633455106975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 109408
    },
    {
      "epoch": 0.0003971794798615342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 109440
    },
    {
      "epoch": 0.0003972956142123709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 109472
    },
    {
      "epoch": 0.0003974117485632076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 109504
    },
    {
      "epoch": 0.0003975278829140443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 109536
    },
    {
      "epoch": 0.000397644017264881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 109568
    },
    {
      "epoch": 0.0003977601516157177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 109600
    },
    {
      "epoch": 0.0003978762859665544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 109632
    },
    {
      "epoch": 0.00039799242031739114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6765,
      "step": 109664
    },
    {
      "epoch": 0.00039810855466822784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 109696
    },
    {
      "epoch": 0.00039822468901906454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 109728
    },
    {
      "epoch": 0.00039834082336990124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 109760
    },
    {
      "epoch": 0.00039845695772073794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 109792
    },
    {
      "epoch": 0.00039857309207157464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 109824
    },
    {
      "epoch": 0.00039868922642241134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 109856
    },
    {
      "epoch": 0.00039880536077324804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 109888
    },
    {
      "epoch": 0.00039892149512408474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 109920
    },
    {
      "epoch": 0.0003990376294749215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 109952
    },
    {
      "epoch": 0.0003991537638257582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 109984
    },
    {
      "epoch": 0.0003992698981765949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 110016
    },
    {
      "epoch": 0.0003993860325274316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 110048
    },
    {
      "epoch": 0.0003995021668782683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 110080
    },
    {
      "epoch": 0.000399618301229105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 110112
    },
    {
      "epoch": 0.0003997344355799417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 110144
    },
    {
      "epoch": 0.0003998505699307784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 110176
    },
    {
      "epoch": 0.0003999667042816151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 110208
    },
    {
      "epoch": 0.00040008283863245185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 110240
    },
    {
      "epoch": 0.00040019897298328855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6902,
      "step": 110272
    },
    {
      "epoch": 0.00040031510733412525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 110304
    },
    {
      "epoch": 0.00040043124168496195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 110336
    },
    {
      "epoch": 0.00040054737603579865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 110368
    },
    {
      "epoch": 0.00040066351038663535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6905,
      "step": 110400
    },
    {
      "epoch": 0.00040077964473747205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6888,
      "step": 110432
    },
    {
      "epoch": 0.00040089577908830875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 110464
    },
    {
      "epoch": 0.00040101191343914545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 110496
    },
    {
      "epoch": 0.00040112804778998215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 110528
    },
    {
      "epoch": 0.0004012441821408189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 110560
    },
    {
      "epoch": 0.0004013603164916556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 110592
    },
    {
      "epoch": 0.0004014764508424923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 110624
    },
    {
      "epoch": 0.000401592585193329,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 110656
    },
    {
      "epoch": 0.0004017087195441657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 110688
    },
    {
      "epoch": 0.0004018248538950024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 110720
    },
    {
      "epoch": 0.0004019409882458391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 110752
    },
    {
      "epoch": 0.0004020571225966758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 110784
    },
    {
      "epoch": 0.0004021732569475125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 110816
    },
    {
      "epoch": 0.00040228939129834926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 110848
    },
    {
      "epoch": 0.00040240552564918596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 110880
    },
    {
      "epoch": 0.00040252166000002266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 110912
    },
    {
      "epoch": 0.00040263779435085936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 110944
    },
    {
      "epoch": 0.00040275392870169606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 110976
    },
    {
      "epoch": 0.00040287006305253276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 111008
    },
    {
      "epoch": 0.00040298619740336946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 111040
    },
    {
      "epoch": 0.00040310233175420616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 111072
    },
    {
      "epoch": 0.00040321846610504286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 111104
    },
    {
      "epoch": 0.0004033346004558796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 111136
    },
    {
      "epoch": 0.0004034507348067163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6871,
      "step": 111168
    },
    {
      "epoch": 0.000403566869157553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6776,
      "step": 111200
    },
    {
      "epoch": 0.0004036830035083897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6894,
      "step": 111232
    },
    {
      "epoch": 0.0004037991378592264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6911,
      "step": 111264
    },
    {
      "epoch": 0.0004039152722100631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 111296
    },
    {
      "epoch": 0.0004040314065608998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 111328
    },
    {
      "epoch": 0.0004041475409117365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 111360
    },
    {
      "epoch": 0.0004042636752625732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 111392
    },
    {
      "epoch": 0.00040437980961340996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6913,
      "step": 111424
    },
    {
      "epoch": 0.00040449594396424666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 111456
    },
    {
      "epoch": 0.00040461207831508336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 111488
    },
    {
      "epoch": 0.00040472821266592006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 111520
    },
    {
      "epoch": 0.00040484434701675676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 111552
    },
    {
      "epoch": 0.00040496048136759346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 111584
    },
    {
      "epoch": 0.00040507661571843016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 111616
    },
    {
      "epoch": 0.00040519275006926686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 111648
    },
    {
      "epoch": 0.00040530888442010356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 111680
    },
    {
      "epoch": 0.0004054250187709403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 111712
    },
    {
      "epoch": 0.000405541153121777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 111744
    },
    {
      "epoch": 0.0004056572874726137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 111776
    },
    {
      "epoch": 0.0004057734218234504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 111808
    },
    {
      "epoch": 0.0004058895561742871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 111840
    },
    {
      "epoch": 0.0004060056905251238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 111872
    },
    {
      "epoch": 0.0004061218248759605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 111904
    },
    {
      "epoch": 0.0004062379592267972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 111936
    },
    {
      "epoch": 0.0004063540935776339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 111968
    },
    {
      "epoch": 0.00040647022792847067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 112000
    },
    {
      "epoch": 0.00040658636227930737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6823,
      "step": 112032
    },
    {
      "epoch": 0.00040670249663014407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 112064
    },
    {
      "epoch": 0.00040681863098098077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 112096
    },
    {
      "epoch": 0.00040693476533181747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 112128
    },
    {
      "epoch": 0.00040705089968265417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 112160
    },
    {
      "epoch": 0.00040716703403349087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 112192
    },
    {
      "epoch": 0.00040728316838432757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 112224
    },
    {
      "epoch": 0.00040739930273516427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 112256
    },
    {
      "epoch": 0.000407515437086001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 112288
    },
    {
      "epoch": 0.0004076315714368377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 112320
    },
    {
      "epoch": 0.0004077477057876744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 112352
    },
    {
      "epoch": 0.0004078638401385111,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 112384
    },
    {
      "epoch": 0.0004079799744893478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 112416
    },
    {
      "epoch": 0.0004080961088401845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 112448
    },
    {
      "epoch": 0.0004082122431910212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 112480
    },
    {
      "epoch": 0.0004083283775418579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 112512
    },
    {
      "epoch": 0.0004084445118926946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 112544
    },
    {
      "epoch": 0.0004085606462435314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 112576
    },
    {
      "epoch": 0.0004086767805943681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 112608
    },
    {
      "epoch": 0.0004087929149452048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 112640
    },
    {
      "epoch": 0.0004089090492960415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 112672
    },
    {
      "epoch": 0.0004090251836468782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 112704
    },
    {
      "epoch": 0.0004091413179977149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 112736
    },
    {
      "epoch": 0.0004092574523485516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 112768
    },
    {
      "epoch": 0.0004093735866993883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 112800
    },
    {
      "epoch": 0.000409489721050225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6865,
      "step": 112832
    },
    {
      "epoch": 0.00040960585540106173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 112864
    },
    {
      "epoch": 0.00040972198975189843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 112896
    },
    {
      "epoch": 0.00040983812410273513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 112928
    },
    {
      "epoch": 0.00040995425845357183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 112960
    },
    {
      "epoch": 0.00041007039280440853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 112992
    },
    {
      "epoch": 0.00041018652715524523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 113024
    },
    {
      "epoch": 0.00041030266150608193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 113056
    },
    {
      "epoch": 0.00041041879585691863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 113088
    },
    {
      "epoch": 0.00041053493020775533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 113120
    },
    {
      "epoch": 0.0004106510645585921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 113152
    },
    {
      "epoch": 0.0004107671989094288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 113184
    },
    {
      "epoch": 0.0004108833332602655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 113216
    },
    {
      "epoch": 0.0004109994676111022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 113248
    },
    {
      "epoch": 0.0004111156019619389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 113280
    },
    {
      "epoch": 0.0004112317363127756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 113312
    },
    {
      "epoch": 0.0004113478706636123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 113344
    },
    {
      "epoch": 0.000411464005014449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 113376
    },
    {
      "epoch": 0.0004115801393652857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 113408
    },
    {
      "epoch": 0.00041169627371612244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 113440
    },
    {
      "epoch": 0.00041181240806695914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 113472
    },
    {
      "epoch": 0.00041192854241779584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 113504
    },
    {
      "epoch": 0.00041204467676863254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 113536
    },
    {
      "epoch": 0.00041216081111946924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 113568
    },
    {
      "epoch": 0.00041227694547030594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 113600
    },
    {
      "epoch": 0.00041239307982114264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 113632
    },
    {
      "epoch": 0.00041250921417197934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 113664
    },
    {
      "epoch": 0.00041262534852281604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 113696
    },
    {
      "epoch": 0.0004127414828736528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 113728
    },
    {
      "epoch": 0.0004128576172244895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 113760
    },
    {
      "epoch": 0.0004129737515753262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 113792
    },
    {
      "epoch": 0.0004130898859261629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 113824
    },
    {
      "epoch": 0.0004132060202769996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 113856
    },
    {
      "epoch": 0.0004133221546278363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 113888
    },
    {
      "epoch": 0.000413438288978673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 113920
    },
    {
      "epoch": 0.0004135544233295097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 113952
    },
    {
      "epoch": 0.0004136705576803464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.692,
      "step": 113984
    },
    {
      "epoch": 0.00041378669203118314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 114016
    },
    {
      "epoch": 0.00041390282638201984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 114048
    },
    {
      "epoch": 0.00041401896073285654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 114080
    },
    {
      "epoch": 0.00041413509508369324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 114112
    },
    {
      "epoch": 0.00041425122943452994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6957,
      "step": 114144
    },
    {
      "epoch": 0.00041436736378536664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 114176
    },
    {
      "epoch": 0.00041448349813620334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 114208
    },
    {
      "epoch": 0.00041459963248704004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 114240
    },
    {
      "epoch": 0.00041471576683787674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 114272
    },
    {
      "epoch": 0.0004148319011887135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 114304
    },
    {
      "epoch": 0.0004149480355395502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 114336
    },
    {
      "epoch": 0.0004150641698903869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 114368
    },
    {
      "epoch": 0.0004151803042412236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 114400
    },
    {
      "epoch": 0.0004152964385920603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 114432
    },
    {
      "epoch": 0.000415412572942897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 114464
    },
    {
      "epoch": 0.0004155287072937337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 114496
    },
    {
      "epoch": 0.0004156448416445704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 114528
    },
    {
      "epoch": 0.0004157609759954071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 114560
    },
    {
      "epoch": 0.00041587711034624385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 114592
    },
    {
      "epoch": 0.00041599324469708055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 114624
    },
    {
      "epoch": 0.00041610937904791725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 114656
    },
    {
      "epoch": 0.00041622551339875395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 114688
    },
    {
      "epoch": 0.00041634164774959065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 114720
    },
    {
      "epoch": 0.00041645778210042735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 114752
    },
    {
      "epoch": 0.00041657391645126405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 114784
    },
    {
      "epoch": 0.00041669005080210075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 114816
    },
    {
      "epoch": 0.00041680618515293745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 114848
    },
    {
      "epoch": 0.0004169223195037742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 114880
    },
    {
      "epoch": 0.0004170384538546109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 114912
    },
    {
      "epoch": 0.0004171545882054476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 114944
    },
    {
      "epoch": 0.0004172707225562843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 114976
    },
    {
      "epoch": 0.000417386856907121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6824,
      "step": 115008
    },
    {
      "epoch": 0.0004175029912579577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6878,
      "step": 115040
    },
    {
      "epoch": 0.0004176191256087944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 115072
    },
    {
      "epoch": 0.0004177352599596311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 115104
    },
    {
      "epoch": 0.0004178513943104678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7286,
      "step": 115136
    },
    {
      "epoch": 0.00041796752866130456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 115168
    },
    {
      "epoch": 0.00041808366301214126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 115200
    },
    {
      "epoch": 0.00041819979736297796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 115232
    },
    {
      "epoch": 0.00041831593171381466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 115264
    },
    {
      "epoch": 0.00041843206606465136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 115296
    },
    {
      "epoch": 0.00041854820041548806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 115328
    },
    {
      "epoch": 0.00041866433476632476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 115360
    },
    {
      "epoch": 0.00041878046911716146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 115392
    },
    {
      "epoch": 0.00041889660346799816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 115424
    },
    {
      "epoch": 0.0004190127378188349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 115456
    },
    {
      "epoch": 0.0004191288721696716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 115488
    },
    {
      "epoch": 0.0004192450065205083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 115520
    },
    {
      "epoch": 0.000419361140871345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 115552
    },
    {
      "epoch": 0.0004194772752221817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 115584
    },
    {
      "epoch": 0.0004195934095730184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 115616
    },
    {
      "epoch": 0.0004197095439238551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 115648
    },
    {
      "epoch": 0.0004198256782746918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 115680
    },
    {
      "epoch": 0.0004199418126255285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6864,
      "step": 115712
    },
    {
      "epoch": 0.00042005794697636526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 115744
    },
    {
      "epoch": 0.00042017408132720196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6858,
      "step": 115776
    },
    {
      "epoch": 0.00042029021567803866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 115808
    },
    {
      "epoch": 0.00042040635002887536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 115840
    },
    {
      "epoch": 0.00042052248437971206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 115872
    },
    {
      "epoch": 0.00042063861873054876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 115904
    },
    {
      "epoch": 0.00042075475308138546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6956,
      "step": 115936
    },
    {
      "epoch": 0.00042087088743222216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 115968
    },
    {
      "epoch": 0.00042098702178305886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 116000
    },
    {
      "epoch": 0.00042110315613389556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 116032
    },
    {
      "epoch": 0.0004212192904847323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 116064
    },
    {
      "epoch": 0.000421335424835569,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 116096
    },
    {
      "epoch": 0.0004214515591864057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 116128
    },
    {
      "epoch": 0.0004215676935372424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 116160
    },
    {
      "epoch": 0.0004216838278880791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 116192
    },
    {
      "epoch": 0.0004217999622389158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 116224
    },
    {
      "epoch": 0.0004219160965897525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 116256
    },
    {
      "epoch": 0.0004220322309405892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 116288
    },
    {
      "epoch": 0.0004221483652914259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 116320
    },
    {
      "epoch": 0.00042226449964226267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 116352
    },
    {
      "epoch": 0.00042238063399309937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 116384
    },
    {
      "epoch": 0.00042249676834393607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 116416
    },
    {
      "epoch": 0.00042261290269477277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 116448
    },
    {
      "epoch": 0.00042272903704560947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 116480
    },
    {
      "epoch": 0.00042284517139644617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 116512
    },
    {
      "epoch": 0.00042296130574728287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6829,
      "step": 116544
    },
    {
      "epoch": 0.00042307744009811957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6882,
      "step": 116576
    },
    {
      "epoch": 0.00042319357444895627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 116608
    },
    {
      "epoch": 0.000423309708799793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 116640
    },
    {
      "epoch": 0.0004234258431506297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 116672
    },
    {
      "epoch": 0.0004235419775014664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 116704
    },
    {
      "epoch": 0.0004236581118523031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6864,
      "step": 116736
    },
    {
      "epoch": 0.0004237742462031398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 116768
    },
    {
      "epoch": 0.0004238903805539765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 116800
    },
    {
      "epoch": 0.0004240065149048132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 116832
    },
    {
      "epoch": 0.0004241226492556499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 116864
    },
    {
      "epoch": 0.0004242387836064866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 116896
    },
    {
      "epoch": 0.0004243549179573234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 116928
    },
    {
      "epoch": 0.0004244710523081601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 116960
    },
    {
      "epoch": 0.0004245871866589968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 116992
    },
    {
      "epoch": 0.0004247033210098335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 117024
    },
    {
      "epoch": 0.0004248194553606702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 117056
    },
    {
      "epoch": 0.0004249355897115069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 117088
    },
    {
      "epoch": 0.0004250517240623436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 117120
    },
    {
      "epoch": 0.0004251678584131803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 117152
    },
    {
      "epoch": 0.000425283992764017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 117184
    },
    {
      "epoch": 0.00042540012711485373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 117216
    },
    {
      "epoch": 0.00042551626146569043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 117248
    },
    {
      "epoch": 0.00042563239581652713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 117280
    },
    {
      "epoch": 0.00042574853016736383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6814,
      "step": 117312
    },
    {
      "epoch": 0.00042586466451820053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 117344
    },
    {
      "epoch": 0.00042598079886903723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.693,
      "step": 117376
    },
    {
      "epoch": 0.00042609693321987393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 117408
    },
    {
      "epoch": 0.00042621306757071063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 117440
    },
    {
      "epoch": 0.00042632920192154733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 117472
    },
    {
      "epoch": 0.0004264453362723841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 117504
    },
    {
      "epoch": 0.0004265614706232208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 117536
    },
    {
      "epoch": 0.0004266776049740575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 117568
    },
    {
      "epoch": 0.0004267937393248942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 117600
    },
    {
      "epoch": 0.0004269098736757309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 117632
    },
    {
      "epoch": 0.0004270260080265676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 117664
    },
    {
      "epoch": 0.0004271421423774043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 117696
    },
    {
      "epoch": 0.000427258276728241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 117728
    },
    {
      "epoch": 0.0004273744110790777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 117760
    },
    {
      "epoch": 0.00042749054542991444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 117792
    },
    {
      "epoch": 0.00042760667978075114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 117824
    },
    {
      "epoch": 0.00042772281413158784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 117856
    },
    {
      "epoch": 0.00042783894848242454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 117888
    },
    {
      "epoch": 0.00042795508283326124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 117920
    },
    {
      "epoch": 0.00042807121718409794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 117952
    },
    {
      "epoch": 0.00042818735153493464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6851,
      "step": 117984
    },
    {
      "epoch": 0.00042830348588577134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 118016
    },
    {
      "epoch": 0.00042841962023660804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 118048
    },
    {
      "epoch": 0.0004285357545874448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6871,
      "step": 118080
    },
    {
      "epoch": 0.0004286518889382815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.692,
      "step": 118112
    },
    {
      "epoch": 0.0004287680232891182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 118144
    },
    {
      "epoch": 0.0004288841576399549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 118176
    },
    {
      "epoch": 0.0004290002919907916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 118208
    },
    {
      "epoch": 0.0004291164263416283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6878,
      "step": 118240
    },
    {
      "epoch": 0.000429232560692465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 118272
    },
    {
      "epoch": 0.0004293486950433017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 118304
    },
    {
      "epoch": 0.0004294648293941384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 118336
    },
    {
      "epoch": 0.00042958096374497514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 118368
    },
    {
      "epoch": 0.00042969709809581184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 118400
    },
    {
      "epoch": 0.00042981323244664854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 118432
    },
    {
      "epoch": 0.00042992936679748524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 118464
    },
    {
      "epoch": 0.00043004550114832194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 118496
    },
    {
      "epoch": 0.00043016163549915864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6855,
      "step": 118528
    },
    {
      "epoch": 0.00043027776984999534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 118560
    },
    {
      "epoch": 0.00043039390420083204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 118592
    },
    {
      "epoch": 0.00043051003855166874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 118624
    },
    {
      "epoch": 0.0004306261729025055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 118656
    },
    {
      "epoch": 0.0004307423072533422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 118688
    },
    {
      "epoch": 0.0004308584416041789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 118720
    },
    {
      "epoch": 0.0004309745759550156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 118752
    },
    {
      "epoch": 0.0004310907103058523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 118784
    },
    {
      "epoch": 0.000431206844656689,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 118816
    },
    {
      "epoch": 0.0004313229790075257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6747,
      "step": 118848
    },
    {
      "epoch": 0.0004314391133583624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6779,
      "step": 118880
    },
    {
      "epoch": 0.0004315552477091991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 118912
    },
    {
      "epoch": 0.00043167138206003585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 118944
    },
    {
      "epoch": 0.00043178751641087255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 118976
    },
    {
      "epoch": 0.00043190365076170925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 119008
    },
    {
      "epoch": 0.00043201978511254595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 119040
    },
    {
      "epoch": 0.00043213591946338265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 119072
    },
    {
      "epoch": 0.00043225205381421935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 119104
    },
    {
      "epoch": 0.00043236818816505605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 119136
    },
    {
      "epoch": 0.00043248432251589275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 119168
    },
    {
      "epoch": 0.00043260045686672945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 119200
    },
    {
      "epoch": 0.0004327165912175662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 119232
    },
    {
      "epoch": 0.0004328327255684029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 119264
    },
    {
      "epoch": 0.0004329488599192396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6857,
      "step": 119296
    },
    {
      "epoch": 0.0004330649942700763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 119328
    },
    {
      "epoch": 0.000433181128620913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 119360
    },
    {
      "epoch": 0.0004332972629717497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 119392
    },
    {
      "epoch": 0.0004334133973225864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 119424
    },
    {
      "epoch": 0.0004335295316734231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 119456
    },
    {
      "epoch": 0.0004336456660242598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 119488
    },
    {
      "epoch": 0.00043376180037509656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 119520
    },
    {
      "epoch": 0.00043387793472593326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 119552
    },
    {
      "epoch": 0.00043399406907676996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 119584
    },
    {
      "epoch": 0.00043411020342760666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 119616
    },
    {
      "epoch": 0.00043422633777844336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 119648
    },
    {
      "epoch": 0.00043434247212928006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 119680
    },
    {
      "epoch": 0.00043445860648011676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 119712
    },
    {
      "epoch": 0.00043457474083095346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 119744
    },
    {
      "epoch": 0.00043469087518179016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 119776
    },
    {
      "epoch": 0.0004348070095326269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 119808
    },
    {
      "epoch": 0.0004349231438834636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 119840
    },
    {
      "epoch": 0.0004350392782343003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 119872
    },
    {
      "epoch": 0.000435155412585137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 119904
    },
    {
      "epoch": 0.0004352715469359737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 119936
    },
    {
      "epoch": 0.0004353876812868104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 119968
    },
    {
      "epoch": 0.0004355038156376471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 120000
    },
    {
      "epoch": 0.0004356199499884838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 120032
    },
    {
      "epoch": 0.0004357360843393205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6888,
      "step": 120064
    },
    {
      "epoch": 0.00043585221869015727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 120096
    },
    {
      "epoch": 0.00043596835304099397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 120128
    },
    {
      "epoch": 0.00043608448739183066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 120160
    },
    {
      "epoch": 0.00043620062174266736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 120192
    },
    {
      "epoch": 0.00043631675609350406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 120224
    },
    {
      "epoch": 0.00043643289044434076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 120256
    },
    {
      "epoch": 0.00043654902479517746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 120288
    },
    {
      "epoch": 0.00043666515914601416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 120320
    },
    {
      "epoch": 0.00043678129349685086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 120352
    },
    {
      "epoch": 0.0004368974278476876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 120384
    },
    {
      "epoch": 0.0004370135621985243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 120416
    },
    {
      "epoch": 0.000437129696549361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 120448
    },
    {
      "epoch": 0.0004372458309001977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7286,
      "step": 120480
    },
    {
      "epoch": 0.0004373619652510344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 120512
    },
    {
      "epoch": 0.0004374780996018711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 120544
    },
    {
      "epoch": 0.0004375942339527078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 120576
    },
    {
      "epoch": 0.0004377103683035445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 120608
    },
    {
      "epoch": 0.0004378265026543812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 120640
    },
    {
      "epoch": 0.00043794263700521797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 120672
    },
    {
      "epoch": 0.00043805877135605467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 120704
    },
    {
      "epoch": 0.00043817490570689137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 120736
    },
    {
      "epoch": 0.00043829104005772807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 120768
    },
    {
      "epoch": 0.00043840717440856477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 120800
    },
    {
      "epoch": 0.00043852330875940147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 120832
    },
    {
      "epoch": 0.00043863944311023817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 120864
    },
    {
      "epoch": 0.00043875557746107487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 120896
    },
    {
      "epoch": 0.00043887171181191157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 120928
    },
    {
      "epoch": 0.0004389878461627483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 120960
    },
    {
      "epoch": 0.000439103980513585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6882,
      "step": 120992
    },
    {
      "epoch": 0.0004392201148644217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 121024
    },
    {
      "epoch": 0.0004393362492152584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6927,
      "step": 121056
    },
    {
      "epoch": 0.0004394523835660951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 121088
    },
    {
      "epoch": 0.0004395685179169318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 121120
    },
    {
      "epoch": 0.0004396846522677685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 121152
    },
    {
      "epoch": 0.0004398007866186052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 121184
    },
    {
      "epoch": 0.0004399169209694419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 121216
    },
    {
      "epoch": 0.0004400330553202786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 121248
    },
    {
      "epoch": 0.0004401491896711154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 121280
    },
    {
      "epoch": 0.0004402653240219521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 121312
    },
    {
      "epoch": 0.0004403814583727888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 121344
    },
    {
      "epoch": 0.0004404975927236255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7573,
      "step": 121376
    },
    {
      "epoch": 0.0004406137270744622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 121408
    },
    {
      "epoch": 0.0004407298614252989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6891,
      "step": 121440
    },
    {
      "epoch": 0.0004408459957761356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 121472
    },
    {
      "epoch": 0.0004409621301269723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 121504
    },
    {
      "epoch": 0.000441078264477809,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 121536
    },
    {
      "epoch": 0.00044119439882864573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 121568
    },
    {
      "epoch": 0.00044131053317948243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 121600
    },
    {
      "epoch": 0.00044142666753031913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 121632
    },
    {
      "epoch": 0.00044154280188115583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 121664
    },
    {
      "epoch": 0.00044165893623199253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 121696
    },
    {
      "epoch": 0.00044177507058282923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 121728
    },
    {
      "epoch": 0.00044189120493366593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6867,
      "step": 121760
    },
    {
      "epoch": 0.00044200733928450263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6899,
      "step": 121792
    },
    {
      "epoch": 0.00044212347363533933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6795,
      "step": 121824
    },
    {
      "epoch": 0.0004422396079861761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 121856
    },
    {
      "epoch": 0.0004423557423370128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 121888
    },
    {
      "epoch": 0.0004424718766878495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6875,
      "step": 121920
    },
    {
      "epoch": 0.0004425880110386862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 121952
    },
    {
      "epoch": 0.0004427041453895229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 121984
    },
    {
      "epoch": 0.0004428202797403596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 122016
    },
    {
      "epoch": 0.0004429364140911963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 122048
    },
    {
      "epoch": 0.000443052548442033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 122080
    },
    {
      "epoch": 0.0004431686827928697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 122112
    },
    {
      "epoch": 0.00044328481714370644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7473,
      "step": 122144
    },
    {
      "epoch": 0.00044340095149454314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 122176
    },
    {
      "epoch": 0.00044351708584537984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 122208
    },
    {
      "epoch": 0.00044363322019621654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 122240
    },
    {
      "epoch": 0.00044374935454705324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 122272
    },
    {
      "epoch": 0.00044386548889788994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 122304
    },
    {
      "epoch": 0.00044398162324872664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 122336
    },
    {
      "epoch": 0.00044409775759956334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 122368
    },
    {
      "epoch": 0.00044421389195040004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 122400
    },
    {
      "epoch": 0.0004443300263012368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 122432
    },
    {
      "epoch": 0.0004444461606520735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 122464
    },
    {
      "epoch": 0.0004445622950029102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 122496
    },
    {
      "epoch": 0.0004446784293537469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6869,
      "step": 122528
    },
    {
      "epoch": 0.0004447945637045836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 122560
    },
    {
      "epoch": 0.0004449106980554203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 122592
    },
    {
      "epoch": 0.000445026832406257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6859,
      "step": 122624
    },
    {
      "epoch": 0.0004451429667570937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6817,
      "step": 122656
    },
    {
      "epoch": 0.0004452591011079304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.682,
      "step": 122688
    },
    {
      "epoch": 0.00044537523545876715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 122720
    },
    {
      "epoch": 0.00044549136980960385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 122752
    },
    {
      "epoch": 0.00044560750416044055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 122784
    },
    {
      "epoch": 0.00044572363851127725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 122816
    },
    {
      "epoch": 0.00044583977286211395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 122848
    },
    {
      "epoch": 0.00044595590721295065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 122880
    },
    {
      "epoch": 0.00044607204156378735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 122912
    },
    {
      "epoch": 0.00044618817591462405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 122944
    },
    {
      "epoch": 0.00044630431026546075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 122976
    },
    {
      "epoch": 0.0004464204446162975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 123008
    },
    {
      "epoch": 0.0004465365789671342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 123040
    },
    {
      "epoch": 0.0004466527133179709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 123072
    },
    {
      "epoch": 0.0004467688476688076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 123104
    },
    {
      "epoch": 0.0004468849820196443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 123136
    },
    {
      "epoch": 0.000447001116370481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 123168
    },
    {
      "epoch": 0.0004471172507213177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 123200
    },
    {
      "epoch": 0.0004472333850721544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 123232
    },
    {
      "epoch": 0.0004473495194229911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 123264
    },
    {
      "epoch": 0.00044746565377382785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6861,
      "step": 123296
    },
    {
      "epoch": 0.00044758178812466455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 123328
    },
    {
      "epoch": 0.00044769792247550125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 123360
    },
    {
      "epoch": 0.00044781405682633795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6885,
      "step": 123392
    },
    {
      "epoch": 0.00044793019117717465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6887,
      "step": 123424
    },
    {
      "epoch": 0.00044804632552801135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 123456
    },
    {
      "epoch": 0.00044816245987884805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 123488
    },
    {
      "epoch": 0.00044827859422968475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 123520
    },
    {
      "epoch": 0.00044839472858052145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 123552
    },
    {
      "epoch": 0.0004485108629313582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 123584
    },
    {
      "epoch": 0.0004486269972821949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 123616
    },
    {
      "epoch": 0.0004487431316330316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 123648
    },
    {
      "epoch": 0.0004488592659838683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 123680
    },
    {
      "epoch": 0.000448975400334705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 123712
    },
    {
      "epoch": 0.0004490915346855417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7605,
      "step": 123744
    },
    {
      "epoch": 0.0004492076690363784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 123776
    },
    {
      "epoch": 0.0004493238033872151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 123808
    },
    {
      "epoch": 0.0004494399377380518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 123840
    },
    {
      "epoch": 0.00044955607208888856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 123872
    },
    {
      "epoch": 0.00044967220643972526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 123904
    },
    {
      "epoch": 0.00044978834079056196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 123936
    },
    {
      "epoch": 0.00044990447514139866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 123968
    },
    {
      "epoch": 0.00045002060949223536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 124000
    },
    {
      "epoch": 0.00045013674384307206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 124032
    },
    {
      "epoch": 0.00045025287819390876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 124064
    },
    {
      "epoch": 0.00045036901254474546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6864,
      "step": 124096
    },
    {
      "epoch": 0.00045048514689558216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 124128
    },
    {
      "epoch": 0.0004506012812464189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.691,
      "step": 124160
    },
    {
      "epoch": 0.0004507174155972556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 124192
    },
    {
      "epoch": 0.0004508335499480923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 124224
    },
    {
      "epoch": 0.000450949684298929,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 124256
    },
    {
      "epoch": 0.0004510658186497657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 124288
    },
    {
      "epoch": 0.0004511819530006024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 124320
    },
    {
      "epoch": 0.0004512980873514391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 124352
    },
    {
      "epoch": 0.0004514142217022758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 124384
    },
    {
      "epoch": 0.0004515303560531125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 124416
    },
    {
      "epoch": 0.00045164649040394927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 124448
    },
    {
      "epoch": 0.00045176262475478597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 124480
    },
    {
      "epoch": 0.00045187875910562267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 124512
    },
    {
      "epoch": 0.00045199489345645937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 124544
    },
    {
      "epoch": 0.00045211102780729607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 124576
    },
    {
      "epoch": 0.00045222716215813277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 124608
    },
    {
      "epoch": 0.00045234329650896947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7428,
      "step": 124640
    },
    {
      "epoch": 0.00045245943085980617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 124672
    },
    {
      "epoch": 0.00045257556521064287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 124704
    },
    {
      "epoch": 0.0004526916995614796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 124736
    },
    {
      "epoch": 0.0004528078339123163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 124768
    },
    {
      "epoch": 0.000452923968263153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 124800
    },
    {
      "epoch": 0.0004530401026139897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 124832
    },
    {
      "epoch": 0.0004531562369648264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 124864
    },
    {
      "epoch": 0.0004532723713156631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 124896
    },
    {
      "epoch": 0.0004533885056664998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 124928
    },
    {
      "epoch": 0.0004535046400173365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6787,
      "step": 124960
    },
    {
      "epoch": 0.0004536207743681732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 124992
    },
    {
      "epoch": 0.00045373690871901,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 125024
    },
    {
      "epoch": 0.0004538530430698467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 125056
    },
    {
      "epoch": 0.00045396917742068337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 125088
    },
    {
      "epoch": 0.00045408531177152007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 125120
    },
    {
      "epoch": 0.00045420144612235677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 125152
    },
    {
      "epoch": 0.00045431758047319347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 125184
    },
    {
      "epoch": 0.00045443371482403017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 125216
    },
    {
      "epoch": 0.00045454984917486687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 125248
    },
    {
      "epoch": 0.00045466598352570357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 125280
    },
    {
      "epoch": 0.0004547821178765403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 125312
    },
    {
      "epoch": 0.000454898252227377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.69,
      "step": 125344
    },
    {
      "epoch": 0.0004550143865782137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 125376
    },
    {
      "epoch": 0.0004551305209290504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 125408
    },
    {
      "epoch": 0.0004552466552798871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 125440
    },
    {
      "epoch": 0.0004553627896307238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 125472
    },
    {
      "epoch": 0.0004554789239815605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 125504
    },
    {
      "epoch": 0.0004555950583323972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 125536
    },
    {
      "epoch": 0.0004557111926832339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 125568
    },
    {
      "epoch": 0.0004558273270340707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 125600
    },
    {
      "epoch": 0.0004559434613849074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 125632
    },
    {
      "epoch": 0.0004560595957357441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 125664
    },
    {
      "epoch": 0.0004561757300865808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 125696
    },
    {
      "epoch": 0.0004562918644374175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6787,
      "step": 125728
    },
    {
      "epoch": 0.0004564079987882542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 125760
    },
    {
      "epoch": 0.0004565241331390909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 125792
    },
    {
      "epoch": 0.0004566402674899276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 125824
    },
    {
      "epoch": 0.0004567564018407643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 125856
    },
    {
      "epoch": 0.00045687253619160103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 125888
    },
    {
      "epoch": 0.00045698867054243773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 125920
    },
    {
      "epoch": 0.00045710480489327443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 125952
    },
    {
      "epoch": 0.00045722093924411113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 125984
    },
    {
      "epoch": 0.00045733707359494783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 126016
    },
    {
      "epoch": 0.00045745320794578453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 126048
    },
    {
      "epoch": 0.00045756934229662123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 126080
    },
    {
      "epoch": 0.00045768547664745793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 126112
    },
    {
      "epoch": 0.00045780161099829463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 126144
    },
    {
      "epoch": 0.0004579177453491314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 126176
    },
    {
      "epoch": 0.0004580338796999681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 126208
    },
    {
      "epoch": 0.0004581500140508048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 126240
    },
    {
      "epoch": 0.0004582661484016415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 126272
    },
    {
      "epoch": 0.0004583822827524782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 126304
    },
    {
      "epoch": 0.0004584984171033149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 126336
    },
    {
      "epoch": 0.0004586145514541516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 126368
    },
    {
      "epoch": 0.0004587306858049883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 126400
    },
    {
      "epoch": 0.000458846820155825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 126432
    },
    {
      "epoch": 0.00045896295450666174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 126464
    },
    {
      "epoch": 0.00045907908885749844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 126496
    },
    {
      "epoch": 0.00045919522320833514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 126528
    },
    {
      "epoch": 0.00045931135755917184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6836,
      "step": 126560
    },
    {
      "epoch": 0.00045942749191000854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6832,
      "step": 126592
    },
    {
      "epoch": 0.00045954362626084524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 126624
    },
    {
      "epoch": 0.00045965976061168194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.681,
      "step": 126656
    },
    {
      "epoch": 0.00045977589496251864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 126688
    },
    {
      "epoch": 0.00045989202931335534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6865,
      "step": 126720
    },
    {
      "epoch": 0.00046000816366419204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6829,
      "step": 126752
    },
    {
      "epoch": 0.0004601242980150288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 126784
    },
    {
      "epoch": 0.0004602404323658655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6761,
      "step": 126816
    },
    {
      "epoch": 0.0004603565667167022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6881,
      "step": 126848
    },
    {
      "epoch": 0.0004604727010675389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6835,
      "step": 126880
    },
    {
      "epoch": 0.0004605888354183756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6902,
      "step": 126912
    },
    {
      "epoch": 0.0004607049697692123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6776,
      "step": 126944
    },
    {
      "epoch": 0.000460821104120049,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 126976
    },
    {
      "epoch": 0.0004609372384708857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 127008
    },
    {
      "epoch": 0.0004610533728217224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6801,
      "step": 127040
    },
    {
      "epoch": 0.00046116950717255915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6914,
      "step": 127072
    },
    {
      "epoch": 0.00046128564152339585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6795,
      "step": 127104
    },
    {
      "epoch": 0.00046140177587423255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6758,
      "step": 127136
    },
    {
      "epoch": 0.00046151791022506925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6899,
      "step": 127168
    },
    {
      "epoch": 0.00046163404457590595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 127200
    },
    {
      "epoch": 0.00046175017892674265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 127232
    },
    {
      "epoch": 0.00046186631327757935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 127264
    },
    {
      "epoch": 0.00046198244762841605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 127296
    },
    {
      "epoch": 0.00046209858197925275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 127328
    },
    {
      "epoch": 0.0004622147163300895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 127360
    },
    {
      "epoch": 0.0004623308506809262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 127392
    },
    {
      "epoch": 0.0004624469850317629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6841,
      "step": 127424
    },
    {
      "epoch": 0.0004625631193825996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6839,
      "step": 127456
    },
    {
      "epoch": 0.0004626792537334363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6837,
      "step": 127488
    },
    {
      "epoch": 0.000462795388084273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6845,
      "step": 127520
    },
    {
      "epoch": 0.0004629115224351097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 127552
    },
    {
      "epoch": 0.0004630276567859464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 127584
    },
    {
      "epoch": 0.0004631437911367831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6804,
      "step": 127616
    },
    {
      "epoch": 0.00046325992548761985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 127648
    },
    {
      "epoch": 0.00046337605983845655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6844,
      "step": 127680
    },
    {
      "epoch": 0.00046349219418929325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6779,
      "step": 127712
    },
    {
      "epoch": 0.00046360832854012995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6742,
      "step": 127744
    },
    {
      "epoch": 0.00046372446289096665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6892,
      "step": 127776
    },
    {
      "epoch": 0.00046384059724180335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6904,
      "step": 127808
    },
    {
      "epoch": 0.00046395673159264005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 127840
    },
    {
      "epoch": 0.00046407286594347675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6911,
      "step": 127872
    },
    {
      "epoch": 0.00046418900029431345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6744,
      "step": 127904
    },
    {
      "epoch": 0.0004643051346451502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6874,
      "step": 127936
    },
    {
      "epoch": 0.0004644212689959869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6787,
      "step": 127968
    },
    {
      "epoch": 0.0004645374033468236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 128000
    },
    {
      "epoch": 0.0004646535376976603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 128032
    },
    {
      "epoch": 0.000464769672048497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 128064
    },
    {
      "epoch": 0.0004648858063993337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 128096
    },
    {
      "epoch": 0.0004650019407501704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 128128
    },
    {
      "epoch": 0.0004651180751010071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 128160
    },
    {
      "epoch": 0.0004652342094518438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 128192
    },
    {
      "epoch": 0.00046535034380268056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 128224
    },
    {
      "epoch": 0.00046546647815351726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 128256
    },
    {
      "epoch": 0.00046558261250435396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6688,
      "step": 128288
    },
    {
      "epoch": 0.00046569874685519066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6911,
      "step": 128320
    },
    {
      "epoch": 0.00046581488120602736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6783,
      "step": 128352
    },
    {
      "epoch": 0.00046593101555686406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6816,
      "step": 128384
    },
    {
      "epoch": 0.00046604714990770076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6805,
      "step": 128416
    },
    {
      "epoch": 0.00046616328425853746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 128448
    },
    {
      "epoch": 0.00046627941860937416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6792,
      "step": 128480
    },
    {
      "epoch": 0.0004663955529602109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6847,
      "step": 128512
    },
    {
      "epoch": 0.0004665116873110476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6842,
      "step": 128544
    },
    {
      "epoch": 0.0004666278216618843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6792,
      "step": 128576
    },
    {
      "epoch": 0.000466743956012721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 128608
    },
    {
      "epoch": 0.0004668600903635577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6859,
      "step": 128640
    },
    {
      "epoch": 0.0004669762247143944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6776,
      "step": 128672
    },
    {
      "epoch": 0.0004670923590652311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6708,
      "step": 128704
    },
    {
      "epoch": 0.0004672084934160678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6943,
      "step": 128736
    },
    {
      "epoch": 0.0004673246277669045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 128768
    },
    {
      "epoch": 0.00046744076211774127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6766,
      "step": 128800
    },
    {
      "epoch": 0.00046755689646857797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6696,
      "step": 128832
    },
    {
      "epoch": 0.00046767303081941467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 128864
    },
    {
      "epoch": 0.00046778916517025137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 128896
    },
    {
      "epoch": 0.00046790529952108807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 128928
    },
    {
      "epoch": 0.00046802143387192477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 128960
    },
    {
      "epoch": 0.00046813756822276147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 128992
    },
    {
      "epoch": 0.00046825370257359817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 129024
    },
    {
      "epoch": 0.00046836983692443487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 129056
    },
    {
      "epoch": 0.0004684859712752716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 129088
    },
    {
      "epoch": 0.0004686021056261083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 129120
    },
    {
      "epoch": 0.000468718239976945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6814,
      "step": 129152
    },
    {
      "epoch": 0.0004688343743277817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6795,
      "step": 129184
    },
    {
      "epoch": 0.0004689505086786184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 129216
    },
    {
      "epoch": 0.0004690666430294551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.679,
      "step": 129248
    },
    {
      "epoch": 0.0004691827773802918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6757,
      "step": 129280
    },
    {
      "epoch": 0.0004692989117311285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6885,
      "step": 129312
    },
    {
      "epoch": 0.0004694150460819652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 129344
    },
    {
      "epoch": 0.000469531180432802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 129376
    },
    {
      "epoch": 0.0004696473147836387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6741,
      "step": 129408
    },
    {
      "epoch": 0.0004697634491344754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6797,
      "step": 129440
    },
    {
      "epoch": 0.0004698795834853121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.679,
      "step": 129472
    },
    {
      "epoch": 0.0004699957178361488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6767,
      "step": 129504
    },
    {
      "epoch": 0.0004701118521869855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6918,
      "step": 129536
    },
    {
      "epoch": 0.0004702279865378222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6849,
      "step": 129568
    },
    {
      "epoch": 0.0004703441208886589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6828,
      "step": 129600
    },
    {
      "epoch": 0.0004704602552394956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 129632
    },
    {
      "epoch": 0.00047057638959033233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6769,
      "step": 129664
    },
    {
      "epoch": 0.00047069252394116903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6894,
      "step": 129696
    },
    {
      "epoch": 0.00047080865829200573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 129728
    },
    {
      "epoch": 0.00047092479264284243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6919,
      "step": 129760
    },
    {
      "epoch": 0.00047104092699367913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 129792
    },
    {
      "epoch": 0.00047115706134451583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 129824
    },
    {
      "epoch": 0.0004712731956953525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 129856
    },
    {
      "epoch": 0.0004713893300461892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 129888
    },
    {
      "epoch": 0.0004715054643970259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.74,
      "step": 129920
    },
    {
      "epoch": 0.0004716215987478627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 129952
    },
    {
      "epoch": 0.0004717377330986994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 129984
    },
    {
      "epoch": 0.0004718538674495361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 130016
    },
    {
      "epoch": 0.0004719700018003728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 130048
    },
    {
      "epoch": 0.0004720861361512095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 130080
    },
    {
      "epoch": 0.0004722022705020462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 130112
    },
    {
      "epoch": 0.0004723184048528829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 130144
    },
    {
      "epoch": 0.0004724345392037196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 130176
    },
    {
      "epoch": 0.0004725506735545563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 130208
    },
    {
      "epoch": 0.00047266680790539303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 130240
    },
    {
      "epoch": 0.00047278294225622973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 130272
    },
    {
      "epoch": 0.00047289907660706643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 130304
    },
    {
      "epoch": 0.00047301521095790313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 130336
    },
    {
      "epoch": 0.00047313134530873983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 130368
    },
    {
      "epoch": 0.00047324747965957653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 130400
    },
    {
      "epoch": 0.00047336361401041323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 130432
    },
    {
      "epoch": 0.00047347974836124993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 130464
    },
    {
      "epoch": 0.00047359588271208663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 130496
    },
    {
      "epoch": 0.0004737120170629234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 130528
    },
    {
      "epoch": 0.0004738281514137601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 130560
    },
    {
      "epoch": 0.0004739442857645968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 130592
    },
    {
      "epoch": 0.0004740604201154335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 130624
    },
    {
      "epoch": 0.0004741765544662702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 130656
    },
    {
      "epoch": 0.0004742926888171069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 130688
    },
    {
      "epoch": 0.0004744088231679436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 130720
    },
    {
      "epoch": 0.0004745249575187803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 130752
    },
    {
      "epoch": 0.000474641091869617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 130784
    },
    {
      "epoch": 0.00047475722622045374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 130816
    },
    {
      "epoch": 0.00047487336057129044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 130848
    },
    {
      "epoch": 0.00047498949492212714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 130880
    },
    {
      "epoch": 0.00047510562927296384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 130912
    },
    {
      "epoch": 0.00047522176362380054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 130944
    },
    {
      "epoch": 0.00047533789797463724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 130976
    },
    {
      "epoch": 0.00047545403232547394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 131008
    },
    {
      "epoch": 0.00047557016667631064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 131040
    },
    {
      "epoch": 0.00047568630102714734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 131072
    },
    {
      "epoch": 0.0004758024353779841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 131104
    },
    {
      "epoch": 0.0004759185697288208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 131136
    },
    {
      "epoch": 0.0004760347040796575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 131168
    },
    {
      "epoch": 0.0004761508384304942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 131200
    },
    {
      "epoch": 0.0004762669727813309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 131232
    },
    {
      "epoch": 0.0004763831071321676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 131264
    },
    {
      "epoch": 0.0004764992414830043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 131296
    },
    {
      "epoch": 0.000476615375833841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 131328
    },
    {
      "epoch": 0.0004767315101846777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 131360
    },
    {
      "epoch": 0.00047684764453551445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 131392
    },
    {
      "epoch": 0.00047696377888635115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 131424
    },
    {
      "epoch": 0.00047707991323718785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 131456
    },
    {
      "epoch": 0.00047719604758802455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 131488
    },
    {
      "epoch": 0.00047731218193886125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 131520
    },
    {
      "epoch": 0.00047742831628969795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7479,
      "step": 131552
    },
    {
      "epoch": 0.00047754445064053465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 131584
    },
    {
      "epoch": 0.00047766058499137135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 131616
    },
    {
      "epoch": 0.00047777671934220805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 131648
    },
    {
      "epoch": 0.0004778928536930448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7592,
      "step": 131680
    },
    {
      "epoch": 0.0004780089880438815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 131712
    },
    {
      "epoch": 0.0004781251223947182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 131744
    },
    {
      "epoch": 0.0004782412567455549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 131776
    },
    {
      "epoch": 0.0004783573910963916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 131808
    },
    {
      "epoch": 0.0004784735254472283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 131840
    },
    {
      "epoch": 0.000478589659798065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 131872
    },
    {
      "epoch": 0.0004787057941489017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 131904
    },
    {
      "epoch": 0.0004788219284997384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 131936
    },
    {
      "epoch": 0.0004789380628505751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 131968
    },
    {
      "epoch": 0.00047905419720141185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 132000
    },
    {
      "epoch": 0.00047917033155224855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 132032
    },
    {
      "epoch": 0.00047928646590308525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 132064
    },
    {
      "epoch": 0.00047940260025392195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 132096
    },
    {
      "epoch": 0.00047951873460475865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 132128
    },
    {
      "epoch": 0.00047963486895559535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 132160
    },
    {
      "epoch": 0.00047975100330643205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 132192
    },
    {
      "epoch": 0.00047986713765726875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 132224
    },
    {
      "epoch": 0.00047998327200810545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 132256
    },
    {
      "epoch": 0.0004800994063589422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7469,
      "step": 132288
    },
    {
      "epoch": 0.0004802155407097789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 132320
    },
    {
      "epoch": 0.0004803316750606156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 132352
    },
    {
      "epoch": 0.0004804478094114523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 132384
    },
    {
      "epoch": 0.000480563943762289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 132416
    },
    {
      "epoch": 0.0004806800781131257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7509,
      "step": 132448
    },
    {
      "epoch": 0.0004807962124639624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7452,
      "step": 132480
    },
    {
      "epoch": 0.0004809123468147991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 132512
    },
    {
      "epoch": 0.0004810284811656358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7574,
      "step": 132544
    },
    {
      "epoch": 0.00048114461551647256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 132576
    },
    {
      "epoch": 0.00048126074986730926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 132608
    },
    {
      "epoch": 0.00048137688421814596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 132640
    },
    {
      "epoch": 0.00048149301856898266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 132672
    },
    {
      "epoch": 0.00048160915291981936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 132704
    },
    {
      "epoch": 0.00048172528727065606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 132736
    },
    {
      "epoch": 0.00048184142162149276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 132768
    },
    {
      "epoch": 0.00048195755597232946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 132800
    },
    {
      "epoch": 0.00048207369032316616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 132832
    },
    {
      "epoch": 0.0004821898246740029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 132864
    },
    {
      "epoch": 0.0004823059590248396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 132896
    },
    {
      "epoch": 0.0004824220933756763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 132928
    },
    {
      "epoch": 0.000482538227726513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.695,
      "step": 132960
    },
    {
      "epoch": 0.0004826543620773497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 132992
    },
    {
      "epoch": 0.0004827704964281864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 133024
    },
    {
      "epoch": 0.0004828866307790231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 133056
    },
    {
      "epoch": 0.0004830027651298598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 133088
    },
    {
      "epoch": 0.0004831188994806965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 133120
    },
    {
      "epoch": 0.00048323503383153327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 133152
    },
    {
      "epoch": 0.00048335116818236997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 133184
    },
    {
      "epoch": 0.00048346730253320667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 133216
    },
    {
      "epoch": 0.00048358343688404337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 133248
    },
    {
      "epoch": 0.00048369957123488007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 133280
    },
    {
      "epoch": 0.00048381570558571677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7529,
      "step": 133312
    },
    {
      "epoch": 0.00048393183993655347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 133344
    },
    {
      "epoch": 0.00048404797428739017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 133376
    },
    {
      "epoch": 0.00048416410863822687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 133408
    },
    {
      "epoch": 0.0004842802429890636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 133440
    },
    {
      "epoch": 0.0004843963773399003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 133472
    },
    {
      "epoch": 0.000484512511690737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 133504
    },
    {
      "epoch": 0.0004846286460415737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 133536
    },
    {
      "epoch": 0.0004847447803924104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 133568
    },
    {
      "epoch": 0.0004848609147432471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 133600
    },
    {
      "epoch": 0.0004849770490940838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 133632
    },
    {
      "epoch": 0.0004850931834449205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 133664
    },
    {
      "epoch": 0.0004852093177957572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 133696
    },
    {
      "epoch": 0.000485325452146594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 133728
    },
    {
      "epoch": 0.0004854415864974307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 133760
    },
    {
      "epoch": 0.0004855577208482674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 133792
    },
    {
      "epoch": 0.0004856738551991041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 133824
    },
    {
      "epoch": 0.0004857899895499408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 133856
    },
    {
      "epoch": 0.0004859061239007775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 133888
    },
    {
      "epoch": 0.0004860222582516142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 133920
    },
    {
      "epoch": 0.0004861383926024509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 133952
    },
    {
      "epoch": 0.0004862545269532876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 133984
    },
    {
      "epoch": 0.00048637066130412433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 134016
    },
    {
      "epoch": 0.00048648679565496103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 134048
    },
    {
      "epoch": 0.00048660293000579773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 134080
    },
    {
      "epoch": 0.00048671906435663443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 134112
    },
    {
      "epoch": 0.00048683519870747113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 134144
    },
    {
      "epoch": 0.00048695133305830783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 134176
    },
    {
      "epoch": 0.00048706746740914453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 134208
    },
    {
      "epoch": 0.00048718360175998123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 134240
    },
    {
      "epoch": 0.00048729973611081793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7601,
      "step": 134272
    },
    {
      "epoch": 0.0004874158704616547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 134304
    },
    {
      "epoch": 0.0004875320048124914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 134336
    },
    {
      "epoch": 0.0004876481391633281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 134368
    },
    {
      "epoch": 0.0004877642735141648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 134400
    },
    {
      "epoch": 0.0004878804078650015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 134432
    },
    {
      "epoch": 0.0004879965422158382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 134464
    },
    {
      "epoch": 0.0004881126765666749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 134496
    },
    {
      "epoch": 0.0004882288109175116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 134528
    },
    {
      "epoch": 0.0004883449452683483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 134560
    },
    {
      "epoch": 0.000488461079619185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 134592
    },
    {
      "epoch": 0.0004885772139700217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 134624
    },
    {
      "epoch": 0.0004886933483208584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 134656
    },
    {
      "epoch": 0.0004888094826716951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 134688
    },
    {
      "epoch": 0.0004889256170225318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 134720
    },
    {
      "epoch": 0.0004890417513733685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 134752
    },
    {
      "epoch": 0.0004891578857242053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 134784
    },
    {
      "epoch": 0.0004892740200750419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 134816
    },
    {
      "epoch": 0.0004893901544258787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 134848
    },
    {
      "epoch": 0.0004895062887767153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 134880
    },
    {
      "epoch": 0.0004896224231275521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 134912
    },
    {
      "epoch": 0.0004897385574783887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 134944
    },
    {
      "epoch": 0.0004898546918292255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 134976
    },
    {
      "epoch": 0.0004899708261800621,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 135008
    },
    {
      "epoch": 0.0004900869605308989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 135040
    },
    {
      "epoch": 0.0004902030948817356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 135072
    },
    {
      "epoch": 0.0004903192292325723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 135104
    },
    {
      "epoch": 0.000490435363583409,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7417,
      "step": 135136
    },
    {
      "epoch": 0.0004905514979342457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 135168
    },
    {
      "epoch": 0.0004906676322850824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 135200
    },
    {
      "epoch": 0.0004907837666359191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 135232
    },
    {
      "epoch": 0.0004908999009867558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 135264
    },
    {
      "epoch": 0.0004910160353375925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 135296
    },
    {
      "epoch": 0.0004911321696884292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 135328
    },
    {
      "epoch": 0.000491248304039266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 135360
    },
    {
      "epoch": 0.0004913644383901026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 135392
    },
    {
      "epoch": 0.0004914805727409394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 135424
    },
    {
      "epoch": 0.000491596707091776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 135456
    },
    {
      "epoch": 0.0004917128414426128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 135488
    },
    {
      "epoch": 0.0004918289757934494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 135520
    },
    {
      "epoch": 0.0004919451101442862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 135552
    },
    {
      "epoch": 0.0004920612444951228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 135584
    },
    {
      "epoch": 0.0004921773788459596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 135616
    },
    {
      "epoch": 0.0004922935131967963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 135648
    },
    {
      "epoch": 0.000492409647547633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 135680
    },
    {
      "epoch": 0.0004925257818984697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 135712
    },
    {
      "epoch": 0.0004926419162493064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 135744
    },
    {
      "epoch": 0.0004927580506001431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 135776
    },
    {
      "epoch": 0.0004928741849509798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 135808
    },
    {
      "epoch": 0.0004929903193018165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 135840
    },
    {
      "epoch": 0.0004931064536526532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 135872
    },
    {
      "epoch": 0.00049322258800349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 135904
    },
    {
      "epoch": 0.0004933387223543267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 135936
    },
    {
      "epoch": 0.0004934548567051633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7509,
      "step": 135968
    },
    {
      "epoch": 0.0004935709910560001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 136000
    },
    {
      "epoch": 0.0004936871254068367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 136032
    },
    {
      "epoch": 0.0004938032597576735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 136064
    },
    {
      "epoch": 0.0004939193941085101,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 136096
    },
    {
      "epoch": 0.0004940355284593469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 136128
    },
    {
      "epoch": 0.0004941516628101835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 136160
    },
    {
      "epoch": 0.0004942677971610203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 136192
    },
    {
      "epoch": 0.0004943839315118571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 136224
    },
    {
      "epoch": 0.0004945000658626937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 136256
    },
    {
      "epoch": 0.0004946162002135305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 136288
    },
    {
      "epoch": 0.0004947323345643671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 136320
    },
    {
      "epoch": 0.0004948484689152039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 136352
    },
    {
      "epoch": 0.0004949646032660405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 136384
    },
    {
      "epoch": 0.0004950807376168773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 136416
    },
    {
      "epoch": 0.0004951968719677139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 136448
    },
    {
      "epoch": 0.0004953130063185507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 136480
    },
    {
      "epoch": 0.0004954291406693874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 136512
    },
    {
      "epoch": 0.0004955452750202241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 136544
    },
    {
      "epoch": 0.0004956614093710608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7417,
      "step": 136576
    },
    {
      "epoch": 0.0004957775437218975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 136608
    },
    {
      "epoch": 0.0004958936780727342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 136640
    },
    {
      "epoch": 0.0004960098124235709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 136672
    },
    {
      "epoch": 0.0004961259467744076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 136704
    },
    {
      "epoch": 0.0004962420811252443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 136736
    },
    {
      "epoch": 0.000496358215476081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 136768
    },
    {
      "epoch": 0.0004964743498269178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 136800
    },
    {
      "epoch": 0.0004965904841777544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7408,
      "step": 136832
    },
    {
      "epoch": 0.0004967066185285912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 136864
    },
    {
      "epoch": 0.0004968227528794278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 136896
    },
    {
      "epoch": 0.0004969388872302646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 136928
    },
    {
      "epoch": 0.0004970550215811012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 136960
    },
    {
      "epoch": 0.000497171155931938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 136992
    },
    {
      "epoch": 0.0004972872902827746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 137024
    },
    {
      "epoch": 0.0004974034246336114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 137056
    },
    {
      "epoch": 0.0004975195589844481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 137088
    },
    {
      "epoch": 0.0004976356933352848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 137120
    },
    {
      "epoch": 0.0004977518276861215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 137152
    },
    {
      "epoch": 0.0004978679620369582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 137184
    },
    {
      "epoch": 0.0004979840963877949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 137216
    },
    {
      "epoch": 0.0004981002307386316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 137248
    },
    {
      "epoch": 0.0004982163650894683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 137280
    },
    {
      "epoch": 0.000498332499440305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 137312
    },
    {
      "epoch": 0.0004984486337911417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 137344
    },
    {
      "epoch": 0.0004985647681419785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 137376
    },
    {
      "epoch": 0.0004986809024928151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 137408
    },
    {
      "epoch": 0.0004987970368436519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 137440
    },
    {
      "epoch": 0.0004989131711944885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 137472
    },
    {
      "epoch": 0.0004990293055453253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 137504
    },
    {
      "epoch": 0.0004991454398961619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 137536
    },
    {
      "epoch": 0.0004992615742469987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 137568
    },
    {
      "epoch": 0.0004993777085978353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7532,
      "step": 137600
    },
    {
      "epoch": 0.0004994938429486721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 137632
    },
    {
      "epoch": 0.0004996099772995088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 137664
    },
    {
      "epoch": 0.0004997261116503455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7453,
      "step": 137696
    },
    {
      "epoch": 0.0004998422460011822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 137728
    },
    {
      "epoch": 0.0004999583803520189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 137760
    },
    {
      "epoch": 0.0005000745147028556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 137792
    },
    {
      "epoch": 0.0005001906490536923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 137824
    },
    {
      "epoch": 0.000500306783404529,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 137856
    },
    {
      "epoch": 0.0005004229177553657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 137888
    },
    {
      "epoch": 0.0005005390521062024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 137920
    },
    {
      "epoch": 0.0005006551864570392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 137952
    },
    {
      "epoch": 0.0005007713208078758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 137984
    },
    {
      "epoch": 0.0005008874551587126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 138016
    },
    {
      "epoch": 0.0005010035895095492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 138048
    },
    {
      "epoch": 0.000501119723860386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 138080
    },
    {
      "epoch": 0.0005012358582112226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 138112
    },
    {
      "epoch": 0.0005013519925620594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 138144
    },
    {
      "epoch": 0.000501468126912896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 138176
    },
    {
      "epoch": 0.0005015842612637328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 138208
    },
    {
      "epoch": 0.0005017003956145695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 138240
    },
    {
      "epoch": 0.0005018165299654062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 138272
    },
    {
      "epoch": 0.0005019326643162429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 138304
    },
    {
      "epoch": 0.0005020487986670796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 138336
    },
    {
      "epoch": 0.0005021649330179163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7491,
      "step": 138368
    },
    {
      "epoch": 0.000502281067368753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 138400
    },
    {
      "epoch": 0.0005023972017195897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 138432
    },
    {
      "epoch": 0.0005025133360704264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7492,
      "step": 138464
    },
    {
      "epoch": 0.0005026294704212631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 138496
    },
    {
      "epoch": 0.0005027456047720999,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 138528
    },
    {
      "epoch": 0.0005028617391229365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7627,
      "step": 138560
    },
    {
      "epoch": 0.0005029778734737733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 138592
    },
    {
      "epoch": 0.0005030940078246099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 138624
    },
    {
      "epoch": 0.0005032101421754467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 138656
    },
    {
      "epoch": 0.0005033262765262833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 138688
    },
    {
      "epoch": 0.0005034424108771201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 138720
    },
    {
      "epoch": 0.0005035585452279567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7419,
      "step": 138752
    },
    {
      "epoch": 0.0005036746795787935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 138784
    },
    {
      "epoch": 0.0005037908139296302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 138816
    },
    {
      "epoch": 0.0005039069482804669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 138848
    },
    {
      "epoch": 0.0005040230826313036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 138880
    },
    {
      "epoch": 0.0005041392169821403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 138912
    },
    {
      "epoch": 0.000504255351332977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 138944
    },
    {
      "epoch": 0.0005043714856838137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 138976
    },
    {
      "epoch": 0.0005044876200346504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 139008
    },
    {
      "epoch": 0.0005046037543854871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7499,
      "step": 139040
    },
    {
      "epoch": 0.0005047198887363238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 139072
    },
    {
      "epoch": 0.0005048360230871606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 139104
    },
    {
      "epoch": 0.0005049521574379972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 139136
    },
    {
      "epoch": 0.000505068291788834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 139168
    },
    {
      "epoch": 0.0005051844261396706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 139200
    },
    {
      "epoch": 0.0005053005604905074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 139232
    },
    {
      "epoch": 0.000505416694841344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 139264
    },
    {
      "epoch": 0.0005055328291921808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7431,
      "step": 139296
    },
    {
      "epoch": 0.0005056489635430174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7462,
      "step": 139328
    },
    {
      "epoch": 0.0005057650978938542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7548,
      "step": 139360
    },
    {
      "epoch": 0.0005058812322446909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 139392
    },
    {
      "epoch": 0.0005059973665955276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 139424
    },
    {
      "epoch": 0.0005061135009463643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 139456
    },
    {
      "epoch": 0.000506229635297201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 139488
    },
    {
      "epoch": 0.0005063457696480377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 139520
    },
    {
      "epoch": 0.0005064619039988744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 139552
    },
    {
      "epoch": 0.0005065780383497111,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 139584
    },
    {
      "epoch": 0.0005066941727005478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 139616
    },
    {
      "epoch": 0.0005068103070513845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 139648
    },
    {
      "epoch": 0.0005069264414022213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 139680
    },
    {
      "epoch": 0.0005070425757530579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 139712
    },
    {
      "epoch": 0.0005071587101038947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 139744
    },
    {
      "epoch": 0.0005072748444547313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 139776
    },
    {
      "epoch": 0.0005073909788055681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 139808
    },
    {
      "epoch": 0.0005075071131564047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 139840
    },
    {
      "epoch": 0.0005076232475072415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 139872
    },
    {
      "epoch": 0.0005077393818580781,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 139904
    },
    {
      "epoch": 0.0005078555162089149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7419,
      "step": 139936
    },
    {
      "epoch": 0.0005079716505597517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 139968
    },
    {
      "epoch": 0.0005080877849105883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 140000
    },
    {
      "epoch": 0.000508203919261425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 140032
    },
    {
      "epoch": 0.0005083200536122617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 140064
    },
    {
      "epoch": 0.0005084361879630985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 140096
    },
    {
      "epoch": 0.0005085523223139351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 140128
    },
    {
      "epoch": 0.0005086684566647719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 140160
    },
    {
      "epoch": 0.0005087845910156085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 140192
    },
    {
      "epoch": 0.0005089007253664452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 140224
    },
    {
      "epoch": 0.000509016859717282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 140256
    },
    {
      "epoch": 0.0005091329940681186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 140288
    },
    {
      "epoch": 0.0005092491284189554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 140320
    },
    {
      "epoch": 0.000509365262769792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6892,
      "step": 140352
    },
    {
      "epoch": 0.0005094813971206288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 140384
    },
    {
      "epoch": 0.0005095975314714654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 140416
    },
    {
      "epoch": 0.0005097136658223022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 140448
    },
    {
      "epoch": 0.0005098298001731388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 140480
    },
    {
      "epoch": 0.0005099459345239756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 140512
    },
    {
      "epoch": 0.0005100620688748124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 140544
    },
    {
      "epoch": 0.000510178203225649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 140576
    },
    {
      "epoch": 0.0005102943375764858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 140608
    },
    {
      "epoch": 0.0005104104719273224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 140640
    },
    {
      "epoch": 0.0005105266062781592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 140672
    },
    {
      "epoch": 0.0005106427406289958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 140704
    },
    {
      "epoch": 0.0005107588749798326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 140736
    },
    {
      "epoch": 0.0005108750093306692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 140768
    },
    {
      "epoch": 0.000510991143681506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 140800
    },
    {
      "epoch": 0.0005111072780323427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 140832
    },
    {
      "epoch": 0.0005112234123831794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 140864
    },
    {
      "epoch": 0.0005113395467340161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 140896
    },
    {
      "epoch": 0.0005114556810848528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 140928
    },
    {
      "epoch": 0.0005115718154356895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 140960
    },
    {
      "epoch": 0.0005116879497865262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 140992
    },
    {
      "epoch": 0.0005118040841373629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 141024
    },
    {
      "epoch": 0.0005119202184881996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 141056
    },
    {
      "epoch": 0.0005120363528390363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.746,
      "step": 141088
    },
    {
      "epoch": 0.0005121524871898731,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7564,
      "step": 141120
    },
    {
      "epoch": 0.0005122686215407097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 141152
    },
    {
      "epoch": 0.0005123847558915465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 141184
    },
    {
      "epoch": 0.0005125008902423831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 141216
    },
    {
      "epoch": 0.0005126170245932199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 141248
    },
    {
      "epoch": 0.0005127331589440565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 141280
    },
    {
      "epoch": 0.0005128492932948933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 141312
    },
    {
      "epoch": 0.0005129654276457299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 141344
    },
    {
      "epoch": 0.0005130815619965667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 141376
    },
    {
      "epoch": 0.0005131976963474034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 141408
    },
    {
      "epoch": 0.0005133138306982401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 141440
    },
    {
      "epoch": 0.0005134299650490768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 141472
    },
    {
      "epoch": 0.0005135460993999135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 141504
    },
    {
      "epoch": 0.0005136622337507502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 141536
    },
    {
      "epoch": 0.0005137783681015869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 141568
    },
    {
      "epoch": 0.0005138945024524236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 141600
    },
    {
      "epoch": 0.0005140106368032603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 141632
    },
    {
      "epoch": 0.000514126771154097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 141664
    },
    {
      "epoch": 0.0005142429055049338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 141696
    },
    {
      "epoch": 0.0005143590398557704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 141728
    },
    {
      "epoch": 0.0005144751742066072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 141760
    },
    {
      "epoch": 0.0005145913085574438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 141792
    },
    {
      "epoch": 0.0005147074429082806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 141824
    },
    {
      "epoch": 0.0005148235772591172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 141856
    },
    {
      "epoch": 0.000514939711609954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 141888
    },
    {
      "epoch": 0.0005150558459607906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 141920
    },
    {
      "epoch": 0.0005151719803116274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 141952
    },
    {
      "epoch": 0.0005152881146624641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7632,
      "step": 141984
    },
    {
      "epoch": 0.0005154042490133008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 142016
    },
    {
      "epoch": 0.0005155203833641375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 142048
    },
    {
      "epoch": 0.0005156365177149742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 142080
    },
    {
      "epoch": 0.0005157526520658109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 142112
    },
    {
      "epoch": 0.0005158687864166476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 142144
    },
    {
      "epoch": 0.0005159849207674843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 142176
    },
    {
      "epoch": 0.000516101055118321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 142208
    },
    {
      "epoch": 0.0005162171894691577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 142240
    },
    {
      "epoch": 0.0005163333238199945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 142272
    },
    {
      "epoch": 0.0005164494581708311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 142304
    },
    {
      "epoch": 0.0005165655925216679,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 142336
    },
    {
      "epoch": 0.0005166817268725045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 142368
    },
    {
      "epoch": 0.0005167978612233413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 142400
    },
    {
      "epoch": 0.0005169139955741779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 142432
    },
    {
      "epoch": 0.0005170301299250147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 142464
    },
    {
      "epoch": 0.0005171462642758513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 142496
    },
    {
      "epoch": 0.0005172623986266881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 142528
    },
    {
      "epoch": 0.0005173785329775248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 142560
    },
    {
      "epoch": 0.0005174946673283615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 142592
    },
    {
      "epoch": 0.0005176108016791982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 142624
    },
    {
      "epoch": 0.0005177269360300349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 142656
    },
    {
      "epoch": 0.0005178430703808716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 142688
    },
    {
      "epoch": 0.0005179592047317083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.752,
      "step": 142720
    },
    {
      "epoch": 0.000518075339082545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 142752
    },
    {
      "epoch": 0.0005181914734333817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 142784
    },
    {
      "epoch": 0.0005183076077842184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 142816
    },
    {
      "epoch": 0.0005184237421350552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7605,
      "step": 142848
    },
    {
      "epoch": 0.0005185398764858918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 142880
    },
    {
      "epoch": 0.0005186560108367286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 142912
    },
    {
      "epoch": 0.0005187721451875652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 142944
    },
    {
      "epoch": 0.000518888279538402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 142976
    },
    {
      "epoch": 0.0005190044138892386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 143008
    },
    {
      "epoch": 0.0005191205482400754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 143040
    },
    {
      "epoch": 0.000519236682590912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 143072
    },
    {
      "epoch": 0.0005193528169417488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 143104
    },
    {
      "epoch": 0.0005194689512925855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 143136
    },
    {
      "epoch": 0.0005195850856434222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 143168
    },
    {
      "epoch": 0.0005197012199942589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 143200
    },
    {
      "epoch": 0.0005198173543450956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 143232
    },
    {
      "epoch": 0.0005199334886959323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 143264
    },
    {
      "epoch": 0.000520049623046769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 143296
    },
    {
      "epoch": 0.0005201657573976057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 143328
    },
    {
      "epoch": 0.0005202818917484424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 143360
    },
    {
      "epoch": 0.0005203980260992791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 143392
    },
    {
      "epoch": 0.0005205141604501159,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 143424
    },
    {
      "epoch": 0.0005206302948009525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 143456
    },
    {
      "epoch": 0.0005207464291517893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 143488
    },
    {
      "epoch": 0.0005208625635026259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 143520
    },
    {
      "epoch": 0.0005209786978534627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 143552
    },
    {
      "epoch": 0.0005210948322042993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 143584
    },
    {
      "epoch": 0.0005212109665551361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 143616
    },
    {
      "epoch": 0.0005213271009059727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 143648
    },
    {
      "epoch": 0.0005214432352568095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 143680
    },
    {
      "epoch": 0.0005215593696076462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 143712
    },
    {
      "epoch": 0.0005216755039584829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 143744
    },
    {
      "epoch": 0.0005217916383093196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 143776
    },
    {
      "epoch": 0.0005219077726601563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 143808
    },
    {
      "epoch": 0.000522023907010993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 143840
    },
    {
      "epoch": 0.0005221400413618297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 143872
    },
    {
      "epoch": 0.0005222561757126664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 143904
    },
    {
      "epoch": 0.0005223723100635031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 143936
    },
    {
      "epoch": 0.0005224884444143398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 143968
    },
    {
      "epoch": 0.0005226045787651766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 144000
    },
    {
      "epoch": 0.0005227207131160132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 144032
    },
    {
      "epoch": 0.00052283684746685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 144064
    },
    {
      "epoch": 0.0005229529818176866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 144096
    },
    {
      "epoch": 0.0005230691161685234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 144128
    },
    {
      "epoch": 0.00052318525051936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 144160
    },
    {
      "epoch": 0.0005233013848701968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 144192
    },
    {
      "epoch": 0.0005234175192210334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 144224
    },
    {
      "epoch": 0.0005235336535718702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 144256
    },
    {
      "epoch": 0.000523649787922707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 144288
    },
    {
      "epoch": 0.0005237659222735436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 144320
    },
    {
      "epoch": 0.0005238820566243804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 144352
    },
    {
      "epoch": 0.000523998190975217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 144384
    },
    {
      "epoch": 0.0005241143253260538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 144416
    },
    {
      "epoch": 0.0005242304596768904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 144448
    },
    {
      "epoch": 0.0005243465940277272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 144480
    },
    {
      "epoch": 0.0005244627283785638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7484,
      "step": 144512
    },
    {
      "epoch": 0.0005245788627294006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 144544
    },
    {
      "epoch": 0.0005246949970802373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 144576
    },
    {
      "epoch": 0.000524811131431074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 144608
    },
    {
      "epoch": 0.0005249272657819107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 144640
    },
    {
      "epoch": 0.0005250434001327474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 144672
    },
    {
      "epoch": 0.0005251595344835841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 144704
    },
    {
      "epoch": 0.0005252756688344208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 144736
    },
    {
      "epoch": 0.0005253918031852575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 144768
    },
    {
      "epoch": 0.0005255079375360942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 144800
    },
    {
      "epoch": 0.0005256240718869309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 144832
    },
    {
      "epoch": 0.0005257402062377677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 144864
    },
    {
      "epoch": 0.0005258563405886043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 144896
    },
    {
      "epoch": 0.0005259724749394411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 144928
    },
    {
      "epoch": 0.0005260886092902777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 144960
    },
    {
      "epoch": 0.0005262047436411145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 144992
    },
    {
      "epoch": 0.0005263208779919511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 145024
    },
    {
      "epoch": 0.0005264370123427879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 145056
    },
    {
      "epoch": 0.0005265531466936245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 145088
    },
    {
      "epoch": 0.0005266692810444613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 145120
    },
    {
      "epoch": 0.000526785415395298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 145152
    },
    {
      "epoch": 0.0005269015497461347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 145184
    },
    {
      "epoch": 0.0005270176840969714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 145216
    },
    {
      "epoch": 0.0005271338184478081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 145248
    },
    {
      "epoch": 0.0005272499527986448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 145280
    },
    {
      "epoch": 0.0005273660871494815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 145312
    },
    {
      "epoch": 0.0005274822215003182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 145344
    },
    {
      "epoch": 0.0005275983558511549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 145376
    },
    {
      "epoch": 0.0005277144902019916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 145408
    },
    {
      "epoch": 0.0005278306245528283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7484,
      "step": 145440
    },
    {
      "epoch": 0.000527946758903665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 145472
    },
    {
      "epoch": 0.0005280628932545018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 145504
    },
    {
      "epoch": 0.0005281790276053384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 145536
    },
    {
      "epoch": 0.0005282951619561752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 145568
    },
    {
      "epoch": 0.0005284112963070118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 145600
    },
    {
      "epoch": 0.0005285274306578486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 145632
    },
    {
      "epoch": 0.0005286435650086852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 145664
    },
    {
      "epoch": 0.000528759699359522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 145696
    },
    {
      "epoch": 0.0005288758337103586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 145728
    },
    {
      "epoch": 0.0005289919680611954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 145760
    },
    {
      "epoch": 0.0005291081024120321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 145792
    },
    {
      "epoch": 0.0005292242367628688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 145824
    },
    {
      "epoch": 0.0005293403711137055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 145856
    },
    {
      "epoch": 0.0005294565054645422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 145888
    },
    {
      "epoch": 0.0005295726398153789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 145920
    },
    {
      "epoch": 0.0005296887741662156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 145952
    },
    {
      "epoch": 0.0005298049085170523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 145984
    },
    {
      "epoch": 0.000529921042867889,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 146016
    },
    {
      "epoch": 0.0005300371772187257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 146048
    },
    {
      "epoch": 0.0005301533115695625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7543,
      "step": 146080
    },
    {
      "epoch": 0.0005302694459203991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7562,
      "step": 146112
    },
    {
      "epoch": 0.0005303855802712359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 146144
    },
    {
      "epoch": 0.0005305017146220725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 146176
    },
    {
      "epoch": 0.0005306178489729093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 146208
    },
    {
      "epoch": 0.0005307339833237459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 146240
    },
    {
      "epoch": 0.0005308501176745827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 146272
    },
    {
      "epoch": 0.0005309662520254193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 146304
    },
    {
      "epoch": 0.0005310823863762561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 146336
    },
    {
      "epoch": 0.0005311985207270928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 146368
    },
    {
      "epoch": 0.0005313146550779295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 146400
    },
    {
      "epoch": 0.0005314307894287662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 146432
    },
    {
      "epoch": 0.0005315469237796029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 146464
    },
    {
      "epoch": 0.0005316630581304396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 146496
    },
    {
      "epoch": 0.0005317791924812763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 146528
    },
    {
      "epoch": 0.000531895326832113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 146560
    },
    {
      "epoch": 0.0005320114611829497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 146592
    },
    {
      "epoch": 0.0005321275955337864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 146624
    },
    {
      "epoch": 0.0005322437298846232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 146656
    },
    {
      "epoch": 0.0005323598642354598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 146688
    },
    {
      "epoch": 0.0005324759985862966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 146720
    },
    {
      "epoch": 0.0005325921329371332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 146752
    },
    {
      "epoch": 0.00053270826728797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 146784
    },
    {
      "epoch": 0.0005328244016388066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 146816
    },
    {
      "epoch": 0.0005329405359896434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 146848
    },
    {
      "epoch": 0.00053305667034048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 146880
    },
    {
      "epoch": 0.0005331728046913168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 146912
    },
    {
      "epoch": 0.0005332889390421535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 146944
    },
    {
      "epoch": 0.0005334050733929902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 146976
    },
    {
      "epoch": 0.0005335212077438269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 147008
    },
    {
      "epoch": 0.0005336373420946636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 147040
    },
    {
      "epoch": 0.0005337534764455003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 147072
    },
    {
      "epoch": 0.000533869610796337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 147104
    },
    {
      "epoch": 0.0005339857451471737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7516,
      "step": 147136
    },
    {
      "epoch": 0.0005341018794980104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 147168
    },
    {
      "epoch": 0.0005342180138488471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 147200
    },
    {
      "epoch": 0.0005343341481996839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 147232
    },
    {
      "epoch": 0.0005344502825505205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 147264
    },
    {
      "epoch": 0.0005345664169013573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 147296
    },
    {
      "epoch": 0.0005346825512521939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 147328
    },
    {
      "epoch": 0.0005347986856030307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 147360
    },
    {
      "epoch": 0.0005349148199538673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 147392
    },
    {
      "epoch": 0.0005350309543047041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 147424
    },
    {
      "epoch": 0.0005351470886555407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 147456
    },
    {
      "epoch": 0.0005352632230063775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 147488
    },
    {
      "epoch": 0.0005353793573572142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 147520
    },
    {
      "epoch": 0.0005354954917080509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 147552
    },
    {
      "epoch": 0.0005356116260588876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 147584
    },
    {
      "epoch": 0.0005357277604097243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 147616
    },
    {
      "epoch": 0.000535843894760561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 147648
    },
    {
      "epoch": 0.0005359600291113977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6895,
      "step": 147680
    },
    {
      "epoch": 0.0005360761634622344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 147712
    },
    {
      "epoch": 0.0005361922978130711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 147744
    },
    {
      "epoch": 0.0005363084321639078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 147776
    },
    {
      "epoch": 0.0005364245665147446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 147808
    },
    {
      "epoch": 0.0005365407008655812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 147840
    },
    {
      "epoch": 0.000536656835216418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 147872
    },
    {
      "epoch": 0.0005367729695672546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 147904
    },
    {
      "epoch": 0.0005368891039180914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7491,
      "step": 147936
    },
    {
      "epoch": 0.000537005238268928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 147968
    },
    {
      "epoch": 0.0005371213726197648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 148000
    },
    {
      "epoch": 0.0005372375069706014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 148032
    },
    {
      "epoch": 0.0005373536413214382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 148064
    },
    {
      "epoch": 0.000537469775672275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 148096
    },
    {
      "epoch": 0.0005375859100231116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 148128
    },
    {
      "epoch": 0.0005377020443739483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 148160
    },
    {
      "epoch": 0.000537818178724785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 148192
    },
    {
      "epoch": 0.0005379343130756217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 148224
    },
    {
      "epoch": 0.0005380504474264584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 148256
    },
    {
      "epoch": 0.0005381665817772951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 148288
    },
    {
      "epoch": 0.0005382827161281318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 148320
    },
    {
      "epoch": 0.0005383988504789685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 148352
    },
    {
      "epoch": 0.0005385149848298053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 148384
    },
    {
      "epoch": 0.0005386311191806419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 148416
    },
    {
      "epoch": 0.0005387472535314787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 148448
    },
    {
      "epoch": 0.0005388633878823153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 148480
    },
    {
      "epoch": 0.0005389795222331521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 148512
    },
    {
      "epoch": 0.0005390956565839887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 148544
    },
    {
      "epoch": 0.0005392117909348255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 148576
    },
    {
      "epoch": 0.0005393279252856621,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 148608
    },
    {
      "epoch": 0.0005394440596364989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 148640
    },
    {
      "epoch": 0.0005395601939873357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 148672
    },
    {
      "epoch": 0.0005396763283381723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 148704
    },
    {
      "epoch": 0.000539792462689009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 148736
    },
    {
      "epoch": 0.0005399085970398457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7565,
      "step": 148768
    },
    {
      "epoch": 0.0005400247313906825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 148800
    },
    {
      "epoch": 0.0005401408657415191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7462,
      "step": 148832
    },
    {
      "epoch": 0.0005402570000923559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7431,
      "step": 148864
    },
    {
      "epoch": 0.0005403731344431925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 148896
    },
    {
      "epoch": 0.0005404892687940293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 148928
    },
    {
      "epoch": 0.000540605403144866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 148960
    },
    {
      "epoch": 0.0005407215374957027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 148992
    },
    {
      "epoch": 0.0005408376718465394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 149024
    },
    {
      "epoch": 0.000540953806197376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 149056
    },
    {
      "epoch": 0.0005410699405482128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 149088
    },
    {
      "epoch": 0.0005411860748990495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 149120
    },
    {
      "epoch": 0.0005413022092498862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 149152
    },
    {
      "epoch": 0.0005414183436007229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 149184
    },
    {
      "epoch": 0.0005415344779515596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 149216
    },
    {
      "epoch": 0.0005416506123023964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 149248
    },
    {
      "epoch": 0.000541766746653233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 149280
    },
    {
      "epoch": 0.0005418828810040698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 149312
    },
    {
      "epoch": 0.0005419990153549064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 149344
    },
    {
      "epoch": 0.0005421151497057432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 149376
    },
    {
      "epoch": 0.0005422312840565798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 149408
    },
    {
      "epoch": 0.0005423474184074166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 149440
    },
    {
      "epoch": 0.0005424635527582532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 149472
    },
    {
      "epoch": 0.00054257968710909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 149504
    },
    {
      "epoch": 0.0005426958214599267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 149536
    },
    {
      "epoch": 0.0005428119558107634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 149568
    },
    {
      "epoch": 0.0005429280901616001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 149600
    },
    {
      "epoch": 0.0005430442245124368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7434,
      "step": 149632
    },
    {
      "epoch": 0.0005431603588632735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 149664
    },
    {
      "epoch": 0.0005432764932141102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 149696
    },
    {
      "epoch": 0.0005433926275649469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 149728
    },
    {
      "epoch": 0.0005435087619157836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 149760
    },
    {
      "epoch": 0.0005436248962666203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 149792
    },
    {
      "epoch": 0.0005437410306174571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 149824
    },
    {
      "epoch": 0.0005438571649682937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 149856
    },
    {
      "epoch": 0.0005439732993191305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 149888
    },
    {
      "epoch": 0.0005440894336699671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 149920
    },
    {
      "epoch": 0.0005442055680208039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7397,
      "step": 149952
    },
    {
      "epoch": 0.0005443217023716405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 149984
    },
    {
      "epoch": 0.0005444378367224773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 150016
    },
    {
      "epoch": 0.0005445539710733139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 150048
    },
    {
      "epoch": 0.0005446701054241507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 150080
    },
    {
      "epoch": 0.0005447862397749874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 150112
    },
    {
      "epoch": 0.0005449023741258241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 150144
    },
    {
      "epoch": 0.0005450185084766608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 150176
    },
    {
      "epoch": 0.0005451346428274975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 150208
    },
    {
      "epoch": 0.0005452507771783342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 150240
    },
    {
      "epoch": 0.0005453669115291709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 150272
    },
    {
      "epoch": 0.0005454830458800076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 150304
    },
    {
      "epoch": 0.0005455991802308443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 150336
    },
    {
      "epoch": 0.000545715314581681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 150368
    },
    {
      "epoch": 0.0005458314489325178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 150400
    },
    {
      "epoch": 0.0005459475832833544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 150432
    },
    {
      "epoch": 0.0005460637176341912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 150464
    },
    {
      "epoch": 0.0005461798519850278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7462,
      "step": 150496
    },
    {
      "epoch": 0.0005462959863358646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7674,
      "step": 150528
    },
    {
      "epoch": 0.0005464121206867012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 150560
    },
    {
      "epoch": 0.000546528255037538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 150592
    },
    {
      "epoch": 0.0005466443893883746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 150624
    },
    {
      "epoch": 0.0005467605237392114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 150656
    },
    {
      "epoch": 0.0005468766580900481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 150688
    },
    {
      "epoch": 0.0005469927924408848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 150720
    },
    {
      "epoch": 0.0005471089267917215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 150752
    },
    {
      "epoch": 0.0005472250611425582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 150784
    },
    {
      "epoch": 0.0005473411954933949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 150816
    },
    {
      "epoch": 0.0005474573298442316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 150848
    },
    {
      "epoch": 0.0005475734641950683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 150880
    },
    {
      "epoch": 0.000547689598545905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 150912
    },
    {
      "epoch": 0.0005478057328967417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 150944
    },
    {
      "epoch": 0.0005479218672475785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 150976
    },
    {
      "epoch": 0.0005480380015984151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 151008
    },
    {
      "epoch": 0.0005481541359492519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 151040
    },
    {
      "epoch": 0.0005482702703000885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 151072
    },
    {
      "epoch": 0.0005483864046509253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 151104
    },
    {
      "epoch": 0.0005485025390017619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 151136
    },
    {
      "epoch": 0.0005486186733525987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 151168
    },
    {
      "epoch": 0.0005487348077034353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 151200
    },
    {
      "epoch": 0.0005488509420542721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 151232
    },
    {
      "epoch": 0.0005489670764051088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 151264
    },
    {
      "epoch": 0.0005490832107559455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.749,
      "step": 151296
    },
    {
      "epoch": 0.0005491993451067822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 151328
    },
    {
      "epoch": 0.0005493154794576189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 151360
    },
    {
      "epoch": 0.0005494316138084556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7397,
      "step": 151392
    },
    {
      "epoch": 0.0005495477481592923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 151424
    },
    {
      "epoch": 0.000549663882510129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 151456
    },
    {
      "epoch": 0.0005497800168609657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 151488
    },
    {
      "epoch": 0.0005498961512118024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 151520
    },
    {
      "epoch": 0.0005500122855626392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 151552
    },
    {
      "epoch": 0.0005501284199134758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 151584
    },
    {
      "epoch": 0.0005502445542643126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 151616
    },
    {
      "epoch": 0.0005503606886151492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 151648
    },
    {
      "epoch": 0.000550476822965986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 151680
    },
    {
      "epoch": 0.0005505929573168226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 151712
    },
    {
      "epoch": 0.0005507090916676594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 151744
    },
    {
      "epoch": 0.000550825226018496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 151776
    },
    {
      "epoch": 0.0005509413603693328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 151808
    },
    {
      "epoch": 0.0005510574947201695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 151840
    },
    {
      "epoch": 0.0005511736290710062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 151872
    },
    {
      "epoch": 0.0005512897634218429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 151904
    },
    {
      "epoch": 0.0005514058977726796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 151936
    },
    {
      "epoch": 0.0005515220321235163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 151968
    },
    {
      "epoch": 0.000551638166474353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 152000
    },
    {
      "epoch": 0.0005517543008251897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 152032
    },
    {
      "epoch": 0.0005518704351760264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 152064
    },
    {
      "epoch": 0.0005519865695268631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 152096
    },
    {
      "epoch": 0.0005521027038776999,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 152128
    },
    {
      "epoch": 0.0005522188382285365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 152160
    },
    {
      "epoch": 0.0005523349725793733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 152192
    },
    {
      "epoch": 0.0005524511069302099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7446,
      "step": 152224
    },
    {
      "epoch": 0.0005525672412810467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 152256
    },
    {
      "epoch": 0.0005526833756318833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7638,
      "step": 152288
    },
    {
      "epoch": 0.0005527995099827201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 152320
    },
    {
      "epoch": 0.0005529156443335567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 152352
    },
    {
      "epoch": 0.0005530317786843935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 152384
    },
    {
      "epoch": 0.0005531479130352302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 152416
    },
    {
      "epoch": 0.0005532640473860669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 152448
    },
    {
      "epoch": 0.0005533801817369036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 152480
    },
    {
      "epoch": 0.0005534963160877403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 152512
    },
    {
      "epoch": 0.000553612450438577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 152544
    },
    {
      "epoch": 0.0005537285847894137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 152576
    },
    {
      "epoch": 0.0005538447191402504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 152608
    },
    {
      "epoch": 0.0005539608534910871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 152640
    },
    {
      "epoch": 0.0005540769878419238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 152672
    },
    {
      "epoch": 0.0005541931221927606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 152704
    },
    {
      "epoch": 0.0005543092565435972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 152736
    },
    {
      "epoch": 0.000554425390894434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 152768
    },
    {
      "epoch": 0.0005545415252452706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 152800
    },
    {
      "epoch": 0.0005546576595961074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 152832
    },
    {
      "epoch": 0.000554773793946944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 152864
    },
    {
      "epoch": 0.0005548899282977808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 152896
    },
    {
      "epoch": 0.0005550060626486174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 152928
    },
    {
      "epoch": 0.0005551221969994542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 152960
    },
    {
      "epoch": 0.000555238331350291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 152992
    },
    {
      "epoch": 0.0005553544657011276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 153024
    },
    {
      "epoch": 0.0005554706000519644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 153056
    },
    {
      "epoch": 0.000555586734402801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 153088
    },
    {
      "epoch": 0.0005557028687536378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 153120
    },
    {
      "epoch": 0.0005558190031044744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7681,
      "step": 153152
    },
    {
      "epoch": 0.0005559351374553112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 153184
    },
    {
      "epoch": 0.0005560512718061478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 153216
    },
    {
      "epoch": 0.0005561674061569846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 153248
    },
    {
      "epoch": 0.0005562835405078213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 153280
    },
    {
      "epoch": 0.000556399674858658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 153312
    },
    {
      "epoch": 0.0005565158092094947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 153344
    },
    {
      "epoch": 0.0005566319435603314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 153376
    },
    {
      "epoch": 0.0005567480779111681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 153408
    },
    {
      "epoch": 0.0005568642122620048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 153440
    },
    {
      "epoch": 0.0005569803466128415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 153472
    },
    {
      "epoch": 0.0005570964809636782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 153504
    },
    {
      "epoch": 0.0005572126153145149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 153536
    },
    {
      "epoch": 0.0005573287496653517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 153568
    },
    {
      "epoch": 0.0005574448840161883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 153600
    },
    {
      "epoch": 0.0005575610183670251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 153632
    },
    {
      "epoch": 0.0005576771527178617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 153664
    },
    {
      "epoch": 0.0005577932870686985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 153696
    },
    {
      "epoch": 0.0005579094214195351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 153728
    },
    {
      "epoch": 0.0005580255557703719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 153760
    },
    {
      "epoch": 0.0005581416901212085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 153792
    },
    {
      "epoch": 0.0005582578244720453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 153824
    },
    {
      "epoch": 0.000558373958822882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 153856
    },
    {
      "epoch": 0.0005584900931737187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 153888
    },
    {
      "epoch": 0.0005586062275245554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 153920
    },
    {
      "epoch": 0.0005587223618753921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 153952
    },
    {
      "epoch": 0.0005588384962262288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 153984
    },
    {
      "epoch": 0.0005589546305770655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7506,
      "step": 154016
    },
    {
      "epoch": 0.0005590707649279022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 154048
    },
    {
      "epoch": 0.0005591868992787389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 154080
    },
    {
      "epoch": 0.0005593030336295756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 154112
    },
    {
      "epoch": 0.0005594191679804124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 154144
    },
    {
      "epoch": 0.000559535302331249,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 154176
    },
    {
      "epoch": 0.0005596514366820858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 154208
    },
    {
      "epoch": 0.0005597675710329224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 154240
    },
    {
      "epoch": 0.0005598837053837592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 154272
    },
    {
      "epoch": 0.0005599998397345958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 154304
    },
    {
      "epoch": 0.0005601159740854326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 154336
    },
    {
      "epoch": 0.0005602321084362692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 154368
    },
    {
      "epoch": 0.000560348242787106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 154400
    },
    {
      "epoch": 0.0005604643771379427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 154432
    },
    {
      "epoch": 0.0005605805114887794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 154464
    },
    {
      "epoch": 0.0005606966458396161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 154496
    },
    {
      "epoch": 0.0005608127801904528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 154528
    },
    {
      "epoch": 0.0005609289145412895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 154560
    },
    {
      "epoch": 0.0005610450488921262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 154592
    },
    {
      "epoch": 0.0005611611832429629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 154624
    },
    {
      "epoch": 0.0005612773175937996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 154656
    },
    {
      "epoch": 0.0005613934519446363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 154688
    },
    {
      "epoch": 0.0005615095862954731,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 154720
    },
    {
      "epoch": 0.0005616257206463097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 154752
    },
    {
      "epoch": 0.0005617418549971465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.746,
      "step": 154784
    },
    {
      "epoch": 0.0005618579893479831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 154816
    },
    {
      "epoch": 0.0005619741236988199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 154848
    },
    {
      "epoch": 0.0005620902580496565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7453,
      "step": 154880
    },
    {
      "epoch": 0.0005622063924004933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 154912
    },
    {
      "epoch": 0.0005623225267513299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 154944
    },
    {
      "epoch": 0.0005624386611021667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 154976
    },
    {
      "epoch": 0.0005625547954530034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 155008
    },
    {
      "epoch": 0.0005626709298038401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 155040
    },
    {
      "epoch": 0.0005627870641546768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 155072
    },
    {
      "epoch": 0.0005629031985055135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 155104
    },
    {
      "epoch": 0.0005630193328563502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 155136
    },
    {
      "epoch": 0.0005631354672071869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 155168
    },
    {
      "epoch": 0.0005632516015580236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 155200
    },
    {
      "epoch": 0.0005633677359088603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 155232
    },
    {
      "epoch": 0.000563483870259697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 155264
    },
    {
      "epoch": 0.0005636000046105338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 155296
    },
    {
      "epoch": 0.0005637161389613704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 155328
    },
    {
      "epoch": 0.0005638322733122072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 155360
    },
    {
      "epoch": 0.0005639484076630438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 155392
    },
    {
      "epoch": 0.0005640645420138806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 155424
    },
    {
      "epoch": 0.0005641806763647172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 155456
    },
    {
      "epoch": 0.000564296810715554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 155488
    },
    {
      "epoch": 0.0005644129450663906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 155520
    },
    {
      "epoch": 0.0005645290794172274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 155552
    },
    {
      "epoch": 0.0005646452137680641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 155584
    },
    {
      "epoch": 0.0005647613481189008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 155616
    },
    {
      "epoch": 0.0005648774824697375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 155648
    },
    {
      "epoch": 0.0005649936168205742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 155680
    },
    {
      "epoch": 0.0005651097511714109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 155712
    },
    {
      "epoch": 0.0005652258855222476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7469,
      "step": 155744
    },
    {
      "epoch": 0.0005653420198730843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 155776
    },
    {
      "epoch": 0.000565458154223921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 155808
    },
    {
      "epoch": 0.0005655742885747577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 155840
    },
    {
      "epoch": 0.0005656904229255945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 155872
    },
    {
      "epoch": 0.0005658065572764311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 155904
    },
    {
      "epoch": 0.0005659226916272679,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 155936
    },
    {
      "epoch": 0.0005660388259781045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 155968
    },
    {
      "epoch": 0.0005661549603289413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 156000
    },
    {
      "epoch": 0.0005662710946797779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 156032
    },
    {
      "epoch": 0.0005663872290306147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 156064
    },
    {
      "epoch": 0.0005665033633814513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 156096
    },
    {
      "epoch": 0.0005666194977322881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 156128
    },
    {
      "epoch": 0.0005667356320831248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 156160
    },
    {
      "epoch": 0.0005668517664339615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 156192
    },
    {
      "epoch": 0.0005669679007847982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 156224
    },
    {
      "epoch": 0.0005670840351356349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 156256
    },
    {
      "epoch": 0.0005672001694864716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 156288
    },
    {
      "epoch": 0.0005673163038373083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 156320
    },
    {
      "epoch": 0.000567432438188145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 156352
    },
    {
      "epoch": 0.0005675485725389817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 156384
    },
    {
      "epoch": 0.0005676647068898184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 156416
    },
    {
      "epoch": 0.0005677808412406551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 156448
    },
    {
      "epoch": 0.0005678969755914918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 156480
    },
    {
      "epoch": 0.0005680131099423286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 156512
    },
    {
      "epoch": 0.0005681292442931652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 156544
    },
    {
      "epoch": 0.000568245378644002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 156576
    },
    {
      "epoch": 0.0005683615129948386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7498,
      "step": 156608
    },
    {
      "epoch": 0.0005684776473456754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 156640
    },
    {
      "epoch": 0.000568593781696512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 156672
    },
    {
      "epoch": 0.0005687099160473488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 156704
    },
    {
      "epoch": 0.0005688260503981854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 156736
    },
    {
      "epoch": 0.0005689421847490222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 156768
    },
    {
      "epoch": 0.000569058319099859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 156800
    },
    {
      "epoch": 0.0005691744534506956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 156832
    },
    {
      "epoch": 0.0005692905878015323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 156864
    },
    {
      "epoch": 0.000569406722152369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 156896
    },
    {
      "epoch": 0.0005695228565032057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 156928
    },
    {
      "epoch": 0.0005696389908540424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 156960
    },
    {
      "epoch": 0.0005697551252048791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 156992
    },
    {
      "epoch": 0.0005698712595557158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 157024
    },
    {
      "epoch": 0.0005699873939065525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 157056
    },
    {
      "epoch": 0.0005701035282573893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 157088
    },
    {
      "epoch": 0.000570219662608226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 157120
    },
    {
      "epoch": 0.0005703357969590627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 157152
    },
    {
      "epoch": 0.0005704519313098993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 157184
    },
    {
      "epoch": 0.0005705680656607361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 157216
    },
    {
      "epoch": 0.0005706842000115727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 157248
    },
    {
      "epoch": 0.0005708003343624095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7494,
      "step": 157280
    },
    {
      "epoch": 0.0005709164687132461,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 157312
    },
    {
      "epoch": 0.0005710326030640829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 157344
    },
    {
      "epoch": 0.0005711487374149197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 157376
    },
    {
      "epoch": 0.0005712648717657563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 157408
    },
    {
      "epoch": 0.0005713810061165931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7563,
      "step": 157440
    },
    {
      "epoch": 0.0005714971404674297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 157472
    },
    {
      "epoch": 0.0005716132748182665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6943,
      "step": 157504
    },
    {
      "epoch": 0.0005717294091691031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 157536
    },
    {
      "epoch": 0.0005718455435199399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 157568
    },
    {
      "epoch": 0.0005719616778707765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 157600
    },
    {
      "epoch": 0.0005720778122216133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 157632
    },
    {
      "epoch": 0.00057219394657245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 157664
    },
    {
      "epoch": 0.0005723100809232867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 157696
    },
    {
      "epoch": 0.0005724262152741234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 157728
    },
    {
      "epoch": 0.00057254234962496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 157760
    },
    {
      "epoch": 0.0005726584839757968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 157792
    },
    {
      "epoch": 0.0005727746183266335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 157824
    },
    {
      "epoch": 0.0005728907526774702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 157856
    },
    {
      "epoch": 0.0005730068870283069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 157888
    },
    {
      "epoch": 0.0005731230213791436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 157920
    },
    {
      "epoch": 0.0005732391557299804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 157952
    },
    {
      "epoch": 0.000573355290080817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 157984
    },
    {
      "epoch": 0.0005734714244316538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 158016
    },
    {
      "epoch": 0.0005735875587824904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 158048
    },
    {
      "epoch": 0.0005737036931333272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 158080
    },
    {
      "epoch": 0.0005738198274841638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 158112
    },
    {
      "epoch": 0.0005739359618350006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 158144
    },
    {
      "epoch": 0.0005740520961858372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7519,
      "step": 158176
    },
    {
      "epoch": 0.000574168230536674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 158208
    },
    {
      "epoch": 0.0005742843648875107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 158240
    },
    {
      "epoch": 0.0005744004992383474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 158272
    },
    {
      "epoch": 0.0005745166335891841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 158304
    },
    {
      "epoch": 0.0005746327679400208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 158336
    },
    {
      "epoch": 0.0005747489022908575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 158368
    },
    {
      "epoch": 0.0005748650366416942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 158400
    },
    {
      "epoch": 0.0005749811709925309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 158432
    },
    {
      "epoch": 0.0005750973053433676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 158464
    },
    {
      "epoch": 0.0005752134396942043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 158496
    },
    {
      "epoch": 0.0005753295740450411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 158528
    },
    {
      "epoch": 0.0005754457083958777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 158560
    },
    {
      "epoch": 0.0005755618427467145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 158592
    },
    {
      "epoch": 0.0005756779770975511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 158624
    },
    {
      "epoch": 0.0005757941114483879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 158656
    },
    {
      "epoch": 0.0005759102457992245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 158688
    },
    {
      "epoch": 0.0005760263801500613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 158720
    },
    {
      "epoch": 0.0005761425145008979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 158752
    },
    {
      "epoch": 0.0005762586488517347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 158784
    },
    {
      "epoch": 0.0005763747832025714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 158816
    },
    {
      "epoch": 0.0005764909175534081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 158848
    },
    {
      "epoch": 0.0005766070519042448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 158880
    },
    {
      "epoch": 0.0005767231862550815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 158912
    },
    {
      "epoch": 0.0005768393206059182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 158944
    },
    {
      "epoch": 0.0005769554549567549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 158976
    },
    {
      "epoch": 0.0005770715893075916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 159008
    },
    {
      "epoch": 0.0005771877236584283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 159040
    },
    {
      "epoch": 0.000577303858009265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 159072
    },
    {
      "epoch": 0.0005774199923601018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 159104
    },
    {
      "epoch": 0.0005775361267109384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 159136
    },
    {
      "epoch": 0.0005776522610617752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 159168
    },
    {
      "epoch": 0.0005777683954126118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 159200
    },
    {
      "epoch": 0.0005778845297634486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 159232
    },
    {
      "epoch": 0.0005780006641142852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 159264
    },
    {
      "epoch": 0.000578116798465122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 159296
    },
    {
      "epoch": 0.0005782329328159586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 159328
    },
    {
      "epoch": 0.0005783490671667954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 159360
    },
    {
      "epoch": 0.0005784652015176321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 159392
    },
    {
      "epoch": 0.0005785813358684688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 159424
    },
    {
      "epoch": 0.0005786974702193055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 159456
    },
    {
      "epoch": 0.0005788136045701422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 159488
    },
    {
      "epoch": 0.0005789297389209789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 159520
    },
    {
      "epoch": 0.0005790458732718156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 159552
    },
    {
      "epoch": 0.0005791620076226523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 159584
    },
    {
      "epoch": 0.000579278141973489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 159616
    },
    {
      "epoch": 0.0005793942763243257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 159648
    },
    {
      "epoch": 0.0005795104106751625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 159680
    },
    {
      "epoch": 0.0005796265450259991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 159712
    },
    {
      "epoch": 0.0005797426793768359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 159744
    },
    {
      "epoch": 0.0005798588137276725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 159776
    },
    {
      "epoch": 0.0005799749480785093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 159808
    },
    {
      "epoch": 0.0005800910824293459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 159840
    },
    {
      "epoch": 0.0005802072167801827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 159872
    },
    {
      "epoch": 0.0005803233511310193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 159904
    },
    {
      "epoch": 0.0005804394854818561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 159936
    },
    {
      "epoch": 0.0005805556198326928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 159968
    },
    {
      "epoch": 0.0005806717541835295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 160000
    },
    {
      "epoch": 0.0005807878885343662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7585,
      "step": 160032
    },
    {
      "epoch": 0.0005809040228852029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 160064
    },
    {
      "epoch": 0.0005810201572360396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 160096
    },
    {
      "epoch": 0.0005811362915868763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 160128
    },
    {
      "epoch": 0.000581252425937713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 160160
    },
    {
      "epoch": 0.0005813685602885497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 160192
    },
    {
      "epoch": 0.0005814846946393864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 160224
    },
    {
      "epoch": 0.0005816008289902232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 160256
    },
    {
      "epoch": 0.0005817169633410598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 160288
    },
    {
      "epoch": 0.0005818330976918966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 160320
    },
    {
      "epoch": 0.0005819492320427332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 160352
    },
    {
      "epoch": 0.00058206536639357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 160384
    },
    {
      "epoch": 0.0005821815007444066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 160416
    },
    {
      "epoch": 0.0005822976350952434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 160448
    },
    {
      "epoch": 0.00058241376944608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 160480
    },
    {
      "epoch": 0.0005825299037969168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 160512
    },
    {
      "epoch": 0.0005826460381477535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 160544
    },
    {
      "epoch": 0.0005827621724985902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 160576
    },
    {
      "epoch": 0.0005828783068494269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 160608
    },
    {
      "epoch": 0.0005829944412002636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 160640
    },
    {
      "epoch": 0.0005831105755511003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 160672
    },
    {
      "epoch": 0.000583226709901937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 160704
    },
    {
      "epoch": 0.0005833428442527737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.743,
      "step": 160736
    },
    {
      "epoch": 0.0005834589786036104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 160768
    },
    {
      "epoch": 0.0005835751129544471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7621,
      "step": 160800
    },
    {
      "epoch": 0.0005836912473052839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 160832
    },
    {
      "epoch": 0.0005838073816561205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 160864
    },
    {
      "epoch": 0.0005839235160069573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 160896
    },
    {
      "epoch": 0.0005840396503577939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 160928
    },
    {
      "epoch": 0.0005841557847086307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 160960
    },
    {
      "epoch": 0.0005842719190594673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 160992
    },
    {
      "epoch": 0.0005843880534103041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 161024
    },
    {
      "epoch": 0.0005845041877611407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 161056
    },
    {
      "epoch": 0.0005846203221119775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 161088
    },
    {
      "epoch": 0.0005847364564628142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 161120
    },
    {
      "epoch": 0.0005848525908136509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 161152
    },
    {
      "epoch": 0.0005849687251644876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 161184
    },
    {
      "epoch": 0.0005850848595153243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 161216
    },
    {
      "epoch": 0.000585200993866161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 161248
    },
    {
      "epoch": 0.0005853171282169977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 161280
    },
    {
      "epoch": 0.0005854332625678344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 161312
    },
    {
      "epoch": 0.0005855493969186711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 161344
    },
    {
      "epoch": 0.0005856655312695078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 161376
    },
    {
      "epoch": 0.0005857816656203446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 161408
    },
    {
      "epoch": 0.0005858977999711812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 161440
    },
    {
      "epoch": 0.000586013934322018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 161472
    },
    {
      "epoch": 0.0005861300686728546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 161504
    },
    {
      "epoch": 0.0005862462030236914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 161536
    },
    {
      "epoch": 0.000586362337374528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 161568
    },
    {
      "epoch": 0.0005864784717253648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 161600
    },
    {
      "epoch": 0.0005865946060762014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 161632
    },
    {
      "epoch": 0.0005867107404270382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 161664
    },
    {
      "epoch": 0.000586826874777875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 161696
    },
    {
      "epoch": 0.0005869430091287116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7727,
      "step": 161728
    },
    {
      "epoch": 0.0005870591434795484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 161760
    },
    {
      "epoch": 0.000587175277830385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.69,
      "step": 161792
    },
    {
      "epoch": 0.0005872914121812218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 161824
    },
    {
      "epoch": 0.0005874075465320584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 161856
    },
    {
      "epoch": 0.0005875236808828952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 161888
    },
    {
      "epoch": 0.0005876398152337318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 161920
    },
    {
      "epoch": 0.0005877559495845686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 161952
    },
    {
      "epoch": 0.0005878720839354053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 161984
    },
    {
      "epoch": 0.000587988218286242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 162016
    },
    {
      "epoch": 0.0005881043526370787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 162048
    },
    {
      "epoch": 0.0005882204869879154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 162080
    },
    {
      "epoch": 0.0005883366213387521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 162112
    },
    {
      "epoch": 0.0005884527556895888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 162144
    },
    {
      "epoch": 0.0005885688900404255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 162176
    },
    {
      "epoch": 0.0005886850243912622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 162208
    },
    {
      "epoch": 0.0005888011587420989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 162240
    },
    {
      "epoch": 0.0005889172930929357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 162272
    },
    {
      "epoch": 0.0005890334274437723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 162304
    },
    {
      "epoch": 0.0005891495617946091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 162336
    },
    {
      "epoch": 0.0005892656961454457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 162368
    },
    {
      "epoch": 0.0005893818304962825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 162400
    },
    {
      "epoch": 0.0005894979648471191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 162432
    },
    {
      "epoch": 0.0005896140991979559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 162464
    },
    {
      "epoch": 0.0005897302335487925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 162496
    },
    {
      "epoch": 0.0005898463678996293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 162528
    },
    {
      "epoch": 0.000589962502250466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 162560
    },
    {
      "epoch": 0.0005900786366013027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7457,
      "step": 162592
    },
    {
      "epoch": 0.0005901947709521394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 162624
    },
    {
      "epoch": 0.0005903109053029761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6927,
      "step": 162656
    },
    {
      "epoch": 0.0005904270396538128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 162688
    },
    {
      "epoch": 0.0005905431740046495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 162720
    },
    {
      "epoch": 0.0005906593083554862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 162752
    },
    {
      "epoch": 0.0005907754427063229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 162784
    },
    {
      "epoch": 0.0005908915770571596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 162816
    },
    {
      "epoch": 0.0005910077114079964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 162848
    },
    {
      "epoch": 0.000591123845758833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 162880
    },
    {
      "epoch": 0.0005912399801096698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 162912
    },
    {
      "epoch": 0.0005913561144605064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 162944
    },
    {
      "epoch": 0.0005914722488113432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 162976
    },
    {
      "epoch": 0.0005915883831621798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 163008
    },
    {
      "epoch": 0.0005917045175130166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 163040
    },
    {
      "epoch": 0.0005918206518638532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 163072
    },
    {
      "epoch": 0.00059193678621469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 163104
    },
    {
      "epoch": 0.0005920529205655267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 163136
    },
    {
      "epoch": 0.0005921690549163634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 163168
    },
    {
      "epoch": 0.0005922851892672001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 163200
    },
    {
      "epoch": 0.0005924013236180368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 163232
    },
    {
      "epoch": 0.0005925174579688735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 163264
    },
    {
      "epoch": 0.0005926335923197102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 163296
    },
    {
      "epoch": 0.0005927497266705469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 163328
    },
    {
      "epoch": 0.0005928658610213836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 163360
    },
    {
      "epoch": 0.0005929819953722203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 163392
    },
    {
      "epoch": 0.0005930981297230571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 163424
    },
    {
      "epoch": 0.0005932142640738937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7483,
      "step": 163456
    },
    {
      "epoch": 0.0005933303984247305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 163488
    },
    {
      "epoch": 0.0005934465327755671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 163520
    },
    {
      "epoch": 0.0005935626671264039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 163552
    },
    {
      "epoch": 0.0005936788014772405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 163584
    },
    {
      "epoch": 0.0005937949358280773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 163616
    },
    {
      "epoch": 0.0005939110701789139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 163648
    },
    {
      "epoch": 0.0005940272045297507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 163680
    },
    {
      "epoch": 0.0005941433388805874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 163712
    },
    {
      "epoch": 0.0005942594732314241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 163744
    },
    {
      "epoch": 0.0005943756075822608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 163776
    },
    {
      "epoch": 0.0005944917419330975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 163808
    },
    {
      "epoch": 0.0005946078762839342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 163840
    },
    {
      "epoch": 0.0005947240106347709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 163872
    },
    {
      "epoch": 0.0005948401449856076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 163904
    },
    {
      "epoch": 0.0005949562793364443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 163936
    },
    {
      "epoch": 0.000595072413687281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 163968
    },
    {
      "epoch": 0.0005951885480381178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6956,
      "step": 164000
    },
    {
      "epoch": 0.0005953046823889544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 164032
    },
    {
      "epoch": 0.0005954208167397912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 164064
    },
    {
      "epoch": 0.0005955369510906278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 164096
    },
    {
      "epoch": 0.0005956530854414646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 164128
    },
    {
      "epoch": 0.0005957692197923012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 164160
    },
    {
      "epoch": 0.000595885354143138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 164192
    },
    {
      "epoch": 0.0005960014884939746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 164224
    },
    {
      "epoch": 0.0005961176228448114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 164256
    },
    {
      "epoch": 0.0005962337571956481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 164288
    },
    {
      "epoch": 0.0005963498915464848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 164320
    },
    {
      "epoch": 0.0005964660258973215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 164352
    },
    {
      "epoch": 0.0005965821602481582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 164384
    },
    {
      "epoch": 0.0005966982945989949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 164416
    },
    {
      "epoch": 0.0005968144289498316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 164448
    },
    {
      "epoch": 0.0005969305633006683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 164480
    },
    {
      "epoch": 0.000597046697651505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 164512
    },
    {
      "epoch": 0.0005971628320023417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 164544
    },
    {
      "epoch": 0.0005972789663531785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 164576
    },
    {
      "epoch": 0.0005973951007040151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 164608
    },
    {
      "epoch": 0.0005975112350548519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 164640
    },
    {
      "epoch": 0.0005976273694056885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 164672
    },
    {
      "epoch": 0.0005977435037565253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 164704
    },
    {
      "epoch": 0.0005978596381073619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 164736
    },
    {
      "epoch": 0.0005979757724581987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 164768
    },
    {
      "epoch": 0.0005980919068090353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 164800
    },
    {
      "epoch": 0.0005982080411598721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 164832
    },
    {
      "epoch": 0.0005983241755107088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 164864
    },
    {
      "epoch": 0.0005984403098615455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 164896
    },
    {
      "epoch": 0.0005985564442123822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 164928
    },
    {
      "epoch": 0.0005986725785632189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 164960
    },
    {
      "epoch": 0.0005987887129140556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 164992
    },
    {
      "epoch": 0.0005989048472648923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 165024
    },
    {
      "epoch": 0.000599020981615729,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 165056
    },
    {
      "epoch": 0.0005991371159665657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 165088
    },
    {
      "epoch": 0.0005992532503174024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 165120
    },
    {
      "epoch": 0.0005993693846682392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 165152
    },
    {
      "epoch": 0.0005994855190190758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 165184
    },
    {
      "epoch": 0.0005996016533699126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 165216
    },
    {
      "epoch": 0.0005997177877207492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 165248
    },
    {
      "epoch": 0.000599833922071586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 165280
    },
    {
      "epoch": 0.0005999500564224226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6857,
      "step": 165312
    },
    {
      "epoch": 0.0006000661907732594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 165344
    },
    {
      "epoch": 0.000600182325124096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 165376
    },
    {
      "epoch": 0.0006002984594749328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 165408
    },
    {
      "epoch": 0.0006004145938257696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 165440
    },
    {
      "epoch": 0.0006005307281766062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 165472
    },
    {
      "epoch": 0.000600646862527443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 165504
    },
    {
      "epoch": 0.0006007629968782796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 165536
    },
    {
      "epoch": 0.0006008791312291164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 165568
    },
    {
      "epoch": 0.000600995265579953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 165600
    },
    {
      "epoch": 0.0006011113999307898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 165632
    },
    {
      "epoch": 0.0006012275342816264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 165664
    },
    {
      "epoch": 0.0006013436686324632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 165696
    },
    {
      "epoch": 0.0006014598029832999,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 165728
    },
    {
      "epoch": 0.0006015759373341365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 165760
    },
    {
      "epoch": 0.0006016920716849733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 165792
    },
    {
      "epoch": 0.00060180820603581,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 165824
    },
    {
      "epoch": 0.0006019243403866467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 165856
    },
    {
      "epoch": 0.0006020404747374833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 165888
    },
    {
      "epoch": 0.0006021566090883201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 165920
    },
    {
      "epoch": 0.0006022727434391567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 165952
    },
    {
      "epoch": 0.0006023888777899935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 165984
    },
    {
      "epoch": 0.0006025050121408303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 166016
    },
    {
      "epoch": 0.0006026211464916669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 166048
    },
    {
      "epoch": 0.0006027372808425037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 166080
    },
    {
      "epoch": 0.0006028534151933403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7419,
      "step": 166112
    },
    {
      "epoch": 0.0006029695495441771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 166144
    },
    {
      "epoch": 0.0006030856838950137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6936,
      "step": 166176
    },
    {
      "epoch": 0.0006032018182458505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 166208
    },
    {
      "epoch": 0.0006033179525966871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 166240
    },
    {
      "epoch": 0.0006034340869475239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 166272
    },
    {
      "epoch": 0.0006035502212983606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 166304
    },
    {
      "epoch": 0.0006036663556491973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 166336
    },
    {
      "epoch": 0.000603782490000034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 166368
    },
    {
      "epoch": 0.0006038986243508707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 166400
    },
    {
      "epoch": 0.0006040147587017074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 166432
    },
    {
      "epoch": 0.0006041308930525441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 166464
    },
    {
      "epoch": 0.0006042470274033808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 166496
    },
    {
      "epoch": 0.0006043631617542175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 166528
    },
    {
      "epoch": 0.0006044792961050542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 166560
    },
    {
      "epoch": 0.000604595430455891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 166592
    },
    {
      "epoch": 0.0006047115648067276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 166624
    },
    {
      "epoch": 0.0006048276991575644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 166656
    },
    {
      "epoch": 0.000604943833508401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 166688
    },
    {
      "epoch": 0.0006050599678592378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 166720
    },
    {
      "epoch": 0.0006051761022100744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 166752
    },
    {
      "epoch": 0.0006052922365609112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 166784
    },
    {
      "epoch": 0.0006054083709117478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 166816
    },
    {
      "epoch": 0.0006055245052625846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 166848
    },
    {
      "epoch": 0.0006056406396134213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 166880
    },
    {
      "epoch": 0.000605756773964258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 166912
    },
    {
      "epoch": 0.0006058729083150947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 166944
    },
    {
      "epoch": 0.0006059890426659314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 166976
    },
    {
      "epoch": 0.0006061051770167681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 167008
    },
    {
      "epoch": 0.0006062213113676048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 167040
    },
    {
      "epoch": 0.0006063374457184415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 167072
    },
    {
      "epoch": 0.0006064535800692782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 167104
    },
    {
      "epoch": 0.0006065697144201149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 167136
    },
    {
      "epoch": 0.0006066858487709516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 167168
    },
    {
      "epoch": 0.0006068019831217883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 167200
    },
    {
      "epoch": 0.0006069181174726251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 167232
    },
    {
      "epoch": 0.0006070342518234617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 167264
    },
    {
      "epoch": 0.0006071503861742985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 167296
    },
    {
      "epoch": 0.0006072665205251351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 167328
    },
    {
      "epoch": 0.0006073826548759719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 167360
    },
    {
      "epoch": 0.0006074987892268085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 167392
    },
    {
      "epoch": 0.0006076149235776453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 167424
    },
    {
      "epoch": 0.0006077310579284819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 167456
    },
    {
      "epoch": 0.0006078471922793187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 167488
    },
    {
      "epoch": 0.0006079633266301554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 167520
    },
    {
      "epoch": 0.0006080794609809921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 167552
    },
    {
      "epoch": 0.0006081955953318288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 167584
    },
    {
      "epoch": 0.0006083117296826655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 167616
    },
    {
      "epoch": 0.0006084278640335022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 167648
    },
    {
      "epoch": 0.0006085439983843389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 167680
    },
    {
      "epoch": 0.0006086601327351756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 167712
    },
    {
      "epoch": 0.0006087762670860123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 167744
    },
    {
      "epoch": 0.000608892401436849,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 167776
    },
    {
      "epoch": 0.0006090085357876858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 167808
    },
    {
      "epoch": 0.0006091246701385224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 167840
    },
    {
      "epoch": 0.0006092408044893592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 167872
    },
    {
      "epoch": 0.0006093569388401958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 167904
    },
    {
      "epoch": 0.0006094730731910326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 167936
    },
    {
      "epoch": 0.0006095892075418692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 167968
    },
    {
      "epoch": 0.000609705341892706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 168000
    },
    {
      "epoch": 0.0006098214762435426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 168032
    },
    {
      "epoch": 0.0006099376105943794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 168064
    },
    {
      "epoch": 0.0006100537449452161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 168096
    },
    {
      "epoch": 0.0006101698792960528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 168128
    },
    {
      "epoch": 0.0006102860136468895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 168160
    },
    {
      "epoch": 0.0006104021479977262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 168192
    },
    {
      "epoch": 0.0006105182823485629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 168224
    },
    {
      "epoch": 0.0006106344166993996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 168256
    },
    {
      "epoch": 0.0006107505510502363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 168288
    },
    {
      "epoch": 0.000610866685401073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 168320
    },
    {
      "epoch": 0.0006109828197519097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 168352
    },
    {
      "epoch": 0.0006110989541027465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 168384
    },
    {
      "epoch": 0.0006112150884535831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 168416
    },
    {
      "epoch": 0.0006113312228044199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 168448
    },
    {
      "epoch": 0.0006114473571552565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 168480
    },
    {
      "epoch": 0.0006115634915060933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 168512
    },
    {
      "epoch": 0.0006116796258569299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 168544
    },
    {
      "epoch": 0.0006117957602077667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 168576
    },
    {
      "epoch": 0.0006119118945586033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7483,
      "step": 168608
    },
    {
      "epoch": 0.0006120280289094401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 168640
    },
    {
      "epoch": 0.0006121441632602768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 168672
    },
    {
      "epoch": 0.0006122602976111135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 168704
    },
    {
      "epoch": 0.0006123764319619502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 168736
    },
    {
      "epoch": 0.0006124925663127869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 168768
    },
    {
      "epoch": 0.0006126087006636236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 168800
    },
    {
      "epoch": 0.0006127248350144603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 168832
    },
    {
      "epoch": 0.000612840969365297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 168864
    },
    {
      "epoch": 0.0006129571037161337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 168896
    },
    {
      "epoch": 0.0006130732380669704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 168928
    },
    {
      "epoch": 0.0006131893724178072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 168960
    },
    {
      "epoch": 0.0006133055067686438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 168992
    },
    {
      "epoch": 0.0006134216411194806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 169024
    },
    {
      "epoch": 0.0006135377754703172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 169056
    },
    {
      "epoch": 0.000613653909821154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6905,
      "step": 169088
    },
    {
      "epoch": 0.0006137700441719906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 169120
    },
    {
      "epoch": 0.0006138861785228274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 169152
    },
    {
      "epoch": 0.000614002312873664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 169184
    },
    {
      "epoch": 0.0006141184472245008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7447,
      "step": 169216
    },
    {
      "epoch": 0.0006142345815753375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 169248
    },
    {
      "epoch": 0.0006143507159261742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 169280
    },
    {
      "epoch": 0.0006144668502770109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 169312
    },
    {
      "epoch": 0.0006145829846278476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 169344
    },
    {
      "epoch": 0.0006146991189786843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 169376
    },
    {
      "epoch": 0.000614815253329521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 169408
    },
    {
      "epoch": 0.0006149313876803577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 169440
    },
    {
      "epoch": 0.0006150475220311944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 169472
    },
    {
      "epoch": 0.0006151636563820311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 169504
    },
    {
      "epoch": 0.0006152797907328679,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 169536
    },
    {
      "epoch": 0.0006153959250837045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 169568
    },
    {
      "epoch": 0.0006155120594345413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 169600
    },
    {
      "epoch": 0.0006156281937853779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 169632
    },
    {
      "epoch": 0.0006157443281362147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 169664
    },
    {
      "epoch": 0.0006158604624870513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6905,
      "step": 169696
    },
    {
      "epoch": 0.0006159765968378881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 169728
    },
    {
      "epoch": 0.0006160927311887247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 169760
    },
    {
      "epoch": 0.0006162088655395615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 169792
    },
    {
      "epoch": 0.0006163249998903983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 169824
    },
    {
      "epoch": 0.0006164411342412349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 169856
    },
    {
      "epoch": 0.0006165572685920717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 169888
    },
    {
      "epoch": 0.0006166734029429083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 169920
    },
    {
      "epoch": 0.000616789537293745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 169952
    },
    {
      "epoch": 0.0006169056716445817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 169984
    },
    {
      "epoch": 0.0006170218059954185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 170016
    },
    {
      "epoch": 0.0006171379403462551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 170048
    },
    {
      "epoch": 0.0006172540746970919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 170080
    },
    {
      "epoch": 0.0006173702090479286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 170112
    },
    {
      "epoch": 0.0006174863433987653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 170144
    },
    {
      "epoch": 0.000617602477749602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 170176
    },
    {
      "epoch": 0.0006177186121004387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 170208
    },
    {
      "epoch": 0.0006178347464512754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 170240
    },
    {
      "epoch": 0.000617950880802112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 170272
    },
    {
      "epoch": 0.0006180670151529488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 170304
    },
    {
      "epoch": 0.0006181831495037855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 170336
    },
    {
      "epoch": 0.0006182992838546222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 170368
    },
    {
      "epoch": 0.000618415418205459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 170400
    },
    {
      "epoch": 0.0006185315525562956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 170432
    },
    {
      "epoch": 0.0006186476869071324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 170464
    },
    {
      "epoch": 0.000618763821257969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 170496
    },
    {
      "epoch": 0.0006188799556088058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 170528
    },
    {
      "epoch": 0.0006189960899596424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 170560
    },
    {
      "epoch": 0.0006191122243104792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 170592
    },
    {
      "epoch": 0.0006192283586613158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 170624
    },
    {
      "epoch": 0.0006193444930121526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 170656
    },
    {
      "epoch": 0.0006194606273629893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 170688
    },
    {
      "epoch": 0.000619576761713826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 170720
    },
    {
      "epoch": 0.0006196928960646627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 170752
    },
    {
      "epoch": 0.0006198090304154994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 170784
    },
    {
      "epoch": 0.0006199251647663361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 170816
    },
    {
      "epoch": 0.0006200412991171728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 170848
    },
    {
      "epoch": 0.0006201574334680095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 170880
    },
    {
      "epoch": 0.0006202735678188462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 170912
    },
    {
      "epoch": 0.0006203897021696829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 170944
    },
    {
      "epoch": 0.0006205058365205197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 170976
    },
    {
      "epoch": 0.0006206219708713563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 171008
    },
    {
      "epoch": 0.0006207381052221931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 171040
    },
    {
      "epoch": 0.0006208542395730297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 171072
    },
    {
      "epoch": 0.0006209703739238665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 171104
    },
    {
      "epoch": 0.0006210865082747031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 171136
    },
    {
      "epoch": 0.0006212026426255399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 171168
    },
    {
      "epoch": 0.0006213187769763765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 171200
    },
    {
      "epoch": 0.0006214349113272133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 171232
    },
    {
      "epoch": 0.00062155104567805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 171264
    },
    {
      "epoch": 0.0006216671800288867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 171296
    },
    {
      "epoch": 0.0006217833143797234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 171328
    },
    {
      "epoch": 0.0006218994487305601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 171360
    },
    {
      "epoch": 0.0006220155830813968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 171392
    },
    {
      "epoch": 0.0006221317174322335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 171424
    },
    {
      "epoch": 0.0006222478517830702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 171456
    },
    {
      "epoch": 0.0006223639861339069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 171488
    },
    {
      "epoch": 0.0006224801204847436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 171520
    },
    {
      "epoch": 0.0006225962548355804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 171552
    },
    {
      "epoch": 0.000622712389186417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 171584
    },
    {
      "epoch": 0.0006228285235372538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 171616
    },
    {
      "epoch": 0.0006229446578880904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 171648
    },
    {
      "epoch": 0.0006230607922389272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 171680
    },
    {
      "epoch": 0.0006231769265897638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 171712
    },
    {
      "epoch": 0.0006232930609406006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 171744
    },
    {
      "epoch": 0.0006234091952914372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 171776
    },
    {
      "epoch": 0.000623525329642274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 171808
    },
    {
      "epoch": 0.0006236414639931107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7397,
      "step": 171840
    },
    {
      "epoch": 0.0006237575983439474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 171872
    },
    {
      "epoch": 0.0006238737326947841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 171904
    },
    {
      "epoch": 0.0006239898670456208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 171936
    },
    {
      "epoch": 0.0006241060013964575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 171968
    },
    {
      "epoch": 0.0006242221357472942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 172000
    },
    {
      "epoch": 0.0006243382700981309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 172032
    },
    {
      "epoch": 0.0006244544044489676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 172064
    },
    {
      "epoch": 0.0006245705387998043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.692,
      "step": 172096
    },
    {
      "epoch": 0.0006246866731506411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 172128
    },
    {
      "epoch": 0.0006248028075014777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 172160
    },
    {
      "epoch": 0.0006249189418523145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 172192
    },
    {
      "epoch": 0.0006250350762031511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 172224
    },
    {
      "epoch": 0.0006251512105539879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 172256
    },
    {
      "epoch": 0.0006252673449048245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 172288
    },
    {
      "epoch": 0.0006253834792556613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 172320
    },
    {
      "epoch": 0.0006254996136064979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 172352
    },
    {
      "epoch": 0.0006256157479573347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 172384
    },
    {
      "epoch": 0.0006257318823081714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 172416
    },
    {
      "epoch": 0.0006258480166590081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 172448
    },
    {
      "epoch": 0.0006259641510098448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 172480
    },
    {
      "epoch": 0.0006260802853606815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 172512
    },
    {
      "epoch": 0.0006261964197115182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 172544
    },
    {
      "epoch": 0.0006263125540623549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 172576
    },
    {
      "epoch": 0.0006264286884131916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 172608
    },
    {
      "epoch": 0.0006265448227640283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 172640
    },
    {
      "epoch": 0.000626660957114865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 172672
    },
    {
      "epoch": 0.0006267770914657018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 172704
    },
    {
      "epoch": 0.0006268932258165384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7428,
      "step": 172736
    },
    {
      "epoch": 0.0006270093601673752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 172768
    },
    {
      "epoch": 0.0006271254945182118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 172800
    },
    {
      "epoch": 0.0006272416288690486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 172832
    },
    {
      "epoch": 0.0006273577632198852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 172864
    },
    {
      "epoch": 0.000627473897570722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 172896
    },
    {
      "epoch": 0.0006275900319215586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 172928
    },
    {
      "epoch": 0.0006277061662723954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 172960
    },
    {
      "epoch": 0.0006278223006232321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 172992
    },
    {
      "epoch": 0.0006279384349740688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 173024
    },
    {
      "epoch": 0.0006280545693249055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 173056
    },
    {
      "epoch": 0.0006281707036757422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 173088
    },
    {
      "epoch": 0.0006282868380265789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 173120
    },
    {
      "epoch": 0.0006284029723774156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 173152
    },
    {
      "epoch": 0.0006285191067282523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 173184
    },
    {
      "epoch": 0.000628635241079089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 173216
    },
    {
      "epoch": 0.0006287513754299257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 173248
    },
    {
      "epoch": 0.0006288675097807625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 173280
    },
    {
      "epoch": 0.0006289836441315991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 173312
    },
    {
      "epoch": 0.0006290997784824359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 173344
    },
    {
      "epoch": 0.0006292159128332725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 173376
    },
    {
      "epoch": 0.0006293320471841093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 173408
    },
    {
      "epoch": 0.0006294481815349459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 173440
    },
    {
      "epoch": 0.0006295643158857827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 173472
    },
    {
      "epoch": 0.0006296804502366193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 173504
    },
    {
      "epoch": 0.0006297965845874561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 173536
    },
    {
      "epoch": 0.0006299127189382928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 173568
    },
    {
      "epoch": 0.0006300288532891295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 173600
    },
    {
      "epoch": 0.0006301449876399662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 173632
    },
    {
      "epoch": 0.0006302611219908029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 173664
    },
    {
      "epoch": 0.0006303772563416396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 173696
    },
    {
      "epoch": 0.0006304933906924763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 173728
    },
    {
      "epoch": 0.000630609525043313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7412,
      "step": 173760
    },
    {
      "epoch": 0.0006307256593941497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 173792
    },
    {
      "epoch": 0.0006308417937449864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 173824
    },
    {
      "epoch": 0.0006309579280958232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 173856
    },
    {
      "epoch": 0.0006310740624466598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 173888
    },
    {
      "epoch": 0.0006311901967974966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 173920
    },
    {
      "epoch": 0.0006313063311483332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 173952
    },
    {
      "epoch": 0.00063142246549917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 173984
    },
    {
      "epoch": 0.0006315385998500066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 174016
    },
    {
      "epoch": 0.0006316547342008434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 174048
    },
    {
      "epoch": 0.00063177086855168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 174080
    },
    {
      "epoch": 0.0006318870029025168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 174112
    },
    {
      "epoch": 0.0006320031372533536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 174144
    },
    {
      "epoch": 0.0006321192716041902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 174176
    },
    {
      "epoch": 0.000632235405955027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 174208
    },
    {
      "epoch": 0.0006323515403058636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 174240
    },
    {
      "epoch": 0.0006324676746567004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 174272
    },
    {
      "epoch": 0.000632583809007537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 174304
    },
    {
      "epoch": 0.0006326999433583738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 174336
    },
    {
      "epoch": 0.0006328160777092104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 174368
    },
    {
      "epoch": 0.0006329322120600472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 174400
    },
    {
      "epoch": 0.0006330483464108839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 174432
    },
    {
      "epoch": 0.0006331644807617206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.743,
      "step": 174464
    },
    {
      "epoch": 0.0006332806151125573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 174496
    },
    {
      "epoch": 0.000633396749463394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 174528
    },
    {
      "epoch": 0.0006335128838142307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 174560
    },
    {
      "epoch": 0.0006336290181650674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 174592
    },
    {
      "epoch": 0.0006337451525159041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.743,
      "step": 174624
    },
    {
      "epoch": 0.0006338612868667408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 174656
    },
    {
      "epoch": 0.0006339774212175775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 174688
    },
    {
      "epoch": 0.0006340935555684143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 174720
    },
    {
      "epoch": 0.0006342096899192509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 174752
    },
    {
      "epoch": 0.0006343258242700877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 174784
    },
    {
      "epoch": 0.0006344419586209243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 174816
    },
    {
      "epoch": 0.0006345580929717611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 174848
    },
    {
      "epoch": 0.0006346742273225977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 174880
    },
    {
      "epoch": 0.0006347903616734345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 174912
    },
    {
      "epoch": 0.0006349064960242711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 174944
    },
    {
      "epoch": 0.0006350226303751079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 174976
    },
    {
      "epoch": 0.0006351387647259446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 175008
    },
    {
      "epoch": 0.0006352548990767813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 175040
    },
    {
      "epoch": 0.000635371033427618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 175072
    },
    {
      "epoch": 0.0006354871677784547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 175104
    },
    {
      "epoch": 0.0006356033021292914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 175136
    },
    {
      "epoch": 0.0006357194364801281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 175168
    },
    {
      "epoch": 0.0006358355708309648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 175200
    },
    {
      "epoch": 0.0006359517051818015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 175232
    },
    {
      "epoch": 0.0006360678395326382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 175264
    },
    {
      "epoch": 0.000636183973883475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 175296
    },
    {
      "epoch": 0.0006363001082343116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7493,
      "step": 175328
    },
    {
      "epoch": 0.0006364162425851484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 175360
    },
    {
      "epoch": 0.000636532376935985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 175392
    },
    {
      "epoch": 0.0006366485112868218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 175424
    },
    {
      "epoch": 0.0006367646456376584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 175456
    },
    {
      "epoch": 0.0006368807799884952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 175488
    },
    {
      "epoch": 0.0006369969143393318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 175520
    },
    {
      "epoch": 0.0006371130486901686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6836,
      "step": 175552
    },
    {
      "epoch": 0.0006372291830410053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 175584
    },
    {
      "epoch": 0.000637345317391842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 175616
    },
    {
      "epoch": 0.0006374614517426787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 175648
    },
    {
      "epoch": 0.0006375775860935154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 175680
    },
    {
      "epoch": 0.0006376937204443521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 175712
    },
    {
      "epoch": 0.0006378098547951888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 175744
    },
    {
      "epoch": 0.0006379259891460255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 175776
    },
    {
      "epoch": 0.0006380421234968622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 175808
    },
    {
      "epoch": 0.0006381582578476989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 175840
    },
    {
      "epoch": 0.0006382743921985357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 175872
    },
    {
      "epoch": 0.0006383905265493723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 175904
    },
    {
      "epoch": 0.0006385066609002091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 175936
    },
    {
      "epoch": 0.0006386227952510457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 175968
    },
    {
      "epoch": 0.0006387389296018825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 176000
    },
    {
      "epoch": 0.0006388550639527191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 176032
    },
    {
      "epoch": 0.0006389711983035559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 176064
    },
    {
      "epoch": 0.0006390873326543925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 176096
    },
    {
      "epoch": 0.0006392034670052293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 176128
    },
    {
      "epoch": 0.000639319601356066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 176160
    },
    {
      "epoch": 0.0006394357357069027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 176192
    },
    {
      "epoch": 0.0006395518700577394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 176224
    },
    {
      "epoch": 0.0006396680044085761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 176256
    },
    {
      "epoch": 0.0006397841387594128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 176288
    },
    {
      "epoch": 0.0006399002731102495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 176320
    },
    {
      "epoch": 0.0006400164074610862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 176352
    },
    {
      "epoch": 0.0006401325418119229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 176384
    },
    {
      "epoch": 0.0006402486761627596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 176416
    },
    {
      "epoch": 0.0006403648105135964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 176448
    },
    {
      "epoch": 0.000640480944864433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 176480
    },
    {
      "epoch": 0.0006405970792152698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 176512
    },
    {
      "epoch": 0.0006407132135661064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 176544
    },
    {
      "epoch": 0.0006408293479169432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 176576
    },
    {
      "epoch": 0.0006409454822677798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 176608
    },
    {
      "epoch": 0.0006410616166186166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 176640
    },
    {
      "epoch": 0.0006411777509694532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 176672
    },
    {
      "epoch": 0.00064129388532029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 176704
    },
    {
      "epoch": 0.0006414100196711267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 176736
    },
    {
      "epoch": 0.0006415261540219634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 176768
    },
    {
      "epoch": 0.0006416422883728001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 176800
    },
    {
      "epoch": 0.0006417584227236368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 176832
    },
    {
      "epoch": 0.0006418745570744735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 176864
    },
    {
      "epoch": 0.0006419906914253102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 176896
    },
    {
      "epoch": 0.0006421068257761469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 176928
    },
    {
      "epoch": 0.0006422229601269836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 176960
    },
    {
      "epoch": 0.0006423390944778203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 176992
    },
    {
      "epoch": 0.0006424552288286571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 177024
    },
    {
      "epoch": 0.0006425713631794937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 177056
    },
    {
      "epoch": 0.0006426874975303305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7432,
      "step": 177088
    },
    {
      "epoch": 0.0006428036318811671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 177120
    },
    {
      "epoch": 0.0006429197662320039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 177152
    },
    {
      "epoch": 0.0006430359005828405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 177184
    },
    {
      "epoch": 0.0006431520349336773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 177216
    },
    {
      "epoch": 0.0006432681692845139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 177248
    },
    {
      "epoch": 0.0006433843036353507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 177280
    },
    {
      "epoch": 0.0006435004379861874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 177312
    },
    {
      "epoch": 0.0006436165723370241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 177344
    },
    {
      "epoch": 0.0006437327066878608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 177376
    },
    {
      "epoch": 0.0006438488410386975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 177408
    },
    {
      "epoch": 0.0006439649753895342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 177440
    },
    {
      "epoch": 0.0006440811097403709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 177472
    },
    {
      "epoch": 0.0006441972440912076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 177504
    },
    {
      "epoch": 0.0006443133784420443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 177536
    },
    {
      "epoch": 0.000644429512792881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 177568
    },
    {
      "epoch": 0.0006445456471437178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 177600
    },
    {
      "epoch": 0.0006446617814945544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 177632
    },
    {
      "epoch": 0.0006447779158453912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 177664
    },
    {
      "epoch": 0.0006448940501962278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 177696
    },
    {
      "epoch": 0.0006450101845470646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 177728
    },
    {
      "epoch": 0.0006451263188979012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 177760
    },
    {
      "epoch": 0.000645242453248738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 177792
    },
    {
      "epoch": 0.0006453585875995746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 177824
    },
    {
      "epoch": 0.0006454747219504114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 177856
    },
    {
      "epoch": 0.000645590856301248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 177888
    },
    {
      "epoch": 0.0006457069906520848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 177920
    },
    {
      "epoch": 0.0006458231250029215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 177952
    },
    {
      "epoch": 0.0006459392593537582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 177984
    },
    {
      "epoch": 0.000646055393704595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 178016
    },
    {
      "epoch": 0.0006461715280554316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 178048
    },
    {
      "epoch": 0.0006462876624062683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 178080
    },
    {
      "epoch": 0.000646403796757105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 178112
    },
    {
      "epoch": 0.0006465199311079417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 178144
    },
    {
      "epoch": 0.0006466360654587784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 178176
    },
    {
      "epoch": 0.0006467521998096151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 178208
    },
    {
      "epoch": 0.0006468683341604519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 178240
    },
    {
      "epoch": 0.0006469844685112885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 178272
    },
    {
      "epoch": 0.0006471006028621253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 178304
    },
    {
      "epoch": 0.000647216737212962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 178336
    },
    {
      "epoch": 0.0006473328715637987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 178368
    },
    {
      "epoch": 0.0006474490059146353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 178400
    },
    {
      "epoch": 0.0006475651402654721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 178432
    },
    {
      "epoch": 0.0006476812746163087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 178464
    },
    {
      "epoch": 0.0006477974089671455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 178496
    },
    {
      "epoch": 0.0006479135433179823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 178528
    },
    {
      "epoch": 0.0006480296776688189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 178560
    },
    {
      "epoch": 0.0006481458120196557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 178592
    },
    {
      "epoch": 0.0006482619463704923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 178624
    },
    {
      "epoch": 0.000648378080721329,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 178656
    },
    {
      "epoch": 0.0006484942150721657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 178688
    },
    {
      "epoch": 0.0006486103494230025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 178720
    },
    {
      "epoch": 0.0006487264837738391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 178752
    },
    {
      "epoch": 0.0006488426181246759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 178784
    },
    {
      "epoch": 0.0006489587524755126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 178816
    },
    {
      "epoch": 0.0006490748868263493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7556,
      "step": 178848
    },
    {
      "epoch": 0.000649191021177186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 178880
    },
    {
      "epoch": 0.0006493071555280227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 178912
    },
    {
      "epoch": 0.0006494232898788594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 178944
    },
    {
      "epoch": 0.000649539424229696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 178976
    },
    {
      "epoch": 0.0006496555585805328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 179008
    },
    {
      "epoch": 0.0006497716929313695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 179040
    },
    {
      "epoch": 0.0006498878272822062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 179072
    },
    {
      "epoch": 0.000650003961633043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 179104
    },
    {
      "epoch": 0.0006501200959838796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 179136
    },
    {
      "epoch": 0.0006502362303347164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 179168
    },
    {
      "epoch": 0.000650352364685553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 179200
    },
    {
      "epoch": 0.0006504684990363898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 179232
    },
    {
      "epoch": 0.0006505846333872264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 179264
    },
    {
      "epoch": 0.0006507007677380632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 179296
    },
    {
      "epoch": 0.0006508169020888998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 179328
    },
    {
      "epoch": 0.0006509330364397366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 179360
    },
    {
      "epoch": 0.0006510491707905733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 179392
    },
    {
      "epoch": 0.00065116530514141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 179424
    },
    {
      "epoch": 0.0006512814394922467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 179456
    },
    {
      "epoch": 0.0006513975738430834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 179488
    },
    {
      "epoch": 0.0006515137081939201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 179520
    },
    {
      "epoch": 0.0006516298425447568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 179552
    },
    {
      "epoch": 0.0006517459768955935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 179584
    },
    {
      "epoch": 0.0006518621112464302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 179616
    },
    {
      "epoch": 0.0006519782455972669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 179648
    },
    {
      "epoch": 0.0006520943799481037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 179680
    },
    {
      "epoch": 0.0006522105142989403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7645,
      "step": 179712
    },
    {
      "epoch": 0.0006523266486497771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 179744
    },
    {
      "epoch": 0.0006524427830006137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 179776
    },
    {
      "epoch": 0.0006525589173514505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 179808
    },
    {
      "epoch": 0.0006526750517022871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 179840
    },
    {
      "epoch": 0.0006527911860531239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 179872
    },
    {
      "epoch": 0.0006529073204039605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 179904
    },
    {
      "epoch": 0.0006530234547547973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 179936
    },
    {
      "epoch": 0.000653139589105634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 179968
    },
    {
      "epoch": 0.0006532557234564707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 180000
    },
    {
      "epoch": 0.0006533718578073074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 180032
    },
    {
      "epoch": 0.0006534879921581441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 180064
    },
    {
      "epoch": 0.0006536041265089808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 180096
    },
    {
      "epoch": 0.0006537202608598175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 180128
    },
    {
      "epoch": 0.0006538363952106542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 180160
    },
    {
      "epoch": 0.0006539525295614909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 180192
    },
    {
      "epoch": 0.0006540686639123276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 180224
    },
    {
      "epoch": 0.0006541847982631644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 180256
    },
    {
      "epoch": 0.000654300932614001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 180288
    },
    {
      "epoch": 0.0006544170669648378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.691,
      "step": 180320
    },
    {
      "epoch": 0.0006545332013156744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 180352
    },
    {
      "epoch": 0.0006546493356665112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 180384
    },
    {
      "epoch": 0.0006547654700173478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 180416
    },
    {
      "epoch": 0.0006548816043681846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 180448
    },
    {
      "epoch": 0.0006549977387190212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 180480
    },
    {
      "epoch": 0.000655113873069858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 180512
    },
    {
      "epoch": 0.0006552300074206947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7502,
      "step": 180544
    },
    {
      "epoch": 0.0006553461417715314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7485,
      "step": 180576
    },
    {
      "epoch": 0.0006554622761223681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 180608
    },
    {
      "epoch": 0.0006555784104732048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 180640
    },
    {
      "epoch": 0.0006556945448240415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 180672
    },
    {
      "epoch": 0.0006558106791748782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 180704
    },
    {
      "epoch": 0.0006559268135257149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 180736
    },
    {
      "epoch": 0.0006560429478765516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 180768
    },
    {
      "epoch": 0.0006561590822273883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 180800
    },
    {
      "epoch": 0.0006562752165782251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 180832
    },
    {
      "epoch": 0.0006563913509290617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 180864
    },
    {
      "epoch": 0.0006565074852798985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 180896
    },
    {
      "epoch": 0.0006566236196307351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 180928
    },
    {
      "epoch": 0.0006567397539815719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 180960
    },
    {
      "epoch": 0.0006568558883324085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 180992
    },
    {
      "epoch": 0.0006569720226832453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 181024
    },
    {
      "epoch": 0.0006570881570340819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 181056
    },
    {
      "epoch": 0.0006572042913849187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 181088
    },
    {
      "epoch": 0.0006573204257357554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 181120
    },
    {
      "epoch": 0.0006574365600865921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 181152
    },
    {
      "epoch": 0.0006575526944374288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 181184
    },
    {
      "epoch": 0.0006576688287882655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 181216
    },
    {
      "epoch": 0.0006577849631391022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 181248
    },
    {
      "epoch": 0.0006579010974899389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 181280
    },
    {
      "epoch": 0.0006580172318407756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 181312
    },
    {
      "epoch": 0.0006581333661916123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 181344
    },
    {
      "epoch": 0.000658249500542449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 181376
    },
    {
      "epoch": 0.0006583656348932858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 181408
    },
    {
      "epoch": 0.0006584817692441224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 181440
    },
    {
      "epoch": 0.0006585979035949592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7509,
      "step": 181472
    },
    {
      "epoch": 0.0006587140379457958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 181504
    },
    {
      "epoch": 0.0006588301722966326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 181536
    },
    {
      "epoch": 0.0006589463066474692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 181568
    },
    {
      "epoch": 0.000659062440998306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 181600
    },
    {
      "epoch": 0.0006591785753491426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 181632
    },
    {
      "epoch": 0.0006592947096999794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 181664
    },
    {
      "epoch": 0.0006594108440508161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 181696
    },
    {
      "epoch": 0.0006595269784016528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 181728
    },
    {
      "epoch": 0.0006596431127524895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 181760
    },
    {
      "epoch": 0.0006597592471033262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 181792
    },
    {
      "epoch": 0.0006598753814541629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 181824
    },
    {
      "epoch": 0.0006599915158049996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 181856
    },
    {
      "epoch": 0.0006601076501558363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 181888
    },
    {
      "epoch": 0.000660223784506673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 181920
    },
    {
      "epoch": 0.0006603399188575097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 181952
    },
    {
      "epoch": 0.0006604560532083465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 181984
    },
    {
      "epoch": 0.0006605721875591831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 182016
    },
    {
      "epoch": 0.0006606883219100199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 182048
    },
    {
      "epoch": 0.0006608044562608565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 182080
    },
    {
      "epoch": 0.0006609205906116933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 182112
    },
    {
      "epoch": 0.0006610367249625299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.754,
      "step": 182144
    },
    {
      "epoch": 0.0006611528593133667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 182176
    },
    {
      "epoch": 0.0006612689936642033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 182208
    },
    {
      "epoch": 0.0006613851280150401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 182240
    },
    {
      "epoch": 0.0006615012623658768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 182272
    },
    {
      "epoch": 0.0006616173967167135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 182304
    },
    {
      "epoch": 0.0006617335310675502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7464,
      "step": 182336
    },
    {
      "epoch": 0.0006618496654183869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 182368
    },
    {
      "epoch": 0.0006619657997692236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 182400
    },
    {
      "epoch": 0.0006620819341200603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 182432
    },
    {
      "epoch": 0.000662198068470897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 182464
    },
    {
      "epoch": 0.0006623142028217337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 182496
    },
    {
      "epoch": 0.0006624303371725704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 182528
    },
    {
      "epoch": 0.0006625464715234072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 182560
    },
    {
      "epoch": 0.0006626626058742438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 182592
    },
    {
      "epoch": 0.0006627787402250806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 182624
    },
    {
      "epoch": 0.0006628948745759172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 182656
    },
    {
      "epoch": 0.000663011008926754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 182688
    },
    {
      "epoch": 0.0006631271432775906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 182720
    },
    {
      "epoch": 0.0006632432776284274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 182752
    },
    {
      "epoch": 0.000663359411979264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 182784
    },
    {
      "epoch": 0.0006634755463301008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 182816
    },
    {
      "epoch": 0.0006635916806809376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 182848
    },
    {
      "epoch": 0.0006637078150317742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 182880
    },
    {
      "epoch": 0.000663823949382611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 182912
    },
    {
      "epoch": 0.0006639400837334476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 182944
    },
    {
      "epoch": 0.0006640562180842844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 182976
    },
    {
      "epoch": 0.000664172352435121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 183008
    },
    {
      "epoch": 0.0006642884867859578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 183040
    },
    {
      "epoch": 0.0006644046211367944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 183072
    },
    {
      "epoch": 0.0006645207554876312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 183104
    },
    {
      "epoch": 0.0006646368898384679,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 183136
    },
    {
      "epoch": 0.0006647530241893046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7446,
      "step": 183168
    },
    {
      "epoch": 0.0006648691585401413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 183200
    },
    {
      "epoch": 0.000664985292890978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7484,
      "step": 183232
    },
    {
      "epoch": 0.0006651014272418147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 183264
    },
    {
      "epoch": 0.0006652175615926514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 183296
    },
    {
      "epoch": 0.0006653336959434881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 183328
    },
    {
      "epoch": 0.0006654498302943248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 183360
    },
    {
      "epoch": 0.0006655659646451615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 183392
    },
    {
      "epoch": 0.0006656820989959983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 183424
    },
    {
      "epoch": 0.0006657982333468349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 183456
    },
    {
      "epoch": 0.0006659143676976717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 183488
    },
    {
      "epoch": 0.0006660305020485083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 183520
    },
    {
      "epoch": 0.0006661466363993451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 183552
    },
    {
      "epoch": 0.0006662627707501817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 183584
    },
    {
      "epoch": 0.0006663789051010185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 183616
    },
    {
      "epoch": 0.0006664950394518551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 183648
    },
    {
      "epoch": 0.0006666111738026919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 183680
    },
    {
      "epoch": 0.0006667273081535286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 183712
    },
    {
      "epoch": 0.0006668434425043653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 183744
    },
    {
      "epoch": 0.000666959576855202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 183776
    },
    {
      "epoch": 0.0006670757112060387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 183808
    },
    {
      "epoch": 0.0006671918455568754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 183840
    },
    {
      "epoch": 0.0006673079799077121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 183872
    },
    {
      "epoch": 0.0006674241142585488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 183904
    },
    {
      "epoch": 0.0006675402486093855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 183936
    },
    {
      "epoch": 0.0006676563829602222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 183968
    },
    {
      "epoch": 0.000667772517311059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 184000
    },
    {
      "epoch": 0.0006678886516618956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 184032
    },
    {
      "epoch": 0.0006680047860127324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7545,
      "step": 184064
    },
    {
      "epoch": 0.000668120920363569,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7456,
      "step": 184096
    },
    {
      "epoch": 0.0006682370547144058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 184128
    },
    {
      "epoch": 0.0006683531890652424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 184160
    },
    {
      "epoch": 0.0006684693234160792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 184192
    },
    {
      "epoch": 0.0006685854577669158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 184224
    },
    {
      "epoch": 0.0006687015921177526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 184256
    },
    {
      "epoch": 0.0006688177264685893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 184288
    },
    {
      "epoch": 0.000668933860819426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 184320
    },
    {
      "epoch": 0.0006690499951702627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 184352
    },
    {
      "epoch": 0.0006691661295210994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 184384
    },
    {
      "epoch": 0.0006692822638719361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 184416
    },
    {
      "epoch": 0.0006693983982227728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 184448
    },
    {
      "epoch": 0.0006695145325736095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 184480
    },
    {
      "epoch": 0.0006696306669244462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 184512
    },
    {
      "epoch": 0.0006697468012752829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 184544
    },
    {
      "epoch": 0.0006698629356261197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 184576
    },
    {
      "epoch": 0.0006699790699769563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 184608
    },
    {
      "epoch": 0.0006700952043277931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 184640
    },
    {
      "epoch": 0.0006702113386786297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 184672
    },
    {
      "epoch": 0.0006703274730294665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 184704
    },
    {
      "epoch": 0.0006704436073803031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 184736
    },
    {
      "epoch": 0.0006705597417311399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 184768
    },
    {
      "epoch": 0.0006706758760819765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 184800
    },
    {
      "epoch": 0.0006707920104328133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 184832
    },
    {
      "epoch": 0.00067090814478365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 184864
    },
    {
      "epoch": 0.0006710242791344867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 184896
    },
    {
      "epoch": 0.0006711404134853234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7609,
      "step": 184928
    },
    {
      "epoch": 0.0006712565478361601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 184960
    },
    {
      "epoch": 0.0006713726821869968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 184992
    },
    {
      "epoch": 0.0006714888165378335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 185024
    },
    {
      "epoch": 0.0006716049508886702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 185056
    },
    {
      "epoch": 0.0006717210852395069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 185088
    },
    {
      "epoch": 0.0006718372195903436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 185120
    },
    {
      "epoch": 0.0006719533539411804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6915,
      "step": 185152
    },
    {
      "epoch": 0.000672069488292017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 185184
    },
    {
      "epoch": 0.0006721856226428538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 185216
    },
    {
      "epoch": 0.0006723017569936904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 185248
    },
    {
      "epoch": 0.0006724178913445272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 185280
    },
    {
      "epoch": 0.0006725340256953638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 185312
    },
    {
      "epoch": 0.0006726501600462006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 185344
    },
    {
      "epoch": 0.0006727662943970372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 185376
    },
    {
      "epoch": 0.000672882428747874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 185408
    },
    {
      "epoch": 0.0006729985630987107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 185440
    },
    {
      "epoch": 0.0006731146974495474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 185472
    },
    {
      "epoch": 0.0006732308318003841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 185504
    },
    {
      "epoch": 0.0006733469661512208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 185536
    },
    {
      "epoch": 0.0006734631005020575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 185568
    },
    {
      "epoch": 0.0006735792348528942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 185600
    },
    {
      "epoch": 0.0006736953692037309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 185632
    },
    {
      "epoch": 0.0006738115035545676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 185664
    },
    {
      "epoch": 0.0006739276379054043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 185696
    },
    {
      "epoch": 0.0006740437722562411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 185728
    },
    {
      "epoch": 0.0006741599066070777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 185760
    },
    {
      "epoch": 0.0006742760409579145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7545,
      "step": 185792
    },
    {
      "epoch": 0.0006743921753087511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 185824
    },
    {
      "epoch": 0.0006745083096595879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 185856
    },
    {
      "epoch": 0.0006746244440104245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 185888
    },
    {
      "epoch": 0.0006747405783612613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 185920
    },
    {
      "epoch": 0.0006748567127120979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 185952
    },
    {
      "epoch": 0.0006749728470629347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 185984
    },
    {
      "epoch": 0.0006750889814137714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 186016
    },
    {
      "epoch": 0.0006752051157646081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 186048
    },
    {
      "epoch": 0.0006753212501154448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 186080
    },
    {
      "epoch": 0.0006754373844662815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 186112
    },
    {
      "epoch": 0.0006755535188171182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 186144
    },
    {
      "epoch": 0.0006756696531679549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 186176
    },
    {
      "epoch": 0.0006757857875187916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 186208
    },
    {
      "epoch": 0.0006759019218696283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 186240
    },
    {
      "epoch": 0.000676018056220465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 186272
    },
    {
      "epoch": 0.0006761341905713018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 186304
    },
    {
      "epoch": 0.0006762503249221384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 186336
    },
    {
      "epoch": 0.0006763664592729752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 186368
    },
    {
      "epoch": 0.0006764825936238118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 186400
    },
    {
      "epoch": 0.0006765987279746486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 186432
    },
    {
      "epoch": 0.0006767148623254852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 186464
    },
    {
      "epoch": 0.000676830996676322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 186496
    },
    {
      "epoch": 0.0006769471310271586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 186528
    },
    {
      "epoch": 0.0006770632653779954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 186560
    },
    {
      "epoch": 0.0006771793997288321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 186592
    },
    {
      "epoch": 0.0006772955340796688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 186624
    },
    {
      "epoch": 0.0006774116684305055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 186656
    },
    {
      "epoch": 0.0006775278027813422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 186688
    },
    {
      "epoch": 0.000677643937132179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 186720
    },
    {
      "epoch": 0.0006777600714830156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 186752
    },
    {
      "epoch": 0.0006778762058338523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 186784
    },
    {
      "epoch": 0.000677992340184689,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 186816
    },
    {
      "epoch": 0.0006781084745355257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 186848
    },
    {
      "epoch": 0.0006782246088863625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 186880
    },
    {
      "epoch": 0.0006783407432371991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 186912
    },
    {
      "epoch": 0.0006784568775880359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 186944
    },
    {
      "epoch": 0.0006785730119388725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 186976
    },
    {
      "epoch": 0.0006786891462897093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 187008
    },
    {
      "epoch": 0.000678805280640546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 187040
    },
    {
      "epoch": 0.0006789214149913827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 187072
    },
    {
      "epoch": 0.0006790375493422193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 187104
    },
    {
      "epoch": 0.0006791536836930561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 187136
    },
    {
      "epoch": 0.0006792698180438929,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 187168
    },
    {
      "epoch": 0.0006793859523947295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 187200
    },
    {
      "epoch": 0.0006795020867455663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 187232
    },
    {
      "epoch": 0.0006796182210964029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 187264
    },
    {
      "epoch": 0.0006797343554472397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 187296
    },
    {
      "epoch": 0.0006798504897980763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 187328
    },
    {
      "epoch": 0.0006799666241489131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 187360
    },
    {
      "epoch": 0.0006800827584997497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 187392
    },
    {
      "epoch": 0.0006801988928505865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 187424
    },
    {
      "epoch": 0.0006803150272014232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 187456
    },
    {
      "epoch": 0.0006804311615522599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 187488
    },
    {
      "epoch": 0.0006805472959030966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 187520
    },
    {
      "epoch": 0.0006806634302539333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 187552
    },
    {
      "epoch": 0.00068077956460477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7459,
      "step": 187584
    },
    {
      "epoch": 0.0006808956989556067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 187616
    },
    {
      "epoch": 0.0006810118333064434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 187648
    },
    {
      "epoch": 0.0006811279676572801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 187680
    },
    {
      "epoch": 0.0006812441020081168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 187712
    },
    {
      "epoch": 0.0006813602363589536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 187744
    },
    {
      "epoch": 0.0006814763707097902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 187776
    },
    {
      "epoch": 0.000681592505060627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 187808
    },
    {
      "epoch": 0.0006817086394114636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 187840
    },
    {
      "epoch": 0.0006818247737623004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 187872
    },
    {
      "epoch": 0.000681940908113137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 187904
    },
    {
      "epoch": 0.0006820570424639738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 187936
    },
    {
      "epoch": 0.0006821731768148104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 187968
    },
    {
      "epoch": 0.0006822893111656472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 188000
    },
    {
      "epoch": 0.0006824054455164839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 188032
    },
    {
      "epoch": 0.0006825215798673206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 188064
    },
    {
      "epoch": 0.0006826377142181573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6818,
      "step": 188096
    },
    {
      "epoch": 0.000682753848568994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 188128
    },
    {
      "epoch": 0.0006828699829198307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 188160
    },
    {
      "epoch": 0.0006829861172706674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 188192
    },
    {
      "epoch": 0.0006831022516215041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 188224
    },
    {
      "epoch": 0.0006832183859723408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 188256
    },
    {
      "epoch": 0.0006833345203231775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 188288
    },
    {
      "epoch": 0.0006834506546740143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 188320
    },
    {
      "epoch": 0.0006835667890248509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 188352
    },
    {
      "epoch": 0.0006836829233756877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 188384
    },
    {
      "epoch": 0.0006837990577265243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 188416
    },
    {
      "epoch": 0.0006839151920773611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 188448
    },
    {
      "epoch": 0.0006840313264281977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 188480
    },
    {
      "epoch": 0.0006841474607790345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 188512
    },
    {
      "epoch": 0.0006842635951298711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 188544
    },
    {
      "epoch": 0.0006843797294807079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 188576
    },
    {
      "epoch": 0.0006844958638315445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 188608
    },
    {
      "epoch": 0.0006846119981823813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 188640
    },
    {
      "epoch": 0.000684728132533218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 188672
    },
    {
      "epoch": 0.0006848442668840547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 188704
    },
    {
      "epoch": 0.0006849604012348914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 188736
    },
    {
      "epoch": 0.0006850765355857281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 188768
    },
    {
      "epoch": 0.0006851926699365648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 188800
    },
    {
      "epoch": 0.0006853088042874015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 188832
    },
    {
      "epoch": 0.0006854249386382382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 188864
    },
    {
      "epoch": 0.0006855410729890749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 188896
    },
    {
      "epoch": 0.0006856572073399116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 188928
    },
    {
      "epoch": 0.0006857733416907484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 188960
    },
    {
      "epoch": 0.000685889476041585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 188992
    },
    {
      "epoch": 0.0006860056103924218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 189024
    },
    {
      "epoch": 0.0006861217447432584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 189056
    },
    {
      "epoch": 0.0006862378790940952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 189088
    },
    {
      "epoch": 0.0006863540134449318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 189120
    },
    {
      "epoch": 0.0006864701477957686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 189152
    },
    {
      "epoch": 0.0006865862821466052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 189184
    },
    {
      "epoch": 0.000686702416497442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 189216
    },
    {
      "epoch": 0.0006868185508482787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 189248
    },
    {
      "epoch": 0.0006869346851991154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 189280
    },
    {
      "epoch": 0.0006870508195499521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 189312
    },
    {
      "epoch": 0.0006871669539007888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 189344
    },
    {
      "epoch": 0.0006872830882516255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7286,
      "step": 189376
    },
    {
      "epoch": 0.0006873992226024622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 189408
    },
    {
      "epoch": 0.0006875153569532989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 189440
    },
    {
      "epoch": 0.0006876314913041356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 189472
    },
    {
      "epoch": 0.0006877476256549723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 189504
    },
    {
      "epoch": 0.0006878637600058091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 189536
    },
    {
      "epoch": 0.0006879798943566457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 189568
    },
    {
      "epoch": 0.0006880960287074825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 189600
    },
    {
      "epoch": 0.0006882121630583191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 189632
    },
    {
      "epoch": 0.0006883282974091559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 189664
    },
    {
      "epoch": 0.0006884444317599925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 189696
    },
    {
      "epoch": 0.0006885605661108293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 189728
    },
    {
      "epoch": 0.0006886767004616659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 189760
    },
    {
      "epoch": 0.0006887928348125027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 189792
    },
    {
      "epoch": 0.0006889089691633394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 189824
    },
    {
      "epoch": 0.0006890251035141761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 189856
    },
    {
      "epoch": 0.0006891412378650128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 189888
    },
    {
      "epoch": 0.0006892573722158495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 189920
    },
    {
      "epoch": 0.0006893735065666862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7479,
      "step": 189952
    },
    {
      "epoch": 0.0006894896409175229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 189984
    },
    {
      "epoch": 0.0006896057752683596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 190016
    },
    {
      "epoch": 0.0006897219096191963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 190048
    },
    {
      "epoch": 0.000689838043970033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 190080
    },
    {
      "epoch": 0.0006899541783208698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 190112
    },
    {
      "epoch": 0.0006900703126717064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 190144
    },
    {
      "epoch": 0.0006901864470225432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 190176
    },
    {
      "epoch": 0.0006903025813733798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 190208
    },
    {
      "epoch": 0.0006904187157242166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 190240
    },
    {
      "epoch": 0.0006905348500750532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 190272
    },
    {
      "epoch": 0.00069065098442589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 190304
    },
    {
      "epoch": 0.0006907671187767266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 190336
    },
    {
      "epoch": 0.0006908832531275634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 190368
    },
    {
      "epoch": 0.0006909993874784001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 190400
    },
    {
      "epoch": 0.0006911155218292368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 190432
    },
    {
      "epoch": 0.0006912316561800735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 190464
    },
    {
      "epoch": 0.0006913477905309102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 190496
    },
    {
      "epoch": 0.0006914639248817469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 190528
    },
    {
      "epoch": 0.0006915800592325836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 190560
    },
    {
      "epoch": 0.0006916961935834203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 190592
    },
    {
      "epoch": 0.000691812327934257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 190624
    },
    {
      "epoch": 0.0006919284622850937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 190656
    },
    {
      "epoch": 0.0006920445966359305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 190688
    },
    {
      "epoch": 0.0006921607309867671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 190720
    },
    {
      "epoch": 0.0006922768653376039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 190752
    },
    {
      "epoch": 0.0006923929996884405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 190784
    },
    {
      "epoch": 0.0006925091340392773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 190816
    },
    {
      "epoch": 0.0006926252683901139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 190848
    },
    {
      "epoch": 0.0006927414027409507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 190880
    },
    {
      "epoch": 0.0006928575370917873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 190912
    },
    {
      "epoch": 0.0006929736714426241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 190944
    },
    {
      "epoch": 0.0006930898057934609,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 190976
    },
    {
      "epoch": 0.0006932059401442975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 191008
    },
    {
      "epoch": 0.0006933220744951343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6956,
      "step": 191040
    },
    {
      "epoch": 0.0006934382088459709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 191072
    },
    {
      "epoch": 0.0006935543431968077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 191104
    },
    {
      "epoch": 0.0006936704775476443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 191136
    },
    {
      "epoch": 0.000693786611898481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 191168
    },
    {
      "epoch": 0.0006939027462493177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 191200
    },
    {
      "epoch": 0.0006940188806001544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 191232
    },
    {
      "epoch": 0.0006941350149509912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 191264
    },
    {
      "epoch": 0.0006942511493018278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 191296
    },
    {
      "epoch": 0.0006943672836526646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 191328
    },
    {
      "epoch": 0.0006944834180035012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 191360
    },
    {
      "epoch": 0.000694599552354338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 191392
    },
    {
      "epoch": 0.0006947156867051746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 191424
    },
    {
      "epoch": 0.0006948318210560114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 191456
    },
    {
      "epoch": 0.000694947955406848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 191488
    },
    {
      "epoch": 0.0006950640897576848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 191520
    },
    {
      "epoch": 0.0006951802241085216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 191552
    },
    {
      "epoch": 0.0006952963584593582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 191584
    },
    {
      "epoch": 0.000695412492810195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 191616
    },
    {
      "epoch": 0.0006955286271610316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 191648
    },
    {
      "epoch": 0.0006956447615118684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 191680
    },
    {
      "epoch": 0.000695760895862705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 191712
    },
    {
      "epoch": 0.0006958770302135418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 191744
    },
    {
      "epoch": 0.0006959931645643784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 191776
    },
    {
      "epoch": 0.0006961092989152152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 191808
    },
    {
      "epoch": 0.0006962254332660519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 191840
    },
    {
      "epoch": 0.0006963415676168886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 191872
    },
    {
      "epoch": 0.0006964577019677253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 191904
    },
    {
      "epoch": 0.000696573836318562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 191936
    },
    {
      "epoch": 0.0006966899706693987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 191968
    },
    {
      "epoch": 0.0006968061050202354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 192000
    },
    {
      "epoch": 0.0006969222393710721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 192032
    },
    {
      "epoch": 0.0006970383737219088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 192064
    },
    {
      "epoch": 0.0006971545080727455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 192096
    },
    {
      "epoch": 0.0006972706424235823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 192128
    },
    {
      "epoch": 0.0006973867767744189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 192160
    },
    {
      "epoch": 0.0006975029111252557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 192192
    },
    {
      "epoch": 0.0006976190454760923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 192224
    },
    {
      "epoch": 0.0006977351798269291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 192256
    },
    {
      "epoch": 0.0006978513141777657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 192288
    },
    {
      "epoch": 0.0006979674485286025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 192320
    },
    {
      "epoch": 0.0006980835828794391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 192352
    },
    {
      "epoch": 0.0006981997172302759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 192384
    },
    {
      "epoch": 0.0006983158515811126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 192416
    },
    {
      "epoch": 0.0006984319859319493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 192448
    },
    {
      "epoch": 0.000698548120282786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 192480
    },
    {
      "epoch": 0.0006986642546336227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 192512
    },
    {
      "epoch": 0.0006987803889844594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 192544
    },
    {
      "epoch": 0.0006988965233352961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 192576
    },
    {
      "epoch": 0.0006990126576861328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 192608
    },
    {
      "epoch": 0.0006991287920369695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 192640
    },
    {
      "epoch": 0.0006992449263878062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7593,
      "step": 192672
    },
    {
      "epoch": 0.000699361060738643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 192704
    },
    {
      "epoch": 0.0006994771950894796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 192736
    },
    {
      "epoch": 0.0006995933294403164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 192768
    },
    {
      "epoch": 0.000699709463791153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 192800
    },
    {
      "epoch": 0.0006998255981419898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 192832
    },
    {
      "epoch": 0.0006999417324928264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 192864
    },
    {
      "epoch": 0.0007000578668436632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 192896
    },
    {
      "epoch": 0.0007001740011944998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 192928
    },
    {
      "epoch": 0.0007002901355453366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 192960
    },
    {
      "epoch": 0.0007004062698961733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 192992
    },
    {
      "epoch": 0.00070052240424701,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 193024
    },
    {
      "epoch": 0.0007006385385978467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 193056
    },
    {
      "epoch": 0.0007007546729486834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 193088
    },
    {
      "epoch": 0.0007008708072995201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 193120
    },
    {
      "epoch": 0.0007009869416503568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 193152
    },
    {
      "epoch": 0.0007011030760011935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 193184
    },
    {
      "epoch": 0.0007012192103520302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6919,
      "step": 193216
    },
    {
      "epoch": 0.0007013353447028669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 193248
    },
    {
      "epoch": 0.0007014514790537037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 193280
    },
    {
      "epoch": 0.0007015676134045403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 193312
    },
    {
      "epoch": 0.0007016837477553771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 193344
    },
    {
      "epoch": 0.0007017998821062137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 193376
    },
    {
      "epoch": 0.0007019160164570505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 193408
    },
    {
      "epoch": 0.0007020321508078871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 193440
    },
    {
      "epoch": 0.0007021482851587239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 193472
    },
    {
      "epoch": 0.0007022644195095605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 193504
    },
    {
      "epoch": 0.0007023805538603973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 193536
    },
    {
      "epoch": 0.000702496688211234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 193568
    },
    {
      "epoch": 0.0007026128225620707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 193600
    },
    {
      "epoch": 0.0007027289569129074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 193632
    },
    {
      "epoch": 0.0007028450912637441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 193664
    },
    {
      "epoch": 0.0007029612256145808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 193696
    },
    {
      "epoch": 0.0007030773599654175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 193728
    },
    {
      "epoch": 0.0007031934943162542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 193760
    },
    {
      "epoch": 0.0007033096286670909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 193792
    },
    {
      "epoch": 0.0007034257630179276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 193824
    },
    {
      "epoch": 0.0007035418973687644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 193856
    },
    {
      "epoch": 0.000703658031719601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 193888
    },
    {
      "epoch": 0.0007037741660704378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 193920
    },
    {
      "epoch": 0.0007038903004212744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 193952
    },
    {
      "epoch": 0.0007040064347721112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 193984
    },
    {
      "epoch": 0.0007041225691229478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 194016
    },
    {
      "epoch": 0.0007042387034737846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 194048
    },
    {
      "epoch": 0.0007043548378246212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 194080
    },
    {
      "epoch": 0.000704470972175458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 194112
    },
    {
      "epoch": 0.0007045871065262947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 194144
    },
    {
      "epoch": 0.0007047032408771314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 194176
    },
    {
      "epoch": 0.0007048193752279681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 194208
    },
    {
      "epoch": 0.0007049355095788048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 194240
    },
    {
      "epoch": 0.0007050516439296415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 194272
    },
    {
      "epoch": 0.0007051677782804782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 194304
    },
    {
      "epoch": 0.0007052839126313149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 194336
    },
    {
      "epoch": 0.0007054000469821516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 194368
    },
    {
      "epoch": 0.0007055161813329883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 194400
    },
    {
      "epoch": 0.0007056323156838251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 194432
    },
    {
      "epoch": 0.0007057484500346617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 194464
    },
    {
      "epoch": 0.0007058645843854985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 194496
    },
    {
      "epoch": 0.0007059807187363351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 194528
    },
    {
      "epoch": 0.0007060968530871719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 194560
    },
    {
      "epoch": 0.0007062129874380085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 194592
    },
    {
      "epoch": 0.0007063291217888453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 194624
    },
    {
      "epoch": 0.0007064452561396819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 194656
    },
    {
      "epoch": 0.0007065613904905187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 194688
    },
    {
      "epoch": 0.0007066775248413554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 194720
    },
    {
      "epoch": 0.0007067936591921921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 194752
    },
    {
      "epoch": 0.0007069097935430288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 194784
    },
    {
      "epoch": 0.0007070259278938655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 194816
    },
    {
      "epoch": 0.0007071420622447022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 194848
    },
    {
      "epoch": 0.0007072581965955389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6863,
      "step": 194880
    },
    {
      "epoch": 0.0007073743309463756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 194912
    },
    {
      "epoch": 0.0007074904652972123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 194944
    },
    {
      "epoch": 0.000707606599648049,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 194976
    },
    {
      "epoch": 0.0007077227339988858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 195008
    },
    {
      "epoch": 0.0007078388683497224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 195040
    },
    {
      "epoch": 0.0007079550027005592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 195072
    },
    {
      "epoch": 0.0007080711370513958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 195104
    },
    {
      "epoch": 0.0007081872714022326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 195136
    },
    {
      "epoch": 0.0007083034057530692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 195168
    },
    {
      "epoch": 0.000708419540103906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 195200
    },
    {
      "epoch": 0.0007085356744547426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 195232
    },
    {
      "epoch": 0.0007086518088055794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 195264
    },
    {
      "epoch": 0.0007087679431564162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 195296
    },
    {
      "epoch": 0.0007088840775072528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 195328
    },
    {
      "epoch": 0.0007090002118580896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 195360
    },
    {
      "epoch": 0.0007091163462089262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 195392
    },
    {
      "epoch": 0.000709232480559763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 195424
    },
    {
      "epoch": 0.0007093486149105996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 195456
    },
    {
      "epoch": 0.0007094647492614364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7493,
      "step": 195488
    },
    {
      "epoch": 0.000709580883612273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 195520
    },
    {
      "epoch": 0.0007096970179631098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 195552
    },
    {
      "epoch": 0.0007098131523139465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 195584
    },
    {
      "epoch": 0.0007099292866647832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 195616
    },
    {
      "epoch": 0.0007100454210156199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 195648
    },
    {
      "epoch": 0.0007101615553664566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 195680
    },
    {
      "epoch": 0.0007102776897172933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 195712
    },
    {
      "epoch": 0.00071039382406813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 195744
    },
    {
      "epoch": 0.0007105099584189667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 195776
    },
    {
      "epoch": 0.0007106260927698034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 195808
    },
    {
      "epoch": 0.0007107422271206401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 195840
    },
    {
      "epoch": 0.0007108583614714769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 195872
    },
    {
      "epoch": 0.0007109744958223135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 195904
    },
    {
      "epoch": 0.0007110906301731503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 195936
    },
    {
      "epoch": 0.0007112067645239869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 195968
    },
    {
      "epoch": 0.0007113228988748237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 196000
    },
    {
      "epoch": 0.0007114390332256603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 196032
    },
    {
      "epoch": 0.0007115551675764971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 196064
    },
    {
      "epoch": 0.0007116713019273337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7459,
      "step": 196096
    },
    {
      "epoch": 0.0007117874362781705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 196128
    },
    {
      "epoch": 0.0007119035706290072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 196160
    },
    {
      "epoch": 0.0007120197049798439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 196192
    },
    {
      "epoch": 0.0007121358393306806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 196224
    },
    {
      "epoch": 0.0007122519736815173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 196256
    },
    {
      "epoch": 0.000712368108032354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 196288
    },
    {
      "epoch": 0.0007124842423831907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 196320
    },
    {
      "epoch": 0.0007126003767340274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 196352
    },
    {
      "epoch": 0.0007127165110848641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 196384
    },
    {
      "epoch": 0.0007128326454357008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 196416
    },
    {
      "epoch": 0.0007129487797865376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 196448
    },
    {
      "epoch": 0.0007130649141373742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 196480
    },
    {
      "epoch": 0.000713181048488211,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 196512
    },
    {
      "epoch": 0.0007132971828390476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 196544
    },
    {
      "epoch": 0.0007134133171898844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6886,
      "step": 196576
    },
    {
      "epoch": 0.000713529451540721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 196608
    },
    {
      "epoch": 0.0007136455858915578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 196640
    },
    {
      "epoch": 0.0007137617202423944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 196672
    },
    {
      "epoch": 0.0007138778545932312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 196704
    },
    {
      "epoch": 0.0007139939889440679,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 196736
    },
    {
      "epoch": 0.0007141101232949046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 196768
    },
    {
      "epoch": 0.0007142262576457413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 196800
    },
    {
      "epoch": 0.000714342391996578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 196832
    },
    {
      "epoch": 0.0007144585263474147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 196864
    },
    {
      "epoch": 0.0007145746606982514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 196896
    },
    {
      "epoch": 0.0007146907950490881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 196928
    },
    {
      "epoch": 0.0007148069293999248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 196960
    },
    {
      "epoch": 0.0007149230637507615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 196992
    },
    {
      "epoch": 0.0007150391981015983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 197024
    },
    {
      "epoch": 0.0007151553324524349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 197056
    },
    {
      "epoch": 0.0007152714668032717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 197088
    },
    {
      "epoch": 0.0007153876011541083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 197120
    },
    {
      "epoch": 0.0007155037355049451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 197152
    },
    {
      "epoch": 0.0007156198698557817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 197184
    },
    {
      "epoch": 0.0007157360042066185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 197216
    },
    {
      "epoch": 0.0007158521385574551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 197248
    },
    {
      "epoch": 0.0007159682729082919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 197280
    },
    {
      "epoch": 0.0007160844072591286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 197312
    },
    {
      "epoch": 0.0007162005416099653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 197344
    },
    {
      "epoch": 0.000716316675960802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 197376
    },
    {
      "epoch": 0.0007164328103116387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 197408
    },
    {
      "epoch": 0.0007165489446624754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 197440
    },
    {
      "epoch": 0.0007166650790133121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 197472
    },
    {
      "epoch": 0.0007167812133641488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 197504
    },
    {
      "epoch": 0.0007168973477149855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 197536
    },
    {
      "epoch": 0.0007170134820658222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 197568
    },
    {
      "epoch": 0.000717129616416659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 197600
    },
    {
      "epoch": 0.0007172457507674956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 197632
    },
    {
      "epoch": 0.0007173618851183324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 197664
    },
    {
      "epoch": 0.000717478019469169,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 197696
    },
    {
      "epoch": 0.0007175941538200058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 197728
    },
    {
      "epoch": 0.0007177102881708424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 197760
    },
    {
      "epoch": 0.0007178264225216792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 197792
    },
    {
      "epoch": 0.0007179425568725158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 197824
    },
    {
      "epoch": 0.0007180586912233526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 197856
    },
    {
      "epoch": 0.0007181748255741893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 197888
    },
    {
      "epoch": 0.000718290959925026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 197920
    },
    {
      "epoch": 0.0007184070942758627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 197952
    },
    {
      "epoch": 0.0007185232286266994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 197984
    },
    {
      "epoch": 0.0007186393629775361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 198016
    },
    {
      "epoch": 0.0007187554973283728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 198048
    },
    {
      "epoch": 0.0007188716316792095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 198080
    },
    {
      "epoch": 0.0007189877660300462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 198112
    },
    {
      "epoch": 0.0007191039003808829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 198144
    },
    {
      "epoch": 0.0007192200347317197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 198176
    },
    {
      "epoch": 0.0007193361690825563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 198208
    },
    {
      "epoch": 0.0007194523034333931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 198240
    },
    {
      "epoch": 0.0007195684377842297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 198272
    },
    {
      "epoch": 0.0007196845721350665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6832,
      "step": 198304
    },
    {
      "epoch": 0.0007198007064859031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6934,
      "step": 198336
    },
    {
      "epoch": 0.0007199168408367399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 198368
    },
    {
      "epoch": 0.0007200329751875765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 198400
    },
    {
      "epoch": 0.0007201491095384133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 198432
    },
    {
      "epoch": 0.00072026524388925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 198464
    },
    {
      "epoch": 0.0007203813782400867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 198496
    },
    {
      "epoch": 0.0007204975125909234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 198528
    },
    {
      "epoch": 0.0007206136469417601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7488,
      "step": 198560
    },
    {
      "epoch": 0.0007207297812925968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 198592
    },
    {
      "epoch": 0.0007208459156434335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 198624
    },
    {
      "epoch": 0.0007209620499942702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 198656
    },
    {
      "epoch": 0.0007210781843451069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 198688
    },
    {
      "epoch": 0.0007211943186959436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 198720
    },
    {
      "epoch": 0.0007213104530467804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 198752
    },
    {
      "epoch": 0.000721426587397617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 198784
    },
    {
      "epoch": 0.0007215427217484538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 198816
    },
    {
      "epoch": 0.0007216588560992904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 198848
    },
    {
      "epoch": 0.0007217749904501272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 198880
    },
    {
      "epoch": 0.0007218911248009638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 198912
    },
    {
      "epoch": 0.0007220072591518006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 198944
    },
    {
      "epoch": 0.0007221233935026372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 198976
    },
    {
      "epoch": 0.000722239527853474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 199008
    },
    {
      "epoch": 0.0007223556622043107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 199040
    },
    {
      "epoch": 0.0007224717965551474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 199072
    },
    {
      "epoch": 0.0007225879309059841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 199104
    },
    {
      "epoch": 0.0007227040652568208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 199136
    },
    {
      "epoch": 0.0007228201996076575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 199168
    },
    {
      "epoch": 0.0007229363339584942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 199200
    },
    {
      "epoch": 0.0007230524683093309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 199232
    },
    {
      "epoch": 0.0007231686026601676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 199264
    },
    {
      "epoch": 0.0007232847370110043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 199296
    },
    {
      "epoch": 0.000723400871361841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 199328
    },
    {
      "epoch": 0.0007235170057126777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 199360
    },
    {
      "epoch": 0.0007236331400635145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 199392
    },
    {
      "epoch": 0.0007237492744143511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 199424
    },
    {
      "epoch": 0.0007238654087651879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 199456
    },
    {
      "epoch": 0.0007239815431160245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 199488
    },
    {
      "epoch": 0.0007240976774668613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 199520
    },
    {
      "epoch": 0.0007242138118176979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 199552
    },
    {
      "epoch": 0.0007243299461685347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 199584
    },
    {
      "epoch": 0.0007244460805193713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 199616
    },
    {
      "epoch": 0.0007245622148702081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 199648
    },
    {
      "epoch": 0.0007246783492210449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 199680
    },
    {
      "epoch": 0.0007247944835718815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 199712
    },
    {
      "epoch": 0.0007249106179227183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 199744
    },
    {
      "epoch": 0.0007250267522735549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 199776
    },
    {
      "epoch": 0.0007251428866243917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 199808
    },
    {
      "epoch": 0.0007252590209752283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 199840
    },
    {
      "epoch": 0.000725375155326065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 199872
    },
    {
      "epoch": 0.0007254912896769017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 199904
    },
    {
      "epoch": 0.0007256074240277385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 199936
    },
    {
      "epoch": 0.0007257235583785752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 199968
    },
    {
      "epoch": 0.0007258396927294119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 200000
    },
    {
      "epoch": 0.0007259558270802486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 200032
    },
    {
      "epoch": 0.0007260719614310853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6853,
      "step": 200064
    },
    {
      "epoch": 0.000726188095781922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 200096
    },
    {
      "epoch": 0.0007263042301327587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 200128
    },
    {
      "epoch": 0.0007264203644835954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 200160
    },
    {
      "epoch": 0.000726536498834432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 200192
    },
    {
      "epoch": 0.0007266526331852688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 200224
    },
    {
      "epoch": 0.0007267687675361056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 200256
    },
    {
      "epoch": 0.0007268849018869422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 200288
    },
    {
      "epoch": 0.000727001036237779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 200320
    },
    {
      "epoch": 0.0007271171705886156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 200352
    },
    {
      "epoch": 0.0007272333049394524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 200384
    },
    {
      "epoch": 0.000727349439290289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 200416
    },
    {
      "epoch": 0.0007274655736411258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 200448
    },
    {
      "epoch": 0.0007275817079919624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 200480
    },
    {
      "epoch": 0.0007276978423427992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 200512
    },
    {
      "epoch": 0.0007278139766936359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 200544
    },
    {
      "epoch": 0.0007279301110444726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 200576
    },
    {
      "epoch": 0.0007280462453953093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 200608
    },
    {
      "epoch": 0.000728162379746146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 200640
    },
    {
      "epoch": 0.0007282785140969827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 200672
    },
    {
      "epoch": 0.0007283946484478194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 200704
    },
    {
      "epoch": 0.0007285107827986561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 200736
    },
    {
      "epoch": 0.0007286269171494928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 200768
    },
    {
      "epoch": 0.0007287430515003295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 200800
    },
    {
      "epoch": 0.0007288591858511663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 200832
    },
    {
      "epoch": 0.0007289753202020029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 200864
    },
    {
      "epoch": 0.0007290914545528397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 200896
    },
    {
      "epoch": 0.0007292075889036763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 200928
    },
    {
      "epoch": 0.0007293237232545131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 200960
    },
    {
      "epoch": 0.0007294398576053497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 200992
    },
    {
      "epoch": 0.0007295559919561865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 201024
    },
    {
      "epoch": 0.0007296721263070231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 201056
    },
    {
      "epoch": 0.0007297882606578599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 201088
    },
    {
      "epoch": 0.0007299043950086966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 201120
    },
    {
      "epoch": 0.0007300205293595333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 201152
    },
    {
      "epoch": 0.00073013666371037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7554,
      "step": 201184
    },
    {
      "epoch": 0.0007302527980612067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 201216
    },
    {
      "epoch": 0.0007303689324120434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 201248
    },
    {
      "epoch": 0.0007304850667628801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 201280
    },
    {
      "epoch": 0.0007306012011137168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 201312
    },
    {
      "epoch": 0.0007307173354645535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 201344
    },
    {
      "epoch": 0.0007308334698153902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 201376
    },
    {
      "epoch": 0.000730949604166227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 201408
    },
    {
      "epoch": 0.0007310657385170636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 201440
    },
    {
      "epoch": 0.0007311818728679004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 201472
    },
    {
      "epoch": 0.000731298007218737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 201504
    },
    {
      "epoch": 0.0007314141415695738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 201536
    },
    {
      "epoch": 0.0007315302759204104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 201568
    },
    {
      "epoch": 0.0007316464102712472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 201600
    },
    {
      "epoch": 0.0007317625446220838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 201632
    },
    {
      "epoch": 0.0007318786789729206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 201664
    },
    {
      "epoch": 0.0007319948133237573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 201696
    },
    {
      "epoch": 0.000732110947674594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 201728
    },
    {
      "epoch": 0.0007322270820254307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 201760
    },
    {
      "epoch": 0.0007323432163762674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 201792
    },
    {
      "epoch": 0.0007324593507271041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6886,
      "step": 201824
    },
    {
      "epoch": 0.0007325754850779408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 201856
    },
    {
      "epoch": 0.0007326916194287775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 201888
    },
    {
      "epoch": 0.0007328077537796142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 201920
    },
    {
      "epoch": 0.0007329238881304509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 201952
    },
    {
      "epoch": 0.0007330400224812877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7487,
      "step": 201984
    },
    {
      "epoch": 0.0007331561568321243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 202016
    },
    {
      "epoch": 0.0007332722911829611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 202048
    },
    {
      "epoch": 0.0007333884255337977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 202080
    },
    {
      "epoch": 0.0007335045598846345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 202112
    },
    {
      "epoch": 0.0007336206942354711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 202144
    },
    {
      "epoch": 0.0007337368285863079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 202176
    },
    {
      "epoch": 0.0007338529629371445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 202208
    },
    {
      "epoch": 0.0007339690972879813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 202240
    },
    {
      "epoch": 0.000734085231638818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 202272
    },
    {
      "epoch": 0.0007342013659896547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 202304
    },
    {
      "epoch": 0.0007343175003404914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 202336
    },
    {
      "epoch": 0.0007344336346913281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 202368
    },
    {
      "epoch": 0.0007345497690421648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 202400
    },
    {
      "epoch": 0.0007346659033930015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 202432
    },
    {
      "epoch": 0.0007347820377438382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 202464
    },
    {
      "epoch": 0.0007348981720946749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 202496
    },
    {
      "epoch": 0.0007350143064455116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 202528
    },
    {
      "epoch": 0.0007351304407963484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 202560
    },
    {
      "epoch": 0.000735246575147185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 202592
    },
    {
      "epoch": 0.0007353627094980218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 202624
    },
    {
      "epoch": 0.0007354788438488584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 202656
    },
    {
      "epoch": 0.0007355949781996952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 202688
    },
    {
      "epoch": 0.0007357111125505318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 202720
    },
    {
      "epoch": 0.0007358272469013686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 202752
    },
    {
      "epoch": 0.0007359433812522052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 202784
    },
    {
      "epoch": 0.000736059515603042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 202816
    },
    {
      "epoch": 0.0007361756499538787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 202848
    },
    {
      "epoch": 0.0007362917843047154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7504,
      "step": 202880
    },
    {
      "epoch": 0.0007364079186555521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7286,
      "step": 202912
    },
    {
      "epoch": 0.0007365240530063888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7493,
      "step": 202944
    },
    {
      "epoch": 0.0007366401873572255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 202976
    },
    {
      "epoch": 0.0007367563217080622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 203008
    },
    {
      "epoch": 0.0007368724560588989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 203040
    },
    {
      "epoch": 0.0007369885904097356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 203072
    },
    {
      "epoch": 0.0007371047247605723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 203104
    },
    {
      "epoch": 0.0007372208591114091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 203136
    },
    {
      "epoch": 0.0007373369934622457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 203168
    },
    {
      "epoch": 0.0007374531278130825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 203200
    },
    {
      "epoch": 0.0007375692621639191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 203232
    },
    {
      "epoch": 0.0007376853965147559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 203264
    },
    {
      "epoch": 0.0007378015308655925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 203296
    },
    {
      "epoch": 0.0007379176652164293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 203328
    },
    {
      "epoch": 0.0007380337995672659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 203360
    },
    {
      "epoch": 0.0007381499339181027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 203392
    },
    {
      "epoch": 0.0007382660682689394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 203424
    },
    {
      "epoch": 0.0007383822026197761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 203456
    },
    {
      "epoch": 0.0007384983369706128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 203488
    },
    {
      "epoch": 0.0007386144713214495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 203520
    },
    {
      "epoch": 0.0007387306056722862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.693,
      "step": 203552
    },
    {
      "epoch": 0.0007388467400231229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 203584
    },
    {
      "epoch": 0.0007389628743739596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 203616
    },
    {
      "epoch": 0.0007390790087247963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 203648
    },
    {
      "epoch": 0.000739195143075633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 203680
    },
    {
      "epoch": 0.0007393112774264698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 203712
    },
    {
      "epoch": 0.0007394274117773064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7502,
      "step": 203744
    },
    {
      "epoch": 0.0007395435461281432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 203776
    },
    {
      "epoch": 0.0007396596804789798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7503,
      "step": 203808
    },
    {
      "epoch": 0.0007397758148298166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 203840
    },
    {
      "epoch": 0.0007398919491806532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 203872
    },
    {
      "epoch": 0.00074000808353149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 203904
    },
    {
      "epoch": 0.0007401242178823266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 203936
    },
    {
      "epoch": 0.0007402403522331634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 203968
    },
    {
      "epoch": 0.0007403564865840002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 204000
    },
    {
      "epoch": 0.0007404726209348368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 204032
    },
    {
      "epoch": 0.0007405887552856736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 204064
    },
    {
      "epoch": 0.0007407048896365102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 204096
    },
    {
      "epoch": 0.000740821023987347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 204128
    },
    {
      "epoch": 0.0007409371583381836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 204160
    },
    {
      "epoch": 0.0007410532926890204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.695,
      "step": 204192
    },
    {
      "epoch": 0.000741169427039857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 204224
    },
    {
      "epoch": 0.0007412855613906938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 204256
    },
    {
      "epoch": 0.0007414016957415305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 204288
    },
    {
      "epoch": 0.0007415178300923672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 204320
    },
    {
      "epoch": 0.0007416339644432039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 204352
    },
    {
      "epoch": 0.0007417500987940406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 204384
    },
    {
      "epoch": 0.0007418662331448773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 204416
    },
    {
      "epoch": 0.000741982367495714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 204448
    },
    {
      "epoch": 0.0007420985018465507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 204480
    },
    {
      "epoch": 0.0007422146361973874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 204512
    },
    {
      "epoch": 0.0007423307705482241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 204544
    },
    {
      "epoch": 0.0007424469048990609,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 204576
    },
    {
      "epoch": 0.0007425630392498975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7457,
      "step": 204608
    },
    {
      "epoch": 0.0007426791736007343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 204640
    },
    {
      "epoch": 0.0007427953079515709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 204672
    },
    {
      "epoch": 0.0007429114423024077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 204704
    },
    {
      "epoch": 0.0007430275766532443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 204736
    },
    {
      "epoch": 0.0007431437110040811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6911,
      "step": 204768
    },
    {
      "epoch": 0.0007432598453549177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 204800
    },
    {
      "epoch": 0.0007433759797057545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 204832
    },
    {
      "epoch": 0.0007434921140565912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 204864
    },
    {
      "epoch": 0.0007436082484074279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 204896
    },
    {
      "epoch": 0.0007437243827582646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 204928
    },
    {
      "epoch": 0.0007438405171091013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 204960
    },
    {
      "epoch": 0.000743956651459938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 204992
    },
    {
      "epoch": 0.0007440727858107747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 205024
    },
    {
      "epoch": 0.0007441889201616114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 205056
    },
    {
      "epoch": 0.0007443050545124481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 205088
    },
    {
      "epoch": 0.0007444211888632848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 205120
    },
    {
      "epoch": 0.0007445373232141216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 205152
    },
    {
      "epoch": 0.0007446534575649582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 205184
    },
    {
      "epoch": 0.000744769591915795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 205216
    },
    {
      "epoch": 0.0007448857262666316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 205248
    },
    {
      "epoch": 0.0007450018606174684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 205280
    },
    {
      "epoch": 0.000745117994968305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 205312
    },
    {
      "epoch": 0.0007452341293191418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 205344
    },
    {
      "epoch": 0.0007453502636699784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 205376
    },
    {
      "epoch": 0.0007454663980208152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 205408
    },
    {
      "epoch": 0.0007455825323716519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 205440
    },
    {
      "epoch": 0.0007456986667224886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 205472
    },
    {
      "epoch": 0.0007458148010733253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 205504
    },
    {
      "epoch": 0.000745930935424162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 205536
    },
    {
      "epoch": 0.0007460470697749987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 205568
    },
    {
      "epoch": 0.0007461632041258354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 205600
    },
    {
      "epoch": 0.0007462793384766721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 205632
    },
    {
      "epoch": 0.0007463954728275088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 205664
    },
    {
      "epoch": 0.0007465116071783455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 205696
    },
    {
      "epoch": 0.0007466277415291823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 205728
    },
    {
      "epoch": 0.0007467438758800189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 205760
    },
    {
      "epoch": 0.0007468600102308557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 205792
    },
    {
      "epoch": 0.0007469761445816923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 205824
    },
    {
      "epoch": 0.0007470922789325291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 205856
    },
    {
      "epoch": 0.0007472084132833657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 205888
    },
    {
      "epoch": 0.0007473245476342025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 205920
    },
    {
      "epoch": 0.0007474406819850391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 205952
    },
    {
      "epoch": 0.0007475568163358759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 205984
    },
    {
      "epoch": 0.0007476729506867126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 206016
    },
    {
      "epoch": 0.0007477890850375493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 206048
    },
    {
      "epoch": 0.000747905219388386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 206080
    },
    {
      "epoch": 0.0007480213537392227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 206112
    },
    {
      "epoch": 0.0007481374880900594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 206144
    },
    {
      "epoch": 0.0007482536224408961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 206176
    },
    {
      "epoch": 0.0007483697567917328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 206208
    },
    {
      "epoch": 0.0007484858911425695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 206240
    },
    {
      "epoch": 0.0007486020254934062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 206272
    },
    {
      "epoch": 0.000748718159844243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 206304
    },
    {
      "epoch": 0.0007488342941950796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 206336
    },
    {
      "epoch": 0.0007489504285459164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 206368
    },
    {
      "epoch": 0.000749066562896753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 206400
    },
    {
      "epoch": 0.0007491826972475898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 206432
    },
    {
      "epoch": 0.0007492988315984264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 206464
    },
    {
      "epoch": 0.0007494149659492632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 206496
    },
    {
      "epoch": 0.0007495311003000998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6905,
      "step": 206528
    },
    {
      "epoch": 0.0007496472346509366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 206560
    },
    {
      "epoch": 0.0007497633690017733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 206592
    },
    {
      "epoch": 0.00074987950335261,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 206624
    },
    {
      "epoch": 0.0007499956377034467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 206656
    },
    {
      "epoch": 0.0007501117720542834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 206688
    },
    {
      "epoch": 0.0007502279064051201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 206720
    },
    {
      "epoch": 0.0007503440407559568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 206752
    },
    {
      "epoch": 0.0007504601751067935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 206784
    },
    {
      "epoch": 0.0007505763094576302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 206816
    },
    {
      "epoch": 0.0007506924438084669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 206848
    },
    {
      "epoch": 0.0007508085781593037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 206880
    },
    {
      "epoch": 0.0007509247125101403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 206912
    },
    {
      "epoch": 0.0007510408468609771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 206944
    },
    {
      "epoch": 0.0007511569812118137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 206976
    },
    {
      "epoch": 0.0007512731155626505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 207008
    },
    {
      "epoch": 0.0007513892499134871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 207040
    },
    {
      "epoch": 0.0007515053842643239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 207072
    },
    {
      "epoch": 0.0007516215186151605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 207104
    },
    {
      "epoch": 0.0007517376529659973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 207136
    },
    {
      "epoch": 0.000751853787316834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 207168
    },
    {
      "epoch": 0.0007519699216676707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 207200
    },
    {
      "epoch": 0.0007520860560185074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 207232
    },
    {
      "epoch": 0.0007522021903693441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 207264
    },
    {
      "epoch": 0.0007523183247201808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 207296
    },
    {
      "epoch": 0.0007524344590710175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 207328
    },
    {
      "epoch": 0.0007525505934218542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 207360
    },
    {
      "epoch": 0.0007526667277726909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 207392
    },
    {
      "epoch": 0.0007527828621235276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 207424
    },
    {
      "epoch": 0.0007528989964743644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 207456
    },
    {
      "epoch": 0.000753015130825201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 207488
    },
    {
      "epoch": 0.0007531312651760378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 207520
    },
    {
      "epoch": 0.0007532473995268744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 207552
    },
    {
      "epoch": 0.0007533635338777112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 207584
    },
    {
      "epoch": 0.0007534796682285478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 207616
    },
    {
      "epoch": 0.0007535958025793846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 207648
    },
    {
      "epoch": 0.0007537119369302212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 207680
    },
    {
      "epoch": 0.000753828071281058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 207712
    },
    {
      "epoch": 0.0007539442056318947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 207744
    },
    {
      "epoch": 0.0007540603399827314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 207776
    },
    {
      "epoch": 0.0007541764743335681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 207808
    },
    {
      "epoch": 0.0007542926086844048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 207840
    },
    {
      "epoch": 0.0007544087430352415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 207872
    },
    {
      "epoch": 0.0007545248773860782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 207904
    },
    {
      "epoch": 0.000754641011736915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 207936
    },
    {
      "epoch": 0.0007547571460877516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 207968
    },
    {
      "epoch": 0.0007548732804385883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 208000
    },
    {
      "epoch": 0.0007549894147894251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 208032
    },
    {
      "epoch": 0.0007551055491402617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 208064
    },
    {
      "epoch": 0.0007552216834910985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 208096
    },
    {
      "epoch": 0.0007553378178419351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 208128
    },
    {
      "epoch": 0.0007554539521927719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 208160
    },
    {
      "epoch": 0.0007555700865436085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 208192
    },
    {
      "epoch": 0.0007556862208944453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 208224
    },
    {
      "epoch": 0.000755802355245282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 208256
    },
    {
      "epoch": 0.0007559184895961187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 208288
    },
    {
      "epoch": 0.0007560346239469555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 208320
    },
    {
      "epoch": 0.0007561507582977921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 208352
    },
    {
      "epoch": 0.0007562668926486289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 208384
    },
    {
      "epoch": 0.0007563830269994655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 208416
    },
    {
      "epoch": 0.0007564991613503023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 208448
    },
    {
      "epoch": 0.0007566152957011389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 208480
    },
    {
      "epoch": 0.0007567314300519757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 208512
    },
    {
      "epoch": 0.0007568475644028123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 208544
    },
    {
      "epoch": 0.0007569636987536491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 208576
    },
    {
      "epoch": 0.0007570798331044858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 208608
    },
    {
      "epoch": 0.0007571959674553225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 208640
    },
    {
      "epoch": 0.0007573121018061592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 208672
    },
    {
      "epoch": 0.0007574282361569959,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 208704
    },
    {
      "epoch": 0.0007575443705078326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 208736
    },
    {
      "epoch": 0.0007576605048586693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 208768
    },
    {
      "epoch": 0.000757776639209506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 208800
    },
    {
      "epoch": 0.0007578927735603427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 208832
    },
    {
      "epoch": 0.0007580089079111794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 208864
    },
    {
      "epoch": 0.0007581250422620162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 208896
    },
    {
      "epoch": 0.0007582411766128528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 208928
    },
    {
      "epoch": 0.0007583573109636896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 208960
    },
    {
      "epoch": 0.0007584734453145262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 208992
    },
    {
      "epoch": 0.000758589579665363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 209024
    },
    {
      "epoch": 0.0007587057140161996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 209056
    },
    {
      "epoch": 0.0007588218483670364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 209088
    },
    {
      "epoch": 0.000758937982717873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 209120
    },
    {
      "epoch": 0.0007590541170687098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 209152
    },
    {
      "epoch": 0.0007591702514195465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 209184
    },
    {
      "epoch": 0.0007592863857703832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 209216
    },
    {
      "epoch": 0.0007594025201212199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 209248
    },
    {
      "epoch": 0.0007595186544720566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 209280
    },
    {
      "epoch": 0.0007596347888228933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 209312
    },
    {
      "epoch": 0.00075975092317373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 209344
    },
    {
      "epoch": 0.0007598670575245667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 209376
    },
    {
      "epoch": 0.0007599831918754034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 209408
    },
    {
      "epoch": 0.0007600993262262401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 209440
    },
    {
      "epoch": 0.0007602154605770769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 209472
    },
    {
      "epoch": 0.0007603315949279135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 209504
    },
    {
      "epoch": 0.0007604477292787503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 209536
    },
    {
      "epoch": 0.0007605638636295869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 209568
    },
    {
      "epoch": 0.0007606799979804237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 209600
    },
    {
      "epoch": 0.0007607961323312603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 209632
    },
    {
      "epoch": 0.0007609122666820971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 209664
    },
    {
      "epoch": 0.0007610284010329337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 209696
    },
    {
      "epoch": 0.0007611445353837705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 209728
    },
    {
      "epoch": 0.0007612606697346072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 209760
    },
    {
      "epoch": 0.0007613768040854439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 209792
    },
    {
      "epoch": 0.0007614929384362806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 209824
    },
    {
      "epoch": 0.0007616090727871173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 209856
    },
    {
      "epoch": 0.000761725207137954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 209888
    },
    {
      "epoch": 0.0007618413414887907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 209920
    },
    {
      "epoch": 0.0007619574758396274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 209952
    },
    {
      "epoch": 0.0007620736101904641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 209984
    },
    {
      "epoch": 0.0007621897445413008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 210016
    },
    {
      "epoch": 0.0007623058788921375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 210048
    },
    {
      "epoch": 0.0007624220132429742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 210080
    },
    {
      "epoch": 0.000762538147593811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 210112
    },
    {
      "epoch": 0.0007626542819446476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 210144
    },
    {
      "epoch": 0.0007627704162954844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 210176
    },
    {
      "epoch": 0.000762886550646321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 210208
    },
    {
      "epoch": 0.0007630026849971578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 210240
    },
    {
      "epoch": 0.0007631188193479944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 210272
    },
    {
      "epoch": 0.0007632349536988312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 210304
    },
    {
      "epoch": 0.0007633510880496678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 210336
    },
    {
      "epoch": 0.0007634672224005046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 210368
    },
    {
      "epoch": 0.0007635833567513413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 210400
    },
    {
      "epoch": 0.000763699491102178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 210432
    },
    {
      "epoch": 0.0007638156254530147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 210464
    },
    {
      "epoch": 0.0007639317598038514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 210496
    },
    {
      "epoch": 0.0007640478941546881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 210528
    },
    {
      "epoch": 0.0007641640285055248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 210560
    },
    {
      "epoch": 0.0007642801628563615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 210592
    },
    {
      "epoch": 0.0007643962972071982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 210624
    },
    {
      "epoch": 0.0007645124315580349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 210656
    },
    {
      "epoch": 0.0007646285659088717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 210688
    },
    {
      "epoch": 0.0007647447002597083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 210720
    },
    {
      "epoch": 0.0007648608346105451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 210752
    },
    {
      "epoch": 0.0007649769689613817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 210784
    },
    {
      "epoch": 0.0007650931033122185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 210816
    },
    {
      "epoch": 0.0007652092376630551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 210848
    },
    {
      "epoch": 0.0007653253720138919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 210880
    },
    {
      "epoch": 0.0007654415063647285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 210912
    },
    {
      "epoch": 0.0007655576407155653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 210944
    },
    {
      "epoch": 0.000765673775066402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 210976
    },
    {
      "epoch": 0.0007657899094172387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 211008
    },
    {
      "epoch": 0.0007659060437680754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 211040
    },
    {
      "epoch": 0.0007660221781189121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 211072
    },
    {
      "epoch": 0.0007661383124697488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 211104
    },
    {
      "epoch": 0.0007662544468205855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 211136
    },
    {
      "epoch": 0.0007663705811714222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 211168
    },
    {
      "epoch": 0.0007664867155222589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 211200
    },
    {
      "epoch": 0.0007666028498730956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 211232
    },
    {
      "epoch": 0.0007667189842239324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 211264
    },
    {
      "epoch": 0.000766835118574769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 211296
    },
    {
      "epoch": 0.0007669512529256058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 211328
    },
    {
      "epoch": 0.0007670673872764424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 211360
    },
    {
      "epoch": 0.0007671835216272792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 211392
    },
    {
      "epoch": 0.0007672996559781158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 211424
    },
    {
      "epoch": 0.0007674157903289526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 211456
    },
    {
      "epoch": 0.0007675319246797892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 211488
    },
    {
      "epoch": 0.000767648059030626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 211520
    },
    {
      "epoch": 0.0007677641933814627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 211552
    },
    {
      "epoch": 0.0007678803277322994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 211584
    },
    {
      "epoch": 0.0007679964620831361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 211616
    },
    {
      "epoch": 0.0007681125964339728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 211648
    },
    {
      "epoch": 0.0007682287307848095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 211680
    },
    {
      "epoch": 0.0007683448651356462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7479,
      "step": 211712
    },
    {
      "epoch": 0.0007684609994864829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 211744
    },
    {
      "epoch": 0.0007685771338373196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 211776
    },
    {
      "epoch": 0.0007686932681881563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 211808
    },
    {
      "epoch": 0.0007688094025389931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 211840
    },
    {
      "epoch": 0.0007689255368898297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 211872
    },
    {
      "epoch": 0.0007690416712406665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 211904
    },
    {
      "epoch": 0.0007691578055915031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 211936
    },
    {
      "epoch": 0.0007692739399423399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 211968
    },
    {
      "epoch": 0.0007693900742931765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 212000
    },
    {
      "epoch": 0.0007695062086440133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 212032
    },
    {
      "epoch": 0.0007696223429948499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 212064
    },
    {
      "epoch": 0.0007697384773456867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 212096
    },
    {
      "epoch": 0.0007698546116965234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 212128
    },
    {
      "epoch": 0.0007699707460473601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 212160
    },
    {
      "epoch": 0.0007700868803981968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 212192
    },
    {
      "epoch": 0.0007702030147490335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 212224
    },
    {
      "epoch": 0.0007703191490998702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 212256
    },
    {
      "epoch": 0.0007704352834507069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 212288
    },
    {
      "epoch": 0.0007705514178015436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 212320
    },
    {
      "epoch": 0.0007706675521523803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 212352
    },
    {
      "epoch": 0.000770783686503217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 212384
    },
    {
      "epoch": 0.0007708998208540538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 212416
    },
    {
      "epoch": 0.0007710159552048904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 212448
    },
    {
      "epoch": 0.0007711320895557272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 212480
    },
    {
      "epoch": 0.0007712482239065638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 212512
    },
    {
      "epoch": 0.0007713643582574006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 212544
    },
    {
      "epoch": 0.0007714804926082372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 212576
    },
    {
      "epoch": 0.000771596626959074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 212608
    },
    {
      "epoch": 0.0007717127613099106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 212640
    },
    {
      "epoch": 0.0007718288956607474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 212672
    },
    {
      "epoch": 0.0007719450300115842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 212704
    },
    {
      "epoch": 0.0007720611643624208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 212736
    },
    {
      "epoch": 0.0007721772987132576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 212768
    },
    {
      "epoch": 0.0007722934330640942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 212800
    },
    {
      "epoch": 0.000772409567414931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 212832
    },
    {
      "epoch": 0.0007725257017657676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 212864
    },
    {
      "epoch": 0.0007726418361166044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 212896
    },
    {
      "epoch": 0.000772757970467441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 212928
    },
    {
      "epoch": 0.0007728741048182778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 212960
    },
    {
      "epoch": 0.0007729902391691145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 212992
    },
    {
      "epoch": 0.0007731063735199512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 213024
    },
    {
      "epoch": 0.0007732225078707879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 213056
    },
    {
      "epoch": 0.0007733386422216246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 213088
    },
    {
      "epoch": 0.0007734547765724613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 213120
    },
    {
      "epoch": 0.000773570910923298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 213152
    },
    {
      "epoch": 0.0007736870452741347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 213184
    },
    {
      "epoch": 0.0007738031796249714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 213216
    },
    {
      "epoch": 0.0007739193139758081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 213248
    },
    {
      "epoch": 0.0007740354483266449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 213280
    },
    {
      "epoch": 0.0007741515826774815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 213312
    },
    {
      "epoch": 0.0007742677170283183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 213344
    },
    {
      "epoch": 0.0007743838513791549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 213376
    },
    {
      "epoch": 0.0007744999857299917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 213408
    },
    {
      "epoch": 0.0007746161200808283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 213440
    },
    {
      "epoch": 0.0007747322544316651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 213472
    },
    {
      "epoch": 0.0007748483887825017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 213504
    },
    {
      "epoch": 0.0007749645231333385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 213536
    },
    {
      "epoch": 0.0007750806574841752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 213568
    },
    {
      "epoch": 0.0007751967918350119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 213600
    },
    {
      "epoch": 0.0007753129261858486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 213632
    },
    {
      "epoch": 0.0007754290605366853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 213664
    },
    {
      "epoch": 0.000775545194887522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 213696
    },
    {
      "epoch": 0.0007756613292383587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 213728
    },
    {
      "epoch": 0.0007757774635891954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 213760
    },
    {
      "epoch": 0.0007758935979400321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 213792
    },
    {
      "epoch": 0.0007760097322908688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 213824
    },
    {
      "epoch": 0.0007761258666417056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 213856
    },
    {
      "epoch": 0.0007762420009925422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 213888
    },
    {
      "epoch": 0.000776358135343379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.764,
      "step": 213920
    },
    {
      "epoch": 0.0007764742696942156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 213952
    },
    {
      "epoch": 0.0007765904040450524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 213984
    },
    {
      "epoch": 0.000776706538395889,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 214016
    },
    {
      "epoch": 0.0007768226727467258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7477,
      "step": 214048
    },
    {
      "epoch": 0.0007769388070975624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 214080
    },
    {
      "epoch": 0.0007770549414483992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 214112
    },
    {
      "epoch": 0.0007771710757992359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 214144
    },
    {
      "epoch": 0.0007772872101500726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6764,
      "step": 214176
    },
    {
      "epoch": 0.0007774033445009093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6957,
      "step": 214208
    },
    {
      "epoch": 0.000777519478851746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 214240
    },
    {
      "epoch": 0.0007776356132025827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 214272
    },
    {
      "epoch": 0.0007777517475534194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 214304
    },
    {
      "epoch": 0.0007778678819042561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 214336
    },
    {
      "epoch": 0.0007779840162550928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6956,
      "step": 214368
    },
    {
      "epoch": 0.0007781001506059295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 214400
    },
    {
      "epoch": 0.0007782162849567663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 214432
    },
    {
      "epoch": 0.0007783324193076029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 214464
    },
    {
      "epoch": 0.0007784485536584397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 214496
    },
    {
      "epoch": 0.0007785646880092763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 214528
    },
    {
      "epoch": 0.0007786808223601131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 214560
    },
    {
      "epoch": 0.0007787969567109497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 214592
    },
    {
      "epoch": 0.0007789130910617865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 214624
    },
    {
      "epoch": 0.0007790292254126231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 214656
    },
    {
      "epoch": 0.0007791453597634599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 214688
    },
    {
      "epoch": 0.0007792614941142966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6915,
      "step": 214720
    },
    {
      "epoch": 0.0007793776284651333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 214752
    },
    {
      "epoch": 0.00077949376281597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 214784
    },
    {
      "epoch": 0.0007796098971668067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 214816
    },
    {
      "epoch": 0.0007797260315176434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 214848
    },
    {
      "epoch": 0.0007798421658684801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 214880
    },
    {
      "epoch": 0.0007799583002193168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 214912
    },
    {
      "epoch": 0.0007800744345701535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 214944
    },
    {
      "epoch": 0.0007801905689209902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 214976
    },
    {
      "epoch": 0.000780306703271827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 215008
    },
    {
      "epoch": 0.0007804228376226636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 215040
    },
    {
      "epoch": 0.0007805389719735004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 215072
    },
    {
      "epoch": 0.000780655106324337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 215104
    },
    {
      "epoch": 0.0007807712406751738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 215136
    },
    {
      "epoch": 0.0007808873750260104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 215168
    },
    {
      "epoch": 0.0007810035093768472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 215200
    },
    {
      "epoch": 0.0007811196437276838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 215232
    },
    {
      "epoch": 0.0007812357780785206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 215264
    },
    {
      "epoch": 0.0007813519124293573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 215296
    },
    {
      "epoch": 0.000781468046780194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 215328
    },
    {
      "epoch": 0.0007815841811310307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 215360
    },
    {
      "epoch": 0.0007817003154818674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 215392
    },
    {
      "epoch": 0.0007818164498327041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.695,
      "step": 215424
    },
    {
      "epoch": 0.0007819325841835408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 215456
    },
    {
      "epoch": 0.0007820487185343775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 215488
    },
    {
      "epoch": 0.0007821648528852142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 215520
    },
    {
      "epoch": 0.0007822809872360509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 215552
    },
    {
      "epoch": 0.0007823971215868877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 215584
    },
    {
      "epoch": 0.0007825132559377243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 215616
    },
    {
      "epoch": 0.0007826293902885611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 215648
    },
    {
      "epoch": 0.0007827455246393977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 215680
    },
    {
      "epoch": 0.0007828616589902345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 215712
    },
    {
      "epoch": 0.0007829777933410711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 215744
    },
    {
      "epoch": 0.0007830939276919079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 215776
    },
    {
      "epoch": 0.0007832100620427445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 215808
    },
    {
      "epoch": 0.0007833261963935813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 215840
    },
    {
      "epoch": 0.000783442330744418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 215872
    },
    {
      "epoch": 0.0007835584650952547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 215904
    },
    {
      "epoch": 0.0007836745994460914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 215936
    },
    {
      "epoch": 0.0007837907337969281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 215968
    },
    {
      "epoch": 0.0007839068681477648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 216000
    },
    {
      "epoch": 0.0007840230024986015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 216032
    },
    {
      "epoch": 0.0007841391368494382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 216064
    },
    {
      "epoch": 0.0007842552712002749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 216096
    },
    {
      "epoch": 0.0007843714055511116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 216128
    },
    {
      "epoch": 0.0007844875399019484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.691,
      "step": 216160
    },
    {
      "epoch": 0.000784603674252785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 216192
    },
    {
      "epoch": 0.0007847198086036218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 216224
    },
    {
      "epoch": 0.0007848359429544584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 216256
    },
    {
      "epoch": 0.0007849520773052952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 216288
    },
    {
      "epoch": 0.0007850682116561318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 216320
    },
    {
      "epoch": 0.0007851843460069686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 216352
    },
    {
      "epoch": 0.0007853004803578052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 216384
    },
    {
      "epoch": 0.000785416614708642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 216416
    },
    {
      "epoch": 0.0007855327490594788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 216448
    },
    {
      "epoch": 0.0007856488834103154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 216480
    },
    {
      "epoch": 0.0007857650177611522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 216512
    },
    {
      "epoch": 0.0007858811521119888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 216544
    },
    {
      "epoch": 0.0007859972864628256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 216576
    },
    {
      "epoch": 0.0007861134208136622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 216608
    },
    {
      "epoch": 0.000786229555164499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 216640
    },
    {
      "epoch": 0.0007863456895153356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 216672
    },
    {
      "epoch": 0.0007864618238661723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7409,
      "step": 216704
    },
    {
      "epoch": 0.0007865779582170091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 216736
    },
    {
      "epoch": 0.0007866940925678457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 216768
    },
    {
      "epoch": 0.0007868102269186825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 216800
    },
    {
      "epoch": 0.0007869263612695191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 216832
    },
    {
      "epoch": 0.0007870424956203559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 216864
    },
    {
      "epoch": 0.0007871586299711925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 216896
    },
    {
      "epoch": 0.0007872747643220293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 216928
    },
    {
      "epoch": 0.000787390898672866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 216960
    },
    {
      "epoch": 0.0007875070330237027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 216992
    },
    {
      "epoch": 0.0007876231673745395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 217024
    },
    {
      "epoch": 0.0007877393017253761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 217056
    },
    {
      "epoch": 0.0007878554360762129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6875,
      "step": 217088
    },
    {
      "epoch": 0.0007879715704270495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 217120
    },
    {
      "epoch": 0.0007880877047778863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 217152
    },
    {
      "epoch": 0.0007882038391287229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 217184
    },
    {
      "epoch": 0.0007883199734795597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 217216
    },
    {
      "epoch": 0.0007884361078303963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 217248
    },
    {
      "epoch": 0.0007885522421812331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 217280
    },
    {
      "epoch": 0.0007886683765320698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 217312
    },
    {
      "epoch": 0.0007887845108829065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 217344
    },
    {
      "epoch": 0.0007889006452337432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 217376
    },
    {
      "epoch": 0.0007890167795845799,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 217408
    },
    {
      "epoch": 0.0007891329139354166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 217440
    },
    {
      "epoch": 0.0007892490482862533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 217472
    },
    {
      "epoch": 0.00078936518263709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 217504
    },
    {
      "epoch": 0.0007894813169879267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 217536
    },
    {
      "epoch": 0.0007895974513387634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 217568
    },
    {
      "epoch": 0.0007897135856896002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 217600
    },
    {
      "epoch": 0.0007898297200404368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6943,
      "step": 217632
    },
    {
      "epoch": 0.0007899458543912736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 217664
    },
    {
      "epoch": 0.0007900619887421102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6868,
      "step": 217696
    },
    {
      "epoch": 0.000790178123092947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6855,
      "step": 217728
    },
    {
      "epoch": 0.0007902942574437836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 217760
    },
    {
      "epoch": 0.0007904103917946204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 217792
    },
    {
      "epoch": 0.000790526526145457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 217824
    },
    {
      "epoch": 0.0007906426604962938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 217856
    },
    {
      "epoch": 0.0007907587948471305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 217888
    },
    {
      "epoch": 0.0007908749291979672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6793,
      "step": 217920
    },
    {
      "epoch": 0.0007909910635488039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 217952
    },
    {
      "epoch": 0.0007911071978996406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 217984
    },
    {
      "epoch": 0.0007912233322504773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 218016
    },
    {
      "epoch": 0.000791339466601314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 218048
    },
    {
      "epoch": 0.0007914556009521507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 218080
    },
    {
      "epoch": 0.0007915717353029874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 218112
    },
    {
      "epoch": 0.0007916878696538241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 218144
    },
    {
      "epoch": 0.0007918040040046609,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 218176
    },
    {
      "epoch": 0.0007919201383554975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 218208
    },
    {
      "epoch": 0.0007920362727063343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 218240
    },
    {
      "epoch": 0.0007921524070571709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 218272
    },
    {
      "epoch": 0.0007922685414080077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 218304
    },
    {
      "epoch": 0.0007923846757588443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 218336
    },
    {
      "epoch": 0.0007925008101096811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 218368
    },
    {
      "epoch": 0.0007926169444605177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 218400
    },
    {
      "epoch": 0.0007927330788113545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 218432
    },
    {
      "epoch": 0.0007928492131621912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 218464
    },
    {
      "epoch": 0.0007929653475130279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 218496
    },
    {
      "epoch": 0.0007930814818638646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6809,
      "step": 218528
    },
    {
      "epoch": 0.0007931976162147013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 218560
    },
    {
      "epoch": 0.000793313750565538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 218592
    },
    {
      "epoch": 0.0007934298849163747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 218624
    },
    {
      "epoch": 0.0007935460192672114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 218656
    },
    {
      "epoch": 0.0007936621536180481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 218688
    },
    {
      "epoch": 0.0007937782879688848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 218720
    },
    {
      "epoch": 0.0007938944223197216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 218752
    },
    {
      "epoch": 0.0007940105566705582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 218784
    },
    {
      "epoch": 0.000794126691021395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 218816
    },
    {
      "epoch": 0.0007942428253722316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 218848
    },
    {
      "epoch": 0.0007943589597230684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 218880
    },
    {
      "epoch": 0.000794475094073905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 218912
    },
    {
      "epoch": 0.0007945912284247418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 218944
    },
    {
      "epoch": 0.0007947073627755784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 218976
    },
    {
      "epoch": 0.0007948234971264152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 219008
    },
    {
      "epoch": 0.0007949396314772519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 219040
    },
    {
      "epoch": 0.0007950557658280886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 219072
    },
    {
      "epoch": 0.0007951719001789253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 219104
    },
    {
      "epoch": 0.000795288034529762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 219136
    },
    {
      "epoch": 0.0007954041688805987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 219168
    },
    {
      "epoch": 0.0007955203032314354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 219200
    },
    {
      "epoch": 0.0007956364375822721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 219232
    },
    {
      "epoch": 0.0007957525719331088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 219264
    },
    {
      "epoch": 0.0007958687062839455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 219296
    },
    {
      "epoch": 0.0007959848406347823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 219328
    },
    {
      "epoch": 0.0007961009749856189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 219360
    },
    {
      "epoch": 0.0007962171093364557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 219392
    },
    {
      "epoch": 0.0007963332436872923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 219424
    },
    {
      "epoch": 0.0007964493780381291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 219456
    },
    {
      "epoch": 0.0007965655123889657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 219488
    },
    {
      "epoch": 0.0007966816467398025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 219520
    },
    {
      "epoch": 0.0007967977810906391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 219552
    },
    {
      "epoch": 0.0007969139154414759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 219584
    },
    {
      "epoch": 0.0007970300497923126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 219616
    },
    {
      "epoch": 0.0007971461841431493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 219648
    },
    {
      "epoch": 0.000797262318493986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 219680
    },
    {
      "epoch": 0.0007973784528448227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 219712
    },
    {
      "epoch": 0.0007974945871956594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 219744
    },
    {
      "epoch": 0.0007976107215464961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6846,
      "step": 219776
    },
    {
      "epoch": 0.0007977268558973328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 219808
    },
    {
      "epoch": 0.0007978429902481695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 219840
    },
    {
      "epoch": 0.0007979591245990062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 219872
    },
    {
      "epoch": 0.000798075258949843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 219904
    },
    {
      "epoch": 0.0007981913933006796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 219936
    },
    {
      "epoch": 0.0007983075276515164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 219968
    },
    {
      "epoch": 0.000798423662002353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 220000
    },
    {
      "epoch": 0.0007985397963531898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 220032
    },
    {
      "epoch": 0.0007986559307040264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 220064
    },
    {
      "epoch": 0.0007987720650548632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 220096
    },
    {
      "epoch": 0.0007988881994056998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 220128
    },
    {
      "epoch": 0.0007990043337565366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 220160
    },
    {
      "epoch": 0.0007991204681073733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 220192
    },
    {
      "epoch": 0.00079923660245821,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 220224
    },
    {
      "epoch": 0.0007993527368090467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 220256
    },
    {
      "epoch": 0.0007994688711598834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 220288
    },
    {
      "epoch": 0.0007995850055107201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 220320
    },
    {
      "epoch": 0.0007997011398615568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 220352
    },
    {
      "epoch": 0.0007998172742123935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 220384
    },
    {
      "epoch": 0.0007999334085632302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6904,
      "step": 220416
    },
    {
      "epoch": 0.0008000495429140669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 220448
    },
    {
      "epoch": 0.0008001656772649037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 220480
    },
    {
      "epoch": 0.0008002818116157403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 220512
    },
    {
      "epoch": 0.0008003979459665771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 220544
    },
    {
      "epoch": 0.0008005140803174137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 220576
    },
    {
      "epoch": 0.0008006302146682505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 220608
    },
    {
      "epoch": 0.0008007463490190871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 220640
    },
    {
      "epoch": 0.0008008624833699239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 220672
    },
    {
      "epoch": 0.0008009786177207605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 220704
    },
    {
      "epoch": 0.0008010947520715973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 220736
    },
    {
      "epoch": 0.000801210886422434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 220768
    },
    {
      "epoch": 0.0008013270207732707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 220800
    },
    {
      "epoch": 0.0008014431551241075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 220832
    },
    {
      "epoch": 0.0008015592894749441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 220864
    },
    {
      "epoch": 0.0008016754238257809,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 220896
    },
    {
      "epoch": 0.0008017915581766175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7491,
      "step": 220928
    },
    {
      "epoch": 0.0008019076925274543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 220960
    },
    {
      "epoch": 0.0008020238268782909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 220992
    },
    {
      "epoch": 0.0008021399612291277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 221024
    },
    {
      "epoch": 0.0008022560955799643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 221056
    },
    {
      "epoch": 0.000802372229930801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 221088
    },
    {
      "epoch": 0.0008024883642816378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 221120
    },
    {
      "epoch": 0.0008026044986324745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 221152
    },
    {
      "epoch": 0.0008027206329833112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 221184
    },
    {
      "epoch": 0.0008028367673341479,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 221216
    },
    {
      "epoch": 0.0008029529016849846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 221248
    },
    {
      "epoch": 0.0008030690360358213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 221280
    },
    {
      "epoch": 0.000803185170386658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 221312
    },
    {
      "epoch": 0.0008033013047374947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 221344
    },
    {
      "epoch": 0.0008034174390883314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 221376
    },
    {
      "epoch": 0.0008035335734391682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 221408
    },
    {
      "epoch": 0.0008036497077900048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 221440
    },
    {
      "epoch": 0.0008037658421408416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 221472
    },
    {
      "epoch": 0.0008038819764916782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 221504
    },
    {
      "epoch": 0.000803998110842515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 221536
    },
    {
      "epoch": 0.0008041142451933516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 221568
    },
    {
      "epoch": 0.0008042303795441884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 221600
    },
    {
      "epoch": 0.000804346513895025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 221632
    },
    {
      "epoch": 0.0008044626482458618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 221664
    },
    {
      "epoch": 0.0008045787825966985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 221696
    },
    {
      "epoch": 0.0008046949169475352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 221728
    },
    {
      "epoch": 0.0008048110512983719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 221760
    },
    {
      "epoch": 0.0008049271856492086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 221792
    },
    {
      "epoch": 0.0008050433200000453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 221824
    },
    {
      "epoch": 0.000805159454350882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 221856
    },
    {
      "epoch": 0.0008052755887017187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 221888
    },
    {
      "epoch": 0.0008053917230525554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 221920
    },
    {
      "epoch": 0.0008055078574033921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 221952
    },
    {
      "epoch": 0.0008056239917542289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 221984
    },
    {
      "epoch": 0.0008057401261050655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 222016
    },
    {
      "epoch": 0.0008058562604559023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.675,
      "step": 222048
    },
    {
      "epoch": 0.0008059723948067389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 222080
    },
    {
      "epoch": 0.0008060885291575757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 222112
    },
    {
      "epoch": 0.0008062046635084123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 222144
    },
    {
      "epoch": 0.0008063207978592491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 222176
    },
    {
      "epoch": 0.0008064369322100857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 222208
    },
    {
      "epoch": 0.0008065530665609225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 222240
    },
    {
      "epoch": 0.0008066692009117592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 222272
    },
    {
      "epoch": 0.0008067853352625959,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 222304
    },
    {
      "epoch": 0.0008069014696134326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 222336
    },
    {
      "epoch": 0.0008070176039642693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 222368
    },
    {
      "epoch": 0.000807133738315106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 222400
    },
    {
      "epoch": 0.0008072498726659427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 222432
    },
    {
      "epoch": 0.0008073660070167794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 222464
    },
    {
      "epoch": 0.0008074821413676161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 222496
    },
    {
      "epoch": 0.0008075982757184528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 222528
    },
    {
      "epoch": 0.0008077144100692896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 222560
    },
    {
      "epoch": 0.0008078305444201262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 222592
    },
    {
      "epoch": 0.000807946678770963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 222624
    },
    {
      "epoch": 0.0008080628131217996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 222656
    },
    {
      "epoch": 0.0008081789474726364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 222688
    },
    {
      "epoch": 0.000808295081823473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 222720
    },
    {
      "epoch": 0.0008084112161743098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 222752
    },
    {
      "epoch": 0.0008085273505251464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 222784
    },
    {
      "epoch": 0.0008086434848759832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 222816
    },
    {
      "epoch": 0.0008087596192268199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 222848
    },
    {
      "epoch": 0.0008088757535776566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 222880
    },
    {
      "epoch": 0.0008089918879284933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 222912
    },
    {
      "epoch": 0.00080910802227933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 222944
    },
    {
      "epoch": 0.0008092241566301667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 222976
    },
    {
      "epoch": 0.0008093402909810034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 223008
    },
    {
      "epoch": 0.0008094564253318401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 223040
    },
    {
      "epoch": 0.0008095725596826768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 223072
    },
    {
      "epoch": 0.0008096886940335135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 223104
    },
    {
      "epoch": 0.0008098048283843503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 223136
    },
    {
      "epoch": 0.0008099209627351869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 223168
    },
    {
      "epoch": 0.0008100370970860237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 223200
    },
    {
      "epoch": 0.0008101532314368603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 223232
    },
    {
      "epoch": 0.0008102693657876971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 223264
    },
    {
      "epoch": 0.0008103855001385337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 223296
    },
    {
      "epoch": 0.0008105016344893705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 223328
    },
    {
      "epoch": 0.0008106177688402071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 223360
    },
    {
      "epoch": 0.0008107339031910439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7499,
      "step": 223392
    },
    {
      "epoch": 0.0008108500375418806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 223424
    },
    {
      "epoch": 0.0008109661718927173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 223456
    },
    {
      "epoch": 0.000811082306243554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 223488
    },
    {
      "epoch": 0.0008111984405943907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 223520
    },
    {
      "epoch": 0.0008113145749452274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 223552
    },
    {
      "epoch": 0.0008114307092960641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 223584
    },
    {
      "epoch": 0.0008115468436469008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 223616
    },
    {
      "epoch": 0.0008116629779977375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 223648
    },
    {
      "epoch": 0.0008117791123485742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 223680
    },
    {
      "epoch": 0.000811895246699411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 223712
    },
    {
      "epoch": 0.0008120113810502476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 223744
    },
    {
      "epoch": 0.0008121275154010844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 223776
    },
    {
      "epoch": 0.000812243649751921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 223808
    },
    {
      "epoch": 0.0008123597841027578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 223840
    },
    {
      "epoch": 0.0008124759184535944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 223872
    },
    {
      "epoch": 0.0008125920528044312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 223904
    },
    {
      "epoch": 0.0008127081871552678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 223936
    },
    {
      "epoch": 0.0008128243215061046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 223968
    },
    {
      "epoch": 0.0008129404558569413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 224000
    },
    {
      "epoch": 0.000813056590207778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 224032
    },
    {
      "epoch": 0.0008131727245586147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 224064
    },
    {
      "epoch": 0.0008132888589094514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 224096
    },
    {
      "epoch": 0.0008134049932602881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 224128
    },
    {
      "epoch": 0.0008135211276111248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 224160
    },
    {
      "epoch": 0.0008136372619619615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 224192
    },
    {
      "epoch": 0.0008137533963127982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 224224
    },
    {
      "epoch": 0.0008138695306636349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 224256
    },
    {
      "epoch": 0.0008139856650144717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7497,
      "step": 224288
    },
    {
      "epoch": 0.0008141017993653083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 224320
    },
    {
      "epoch": 0.0008142179337161451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 224352
    },
    {
      "epoch": 0.0008143340680669817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 224384
    },
    {
      "epoch": 0.0008144502024178185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 224416
    },
    {
      "epoch": 0.0008145663367686551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 224448
    },
    {
      "epoch": 0.0008146824711194919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 224480
    },
    {
      "epoch": 0.0008147986054703285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 224512
    },
    {
      "epoch": 0.0008149147398211653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 224544
    },
    {
      "epoch": 0.000815030874172002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 224576
    },
    {
      "epoch": 0.0008151470085228387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 224608
    },
    {
      "epoch": 0.0008152631428736754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 224640
    },
    {
      "epoch": 0.0008153792772245121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 224672
    },
    {
      "epoch": 0.0008154954115753488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 224704
    },
    {
      "epoch": 0.0008156115459261855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 224736
    },
    {
      "epoch": 0.0008157276802770222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 224768
    },
    {
      "epoch": 0.0008158438146278589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 224800
    },
    {
      "epoch": 0.0008159599489786956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 224832
    },
    {
      "epoch": 0.0008160760833295324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 224864
    },
    {
      "epoch": 0.000816192217680369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 224896
    },
    {
      "epoch": 0.0008163083520312058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 224928
    },
    {
      "epoch": 0.0008164244863820424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 224960
    },
    {
      "epoch": 0.0008165406207328792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 224992
    },
    {
      "epoch": 0.0008166567550837158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 225024
    },
    {
      "epoch": 0.0008167728894345526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6922,
      "step": 225056
    },
    {
      "epoch": 0.0008168890237853892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 225088
    },
    {
      "epoch": 0.000817005158136226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 225120
    },
    {
      "epoch": 0.0008171212924870628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 225152
    },
    {
      "epoch": 0.0008172374268378994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 225184
    },
    {
      "epoch": 0.0008173535611887362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 225216
    },
    {
      "epoch": 0.0008174696955395728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7449,
      "step": 225248
    },
    {
      "epoch": 0.0008175858298904096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 225280
    },
    {
      "epoch": 0.0008177019642412462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7478,
      "step": 225312
    },
    {
      "epoch": 0.000817818098592083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6901,
      "step": 225344
    },
    {
      "epoch": 0.0008179342329429196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.692,
      "step": 225376
    },
    {
      "epoch": 0.0008180503672937564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 225408
    },
    {
      "epoch": 0.0008181665016445931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 225440
    },
    {
      "epoch": 0.0008182826359954298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 225472
    },
    {
      "epoch": 0.0008183987703462665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 225504
    },
    {
      "epoch": 0.0008185149046971032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 225536
    },
    {
      "epoch": 0.0008186310390479399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6881,
      "step": 225568
    },
    {
      "epoch": 0.0008187471733987766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 225600
    },
    {
      "epoch": 0.0008188633077496133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 225632
    },
    {
      "epoch": 0.00081897944210045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 225664
    },
    {
      "epoch": 0.0008190955764512867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 225696
    },
    {
      "epoch": 0.0008192117108021235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 225728
    },
    {
      "epoch": 0.0008193278451529601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 225760
    },
    {
      "epoch": 0.0008194439795037969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 225792
    },
    {
      "epoch": 0.0008195601138546335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 225824
    },
    {
      "epoch": 0.0008196762482054703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 225856
    },
    {
      "epoch": 0.0008197923825563069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 225888
    },
    {
      "epoch": 0.0008199085169071437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 225920
    },
    {
      "epoch": 0.0008200246512579803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 225952
    },
    {
      "epoch": 0.0008201407856088171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 225984
    },
    {
      "epoch": 0.0008202569199596538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 226016
    },
    {
      "epoch": 0.0008203730543104905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 226048
    },
    {
      "epoch": 0.0008204891886613272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 226080
    },
    {
      "epoch": 0.0008206053230121639,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 226112
    },
    {
      "epoch": 0.0008207214573630006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 226144
    },
    {
      "epoch": 0.0008208375917138373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 226176
    },
    {
      "epoch": 0.000820953726064674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 226208
    },
    {
      "epoch": 0.0008210698604155107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 226240
    },
    {
      "epoch": 0.0008211859947663474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 226272
    },
    {
      "epoch": 0.0008213021291171842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 226304
    },
    {
      "epoch": 0.0008214182634680208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 226336
    },
    {
      "epoch": 0.0008215343978188576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 226368
    },
    {
      "epoch": 0.0008216505321696942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 226400
    },
    {
      "epoch": 0.000821766666520531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 226432
    },
    {
      "epoch": 0.0008218828008713676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 226464
    },
    {
      "epoch": 0.0008219989352222044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 226496
    },
    {
      "epoch": 0.000822115069573041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 226528
    },
    {
      "epoch": 0.0008222312039238778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 226560
    },
    {
      "epoch": 0.0008223473382747145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 226592
    },
    {
      "epoch": 0.0008224634726255512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 226624
    },
    {
      "epoch": 0.0008225796069763879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 226656
    },
    {
      "epoch": 0.0008226957413272246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 226688
    },
    {
      "epoch": 0.0008228118756780613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 226720
    },
    {
      "epoch": 0.000822928010028898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 226752
    },
    {
      "epoch": 0.0008230441443797347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 226784
    },
    {
      "epoch": 0.0008231602787305714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 226816
    },
    {
      "epoch": 0.0008232764130814081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 226848
    },
    {
      "epoch": 0.0008233925474322449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7521,
      "step": 226880
    },
    {
      "epoch": 0.0008235086817830815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.756,
      "step": 226912
    },
    {
      "epoch": 0.0008236248161339183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 226944
    },
    {
      "epoch": 0.0008237409504847549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 226976
    },
    {
      "epoch": 0.0008238570848355917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 227008
    },
    {
      "epoch": 0.0008239732191864283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 227040
    },
    {
      "epoch": 0.0008240893535372651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 227072
    },
    {
      "epoch": 0.0008242054878881017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 227104
    },
    {
      "epoch": 0.0008243216222389385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 227136
    },
    {
      "epoch": 0.0008244377565897752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 227168
    },
    {
      "epoch": 0.0008245538909406119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 227200
    },
    {
      "epoch": 0.0008246700252914486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6957,
      "step": 227232
    },
    {
      "epoch": 0.0008247861596422853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 227264
    },
    {
      "epoch": 0.000824902293993122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 227296
    },
    {
      "epoch": 0.0008250184283439587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 227328
    },
    {
      "epoch": 0.0008251345626947954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 227360
    },
    {
      "epoch": 0.0008252506970456321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 227392
    },
    {
      "epoch": 0.0008253668313964688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6872,
      "step": 227424
    },
    {
      "epoch": 0.0008254829657473056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 227456
    },
    {
      "epoch": 0.0008255991000981422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 227488
    },
    {
      "epoch": 0.000825715234448979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 227520
    },
    {
      "epoch": 0.0008258313687998156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 227552
    },
    {
      "epoch": 0.0008259475031506524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 227584
    },
    {
      "epoch": 0.000826063637501489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 227616
    },
    {
      "epoch": 0.0008261797718523258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 227648
    },
    {
      "epoch": 0.0008262959062031624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 227680
    },
    {
      "epoch": 0.0008264120405539992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 227712
    },
    {
      "epoch": 0.0008265281749048359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 227744
    },
    {
      "epoch": 0.0008266443092556726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 227776
    },
    {
      "epoch": 0.0008267604436065093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 227808
    },
    {
      "epoch": 0.000826876577957346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6876,
      "step": 227840
    },
    {
      "epoch": 0.0008269927123081827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 227872
    },
    {
      "epoch": 0.0008271088466590194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 227904
    },
    {
      "epoch": 0.0008272249810098561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 227936
    },
    {
      "epoch": 0.0008273411153606928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 227968
    },
    {
      "epoch": 0.0008274572497115295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 228000
    },
    {
      "epoch": 0.0008275733840623663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 228032
    },
    {
      "epoch": 0.0008276895184132029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 228064
    },
    {
      "epoch": 0.0008278056527640397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 228096
    },
    {
      "epoch": 0.0008279217871148763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 228128
    },
    {
      "epoch": 0.0008280379214657131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 228160
    },
    {
      "epoch": 0.0008281540558165497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 228192
    },
    {
      "epoch": 0.0008282701901673865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 228224
    },
    {
      "epoch": 0.0008283863245182231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 228256
    },
    {
      "epoch": 0.0008285024588690599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 228288
    },
    {
      "epoch": 0.0008286185932198966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 228320
    },
    {
      "epoch": 0.0008287347275707333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7529,
      "step": 228352
    },
    {
      "epoch": 0.00082885086192157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 228384
    },
    {
      "epoch": 0.0008289669962724067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 228416
    },
    {
      "epoch": 0.0008290831306232434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 228448
    },
    {
      "epoch": 0.0008291992649740801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 228480
    },
    {
      "epoch": 0.0008293153993249168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 228512
    },
    {
      "epoch": 0.0008294315336757535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 228544
    },
    {
      "epoch": 0.0008295476680265902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 228576
    },
    {
      "epoch": 0.000829663802377427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7505,
      "step": 228608
    },
    {
      "epoch": 0.0008297799367282636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 228640
    },
    {
      "epoch": 0.0008298960710791004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 228672
    },
    {
      "epoch": 0.000830012205429937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 228704
    },
    {
      "epoch": 0.0008301283397807738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 228736
    },
    {
      "epoch": 0.0008302444741316104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7528,
      "step": 228768
    },
    {
      "epoch": 0.0008303606084824472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 228800
    },
    {
      "epoch": 0.0008304767428332838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 228832
    },
    {
      "epoch": 0.0008305928771841206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 228864
    },
    {
      "epoch": 0.0008307090115349573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 228896
    },
    {
      "epoch": 0.000830825145885794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 228928
    },
    {
      "epoch": 0.0008309412802366307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 228960
    },
    {
      "epoch": 0.0008310574145874674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 228992
    },
    {
      "epoch": 0.0008311735489383041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 229024
    },
    {
      "epoch": 0.0008312896832891408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 229056
    },
    {
      "epoch": 0.0008314058176399775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 229088
    },
    {
      "epoch": 0.0008315219519908142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 229120
    },
    {
      "epoch": 0.000831638086341651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 229152
    },
    {
      "epoch": 0.0008317542206924877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 229184
    },
    {
      "epoch": 0.0008318703550433243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.756,
      "step": 229216
    },
    {
      "epoch": 0.0008319864893941611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7728,
      "step": 229248
    },
    {
      "epoch": 0.0008321026237449977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 229280
    },
    {
      "epoch": 0.0008322187580958345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.681,
      "step": 229312
    },
    {
      "epoch": 0.0008323348924466711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 229344
    },
    {
      "epoch": 0.0008324510267975079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 229376
    },
    {
      "epoch": 0.0008325671611483445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 229408
    },
    {
      "epoch": 0.0008326832954991813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 229440
    },
    {
      "epoch": 0.000832799429850018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 229472
    },
    {
      "epoch": 0.0008329155642008547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 229504
    },
    {
      "epoch": 0.0008330316985516915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 229536
    },
    {
      "epoch": 0.0008331478329025281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 229568
    },
    {
      "epoch": 0.0008332639672533649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 229600
    },
    {
      "epoch": 0.0008333801016042015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 229632
    },
    {
      "epoch": 0.0008334962359550383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 229664
    },
    {
      "epoch": 0.0008336123703058749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 229696
    },
    {
      "epoch": 0.0008337285046567117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 229728
    },
    {
      "epoch": 0.0008338446390075484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 229760
    },
    {
      "epoch": 0.000833960773358385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 229792
    },
    {
      "epoch": 0.0008340769077092218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 229824
    },
    {
      "epoch": 0.0008341930420600585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 229856
    },
    {
      "epoch": 0.0008343091764108952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 229888
    },
    {
      "epoch": 0.0008344253107617319,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 229920
    },
    {
      "epoch": 0.0008345414451125686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 229952
    },
    {
      "epoch": 0.0008346575794634053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 229984
    },
    {
      "epoch": 0.000834773713814242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 230016
    },
    {
      "epoch": 0.0008348898481650788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 230048
    },
    {
      "epoch": 0.0008350059825159154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 230080
    },
    {
      "epoch": 0.0008351221168667522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 230112
    },
    {
      "epoch": 0.0008352382512175888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 230144
    },
    {
      "epoch": 0.0008353543855684256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 230176
    },
    {
      "epoch": 0.0008354705199192622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 230208
    },
    {
      "epoch": 0.000835586654270099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 230240
    },
    {
      "epoch": 0.0008357027886209356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 230272
    },
    {
      "epoch": 0.0008358189229717724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 230304
    },
    {
      "epoch": 0.0008359350573226091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 230336
    },
    {
      "epoch": 0.0008360511916734458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 230368
    },
    {
      "epoch": 0.0008361673260242825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 230400
    },
    {
      "epoch": 0.0008362834603751192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 230432
    },
    {
      "epoch": 0.0008363995947259559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6908,
      "step": 230464
    },
    {
      "epoch": 0.0008365157290767926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 230496
    },
    {
      "epoch": 0.0008366318634276293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 230528
    },
    {
      "epoch": 0.000836747997778466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 230560
    },
    {
      "epoch": 0.0008368641321293027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 230592
    },
    {
      "epoch": 0.0008369802664801395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 230624
    },
    {
      "epoch": 0.0008370964008309761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 230656
    },
    {
      "epoch": 0.0008372125351818129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 230688
    },
    {
      "epoch": 0.0008373286695326495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 230720
    },
    {
      "epoch": 0.0008374448038834863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6866,
      "step": 230752
    },
    {
      "epoch": 0.0008375609382343229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 230784
    },
    {
      "epoch": 0.0008376770725851597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 230816
    },
    {
      "epoch": 0.0008377932069359963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 230848
    },
    {
      "epoch": 0.0008379093412868331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 230880
    },
    {
      "epoch": 0.0008380254756376698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 230912
    },
    {
      "epoch": 0.0008381416099885065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6877,
      "step": 230944
    },
    {
      "epoch": 0.0008382577443393432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 230976
    },
    {
      "epoch": 0.0008383738786901799,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 231008
    },
    {
      "epoch": 0.0008384900130410166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 231040
    },
    {
      "epoch": 0.0008386061473918533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 231072
    },
    {
      "epoch": 0.00083872228174269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 231104
    },
    {
      "epoch": 0.0008388384160935267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7473,
      "step": 231136
    },
    {
      "epoch": 0.0008389545504443634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 231168
    },
    {
      "epoch": 0.0008390706847952002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 231200
    },
    {
      "epoch": 0.0008391868191460368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 231232
    },
    {
      "epoch": 0.0008393029534968736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 231264
    },
    {
      "epoch": 0.0008394190878477102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 231296
    },
    {
      "epoch": 0.000839535222198547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 231328
    },
    {
      "epoch": 0.0008396513565493836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 231360
    },
    {
      "epoch": 0.0008397674909002204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 231392
    },
    {
      "epoch": 0.000839883625251057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 231424
    },
    {
      "epoch": 0.0008399997596018938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 231456
    },
    {
      "epoch": 0.0008401158939527305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 231488
    },
    {
      "epoch": 0.0008402320283035672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 231520
    },
    {
      "epoch": 0.0008403481626544039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 231552
    },
    {
      "epoch": 0.0008404642970052406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 231584
    },
    {
      "epoch": 0.0008405804313560773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 231616
    },
    {
      "epoch": 0.000840696565706914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 231648
    },
    {
      "epoch": 0.0008408127000577507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 231680
    },
    {
      "epoch": 0.0008409288344085874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 231712
    },
    {
      "epoch": 0.0008410449687594241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 231744
    },
    {
      "epoch": 0.0008411611031102608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 231776
    },
    {
      "epoch": 0.0008412772374610975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 231808
    },
    {
      "epoch": 0.0008413933718119343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 231840
    },
    {
      "epoch": 0.0008415095061627709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7498,
      "step": 231872
    },
    {
      "epoch": 0.0008416256405136077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 231904
    },
    {
      "epoch": 0.0008417417748644443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 231936
    },
    {
      "epoch": 0.0008418579092152811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 231968
    },
    {
      "epoch": 0.0008419740435661177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 232000
    },
    {
      "epoch": 0.0008420901779169545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 232032
    },
    {
      "epoch": 0.0008422063122677911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 232064
    },
    {
      "epoch": 0.0008423224466186279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 232096
    },
    {
      "epoch": 0.0008424385809694646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 232128
    },
    {
      "epoch": 0.0008425547153203013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 232160
    },
    {
      "epoch": 0.000842670849671138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 232192
    },
    {
      "epoch": 0.0008427869840219747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6827,
      "step": 232224
    },
    {
      "epoch": 0.0008429031183728114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 232256
    },
    {
      "epoch": 0.0008430192527236481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 232288
    },
    {
      "epoch": 0.0008431353870744848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7428,
      "step": 232320
    },
    {
      "epoch": 0.0008432515214253215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 232352
    },
    {
      "epoch": 0.0008433676557761582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 232384
    },
    {
      "epoch": 0.000843483790126995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 232416
    },
    {
      "epoch": 0.0008435999244778316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 232448
    },
    {
      "epoch": 0.0008437160588286684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 232480
    },
    {
      "epoch": 0.000843832193179505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 232512
    },
    {
      "epoch": 0.0008439483275303418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 232544
    },
    {
      "epoch": 0.0008440644618811784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 232576
    },
    {
      "epoch": 0.0008441805962320152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 232608
    },
    {
      "epoch": 0.0008442967305828518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 232640
    },
    {
      "epoch": 0.0008444128649336886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 232672
    },
    {
      "epoch": 0.0008445289992845253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 232704
    },
    {
      "epoch": 0.000844645133635362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7528,
      "step": 232736
    },
    {
      "epoch": 0.0008447612679861987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 232768
    },
    {
      "epoch": 0.0008448774023370354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 232800
    },
    {
      "epoch": 0.0008449935366878721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6901,
      "step": 232832
    },
    {
      "epoch": 0.0008451096710387088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 232864
    },
    {
      "epoch": 0.0008452258053895455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 232896
    },
    {
      "epoch": 0.0008453419397403822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 232928
    },
    {
      "epoch": 0.0008454580740912189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 232960
    },
    {
      "epoch": 0.0008455742084420557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 232992
    },
    {
      "epoch": 0.0008456903427928923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 233024
    },
    {
      "epoch": 0.0008458064771437291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 233056
    },
    {
      "epoch": 0.0008459226114945657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 233088
    },
    {
      "epoch": 0.0008460387458454025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 233120
    },
    {
      "epoch": 0.0008461548801962391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 233152
    },
    {
      "epoch": 0.0008462710145470759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7459,
      "step": 233184
    },
    {
      "epoch": 0.0008463871488979125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 233216
    },
    {
      "epoch": 0.0008465032832487493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 233248
    },
    {
      "epoch": 0.000846619417599586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 233280
    },
    {
      "epoch": 0.0008467355519504227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 233312
    },
    {
      "epoch": 0.0008468516863012594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 233344
    },
    {
      "epoch": 0.0008469678206520961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 233376
    },
    {
      "epoch": 0.0008470839550029328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 233408
    },
    {
      "epoch": 0.0008472000893537695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 233440
    },
    {
      "epoch": 0.0008473162237046062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 233472
    },
    {
      "epoch": 0.0008474323580554429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 233504
    },
    {
      "epoch": 0.0008475484924062796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 233536
    },
    {
      "epoch": 0.0008476646267571164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 233568
    },
    {
      "epoch": 0.000847780761107953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 233600
    },
    {
      "epoch": 0.0008478968954587898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 233632
    },
    {
      "epoch": 0.0008480130298096264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 233664
    },
    {
      "epoch": 0.0008481291641604632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 233696
    },
    {
      "epoch": 0.0008482452985112998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 233728
    },
    {
      "epoch": 0.0008483614328621366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 233760
    },
    {
      "epoch": 0.0008484775672129732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 233792
    },
    {
      "epoch": 0.00084859370156381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 233824
    },
    {
      "epoch": 0.0008487098359146468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 233856
    },
    {
      "epoch": 0.0008488259702654834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 233888
    },
    {
      "epoch": 0.0008489421046163202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 233920
    },
    {
      "epoch": 0.0008490582389671568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 233952
    },
    {
      "epoch": 0.0008491743733179936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 233984
    },
    {
      "epoch": 0.0008492905076688302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 234016
    },
    {
      "epoch": 0.000849406642019667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 234048
    },
    {
      "epoch": 0.0008495227763705036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.749,
      "step": 234080
    },
    {
      "epoch": 0.0008496389107213404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 234112
    },
    {
      "epoch": 0.0008497550450721771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 234144
    },
    {
      "epoch": 0.0008498711794230138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 234176
    },
    {
      "epoch": 0.0008499873137738505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 234208
    },
    {
      "epoch": 0.0008501034481246872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6923,
      "step": 234240
    },
    {
      "epoch": 0.0008502195824755239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 234272
    },
    {
      "epoch": 0.0008503357168263606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 234304
    },
    {
      "epoch": 0.0008504518511771973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 234336
    },
    {
      "epoch": 0.000850567985528034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 234368
    },
    {
      "epoch": 0.0008506841198788707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 234400
    },
    {
      "epoch": 0.0008508002542297075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 234432
    },
    {
      "epoch": 0.0008509163885805441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6934,
      "step": 234464
    },
    {
      "epoch": 0.0008510325229313809,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 234496
    },
    {
      "epoch": 0.0008511486572822175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 234528
    },
    {
      "epoch": 0.0008512647916330543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 234560
    },
    {
      "epoch": 0.0008513809259838909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 234592
    },
    {
      "epoch": 0.0008514970603347277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 234624
    },
    {
      "epoch": 0.0008516131946855643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 234656
    },
    {
      "epoch": 0.0008517293290364011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 234688
    },
    {
      "epoch": 0.0008518454633872378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 234720
    },
    {
      "epoch": 0.0008519615977380745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 234752
    },
    {
      "epoch": 0.0008520777320889112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 234784
    },
    {
      "epoch": 0.0008521938664397479,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 234816
    },
    {
      "epoch": 0.0008523100007905846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 234848
    },
    {
      "epoch": 0.0008524261351414213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 234880
    },
    {
      "epoch": 0.000852542269492258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 234912
    },
    {
      "epoch": 0.0008526584038430947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 234944
    },
    {
      "epoch": 0.0008527745381939314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 234976
    },
    {
      "epoch": 0.0008528906725447682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 235008
    },
    {
      "epoch": 0.0008530068068956048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 235040
    },
    {
      "epoch": 0.0008531229412464416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 235072
    },
    {
      "epoch": 0.0008532390755972782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 235104
    },
    {
      "epoch": 0.000853355209948115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 235136
    },
    {
      "epoch": 0.0008534713442989516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 235168
    },
    {
      "epoch": 0.0008535874786497884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 235200
    },
    {
      "epoch": 0.000853703613000625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 235232
    },
    {
      "epoch": 0.0008538197473514618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 235264
    },
    {
      "epoch": 0.0008539358817022985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 235296
    },
    {
      "epoch": 0.0008540520160531352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 235328
    },
    {
      "epoch": 0.0008541681504039719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 235360
    },
    {
      "epoch": 0.0008542842847548086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 235392
    },
    {
      "epoch": 0.0008544004191056453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 235424
    },
    {
      "epoch": 0.000854516553456482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 235456
    },
    {
      "epoch": 0.0008546326878073187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 235488
    },
    {
      "epoch": 0.0008547488221581554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 235520
    },
    {
      "epoch": 0.0008548649565089921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 235552
    },
    {
      "epoch": 0.0008549810908598289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 235584
    },
    {
      "epoch": 0.0008550972252106655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 235616
    },
    {
      "epoch": 0.0008552133595615023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 235648
    },
    {
      "epoch": 0.0008553294939123389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 235680
    },
    {
      "epoch": 0.0008554456282631757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 235712
    },
    {
      "epoch": 0.0008555617626140123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 235744
    },
    {
      "epoch": 0.0008556778969648491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 235776
    },
    {
      "epoch": 0.0008557940313156857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7509,
      "step": 235808
    },
    {
      "epoch": 0.0008559101656665225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7662,
      "step": 235840
    },
    {
      "epoch": 0.0008560263000173592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 235872
    },
    {
      "epoch": 0.0008561424343681959,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 235904
    },
    {
      "epoch": 0.0008562585687190326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 235936
    },
    {
      "epoch": 0.0008563747030698693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 235968
    },
    {
      "epoch": 0.000856490837420706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 236000
    },
    {
      "epoch": 0.0008566069717715427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 236032
    },
    {
      "epoch": 0.0008567231061223794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 236064
    },
    {
      "epoch": 0.0008568392404732161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 236096
    },
    {
      "epoch": 0.0008569553748240528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 236128
    },
    {
      "epoch": 0.0008570715091748896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 236160
    },
    {
      "epoch": 0.0008571876435257262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 236192
    },
    {
      "epoch": 0.000857303777876563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 236224
    },
    {
      "epoch": 0.0008574199122273996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 236256
    },
    {
      "epoch": 0.0008575360465782364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 236288
    },
    {
      "epoch": 0.000857652180929073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 236320
    },
    {
      "epoch": 0.0008577683152799098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6934,
      "step": 236352
    },
    {
      "epoch": 0.0008578844496307464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 236384
    },
    {
      "epoch": 0.0008580005839815832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 236416
    },
    {
      "epoch": 0.0008581167183324199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 236448
    },
    {
      "epoch": 0.0008582328526832566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 236480
    },
    {
      "epoch": 0.0008583489870340933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 236512
    },
    {
      "epoch": 0.00085846512138493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6882,
      "step": 236544
    },
    {
      "epoch": 0.0008585812557357667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 236576
    },
    {
      "epoch": 0.0008586973900866034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 236608
    },
    {
      "epoch": 0.0008588135244374401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 236640
    },
    {
      "epoch": 0.0008589296587882768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 236672
    },
    {
      "epoch": 0.0008590457931391135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7649,
      "step": 236704
    },
    {
      "epoch": 0.0008591619274899503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 236736
    },
    {
      "epoch": 0.0008592780618407869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 236768
    },
    {
      "epoch": 0.0008593941961916237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 236800
    },
    {
      "epoch": 0.0008595103305424603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 236832
    },
    {
      "epoch": 0.0008596264648932971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 236864
    },
    {
      "epoch": 0.0008597425992441337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 236896
    },
    {
      "epoch": 0.0008598587335949705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 236928
    },
    {
      "epoch": 0.0008599748679458071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6863,
      "step": 236960
    },
    {
      "epoch": 0.0008600910022966439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 236992
    },
    {
      "epoch": 0.0008602071366474806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 237024
    },
    {
      "epoch": 0.0008603232709983173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 237056
    },
    {
      "epoch": 0.000860439405349154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 237088
    },
    {
      "epoch": 0.0008605555396999907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 237120
    },
    {
      "epoch": 0.0008606716740508274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 237152
    },
    {
      "epoch": 0.0008607878084016641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 237184
    },
    {
      "epoch": 0.0008609039427525008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 237216
    },
    {
      "epoch": 0.0008610200771033375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 237248
    },
    {
      "epoch": 0.0008611362114541742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 237280
    },
    {
      "epoch": 0.000861252345805011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 237312
    },
    {
      "epoch": 0.0008613684801558476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 237344
    },
    {
      "epoch": 0.0008614846145066844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 237376
    },
    {
      "epoch": 0.000861600748857521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 237408
    },
    {
      "epoch": 0.0008617168832083578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 237440
    },
    {
      "epoch": 0.0008618330175591944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 237472
    },
    {
      "epoch": 0.0008619491519100312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 237504
    },
    {
      "epoch": 0.0008620652862608678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 237536
    },
    {
      "epoch": 0.0008621814206117046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 237568
    },
    {
      "epoch": 0.0008622975549625413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7554,
      "step": 237600
    },
    {
      "epoch": 0.000862413689313378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 237632
    },
    {
      "epoch": 0.0008625298236642147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7409,
      "step": 237664
    },
    {
      "epoch": 0.0008626459580150514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 237696
    },
    {
      "epoch": 0.0008627620923658881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 237728
    },
    {
      "epoch": 0.0008628782267167248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 237760
    },
    {
      "epoch": 0.0008629943610675615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 237792
    },
    {
      "epoch": 0.0008631104954183982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 237824
    },
    {
      "epoch": 0.000863226629769235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 237856
    },
    {
      "epoch": 0.0008633427641200717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 237888
    },
    {
      "epoch": 0.0008634588984709083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 237920
    },
    {
      "epoch": 0.0008635750328217451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 237952
    },
    {
      "epoch": 0.0008636911671725817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 237984
    },
    {
      "epoch": 0.0008638073015234185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 238016
    },
    {
      "epoch": 0.0008639234358742551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 238048
    },
    {
      "epoch": 0.0008640395702250919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 238080
    },
    {
      "epoch": 0.0008641557045759285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 238112
    },
    {
      "epoch": 0.0008642718389267653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 238144
    },
    {
      "epoch": 0.0008643879732776021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 238176
    },
    {
      "epoch": 0.0008645041076284387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 238208
    },
    {
      "epoch": 0.0008646202419792755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 238240
    },
    {
      "epoch": 0.0008647363763301121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 238272
    },
    {
      "epoch": 0.0008648525106809489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 238304
    },
    {
      "epoch": 0.0008649686450317855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 238336
    },
    {
      "epoch": 0.0008650847793826223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 238368
    },
    {
      "epoch": 0.0008652009137334589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 238400
    },
    {
      "epoch": 0.0008653170480842957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 238432
    },
    {
      "epoch": 0.0008654331824351324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7733,
      "step": 238464
    },
    {
      "epoch": 0.0008655493167859691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 238496
    },
    {
      "epoch": 0.0008656654511368058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 238528
    },
    {
      "epoch": 0.0008657815854876425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 238560
    },
    {
      "epoch": 0.0008658977198384792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 238592
    },
    {
      "epoch": 0.0008660138541893159,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 238624
    },
    {
      "epoch": 0.0008661299885401526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 238656
    },
    {
      "epoch": 0.0008662461228909893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 238688
    },
    {
      "epoch": 0.000866362257241826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 238720
    },
    {
      "epoch": 0.0008664783915926628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 238752
    },
    {
      "epoch": 0.0008665945259434994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 238784
    },
    {
      "epoch": 0.0008667106602943362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 238816
    },
    {
      "epoch": 0.0008668267946451728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 238848
    },
    {
      "epoch": 0.0008669429289960096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 238880
    },
    {
      "epoch": 0.0008670590633468462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 238912
    },
    {
      "epoch": 0.000867175197697683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 238944
    },
    {
      "epoch": 0.0008672913320485196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 238976
    },
    {
      "epoch": 0.0008674074663993564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 239008
    },
    {
      "epoch": 0.0008675236007501931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 239040
    },
    {
      "epoch": 0.0008676397351010298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 239072
    },
    {
      "epoch": 0.0008677558694518665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 239104
    },
    {
      "epoch": 0.0008678720038027032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 239136
    },
    {
      "epoch": 0.0008679881381535399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 239168
    },
    {
      "epoch": 0.0008681042725043766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 239200
    },
    {
      "epoch": 0.0008682204068552133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 239232
    },
    {
      "epoch": 0.00086833654120605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 239264
    },
    {
      "epoch": 0.0008684526755568867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 239296
    },
    {
      "epoch": 0.0008685688099077235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 239328
    },
    {
      "epoch": 0.0008686849442585601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 239360
    },
    {
      "epoch": 0.0008688010786093969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 239392
    },
    {
      "epoch": 0.0008689172129602335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 239424
    },
    {
      "epoch": 0.0008690333473110703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 239456
    },
    {
      "epoch": 0.0008691494816619069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 239488
    },
    {
      "epoch": 0.0008692656160127437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 239520
    },
    {
      "epoch": 0.0008693817503635803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 239552
    },
    {
      "epoch": 0.0008694978847144171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 239584
    },
    {
      "epoch": 0.0008696140190652538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 239616
    },
    {
      "epoch": 0.0008697301534160905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 239648
    },
    {
      "epoch": 0.0008698462877669272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 239680
    },
    {
      "epoch": 0.0008699624221177639,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 239712
    },
    {
      "epoch": 0.0008700785564686006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 239744
    },
    {
      "epoch": 0.0008701946908194373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 239776
    },
    {
      "epoch": 0.000870310825170274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 239808
    },
    {
      "epoch": 0.0008704269595211107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6834,
      "step": 239840
    },
    {
      "epoch": 0.0008705430938719474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 239872
    },
    {
      "epoch": 0.0008706592282227842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 239904
    },
    {
      "epoch": 0.0008707753625736208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 239936
    },
    {
      "epoch": 0.0008708914969244576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 239968
    },
    {
      "epoch": 0.0008710076312752942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 240000
    },
    {
      "epoch": 0.000871123765626131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 240032
    },
    {
      "epoch": 0.0008712398999769676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 240064
    },
    {
      "epoch": 0.0008713560343278044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 240096
    },
    {
      "epoch": 0.000871472168678641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 240128
    },
    {
      "epoch": 0.0008715883030294778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 240160
    },
    {
      "epoch": 0.0008717044373803145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 240192
    },
    {
      "epoch": 0.0008718205717311512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7625,
      "step": 240224
    },
    {
      "epoch": 0.0008719367060819879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 240256
    },
    {
      "epoch": 0.0008720528404328246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 240288
    },
    {
      "epoch": 0.0008721689747836613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 240320
    },
    {
      "epoch": 0.000872285109134498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 240352
    },
    {
      "epoch": 0.0008724012434853347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 240384
    },
    {
      "epoch": 0.0008725173778361714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 240416
    },
    {
      "epoch": 0.0008726335121870081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 240448
    },
    {
      "epoch": 0.0008727496465378449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 240480
    },
    {
      "epoch": 0.0008728657808886815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 240512
    },
    {
      "epoch": 0.0008729819152395183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 240544
    },
    {
      "epoch": 0.0008730980495903549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 240576
    },
    {
      "epoch": 0.0008732141839411917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 240608
    },
    {
      "epoch": 0.0008733303182920283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 240640
    },
    {
      "epoch": 0.0008734464526428651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 240672
    },
    {
      "epoch": 0.0008735625869937017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 240704
    },
    {
      "epoch": 0.0008736787213445385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 240736
    },
    {
      "epoch": 0.0008737948556953752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 240768
    },
    {
      "epoch": 0.0008739109900462119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 240800
    },
    {
      "epoch": 0.0008740271243970486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 240832
    },
    {
      "epoch": 0.0008741432587478853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6919,
      "step": 240864
    },
    {
      "epoch": 0.000874259393098722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 240896
    },
    {
      "epoch": 0.0008743755274495587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 240928
    },
    {
      "epoch": 0.0008744916618003954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 240960
    },
    {
      "epoch": 0.0008746077961512321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 240992
    },
    {
      "epoch": 0.0008747239305020688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 241024
    },
    {
      "epoch": 0.0008748400648529056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.69,
      "step": 241056
    },
    {
      "epoch": 0.0008749561992037422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 241088
    },
    {
      "epoch": 0.000875072333554579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7495,
      "step": 241120
    },
    {
      "epoch": 0.0008751884679054156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 241152
    },
    {
      "epoch": 0.0008753046022562524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 241184
    },
    {
      "epoch": 0.000875420736607089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 241216
    },
    {
      "epoch": 0.0008755368709579258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6877,
      "step": 241248
    },
    {
      "epoch": 0.0008756530053087624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 241280
    },
    {
      "epoch": 0.0008757691396595992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 241312
    },
    {
      "epoch": 0.0008758852740104359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 241344
    },
    {
      "epoch": 0.0008760014083612726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 241376
    },
    {
      "epoch": 0.0008761175427121093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 241408
    },
    {
      "epoch": 0.000876233677062946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 241440
    },
    {
      "epoch": 0.0008763498114137827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 241472
    },
    {
      "epoch": 0.0008764659457646194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 241504
    },
    {
      "epoch": 0.0008765820801154561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 241536
    },
    {
      "epoch": 0.0008766982144662928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 241568
    },
    {
      "epoch": 0.0008768143488171295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 241600
    },
    {
      "epoch": 0.0008769304831679663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 241632
    },
    {
      "epoch": 0.0008770466175188029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 241664
    },
    {
      "epoch": 0.0008771627518696397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7453,
      "step": 241696
    },
    {
      "epoch": 0.0008772788862204763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 241728
    },
    {
      "epoch": 0.0008773950205713131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 241760
    },
    {
      "epoch": 0.0008775111549221497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 241792
    },
    {
      "epoch": 0.0008776272892729865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 241824
    },
    {
      "epoch": 0.0008777434236238231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 241856
    },
    {
      "epoch": 0.0008778595579746599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 241888
    },
    {
      "epoch": 0.0008779756923254967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 241920
    },
    {
      "epoch": 0.0008780918266763333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 241952
    },
    {
      "epoch": 0.00087820796102717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7533,
      "step": 241984
    },
    {
      "epoch": 0.0008783240953780067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 242016
    },
    {
      "epoch": 0.0008784402297288435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 242048
    },
    {
      "epoch": 0.0008785563640796801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 242080
    },
    {
      "epoch": 0.0008786724984305169,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.688,
      "step": 242112
    },
    {
      "epoch": 0.0008787886327813535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 242144
    },
    {
      "epoch": 0.0008789047671321903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 242176
    },
    {
      "epoch": 0.000879020901483027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 242208
    },
    {
      "epoch": 0.0008791370358338636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 242240
    },
    {
      "epoch": 0.0008792531701847004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 242272
    },
    {
      "epoch": 0.000879369304535537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 242304
    },
    {
      "epoch": 0.0008794854388863738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 242336
    },
    {
      "epoch": 0.0008796015732372104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 242368
    },
    {
      "epoch": 0.0008797177075880472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 242400
    },
    {
      "epoch": 0.0008798338419388838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 242432
    },
    {
      "epoch": 0.0008799499762897206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.684,
      "step": 242464
    },
    {
      "epoch": 0.0008800661106405572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 242496
    },
    {
      "epoch": 0.000880182244991394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 242528
    },
    {
      "epoch": 0.0008802983793422308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 242560
    },
    {
      "epoch": 0.0008804145136930674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 242592
    },
    {
      "epoch": 0.0008805306480439042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.68,
      "step": 242624
    },
    {
      "epoch": 0.0008806467823947408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 242656
    },
    {
      "epoch": 0.0008807629167455776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 242688
    },
    {
      "epoch": 0.0008808790510964142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 242720
    },
    {
      "epoch": 0.000880995185447251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 242752
    },
    {
      "epoch": 0.0008811113197980876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 242784
    },
    {
      "epoch": 0.0008812274541489244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 242816
    },
    {
      "epoch": 0.0008813435884997611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 242848
    },
    {
      "epoch": 0.0008814597228505978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 242880
    },
    {
      "epoch": 0.0008815758572014345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 242912
    },
    {
      "epoch": 0.0008816919915522712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 242944
    },
    {
      "epoch": 0.0008818081259031079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 242976
    },
    {
      "epoch": 0.0008819242602539446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 243008
    },
    {
      "epoch": 0.0008820403946047813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 243040
    },
    {
      "epoch": 0.000882156528955618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 243072
    },
    {
      "epoch": 0.0008822726633064547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 243104
    },
    {
      "epoch": 0.0008823887976572915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 243136
    },
    {
      "epoch": 0.0008825049320081281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 243168
    },
    {
      "epoch": 0.0008826210663589649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 243200
    },
    {
      "epoch": 0.0008827372007098015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 243232
    },
    {
      "epoch": 0.0008828533350606383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 243264
    },
    {
      "epoch": 0.0008829694694114749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 243296
    },
    {
      "epoch": 0.0008830856037623117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 243328
    },
    {
      "epoch": 0.0008832017381131483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 243360
    },
    {
      "epoch": 0.0008833178724639851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 243392
    },
    {
      "epoch": 0.0008834340068148218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 243424
    },
    {
      "epoch": 0.0008835501411656585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 243456
    },
    {
      "epoch": 0.0008836662755164952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 243488
    },
    {
      "epoch": 0.0008837824098673319,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 243520
    },
    {
      "epoch": 0.0008838985442181686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 243552
    },
    {
      "epoch": 0.0008840146785690053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 243584
    },
    {
      "epoch": 0.000884130812919842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 243616
    },
    {
      "epoch": 0.0008842469472706787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 243648
    },
    {
      "epoch": 0.0008843630816215154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 243680
    },
    {
      "epoch": 0.0008844792159723522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 243712
    },
    {
      "epoch": 0.0008845953503231888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7607,
      "step": 243744
    },
    {
      "epoch": 0.0008847114846740256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 243776
    },
    {
      "epoch": 0.0008848276190248622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 243808
    },
    {
      "epoch": 0.000884943753375699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 243840
    },
    {
      "epoch": 0.0008850598877265356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 243872
    },
    {
      "epoch": 0.0008851760220773724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 243904
    },
    {
      "epoch": 0.000885292156428209,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 243936
    },
    {
      "epoch": 0.0008854082907790458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 243968
    },
    {
      "epoch": 0.0008855244251298825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 244000
    },
    {
      "epoch": 0.0008856405594807192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 244032
    },
    {
      "epoch": 0.0008857566938315559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 244064
    },
    {
      "epoch": 0.0008858728281823926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 244096
    },
    {
      "epoch": 0.0008859889625332293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 244128
    },
    {
      "epoch": 0.000886105096884066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 244160
    },
    {
      "epoch": 0.0008862212312349027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 244192
    },
    {
      "epoch": 0.0008863373655857394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6837,
      "step": 244224
    },
    {
      "epoch": 0.0008864534999365761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 244256
    },
    {
      "epoch": 0.0008865696342874129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 244288
    },
    {
      "epoch": 0.0008866857686382495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 244320
    },
    {
      "epoch": 0.0008868019029890863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 244352
    },
    {
      "epoch": 0.0008869180373399229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6911,
      "step": 244384
    },
    {
      "epoch": 0.0008870341716907597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 244416
    },
    {
      "epoch": 0.0008871503060415963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 244448
    },
    {
      "epoch": 0.0008872664403924331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 244480
    },
    {
      "epoch": 0.0008873825747432697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 244512
    },
    {
      "epoch": 0.0008874987090941065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 244544
    },
    {
      "epoch": 0.0008876148434449432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 244576
    },
    {
      "epoch": 0.0008877309777957799,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 244608
    },
    {
      "epoch": 0.0008878471121466166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 244640
    },
    {
      "epoch": 0.0008879632464974533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 244672
    },
    {
      "epoch": 0.00088807938084829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 244704
    },
    {
      "epoch": 0.0008881955151991267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 244736
    },
    {
      "epoch": 0.0008883116495499634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.691,
      "step": 244768
    },
    {
      "epoch": 0.0008884277839008001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6819,
      "step": 244800
    },
    {
      "epoch": 0.0008885439182516368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 244832
    },
    {
      "epoch": 0.0008886600526024736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 244864
    },
    {
      "epoch": 0.0008887761869533102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 244896
    },
    {
      "epoch": 0.000888892321304147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 244928
    },
    {
      "epoch": 0.0008890084556549836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 244960
    },
    {
      "epoch": 0.0008891245900058204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7286,
      "step": 244992
    },
    {
      "epoch": 0.000889240724356657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7456,
      "step": 245024
    },
    {
      "epoch": 0.0008893568587074938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 245056
    },
    {
      "epoch": 0.0008894729930583304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 245088
    },
    {
      "epoch": 0.0008895891274091672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 245120
    },
    {
      "epoch": 0.0008897052617600039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 245152
    },
    {
      "epoch": 0.0008898213961108406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 245184
    },
    {
      "epoch": 0.0008899375304616773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 245216
    },
    {
      "epoch": 0.000890053664812514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 245248
    },
    {
      "epoch": 0.0008901697991633507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 245280
    },
    {
      "epoch": 0.0008902859335141874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 245312
    },
    {
      "epoch": 0.0008904020678650241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 245344
    },
    {
      "epoch": 0.0008905182022158608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 245376
    },
    {
      "epoch": 0.0008906343365666975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 245408
    },
    {
      "epoch": 0.0008907504709175343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 245440
    },
    {
      "epoch": 0.0008908666052683709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 245472
    },
    {
      "epoch": 0.0008909827396192077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 245504
    },
    {
      "epoch": 0.0008910988739700443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 245536
    },
    {
      "epoch": 0.0008912150083208811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6891,
      "step": 245568
    },
    {
      "epoch": 0.0008913311426717177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 245600
    },
    {
      "epoch": 0.0008914472770225545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 245632
    },
    {
      "epoch": 0.0008915634113733911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 245664
    },
    {
      "epoch": 0.0008916795457242279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 245696
    },
    {
      "epoch": 0.0008917956800750646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 245728
    },
    {
      "epoch": 0.0008919118144259013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 245760
    },
    {
      "epoch": 0.000892027948776738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 245792
    },
    {
      "epoch": 0.0008921440831275747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 245824
    },
    {
      "epoch": 0.0008922602174784114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 245856
    },
    {
      "epoch": 0.0008923763518292481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 245888
    },
    {
      "epoch": 0.0008924924861800848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.749,
      "step": 245920
    },
    {
      "epoch": 0.0008926086205309215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 245952
    },
    {
      "epoch": 0.0008927247548817582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6786,
      "step": 245984
    },
    {
      "epoch": 0.000892840889232595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 246016
    },
    {
      "epoch": 0.0008929570235834316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 246048
    },
    {
      "epoch": 0.0008930731579342684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 246080
    },
    {
      "epoch": 0.000893189292285105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 246112
    },
    {
      "epoch": 0.0008933054266359418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 246144
    },
    {
      "epoch": 0.0008934215609867784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 246176
    },
    {
      "epoch": 0.0008935376953376152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 246208
    },
    {
      "epoch": 0.0008936538296884518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 246240
    },
    {
      "epoch": 0.0008937699640392886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 246272
    },
    {
      "epoch": 0.0008938860983901254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 246304
    },
    {
      "epoch": 0.000894002232740962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 246336
    },
    {
      "epoch": 0.0008941183670917988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 246368
    },
    {
      "epoch": 0.0008942345014426354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 246400
    },
    {
      "epoch": 0.0008943506357934722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 246432
    },
    {
      "epoch": 0.0008944667701443088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 246464
    },
    {
      "epoch": 0.0008945829044951456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 246496
    },
    {
      "epoch": 0.0008946990388459822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 246528
    },
    {
      "epoch": 0.000894815173196819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 246560
    },
    {
      "epoch": 0.0008949313075476557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 246592
    },
    {
      "epoch": 0.0008950474418984924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7488,
      "step": 246624
    },
    {
      "epoch": 0.0008951635762493291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 246656
    },
    {
      "epoch": 0.0008952797106001658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 246688
    },
    {
      "epoch": 0.0008953958449510025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6892,
      "step": 246720
    },
    {
      "epoch": 0.0008955119793018392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 246752
    },
    {
      "epoch": 0.0008956281136526759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 246784
    },
    {
      "epoch": 0.0008957442480035126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 246816
    },
    {
      "epoch": 0.0008958603823543493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 246848
    },
    {
      "epoch": 0.0008959765167051861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 246880
    },
    {
      "epoch": 0.0008960926510560227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 246912
    },
    {
      "epoch": 0.0008962087854068595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 246944
    },
    {
      "epoch": 0.0008963249197576961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 246976
    },
    {
      "epoch": 0.0008964410541085329,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 247008
    },
    {
      "epoch": 0.0008965571884593695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 247040
    },
    {
      "epoch": 0.0008966733228102063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 247072
    },
    {
      "epoch": 0.0008967894571610429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 247104
    },
    {
      "epoch": 0.0008969055915118797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 247136
    },
    {
      "epoch": 0.0008970217258627164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 247168
    },
    {
      "epoch": 0.0008971378602135531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 247200
    },
    {
      "epoch": 0.0008972539945643898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 247232
    },
    {
      "epoch": 0.0008973701289152265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 247264
    },
    {
      "epoch": 0.0008974862632660632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 247296
    },
    {
      "epoch": 0.0008976023976168999,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6936,
      "step": 247328
    },
    {
      "epoch": 0.0008977185319677366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 247360
    },
    {
      "epoch": 0.0008978346663185733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 247392
    },
    {
      "epoch": 0.00089795080066941,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 247424
    },
    {
      "epoch": 0.0008980669350202468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 247456
    },
    {
      "epoch": 0.0008981830693710834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 247488
    },
    {
      "epoch": 0.0008982992037219202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 247520
    },
    {
      "epoch": 0.0008984153380727568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6922,
      "step": 247552
    },
    {
      "epoch": 0.0008985314724235936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 247584
    },
    {
      "epoch": 0.0008986476067744302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 247616
    },
    {
      "epoch": 0.000898763741125267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 247648
    },
    {
      "epoch": 0.0008988798754761036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 247680
    },
    {
      "epoch": 0.0008989960098269404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 247712
    },
    {
      "epoch": 0.0008991121441777771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 247744
    },
    {
      "epoch": 0.0008992282785286138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 247776
    },
    {
      "epoch": 0.0008993444128794505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 247808
    },
    {
      "epoch": 0.0008994605472302872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 247840
    },
    {
      "epoch": 0.0008995766815811239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 247872
    },
    {
      "epoch": 0.0008996928159319606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 247904
    },
    {
      "epoch": 0.0008998089502827973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 247936
    },
    {
      "epoch": 0.000899925084633634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 247968
    },
    {
      "epoch": 0.0009000412189844707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 248000
    },
    {
      "epoch": 0.0009001573533353075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 248032
    },
    {
      "epoch": 0.0009002734876861441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7434,
      "step": 248064
    },
    {
      "epoch": 0.0009003896220369809,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 248096
    },
    {
      "epoch": 0.0009005057563878175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 248128
    },
    {
      "epoch": 0.0009006218907386543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 248160
    },
    {
      "epoch": 0.0009007380250894909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 248192
    },
    {
      "epoch": 0.0009008541594403277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 248224
    },
    {
      "epoch": 0.0009009702937911643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 248256
    },
    {
      "epoch": 0.0009010864281420011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 248288
    },
    {
      "epoch": 0.0009012025624928378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 248320
    },
    {
      "epoch": 0.0009013186968436745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 248352
    },
    {
      "epoch": 0.0009014348311945112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 248384
    },
    {
      "epoch": 0.0009015509655453479,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 248416
    },
    {
      "epoch": 0.0009016670998961846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 248448
    },
    {
      "epoch": 0.0009017832342470213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 248480
    },
    {
      "epoch": 0.000901899368597858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 248512
    },
    {
      "epoch": 0.0009020155029486947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7431,
      "step": 248544
    },
    {
      "epoch": 0.0009021316372995314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 248576
    },
    {
      "epoch": 0.0009022477716503682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 248608
    },
    {
      "epoch": 0.0009023639060012048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 248640
    },
    {
      "epoch": 0.0009024800403520416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 248672
    },
    {
      "epoch": 0.0009025961747028782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 248704
    },
    {
      "epoch": 0.000902712309053715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 248736
    },
    {
      "epoch": 0.0009028284434045516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 248768
    },
    {
      "epoch": 0.0009029445777553884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 248800
    },
    {
      "epoch": 0.000903060712106225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 248832
    },
    {
      "epoch": 0.0009031768464570618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 248864
    },
    {
      "epoch": 0.0009032929808078985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 248896
    },
    {
      "epoch": 0.0009034091151587352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 248928
    },
    {
      "epoch": 0.0009035252495095719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 248960
    },
    {
      "epoch": 0.0009036413838604086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7538,
      "step": 248992
    },
    {
      "epoch": 0.0009037575182112453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 249024
    },
    {
      "epoch": 0.000903873652562082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 249056
    },
    {
      "epoch": 0.0009039897869129187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 249088
    },
    {
      "epoch": 0.0009041059212637554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 249120
    },
    {
      "epoch": 0.0009042220556145921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6927,
      "step": 249152
    },
    {
      "epoch": 0.0009043381899654289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 249184
    },
    {
      "epoch": 0.0009044543243162655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 249216
    },
    {
      "epoch": 0.0009045704586671023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 249248
    },
    {
      "epoch": 0.0009046865930179389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 249280
    },
    {
      "epoch": 0.0009048027273687757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 249312
    },
    {
      "epoch": 0.0009049188617196123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 249344
    },
    {
      "epoch": 0.0009050349960704491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 249376
    },
    {
      "epoch": 0.0009051511304212857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 249408
    },
    {
      "epoch": 0.0009052672647721225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 249440
    },
    {
      "epoch": 0.0009053833991229592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 249472
    },
    {
      "epoch": 0.0009054995334737959,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 249504
    },
    {
      "epoch": 0.0009056156678246326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 249536
    },
    {
      "epoch": 0.0009057318021754693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 249568
    },
    {
      "epoch": 0.000905847936526306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 249600
    },
    {
      "epoch": 0.0009059640708771427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 249632
    },
    {
      "epoch": 0.0009060802052279794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 249664
    },
    {
      "epoch": 0.0009061963395788161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 249696
    },
    {
      "epoch": 0.0009063124739296528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 249728
    },
    {
      "epoch": 0.0009064286082804896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 249760
    },
    {
      "epoch": 0.0009065447426313262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 249792
    },
    {
      "epoch": 0.000906660876982163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 249824
    },
    {
      "epoch": 0.0009067770113329996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 249856
    },
    {
      "epoch": 0.0009068931456838364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 249888
    },
    {
      "epoch": 0.000907009280034673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 249920
    },
    {
      "epoch": 0.0009071254143855098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 249952
    },
    {
      "epoch": 0.0009072415487363464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 249984
    },
    {
      "epoch": 0.0009073576830871832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6841,
      "step": 250016
    },
    {
      "epoch": 0.00090747381743802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 250048
    },
    {
      "epoch": 0.0009075899517888566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 250080
    },
    {
      "epoch": 0.0009077060861396933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 250112
    },
    {
      "epoch": 0.00090782222049053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 250144
    },
    {
      "epoch": 0.0009079383548413667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6909,
      "step": 250176
    },
    {
      "epoch": 0.0009080544891922034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 250208
    },
    {
      "epoch": 0.0009081706235430401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 250240
    },
    {
      "epoch": 0.0009082867578938768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 250272
    },
    {
      "epoch": 0.0009084028922447135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 250304
    },
    {
      "epoch": 0.0009085190265955503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 250336
    },
    {
      "epoch": 0.0009086351609463869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6881,
      "step": 250368
    },
    {
      "epoch": 0.0009087512952972237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 250400
    },
    {
      "epoch": 0.0009088674296480603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 250432
    },
    {
      "epoch": 0.0009089835639988971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 250464
    },
    {
      "epoch": 0.0009090996983497337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 250496
    },
    {
      "epoch": 0.0009092158327005705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 250528
    },
    {
      "epoch": 0.0009093319670514071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 250560
    },
    {
      "epoch": 0.0009094481014022439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 250592
    },
    {
      "epoch": 0.0009095642357530807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 250624
    },
    {
      "epoch": 0.0009096803701039173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 250656
    },
    {
      "epoch": 0.000909796504454754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 250688
    },
    {
      "epoch": 0.0009099126388055907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 250720
    },
    {
      "epoch": 0.0009100287731564275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 250752
    },
    {
      "epoch": 0.0009101449075072641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 250784
    },
    {
      "epoch": 0.0009102610418581009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 250816
    },
    {
      "epoch": 0.0009103771762089375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 250848
    },
    {
      "epoch": 0.0009104933105597743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 250880
    },
    {
      "epoch": 0.000910609444910611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6783,
      "step": 250912
    },
    {
      "epoch": 0.0009107255792614477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 250944
    },
    {
      "epoch": 0.0009108417136122844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 250976
    },
    {
      "epoch": 0.000910957847963121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 251008
    },
    {
      "epoch": 0.0009110739823139578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 251040
    },
    {
      "epoch": 0.0009111901166647945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 251072
    },
    {
      "epoch": 0.0009113062510156312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 251104
    },
    {
      "epoch": 0.0009114223853664679,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 251136
    },
    {
      "epoch": 0.0009115385197173046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 251168
    },
    {
      "epoch": 0.0009116546540681414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 251200
    },
    {
      "epoch": 0.000911770788418978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6791,
      "step": 251232
    },
    {
      "epoch": 0.0009118869227698148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 251264
    },
    {
      "epoch": 0.0009120030571206514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 251296
    },
    {
      "epoch": 0.0009121191914714882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 251328
    },
    {
      "epoch": 0.0009122353258223248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 251360
    },
    {
      "epoch": 0.0009123514601731616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 251392
    },
    {
      "epoch": 0.0009124675945239982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 251424
    },
    {
      "epoch": 0.000912583728874835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 251456
    },
    {
      "epoch": 0.0009126998632256717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 251488
    },
    {
      "epoch": 0.0009128159975765084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 251520
    },
    {
      "epoch": 0.0009129321319273451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 251552
    },
    {
      "epoch": 0.0009130482662781818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 251584
    },
    {
      "epoch": 0.0009131644006290185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 251616
    },
    {
      "epoch": 0.0009132805349798552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 251648
    },
    {
      "epoch": 0.0009133966693306919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 251680
    },
    {
      "epoch": 0.0009135128036815286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 251712
    },
    {
      "epoch": 0.0009136289380323653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 251744
    },
    {
      "epoch": 0.0009137450723832021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 251776
    },
    {
      "epoch": 0.0009138612067340387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 251808
    },
    {
      "epoch": 0.0009139773410848755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 251840
    },
    {
      "epoch": 0.0009140934754357121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 251872
    },
    {
      "epoch": 0.0009142096097865489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 251904
    },
    {
      "epoch": 0.0009143257441373855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 251936
    },
    {
      "epoch": 0.0009144418784882223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 251968
    },
    {
      "epoch": 0.0009145580128390589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 252000
    },
    {
      "epoch": 0.0009146741471898957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 252032
    },
    {
      "epoch": 0.0009147902815407324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 252064
    },
    {
      "epoch": 0.0009149064158915691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 252096
    },
    {
      "epoch": 0.0009150225502424058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6912,
      "step": 252128
    },
    {
      "epoch": 0.0009151386845932425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 252160
    },
    {
      "epoch": 0.0009152548189440792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 252192
    },
    {
      "epoch": 0.0009153709532949159,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6918,
      "step": 252224
    },
    {
      "epoch": 0.0009154870876457526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 252256
    },
    {
      "epoch": 0.0009156032219965893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 252288
    },
    {
      "epoch": 0.000915719356347426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 252320
    },
    {
      "epoch": 0.0009158354906982628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 252352
    },
    {
      "epoch": 0.0009159516250490994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 252384
    },
    {
      "epoch": 0.0009160677593999362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 252416
    },
    {
      "epoch": 0.0009161838937507728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 252448
    },
    {
      "epoch": 0.0009163000281016096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 252480
    },
    {
      "epoch": 0.0009164161624524462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 252512
    },
    {
      "epoch": 0.000916532296803283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 252544
    },
    {
      "epoch": 0.0009166484311541196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 252576
    },
    {
      "epoch": 0.0009167645655049564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 252608
    },
    {
      "epoch": 0.0009168806998557931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 252640
    },
    {
      "epoch": 0.0009169968342066298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 252672
    },
    {
      "epoch": 0.0009171129685574665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 252704
    },
    {
      "epoch": 0.0009172291029083032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 252736
    },
    {
      "epoch": 0.0009173452372591399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 252768
    },
    {
      "epoch": 0.0009174613716099766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 252800
    },
    {
      "epoch": 0.0009175775059608133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 252832
    },
    {
      "epoch": 0.00091769364031165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 252864
    },
    {
      "epoch": 0.0009178097746624867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7561,
      "step": 252896
    },
    {
      "epoch": 0.0009179259090133235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 252928
    },
    {
      "epoch": 0.0009180420433641601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 252960
    },
    {
      "epoch": 0.0009181581777149969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 252992
    },
    {
      "epoch": 0.0009182743120658335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 253024
    },
    {
      "epoch": 0.0009183904464166703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 253056
    },
    {
      "epoch": 0.0009185065807675069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 253088
    },
    {
      "epoch": 0.0009186227151183437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 253120
    },
    {
      "epoch": 0.0009187388494691803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 253152
    },
    {
      "epoch": 0.0009188549838200171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 253184
    },
    {
      "epoch": 0.0009189711181708537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 253216
    },
    {
      "epoch": 0.0009190872525216905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7481,
      "step": 253248
    },
    {
      "epoch": 0.0009192033868725272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 253280
    },
    {
      "epoch": 0.0009193195212233639,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 253312
    },
    {
      "epoch": 0.0009194356555742006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 253344
    },
    {
      "epoch": 0.0009195517899250373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.74,
      "step": 253376
    },
    {
      "epoch": 0.000919667924275874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 253408
    },
    {
      "epoch": 0.0009197840586267107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 253440
    },
    {
      "epoch": 0.0009199001929775474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 253472
    },
    {
      "epoch": 0.0009200163273283841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 253504
    },
    {
      "epoch": 0.0009201324616792208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 253536
    },
    {
      "epoch": 0.0009202485960300576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 253568
    },
    {
      "epoch": 0.0009203647303808942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 253600
    },
    {
      "epoch": 0.000920480864731731,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 253632
    },
    {
      "epoch": 0.0009205969990825676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 253664
    },
    {
      "epoch": 0.0009207131334334044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 253696
    },
    {
      "epoch": 0.000920829267784241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 253728
    },
    {
      "epoch": 0.0009209454021350778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 253760
    },
    {
      "epoch": 0.0009210615364859144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 253792
    },
    {
      "epoch": 0.0009211776708367512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 253824
    },
    {
      "epoch": 0.0009212938051875879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 253856
    },
    {
      "epoch": 0.0009214099395384246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 253888
    },
    {
      "epoch": 0.0009215260738892613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 253920
    },
    {
      "epoch": 0.000921642208240098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 253952
    },
    {
      "epoch": 0.0009217583425909347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 253984
    },
    {
      "epoch": 0.0009218744769417714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 254016
    },
    {
      "epoch": 0.0009219906112926081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 254048
    },
    {
      "epoch": 0.0009221067456434448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 254080
    },
    {
      "epoch": 0.0009222228799942815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 254112
    },
    {
      "epoch": 0.0009223390143451183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 254144
    },
    {
      "epoch": 0.0009224551486959549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 254176
    },
    {
      "epoch": 0.0009225712830467917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 254208
    },
    {
      "epoch": 0.0009226874173976283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 254240
    },
    {
      "epoch": 0.0009228035517484651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 254272
    },
    {
      "epoch": 0.0009229196860993017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 254304
    },
    {
      "epoch": 0.0009230358204501385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 254336
    },
    {
      "epoch": 0.0009231519548009751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 254368
    },
    {
      "epoch": 0.0009232680891518119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 254400
    },
    {
      "epoch": 0.0009233842235026486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 254432
    },
    {
      "epoch": 0.0009235003578534853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 254464
    },
    {
      "epoch": 0.000923616492204322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 254496
    },
    {
      "epoch": 0.0009237326265551587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 254528
    },
    {
      "epoch": 0.0009238487609059954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 254560
    },
    {
      "epoch": 0.0009239648952568321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 254592
    },
    {
      "epoch": 0.0009240810296076688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 254624
    },
    {
      "epoch": 0.0009241971639585055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 254656
    },
    {
      "epoch": 0.0009243132983093422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 254688
    },
    {
      "epoch": 0.000924429432660179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 254720
    },
    {
      "epoch": 0.0009245455670110156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6807,
      "step": 254752
    },
    {
      "epoch": 0.0009246617013618524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 254784
    },
    {
      "epoch": 0.000924777835712689,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 254816
    },
    {
      "epoch": 0.0009248939700635258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 254848
    },
    {
      "epoch": 0.0009250101044143624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 254880
    },
    {
      "epoch": 0.0009251262387651992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 254912
    },
    {
      "epoch": 0.0009252423731160358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 254944
    },
    {
      "epoch": 0.0009253585074668726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 254976
    },
    {
      "epoch": 0.0009254746418177094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 255008
    },
    {
      "epoch": 0.000925590776168546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 255040
    },
    {
      "epoch": 0.0009257069105193828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 255072
    },
    {
      "epoch": 0.0009258230448702194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 255104
    },
    {
      "epoch": 0.0009259391792210562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 255136
    },
    {
      "epoch": 0.0009260553135718928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 255168
    },
    {
      "epoch": 0.0009261714479227296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 255200
    },
    {
      "epoch": 0.0009262875822735662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 255232
    },
    {
      "epoch": 0.000926403716624403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 255264
    },
    {
      "epoch": 0.0009265198509752397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 255296
    },
    {
      "epoch": 0.0009266359853260764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.695,
      "step": 255328
    },
    {
      "epoch": 0.0009267521196769131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 255360
    },
    {
      "epoch": 0.0009268682540277498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 255392
    },
    {
      "epoch": 0.0009269843883785865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 255424
    },
    {
      "epoch": 0.0009271005227294232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 255456
    },
    {
      "epoch": 0.0009272166570802599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 255488
    },
    {
      "epoch": 0.0009273327914310966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 255520
    },
    {
      "epoch": 0.0009274489257819333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 255552
    },
    {
      "epoch": 0.0009275650601327701,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 255584
    },
    {
      "epoch": 0.0009276811944836067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6843,
      "step": 255616
    },
    {
      "epoch": 0.0009277973288344435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 255648
    },
    {
      "epoch": 0.0009279134631852801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 255680
    },
    {
      "epoch": 0.0009280295975361169,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 255712
    },
    {
      "epoch": 0.0009281457318869535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 255744
    },
    {
      "epoch": 0.0009282618662377903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 255776
    },
    {
      "epoch": 0.0009283780005886269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7557,
      "step": 255808
    },
    {
      "epoch": 0.0009284941349394637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 255840
    },
    {
      "epoch": 0.0009286102692903004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 255872
    },
    {
      "epoch": 0.0009287264036411371,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 255904
    },
    {
      "epoch": 0.0009288425379919738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 255936
    },
    {
      "epoch": 0.0009289586723428105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 255968
    },
    {
      "epoch": 0.0009290748066936472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 256000
    },
    {
      "epoch": 0.0009291909410444839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 256032
    },
    {
      "epoch": 0.0009293070753953206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 256064
    },
    {
      "epoch": 0.0009294232097461573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 256096
    },
    {
      "epoch": 0.000929539344096994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 256128
    },
    {
      "epoch": 0.0009296554784478308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6829,
      "step": 256160
    },
    {
      "epoch": 0.0009297716127986674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 256192
    },
    {
      "epoch": 0.0009298877471495042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 256224
    },
    {
      "epoch": 0.0009300038815003408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 256256
    },
    {
      "epoch": 0.0009301200158511776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 256288
    },
    {
      "epoch": 0.0009302361502020142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 256320
    },
    {
      "epoch": 0.000930352284552851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 256352
    },
    {
      "epoch": 0.0009304684189036876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 256384
    },
    {
      "epoch": 0.0009305845532545244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7586,
      "step": 256416
    },
    {
      "epoch": 0.0009307006876053611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 256448
    },
    {
      "epoch": 0.0009308168219561978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 256480
    },
    {
      "epoch": 0.0009309329563070345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 256512
    },
    {
      "epoch": 0.0009310490906578712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 256544
    },
    {
      "epoch": 0.0009311652250087079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 256576
    },
    {
      "epoch": 0.0009312813593595446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 256608
    },
    {
      "epoch": 0.0009313974937103813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 256640
    },
    {
      "epoch": 0.000931513628061218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 256672
    },
    {
      "epoch": 0.0009316297624120547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 256704
    },
    {
      "epoch": 0.0009317458967628915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 256736
    },
    {
      "epoch": 0.0009318620311137281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 256768
    },
    {
      "epoch": 0.0009319781654645649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 256800
    },
    {
      "epoch": 0.0009320942998154015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 256832
    },
    {
      "epoch": 0.0009322104341662383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 256864
    },
    {
      "epoch": 0.0009323265685170749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 256896
    },
    {
      "epoch": 0.0009324427028679117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 256928
    },
    {
      "epoch": 0.0009325588372187483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 256960
    },
    {
      "epoch": 0.0009326749715695851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 256992
    },
    {
      "epoch": 0.0009327911059204218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 257024
    },
    {
      "epoch": 0.0009329072402712585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 257056
    },
    {
      "epoch": 0.0009330233746220952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.691,
      "step": 257088
    },
    {
      "epoch": 0.0009331395089729319,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 257120
    },
    {
      "epoch": 0.0009332556433237686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6868,
      "step": 257152
    },
    {
      "epoch": 0.0009333717776746053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 257184
    },
    {
      "epoch": 0.000933487912025442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 257216
    },
    {
      "epoch": 0.0009336040463762787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 257248
    },
    {
      "epoch": 0.0009337201807271154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 257280
    },
    {
      "epoch": 0.0009338363150779522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 257312
    },
    {
      "epoch": 0.0009339524494287888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 257344
    },
    {
      "epoch": 0.0009340685837796256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 257376
    },
    {
      "epoch": 0.0009341847181304622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 257408
    },
    {
      "epoch": 0.000934300852481299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 257440
    },
    {
      "epoch": 0.0009344169868321356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 257472
    },
    {
      "epoch": 0.0009345331211829724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 257504
    },
    {
      "epoch": 0.000934649255533809,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 257536
    },
    {
      "epoch": 0.0009347653898846458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 257568
    },
    {
      "epoch": 0.0009348815242354825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 257600
    },
    {
      "epoch": 0.0009349976585863192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 257632
    },
    {
      "epoch": 0.0009351137929371559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 257664
    },
    {
      "epoch": 0.0009352299272879926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 257696
    },
    {
      "epoch": 0.0009353460616388293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 257728
    },
    {
      "epoch": 0.000935462195989666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7453,
      "step": 257760
    },
    {
      "epoch": 0.0009355783303405027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 257792
    },
    {
      "epoch": 0.0009356944646913394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 257824
    },
    {
      "epoch": 0.0009358105990421761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 257856
    },
    {
      "epoch": 0.0009359267333930129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 257888
    },
    {
      "epoch": 0.0009360428677438495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 257920
    },
    {
      "epoch": 0.0009361590020946863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 257952
    },
    {
      "epoch": 0.0009362751364455229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 257984
    },
    {
      "epoch": 0.0009363912707963597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 258016
    },
    {
      "epoch": 0.0009365074051471963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 258048
    },
    {
      "epoch": 0.0009366235394980331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 258080
    },
    {
      "epoch": 0.0009367396738488697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 258112
    },
    {
      "epoch": 0.0009368558081997065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 258144
    },
    {
      "epoch": 0.0009369719425505432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7641,
      "step": 258176
    },
    {
      "epoch": 0.0009370880769013799,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7581,
      "step": 258208
    },
    {
      "epoch": 0.0009372042112522166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 258240
    },
    {
      "epoch": 0.0009373203456030533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6934,
      "step": 258272
    },
    {
      "epoch": 0.00093743647995389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 258304
    },
    {
      "epoch": 0.0009375526143047267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 258336
    },
    {
      "epoch": 0.0009376687486555634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 258368
    },
    {
      "epoch": 0.0009377848830064001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 258400
    },
    {
      "epoch": 0.0009379010173572368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 258432
    },
    {
      "epoch": 0.0009380171517080736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 258464
    },
    {
      "epoch": 0.0009381332860589102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 258496
    },
    {
      "epoch": 0.000938249420409747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 258528
    },
    {
      "epoch": 0.0009383655547605836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 258560
    },
    {
      "epoch": 0.0009384816891114204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 258592
    },
    {
      "epoch": 0.000938597823462257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7493,
      "step": 258624
    },
    {
      "epoch": 0.0009387139578130938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 258656
    },
    {
      "epoch": 0.0009388300921639304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 258688
    },
    {
      "epoch": 0.0009389462265147672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 258720
    },
    {
      "epoch": 0.000939062360865604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 258752
    },
    {
      "epoch": 0.0009391784952164406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 258784
    },
    {
      "epoch": 0.0009392946295672773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 258816
    },
    {
      "epoch": 0.000939410763918114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 258848
    },
    {
      "epoch": 0.0009395268982689507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 258880
    },
    {
      "epoch": 0.0009396430326197874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 258912
    },
    {
      "epoch": 0.0009397591669706241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 258944
    },
    {
      "epoch": 0.0009398753013214608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 258976
    },
    {
      "epoch": 0.0009399914356722975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 259008
    },
    {
      "epoch": 0.0009401075700231343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 259040
    },
    {
      "epoch": 0.000940223704373971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 259072
    },
    {
      "epoch": 0.0009403398387248077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 259104
    },
    {
      "epoch": 0.0009404559730756443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6867,
      "step": 259136
    },
    {
      "epoch": 0.0009405721074264811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 259168
    },
    {
      "epoch": 0.0009406882417773177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 259200
    },
    {
      "epoch": 0.0009408043761281545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 259232
    },
    {
      "epoch": 0.0009409205104789911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 259264
    },
    {
      "epoch": 0.0009410366448298279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 259296
    },
    {
      "epoch": 0.0009411527791806647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 259328
    },
    {
      "epoch": 0.0009412689135315013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 259360
    },
    {
      "epoch": 0.0009413850478823381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 259392
    },
    {
      "epoch": 0.0009415011822331747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 259424
    },
    {
      "epoch": 0.0009416173165840115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 259456
    },
    {
      "epoch": 0.0009417334509348481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 259488
    },
    {
      "epoch": 0.0009418495852856849,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 259520
    },
    {
      "epoch": 0.0009419657196365215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 259552
    },
    {
      "epoch": 0.0009420818539873583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 259584
    },
    {
      "epoch": 0.000942197988338195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 259616
    },
    {
      "epoch": 0.0009423141226890317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 259648
    },
    {
      "epoch": 0.0009424302570398684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6816,
      "step": 259680
    },
    {
      "epoch": 0.000942546391390705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 259712
    },
    {
      "epoch": 0.0009426625257415418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 259744
    },
    {
      "epoch": 0.0009427786600923785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 259776
    },
    {
      "epoch": 0.0009428947944432152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 259808
    },
    {
      "epoch": 0.0009430109287940519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 259840
    },
    {
      "epoch": 0.0009431270631448886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 259872
    },
    {
      "epoch": 0.0009432431974957254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 259904
    },
    {
      "epoch": 0.000943359331846562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7462,
      "step": 259936
    },
    {
      "epoch": 0.0009434754661973988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 259968
    },
    {
      "epoch": 0.0009435916005482354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 260000
    },
    {
      "epoch": 0.0009437077348990722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 260032
    },
    {
      "epoch": 0.0009438238692499088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 260064
    },
    {
      "epoch": 0.0009439400036007456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 260096
    },
    {
      "epoch": 0.0009440561379515822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 260128
    },
    {
      "epoch": 0.000944172272302419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 260160
    },
    {
      "epoch": 0.0009442884066532557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 260192
    },
    {
      "epoch": 0.0009444045410040924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 260224
    },
    {
      "epoch": 0.0009445206753549291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 260256
    },
    {
      "epoch": 0.0009446368097057658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 260288
    },
    {
      "epoch": 0.0009447529440566025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 260320
    },
    {
      "epoch": 0.0009448690784074392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 260352
    },
    {
      "epoch": 0.0009449852127582759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 260384
    },
    {
      "epoch": 0.0009451013471091126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 260416
    },
    {
      "epoch": 0.0009452174814599493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 260448
    },
    {
      "epoch": 0.0009453336158107861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 260480
    },
    {
      "epoch": 0.0009454497501616227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 260512
    },
    {
      "epoch": 0.0009455658845124595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 260544
    },
    {
      "epoch": 0.0009456820188632961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 260576
    },
    {
      "epoch": 0.0009457981532141329,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 260608
    },
    {
      "epoch": 0.0009459142875649695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 260640
    },
    {
      "epoch": 0.0009460304219158063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 260672
    },
    {
      "epoch": 0.0009461465562666429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 260704
    },
    {
      "epoch": 0.0009462626906174797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 260736
    },
    {
      "epoch": 0.0009463788249683164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 260768
    },
    {
      "epoch": 0.0009464949593191531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 260800
    },
    {
      "epoch": 0.0009466110936699898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 260832
    },
    {
      "epoch": 0.0009467272280208265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 260864
    },
    {
      "epoch": 0.0009468433623716632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6915,
      "step": 260896
    },
    {
      "epoch": 0.0009469594967224999,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 260928
    },
    {
      "epoch": 0.0009470756310733366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 260960
    },
    {
      "epoch": 0.0009471917654241733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 260992
    },
    {
      "epoch": 0.00094730789977501,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 261024
    },
    {
      "epoch": 0.0009474240341258468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 261056
    },
    {
      "epoch": 0.0009475401684766834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 261088
    },
    {
      "epoch": 0.0009476563028275202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 261120
    },
    {
      "epoch": 0.0009477724371783568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 261152
    },
    {
      "epoch": 0.0009478885715291936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 261184
    },
    {
      "epoch": 0.0009480047058800302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 261216
    },
    {
      "epoch": 0.000948120840230867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 261248
    },
    {
      "epoch": 0.0009482369745817036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7548,
      "step": 261280
    },
    {
      "epoch": 0.0009483531089325404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 261312
    },
    {
      "epoch": 0.0009484692432833771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 261344
    },
    {
      "epoch": 0.0009485853776342138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 261376
    },
    {
      "epoch": 0.0009487015119850505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 261408
    },
    {
      "epoch": 0.0009488176463358872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 261440
    },
    {
      "epoch": 0.0009489337806867239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 261472
    },
    {
      "epoch": 0.0009490499150375606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 261504
    },
    {
      "epoch": 0.0009491660493883973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6851,
      "step": 261536
    },
    {
      "epoch": 0.000949282183739234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 261568
    },
    {
      "epoch": 0.0009493983180900707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 261600
    },
    {
      "epoch": 0.0009495144524409075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 261632
    },
    {
      "epoch": 0.0009496305867917441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 261664
    },
    {
      "epoch": 0.0009497467211425809,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 261696
    },
    {
      "epoch": 0.0009498628554934175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 261728
    },
    {
      "epoch": 0.0009499789898442543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6848,
      "step": 261760
    },
    {
      "epoch": 0.0009500951241950909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 261792
    },
    {
      "epoch": 0.0009502112585459277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 261824
    },
    {
      "epoch": 0.0009503273928967643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 261856
    },
    {
      "epoch": 0.0009504435272476011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 261888
    },
    {
      "epoch": 0.0009505596615984378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 261920
    },
    {
      "epoch": 0.0009506757959492745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 261952
    },
    {
      "epoch": 0.0009507919303001112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 261984
    },
    {
      "epoch": 0.0009509080646509479,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 262016
    },
    {
      "epoch": 0.0009510241990017846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 262048
    },
    {
      "epoch": 0.0009511403333526213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 262080
    },
    {
      "epoch": 0.000951256467703458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 262112
    },
    {
      "epoch": 0.0009513726020542947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 262144
    },
    {
      "epoch": 0.0009513726020542947,
      "eval_loss": 2.5859375,
      "eval_runtime": 24820.6511,
      "eval_samples_per_second": 214.709,
      "eval_steps_per_second": 0.419,
      "step": 262144
    },
    {
      "epoch": 0.0009514887364051314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 262176
    },
    {
      "epoch": 0.0009516048707559682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 262208
    },
    {
      "epoch": 0.0009517210051068048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 262240
    },
    {
      "epoch": 0.0009518371394576416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 262272
    },
    {
      "epoch": 0.0009519532738084782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 262304
    },
    {
      "epoch": 0.000952069408159315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 262336
    },
    {
      "epoch": 0.0009521855425101516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6866,
      "step": 262368
    },
    {
      "epoch": 0.0009523016768609884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 262400
    },
    {
      "epoch": 0.000952417811211825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 262432
    },
    {
      "epoch": 0.0009525339455626618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 262464
    },
    {
      "epoch": 0.0009526500799134985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 262496
    },
    {
      "epoch": 0.0009527662142643352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 262528
    },
    {
      "epoch": 0.0009528823486151719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7409,
      "step": 262560
    },
    {
      "epoch": 0.0009529984829660086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 262592
    },
    {
      "epoch": 0.0009531146173168453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 262624
    },
    {
      "epoch": 0.000953230751667682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 262656
    },
    {
      "epoch": 0.0009533468860185187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 262688
    },
    {
      "epoch": 0.0009534630203693554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 262720
    },
    {
      "epoch": 0.0009535791547201921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 262752
    },
    {
      "epoch": 0.0009536952890710289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 262784
    },
    {
      "epoch": 0.0009538114234218655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 262816
    },
    {
      "epoch": 0.0009539275577727023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 262848
    },
    {
      "epoch": 0.0009540436921235389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7689,
      "step": 262880
    },
    {
      "epoch": 0.0009541598264743757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 262912
    },
    {
      "epoch": 0.0009542759608252123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 262944
    },
    {
      "epoch": 0.0009543920951760491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 262976
    },
    {
      "epoch": 0.0009545082295268857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 263008
    },
    {
      "epoch": 0.0009546243638777225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 263040
    },
    {
      "epoch": 0.0009547404982285592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 263072
    },
    {
      "epoch": 0.0009548566325793959,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 263104
    },
    {
      "epoch": 0.0009549727669302326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 263136
    },
    {
      "epoch": 0.0009550889012810693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7587,
      "step": 263168
    },
    {
      "epoch": 0.000955205035631906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 263200
    },
    {
      "epoch": 0.0009553211699827427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 263232
    },
    {
      "epoch": 0.0009554373043335794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 263264
    },
    {
      "epoch": 0.0009555534386844161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 263296
    },
    {
      "epoch": 0.0009556695730352528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 263328
    },
    {
      "epoch": 0.0009557857073860896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 263360
    },
    {
      "epoch": 0.0009559018417369262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 263392
    },
    {
      "epoch": 0.000956017976087763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 263424
    },
    {
      "epoch": 0.0009561341104385996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 263456
    },
    {
      "epoch": 0.0009562502447894364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7615,
      "step": 263488
    },
    {
      "epoch": 0.000956366379140273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 263520
    },
    {
      "epoch": 0.0009564825134911098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 263552
    },
    {
      "epoch": 0.0009565986478419464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 263584
    },
    {
      "epoch": 0.0009567147821927832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 263616
    },
    {
      "epoch": 0.00095683091654362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 263648
    },
    {
      "epoch": 0.0009569470508944566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 263680
    },
    {
      "epoch": 0.0009570631852452934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 263712
    },
    {
      "epoch": 0.00095717931959613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7534,
      "step": 263744
    },
    {
      "epoch": 0.0009572954539469668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7565,
      "step": 263776
    },
    {
      "epoch": 0.0009574115882978034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 263808
    },
    {
      "epoch": 0.0009575277226486402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 263840
    },
    {
      "epoch": 0.0009576438569994768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 263872
    },
    {
      "epoch": 0.0009577599913503136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 263904
    },
    {
      "epoch": 0.0009578761257011502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 263936
    },
    {
      "epoch": 0.000957992260051987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 263968
    },
    {
      "epoch": 0.0009581083944028237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 264000
    },
    {
      "epoch": 0.0009582245287536604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 264032
    },
    {
      "epoch": 0.0009583406631044971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 264064
    },
    {
      "epoch": 0.0009584567974553338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 264096
    },
    {
      "epoch": 0.0009585729318061705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 264128
    },
    {
      "epoch": 0.0009586890661570072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 264160
    },
    {
      "epoch": 0.0009588052005078439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 264192
    },
    {
      "epoch": 0.0009589213348586806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 264224
    },
    {
      "epoch": 0.0009590374692095173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 264256
    },
    {
      "epoch": 0.0009591536035603541,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 264288
    },
    {
      "epoch": 0.0009592697379111907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 264320
    },
    {
      "epoch": 0.0009593858722620275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 264352
    },
    {
      "epoch": 0.0009595020066128641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 264384
    },
    {
      "epoch": 0.0009596181409637009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 264416
    },
    {
      "epoch": 0.0009597342753145375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 264448
    },
    {
      "epoch": 0.0009598504096653743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 264480
    },
    {
      "epoch": 0.0009599665440162109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 264512
    },
    {
      "epoch": 0.0009600826783670477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 264544
    },
    {
      "epoch": 0.0009601988127178844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 264576
    },
    {
      "epoch": 0.0009603149470687211,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 264608
    },
    {
      "epoch": 0.0009604310814195578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 264640
    },
    {
      "epoch": 0.0009605472157703945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 264672
    },
    {
      "epoch": 0.0009606633501212312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 264704
    },
    {
      "epoch": 0.0009607794844720679,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 264736
    },
    {
      "epoch": 0.0009608956188229046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7502,
      "step": 264768
    },
    {
      "epoch": 0.0009610117531737413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 264800
    },
    {
      "epoch": 0.000961127887524578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 264832
    },
    {
      "epoch": 0.0009612440218754148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 264864
    },
    {
      "epoch": 0.0009613601562262514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 264896
    },
    {
      "epoch": 0.0009614762905770882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 264928
    },
    {
      "epoch": 0.0009615924249279248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 264960
    },
    {
      "epoch": 0.0009617085592787616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.693,
      "step": 264992
    },
    {
      "epoch": 0.0009618246936295982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 265024
    },
    {
      "epoch": 0.000961940827980435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 265056
    },
    {
      "epoch": 0.0009620569623312716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 265088
    },
    {
      "epoch": 0.0009621730966821084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 265120
    },
    {
      "epoch": 0.0009622892310329451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 265152
    },
    {
      "epoch": 0.0009624053653837818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 265184
    },
    {
      "epoch": 0.0009625214997346185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 265216
    },
    {
      "epoch": 0.0009626376340854552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 265248
    },
    {
      "epoch": 0.0009627537684362919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 265280
    },
    {
      "epoch": 0.0009628699027871286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 265312
    },
    {
      "epoch": 0.0009629860371379653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 265344
    },
    {
      "epoch": 0.000963102171488802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 265376
    },
    {
      "epoch": 0.0009632183058396387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 265408
    },
    {
      "epoch": 0.0009633344401904755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 265440
    },
    {
      "epoch": 0.0009634505745413121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 265472
    },
    {
      "epoch": 0.0009635667088921489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 265504
    },
    {
      "epoch": 0.0009636828432429855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 265536
    },
    {
      "epoch": 0.0009637989775938223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 265568
    },
    {
      "epoch": 0.0009639151119446589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 265600
    },
    {
      "epoch": 0.0009640312462954957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 265632
    },
    {
      "epoch": 0.0009641473806463323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7677,
      "step": 265664
    },
    {
      "epoch": 0.0009642635149971691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 265696
    },
    {
      "epoch": 0.0009643796493480058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 265728
    },
    {
      "epoch": 0.0009644957836988425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 265760
    },
    {
      "epoch": 0.0009646119180496792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 265792
    },
    {
      "epoch": 0.0009647280524005159,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 265824
    },
    {
      "epoch": 0.0009648441867513526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 265856
    },
    {
      "epoch": 0.0009649603211021893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6937,
      "step": 265888
    },
    {
      "epoch": 0.000965076455453026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 265920
    },
    {
      "epoch": 0.0009651925898038627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 265952
    },
    {
      "epoch": 0.0009653087241546994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 265984
    },
    {
      "epoch": 0.0009654248585055362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 266016
    },
    {
      "epoch": 0.0009655409928563728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 266048
    },
    {
      "epoch": 0.0009656571272072096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 266080
    },
    {
      "epoch": 0.0009657732615580462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.748,
      "step": 266112
    },
    {
      "epoch": 0.000965889395908883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6909,
      "step": 266144
    },
    {
      "epoch": 0.0009660055302597196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 266176
    },
    {
      "epoch": 0.0009661216646105564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 266208
    },
    {
      "epoch": 0.000966237798961393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 266240
    },
    {
      "epoch": 0.0009663539333122298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 266272
    },
    {
      "epoch": 0.0009664700676630665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 266304
    },
    {
      "epoch": 0.0009665862020139032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7554,
      "step": 266336
    },
    {
      "epoch": 0.0009667023363647399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 266368
    },
    {
      "epoch": 0.0009668184707155766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 266400
    },
    {
      "epoch": 0.0009669346050664133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 266432
    },
    {
      "epoch": 0.00096705073941725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 266464
    },
    {
      "epoch": 0.0009671668737680867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 266496
    },
    {
      "epoch": 0.0009672830081189234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 266528
    },
    {
      "epoch": 0.0009673991424697601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 266560
    },
    {
      "epoch": 0.0009675152768205969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 266592
    },
    {
      "epoch": 0.0009676314111714335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 266624
    },
    {
      "epoch": 0.0009677475455222703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 266656
    },
    {
      "epoch": 0.0009678636798731069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 266688
    },
    {
      "epoch": 0.0009679798142239437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 266720
    },
    {
      "epoch": 0.0009680959485747803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 266752
    },
    {
      "epoch": 0.0009682120829256171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 266784
    },
    {
      "epoch": 0.0009683282172764537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 266816
    },
    {
      "epoch": 0.0009684443516272905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 266848
    },
    {
      "epoch": 0.0009685604859781272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 266880
    },
    {
      "epoch": 0.0009686766203289639,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 266912
    },
    {
      "epoch": 0.0009687927546798006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 266944
    },
    {
      "epoch": 0.0009689088890306373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 266976
    },
    {
      "epoch": 0.000969025023381474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 267008
    },
    {
      "epoch": 0.0009691411577323107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 267040
    },
    {
      "epoch": 0.0009692572920831474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 267072
    },
    {
      "epoch": 0.0009693734264339841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 267104
    },
    {
      "epoch": 0.0009694895607848208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 267136
    },
    {
      "epoch": 0.0009696056951356576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 267168
    },
    {
      "epoch": 0.0009697218294864942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 267200
    },
    {
      "epoch": 0.000969837963837331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 267232
    },
    {
      "epoch": 0.0009699540981881676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7577,
      "step": 267264
    },
    {
      "epoch": 0.0009700702325390044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 267296
    },
    {
      "epoch": 0.000970186366889841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 267328
    },
    {
      "epoch": 0.0009703025012406778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 267360
    },
    {
      "epoch": 0.0009704186355915144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 267392
    },
    {
      "epoch": 0.0009705347699423512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 267424
    },
    {
      "epoch": 0.000970650904293188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 267456
    },
    {
      "epoch": 0.0009707670386440246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 267488
    },
    {
      "epoch": 0.0009708831729948614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 267520
    },
    {
      "epoch": 0.000970999307345698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 267552
    },
    {
      "epoch": 0.0009711154416965348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 267584
    },
    {
      "epoch": 0.0009712315760473714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 267616
    },
    {
      "epoch": 0.0009713477103982082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 267648
    },
    {
      "epoch": 0.0009714638447490448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 267680
    },
    {
      "epoch": 0.0009715799790998815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 267712
    },
    {
      "epoch": 0.0009716961134507183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 267744
    },
    {
      "epoch": 0.000971812247801555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 267776
    },
    {
      "epoch": 0.0009719283821523917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7644,
      "step": 267808
    },
    {
      "epoch": 0.0009720445165032283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 267840
    },
    {
      "epoch": 0.0009721606508540651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7473,
      "step": 267872
    },
    {
      "epoch": 0.0009722767852049017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 267904
    },
    {
      "epoch": 0.0009723929195557385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 267936
    },
    {
      "epoch": 0.0009725090539065751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 267968
    },
    {
      "epoch": 0.0009726251882574119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 268000
    },
    {
      "epoch": 0.0009727413226082487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 268032
    },
    {
      "epoch": 0.0009728574569590853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 268064
    },
    {
      "epoch": 0.0009729735913099221,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 268096
    },
    {
      "epoch": 0.0009730897256607587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 268128
    },
    {
      "epoch": 0.0009732058600115955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 268160
    },
    {
      "epoch": 0.0009733219943624321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7457,
      "step": 268192
    },
    {
      "epoch": 0.0009734381287132689,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 268224
    },
    {
      "epoch": 0.0009735542630641055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 268256
    },
    {
      "epoch": 0.0009736703974149423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7544,
      "step": 268288
    },
    {
      "epoch": 0.000973786531765779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 268320
    },
    {
      "epoch": 0.0009739026661166157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 268352
    },
    {
      "epoch": 0.0009740188004674524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 268384
    },
    {
      "epoch": 0.0009741349348182891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 268416
    },
    {
      "epoch": 0.0009742510691691258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 268448
    },
    {
      "epoch": 0.0009743672035199625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 268480
    },
    {
      "epoch": 0.0009744833378707992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 268512
    },
    {
      "epoch": 0.0009745994722216359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 268544
    },
    {
      "epoch": 0.0009747156065724726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 268576
    },
    {
      "epoch": 0.0009748317409233094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 268608
    },
    {
      "epoch": 0.000974947875274146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 268640
    },
    {
      "epoch": 0.0009750640096249828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 268672
    },
    {
      "epoch": 0.0009751801439758194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.752,
      "step": 268704
    },
    {
      "epoch": 0.0009752962783266562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 268736
    },
    {
      "epoch": 0.0009754124126774928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 268768
    },
    {
      "epoch": 0.0009755285470283296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 268800
    },
    {
      "epoch": 0.0009756446813791662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 268832
    },
    {
      "epoch": 0.000975760815730003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 268864
    },
    {
      "epoch": 0.0009758769500808397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 268896
    },
    {
      "epoch": 0.0009759930844316764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 268928
    },
    {
      "epoch": 0.0009761092187825131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 268960
    },
    {
      "epoch": 0.0009762253531333498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 268992
    },
    {
      "epoch": 0.0009763414874841865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 269024
    },
    {
      "epoch": 0.0009764576218350232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 269056
    },
    {
      "epoch": 0.00097657375618586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 269088
    },
    {
      "epoch": 0.0009766898905366967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 269120
    },
    {
      "epoch": 0.0009768060248875334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 269152
    },
    {
      "epoch": 0.00097692215923837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 269184
    },
    {
      "epoch": 0.0009770382935892067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 269216
    },
    {
      "epoch": 0.0009771544279400435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 269248
    },
    {
      "epoch": 0.0009772705622908802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 269280
    },
    {
      "epoch": 0.0009773866966417168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 269312
    },
    {
      "epoch": 0.0009775028309925535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 269344
    },
    {
      "epoch": 0.0009776189653433903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 269376
    },
    {
      "epoch": 0.000977735099694227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 269408
    },
    {
      "epoch": 0.0009778512340450636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 269440
    },
    {
      "epoch": 0.0009779673683959003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 269472
    },
    {
      "epoch": 0.000978083502746737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 269504
    },
    {
      "epoch": 0.0009781996370975738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 269536
    },
    {
      "epoch": 0.0009783157714484106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 269568
    },
    {
      "epoch": 0.0009784319057992471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 269600
    },
    {
      "epoch": 0.0009785480401500839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 269632
    },
    {
      "epoch": 0.0009786641745009206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 269664
    },
    {
      "epoch": 0.0009787803088517574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 269696
    },
    {
      "epoch": 0.000978896443202594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 269728
    },
    {
      "epoch": 0.0009790125775534307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 269760
    },
    {
      "epoch": 0.0009791287119042674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 269792
    },
    {
      "epoch": 0.0009792448462551042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 269824
    },
    {
      "epoch": 0.000979360980605941,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 269856
    },
    {
      "epoch": 0.0009794771149567775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 269888
    },
    {
      "epoch": 0.0009795932493076142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 269920
    },
    {
      "epoch": 0.000979709383658451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 269952
    },
    {
      "epoch": 0.0009798255180092877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 269984
    },
    {
      "epoch": 0.0009799416523601243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 270016
    },
    {
      "epoch": 0.000980057786710961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 270048
    },
    {
      "epoch": 0.0009801739210617978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 270080
    },
    {
      "epoch": 0.0009802900554126345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 270112
    },
    {
      "epoch": 0.0009804061897634713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 270144
    },
    {
      "epoch": 0.0009805223241143078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 270176
    },
    {
      "epoch": 0.0009806384584651446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 270208
    },
    {
      "epoch": 0.0009807545928159813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 270240
    },
    {
      "epoch": 0.000980870727166818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6919,
      "step": 270272
    },
    {
      "epoch": 0.0009809868615176546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 270304
    },
    {
      "epoch": 0.0009811029958684914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 270336
    },
    {
      "epoch": 0.0009812191302193281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 270368
    },
    {
      "epoch": 0.0009813352645701649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 270400
    },
    {
      "epoch": 0.0009814513989210016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 270432
    },
    {
      "epoch": 0.0009815675332718382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 270464
    },
    {
      "epoch": 0.000981683667622675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 270496
    },
    {
      "epoch": 0.0009817998019735117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 270528
    },
    {
      "epoch": 0.0009819159363243484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 270560
    },
    {
      "epoch": 0.000982032070675185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 270592
    },
    {
      "epoch": 0.0009821482050260217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 270624
    },
    {
      "epoch": 0.0009822643393768585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 270656
    },
    {
      "epoch": 0.0009823804737276952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 270688
    },
    {
      "epoch": 0.000982496608078532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 270720
    },
    {
      "epoch": 0.0009826127424293685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 270752
    },
    {
      "epoch": 0.0009827288767802053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 270784
    },
    {
      "epoch": 0.000982845011131042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 270816
    },
    {
      "epoch": 0.0009829611454818788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 270848
    },
    {
      "epoch": 0.0009830772798327153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 270880
    },
    {
      "epoch": 0.000983193414183552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 270912
    },
    {
      "epoch": 0.0009833095485343888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 270944
    },
    {
      "epoch": 0.0009834256828852256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 270976
    },
    {
      "epoch": 0.0009835418172360623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 271008
    },
    {
      "epoch": 0.0009836579515868989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 271040
    },
    {
      "epoch": 0.0009837740859377356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 271072
    },
    {
      "epoch": 0.0009838902202885724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 271104
    },
    {
      "epoch": 0.0009840063546394091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7496,
      "step": 271136
    },
    {
      "epoch": 0.0009841224889902457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 271168
    },
    {
      "epoch": 0.0009842386233410824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 271200
    },
    {
      "epoch": 0.0009843547576919192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 271232
    },
    {
      "epoch": 0.000984470892042756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 271264
    },
    {
      "epoch": 0.0009845870263935927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 271296
    },
    {
      "epoch": 0.0009847031607444292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7515,
      "step": 271328
    },
    {
      "epoch": 0.000984819295095266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 271360
    },
    {
      "epoch": 0.0009849354294461027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 271392
    },
    {
      "epoch": 0.0009850515637969395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 271424
    },
    {
      "epoch": 0.000985167698147776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 271456
    },
    {
      "epoch": 0.0009852838324986128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 271488
    },
    {
      "epoch": 0.0009853999668494495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 271520
    },
    {
      "epoch": 0.0009855161012002863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 271552
    },
    {
      "epoch": 0.000985632235551123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 271584
    },
    {
      "epoch": 0.0009857483699019596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 271616
    },
    {
      "epoch": 0.0009858645042527963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 271648
    },
    {
      "epoch": 0.000985980638603633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 271680
    },
    {
      "epoch": 0.0009860967729544699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 271712
    },
    {
      "epoch": 0.0009862129073053064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 271744
    },
    {
      "epoch": 0.0009863290416561431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 271776
    },
    {
      "epoch": 0.00098644517600698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 271808
    },
    {
      "epoch": 0.0009865613103578167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 271840
    },
    {
      "epoch": 0.0009866774447086534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 271872
    },
    {
      "epoch": 0.00098679357905949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 271904
    },
    {
      "epoch": 0.0009869097134103267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 271936
    },
    {
      "epoch": 0.0009870258477611635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 271968
    },
    {
      "epoch": 0.0009871419821120002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 272000
    },
    {
      "epoch": 0.0009872581164628367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 272032
    },
    {
      "epoch": 0.0009873742508136735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 272064
    },
    {
      "epoch": 0.0009874903851645103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 272096
    },
    {
      "epoch": 0.000987606519515347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 272128
    },
    {
      "epoch": 0.0009877226538661838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 272160
    },
    {
      "epoch": 0.0009878387882170203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 272192
    },
    {
      "epoch": 0.000987954922567857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 272224
    },
    {
      "epoch": 0.0009880710569186938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 272256
    },
    {
      "epoch": 0.0009881871912695306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 272288
    },
    {
      "epoch": 0.000988303325620367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 272320
    },
    {
      "epoch": 0.0009884194599712039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 272352
    },
    {
      "epoch": 0.0009885355943220406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 272384
    },
    {
      "epoch": 0.0009886517286728774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 272416
    },
    {
      "epoch": 0.0009887678630237141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 272448
    },
    {
      "epoch": 0.0009888839973745506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 272480
    },
    {
      "epoch": 0.0009890001317253874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 272512
    },
    {
      "epoch": 0.0009891162660762242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 272544
    },
    {
      "epoch": 0.000989232400427061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 272576
    },
    {
      "epoch": 0.0009893485347778974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 272608
    },
    {
      "epoch": 0.0009894646691287342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 272640
    },
    {
      "epoch": 0.000989580803479571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 272672
    },
    {
      "epoch": 0.0009896969378304077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 272704
    },
    {
      "epoch": 0.0009898130721812445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7561,
      "step": 272736
    },
    {
      "epoch": 0.000989929206532081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 272768
    },
    {
      "epoch": 0.0009900453408829178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 272800
    },
    {
      "epoch": 0.0009901614752337545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 272832
    },
    {
      "epoch": 0.0009902776095845913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 272864
    },
    {
      "epoch": 0.0009903937439354278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 272896
    },
    {
      "epoch": 0.0009905098782862646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 272928
    },
    {
      "epoch": 0.0009906260126371013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 272960
    },
    {
      "epoch": 0.000990742146987938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 272992
    },
    {
      "epoch": 0.0009908582813387748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 273024
    },
    {
      "epoch": 0.0009909744156896114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 273056
    },
    {
      "epoch": 0.0009910905500404481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 273088
    },
    {
      "epoch": 0.0009912066843912849,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 273120
    },
    {
      "epoch": 0.0009913228187421216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 273152
    },
    {
      "epoch": 0.0009914389530929582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 273184
    },
    {
      "epoch": 0.000991555087443795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 273216
    },
    {
      "epoch": 0.0009916712217946317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 273248
    },
    {
      "epoch": 0.0009917873561454684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 273280
    },
    {
      "epoch": 0.0009919034904963052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 273312
    },
    {
      "epoch": 0.0009920196248471417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7636,
      "step": 273344
    },
    {
      "epoch": 0.0009921357591979785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 273376
    },
    {
      "epoch": 0.0009922518935488152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 273408
    },
    {
      "epoch": 0.000992368027899652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 273440
    },
    {
      "epoch": 0.0009924841622504885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 273472
    },
    {
      "epoch": 0.0009926002966013253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 273504
    },
    {
      "epoch": 0.000992716430952162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7523,
      "step": 273536
    },
    {
      "epoch": 0.0009928325653029988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 273568
    },
    {
      "epoch": 0.0009929486996538355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 273600
    },
    {
      "epoch": 0.000993064834004672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7397,
      "step": 273632
    },
    {
      "epoch": 0.0009931809683555088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 273664
    },
    {
      "epoch": 0.0009932971027063456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 273696
    },
    {
      "epoch": 0.0009934132370571823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 273728
    },
    {
      "epoch": 0.0009935293714080189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 273760
    },
    {
      "epoch": 0.0009936455057588556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 273792
    },
    {
      "epoch": 0.0009937616401096924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 273824
    },
    {
      "epoch": 0.0009938777744605291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 273856
    },
    {
      "epoch": 0.0009939939088113659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 273888
    },
    {
      "epoch": 0.0009941100431622024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 273920
    },
    {
      "epoch": 0.0009942261775130392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 273952
    },
    {
      "epoch": 0.000994342311863876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 273984
    },
    {
      "epoch": 0.0009944584462147127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.691,
      "step": 274016
    },
    {
      "epoch": 0.0009945745805655492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 274048
    },
    {
      "epoch": 0.000994690714916386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 274080
    },
    {
      "epoch": 0.0009948068492672227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 274112
    },
    {
      "epoch": 0.0009949229836180595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 274144
    },
    {
      "epoch": 0.0009950391179688962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 274176
    },
    {
      "epoch": 0.0009951552523197328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 274208
    },
    {
      "epoch": 0.0009952713866705695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 274240
    },
    {
      "epoch": 0.0009953875210214063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.743,
      "step": 274272
    },
    {
      "epoch": 0.000995503655372243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 274304
    },
    {
      "epoch": 0.0009956197897230796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 274336
    },
    {
      "epoch": 0.0009957359240739163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 274368
    },
    {
      "epoch": 0.000995852058424753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 274400
    },
    {
      "epoch": 0.0009959681927755898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 274432
    },
    {
      "epoch": 0.0009960843271264266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 274464
    },
    {
      "epoch": 0.0009962004614772631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 274496
    },
    {
      "epoch": 0.0009963165958280999,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 274528
    },
    {
      "epoch": 0.0009964327301789366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 274560
    },
    {
      "epoch": 0.0009965488645297734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 274592
    },
    {
      "epoch": 0.00099666499888061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 274624
    },
    {
      "epoch": 0.0009967811332314467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 274656
    },
    {
      "epoch": 0.0009968972675822834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 274688
    },
    {
      "epoch": 0.0009970134019331202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 274720
    },
    {
      "epoch": 0.000997129536283957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 274752
    },
    {
      "epoch": 0.0009972456706347935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 274784
    },
    {
      "epoch": 0.0009973618049856302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 274816
    },
    {
      "epoch": 0.000997477939336467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 274848
    },
    {
      "epoch": 0.0009975940736873037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 274880
    },
    {
      "epoch": 0.0009977102080381403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 274912
    },
    {
      "epoch": 0.000997826342388977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 274944
    },
    {
      "epoch": 0.0009979424767398138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 274976
    },
    {
      "epoch": 0.0009980586110906505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 275008
    },
    {
      "epoch": 0.0009981747454414873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 275040
    },
    {
      "epoch": 0.0009982908797923238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 275072
    },
    {
      "epoch": 0.0009984070141431606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7472,
      "step": 275104
    },
    {
      "epoch": 0.0009985231484939973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 275136
    },
    {
      "epoch": 0.000998639282844834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 275168
    },
    {
      "epoch": 0.0009987554171956706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 275200
    },
    {
      "epoch": 0.0009988715515465074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7532,
      "step": 275232
    },
    {
      "epoch": 0.0009989876858973441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7409,
      "step": 275264
    },
    {
      "epoch": 0.000999103820248181,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7495,
      "step": 275296
    },
    {
      "epoch": 0.0009992199545990176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 275328
    },
    {
      "epoch": 0.0009993360889498542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 275360
    },
    {
      "epoch": 0.000999452223300691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 275392
    },
    {
      "epoch": 0.0009995683576515277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 275424
    },
    {
      "epoch": 0.0009996844920023644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 275456
    },
    {
      "epoch": 0.000999800626353201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 275488
    },
    {
      "epoch": 0.0009999167607040377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7483,
      "step": 275520
    },
    {
      "epoch": 0.0010000328950548745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 275552
    },
    {
      "epoch": 0.0010001490294057112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 275584
    },
    {
      "epoch": 0.001000265163756548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 275616
    },
    {
      "epoch": 0.0010003812981073845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 275648
    },
    {
      "epoch": 0.0010004974324582213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 275680
    },
    {
      "epoch": 0.001000613566809058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 275712
    },
    {
      "epoch": 0.0010007297011598948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 275744
    },
    {
      "epoch": 0.0010008458355107313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 275776
    },
    {
      "epoch": 0.001000961969861568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 275808
    },
    {
      "epoch": 0.0010010781042124048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 275840
    },
    {
      "epoch": 0.0010011942385632416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 275872
    },
    {
      "epoch": 0.0010013103729140784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 275904
    },
    {
      "epoch": 0.0010014265072649149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 275936
    },
    {
      "epoch": 0.0010015426416157516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 275968
    },
    {
      "epoch": 0.0010016587759665884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 276000
    },
    {
      "epoch": 0.0010017749103174252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 276032
    },
    {
      "epoch": 0.0010018910446682617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 276064
    },
    {
      "epoch": 0.0010020071790190984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 276096
    },
    {
      "epoch": 0.0010021233133699352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.749,
      "step": 276128
    },
    {
      "epoch": 0.001002239447720772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 276160
    },
    {
      "epoch": 0.0010023555820716087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7447,
      "step": 276192
    },
    {
      "epoch": 0.0010024717164224452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 276224
    },
    {
      "epoch": 0.001002587850773282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 276256
    },
    {
      "epoch": 0.0010027039851241188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 276288
    },
    {
      "epoch": 0.0010028201194749555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 276320
    },
    {
      "epoch": 0.001002936253825792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 276352
    },
    {
      "epoch": 0.0010030523881766288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 276384
    },
    {
      "epoch": 0.0010031685225274656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 276416
    },
    {
      "epoch": 0.0010032846568783023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 276448
    },
    {
      "epoch": 0.001003400791229139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 276480
    },
    {
      "epoch": 0.0010035169255799756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 276512
    },
    {
      "epoch": 0.0010036330599308124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 276544
    },
    {
      "epoch": 0.001003749194281649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 276576
    },
    {
      "epoch": 0.0010038653286324859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 276608
    },
    {
      "epoch": 0.0010039814629833224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 276640
    },
    {
      "epoch": 0.0010040975973341592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 276672
    },
    {
      "epoch": 0.001004213731684996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 276704
    },
    {
      "epoch": 0.0010043298660358327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 276736
    },
    {
      "epoch": 0.0010044460003866694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 276768
    },
    {
      "epoch": 0.001004562134737506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 276800
    },
    {
      "epoch": 0.0010046782690883427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 276832
    },
    {
      "epoch": 0.0010047944034391795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 276864
    },
    {
      "epoch": 0.0010049105377900162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 276896
    },
    {
      "epoch": 0.0010050266721408528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 276928
    },
    {
      "epoch": 0.0010051428064916895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 276960
    },
    {
      "epoch": 0.0010052589408425263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 276992
    },
    {
      "epoch": 0.001005375075193363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7531,
      "step": 277024
    },
    {
      "epoch": 0.0010054912095441998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 277056
    },
    {
      "epoch": 0.0010056073438950363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 277088
    },
    {
      "epoch": 0.001005723478245873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 277120
    },
    {
      "epoch": 0.0010058396125967098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6876,
      "step": 277152
    },
    {
      "epoch": 0.0010059557469475466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 277184
    },
    {
      "epoch": 0.001006071881298383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 277216
    },
    {
      "epoch": 0.0010061880156492199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 277248
    },
    {
      "epoch": 0.0010063041500000566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 277280
    },
    {
      "epoch": 0.0010064202843508934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 277312
    },
    {
      "epoch": 0.0010065364187017301,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 277344
    },
    {
      "epoch": 0.0010066525530525667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 277376
    },
    {
      "epoch": 0.0010067686874034034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 277408
    },
    {
      "epoch": 0.0010068848217542402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 277440
    },
    {
      "epoch": 0.001007000956105077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7536,
      "step": 277472
    },
    {
      "epoch": 0.0010071170904559135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 277504
    },
    {
      "epoch": 0.0010072332248067502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 277536
    },
    {
      "epoch": 0.001007349359157587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 277568
    },
    {
      "epoch": 0.0010074654935084237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 277600
    },
    {
      "epoch": 0.0010075816278592605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 277632
    },
    {
      "epoch": 0.001007697762210097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 277664
    },
    {
      "epoch": 0.0010078138965609338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 277696
    },
    {
      "epoch": 0.0010079300309117705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 277728
    },
    {
      "epoch": 0.0010080461652626073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 277760
    },
    {
      "epoch": 0.0010081622996134438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 277792
    },
    {
      "epoch": 0.0010082784339642806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 277824
    },
    {
      "epoch": 0.0010083945683151173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 277856
    },
    {
      "epoch": 0.001008510702665954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7549,
      "step": 277888
    },
    {
      "epoch": 0.0010086268370167908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7577,
      "step": 277920
    },
    {
      "epoch": 0.0010087429713676274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7608,
      "step": 277952
    },
    {
      "epoch": 0.0010088591057184641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 277984
    },
    {
      "epoch": 0.0010089752400693009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 278016
    },
    {
      "epoch": 0.0010090913744201376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 278048
    },
    {
      "epoch": 0.0010092075087709742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 278080
    },
    {
      "epoch": 0.001009323643121811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 278112
    },
    {
      "epoch": 0.0010094397774726477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 278144
    },
    {
      "epoch": 0.0010095559118234844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 278176
    },
    {
      "epoch": 0.0010096720461743212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 278208
    },
    {
      "epoch": 0.0010097881805251577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 278240
    },
    {
      "epoch": 0.0010099043148759945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 278272
    },
    {
      "epoch": 0.0010100204492268312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 278304
    },
    {
      "epoch": 0.001010136583577668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 278336
    },
    {
      "epoch": 0.0010102527179285045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7496,
      "step": 278368
    },
    {
      "epoch": 0.0010103688522793413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 278400
    },
    {
      "epoch": 0.001010484986630178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 278432
    },
    {
      "epoch": 0.0010106011209810148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 278464
    },
    {
      "epoch": 0.0010107172553318515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 278496
    },
    {
      "epoch": 0.001010833389682688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 278528
    },
    {
      "epoch": 0.0010109495240335248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7547,
      "step": 278560
    },
    {
      "epoch": 0.0010110656583843616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 278592
    },
    {
      "epoch": 0.0010111817927351983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 278624
    },
    {
      "epoch": 0.0010112979270860349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 278656
    },
    {
      "epoch": 0.0010114140614368716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 278688
    },
    {
      "epoch": 0.0010115301957877084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 278720
    },
    {
      "epoch": 0.0010116463301385451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 278752
    },
    {
      "epoch": 0.0010117624644893819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 278784
    },
    {
      "epoch": 0.0010118785988402184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 278816
    },
    {
      "epoch": 0.0010119947331910552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 278848
    },
    {
      "epoch": 0.001012110867541892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 278880
    },
    {
      "epoch": 0.0010122270018927287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 278912
    },
    {
      "epoch": 0.0010123431362435652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 278944
    },
    {
      "epoch": 0.001012459270594402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 278976
    },
    {
      "epoch": 0.0010125754049452387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 279008
    },
    {
      "epoch": 0.0010126915392960755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 279040
    },
    {
      "epoch": 0.0010128076736469122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7432,
      "step": 279072
    },
    {
      "epoch": 0.0010129238079977488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 279104
    },
    {
      "epoch": 0.0010130399423485855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 279136
    },
    {
      "epoch": 0.0010131560766994223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 279168
    },
    {
      "epoch": 0.001013272211050259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 279200
    },
    {
      "epoch": 0.0010133883454010956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 279232
    },
    {
      "epoch": 0.0010135044797519323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 279264
    },
    {
      "epoch": 0.001013620614102769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 279296
    },
    {
      "epoch": 0.0010137367484536058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 279328
    },
    {
      "epoch": 0.0010138528828044426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 279360
    },
    {
      "epoch": 0.0010139690171552791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 279392
    },
    {
      "epoch": 0.0010140851515061159,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 279424
    },
    {
      "epoch": 0.0010142012858569526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 279456
    },
    {
      "epoch": 0.0010143174202077894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 279488
    },
    {
      "epoch": 0.001014433554558626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 279520
    },
    {
      "epoch": 0.0010145496889094627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 279552
    },
    {
      "epoch": 0.0010146658232602994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 279584
    },
    {
      "epoch": 0.0010147819576111362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 279616
    },
    {
      "epoch": 0.001014898091961973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 279648
    },
    {
      "epoch": 0.0010150142263128095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 279680
    },
    {
      "epoch": 0.0010151303606636462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 279712
    },
    {
      "epoch": 0.001015246495014483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 279744
    },
    {
      "epoch": 0.0010153626293653197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 279776
    },
    {
      "epoch": 0.0010154787637161563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 279808
    },
    {
      "epoch": 0.001015594898066993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 279840
    },
    {
      "epoch": 0.0010157110324178298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 279872
    },
    {
      "epoch": 0.0010158271667686665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 279904
    },
    {
      "epoch": 0.0010159433011195033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 279936
    },
    {
      "epoch": 0.0010160594354703398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 279968
    },
    {
      "epoch": 0.0010161755698211766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 280000
    },
    {
      "epoch": 0.0010162917041720133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 280032
    },
    {
      "epoch": 0.00101640783852285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 280064
    },
    {
      "epoch": 0.0010165239728736866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 280096
    },
    {
      "epoch": 0.0010166401072245234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 280128
    },
    {
      "epoch": 0.0010167562415753601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 280160
    },
    {
      "epoch": 0.001016872375926197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 280192
    },
    {
      "epoch": 0.0010169885102770337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 280224
    },
    {
      "epoch": 0.0010171046446278702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 280256
    },
    {
      "epoch": 0.001017220778978707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 280288
    },
    {
      "epoch": 0.0010173369133295437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 280320
    },
    {
      "epoch": 0.0010174530476803805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 280352
    },
    {
      "epoch": 0.001017569182031217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 280384
    },
    {
      "epoch": 0.0010176853163820537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 280416
    },
    {
      "epoch": 0.0010178014507328905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7484,
      "step": 280448
    },
    {
      "epoch": 0.0010179175850837273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 280480
    },
    {
      "epoch": 0.001018033719434564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 280512
    },
    {
      "epoch": 0.0010181498537854005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.756,
      "step": 280544
    },
    {
      "epoch": 0.0010182659881362373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 280576
    },
    {
      "epoch": 0.001018382122487074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 280608
    },
    {
      "epoch": 0.0010184982568379108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 280640
    },
    {
      "epoch": 0.0010186143911887473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 280672
    },
    {
      "epoch": 0.001018730525539584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 280704
    },
    {
      "epoch": 0.0010188466598904209,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 280736
    },
    {
      "epoch": 0.0010189627942412576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 280768
    },
    {
      "epoch": 0.0010190789285920944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 280800
    },
    {
      "epoch": 0.001019195062942931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 280832
    },
    {
      "epoch": 0.0010193111972937677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 280864
    },
    {
      "epoch": 0.0010194273316446044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 280896
    },
    {
      "epoch": 0.0010195434659954412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 280928
    },
    {
      "epoch": 0.0010196596003462777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 280960
    },
    {
      "epoch": 0.0010197757346971145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 280992
    },
    {
      "epoch": 0.0010198918690479512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 281024
    },
    {
      "epoch": 0.001020008003398788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 281056
    },
    {
      "epoch": 0.0010201241377496247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 281088
    },
    {
      "epoch": 0.0010202402721004613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 281120
    },
    {
      "epoch": 0.001020356406451298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7588,
      "step": 281152
    },
    {
      "epoch": 0.0010204725408021348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 281184
    },
    {
      "epoch": 0.0010205886751529715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 281216
    },
    {
      "epoch": 0.001020704809503808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 281248
    },
    {
      "epoch": 0.0010208209438546448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 281280
    },
    {
      "epoch": 0.0010209370782054816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 281312
    },
    {
      "epoch": 0.0010210532125563183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 281344
    },
    {
      "epoch": 0.001021169346907155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 281376
    },
    {
      "epoch": 0.0010212854812579916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 281408
    },
    {
      "epoch": 0.0010214016156088284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 281440
    },
    {
      "epoch": 0.0010215177499596651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 281472
    },
    {
      "epoch": 0.0010216338843105019,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 281504
    },
    {
      "epoch": 0.0010217500186613384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 281536
    },
    {
      "epoch": 0.0010218661530121752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 281568
    },
    {
      "epoch": 0.001021982287363012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 281600
    },
    {
      "epoch": 0.0010220984217138487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 281632
    },
    {
      "epoch": 0.0010222145560646854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 281664
    },
    {
      "epoch": 0.001022330690415522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 281696
    },
    {
      "epoch": 0.0010224468247663587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 281728
    },
    {
      "epoch": 0.0010225629591171955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 281760
    },
    {
      "epoch": 0.0010226790934680322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 281792
    },
    {
      "epoch": 0.0010227952278188688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 281824
    },
    {
      "epoch": 0.0010229113621697055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 281856
    },
    {
      "epoch": 0.0010230274965205423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 281888
    },
    {
      "epoch": 0.001023143630871379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 281920
    },
    {
      "epoch": 0.0010232597652222158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 281952
    },
    {
      "epoch": 0.0010233758995730523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 281984
    },
    {
      "epoch": 0.001023492033923889,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 282016
    },
    {
      "epoch": 0.0010236081682747258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 282048
    },
    {
      "epoch": 0.0010237243026255626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 282080
    },
    {
      "epoch": 0.0010238404369763991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 282112
    },
    {
      "epoch": 0.0010239565713272359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 282144
    },
    {
      "epoch": 0.0010240727056780726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 282176
    },
    {
      "epoch": 0.0010241888400289094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 282208
    },
    {
      "epoch": 0.0010243049743797461,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 282240
    },
    {
      "epoch": 0.0010244211087305827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 282272
    },
    {
      "epoch": 0.0010245372430814194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7583,
      "step": 282304
    },
    {
      "epoch": 0.0010246533774322562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7456,
      "step": 282336
    },
    {
      "epoch": 0.001024769511783093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 282368
    },
    {
      "epoch": 0.0010248856461339295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 282400
    },
    {
      "epoch": 0.0010250017804847662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 282432
    },
    {
      "epoch": 0.001025117914835603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 282464
    },
    {
      "epoch": 0.0010252340491864397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 282496
    },
    {
      "epoch": 0.0010253501835372765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 282528
    },
    {
      "epoch": 0.001025466317888113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 282560
    },
    {
      "epoch": 0.0010255824522389498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7673,
      "step": 282592
    },
    {
      "epoch": 0.0010256985865897865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 282624
    },
    {
      "epoch": 0.0010258147209406233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 282656
    },
    {
      "epoch": 0.0010259308552914598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 282688
    },
    {
      "epoch": 0.0010260469896422966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 282720
    },
    {
      "epoch": 0.0010261631239931333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 282752
    },
    {
      "epoch": 0.00102627925834397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 282784
    },
    {
      "epoch": 0.0010263953926948068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 282816
    },
    {
      "epoch": 0.0010265115270456434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 282848
    },
    {
      "epoch": 0.0010266276613964801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7577,
      "step": 282880
    },
    {
      "epoch": 0.0010267437957473169,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 282912
    },
    {
      "epoch": 0.0010268599300981536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 282944
    },
    {
      "epoch": 0.0010269760644489902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 282976
    },
    {
      "epoch": 0.001027092198799827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 283008
    },
    {
      "epoch": 0.0010272083331506637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 283040
    },
    {
      "epoch": 0.0010273244675015004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 283072
    },
    {
      "epoch": 0.0010274406018523372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 283104
    },
    {
      "epoch": 0.0010275567362031737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 283136
    },
    {
      "epoch": 0.0010276728705540105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 283168
    },
    {
      "epoch": 0.0010277890049048472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7554,
      "step": 283200
    },
    {
      "epoch": 0.001027905139255684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 283232
    },
    {
      "epoch": 0.0010280212736065205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 283264
    },
    {
      "epoch": 0.0010281374079573573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 283296
    },
    {
      "epoch": 0.001028253542308194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 283328
    },
    {
      "epoch": 0.0010283696766590308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 283360
    },
    {
      "epoch": 0.0010284858110098675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 283392
    },
    {
      "epoch": 0.001028601945360704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 283424
    },
    {
      "epoch": 0.0010287180797115408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 283456
    },
    {
      "epoch": 0.0010288342140623776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7534,
      "step": 283488
    },
    {
      "epoch": 0.0010289503484132143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 283520
    },
    {
      "epoch": 0.0010290664827640509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 283552
    },
    {
      "epoch": 0.0010291826171148876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 283584
    },
    {
      "epoch": 0.0010292987514657244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 283616
    },
    {
      "epoch": 0.0010294148858165611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 283648
    },
    {
      "epoch": 0.001029531020167398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 283680
    },
    {
      "epoch": 0.0010296471545182344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 283712
    },
    {
      "epoch": 0.0010297632888690712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 283744
    },
    {
      "epoch": 0.001029879423219908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 283776
    },
    {
      "epoch": 0.0010299955575707447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 283808
    },
    {
      "epoch": 0.0010301116919215812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 283840
    },
    {
      "epoch": 0.001030227826272418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 283872
    },
    {
      "epoch": 0.0010303439606232547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 283904
    },
    {
      "epoch": 0.0010304600949740915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 283936
    },
    {
      "epoch": 0.0010305762293249282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 283968
    },
    {
      "epoch": 0.0010306923636757648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 284000
    },
    {
      "epoch": 0.0010308084980266015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 284032
    },
    {
      "epoch": 0.0010309246323774383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7505,
      "step": 284064
    },
    {
      "epoch": 0.001031040766728275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 284096
    },
    {
      "epoch": 0.0010311569010791116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 284128
    },
    {
      "epoch": 0.0010312730354299483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 284160
    },
    {
      "epoch": 0.001031389169780785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 284192
    },
    {
      "epoch": 0.0010315053041316218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 284224
    },
    {
      "epoch": 0.0010316214384824586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 284256
    },
    {
      "epoch": 0.0010317375728332951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 284288
    },
    {
      "epoch": 0.001031853707184132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 284320
    },
    {
      "epoch": 0.0010319698415349686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 284352
    },
    {
      "epoch": 0.0010320859758858054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 284384
    },
    {
      "epoch": 0.001032202110236642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 284416
    },
    {
      "epoch": 0.0010323182445874787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 284448
    },
    {
      "epoch": 0.0010324343789383154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 284480
    },
    {
      "epoch": 0.0010325505132891522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 284512
    },
    {
      "epoch": 0.001032666647639989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 284544
    },
    {
      "epoch": 0.0010327827819908255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 284576
    },
    {
      "epoch": 0.0010328989163416622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 284608
    },
    {
      "epoch": 0.001033015050692499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 284640
    },
    {
      "epoch": 0.0010331311850433358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 284672
    },
    {
      "epoch": 0.0010332473193941723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 284704
    },
    {
      "epoch": 0.001033363453745009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 284736
    },
    {
      "epoch": 0.0010334795880958458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 284768
    },
    {
      "epoch": 0.0010335957224466826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 284800
    },
    {
      "epoch": 0.0010337118567975193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 284832
    },
    {
      "epoch": 0.0010338279911483558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 284864
    },
    {
      "epoch": 0.0010339441254991926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 284896
    },
    {
      "epoch": 0.0010340602598500294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7492,
      "step": 284928
    },
    {
      "epoch": 0.0010341763942008661,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 284960
    },
    {
      "epoch": 0.0010342925285517026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 284992
    },
    {
      "epoch": 0.0010344086629025394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 285024
    },
    {
      "epoch": 0.0010345247972533762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 285056
    },
    {
      "epoch": 0.001034640931604213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 285088
    },
    {
      "epoch": 0.0010347570659550497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 285120
    },
    {
      "epoch": 0.0010348732003058862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 285152
    },
    {
      "epoch": 0.001034989334656723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 285184
    },
    {
      "epoch": 0.0010351054690075597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 285216
    },
    {
      "epoch": 0.0010352216033583965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 285248
    },
    {
      "epoch": 0.001035337737709233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 285280
    },
    {
      "epoch": 0.0010354538720600698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 285312
    },
    {
      "epoch": 0.0010355700064109065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 285344
    },
    {
      "epoch": 0.0010356861407617433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 285376
    },
    {
      "epoch": 0.00103580227511258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 285408
    },
    {
      "epoch": 0.0010359184094634166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 285440
    },
    {
      "epoch": 0.0010360345438142533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 285472
    },
    {
      "epoch": 0.00103615067816509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 285504
    },
    {
      "epoch": 0.0010362668125159268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 285536
    },
    {
      "epoch": 0.0010363829468667634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 285568
    },
    {
      "epoch": 0.0010364990812176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 285600
    },
    {
      "epoch": 0.0010366152155684369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 285632
    },
    {
      "epoch": 0.0010367313499192736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 285664
    },
    {
      "epoch": 0.0010368474842701104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 285696
    },
    {
      "epoch": 0.001036963618620947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 285728
    },
    {
      "epoch": 0.0010370797529717837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 285760
    },
    {
      "epoch": 0.0010371958873226204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 285792
    },
    {
      "epoch": 0.0010373120216734572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7632,
      "step": 285824
    },
    {
      "epoch": 0.0010374281560242937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 285856
    },
    {
      "epoch": 0.0010375442903751305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 285888
    },
    {
      "epoch": 0.0010376604247259672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 285920
    },
    {
      "epoch": 0.001037776559076804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 285952
    },
    {
      "epoch": 0.0010378926934276407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 285984
    },
    {
      "epoch": 0.0010380088277784773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 286016
    },
    {
      "epoch": 0.001038124962129314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 286048
    },
    {
      "epoch": 0.0010382410964801508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 286080
    },
    {
      "epoch": 0.0010383572308309875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7666,
      "step": 286112
    },
    {
      "epoch": 0.001038473365181824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 286144
    },
    {
      "epoch": 0.0010385894995326608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 286176
    },
    {
      "epoch": 0.0010387056338834976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 286208
    },
    {
      "epoch": 0.0010388217682343343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 286240
    },
    {
      "epoch": 0.001038937902585171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 286272
    },
    {
      "epoch": 0.0010390540369360076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6957,
      "step": 286304
    },
    {
      "epoch": 0.0010391701712868444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 286336
    },
    {
      "epoch": 0.0010392863056376811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 286368
    },
    {
      "epoch": 0.0010394024399885179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 286400
    },
    {
      "epoch": 0.0010395185743393544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 286432
    },
    {
      "epoch": 0.0010396347086901912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 286464
    },
    {
      "epoch": 0.001039750843041028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 286496
    },
    {
      "epoch": 0.0010398669773918647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 286528
    },
    {
      "epoch": 0.0010399831117427014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 286560
    },
    {
      "epoch": 0.001040099246093538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 286592
    },
    {
      "epoch": 0.0010402153804443747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 286624
    },
    {
      "epoch": 0.0010403315147952115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 286656
    },
    {
      "epoch": 0.0010404476491460482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7431,
      "step": 286688
    },
    {
      "epoch": 0.0010405637834968848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 286720
    },
    {
      "epoch": 0.0010406799178477215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 286752
    },
    {
      "epoch": 0.0010407960521985583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 286784
    },
    {
      "epoch": 0.001040912186549395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 286816
    },
    {
      "epoch": 0.0010410283209002318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 286848
    },
    {
      "epoch": 0.0010411444552510683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 286880
    },
    {
      "epoch": 0.001041260589601905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 286912
    },
    {
      "epoch": 0.0010413767239527418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 286944
    },
    {
      "epoch": 0.0010414928583035786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 286976
    },
    {
      "epoch": 0.0010416089926544151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 287008
    },
    {
      "epoch": 0.0010417251270052519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 287040
    },
    {
      "epoch": 0.0010418412613560886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 287072
    },
    {
      "epoch": 0.0010419573957069254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 287104
    },
    {
      "epoch": 0.0010420735300577621,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 287136
    },
    {
      "epoch": 0.0010421896644085987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 287168
    },
    {
      "epoch": 0.0010423057987594354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 287200
    },
    {
      "epoch": 0.0010424219331102722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 287232
    },
    {
      "epoch": 0.001042538067461109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 287264
    },
    {
      "epoch": 0.0010426542018119455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 287296
    },
    {
      "epoch": 0.0010427703361627822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 287328
    },
    {
      "epoch": 0.001042886470513619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 287360
    },
    {
      "epoch": 0.0010430026048644557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 287392
    },
    {
      "epoch": 0.0010431187392152925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 287424
    },
    {
      "epoch": 0.001043234873566129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 287456
    },
    {
      "epoch": 0.0010433510079169658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 287488
    },
    {
      "epoch": 0.0010434671422678025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 287520
    },
    {
      "epoch": 0.0010435832766186393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 287552
    },
    {
      "epoch": 0.0010436994109694758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7478,
      "step": 287584
    },
    {
      "epoch": 0.0010438155453203126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 287616
    },
    {
      "epoch": 0.0010439316796711493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 287648
    },
    {
      "epoch": 0.001044047814021986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 287680
    },
    {
      "epoch": 0.0010441639483728228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 287712
    },
    {
      "epoch": 0.0010442800827236594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 287744
    },
    {
      "epoch": 0.0010443962170744961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 287776
    },
    {
      "epoch": 0.0010445123514253329,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7572,
      "step": 287808
    },
    {
      "epoch": 0.0010446284857761696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 287840
    },
    {
      "epoch": 0.0010447446201270062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 287872
    },
    {
      "epoch": 0.001044860754477843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 287904
    },
    {
      "epoch": 0.0010449768888286797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 287936
    },
    {
      "epoch": 0.0010450930231795164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 287968
    },
    {
      "epoch": 0.0010452091575303532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 288000
    },
    {
      "epoch": 0.0010453252918811897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 288032
    },
    {
      "epoch": 0.0010454414262320265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 288064
    },
    {
      "epoch": 0.0010455575605828632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 288096
    },
    {
      "epoch": 0.0010456736949337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 288128
    },
    {
      "epoch": 0.0010457898292845365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 288160
    },
    {
      "epoch": 0.0010459059636353733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 288192
    },
    {
      "epoch": 0.00104602209798621,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 288224
    },
    {
      "epoch": 0.0010461382323370468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 288256
    },
    {
      "epoch": 0.0010462543666878836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 288288
    },
    {
      "epoch": 0.00104637050103872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 288320
    },
    {
      "epoch": 0.0010464866353895568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 288352
    },
    {
      "epoch": 0.0010466027697403936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 288384
    },
    {
      "epoch": 0.0010467189040912303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7522,
      "step": 288416
    },
    {
      "epoch": 0.0010468350384420669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7736,
      "step": 288448
    },
    {
      "epoch": 0.0010469511727929036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 288480
    },
    {
      "epoch": 0.0010470673071437404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 288512
    },
    {
      "epoch": 0.0010471834414945771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 288544
    },
    {
      "epoch": 0.001047299575845414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 288576
    },
    {
      "epoch": 0.0010474157101962504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 288608
    },
    {
      "epoch": 0.0010475318445470872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 288640
    },
    {
      "epoch": 0.001047647978897924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 288672
    },
    {
      "epoch": 0.0010477641132487607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 288704
    },
    {
      "epoch": 0.0010478802475995972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 288736
    },
    {
      "epoch": 0.001047996381950434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 288768
    },
    {
      "epoch": 0.0010481125163012707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 288800
    },
    {
      "epoch": 0.0010482286506521075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 288832
    },
    {
      "epoch": 0.0010483447850029443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 288864
    },
    {
      "epoch": 0.0010484609193537808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 288896
    },
    {
      "epoch": 0.0010485770537046175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 288928
    },
    {
      "epoch": 0.0010486931880554543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 288960
    },
    {
      "epoch": 0.001048809322406291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 288992
    },
    {
      "epoch": 0.0010489254567571276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 289024
    },
    {
      "epoch": 0.0010490415911079643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 289056
    },
    {
      "epoch": 0.001049157725458801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 289088
    },
    {
      "epoch": 0.0010492738598096379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 289120
    },
    {
      "epoch": 0.0010493899941604746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 289152
    },
    {
      "epoch": 0.0010495061285113111,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 289184
    },
    {
      "epoch": 0.001049622262862148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 289216
    },
    {
      "epoch": 0.0010497383972129847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 289248
    },
    {
      "epoch": 0.0010498545315638214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 289280
    },
    {
      "epoch": 0.001049970665914658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 289312
    },
    {
      "epoch": 0.0010500868002654947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7494,
      "step": 289344
    },
    {
      "epoch": 0.0010502029346163315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7459,
      "step": 289376
    },
    {
      "epoch": 0.0010503190689671682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 289408
    },
    {
      "epoch": 0.001050435203318005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 289440
    },
    {
      "epoch": 0.0010505513376688415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6878,
      "step": 289472
    },
    {
      "epoch": 0.0010506674720196783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 289504
    },
    {
      "epoch": 0.001050783606370515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 289536
    },
    {
      "epoch": 0.0010508997407213518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 289568
    },
    {
      "epoch": 0.0010510158750721883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 289600
    },
    {
      "epoch": 0.001051132009423025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 289632
    },
    {
      "epoch": 0.0010512481437738618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 289664
    },
    {
      "epoch": 0.0010513642781246986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 289696
    },
    {
      "epoch": 0.0010514804124755353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 289728
    },
    {
      "epoch": 0.0010515965468263719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 289760
    },
    {
      "epoch": 0.0010517126811772086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 289792
    },
    {
      "epoch": 0.0010518288155280454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 289824
    },
    {
      "epoch": 0.0010519449498788821,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 289856
    },
    {
      "epoch": 0.0010520610842297187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 289888
    },
    {
      "epoch": 0.0010521772185805554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 289920
    },
    {
      "epoch": 0.0010522933529313922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 289952
    },
    {
      "epoch": 0.001052409487282229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 289984
    },
    {
      "epoch": 0.0010525256216330657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 290016
    },
    {
      "epoch": 0.0010526417559839022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 290048
    },
    {
      "epoch": 0.001052757890334739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 290080
    },
    {
      "epoch": 0.0010528740246855757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 290112
    },
    {
      "epoch": 0.0010529901590364125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 290144
    },
    {
      "epoch": 0.001053106293387249,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 290176
    },
    {
      "epoch": 0.0010532224277380858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7523,
      "step": 290208
    },
    {
      "epoch": 0.0010533385620889225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 290240
    },
    {
      "epoch": 0.0010534546964397593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 290272
    },
    {
      "epoch": 0.001053570830790596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 290304
    },
    {
      "epoch": 0.0010536869651414326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 290336
    },
    {
      "epoch": 0.0010538030994922693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6824,
      "step": 290368
    },
    {
      "epoch": 0.001053919233843106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 290400
    },
    {
      "epoch": 0.0010540353681939428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 290432
    },
    {
      "epoch": 0.0010541515025447794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 290464
    },
    {
      "epoch": 0.0010542676368956161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 290496
    },
    {
      "epoch": 0.0010543837712464529,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 290528
    },
    {
      "epoch": 0.0010544999055972896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 290560
    },
    {
      "epoch": 0.0010546160399481264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 290592
    },
    {
      "epoch": 0.001054732174298963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 290624
    },
    {
      "epoch": 0.0010548483086497997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 290656
    },
    {
      "epoch": 0.0010549644430006364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 290688
    },
    {
      "epoch": 0.0010550805773514732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 290720
    },
    {
      "epoch": 0.0010551967117023097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 290752
    },
    {
      "epoch": 0.0010553128460531465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 290784
    },
    {
      "epoch": 0.0010554289804039832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 290816
    },
    {
      "epoch": 0.00105554511475482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 290848
    },
    {
      "epoch": 0.0010556612491056565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 290880
    },
    {
      "epoch": 0.0010557773834564933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 290912
    },
    {
      "epoch": 0.00105589351780733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 290944
    },
    {
      "epoch": 0.0010560096521581668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 290976
    },
    {
      "epoch": 0.0010561257865090035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 291008
    },
    {
      "epoch": 0.00105624192085984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 291040
    },
    {
      "epoch": 0.0010563580552106768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 291072
    },
    {
      "epoch": 0.0010564741895615136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 291104
    },
    {
      "epoch": 0.0010565903239123503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 291136
    },
    {
      "epoch": 0.0010567064582631869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 291168
    },
    {
      "epoch": 0.0010568225926140236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 291200
    },
    {
      "epoch": 0.0010569387269648604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 291232
    },
    {
      "epoch": 0.0010570548613156971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 291264
    },
    {
      "epoch": 0.0010571709956665339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 291296
    },
    {
      "epoch": 0.0010572871300173704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 291328
    },
    {
      "epoch": 0.0010574032643682072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 291360
    },
    {
      "epoch": 0.001057519398719044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 291392
    },
    {
      "epoch": 0.0010576355330698807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 291424
    },
    {
      "epoch": 0.0010577516674207172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 291456
    },
    {
      "epoch": 0.001057867801771554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 291488
    },
    {
      "epoch": 0.0010579839361223907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 291520
    },
    {
      "epoch": 0.0010581000704732275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 291552
    },
    {
      "epoch": 0.0010582162048240642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 291584
    },
    {
      "epoch": 0.0010583323391749008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 291616
    },
    {
      "epoch": 0.0010584484735257375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 291648
    },
    {
      "epoch": 0.0010585646078765743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 291680
    },
    {
      "epoch": 0.001058680742227411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 291712
    },
    {
      "epoch": 0.0010587968765782476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 291744
    },
    {
      "epoch": 0.0010589130109290843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 291776
    },
    {
      "epoch": 0.001059029145279921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 291808
    },
    {
      "epoch": 0.0010591452796307578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 291840
    },
    {
      "epoch": 0.0010592614139815946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 291872
    },
    {
      "epoch": 0.0010593775483324311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7493,
      "step": 291904
    },
    {
      "epoch": 0.0010594936826832679,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 291936
    },
    {
      "epoch": 0.0010596098170341046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 291968
    },
    {
      "epoch": 0.0010597259513849414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 292000
    },
    {
      "epoch": 0.001059842085735778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 292032
    },
    {
      "epoch": 0.0010599582200866147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 292064
    },
    {
      "epoch": 0.0010600743544374514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 292096
    },
    {
      "epoch": 0.0010601904887882882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 292128
    },
    {
      "epoch": 0.001060306623139125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 292160
    },
    {
      "epoch": 0.0010604227574899615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 292192
    },
    {
      "epoch": 0.0010605388918407982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 292224
    },
    {
      "epoch": 0.001060655026191635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 292256
    },
    {
      "epoch": 0.0010607711605424717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 292288
    },
    {
      "epoch": 0.0010608872948933083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 292320
    },
    {
      "epoch": 0.001061003429244145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 292352
    },
    {
      "epoch": 0.0010611195635949818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7463,
      "step": 292384
    },
    {
      "epoch": 0.0010612356979458185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 292416
    },
    {
      "epoch": 0.0010613518322966553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 292448
    },
    {
      "epoch": 0.0010614679666474918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 292480
    },
    {
      "epoch": 0.0010615841009983286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 292512
    },
    {
      "epoch": 0.0010617002353491653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 292544
    },
    {
      "epoch": 0.001061816369700002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 292576
    },
    {
      "epoch": 0.0010619325040508386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 292608
    },
    {
      "epoch": 0.0010620486384016754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 292640
    },
    {
      "epoch": 0.0010621647727525121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 292672
    },
    {
      "epoch": 0.001062280907103349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.748,
      "step": 292704
    },
    {
      "epoch": 0.0010623970414541857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7542,
      "step": 292736
    },
    {
      "epoch": 0.0010625131758050222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 292768
    },
    {
      "epoch": 0.001062629310155859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 292800
    },
    {
      "epoch": 0.0010627454445066957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 292832
    },
    {
      "epoch": 0.0010628615788575325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7412,
      "step": 292864
    },
    {
      "epoch": 0.001062977713208369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 292896
    },
    {
      "epoch": 0.0010630938475592057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 292928
    },
    {
      "epoch": 0.0010632099819100425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 292960
    },
    {
      "epoch": 0.0010633261162608793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 292992
    },
    {
      "epoch": 0.001063442250611716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 293024
    },
    {
      "epoch": 0.0010635583849625525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 293056
    },
    {
      "epoch": 0.0010636745193133893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 293088
    },
    {
      "epoch": 0.001063790653664226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 293120
    },
    {
      "epoch": 0.0010639067880150628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7663,
      "step": 293152
    },
    {
      "epoch": 0.0010640229223658993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 293184
    },
    {
      "epoch": 0.001064139056716736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 293216
    },
    {
      "epoch": 0.0010642551910675728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 293248
    },
    {
      "epoch": 0.0010643713254184096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 293280
    },
    {
      "epoch": 0.0010644874597692464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 293312
    },
    {
      "epoch": 0.001064603594120083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 293344
    },
    {
      "epoch": 0.0010647197284709196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 293376
    },
    {
      "epoch": 0.0010648358628217564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 293408
    },
    {
      "epoch": 0.0010649519971725932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 293440
    },
    {
      "epoch": 0.0010650681315234297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 293472
    },
    {
      "epoch": 0.0010651842658742664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 293504
    },
    {
      "epoch": 0.0010653004002251032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 293536
    },
    {
      "epoch": 0.00106541653457594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 293568
    },
    {
      "epoch": 0.0010655326689267767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 293600
    },
    {
      "epoch": 0.0010656488032776132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 293632
    },
    {
      "epoch": 0.00106576493762845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 293664
    },
    {
      "epoch": 0.0010658810719792868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 293696
    },
    {
      "epoch": 0.0010659972063301235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 293728
    },
    {
      "epoch": 0.00106611334068096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 293760
    },
    {
      "epoch": 0.0010662294750317968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 293792
    },
    {
      "epoch": 0.0010663456093826336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 293824
    },
    {
      "epoch": 0.0010664617437334703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 293856
    },
    {
      "epoch": 0.001066577878084307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 293888
    },
    {
      "epoch": 0.0010666940124351436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 293920
    },
    {
      "epoch": 0.0010668101467859804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 293952
    },
    {
      "epoch": 0.0010669262811368171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 293984
    },
    {
      "epoch": 0.0010670424154876539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 294016
    },
    {
      "epoch": 0.0010671585498384904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 294048
    },
    {
      "epoch": 0.0010672746841893272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 294080
    },
    {
      "epoch": 0.001067390818540164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 294112
    },
    {
      "epoch": 0.0010675069528910007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 294144
    },
    {
      "epoch": 0.0010676230872418374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 294176
    },
    {
      "epoch": 0.001067739221592674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 294208
    },
    {
      "epoch": 0.0010678553559435107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 294240
    },
    {
      "epoch": 0.0010679714902943475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 294272
    },
    {
      "epoch": 0.0010680876246451842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 294304
    },
    {
      "epoch": 0.0010682037589960208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 294336
    },
    {
      "epoch": 0.0010683198933468575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 294368
    },
    {
      "epoch": 0.0010684360276976943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 294400
    },
    {
      "epoch": 0.001068552162048531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 294432
    },
    {
      "epoch": 0.0010686682963993678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 294464
    },
    {
      "epoch": 0.0010687844307502043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 294496
    },
    {
      "epoch": 0.001068900565101041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 294528
    },
    {
      "epoch": 0.0010690166994518778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 294560
    },
    {
      "epoch": 0.0010691328338027146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 294592
    },
    {
      "epoch": 0.0010692489681535511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 294624
    },
    {
      "epoch": 0.0010693651025043879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 294656
    },
    {
      "epoch": 0.0010694812368552246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 294688
    },
    {
      "epoch": 0.0010695973712060614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 294720
    },
    {
      "epoch": 0.0010697135055568981,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 294752
    },
    {
      "epoch": 0.0010698296399077347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6856,
      "step": 294784
    },
    {
      "epoch": 0.0010699457742585714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 294816
    },
    {
      "epoch": 0.0010700619086094082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 294848
    },
    {
      "epoch": 0.001070178042960245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 294880
    },
    {
      "epoch": 0.0010702941773110815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 294912
    },
    {
      "epoch": 0.0010704103116619182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 294944
    },
    {
      "epoch": 0.001070526446012755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 294976
    },
    {
      "epoch": 0.0010706425803635917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 295008
    },
    {
      "epoch": 0.0010707587147144285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 295040
    },
    {
      "epoch": 0.001070874849065265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 295072
    },
    {
      "epoch": 0.0010709909834161018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 295104
    },
    {
      "epoch": 0.0010711071177669385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 295136
    },
    {
      "epoch": 0.0010712232521177753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 295168
    },
    {
      "epoch": 0.0010713393864686118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 295200
    },
    {
      "epoch": 0.0010714555208194486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7506,
      "step": 295232
    },
    {
      "epoch": 0.0010715716551702853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7646,
      "step": 295264
    },
    {
      "epoch": 0.001071687789521122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 295296
    },
    {
      "epoch": 0.0010718039238719588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 295328
    },
    {
      "epoch": 0.0010719200582227954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 295360
    },
    {
      "epoch": 0.0010720361925736321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 295392
    },
    {
      "epoch": 0.0010721523269244689,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 295424
    },
    {
      "epoch": 0.0010722684612753056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 295456
    },
    {
      "epoch": 0.0010723845956261422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 295488
    },
    {
      "epoch": 0.001072500729976979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 295520
    },
    {
      "epoch": 0.0010726168643278157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 295552
    },
    {
      "epoch": 0.0010727329986786524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 295584
    },
    {
      "epoch": 0.0010728491330294892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 295616
    },
    {
      "epoch": 0.0010729652673803257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 295648
    },
    {
      "epoch": 0.0010730814017311625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 295680
    },
    {
      "epoch": 0.0010731975360819992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 295712
    },
    {
      "epoch": 0.001073313670432836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 295744
    },
    {
      "epoch": 0.0010734298047836725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 295776
    },
    {
      "epoch": 0.0010735459391345093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 295808
    },
    {
      "epoch": 0.001073662073485346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 295840
    },
    {
      "epoch": 0.0010737782078361828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7512,
      "step": 295872
    },
    {
      "epoch": 0.0010738943421870195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 295904
    },
    {
      "epoch": 0.001074010476537856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 295936
    },
    {
      "epoch": 0.0010741266108886928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 295968
    },
    {
      "epoch": 0.0010742427452395296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 296000
    },
    {
      "epoch": 0.0010743588795903663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 296032
    },
    {
      "epoch": 0.0010744750139412029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 296064
    },
    {
      "epoch": 0.0010745911482920396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 296096
    },
    {
      "epoch": 0.0010747072826428764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 296128
    },
    {
      "epoch": 0.0010748234169937131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 296160
    },
    {
      "epoch": 0.00107493955134455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 296192
    },
    {
      "epoch": 0.0010750556856953864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 296224
    },
    {
      "epoch": 0.0010751718200462232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 296256
    },
    {
      "epoch": 0.00107528795439706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 296288
    },
    {
      "epoch": 0.0010754040887478967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 296320
    },
    {
      "epoch": 0.0010755202230987332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 296352
    },
    {
      "epoch": 0.00107563635744957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 296384
    },
    {
      "epoch": 0.0010757524918004067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 296416
    },
    {
      "epoch": 0.0010758686261512435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 296448
    },
    {
      "epoch": 0.0010759847605020802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 296480
    },
    {
      "epoch": 0.0010761008948529168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 296512
    },
    {
      "epoch": 0.0010762170292037535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6906,
      "step": 296544
    },
    {
      "epoch": 0.0010763331635545903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 296576
    },
    {
      "epoch": 0.001076449297905427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 296608
    },
    {
      "epoch": 0.0010765654322562636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 296640
    },
    {
      "epoch": 0.0010766815666071003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 296672
    },
    {
      "epoch": 0.001076797700957937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 296704
    },
    {
      "epoch": 0.0010769138353087738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 296736
    },
    {
      "epoch": 0.0010770299696596106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 296768
    },
    {
      "epoch": 0.0010771461040104471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 296800
    },
    {
      "epoch": 0.0010772622383612839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 296832
    },
    {
      "epoch": 0.0010773783727121206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 296864
    },
    {
      "epoch": 0.0010774945070629574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 296896
    },
    {
      "epoch": 0.001077610641413794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 296928
    },
    {
      "epoch": 0.0010777267757646307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 296960
    },
    {
      "epoch": 0.0010778429101154674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 296992
    },
    {
      "epoch": 0.0010779590444663042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7462,
      "step": 297024
    },
    {
      "epoch": 0.001078075178817141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 297056
    },
    {
      "epoch": 0.0010781913131679775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 297088
    },
    {
      "epoch": 0.0010783074475188142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 297120
    },
    {
      "epoch": 0.001078423581869651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 297152
    },
    {
      "epoch": 0.0010785397162204878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 297184
    },
    {
      "epoch": 0.0010786558505713243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7593,
      "step": 297216
    },
    {
      "epoch": 0.001078771984922161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 297248
    },
    {
      "epoch": 0.0010788881192729978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 297280
    },
    {
      "epoch": 0.0010790042536238346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 297312
    },
    {
      "epoch": 0.0010791203879746713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 297344
    },
    {
      "epoch": 0.0010792365223255078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 297376
    },
    {
      "epoch": 0.0010793526566763446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 297408
    },
    {
      "epoch": 0.0010794687910271814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 297440
    },
    {
      "epoch": 0.001079584925378018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 297472
    },
    {
      "epoch": 0.0010797010597288546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 297504
    },
    {
      "epoch": 0.0010798171940796914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 297536
    },
    {
      "epoch": 0.0010799333284305282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 297568
    },
    {
      "epoch": 0.001080049462781365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 297600
    },
    {
      "epoch": 0.0010801655971322017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 297632
    },
    {
      "epoch": 0.0010802817314830382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7517,
      "step": 297664
    },
    {
      "epoch": 0.001080397865833875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 297696
    },
    {
      "epoch": 0.0010805140001847117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 297728
    },
    {
      "epoch": 0.0010806301345355485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 297760
    },
    {
      "epoch": 0.001080746268886385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 297792
    },
    {
      "epoch": 0.0010808624032372218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 297824
    },
    {
      "epoch": 0.0010809785375880585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 297856
    },
    {
      "epoch": 0.0010810946719388953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7547,
      "step": 297888
    },
    {
      "epoch": 0.001081210806289732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 297920
    },
    {
      "epoch": 0.0010813269406405685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 297952
    },
    {
      "epoch": 0.0010814430749914053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 297984
    },
    {
      "epoch": 0.001081559209342242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 298016
    },
    {
      "epoch": 0.0010816753436930788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 298048
    },
    {
      "epoch": 0.0010817914780439153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7718,
      "step": 298080
    },
    {
      "epoch": 0.001081907612394752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 298112
    },
    {
      "epoch": 0.0010820237467455889,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 298144
    },
    {
      "epoch": 0.0010821398810964256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 298176
    },
    {
      "epoch": 0.0010822560154472624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 298208
    },
    {
      "epoch": 0.001082372149798099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 298240
    },
    {
      "epoch": 0.0010824882841489357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 298272
    },
    {
      "epoch": 0.0010826044184997724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 298304
    },
    {
      "epoch": 0.0010827205528506092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 298336
    },
    {
      "epoch": 0.0010828366872014457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 298368
    },
    {
      "epoch": 0.0010829528215522825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 298400
    },
    {
      "epoch": 0.0010830689559031192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 298432
    },
    {
      "epoch": 0.001083185090253956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 298464
    },
    {
      "epoch": 0.0010833012246047927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 298496
    },
    {
      "epoch": 0.0010834173589556293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 298528
    },
    {
      "epoch": 0.001083533493306466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 298560
    },
    {
      "epoch": 0.0010836496276573028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 298592
    },
    {
      "epoch": 0.0010837657620081395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 298624
    },
    {
      "epoch": 0.001083881896358976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 298656
    },
    {
      "epoch": 0.0010839980307098128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 298688
    },
    {
      "epoch": 0.0010841141650606496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 298720
    },
    {
      "epoch": 0.0010842302994114863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 298752
    },
    {
      "epoch": 0.001084346433762323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7594,
      "step": 298784
    },
    {
      "epoch": 0.0010844625681131596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 298816
    },
    {
      "epoch": 0.0010845787024639964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 298848
    },
    {
      "epoch": 0.0010846948368148331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 298880
    },
    {
      "epoch": 0.0010848109711656699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 298912
    },
    {
      "epoch": 0.0010849271055165064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 298944
    },
    {
      "epoch": 0.0010850432398673432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7548,
      "step": 298976
    },
    {
      "epoch": 0.00108515937421818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 299008
    },
    {
      "epoch": 0.0010852755085690167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 299040
    },
    {
      "epoch": 0.0010853916429198534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 299072
    },
    {
      "epoch": 0.00108550777727069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 299104
    },
    {
      "epoch": 0.0010856239116215267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 299136
    },
    {
      "epoch": 0.0010857400459723635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6889,
      "step": 299168
    },
    {
      "epoch": 0.0010858561803232002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 299200
    },
    {
      "epoch": 0.0010859723146740368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 299232
    },
    {
      "epoch": 0.0010860884490248735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 299264
    },
    {
      "epoch": 0.0010862045833757103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 299296
    },
    {
      "epoch": 0.001086320717726547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 299328
    },
    {
      "epoch": 0.0010864368520773838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 299360
    },
    {
      "epoch": 0.0010865529864282203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 299392
    },
    {
      "epoch": 0.001086669120779057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 299424
    },
    {
      "epoch": 0.0010867852551298938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 299456
    },
    {
      "epoch": 0.0010869013894807306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6903,
      "step": 299488
    },
    {
      "epoch": 0.0010870175238315671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 299520
    },
    {
      "epoch": 0.0010871336581824039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 299552
    },
    {
      "epoch": 0.0010872497925332406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 299584
    },
    {
      "epoch": 0.0010873659268840774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 299616
    },
    {
      "epoch": 0.0010874820612349141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 299648
    },
    {
      "epoch": 0.0010875981955857507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 299680
    },
    {
      "epoch": 0.0010877143299365874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 299712
    },
    {
      "epoch": 0.0010878304642874242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 299744
    },
    {
      "epoch": 0.001087946598638261,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 299776
    },
    {
      "epoch": 0.0010880627329890975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 299808
    },
    {
      "epoch": 0.0010881788673399342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 299840
    },
    {
      "epoch": 0.001088295001690771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 299872
    },
    {
      "epoch": 0.0010884111360416077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 299904
    },
    {
      "epoch": 0.0010885272703924445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 299936
    },
    {
      "epoch": 0.001088643404743281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.746,
      "step": 299968
    },
    {
      "epoch": 0.0010887595390941178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 300000
    },
    {
      "epoch": 0.0010888756734449545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 300032
    },
    {
      "epoch": 0.0010889918077957913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 300064
    },
    {
      "epoch": 0.0010891079421466278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 300096
    },
    {
      "epoch": 0.0010892240764974646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 300128
    },
    {
      "epoch": 0.0010893402108483013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 300160
    },
    {
      "epoch": 0.001089456345199138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 300192
    },
    {
      "epoch": 0.0010895724795499748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 300224
    },
    {
      "epoch": 0.0010896886139008114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 300256
    },
    {
      "epoch": 0.0010898047482516481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 300288
    },
    {
      "epoch": 0.0010899208826024849,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 300320
    },
    {
      "epoch": 0.0010900370169533216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 300352
    },
    {
      "epoch": 0.0010901531513041582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 300384
    },
    {
      "epoch": 0.001090269285654995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 300416
    },
    {
      "epoch": 0.0010903854200058317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 300448
    },
    {
      "epoch": 0.0010905015543566684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6957,
      "step": 300480
    },
    {
      "epoch": 0.0010906176887075052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 300512
    },
    {
      "epoch": 0.0010907338230583417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7449,
      "step": 300544
    },
    {
      "epoch": 0.0010908499574091785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 300576
    },
    {
      "epoch": 0.0010909660917600152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 300608
    },
    {
      "epoch": 0.001091082226110852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 300640
    },
    {
      "epoch": 0.0010911983604616885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 300672
    },
    {
      "epoch": 0.0010913144948125253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 300704
    },
    {
      "epoch": 0.001091430629163362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7478,
      "step": 300736
    },
    {
      "epoch": 0.0010915467635141988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 300768
    },
    {
      "epoch": 0.0010916628978650355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 300800
    },
    {
      "epoch": 0.001091779032215872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 300832
    },
    {
      "epoch": 0.0010918951665667088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 300864
    },
    {
      "epoch": 0.0010920113009175456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 300896
    },
    {
      "epoch": 0.0010921274352683823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 300928
    },
    {
      "epoch": 0.0010922435696192189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 300960
    },
    {
      "epoch": 0.0010923597039700556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 300992
    },
    {
      "epoch": 0.0010924758383208924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 301024
    },
    {
      "epoch": 0.0010925919726717291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 301056
    },
    {
      "epoch": 0.001092708107022566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 301088
    },
    {
      "epoch": 0.0010928242413734024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 301120
    },
    {
      "epoch": 0.0010929403757242392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 301152
    },
    {
      "epoch": 0.001093056510075076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 301184
    },
    {
      "epoch": 0.0010931726444259127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 301216
    },
    {
      "epoch": 0.0010932887787767492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 301248
    },
    {
      "epoch": 0.001093404913127586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 301280
    },
    {
      "epoch": 0.0010935210474784227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 301312
    },
    {
      "epoch": 0.0010936371818292595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 301344
    },
    {
      "epoch": 0.0010937533161800963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 301376
    },
    {
      "epoch": 0.0010938694505309328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 301408
    },
    {
      "epoch": 0.0010939855848817695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 301440
    },
    {
      "epoch": 0.0010941017192326063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 301472
    },
    {
      "epoch": 0.001094217853583443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 301504
    },
    {
      "epoch": 0.0010943339879342796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 301536
    },
    {
      "epoch": 0.0010944501222851163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 301568
    },
    {
      "epoch": 0.001094566256635953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 301600
    },
    {
      "epoch": 0.0010946823909867899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 301632
    },
    {
      "epoch": 0.0010947985253376266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 301664
    },
    {
      "epoch": 0.0010949146596884631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 301696
    },
    {
      "epoch": 0.0010950307940393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 301728
    },
    {
      "epoch": 0.0010951469283901367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 301760
    },
    {
      "epoch": 0.0010952630627409734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 301792
    },
    {
      "epoch": 0.00109537919709181,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 301824
    },
    {
      "epoch": 0.0010954953314426467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 301856
    },
    {
      "epoch": 0.0010956114657934835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 301888
    },
    {
      "epoch": 0.0010957276001443202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 301920
    },
    {
      "epoch": 0.001095843734495157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 301952
    },
    {
      "epoch": 0.0010959598688459935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 301984
    },
    {
      "epoch": 0.0010960760031968303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 302016
    },
    {
      "epoch": 0.001096192137547667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 302048
    },
    {
      "epoch": 0.0010963082718985038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 302080
    },
    {
      "epoch": 0.0010964244062493403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 302112
    },
    {
      "epoch": 0.001096540540600177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 302144
    },
    {
      "epoch": 0.0010966566749510138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 302176
    },
    {
      "epoch": 0.0010967728093018506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 302208
    },
    {
      "epoch": 0.0010968889436526873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 302240
    },
    {
      "epoch": 0.0010970050780035239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 302272
    },
    {
      "epoch": 0.0010971212123543606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 302304
    },
    {
      "epoch": 0.0010972373467051974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 302336
    },
    {
      "epoch": 0.0010973534810560341,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 302368
    },
    {
      "epoch": 0.0010974696154068707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 302400
    },
    {
      "epoch": 0.0010975857497577074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 302432
    },
    {
      "epoch": 0.0010977018841085442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7468,
      "step": 302464
    },
    {
      "epoch": 0.001097818018459381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7533,
      "step": 302496
    },
    {
      "epoch": 0.0010979341528102177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 302528
    },
    {
      "epoch": 0.0010980502871610542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 302560
    },
    {
      "epoch": 0.001098166421511891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 302592
    },
    {
      "epoch": 0.0010982825558627277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 302624
    },
    {
      "epoch": 0.0010983986902135645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 302656
    },
    {
      "epoch": 0.001098514824564401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 302688
    },
    {
      "epoch": 0.0010986309589152378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 302720
    },
    {
      "epoch": 0.0010987470932660745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 302752
    },
    {
      "epoch": 0.0010988632276169113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7406,
      "step": 302784
    },
    {
      "epoch": 0.001098979361967748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7446,
      "step": 302816
    },
    {
      "epoch": 0.0010990954963185846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 302848
    },
    {
      "epoch": 0.0010992116306694213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 302880
    },
    {
      "epoch": 0.001099327765020258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7551,
      "step": 302912
    },
    {
      "epoch": 0.0010994438993710948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 302944
    },
    {
      "epoch": 0.0010995600337219314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 302976
    },
    {
      "epoch": 0.0010996761680727681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7548,
      "step": 303008
    },
    {
      "epoch": 0.0010997923024236049,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 303040
    },
    {
      "epoch": 0.0010999084367744416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 303072
    },
    {
      "epoch": 0.0011000245711252784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 303104
    },
    {
      "epoch": 0.001100140705476115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 303136
    },
    {
      "epoch": 0.0011002568398269517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7629,
      "step": 303168
    },
    {
      "epoch": 0.0011003729741777884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 303200
    },
    {
      "epoch": 0.0011004891085286252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7501,
      "step": 303232
    },
    {
      "epoch": 0.0011006052428794617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 303264
    },
    {
      "epoch": 0.0011007213772302985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 303296
    },
    {
      "epoch": 0.0011008375115811352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 303328
    },
    {
      "epoch": 0.001100953645931972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 303360
    },
    {
      "epoch": 0.0011010697802828087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 303392
    },
    {
      "epoch": 0.0011011859146336453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 303424
    },
    {
      "epoch": 0.001101302048984482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 303456
    },
    {
      "epoch": 0.0011014181833353188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 303488
    },
    {
      "epoch": 0.0011015343176861555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 303520
    },
    {
      "epoch": 0.001101650452036992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 303552
    },
    {
      "epoch": 0.0011017665863878288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 303584
    },
    {
      "epoch": 0.0011018827207386656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 303616
    },
    {
      "epoch": 0.0011019988550895023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 303648
    },
    {
      "epoch": 0.001102114989440339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 303680
    },
    {
      "epoch": 0.0011022311237911756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 303712
    },
    {
      "epoch": 0.0011023472581420124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 303744
    },
    {
      "epoch": 0.0011024633924928491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 303776
    },
    {
      "epoch": 0.0011025795268436859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 303808
    },
    {
      "epoch": 0.0011026956611945224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 303840
    },
    {
      "epoch": 0.0011028117955453592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 303872
    },
    {
      "epoch": 0.001102927929896196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 303904
    },
    {
      "epoch": 0.0011030440642470327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 303936
    },
    {
      "epoch": 0.0011031601985978694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 303968
    },
    {
      "epoch": 0.001103276332948706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 304000
    },
    {
      "epoch": 0.0011033924672995427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 304032
    },
    {
      "epoch": 0.0011035086016503795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 304064
    },
    {
      "epoch": 0.0011036247360012162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 304096
    },
    {
      "epoch": 0.0011037408703520528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 304128
    },
    {
      "epoch": 0.0011038570047028895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 304160
    },
    {
      "epoch": 0.0011039731390537263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 304192
    },
    {
      "epoch": 0.001104089273404563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 304224
    },
    {
      "epoch": 0.0011042054077553998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 304256
    },
    {
      "epoch": 0.0011043215421062363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 304288
    },
    {
      "epoch": 0.001104437676457073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 304320
    },
    {
      "epoch": 0.0011045538108079098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 304352
    },
    {
      "epoch": 0.0011046699451587466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 304384
    },
    {
      "epoch": 0.0011047860795095831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 304416
    },
    {
      "epoch": 0.0011049022138604199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 304448
    },
    {
      "epoch": 0.0011050183482112566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 304480
    },
    {
      "epoch": 0.0011051344825620934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 304512
    },
    {
      "epoch": 0.0011052506169129301,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 304544
    },
    {
      "epoch": 0.0011053667512637667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 304576
    },
    {
      "epoch": 0.0011054828856146034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 304608
    },
    {
      "epoch": 0.0011055990199654402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 304640
    },
    {
      "epoch": 0.001105715154316277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 304672
    },
    {
      "epoch": 0.0011058312886671135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 304704
    },
    {
      "epoch": 0.0011059474230179502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6826,
      "step": 304736
    },
    {
      "epoch": 0.001106063557368787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 304768
    },
    {
      "epoch": 0.0011061796917196237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 304800
    },
    {
      "epoch": 0.0011062958260704605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 304832
    },
    {
      "epoch": 0.001106411960421297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 304864
    },
    {
      "epoch": 0.0011065280947721338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 304896
    },
    {
      "epoch": 0.0011066442291229705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 304928
    },
    {
      "epoch": 0.0011067603634738073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 304960
    },
    {
      "epoch": 0.0011068764978246438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 304992
    },
    {
      "epoch": 0.0011069926321754806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 305024
    },
    {
      "epoch": 0.0011071087665263173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 305056
    },
    {
      "epoch": 0.001107224900877154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 305088
    },
    {
      "epoch": 0.0011073410352279908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.775,
      "step": 305120
    },
    {
      "epoch": 0.0011074571695788274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 305152
    },
    {
      "epoch": 0.0011075733039296641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 305184
    },
    {
      "epoch": 0.001107689438280501,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 305216
    },
    {
      "epoch": 0.0011078055726313376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 305248
    },
    {
      "epoch": 0.0011079217069821742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 305280
    },
    {
      "epoch": 0.001108037841333011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 305312
    },
    {
      "epoch": 0.0011081539756838477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 305344
    },
    {
      "epoch": 0.0011082701100346844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 305376
    },
    {
      "epoch": 0.0011083862443855212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 305408
    },
    {
      "epoch": 0.0011085023787363577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 305440
    },
    {
      "epoch": 0.0011086185130871945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 305472
    },
    {
      "epoch": 0.0011087346474380312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 305504
    },
    {
      "epoch": 0.001108850781788868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 305536
    },
    {
      "epoch": 0.0011089669161397045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 305568
    },
    {
      "epoch": 0.0011090830504905413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 305600
    },
    {
      "epoch": 0.001109199184841378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 305632
    },
    {
      "epoch": 0.0011093153191922148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 305664
    },
    {
      "epoch": 0.0011094314535430516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 305696
    },
    {
      "epoch": 0.001109547587893888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 305728
    },
    {
      "epoch": 0.0011096637222447248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 305760
    },
    {
      "epoch": 0.0011097798565955616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7543,
      "step": 305792
    },
    {
      "epoch": 0.0011098959909463984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 305824
    },
    {
      "epoch": 0.001110012125297235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 305856
    },
    {
      "epoch": 0.0011101282596480716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 305888
    },
    {
      "epoch": 0.0011102443939989084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 305920
    },
    {
      "epoch": 0.0011103605283497452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 305952
    },
    {
      "epoch": 0.001110476662700582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7471,
      "step": 305984
    },
    {
      "epoch": 0.0011105927970514184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 306016
    },
    {
      "epoch": 0.0011107089314022552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 306048
    },
    {
      "epoch": 0.001110825065753092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 306080
    },
    {
      "epoch": 0.0011109412001039287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 306112
    },
    {
      "epoch": 0.0011110573344547652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 306144
    },
    {
      "epoch": 0.001111173468805602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 306176
    },
    {
      "epoch": 0.0011112896031564388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 306208
    },
    {
      "epoch": 0.0011114057375072755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 306240
    },
    {
      "epoch": 0.0011115218718581123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 306272
    },
    {
      "epoch": 0.0011116380062089488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 306304
    },
    {
      "epoch": 0.0011117541405597856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 306336
    },
    {
      "epoch": 0.0011118702749106223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 306368
    },
    {
      "epoch": 0.001111986409261459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 306400
    },
    {
      "epoch": 0.0011121025436122956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 306432
    },
    {
      "epoch": 0.0011122186779631324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6931,
      "step": 306464
    },
    {
      "epoch": 0.001112334812313969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 306496
    },
    {
      "epoch": 0.0011124509466648059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 306528
    },
    {
      "epoch": 0.0011125670810156426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 306560
    },
    {
      "epoch": 0.0011126832153664792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 306592
    },
    {
      "epoch": 0.001112799349717316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 306624
    },
    {
      "epoch": 0.0011129154840681527,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 306656
    },
    {
      "epoch": 0.0011130316184189894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 306688
    },
    {
      "epoch": 0.001113147752769826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 306720
    },
    {
      "epoch": 0.0011132638871206627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 306752
    },
    {
      "epoch": 0.0011133800214714995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 306784
    },
    {
      "epoch": 0.0011134961558223362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 306816
    },
    {
      "epoch": 0.001113612290173173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 306848
    },
    {
      "epoch": 0.0011137284245240095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7529,
      "step": 306880
    },
    {
      "epoch": 0.0011138445588748463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 306912
    },
    {
      "epoch": 0.001113960693225683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 306944
    },
    {
      "epoch": 0.0011140768275765198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 306976
    },
    {
      "epoch": 0.0011141929619273563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 307008
    },
    {
      "epoch": 0.001114309096278193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 307040
    },
    {
      "epoch": 0.0011144252306290298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 307072
    },
    {
      "epoch": 0.0011145413649798666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 307104
    },
    {
      "epoch": 0.0011146574993307033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 307136
    },
    {
      "epoch": 0.0011147736336815399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 307168
    },
    {
      "epoch": 0.0011148897680323766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 307200
    },
    {
      "epoch": 0.0011150059023832134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 307232
    },
    {
      "epoch": 0.0011151220367340501,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 307264
    },
    {
      "epoch": 0.0011152381710848867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 307296
    },
    {
      "epoch": 0.0011153543054357234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 307328
    },
    {
      "epoch": 0.0011154704397865602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 307360
    },
    {
      "epoch": 0.001115586574137397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 307392
    },
    {
      "epoch": 0.0011157027084882337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 307424
    },
    {
      "epoch": 0.0011158188428390702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 307456
    },
    {
      "epoch": 0.001115934977189907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 307488
    },
    {
      "epoch": 0.0011160511115407437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 307520
    },
    {
      "epoch": 0.0011161672458915805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7474,
      "step": 307552
    },
    {
      "epoch": 0.001116283380242417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 307584
    },
    {
      "epoch": 0.0011163995145932538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 307616
    },
    {
      "epoch": 0.0011165156489440905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 307648
    },
    {
      "epoch": 0.0011166317832949273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 307680
    },
    {
      "epoch": 0.001116747917645764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 307712
    },
    {
      "epoch": 0.0011168640519966006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 307744
    },
    {
      "epoch": 0.0011169801863474373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 307776
    },
    {
      "epoch": 0.001117096320698274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 307808
    },
    {
      "epoch": 0.0011172124550491108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 307840
    },
    {
      "epoch": 0.0011173285893999474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 307872
    },
    {
      "epoch": 0.0011174447237507841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 307904
    },
    {
      "epoch": 0.0011175608581016209,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 307936
    },
    {
      "epoch": 0.0011176769924524576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 307968
    },
    {
      "epoch": 0.0011177931268032944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 308000
    },
    {
      "epoch": 0.001117909261154131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 308032
    },
    {
      "epoch": 0.0011180253955049677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 308064
    },
    {
      "epoch": 0.0011181415298558044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 308096
    },
    {
      "epoch": 0.0011182576642066412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 308128
    },
    {
      "epoch": 0.0011183737985574777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7648,
      "step": 308160
    },
    {
      "epoch": 0.0011184899329083145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 308192
    },
    {
      "epoch": 0.0011186060672591512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 308224
    },
    {
      "epoch": 0.001118722201609988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 308256
    },
    {
      "epoch": 0.0011188383359608247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 308288
    },
    {
      "epoch": 0.0011189544703116613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 308320
    },
    {
      "epoch": 0.001119070604662498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 308352
    },
    {
      "epoch": 0.0011191867390133348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 308384
    },
    {
      "epoch": 0.0011193028733641715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 308416
    },
    {
      "epoch": 0.001119419007715008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 308448
    },
    {
      "epoch": 0.0011195351420658448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 308480
    },
    {
      "epoch": 0.0011196512764166816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 308512
    },
    {
      "epoch": 0.0011197674107675183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 308544
    },
    {
      "epoch": 0.001119883545118355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 308576
    },
    {
      "epoch": 0.0011199996794691916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 308608
    },
    {
      "epoch": 0.0011201158138200284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7535,
      "step": 308640
    },
    {
      "epoch": 0.0011202319481708651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 308672
    },
    {
      "epoch": 0.0011203480825217019,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 308704
    },
    {
      "epoch": 0.0011204642168725384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 308736
    },
    {
      "epoch": 0.0011205803512233752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 308768
    },
    {
      "epoch": 0.001120696485574212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 308800
    },
    {
      "epoch": 0.0011208126199250487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 308832
    },
    {
      "epoch": 0.0011209287542758854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 308864
    },
    {
      "epoch": 0.001121044888626722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 308896
    },
    {
      "epoch": 0.0011211610229775587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 308928
    },
    {
      "epoch": 0.0011212771573283955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 308960
    },
    {
      "epoch": 0.0011213932916792322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 308992
    },
    {
      "epoch": 0.0011215094260300688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 309024
    },
    {
      "epoch": 0.0011216255603809055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7457,
      "step": 309056
    },
    {
      "epoch": 0.0011217416947317423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 309088
    },
    {
      "epoch": 0.001121857829082579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 309120
    },
    {
      "epoch": 0.0011219739634334158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 309152
    },
    {
      "epoch": 0.0011220900977842523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 309184
    },
    {
      "epoch": 0.001122206232135089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 309216
    },
    {
      "epoch": 0.0011223223664859258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 309248
    },
    {
      "epoch": 0.0011224385008367626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 309280
    },
    {
      "epoch": 0.0011225546351875991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.74,
      "step": 309312
    },
    {
      "epoch": 0.0011226707695384359,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6908,
      "step": 309344
    },
    {
      "epoch": 0.0011227869038892726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 309376
    },
    {
      "epoch": 0.0011229030382401094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 309408
    },
    {
      "epoch": 0.0011230191725909461,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 309440
    },
    {
      "epoch": 0.0011231353069417827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 309472
    },
    {
      "epoch": 0.0011232514412926194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7496,
      "step": 309504
    },
    {
      "epoch": 0.0011233675756434562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 309536
    },
    {
      "epoch": 0.001123483709994293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 309568
    },
    {
      "epoch": 0.0011235998443451295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 309600
    },
    {
      "epoch": 0.0011237159786959662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 309632
    },
    {
      "epoch": 0.001123832113046803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 309664
    },
    {
      "epoch": 0.0011239482473976397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 309696
    },
    {
      "epoch": 0.0011240643817484765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 309728
    },
    {
      "epoch": 0.001124180516099313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 309760
    },
    {
      "epoch": 0.0011242966504501498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7452,
      "step": 309792
    },
    {
      "epoch": 0.0011244127848009865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 309824
    },
    {
      "epoch": 0.0011245289191518233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 309856
    },
    {
      "epoch": 0.0011246450535026598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 309888
    },
    {
      "epoch": 0.0011247611878534966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 309920
    },
    {
      "epoch": 0.0011248773222043333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 309952
    },
    {
      "epoch": 0.00112499345655517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 309984
    },
    {
      "epoch": 0.0011251095909060069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 310016
    },
    {
      "epoch": 0.0011252257252568434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7507,
      "step": 310048
    },
    {
      "epoch": 0.0011253418596076801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 310080
    },
    {
      "epoch": 0.001125457993958517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 310112
    },
    {
      "epoch": 0.0011255741283093537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 310144
    },
    {
      "epoch": 0.0011256902626601902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 310176
    },
    {
      "epoch": 0.001125806397011027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 310208
    },
    {
      "epoch": 0.0011259225313618637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 310240
    },
    {
      "epoch": 0.0011260386657127005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 310272
    },
    {
      "epoch": 0.0011261548000635372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 310304
    },
    {
      "epoch": 0.0011262709344143737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7453,
      "step": 310336
    },
    {
      "epoch": 0.0011263870687652105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 310368
    },
    {
      "epoch": 0.0011265032031160473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 310400
    },
    {
      "epoch": 0.001126619337466884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 310432
    },
    {
      "epoch": 0.0011267354718177205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 310464
    },
    {
      "epoch": 0.0011268516061685573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 310496
    },
    {
      "epoch": 0.001126967740519394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 310528
    },
    {
      "epoch": 0.0011270838748702308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 310560
    },
    {
      "epoch": 0.0011272000092210676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 310592
    },
    {
      "epoch": 0.001127316143571904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 310624
    },
    {
      "epoch": 0.0011274322779227409,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 310656
    },
    {
      "epoch": 0.0011275484122735776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 310688
    },
    {
      "epoch": 0.0011276645466244144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 310720
    },
    {
      "epoch": 0.001127780680975251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7494,
      "step": 310752
    },
    {
      "epoch": 0.0011278968153260877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 310784
    },
    {
      "epoch": 0.0011280129496769244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 310816
    },
    {
      "epoch": 0.0011281290840277612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 310848
    },
    {
      "epoch": 0.001128245218378598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 310880
    },
    {
      "epoch": 0.0011283613527294345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 310912
    },
    {
      "epoch": 0.0011284774870802712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 310944
    },
    {
      "epoch": 0.001128593621431108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 310976
    },
    {
      "epoch": 0.0011287097557819447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 311008
    },
    {
      "epoch": 0.0011288258901327813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 311040
    },
    {
      "epoch": 0.001128942024483618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 311072
    },
    {
      "epoch": 0.0011290581588344548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 311104
    },
    {
      "epoch": 0.0011291742931852915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 311136
    },
    {
      "epoch": 0.0011292904275361283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 311168
    },
    {
      "epoch": 0.0011294065618869648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 311200
    },
    {
      "epoch": 0.0011295226962378016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 311232
    },
    {
      "epoch": 0.0011296388305886383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 311264
    },
    {
      "epoch": 0.001129754964939475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 311296
    },
    {
      "epoch": 0.0011298710992903116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 311328
    },
    {
      "epoch": 0.0011299872336411484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 311360
    },
    {
      "epoch": 0.0011301033679919851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 311392
    },
    {
      "epoch": 0.0011302195023428219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 311424
    },
    {
      "epoch": 0.0011303356366936586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 311456
    },
    {
      "epoch": 0.0011304517710444952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 311488
    },
    {
      "epoch": 0.001130567905395332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 311520
    },
    {
      "epoch": 0.0011306840397461687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 311552
    },
    {
      "epoch": 0.0011308001740970054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 311584
    },
    {
      "epoch": 0.001130916308447842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 311616
    },
    {
      "epoch": 0.0011310324427986787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 311648
    },
    {
      "epoch": 0.0011311485771495155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 311680
    },
    {
      "epoch": 0.0011312647115003522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 311712
    },
    {
      "epoch": 0.001131380845851189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 311744
    },
    {
      "epoch": 0.0011314969802020255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 311776
    },
    {
      "epoch": 0.0011316131145528623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 311808
    },
    {
      "epoch": 0.001131729248903699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 311840
    },
    {
      "epoch": 0.0011318453832545358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 311872
    },
    {
      "epoch": 0.0011319615176053723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 311904
    },
    {
      "epoch": 0.001132077651956209,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 311936
    },
    {
      "epoch": 0.0011321937863070458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7449,
      "step": 311968
    },
    {
      "epoch": 0.0011323099206578826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 312000
    },
    {
      "epoch": 0.0011324260550087193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 312032
    },
    {
      "epoch": 0.0011325421893595559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 312064
    },
    {
      "epoch": 0.0011326583237103926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 312096
    },
    {
      "epoch": 0.0011327744580612294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 312128
    },
    {
      "epoch": 0.0011328905924120661,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7521,
      "step": 312160
    },
    {
      "epoch": 0.0011330067267629027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 312192
    },
    {
      "epoch": 0.0011331228611137394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 312224
    },
    {
      "epoch": 0.0011332389954645762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 312256
    },
    {
      "epoch": 0.001133355129815413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 312288
    },
    {
      "epoch": 0.0011334712641662497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 312320
    },
    {
      "epoch": 0.0011335873985170862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 312352
    },
    {
      "epoch": 0.001133703532867923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 312384
    },
    {
      "epoch": 0.0011338196672187597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 312416
    },
    {
      "epoch": 0.0011339358015695965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 312448
    },
    {
      "epoch": 0.001134051935920433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 312480
    },
    {
      "epoch": 0.0011341680702712698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 312512
    },
    {
      "epoch": 0.0011342842046221065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 312544
    },
    {
      "epoch": 0.0011344003389729433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7286,
      "step": 312576
    },
    {
      "epoch": 0.0011345164733237798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 312608
    },
    {
      "epoch": 0.0011346326076746166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 312640
    },
    {
      "epoch": 0.0011347487420254533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 312672
    },
    {
      "epoch": 0.00113486487637629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 312704
    },
    {
      "epoch": 0.0011349810107271268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 312736
    },
    {
      "epoch": 0.0011350971450779634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 312768
    },
    {
      "epoch": 0.0011352132794288001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 312800
    },
    {
      "epoch": 0.0011353294137796369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.74,
      "step": 312832
    },
    {
      "epoch": 0.0011354455481304736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7569,
      "step": 312864
    },
    {
      "epoch": 0.0011355616824813102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 312896
    },
    {
      "epoch": 0.001135677816832147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 312928
    },
    {
      "epoch": 0.0011357939511829837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 312960
    },
    {
      "epoch": 0.0011359100855338204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 312992
    },
    {
      "epoch": 0.0011360262198846572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7684,
      "step": 313024
    },
    {
      "epoch": 0.0011361423542354937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.757,
      "step": 313056
    },
    {
      "epoch": 0.0011362584885863305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 313088
    },
    {
      "epoch": 0.0011363746229371672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 313120
    },
    {
      "epoch": 0.001136490757288004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 313152
    },
    {
      "epoch": 0.0011366068916388405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 313184
    },
    {
      "epoch": 0.0011367230259896773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 313216
    },
    {
      "epoch": 0.001136839160340514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 313248
    },
    {
      "epoch": 0.0011369552946913508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 313280
    },
    {
      "epoch": 0.0011370714290421875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 313312
    },
    {
      "epoch": 0.001137187563393024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 313344
    },
    {
      "epoch": 0.0011373036977438608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 313376
    },
    {
      "epoch": 0.0011374198320946976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 313408
    },
    {
      "epoch": 0.0011375359664455343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 313440
    },
    {
      "epoch": 0.0011376521007963709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 313472
    },
    {
      "epoch": 0.0011377682351472076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 313504
    },
    {
      "epoch": 0.0011378843694980444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 313536
    },
    {
      "epoch": 0.0011380005038488811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 313568
    },
    {
      "epoch": 0.001138116638199718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 313600
    },
    {
      "epoch": 0.0011382327725505544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 313632
    },
    {
      "epoch": 0.0011383489069013912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 313664
    },
    {
      "epoch": 0.001138465041252228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 313696
    },
    {
      "epoch": 0.0011385811756030647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 313728
    },
    {
      "epoch": 0.0011386973099539012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 313760
    },
    {
      "epoch": 0.001138813444304738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 313792
    },
    {
      "epoch": 0.0011389295786555747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 313824
    },
    {
      "epoch": 0.0011390457130064115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 313856
    },
    {
      "epoch": 0.0011391618473572483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 313888
    },
    {
      "epoch": 0.0011392779817080848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 313920
    },
    {
      "epoch": 0.0011393941160589215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 313952
    },
    {
      "epoch": 0.0011395102504097583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 313984
    },
    {
      "epoch": 0.001139626384760595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 314016
    },
    {
      "epoch": 0.0011397425191114316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 314048
    },
    {
      "epoch": 0.0011398586534622683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 314080
    },
    {
      "epoch": 0.001139974787813105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 314112
    },
    {
      "epoch": 0.0011400909221639418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 314144
    },
    {
      "epoch": 0.0011402070565147786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 314176
    },
    {
      "epoch": 0.0011403231908656151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 314208
    },
    {
      "epoch": 0.001140439325216452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 314240
    },
    {
      "epoch": 0.0011405554595672886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 314272
    },
    {
      "epoch": 0.0011406715939181254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 314304
    },
    {
      "epoch": 0.001140787728268962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 314336
    },
    {
      "epoch": 0.0011409038626197987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 314368
    },
    {
      "epoch": 0.0011410199969706354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 314400
    },
    {
      "epoch": 0.0011411361313214722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 314432
    },
    {
      "epoch": 0.001141252265672309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 314464
    },
    {
      "epoch": 0.0011413684000231455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 314496
    },
    {
      "epoch": 0.0011414845343739822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 314528
    },
    {
      "epoch": 0.001141600668724819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 314560
    },
    {
      "epoch": 0.0011417168030756558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 314592
    },
    {
      "epoch": 0.0011418329374264923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 314624
    },
    {
      "epoch": 0.001141949071777329,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 314656
    },
    {
      "epoch": 0.0011420652061281658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 314688
    },
    {
      "epoch": 0.0011421813404790026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 314720
    },
    {
      "epoch": 0.0011422974748298393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 314752
    },
    {
      "epoch": 0.0011424136091806758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 314784
    },
    {
      "epoch": 0.0011425297435315126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 314816
    },
    {
      "epoch": 0.0011426458778823494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 314848
    },
    {
      "epoch": 0.0011427620122331861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 314880
    },
    {
      "epoch": 0.0011428781465840226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 314912
    },
    {
      "epoch": 0.0011429942809348594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 314944
    },
    {
      "epoch": 0.0011431104152856962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 314976
    },
    {
      "epoch": 0.001143226549636533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 315008
    },
    {
      "epoch": 0.0011433426839873697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 315040
    },
    {
      "epoch": 0.0011434588183382062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 315072
    },
    {
      "epoch": 0.001143574952689043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 315104
    },
    {
      "epoch": 0.0011436910870398797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 315136
    },
    {
      "epoch": 0.0011438072213907165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 315168
    },
    {
      "epoch": 0.001143923355741553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 315200
    },
    {
      "epoch": 0.0011440394900923898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 315232
    },
    {
      "epoch": 0.0011441556244432265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 315264
    },
    {
      "epoch": 0.0011442717587940633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 315296
    },
    {
      "epoch": 0.0011443878931449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 315328
    },
    {
      "epoch": 0.0011445040274957366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 315360
    },
    {
      "epoch": 0.0011446201618465733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 315392
    },
    {
      "epoch": 0.00114473629619741,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 315424
    },
    {
      "epoch": 0.0011448524305482468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 315456
    },
    {
      "epoch": 0.0011449685648990834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 315488
    },
    {
      "epoch": 0.00114508469924992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 315520
    },
    {
      "epoch": 0.0011452008336007569,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 315552
    },
    {
      "epoch": 0.0011453169679515936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 315584
    },
    {
      "epoch": 0.0011454331023024304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7408,
      "step": 315616
    },
    {
      "epoch": 0.001145549236653267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7683,
      "step": 315648
    },
    {
      "epoch": 0.0011456653710041037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 315680
    },
    {
      "epoch": 0.0011457815053549404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 315712
    },
    {
      "epoch": 0.0011458976397057772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 315744
    },
    {
      "epoch": 0.0011460137740566137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 315776
    },
    {
      "epoch": 0.0011461299084074505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 315808
    },
    {
      "epoch": 0.0011462460427582872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 315840
    },
    {
      "epoch": 0.001146362177109124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 315872
    },
    {
      "epoch": 0.0011464783114599607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 315904
    },
    {
      "epoch": 0.0011465944458107973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 315936
    },
    {
      "epoch": 0.001146710580161634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 315968
    },
    {
      "epoch": 0.0011468267145124708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 316000
    },
    {
      "epoch": 0.0011469428488633075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 316032
    },
    {
      "epoch": 0.001147058983214144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 316064
    },
    {
      "epoch": 0.0011471751175649808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 316096
    },
    {
      "epoch": 0.0011472912519158176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 316128
    },
    {
      "epoch": 0.0011474073862666543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 316160
    },
    {
      "epoch": 0.001147523520617491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 316192
    },
    {
      "epoch": 0.0011476396549683276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 316224
    },
    {
      "epoch": 0.0011477557893191644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 316256
    },
    {
      "epoch": 0.0011478719236700011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 316288
    },
    {
      "epoch": 0.0011479880580208379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 316320
    },
    {
      "epoch": 0.0011481041923716744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 316352
    },
    {
      "epoch": 0.0011482203267225112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 316384
    },
    {
      "epoch": 0.001148336461073348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 316416
    },
    {
      "epoch": 0.0011484525954241847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 316448
    },
    {
      "epoch": 0.0011485687297750214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 316480
    },
    {
      "epoch": 0.001148684864125858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 316512
    },
    {
      "epoch": 0.0011488009984766947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7609,
      "step": 316544
    },
    {
      "epoch": 0.0011489171328275315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 316576
    },
    {
      "epoch": 0.0011490332671783682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 316608
    },
    {
      "epoch": 0.0011491494015292048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 316640
    },
    {
      "epoch": 0.0011492655358800415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 316672
    },
    {
      "epoch": 0.0011493816702308783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 316704
    },
    {
      "epoch": 0.001149497804581715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 316736
    },
    {
      "epoch": 0.0011496139389325518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 316768
    },
    {
      "epoch": 0.0011497300732833883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 316800
    },
    {
      "epoch": 0.001149846207634225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7518,
      "step": 316832
    },
    {
      "epoch": 0.0011499623419850618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 316864
    },
    {
      "epoch": 0.0011500784763358986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 316896
    },
    {
      "epoch": 0.0011501946106867351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 316928
    },
    {
      "epoch": 0.0011503107450375719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 316960
    },
    {
      "epoch": 0.0011504268793884086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 316992
    },
    {
      "epoch": 0.0011505430137392454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 317024
    },
    {
      "epoch": 0.0011506591480900821,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 317056
    },
    {
      "epoch": 0.0011507752824409187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 317088
    },
    {
      "epoch": 0.0011508914167917554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 317120
    },
    {
      "epoch": 0.0011510075511425922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 317152
    },
    {
      "epoch": 0.001151123685493429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 317184
    },
    {
      "epoch": 0.0011512398198442655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 317216
    },
    {
      "epoch": 0.0011513559541951022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 317248
    },
    {
      "epoch": 0.001151472088545939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 317280
    },
    {
      "epoch": 0.0011515882228967757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 317312
    },
    {
      "epoch": 0.0011517043572476125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 317344
    },
    {
      "epoch": 0.001151820491598449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7612,
      "step": 317376
    },
    {
      "epoch": 0.0011519366259492858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7566,
      "step": 317408
    },
    {
      "epoch": 0.0011520527603001225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 317440
    },
    {
      "epoch": 0.0011521688946509593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 317472
    },
    {
      "epoch": 0.0011522850290017958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 317504
    },
    {
      "epoch": 0.0011524011633526326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 317536
    },
    {
      "epoch": 0.0011525172977034693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 317568
    },
    {
      "epoch": 0.001152633432054306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 317600
    },
    {
      "epoch": 0.0011527495664051428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 317632
    },
    {
      "epoch": 0.0011528657007559794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 317664
    },
    {
      "epoch": 0.0011529818351068161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 317696
    },
    {
      "epoch": 0.0011530979694576529,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 317728
    },
    {
      "epoch": 0.0011532141038084896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 317760
    },
    {
      "epoch": 0.0011533302381593262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7459,
      "step": 317792
    },
    {
      "epoch": 0.001153446372510163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7464,
      "step": 317824
    },
    {
      "epoch": 0.0011535625068609997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 317856
    },
    {
      "epoch": 0.0011536786412118364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 317888
    },
    {
      "epoch": 0.0011537947755626732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 317920
    },
    {
      "epoch": 0.0011539109099135097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 317952
    },
    {
      "epoch": 0.0011540270442643465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 317984
    },
    {
      "epoch": 0.0011541431786151832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 318016
    },
    {
      "epoch": 0.00115425931296602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 318048
    },
    {
      "epoch": 0.0011543754473168565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 318080
    },
    {
      "epoch": 0.0011544915816676933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 318112
    },
    {
      "epoch": 0.00115460771601853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 318144
    },
    {
      "epoch": 0.0011547238503693668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 318176
    },
    {
      "epoch": 0.0011548399847202036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 318208
    },
    {
      "epoch": 0.00115495611907104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 318240
    },
    {
      "epoch": 0.0011550722534218768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 318272
    },
    {
      "epoch": 0.0011551883877727136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 318304
    },
    {
      "epoch": 0.0011553045221235504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 318336
    },
    {
      "epoch": 0.0011554206564743869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 318368
    },
    {
      "epoch": 0.0011555367908252236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 318400
    },
    {
      "epoch": 0.0011556529251760604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 318432
    },
    {
      "epoch": 0.0011557690595268972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 318464
    },
    {
      "epoch": 0.001155885193877734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 318496
    },
    {
      "epoch": 0.0011560013282285704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 318528
    },
    {
      "epoch": 0.0011561174625794072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 318560
    },
    {
      "epoch": 0.001156233596930244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 318592
    },
    {
      "epoch": 0.0011563497312810807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 318624
    },
    {
      "epoch": 0.0011564658656319172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 318656
    },
    {
      "epoch": 0.001156581999982754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 318688
    },
    {
      "epoch": 0.0011566981343335907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 318720
    },
    {
      "epoch": 0.0011568142686844275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 318752
    },
    {
      "epoch": 0.0011569304030352643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6873,
      "step": 318784
    },
    {
      "epoch": 0.0011570465373861008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 318816
    },
    {
      "epoch": 0.0011571626717369375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 318848
    },
    {
      "epoch": 0.0011572788060877743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 318880
    },
    {
      "epoch": 0.001157394940438611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 318912
    },
    {
      "epoch": 0.0011575110747894476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 318944
    },
    {
      "epoch": 0.0011576272091402843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 318976
    },
    {
      "epoch": 0.001157743343491121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7616,
      "step": 319008
    },
    {
      "epoch": 0.0011578594778419579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 319040
    },
    {
      "epoch": 0.0011579756121927946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 319072
    },
    {
      "epoch": 0.0011580917465436311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 319104
    },
    {
      "epoch": 0.001158207880894468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 319136
    },
    {
      "epoch": 0.0011583240152453047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 319168
    },
    {
      "epoch": 0.0011584401495961414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 319200
    },
    {
      "epoch": 0.001158556283946978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 319232
    },
    {
      "epoch": 0.0011586724182978147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 319264
    },
    {
      "epoch": 0.0011587885526486515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 319296
    },
    {
      "epoch": 0.0011589046869994882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 319328
    },
    {
      "epoch": 0.001159020821350325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 319360
    },
    {
      "epoch": 0.0011591369557011615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 319392
    },
    {
      "epoch": 0.0011592530900519983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 319424
    },
    {
      "epoch": 0.001159369224402835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 319456
    },
    {
      "epoch": 0.0011594853587536718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 319488
    },
    {
      "epoch": 0.0011596014931045083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 319520
    },
    {
      "epoch": 0.001159717627455345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 319552
    },
    {
      "epoch": 0.0011598337618061818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 319584
    },
    {
      "epoch": 0.0011599498961570186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 319616
    },
    {
      "epoch": 0.0011600660305078553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 319648
    },
    {
      "epoch": 0.0011601821648586919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 319680
    },
    {
      "epoch": 0.0011602982992095286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 319712
    },
    {
      "epoch": 0.0011604144335603654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 319744
    },
    {
      "epoch": 0.0011605305679112021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 319776
    },
    {
      "epoch": 0.0011606467022620387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 319808
    },
    {
      "epoch": 0.0011607628366128754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 319840
    },
    {
      "epoch": 0.0011608789709637122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 319872
    },
    {
      "epoch": 0.001160995105314549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7519,
      "step": 319904
    },
    {
      "epoch": 0.0011611112396653857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 319936
    },
    {
      "epoch": 0.0011612273740162222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 319968
    },
    {
      "epoch": 0.001161343508367059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 320000
    },
    {
      "epoch": 0.0011614596427178957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 320032
    },
    {
      "epoch": 0.0011615757770687325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 320064
    },
    {
      "epoch": 0.001161691911419569,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 320096
    },
    {
      "epoch": 0.0011618080457704058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 320128
    },
    {
      "epoch": 0.0011619241801212425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 320160
    },
    {
      "epoch": 0.0011620403144720793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 320192
    },
    {
      "epoch": 0.001162156448822916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 320224
    },
    {
      "epoch": 0.0011622725831737526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 320256
    },
    {
      "epoch": 0.0011623887175245893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 320288
    },
    {
      "epoch": 0.001162504851875426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 320320
    },
    {
      "epoch": 0.0011626209862262628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7475,
      "step": 320352
    },
    {
      "epoch": 0.0011627371205770994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 320384
    },
    {
      "epoch": 0.0011628532549279361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 320416
    },
    {
      "epoch": 0.0011629693892787729,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 320448
    },
    {
      "epoch": 0.0011630855236296096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 320480
    },
    {
      "epoch": 0.0011632016579804464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 320512
    },
    {
      "epoch": 0.001163317792331283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 320544
    },
    {
      "epoch": 0.0011634339266821197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 320576
    },
    {
      "epoch": 0.0011635500610329564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7702,
      "step": 320608
    },
    {
      "epoch": 0.0011636661953837932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 320640
    },
    {
      "epoch": 0.0011637823297346297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 320672
    },
    {
      "epoch": 0.0011638984640854665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 320704
    },
    {
      "epoch": 0.0011640145984363032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 320736
    },
    {
      "epoch": 0.00116413073278714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 320768
    },
    {
      "epoch": 0.0011642468671379767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 320800
    },
    {
      "epoch": 0.0011643630014888133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 320832
    },
    {
      "epoch": 0.00116447913583965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 320864
    },
    {
      "epoch": 0.0011645952701904868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 320896
    },
    {
      "epoch": 0.0011647114045413235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 320928
    },
    {
      "epoch": 0.00116482753889216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 320960
    },
    {
      "epoch": 0.0011649436732429968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 320992
    },
    {
      "epoch": 0.0011650598075938336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 321024
    },
    {
      "epoch": 0.0011651759419446703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 321056
    },
    {
      "epoch": 0.001165292076295507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 321088
    },
    {
      "epoch": 0.0011654082106463436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 321120
    },
    {
      "epoch": 0.0011655243449971804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 321152
    },
    {
      "epoch": 0.0011656404793480171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 321184
    },
    {
      "epoch": 0.0011657566136988539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 321216
    },
    {
      "epoch": 0.0011658727480496904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 321248
    },
    {
      "epoch": 0.0011659888824005272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 321280
    },
    {
      "epoch": 0.001166105016751364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 321312
    },
    {
      "epoch": 0.0011662211511022007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 321344
    },
    {
      "epoch": 0.0011663372854530374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 321376
    },
    {
      "epoch": 0.001166453419803874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 321408
    },
    {
      "epoch": 0.0011665695541547107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 321440
    },
    {
      "epoch": 0.0011666856885055475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 321472
    },
    {
      "epoch": 0.0011668018228563842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 321504
    },
    {
      "epoch": 0.0011669179572072208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 321536
    },
    {
      "epoch": 0.0011670340915580575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 321568
    },
    {
      "epoch": 0.0011671502259088943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 321600
    },
    {
      "epoch": 0.001167266360259731,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 321632
    },
    {
      "epoch": 0.0011673824946105678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 321664
    },
    {
      "epoch": 0.0011674986289614043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 321696
    },
    {
      "epoch": 0.001167614763312241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 321728
    },
    {
      "epoch": 0.0011677308976630778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 321760
    },
    {
      "epoch": 0.0011678470320139146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 321792
    },
    {
      "epoch": 0.0011679631663647511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 321824
    },
    {
      "epoch": 0.0011680793007155879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 321856
    },
    {
      "epoch": 0.0011681954350664246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 321888
    },
    {
      "epoch": 0.0011683115694172614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 321920
    },
    {
      "epoch": 0.0011684277037680981,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 321952
    },
    {
      "epoch": 0.0011685438381189347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 321984
    },
    {
      "epoch": 0.0011686599724697714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 322016
    },
    {
      "epoch": 0.0011687761068206082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 322048
    },
    {
      "epoch": 0.001168892241171445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 322080
    },
    {
      "epoch": 0.0011690083755222815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 322112
    },
    {
      "epoch": 0.0011691245098731182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 322144
    },
    {
      "epoch": 0.001169240644223955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 322176
    },
    {
      "epoch": 0.0011693567785747917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 322208
    },
    {
      "epoch": 0.0011694729129256285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 322240
    },
    {
      "epoch": 0.001169589047276465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 322272
    },
    {
      "epoch": 0.0011697051816273018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 322304
    },
    {
      "epoch": 0.0011698213159781385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 322336
    },
    {
      "epoch": 0.0011699374503289753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 322368
    },
    {
      "epoch": 0.0011700535846798118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 322400
    },
    {
      "epoch": 0.0011701697190306486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 322432
    },
    {
      "epoch": 0.0011702858533814853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 322464
    },
    {
      "epoch": 0.001170401987732322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 322496
    },
    {
      "epoch": 0.0011705181220831589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 322528
    },
    {
      "epoch": 0.0011706342564339954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 322560
    },
    {
      "epoch": 0.0011707503907848321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 322592
    },
    {
      "epoch": 0.001170866525135669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 322624
    },
    {
      "epoch": 0.0011709826594865057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7656,
      "step": 322656
    },
    {
      "epoch": 0.0011710987938373422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 322688
    },
    {
      "epoch": 0.001171214928188179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 322720
    },
    {
      "epoch": 0.0011713310625390157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 322752
    },
    {
      "epoch": 0.0011714471968898525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 322784
    },
    {
      "epoch": 0.0011715633312406892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 322816
    },
    {
      "epoch": 0.0011716794655915257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 322848
    },
    {
      "epoch": 0.0011717955999423625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 322880
    },
    {
      "epoch": 0.0011719117342931993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 322912
    },
    {
      "epoch": 0.001172027868644036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 322944
    },
    {
      "epoch": 0.0011721440029948725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 322976
    },
    {
      "epoch": 0.0011722601373457093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 323008
    },
    {
      "epoch": 0.001172376271696546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 323040
    },
    {
      "epoch": 0.0011724924060473828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 323072
    },
    {
      "epoch": 0.0011726085403982196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 323104
    },
    {
      "epoch": 0.001172724674749056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 323136
    },
    {
      "epoch": 0.0011728408090998929,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 323168
    },
    {
      "epoch": 0.0011729569434507296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 323200
    },
    {
      "epoch": 0.0011730730778015664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 323232
    },
    {
      "epoch": 0.001173189212152403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 323264
    },
    {
      "epoch": 0.0011733053465032397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 323296
    },
    {
      "epoch": 0.0011734214808540764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7474,
      "step": 323328
    },
    {
      "epoch": 0.0011735376152049132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 323360
    },
    {
      "epoch": 0.00117365374955575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 323392
    },
    {
      "epoch": 0.0011737698839065864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 323424
    },
    {
      "epoch": 0.0011738860182574232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 323456
    },
    {
      "epoch": 0.00117400215260826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 323488
    },
    {
      "epoch": 0.0011741182869590967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 323520
    },
    {
      "epoch": 0.0011742344213099332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 323552
    },
    {
      "epoch": 0.00117435055566077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 323584
    },
    {
      "epoch": 0.0011744666900116068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 323616
    },
    {
      "epoch": 0.0011745828243624435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 323648
    },
    {
      "epoch": 0.0011746989587132803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 323680
    },
    {
      "epoch": 0.0011748150930641168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6843,
      "step": 323712
    },
    {
      "epoch": 0.0011749312274149536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 323744
    },
    {
      "epoch": 0.0011750473617657903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 323776
    },
    {
      "epoch": 0.001175163496116627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 323808
    },
    {
      "epoch": 0.0011752796304674636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 323840
    },
    {
      "epoch": 0.0011753957648183004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 323872
    },
    {
      "epoch": 0.0011755118991691371,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 323904
    },
    {
      "epoch": 0.0011756280335199739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 323936
    },
    {
      "epoch": 0.0011757441678708106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 323968
    },
    {
      "epoch": 0.0011758603022216472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 324000
    },
    {
      "epoch": 0.001175976436572484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 324032
    },
    {
      "epoch": 0.0011760925709233207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 324064
    },
    {
      "epoch": 0.0011762087052741574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 324096
    },
    {
      "epoch": 0.001176324839624994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 324128
    },
    {
      "epoch": 0.0011764409739758307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 324160
    },
    {
      "epoch": 0.0011765571083266675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 324192
    },
    {
      "epoch": 0.0011766732426775042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 324224
    },
    {
      "epoch": 0.001176789377028341,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 324256
    },
    {
      "epoch": 0.0011769055113791775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 324288
    },
    {
      "epoch": 0.0011770216457300143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 324320
    },
    {
      "epoch": 0.001177137780080851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 324352
    },
    {
      "epoch": 0.0011772539144316878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 324384
    },
    {
      "epoch": 0.0011773700487825243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7632,
      "step": 324416
    },
    {
      "epoch": 0.001177486183133361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 324448
    },
    {
      "epoch": 0.0011776023174841978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 324480
    },
    {
      "epoch": 0.0011777184518350346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 324512
    },
    {
      "epoch": 0.0011778345861858713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 324544
    },
    {
      "epoch": 0.0011779507205367079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 324576
    },
    {
      "epoch": 0.0011780668548875446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 324608
    },
    {
      "epoch": 0.0011781829892383814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 324640
    },
    {
      "epoch": 0.0011782991235892181,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 324672
    },
    {
      "epoch": 0.0011784152579400547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 324704
    },
    {
      "epoch": 0.0011785313922908914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 324736
    },
    {
      "epoch": 0.0011786475266417282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 324768
    },
    {
      "epoch": 0.001178763660992565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 324800
    },
    {
      "epoch": 0.0011788797953434017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7693,
      "step": 324832
    },
    {
      "epoch": 0.0011789959296942382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 324864
    },
    {
      "epoch": 0.001179112064045075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6885,
      "step": 324896
    },
    {
      "epoch": 0.0011792281983959117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 324928
    },
    {
      "epoch": 0.0011793443327467485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 324960
    },
    {
      "epoch": 0.001179460467097585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 324992
    },
    {
      "epoch": 0.0011795766014484218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 325024
    },
    {
      "epoch": 0.0011796927357992585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 325056
    },
    {
      "epoch": 0.0011798088701500953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 325088
    },
    {
      "epoch": 0.001179925004500932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 325120
    },
    {
      "epoch": 0.0011800411388517686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 325152
    },
    {
      "epoch": 0.0011801572732026053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 325184
    },
    {
      "epoch": 0.001180273407553442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 325216
    },
    {
      "epoch": 0.0011803895419042788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 325248
    },
    {
      "epoch": 0.0011805056762551154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7406,
      "step": 325280
    },
    {
      "epoch": 0.0011806218106059521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 325312
    },
    {
      "epoch": 0.0011807379449567889,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 325344
    },
    {
      "epoch": 0.0011808540793076256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 325376
    },
    {
      "epoch": 0.0011809702136584624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 325408
    },
    {
      "epoch": 0.001181086348009299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 325440
    },
    {
      "epoch": 0.0011812024823601357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 325472
    },
    {
      "epoch": 0.0011813186167109724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7518,
      "step": 325504
    },
    {
      "epoch": 0.0011814347510618092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 325536
    },
    {
      "epoch": 0.0011815508854126457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 325568
    },
    {
      "epoch": 0.0011816670197634825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 325600
    },
    {
      "epoch": 0.0011817831541143192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 325632
    },
    {
      "epoch": 0.001181899288465156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 325664
    },
    {
      "epoch": 0.0011820154228159927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 325696
    },
    {
      "epoch": 0.0011821315571668293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 325728
    },
    {
      "epoch": 0.001182247691517666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 325760
    },
    {
      "epoch": 0.0011823638258685028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6914,
      "step": 325792
    },
    {
      "epoch": 0.0011824799602193395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 325824
    },
    {
      "epoch": 0.001182596094570176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 325856
    },
    {
      "epoch": 0.0011827122289210128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 325888
    },
    {
      "epoch": 0.0011828283632718496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 325920
    },
    {
      "epoch": 0.0011829444976226863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 325952
    },
    {
      "epoch": 0.001183060631973523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 325984
    },
    {
      "epoch": 0.0011831767663243596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 326016
    },
    {
      "epoch": 0.0011832929006751964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 326048
    },
    {
      "epoch": 0.0011834090350260331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 326080
    },
    {
      "epoch": 0.00118352516937687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 326112
    },
    {
      "epoch": 0.0011836413037277064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 326144
    },
    {
      "epoch": 0.0011837574380785432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 326176
    },
    {
      "epoch": 0.00118387357242938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 326208
    },
    {
      "epoch": 0.0011839897067802167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 326240
    },
    {
      "epoch": 0.0011841058411310534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 326272
    },
    {
      "epoch": 0.00118422197548189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 326304
    },
    {
      "epoch": 0.0011843381098327267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 326336
    },
    {
      "epoch": 0.0011844542441835635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 326368
    },
    {
      "epoch": 0.0011845703785344002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 326400
    },
    {
      "epoch": 0.0011846865128852368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 326432
    },
    {
      "epoch": 0.0011848026472360735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 326464
    },
    {
      "epoch": 0.0011849187815869103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 326496
    },
    {
      "epoch": 0.001185034915937747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 326528
    },
    {
      "epoch": 0.0011851510502885838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 326560
    },
    {
      "epoch": 0.0011852671846394203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7506,
      "step": 326592
    },
    {
      "epoch": 0.001185383318990257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 326624
    },
    {
      "epoch": 0.0011854994533410938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 326656
    },
    {
      "epoch": 0.0011856155876919306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 326688
    },
    {
      "epoch": 0.0011857317220427671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 326720
    },
    {
      "epoch": 0.0011858478563936039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 326752
    },
    {
      "epoch": 0.0011859639907444406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 326784
    },
    {
      "epoch": 0.0011860801250952774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 326816
    },
    {
      "epoch": 0.0011861962594461142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7547,
      "step": 326848
    },
    {
      "epoch": 0.0011863123937969507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 326880
    },
    {
      "epoch": 0.0011864285281477874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 326912
    },
    {
      "epoch": 0.0011865446624986242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 326944
    },
    {
      "epoch": 0.001186660796849461,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7523,
      "step": 326976
    },
    {
      "epoch": 0.0011867769312002975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 327008
    },
    {
      "epoch": 0.0011868930655511342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7558,
      "step": 327040
    },
    {
      "epoch": 0.001187009199901971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 327072
    },
    {
      "epoch": 0.0011871253342528078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 327104
    },
    {
      "epoch": 0.0011872414686036445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 327136
    },
    {
      "epoch": 0.001187357602954481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 327168
    },
    {
      "epoch": 0.0011874737373053178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 327200
    },
    {
      "epoch": 0.0011875898716561546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 327232
    },
    {
      "epoch": 0.0011877060060069913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 327264
    },
    {
      "epoch": 0.0011878221403578278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 327296
    },
    {
      "epoch": 0.0011879382747086646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 327328
    },
    {
      "epoch": 0.0011880544090595014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 327360
    },
    {
      "epoch": 0.001188170543410338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 327392
    },
    {
      "epoch": 0.0011882866777611749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 327424
    },
    {
      "epoch": 0.0011884028121120114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 327456
    },
    {
      "epoch": 0.0011885189464628482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 327488
    },
    {
      "epoch": 0.001188635080813685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 327520
    },
    {
      "epoch": 0.0011887512151645217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 327552
    },
    {
      "epoch": 0.0011888673495153582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 327584
    },
    {
      "epoch": 0.001188983483866195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 327616
    },
    {
      "epoch": 0.0011890996182170317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 327648
    },
    {
      "epoch": 0.0011892157525678685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 327680
    },
    {
      "epoch": 0.0011893318869187052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 327712
    },
    {
      "epoch": 0.0011894480212695418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 327744
    },
    {
      "epoch": 0.0011895641556203785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 327776
    },
    {
      "epoch": 0.0011896802899712153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 327808
    },
    {
      "epoch": 0.001189796424322052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 327840
    },
    {
      "epoch": 0.0011899125586728886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7591,
      "step": 327872
    },
    {
      "epoch": 0.0011900286930237253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7562,
      "step": 327904
    },
    {
      "epoch": 0.001190144827374562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 327936
    },
    {
      "epoch": 0.0011902609617253988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 327968
    },
    {
      "epoch": 0.0011903770960762356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 328000
    },
    {
      "epoch": 0.001190493230427072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 328032
    },
    {
      "epoch": 0.0011906093647779089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 328064
    },
    {
      "epoch": 0.0011907254991287456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 328096
    },
    {
      "epoch": 0.0011908416334795824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 328128
    },
    {
      "epoch": 0.001190957767830419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 328160
    },
    {
      "epoch": 0.0011910739021812557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 328192
    },
    {
      "epoch": 0.0011911900365320924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 328224
    },
    {
      "epoch": 0.0011913061708829292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 328256
    },
    {
      "epoch": 0.001191422305233766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 328288
    },
    {
      "epoch": 0.0011915384395846025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 328320
    },
    {
      "epoch": 0.0011916545739354392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 328352
    },
    {
      "epoch": 0.001191770708286276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 328384
    },
    {
      "epoch": 0.0011918868426371127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 328416
    },
    {
      "epoch": 0.0011920029769879493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 328448
    },
    {
      "epoch": 0.001192119111338786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 328480
    },
    {
      "epoch": 0.0011922352456896228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 328512
    },
    {
      "epoch": 0.0011923513800404595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 328544
    },
    {
      "epoch": 0.0011924675143912963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 328576
    },
    {
      "epoch": 0.0011925836487421328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 328608
    },
    {
      "epoch": 0.0011926997830929696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 328640
    },
    {
      "epoch": 0.0011928159174438063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 328672
    },
    {
      "epoch": 0.001192932051794643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 328704
    },
    {
      "epoch": 0.0011930481861454796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 328736
    },
    {
      "epoch": 0.0011931643204963164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 328768
    },
    {
      "epoch": 0.0011932804548471531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 328800
    },
    {
      "epoch": 0.0011933965891979899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 328832
    },
    {
      "epoch": 0.0011935127235488266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 328864
    },
    {
      "epoch": 0.0011936288578996632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 328896
    },
    {
      "epoch": 0.0011937449922505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 328928
    },
    {
      "epoch": 0.0011938611266013367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 328960
    },
    {
      "epoch": 0.0011939772609521734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 328992
    },
    {
      "epoch": 0.00119409339530301,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 329024
    },
    {
      "epoch": 0.0011942095296538467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 329056
    },
    {
      "epoch": 0.0011943256640046835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 329088
    },
    {
      "epoch": 0.0011944417983555202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 329120
    },
    {
      "epoch": 0.001194557932706357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 329152
    },
    {
      "epoch": 0.0011946740670571935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 329184
    },
    {
      "epoch": 0.0011947902014080303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 329216
    },
    {
      "epoch": 0.001194906335758867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 329248
    },
    {
      "epoch": 0.0011950224701097038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 329280
    },
    {
      "epoch": 0.0011951386044605403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 329312
    },
    {
      "epoch": 0.001195254738811377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 329344
    },
    {
      "epoch": 0.0011953708731622138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 329376
    },
    {
      "epoch": 0.0011954870075130506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6852,
      "step": 329408
    },
    {
      "epoch": 0.0011956031418638873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 329440
    },
    {
      "epoch": 0.0011957192762147239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 329472
    },
    {
      "epoch": 0.0011958354105655606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 329504
    },
    {
      "epoch": 0.0011959515449163974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7496,
      "step": 329536
    },
    {
      "epoch": 0.0011960676792672341,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 329568
    },
    {
      "epoch": 0.0011961838136180707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 329600
    },
    {
      "epoch": 0.0011962999479689074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 329632
    },
    {
      "epoch": 0.0011964160823197442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 329664
    },
    {
      "epoch": 0.001196532216670581,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 329696
    },
    {
      "epoch": 0.0011966483510214177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 329728
    },
    {
      "epoch": 0.0011967644853722542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 329760
    },
    {
      "epoch": 0.001196880619723091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 329792
    },
    {
      "epoch": 0.0011969967540739277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 329824
    },
    {
      "epoch": 0.0011971128884247645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 329856
    },
    {
      "epoch": 0.001197229022775601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 329888
    },
    {
      "epoch": 0.0011973451571264378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 329920
    },
    {
      "epoch": 0.0011974612914772745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 329952
    },
    {
      "epoch": 0.0011975774258281113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 329984
    },
    {
      "epoch": 0.001197693560178948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 330016
    },
    {
      "epoch": 0.0011978096945297846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 330048
    },
    {
      "epoch": 0.0011979258288806213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 330080
    },
    {
      "epoch": 0.001198041963231458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 330112
    },
    {
      "epoch": 0.0011981580975822948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 330144
    },
    {
      "epoch": 0.0011982742319331314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 330176
    },
    {
      "epoch": 0.0011983903662839681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 330208
    },
    {
      "epoch": 0.0011985065006348049,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 330240
    },
    {
      "epoch": 0.0011986226349856416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 330272
    },
    {
      "epoch": 0.0011987387693364784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 330304
    },
    {
      "epoch": 0.001198854903687315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7449,
      "step": 330336
    },
    {
      "epoch": 0.0011989710380381517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.748,
      "step": 330368
    },
    {
      "epoch": 0.0011990871723889884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 330400
    },
    {
      "epoch": 0.0011992033067398252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 330432
    },
    {
      "epoch": 0.0011993194410906617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 330464
    },
    {
      "epoch": 0.0011994355754414985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 330496
    },
    {
      "epoch": 0.0011995517097923352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 330528
    },
    {
      "epoch": 0.001199667844143172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 330560
    },
    {
      "epoch": 0.0011997839784940087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 330592
    },
    {
      "epoch": 0.0011999001128448453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 330624
    },
    {
      "epoch": 0.001200016247195682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 330656
    },
    {
      "epoch": 0.0012001323815465188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 330688
    },
    {
      "epoch": 0.0012002485158973555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 330720
    },
    {
      "epoch": 0.001200364650248192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6911,
      "step": 330752
    },
    {
      "epoch": 0.0012004807845990288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 330784
    },
    {
      "epoch": 0.0012005969189498656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 330816
    },
    {
      "epoch": 0.0012007130533007023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 330848
    },
    {
      "epoch": 0.001200829187651539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 330880
    },
    {
      "epoch": 0.0012009453220023756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 330912
    },
    {
      "epoch": 0.0012010614563532124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 330944
    },
    {
      "epoch": 0.0012011775907040491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 330976
    },
    {
      "epoch": 0.001201293725054886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 331008
    },
    {
      "epoch": 0.0012014098594057224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 331040
    },
    {
      "epoch": 0.0012015259937565592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 331072
    },
    {
      "epoch": 0.001201642128107396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 331104
    },
    {
      "epoch": 0.0012017582624582327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 331136
    },
    {
      "epoch": 0.0012018743968090695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 331168
    },
    {
      "epoch": 0.001201990531159906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 331200
    },
    {
      "epoch": 0.0012021066655107427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 331232
    },
    {
      "epoch": 0.0012022227998615795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 331264
    },
    {
      "epoch": 0.0012023389342124163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 331296
    },
    {
      "epoch": 0.0012024550685632528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 331328
    },
    {
      "epoch": 0.0012025712029140895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 331360
    },
    {
      "epoch": 0.0012026873372649263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 331392
    },
    {
      "epoch": 0.001202803471615763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 331424
    },
    {
      "epoch": 0.0012029196059665998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 331456
    },
    {
      "epoch": 0.0012030357403174363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 331488
    },
    {
      "epoch": 0.001203151874668273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 331520
    },
    {
      "epoch": 0.0012032680090191099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 331552
    },
    {
      "epoch": 0.0012033841433699466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 331584
    },
    {
      "epoch": 0.0012035002777207831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 331616
    },
    {
      "epoch": 0.00120361641207162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 331648
    },
    {
      "epoch": 0.0012037325464224567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 331680
    },
    {
      "epoch": 0.0012038486807732934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 331712
    },
    {
      "epoch": 0.0012039648151241302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 331744
    },
    {
      "epoch": 0.0012040809494749667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 331776
    },
    {
      "epoch": 0.0012041970838258035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 331808
    },
    {
      "epoch": 0.0012043132181766402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 331840
    },
    {
      "epoch": 0.001204429352527477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7447,
      "step": 331872
    },
    {
      "epoch": 0.0012045454868783135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7522,
      "step": 331904
    },
    {
      "epoch": 0.0012046616212291503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 331936
    },
    {
      "epoch": 0.001204777755579987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 331968
    },
    {
      "epoch": 0.0012048938899308238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 332000
    },
    {
      "epoch": 0.0012050100242816605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 332032
    },
    {
      "epoch": 0.001205126158632497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 332064
    },
    {
      "epoch": 0.0012052422929833338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 332096
    },
    {
      "epoch": 0.0012053584273341706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 332128
    },
    {
      "epoch": 0.0012054745616850073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7511,
      "step": 332160
    },
    {
      "epoch": 0.0012055906960358439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7507,
      "step": 332192
    },
    {
      "epoch": 0.0012057068303866806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 332224
    },
    {
      "epoch": 0.0012058229647375174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 332256
    },
    {
      "epoch": 0.0012059390990883541,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 332288
    },
    {
      "epoch": 0.0012060552334391909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7485,
      "step": 332320
    },
    {
      "epoch": 0.0012061713677900274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 332352
    },
    {
      "epoch": 0.0012062875021408642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 332384
    },
    {
      "epoch": 0.001206403636491701,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 332416
    },
    {
      "epoch": 0.0012065197708425377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 332448
    },
    {
      "epoch": 0.0012066359051933742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 332480
    },
    {
      "epoch": 0.001206752039544211,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 332512
    },
    {
      "epoch": 0.0012068681738950477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 332544
    },
    {
      "epoch": 0.0012069843082458845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 332576
    },
    {
      "epoch": 0.0012071004425967212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 332608
    },
    {
      "epoch": 0.0012072165769475578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 332640
    },
    {
      "epoch": 0.0012073327112983945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 332672
    },
    {
      "epoch": 0.0012074488456492313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 332704
    },
    {
      "epoch": 0.001207564980000068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 332736
    },
    {
      "epoch": 0.0012076811143509046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 332768
    },
    {
      "epoch": 0.0012077972487017413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7417,
      "step": 332800
    },
    {
      "epoch": 0.001207913383052578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 332832
    },
    {
      "epoch": 0.0012080295174034148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 332864
    },
    {
      "epoch": 0.0012081456517542516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 332896
    },
    {
      "epoch": 0.0012082617861050881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 332928
    },
    {
      "epoch": 0.0012083779204559249,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 332960
    },
    {
      "epoch": 0.0012084940548067616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 332992
    },
    {
      "epoch": 0.0012086101891575984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 333024
    },
    {
      "epoch": 0.001208726323508435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 333056
    },
    {
      "epoch": 0.0012088424578592717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 333088
    },
    {
      "epoch": 0.0012089585922101084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 333120
    },
    {
      "epoch": 0.0012090747265609452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 333152
    },
    {
      "epoch": 0.001209190860911782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 333184
    },
    {
      "epoch": 0.0012093069952626185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 333216
    },
    {
      "epoch": 0.0012094231296134552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 333248
    },
    {
      "epoch": 0.001209539263964292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 333280
    },
    {
      "epoch": 0.0012096553983151287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 333312
    },
    {
      "epoch": 0.0012097715326659653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 333344
    },
    {
      "epoch": 0.001209887667016802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 333376
    },
    {
      "epoch": 0.0012100038013676388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 333408
    },
    {
      "epoch": 0.0012101199357184755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 333440
    },
    {
      "epoch": 0.0012102360700693123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 333472
    },
    {
      "epoch": 0.0012103522044201488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 333504
    },
    {
      "epoch": 0.0012104683387709856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 333536
    },
    {
      "epoch": 0.0012105844731218223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 333568
    },
    {
      "epoch": 0.001210700607472659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7563,
      "step": 333600
    },
    {
      "epoch": 0.0012108167418234956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 333632
    },
    {
      "epoch": 0.0012109328761743324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 333664
    },
    {
      "epoch": 0.0012110490105251691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 333696
    },
    {
      "epoch": 0.0012111651448760059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 333728
    },
    {
      "epoch": 0.0012112812792268426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 333760
    },
    {
      "epoch": 0.0012113974135776792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 333792
    },
    {
      "epoch": 0.001211513547928516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 333824
    },
    {
      "epoch": 0.0012116296822793527,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 333856
    },
    {
      "epoch": 0.0012117458166301894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 333888
    },
    {
      "epoch": 0.001211861950981026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 333920
    },
    {
      "epoch": 0.0012119780853318627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 333952
    },
    {
      "epoch": 0.0012120942196826995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 333984
    },
    {
      "epoch": 0.0012122103540335362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 334016
    },
    {
      "epoch": 0.0012123264883843728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 334048
    },
    {
      "epoch": 0.0012124426227352095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 334080
    },
    {
      "epoch": 0.0012125587570860463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 334112
    },
    {
      "epoch": 0.001212674891436883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 334144
    },
    {
      "epoch": 0.0012127910257877198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 334176
    },
    {
      "epoch": 0.0012129071601385563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 334208
    },
    {
      "epoch": 0.001213023294489393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 334240
    },
    {
      "epoch": 0.0012131394288402298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 334272
    },
    {
      "epoch": 0.0012132555631910666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 334304
    },
    {
      "epoch": 0.0012133716975419031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 334336
    },
    {
      "epoch": 0.0012134878318927399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 334368
    },
    {
      "epoch": 0.0012136039662435766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 334400
    },
    {
      "epoch": 0.0012137201005944134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 334432
    },
    {
      "epoch": 0.0012138362349452501,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 334464
    },
    {
      "epoch": 0.0012139523692960867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 334496
    },
    {
      "epoch": 0.0012140685036469234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 334528
    },
    {
      "epoch": 0.0012141846379977602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 334560
    },
    {
      "epoch": 0.001214300772348597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 334592
    },
    {
      "epoch": 0.0012144169066994335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 334624
    },
    {
      "epoch": 0.0012145330410502702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 334656
    },
    {
      "epoch": 0.001214649175401107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7551,
      "step": 334688
    },
    {
      "epoch": 0.0012147653097519437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 334720
    },
    {
      "epoch": 0.0012148814441027805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 334752
    },
    {
      "epoch": 0.001214997578453617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 334784
    },
    {
      "epoch": 0.0012151137128044538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 334816
    },
    {
      "epoch": 0.0012152298471552905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 334848
    },
    {
      "epoch": 0.0012153459815061273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 334880
    },
    {
      "epoch": 0.0012154621158569638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 334912
    },
    {
      "epoch": 0.0012155782502078006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7575,
      "step": 334944
    },
    {
      "epoch": 0.0012156943845586373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 334976
    },
    {
      "epoch": 0.001215810518909474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 335008
    },
    {
      "epoch": 0.0012159266532603108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 335040
    },
    {
      "epoch": 0.0012160427876111474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 335072
    },
    {
      "epoch": 0.0012161589219619841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 335104
    },
    {
      "epoch": 0.001216275056312821,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 335136
    },
    {
      "epoch": 0.0012163911906636576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 335168
    },
    {
      "epoch": 0.0012165073250144942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 335200
    },
    {
      "epoch": 0.001216623459365331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 335232
    },
    {
      "epoch": 0.0012167395937161677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 335264
    },
    {
      "epoch": 0.0012168557280670044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 335296
    },
    {
      "epoch": 0.0012169718624178412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 335328
    },
    {
      "epoch": 0.0012170879967686777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 335360
    },
    {
      "epoch": 0.0012172041311195145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 335392
    },
    {
      "epoch": 0.0012173202654703512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 335424
    },
    {
      "epoch": 0.001217436399821188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 335456
    },
    {
      "epoch": 0.0012175525341720245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 335488
    },
    {
      "epoch": 0.0012176686685228613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 335520
    },
    {
      "epoch": 0.001217784802873698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 335552
    },
    {
      "epoch": 0.0012179009372245348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 335584
    },
    {
      "epoch": 0.0012180170715753716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 335616
    },
    {
      "epoch": 0.001218133205926208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 335648
    },
    {
      "epoch": 0.0012182493402770448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 335680
    },
    {
      "epoch": 0.0012183654746278816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 335712
    },
    {
      "epoch": 0.0012184816089787184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 335744
    },
    {
      "epoch": 0.001218597743329555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 335776
    },
    {
      "epoch": 0.0012187138776803916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 335808
    },
    {
      "epoch": 0.0012188300120312284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 335840
    },
    {
      "epoch": 0.0012189461463820652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 335872
    },
    {
      "epoch": 0.001219062280732902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 335904
    },
    {
      "epoch": 0.0012191784150837384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 335936
    },
    {
      "epoch": 0.0012192945494345752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 335968
    },
    {
      "epoch": 0.001219410683785412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 336000
    },
    {
      "epoch": 0.0012195268181362487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 336032
    },
    {
      "epoch": 0.0012196429524870852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 336064
    },
    {
      "epoch": 0.001219759086837922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 336096
    },
    {
      "epoch": 0.0012198752211887588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 336128
    },
    {
      "epoch": 0.0012199913555395955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 336160
    },
    {
      "epoch": 0.0012201074898904323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 336192
    },
    {
      "epoch": 0.0012202236242412688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 336224
    },
    {
      "epoch": 0.0012203397585921056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 336256
    },
    {
      "epoch": 0.0012204558929429423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 336288
    },
    {
      "epoch": 0.001220572027293779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 336320
    },
    {
      "epoch": 0.0012206881616446156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 336352
    },
    {
      "epoch": 0.0012208042959954524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 336384
    },
    {
      "epoch": 0.001220920430346289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 336416
    },
    {
      "epoch": 0.0012210365646971259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 336448
    },
    {
      "epoch": 0.0012211526990479626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 336480
    },
    {
      "epoch": 0.0012212688333987992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 336512
    },
    {
      "epoch": 0.001221384967749636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 336544
    },
    {
      "epoch": 0.0012215011021004727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 336576
    },
    {
      "epoch": 0.0012216172364513094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 336608
    },
    {
      "epoch": 0.001221733370802146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 336640
    },
    {
      "epoch": 0.0012218495051529827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 336672
    },
    {
      "epoch": 0.0012219656395038195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 336704
    },
    {
      "epoch": 0.0012220817738546562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 336736
    },
    {
      "epoch": 0.001222197908205493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 336768
    },
    {
      "epoch": 0.0012223140425563295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 336800
    },
    {
      "epoch": 0.0012224301769071663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7509,
      "step": 336832
    },
    {
      "epoch": 0.001222546311258003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 336864
    },
    {
      "epoch": 0.0012226624456088398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 336896
    },
    {
      "epoch": 0.0012227785799596763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 336928
    },
    {
      "epoch": 0.001222894714310513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 336960
    },
    {
      "epoch": 0.0012230108486613498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 336992
    },
    {
      "epoch": 0.0012231269830121866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 337024
    },
    {
      "epoch": 0.0012232431173630233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 337056
    },
    {
      "epoch": 0.0012233592517138599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 337088
    },
    {
      "epoch": 0.0012234753860646966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.786,
      "step": 337120
    },
    {
      "epoch": 0.0012235915204155334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 337152
    },
    {
      "epoch": 0.0012237076547663701,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 337184
    },
    {
      "epoch": 0.0012238237891172067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 337216
    },
    {
      "epoch": 0.0012239399234680434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 337248
    },
    {
      "epoch": 0.0012240560578188802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 337280
    },
    {
      "epoch": 0.001224172192169717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 337312
    },
    {
      "epoch": 0.0012242883265205537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 337344
    },
    {
      "epoch": 0.0012244044608713902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7539,
      "step": 337376
    },
    {
      "epoch": 0.001224520595222227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 337408
    },
    {
      "epoch": 0.0012246367295730637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 337440
    },
    {
      "epoch": 0.0012247528639239005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 337472
    },
    {
      "epoch": 0.001224868998274737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 337504
    },
    {
      "epoch": 0.0012249851326255738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 337536
    },
    {
      "epoch": 0.0012251012669764105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 337568
    },
    {
      "epoch": 0.0012252174013272473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 337600
    },
    {
      "epoch": 0.001225333535678084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 337632
    },
    {
      "epoch": 0.0012254496700289206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 337664
    },
    {
      "epoch": 0.0012255658043797573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 337696
    },
    {
      "epoch": 0.001225681938730594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 337728
    },
    {
      "epoch": 0.0012257980730814308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 337760
    },
    {
      "epoch": 0.0012259142074322674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 337792
    },
    {
      "epoch": 0.0012260303417831041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 337824
    },
    {
      "epoch": 0.0012261464761339409,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 337856
    },
    {
      "epoch": 0.0012262626104847776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 337888
    },
    {
      "epoch": 0.0012263787448356144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 337920
    },
    {
      "epoch": 0.001226494879186451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 337952
    },
    {
      "epoch": 0.0012266110135372877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 337984
    },
    {
      "epoch": 0.0012267271478881244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 338016
    },
    {
      "epoch": 0.0012268432822389612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 338048
    },
    {
      "epoch": 0.0012269594165897977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 338080
    },
    {
      "epoch": 0.0012270755509406345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 338112
    },
    {
      "epoch": 0.0012271916852914712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 338144
    },
    {
      "epoch": 0.001227307819642308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 338176
    },
    {
      "epoch": 0.0012274239539931447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 338208
    },
    {
      "epoch": 0.0012275400883439813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 338240
    },
    {
      "epoch": 0.001227656222694818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 338272
    },
    {
      "epoch": 0.0012277723570456548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 338304
    },
    {
      "epoch": 0.0012278884913964915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 338336
    },
    {
      "epoch": 0.001228004625747328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 338368
    },
    {
      "epoch": 0.0012281207600981648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 338400
    },
    {
      "epoch": 0.0012282368944490016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 338432
    },
    {
      "epoch": 0.0012283530287998383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 338464
    },
    {
      "epoch": 0.001228469163150675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 338496
    },
    {
      "epoch": 0.0012285852975015116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 338528
    },
    {
      "epoch": 0.0012287014318523484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 338560
    },
    {
      "epoch": 0.0012288175662031851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6888,
      "step": 338592
    },
    {
      "epoch": 0.0012289337005540219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 338624
    },
    {
      "epoch": 0.0012290498349048584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 338656
    },
    {
      "epoch": 0.0012291659692556952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 338688
    },
    {
      "epoch": 0.001229282103606532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 338720
    },
    {
      "epoch": 0.0012293982379573687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7481,
      "step": 338752
    },
    {
      "epoch": 0.0012295143723082054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6888,
      "step": 338784
    },
    {
      "epoch": 0.001229630506659042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 338816
    },
    {
      "epoch": 0.0012297466410098787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 338848
    },
    {
      "epoch": 0.0012298627753607155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 338880
    },
    {
      "epoch": 0.0012299789097115522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 338912
    },
    {
      "epoch": 0.0012300950440623888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 338944
    },
    {
      "epoch": 0.0012302111784132255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 338976
    },
    {
      "epoch": 0.0012303273127640623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 339008
    },
    {
      "epoch": 0.001230443447114899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 339040
    },
    {
      "epoch": 0.0012305595814657358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 339072
    },
    {
      "epoch": 0.0012306757158165723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 339104
    },
    {
      "epoch": 0.001230791850167409,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 339136
    },
    {
      "epoch": 0.0012309079845182458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 339168
    },
    {
      "epoch": 0.0012310241188690826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 339200
    },
    {
      "epoch": 0.0012311402532199191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 339232
    },
    {
      "epoch": 0.0012312563875707559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 339264
    },
    {
      "epoch": 0.0012313725219215926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 339296
    },
    {
      "epoch": 0.0012314886562724294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 339328
    },
    {
      "epoch": 0.0012316047906232662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 339360
    },
    {
      "epoch": 0.0012317209249741027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 339392
    },
    {
      "epoch": 0.0012318370593249394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 339424
    },
    {
      "epoch": 0.0012319531936757762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 339456
    },
    {
      "epoch": 0.001232069328026613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 339488
    },
    {
      "epoch": 0.0012321854623774495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 339520
    },
    {
      "epoch": 0.0012323015967282862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 339552
    },
    {
      "epoch": 0.001232417731079123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 339584
    },
    {
      "epoch": 0.0012325338654299597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 339616
    },
    {
      "epoch": 0.0012326499997807965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 339648
    },
    {
      "epoch": 0.001232766134131633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6891,
      "step": 339680
    },
    {
      "epoch": 0.0012328822684824698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 339712
    },
    {
      "epoch": 0.0012329984028333065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.746,
      "step": 339744
    },
    {
      "epoch": 0.0012331145371841433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 339776
    },
    {
      "epoch": 0.0012332306715349798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 339808
    },
    {
      "epoch": 0.0012333468058858166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 339840
    },
    {
      "epoch": 0.0012334629402366533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 339872
    },
    {
      "epoch": 0.00123357907458749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 339904
    },
    {
      "epoch": 0.0012336952089383269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 339936
    },
    {
      "epoch": 0.0012338113432891634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 339968
    },
    {
      "epoch": 0.0012339274776400001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 340000
    },
    {
      "epoch": 0.001234043611990837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 340032
    },
    {
      "epoch": 0.0012341597463416737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 340064
    },
    {
      "epoch": 0.0012342758806925102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 340096
    },
    {
      "epoch": 0.001234392015043347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 340128
    },
    {
      "epoch": 0.0012345081493941837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 340160
    },
    {
      "epoch": 0.0012346242837450205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7612,
      "step": 340192
    },
    {
      "epoch": 0.0012347404180958572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 340224
    },
    {
      "epoch": 0.0012348565524466937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 340256
    },
    {
      "epoch": 0.0012349726867975305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 340288
    },
    {
      "epoch": 0.0012350888211483673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 340320
    },
    {
      "epoch": 0.001235204955499204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 340352
    },
    {
      "epoch": 0.0012353210898500405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 340384
    },
    {
      "epoch": 0.0012354372242008773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.69,
      "step": 340416
    },
    {
      "epoch": 0.001235553358551714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 340448
    },
    {
      "epoch": 0.0012356694929025508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 340480
    },
    {
      "epoch": 0.0012357856272533876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 340512
    },
    {
      "epoch": 0.001235901761604224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 340544
    },
    {
      "epoch": 0.0012360178959550609,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 340576
    },
    {
      "epoch": 0.0012361340303058976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 340608
    },
    {
      "epoch": 0.0012362501646567344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 340640
    },
    {
      "epoch": 0.001236366299007571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 340672
    },
    {
      "epoch": 0.0012364824333584077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 340704
    },
    {
      "epoch": 0.0012365985677092444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 340736
    },
    {
      "epoch": 0.0012367147020600812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 340768
    },
    {
      "epoch": 0.001236830836410918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 340800
    },
    {
      "epoch": 0.0012369469707617545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 340832
    },
    {
      "epoch": 0.0012370631051125912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 340864
    },
    {
      "epoch": 0.001237179239463428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 340896
    },
    {
      "epoch": 0.0012372953738142647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 340928
    },
    {
      "epoch": 0.0012374115081651013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 340960
    },
    {
      "epoch": 0.001237527642515938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 340992
    },
    {
      "epoch": 0.0012376437768667748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 341024
    },
    {
      "epoch": 0.0012377599112176115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 341056
    },
    {
      "epoch": 0.0012378760455684483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 341088
    },
    {
      "epoch": 0.0012379921799192848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 341120
    },
    {
      "epoch": 0.0012381083142701216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 341152
    },
    {
      "epoch": 0.0012382244486209583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 341184
    },
    {
      "epoch": 0.001238340582971795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 341216
    },
    {
      "epoch": 0.0012384567173226316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 341248
    },
    {
      "epoch": 0.0012385728516734684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 341280
    },
    {
      "epoch": 0.0012386889860243051,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 341312
    },
    {
      "epoch": 0.0012388051203751419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 341344
    },
    {
      "epoch": 0.0012389212547259786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 341376
    },
    {
      "epoch": 0.0012390373890768152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 341408
    },
    {
      "epoch": 0.001239153523427652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 341440
    },
    {
      "epoch": 0.0012392696577784887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 341472
    },
    {
      "epoch": 0.0012393857921293254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 341504
    },
    {
      "epoch": 0.001239501926480162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 341536
    },
    {
      "epoch": 0.0012396180608309987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 341568
    },
    {
      "epoch": 0.0012397341951818355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 341600
    },
    {
      "epoch": 0.0012398503295326722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 341632
    },
    {
      "epoch": 0.001239966463883509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 341664
    },
    {
      "epoch": 0.0012400825982343455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 341696
    },
    {
      "epoch": 0.0012401987325851823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 341728
    },
    {
      "epoch": 0.001240314866936019,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7533,
      "step": 341760
    },
    {
      "epoch": 0.0012404310012868558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 341792
    },
    {
      "epoch": 0.0012405471356376923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 341824
    },
    {
      "epoch": 0.001240663269988529,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 341856
    },
    {
      "epoch": 0.0012407794043393658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 341888
    },
    {
      "epoch": 0.0012408955386902026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 341920
    },
    {
      "epoch": 0.0012410116730410393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7613,
      "step": 341952
    },
    {
      "epoch": 0.0012411278073918759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 341984
    },
    {
      "epoch": 0.0012412439417427126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 342016
    },
    {
      "epoch": 0.0012413600760935494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 342048
    },
    {
      "epoch": 0.0012414762104443861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 342080
    },
    {
      "epoch": 0.0012415923447952227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 342112
    },
    {
      "epoch": 0.0012417084791460594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 342144
    },
    {
      "epoch": 0.0012418246134968962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 342176
    },
    {
      "epoch": 0.001241940747847733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 342208
    },
    {
      "epoch": 0.0012420568821985697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 342240
    },
    {
      "epoch": 0.0012421730165494062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.759,
      "step": 342272
    },
    {
      "epoch": 0.001242289150900243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 342304
    },
    {
      "epoch": 0.0012424052852510797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 342336
    },
    {
      "epoch": 0.0012425214196019165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7463,
      "step": 342368
    },
    {
      "epoch": 0.001242637553952753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 342400
    },
    {
      "epoch": 0.0012427536883035898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 342432
    },
    {
      "epoch": 0.0012428698226544265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 342464
    },
    {
      "epoch": 0.0012429859570052633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 342496
    },
    {
      "epoch": 0.0012431020913561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7428,
      "step": 342528
    },
    {
      "epoch": 0.0012432182257069366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 342560
    },
    {
      "epoch": 0.0012433343600577733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 342592
    },
    {
      "epoch": 0.00124345049440861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.758,
      "step": 342624
    },
    {
      "epoch": 0.0012435666287594468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 342656
    },
    {
      "epoch": 0.0012436827631102834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7524,
      "step": 342688
    },
    {
      "epoch": 0.0012437988974611201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 342720
    },
    {
      "epoch": 0.0012439150318119569,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 342752
    },
    {
      "epoch": 0.0012440311661627936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 342784
    },
    {
      "epoch": 0.0012441473005136304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7595,
      "step": 342816
    },
    {
      "epoch": 0.001244263434864467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 342848
    },
    {
      "epoch": 0.0012443795692153037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 342880
    },
    {
      "epoch": 0.0012444957035661404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 342912
    },
    {
      "epoch": 0.0012446118379169772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 342944
    },
    {
      "epoch": 0.0012447279722678137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 342976
    },
    {
      "epoch": 0.0012448441066186505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 343008
    },
    {
      "epoch": 0.0012449602409694872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 343040
    },
    {
      "epoch": 0.001245076375320324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 343072
    },
    {
      "epoch": 0.0012451925096711607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 343104
    },
    {
      "epoch": 0.0012453086440219973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 343136
    },
    {
      "epoch": 0.001245424778372834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 343168
    },
    {
      "epoch": 0.0012455409127236708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 343200
    },
    {
      "epoch": 0.0012456570470745075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 343232
    },
    {
      "epoch": 0.001245773181425344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 343264
    },
    {
      "epoch": 0.0012458893157761808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 343296
    },
    {
      "epoch": 0.0012460054501270176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 343328
    },
    {
      "epoch": 0.0012461215844778543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 343360
    },
    {
      "epoch": 0.001246237718828691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 343392
    },
    {
      "epoch": 0.0012463538531795276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6865,
      "step": 343424
    },
    {
      "epoch": 0.0012464699875303644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 343456
    },
    {
      "epoch": 0.0012465861218812011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 343488
    },
    {
      "epoch": 0.001246702256232038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 343520
    },
    {
      "epoch": 0.0012468183905828744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 343552
    },
    {
      "epoch": 0.0012469345249337112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 343584
    },
    {
      "epoch": 0.001247050659284548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 343616
    },
    {
      "epoch": 0.0012471667936353847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 343648
    },
    {
      "epoch": 0.0012472829279862215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 343680
    },
    {
      "epoch": 0.001247399062337058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 343712
    },
    {
      "epoch": 0.0012475151966878947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 343744
    },
    {
      "epoch": 0.0012476313310387315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 343776
    },
    {
      "epoch": 0.0012477474653895683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 343808
    },
    {
      "epoch": 0.0012478635997404048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 343840
    },
    {
      "epoch": 0.0012479797340912415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 343872
    },
    {
      "epoch": 0.0012480958684420783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 343904
    },
    {
      "epoch": 0.001248212002792915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 343936
    },
    {
      "epoch": 0.0012483281371437518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 343968
    },
    {
      "epoch": 0.0012484442714945883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 344000
    },
    {
      "epoch": 0.001248560405845425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 344032
    },
    {
      "epoch": 0.0012486765401962619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 344064
    },
    {
      "epoch": 0.0012487926745470986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 344096
    },
    {
      "epoch": 0.0012489088088979351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 344128
    },
    {
      "epoch": 0.001249024943248772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 344160
    },
    {
      "epoch": 0.0012491410775996086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6839,
      "step": 344192
    },
    {
      "epoch": 0.0012492572119504454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 344224
    },
    {
      "epoch": 0.0012493733463012822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 344256
    },
    {
      "epoch": 0.0012494894806521187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 344288
    },
    {
      "epoch": 0.0012496056150029554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 344320
    },
    {
      "epoch": 0.0012497217493537922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 344352
    },
    {
      "epoch": 0.001249837883704629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 344384
    },
    {
      "epoch": 0.0012499540180554655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 344416
    },
    {
      "epoch": 0.0012500701524063022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 344448
    },
    {
      "epoch": 0.001250186286757139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 344480
    },
    {
      "epoch": 0.0012503024211079758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 344512
    },
    {
      "epoch": 0.0012504185554588125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 344544
    },
    {
      "epoch": 0.001250534689809649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.776,
      "step": 344576
    },
    {
      "epoch": 0.0012506508241604858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 344608
    },
    {
      "epoch": 0.0012507669585113226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 344640
    },
    {
      "epoch": 0.0012508830928621593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 344672
    },
    {
      "epoch": 0.0012509992272129958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 344704
    },
    {
      "epoch": 0.0012511153615638326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 344736
    },
    {
      "epoch": 0.0012512314959146694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 344768
    },
    {
      "epoch": 0.0012513476302655061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 344800
    },
    {
      "epoch": 0.0012514637646163429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 344832
    },
    {
      "epoch": 0.0012515798989671794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 344864
    },
    {
      "epoch": 0.0012516960333180162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 344896
    },
    {
      "epoch": 0.001251812167668853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 344928
    },
    {
      "epoch": 0.0012519283020196897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 344960
    },
    {
      "epoch": 0.0012520444363705262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7572,
      "step": 344992
    },
    {
      "epoch": 0.001252160570721363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 345024
    },
    {
      "epoch": 0.0012522767050721997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 345056
    },
    {
      "epoch": 0.0012523928394230365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 345088
    },
    {
      "epoch": 0.0012525089737738732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 345120
    },
    {
      "epoch": 0.0012526251081247098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 345152
    },
    {
      "epoch": 0.0012527412424755465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 345184
    },
    {
      "epoch": 0.0012528573768263833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 345216
    },
    {
      "epoch": 0.00125297351117722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7742,
      "step": 345248
    },
    {
      "epoch": 0.0012530896455280566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7419,
      "step": 345280
    },
    {
      "epoch": 0.0012532057798788933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 345312
    },
    {
      "epoch": 0.00125332191422973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 345344
    },
    {
      "epoch": 0.0012534380485805668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 345376
    },
    {
      "epoch": 0.0012535541829314036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 345408
    },
    {
      "epoch": 0.0012536703172822401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 345440
    },
    {
      "epoch": 0.0012537864516330769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 345472
    },
    {
      "epoch": 0.0012539025859839136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 345504
    },
    {
      "epoch": 0.0012540187203347504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 345536
    },
    {
      "epoch": 0.001254134854685587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 345568
    },
    {
      "epoch": 0.0012542509890364237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 345600
    },
    {
      "epoch": 0.0012543671233872604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6922,
      "step": 345632
    },
    {
      "epoch": 0.0012544832577380972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 345664
    },
    {
      "epoch": 0.001254599392088934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 345696
    },
    {
      "epoch": 0.0012547155264397705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 345728
    },
    {
      "epoch": 0.0012548316607906072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 345760
    },
    {
      "epoch": 0.001254947795141444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 345792
    },
    {
      "epoch": 0.0012550639294922807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 345824
    },
    {
      "epoch": 0.0012551800638431173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 345856
    },
    {
      "epoch": 0.001255296198193954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 345888
    },
    {
      "epoch": 0.0012554123325447908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 345920
    },
    {
      "epoch": 0.0012555284668956275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 345952
    },
    {
      "epoch": 0.0012556446012464643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 345984
    },
    {
      "epoch": 0.0012557607355973008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 346016
    },
    {
      "epoch": 0.0012558768699481376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 346048
    },
    {
      "epoch": 0.0012559930042989743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 346080
    },
    {
      "epoch": 0.001256109138649811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 346112
    },
    {
      "epoch": 0.0012562252730006476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 346144
    },
    {
      "epoch": 0.0012563414073514844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 346176
    },
    {
      "epoch": 0.0012564575417023211,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 346208
    },
    {
      "epoch": 0.0012565736760531579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 346240
    },
    {
      "epoch": 0.0012566898104039946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 346272
    },
    {
      "epoch": 0.0012568059447548312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 346304
    },
    {
      "epoch": 0.001256922079105668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7596,
      "step": 346336
    },
    {
      "epoch": 0.0012570382134565047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 346368
    },
    {
      "epoch": 0.0012571543478073414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 346400
    },
    {
      "epoch": 0.001257270482158178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 346432
    },
    {
      "epoch": 0.0012573866165090147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 346464
    },
    {
      "epoch": 0.0012575027508598515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 346496
    },
    {
      "epoch": 0.0012576188852106882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 346528
    },
    {
      "epoch": 0.001257735019561525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 346560
    },
    {
      "epoch": 0.0012578511539123615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 346592
    },
    {
      "epoch": 0.0012579672882631983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 346624
    },
    {
      "epoch": 0.001258083422614035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 346656
    },
    {
      "epoch": 0.0012581995569648718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 346688
    },
    {
      "epoch": 0.0012583156913157083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 346720
    },
    {
      "epoch": 0.001258431825666545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 346752
    },
    {
      "epoch": 0.0012585479600173818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 346784
    },
    {
      "epoch": 0.0012586640943682186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 346816
    },
    {
      "epoch": 0.0012587802287190553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 346848
    },
    {
      "epoch": 0.0012588963630698919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 346880
    },
    {
      "epoch": 0.0012590124974207286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 346912
    },
    {
      "epoch": 0.0012591286317715654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 346944
    },
    {
      "epoch": 0.0012592447661224021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 346976
    },
    {
      "epoch": 0.0012593609004732387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7498,
      "step": 347008
    },
    {
      "epoch": 0.0012594770348240754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 347040
    },
    {
      "epoch": 0.0012595931691749122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 347072
    },
    {
      "epoch": 0.001259709303525749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 347104
    },
    {
      "epoch": 0.0012598254378765857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 347136
    },
    {
      "epoch": 0.0012599415722274222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 347168
    },
    {
      "epoch": 0.001260057706578259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7519,
      "step": 347200
    },
    {
      "epoch": 0.0012601738409290957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 347232
    },
    {
      "epoch": 0.0012602899752799325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 347264
    },
    {
      "epoch": 0.001260406109630769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 347296
    },
    {
      "epoch": 0.0012605222439816058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 347328
    },
    {
      "epoch": 0.0012606383783324425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 347360
    },
    {
      "epoch": 0.0012607545126832793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7495,
      "step": 347392
    },
    {
      "epoch": 0.001260870647034116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 347424
    },
    {
      "epoch": 0.0012609867813849526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 347456
    },
    {
      "epoch": 0.0012611029157357893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 347488
    },
    {
      "epoch": 0.001261219050086626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 347520
    },
    {
      "epoch": 0.0012613351844374628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 347552
    },
    {
      "epoch": 0.0012614513187882994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 347584
    },
    {
      "epoch": 0.0012615674531391361,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7512,
      "step": 347616
    },
    {
      "epoch": 0.0012616835874899729,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 347648
    },
    {
      "epoch": 0.0012617997218408096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 347680
    },
    {
      "epoch": 0.0012619158561916464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 347712
    },
    {
      "epoch": 0.001262031990542483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 347744
    },
    {
      "epoch": 0.0012621481248933197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 347776
    },
    {
      "epoch": 0.0012622642592441564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 347808
    },
    {
      "epoch": 0.0012623803935949932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 347840
    },
    {
      "epoch": 0.0012624965279458297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 347872
    },
    {
      "epoch": 0.0012626126622966665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 347904
    },
    {
      "epoch": 0.0012627287966475032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 347936
    },
    {
      "epoch": 0.00126284493099834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 347968
    },
    {
      "epoch": 0.0012629610653491768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 348000
    },
    {
      "epoch": 0.0012630771997000133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 348032
    },
    {
      "epoch": 0.00126319333405085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 348064
    },
    {
      "epoch": 0.0012633094684016868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7562,
      "step": 348096
    },
    {
      "epoch": 0.0012634256027525236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 348128
    },
    {
      "epoch": 0.00126354173710336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 348160
    },
    {
      "epoch": 0.0012636578714541968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 348192
    },
    {
      "epoch": 0.0012637740058050336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 348224
    },
    {
      "epoch": 0.0012638901401558704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 348256
    },
    {
      "epoch": 0.001264006274506707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 348288
    },
    {
      "epoch": 0.0012641224088575436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 348320
    },
    {
      "epoch": 0.0012642385432083804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 348352
    },
    {
      "epoch": 0.0012643546775592172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 348384
    },
    {
      "epoch": 0.001264470811910054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 348416
    },
    {
      "epoch": 0.0012645869462608904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 348448
    },
    {
      "epoch": 0.0012647030806117272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 348480
    },
    {
      "epoch": 0.001264819214962564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 348512
    },
    {
      "epoch": 0.0012649353493134007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 348544
    },
    {
      "epoch": 0.0012650514836642375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 348576
    },
    {
      "epoch": 0.001265167618015074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 348608
    },
    {
      "epoch": 0.0012652837523659108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 348640
    },
    {
      "epoch": 0.0012653998867167475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 348672
    },
    {
      "epoch": 0.0012655160210675843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 348704
    },
    {
      "epoch": 0.0012656321554184208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 348736
    },
    {
      "epoch": 0.0012657482897692576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7553,
      "step": 348768
    },
    {
      "epoch": 0.0012658644241200943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 348800
    },
    {
      "epoch": 0.001265980558470931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 348832
    },
    {
      "epoch": 0.0012660966928217678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 348864
    },
    {
      "epoch": 0.0012662128271726044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 348896
    },
    {
      "epoch": 0.001266328961523441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 348928
    },
    {
      "epoch": 0.0012664450958742779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 348960
    },
    {
      "epoch": 0.0012665612302251146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 348992
    },
    {
      "epoch": 0.0012666773645759511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 349024
    },
    {
      "epoch": 0.001266793498926788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 349056
    },
    {
      "epoch": 0.0012669096332776247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 349088
    },
    {
      "epoch": 0.0012670257676284614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 349120
    },
    {
      "epoch": 0.0012671419019792982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 349152
    },
    {
      "epoch": 0.0012672580363301347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 349184
    },
    {
      "epoch": 0.0012673741706809715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 349216
    },
    {
      "epoch": 0.0012674903050318082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 349248
    },
    {
      "epoch": 0.001267606439382645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 349280
    },
    {
      "epoch": 0.0012677225737334815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 349312
    },
    {
      "epoch": 0.0012678387080843183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 349344
    },
    {
      "epoch": 0.001267954842435155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 349376
    },
    {
      "epoch": 0.0012680709767859918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 349408
    },
    {
      "epoch": 0.0012681871111368285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 349440
    },
    {
      "epoch": 0.001268303245487665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 349472
    },
    {
      "epoch": 0.0012684193798385018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7501,
      "step": 349504
    },
    {
      "epoch": 0.0012685355141893386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 349536
    },
    {
      "epoch": 0.0012686516485401753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 349568
    },
    {
      "epoch": 0.0012687677828910119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 349600
    },
    {
      "epoch": 0.0012688839172418486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 349632
    },
    {
      "epoch": 0.0012690000515926854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 349664
    },
    {
      "epoch": 0.0012691161859435221,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 349696
    },
    {
      "epoch": 0.0012692323202943589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 349728
    },
    {
      "epoch": 0.0012693484546451954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 349760
    },
    {
      "epoch": 0.0012694645889960322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 349792
    },
    {
      "epoch": 0.001269580723346869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 349824
    },
    {
      "epoch": 0.0012696968576977057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 349856
    },
    {
      "epoch": 0.0012698129920485422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 349888
    },
    {
      "epoch": 0.001269929126399379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 349920
    },
    {
      "epoch": 0.0012700452607502157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 349952
    },
    {
      "epoch": 0.0012701613951010525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 349984
    },
    {
      "epoch": 0.0012702775294518892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 350016
    },
    {
      "epoch": 0.0012703936638027258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 350048
    },
    {
      "epoch": 0.0012705097981535625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 350080
    },
    {
      "epoch": 0.0012706259325043993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 350112
    },
    {
      "epoch": 0.001270742066855236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 350144
    },
    {
      "epoch": 0.0012708582012060726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 350176
    },
    {
      "epoch": 0.0012709743355569093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7481,
      "step": 350208
    },
    {
      "epoch": 0.001271090469907746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 350240
    },
    {
      "epoch": 0.0012712066042585828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 350272
    },
    {
      "epoch": 0.0012713227386094196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 350304
    },
    {
      "epoch": 0.0012714388729602561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6918,
      "step": 350336
    },
    {
      "epoch": 0.0012715550073110929,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 350368
    },
    {
      "epoch": 0.0012716711416619296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 350400
    },
    {
      "epoch": 0.0012717872760127664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 350432
    },
    {
      "epoch": 0.001271903410363603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 350464
    },
    {
      "epoch": 0.0012720195447144397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 350496
    },
    {
      "epoch": 0.0012721356790652764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 350528
    },
    {
      "epoch": 0.0012722518134161132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 350560
    },
    {
      "epoch": 0.00127236794776695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 350592
    },
    {
      "epoch": 0.0012724840821177865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 350624
    },
    {
      "epoch": 0.0012726002164686232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7431,
      "step": 350656
    },
    {
      "epoch": 0.00127271635081946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 350688
    },
    {
      "epoch": 0.0012728324851702967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 350720
    },
    {
      "epoch": 0.0012729486195211333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 350752
    },
    {
      "epoch": 0.00127306475387197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 350784
    },
    {
      "epoch": 0.0012731808882228068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 350816
    },
    {
      "epoch": 0.0012732970225736435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 350848
    },
    {
      "epoch": 0.0012734131569244803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 350880
    },
    {
      "epoch": 0.0012735292912753168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 350912
    },
    {
      "epoch": 0.0012736454256261536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 350944
    },
    {
      "epoch": 0.0012737615599769903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 350976
    },
    {
      "epoch": 0.001273877694327827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 351008
    },
    {
      "epoch": 0.0012739938286786636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 351040
    },
    {
      "epoch": 0.0012741099630295004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 351072
    },
    {
      "epoch": 0.0012742260973803371,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7504,
      "step": 351104
    },
    {
      "epoch": 0.0012743422317311739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 351136
    },
    {
      "epoch": 0.0012744583660820106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 351168
    },
    {
      "epoch": 0.0012745745004328472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 351200
    },
    {
      "epoch": 0.001274690634783684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 351232
    },
    {
      "epoch": 0.0012748067691345207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 351264
    },
    {
      "epoch": 0.0012749229034853574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 351296
    },
    {
      "epoch": 0.001275039037836194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 351328
    },
    {
      "epoch": 0.0012751551721870307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 351360
    },
    {
      "epoch": 0.0012752713065378675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7503,
      "step": 351392
    },
    {
      "epoch": 0.0012753874408887042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 351424
    },
    {
      "epoch": 0.001275503575239541,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 351456
    },
    {
      "epoch": 0.0012756197095903775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 351488
    },
    {
      "epoch": 0.0012757358439412143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 351520
    },
    {
      "epoch": 0.001275851978292051,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 351552
    },
    {
      "epoch": 0.0012759681126428878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 351584
    },
    {
      "epoch": 0.0012760842469937243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 351616
    },
    {
      "epoch": 0.001276200381344561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7483,
      "step": 351648
    },
    {
      "epoch": 0.0012763165156953978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 351680
    },
    {
      "epoch": 0.0012764326500462346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 351712
    },
    {
      "epoch": 0.0012765487843970713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 351744
    },
    {
      "epoch": 0.0012766649187479079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 351776
    },
    {
      "epoch": 0.0012767810530987446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 351808
    },
    {
      "epoch": 0.0012768971874495814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 351840
    },
    {
      "epoch": 0.0012770133218004181,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 351872
    },
    {
      "epoch": 0.0012771294561512547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 351904
    },
    {
      "epoch": 0.0012772455905020914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7512,
      "step": 351936
    },
    {
      "epoch": 0.0012773617248529282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 351968
    },
    {
      "epoch": 0.001277477859203765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 352000
    },
    {
      "epoch": 0.0012775939935546017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 352032
    },
    {
      "epoch": 0.0012777101279054382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 352064
    },
    {
      "epoch": 0.001277826262256275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 352096
    },
    {
      "epoch": 0.0012779423966071117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 352128
    },
    {
      "epoch": 0.0012780585309579485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 352160
    },
    {
      "epoch": 0.001278174665308785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 352192
    },
    {
      "epoch": 0.0012782907996596218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 352224
    },
    {
      "epoch": 0.0012784069340104585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7583,
      "step": 352256
    },
    {
      "epoch": 0.0012785230683612953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 352288
    },
    {
      "epoch": 0.001278639202712132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 352320
    },
    {
      "epoch": 0.0012787553370629686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 352352
    },
    {
      "epoch": 0.0012788714714138053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 352384
    },
    {
      "epoch": 0.001278987605764642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 352416
    },
    {
      "epoch": 0.0012791037401154789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 352448
    },
    {
      "epoch": 0.0012792198744663154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7587,
      "step": 352480
    },
    {
      "epoch": 0.0012793360088171521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 352512
    },
    {
      "epoch": 0.001279452143167989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7601,
      "step": 352544
    },
    {
      "epoch": 0.0012795682775188257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 352576
    },
    {
      "epoch": 0.0012796844118696624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 352608
    },
    {
      "epoch": 0.001279800546220499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 352640
    },
    {
      "epoch": 0.0012799166805713357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 352672
    },
    {
      "epoch": 0.0012800328149221725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 352704
    },
    {
      "epoch": 0.0012801489492730092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 352736
    },
    {
      "epoch": 0.0012802650836238457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 352768
    },
    {
      "epoch": 0.0012803812179746825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 352800
    },
    {
      "epoch": 0.0012804973523255193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 352832
    },
    {
      "epoch": 0.001280613486676356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 352864
    },
    {
      "epoch": 0.0012807296210271928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 352896
    },
    {
      "epoch": 0.0012808457553780293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 352928
    },
    {
      "epoch": 0.001280961889728866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 352960
    },
    {
      "epoch": 0.0012810780240797028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 352992
    },
    {
      "epoch": 0.0012811941584305396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 353024
    },
    {
      "epoch": 0.001281310292781376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 353056
    },
    {
      "epoch": 0.0012814264271322129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 353088
    },
    {
      "epoch": 0.0012815425614830496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 353120
    },
    {
      "epoch": 0.0012816586958338864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 353152
    },
    {
      "epoch": 0.0012817748301847231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 353184
    },
    {
      "epoch": 0.0012818909645355597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 353216
    },
    {
      "epoch": 0.0012820070988863964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 353248
    },
    {
      "epoch": 0.0012821232332372332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 353280
    },
    {
      "epoch": 0.00128223936758807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 353312
    },
    {
      "epoch": 0.0012823555019389065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7539,
      "step": 353344
    },
    {
      "epoch": 0.0012824716362897432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 353376
    },
    {
      "epoch": 0.00128258777064058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 353408
    },
    {
      "epoch": 0.0012827039049914167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 353440
    },
    {
      "epoch": 0.0012828200393422535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 353472
    },
    {
      "epoch": 0.00128293617369309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 353504
    },
    {
      "epoch": 0.0012830523080439268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 353536
    },
    {
      "epoch": 0.0012831684423947635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 353568
    },
    {
      "epoch": 0.0012832845767456003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.687,
      "step": 353600
    },
    {
      "epoch": 0.0012834007110964368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 353632
    },
    {
      "epoch": 0.0012835168454472736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 353664
    },
    {
      "epoch": 0.0012836329797981103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 353696
    },
    {
      "epoch": 0.001283749114148947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 353728
    },
    {
      "epoch": 0.0012838652484997838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 353760
    },
    {
      "epoch": 0.0012839813828506204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 353792
    },
    {
      "epoch": 0.0012840975172014571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 353824
    },
    {
      "epoch": 0.0012842136515522939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 353856
    },
    {
      "epoch": 0.0012843297859031306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 353888
    },
    {
      "epoch": 0.0012844459202539672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 353920
    },
    {
      "epoch": 0.001284562054604804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 353952
    },
    {
      "epoch": 0.0012846781889556407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 353984
    },
    {
      "epoch": 0.0012847943233064774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 354016
    },
    {
      "epoch": 0.0012849104576573142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 354048
    },
    {
      "epoch": 0.0012850265920081507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 354080
    },
    {
      "epoch": 0.0012851427263589875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 354112
    },
    {
      "epoch": 0.0012852588607098242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 354144
    },
    {
      "epoch": 0.001285374995060661,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 354176
    },
    {
      "epoch": 0.0012854911294114975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 354208
    },
    {
      "epoch": 0.0012856072637623343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 354240
    },
    {
      "epoch": 0.001285723398113171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 354272
    },
    {
      "epoch": 0.0012858395324640078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 354304
    },
    {
      "epoch": 0.0012859556668148445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 354336
    },
    {
      "epoch": 0.001286071801165681,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 354368
    },
    {
      "epoch": 0.0012861879355165178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 354400
    },
    {
      "epoch": 0.0012863040698673546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 354432
    },
    {
      "epoch": 0.0012864202042181913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 354464
    },
    {
      "epoch": 0.0012865363385690279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6938,
      "step": 354496
    },
    {
      "epoch": 0.0012866524729198646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 354528
    },
    {
      "epoch": 0.0012867686072707014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 354560
    },
    {
      "epoch": 0.0012868847416215381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 354592
    },
    {
      "epoch": 0.0012870008759723749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 354624
    },
    {
      "epoch": 0.0012871170103232114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 354656
    },
    {
      "epoch": 0.0012872331446740482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 354688
    },
    {
      "epoch": 0.001287349279024885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 354720
    },
    {
      "epoch": 0.0012874654133757217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 354752
    },
    {
      "epoch": 0.0012875815477265582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 354784
    },
    {
      "epoch": 0.001287697682077395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 354816
    },
    {
      "epoch": 0.0012878138164282317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 354848
    },
    {
      "epoch": 0.0012879299507790685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 354880
    },
    {
      "epoch": 0.0012880460851299052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 354912
    },
    {
      "epoch": 0.0012881622194807418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 354944
    },
    {
      "epoch": 0.0012882783538315785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 354976
    },
    {
      "epoch": 0.0012883944881824153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 355008
    },
    {
      "epoch": 0.001288510622533252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 355040
    },
    {
      "epoch": 0.0012886267568840886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 355072
    },
    {
      "epoch": 0.0012887428912349253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7626,
      "step": 355104
    },
    {
      "epoch": 0.001288859025585762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7568,
      "step": 355136
    },
    {
      "epoch": 0.0012889751599365988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 355168
    },
    {
      "epoch": 0.0012890912942874356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 355200
    },
    {
      "epoch": 0.0012892074286382721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 355232
    },
    {
      "epoch": 0.0012893235629891089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 355264
    },
    {
      "epoch": 0.0012894396973399456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 355296
    },
    {
      "epoch": 0.0012895558316907824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 355328
    },
    {
      "epoch": 0.001289671966041619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 355360
    },
    {
      "epoch": 0.0012897881003924557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 355392
    },
    {
      "epoch": 0.0012899042347432924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 355424
    },
    {
      "epoch": 0.0012900203690941292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 355456
    },
    {
      "epoch": 0.0012901365034449657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 355488
    },
    {
      "epoch": 0.0012902526377958025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 355520
    },
    {
      "epoch": 0.0012903687721466392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 355552
    },
    {
      "epoch": 0.001290484906497476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 355584
    },
    {
      "epoch": 0.0012906010408483127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 355616
    },
    {
      "epoch": 0.0012907171751991493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 355648
    },
    {
      "epoch": 0.001290833309549986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 355680
    },
    {
      "epoch": 0.0012909494439008228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 355712
    },
    {
      "epoch": 0.0012910655782516595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 355744
    },
    {
      "epoch": 0.001291181712602496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.74,
      "step": 355776
    },
    {
      "epoch": 0.0012912978469533328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 355808
    },
    {
      "epoch": 0.0012914139813041696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 355840
    },
    {
      "epoch": 0.0012915301156550063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6939,
      "step": 355872
    },
    {
      "epoch": 0.001291646250005843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 355904
    },
    {
      "epoch": 0.0012917623843566796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 355936
    },
    {
      "epoch": 0.0012918785187075164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 355968
    },
    {
      "epoch": 0.0012919946530583531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 356000
    },
    {
      "epoch": 0.00129211078740919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 356032
    },
    {
      "epoch": 0.0012922269217600264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 356064
    },
    {
      "epoch": 0.0012923430561108632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 356096
    },
    {
      "epoch": 0.0012924591904617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 356128
    },
    {
      "epoch": 0.0012925753248125367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6956,
      "step": 356160
    },
    {
      "epoch": 0.0012926914591633734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 356192
    },
    {
      "epoch": 0.00129280759351421,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 356224
    },
    {
      "epoch": 0.0012929237278650467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 356256
    },
    {
      "epoch": 0.0012930398622158835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 356288
    },
    {
      "epoch": 0.0012931559965667202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 356320
    },
    {
      "epoch": 0.0012932721309175568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7499,
      "step": 356352
    },
    {
      "epoch": 0.0012933882652683935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 356384
    },
    {
      "epoch": 0.0012935043996192303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 356416
    },
    {
      "epoch": 0.001293620533970067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 356448
    },
    {
      "epoch": 0.0012937366683209038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 356480
    },
    {
      "epoch": 0.0012938528026717403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 356512
    },
    {
      "epoch": 0.001293968937022577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 356544
    },
    {
      "epoch": 0.0012940850713734138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7497,
      "step": 356576
    },
    {
      "epoch": 0.0012942012057242506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 356608
    },
    {
      "epoch": 0.0012943173400750871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 356640
    },
    {
      "epoch": 0.001294433474425924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 356672
    },
    {
      "epoch": 0.0012945496087767606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 356704
    },
    {
      "epoch": 0.0012946657431275974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 356736
    },
    {
      "epoch": 0.0012947818774784342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 356768
    },
    {
      "epoch": 0.0012948980118292707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 356800
    },
    {
      "epoch": 0.0012950141461801074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7477,
      "step": 356832
    },
    {
      "epoch": 0.0012951302805309442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7766,
      "step": 356864
    },
    {
      "epoch": 0.001295246414881781,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 356896
    },
    {
      "epoch": 0.0012953625492326175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 356928
    },
    {
      "epoch": 0.0012954786835834542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 356960
    },
    {
      "epoch": 0.001295594817934291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 356992
    },
    {
      "epoch": 0.0012957109522851278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 357024
    },
    {
      "epoch": 0.0012958270866359645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 357056
    },
    {
      "epoch": 0.001295943220986801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 357088
    },
    {
      "epoch": 0.0012960593553376378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 357120
    },
    {
      "epoch": 0.0012961754896884746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 357152
    },
    {
      "epoch": 0.0012962916240393113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 357184
    },
    {
      "epoch": 0.0012964077583901478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 357216
    },
    {
      "epoch": 0.0012965238927409846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 357248
    },
    {
      "epoch": 0.0012966400270918214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7567,
      "step": 357280
    },
    {
      "epoch": 0.001296756161442658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 357312
    },
    {
      "epoch": 0.0012968722957934949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 357344
    },
    {
      "epoch": 0.0012969884301443314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 357376
    },
    {
      "epoch": 0.0012971045644951682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 357408
    },
    {
      "epoch": 0.001297220698846005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7462,
      "step": 357440
    },
    {
      "epoch": 0.0012973368331968417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 357472
    },
    {
      "epoch": 0.0012974529675476782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 357504
    },
    {
      "epoch": 0.001297569101898515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 357536
    },
    {
      "epoch": 0.0012976852362493517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 357568
    },
    {
      "epoch": 0.0012978013706001885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 357600
    },
    {
      "epoch": 0.0012979175049510252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 357632
    },
    {
      "epoch": 0.0012980336393018618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 357664
    },
    {
      "epoch": 0.0012981497736526985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 357696
    },
    {
      "epoch": 0.0012982659080035353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 357728
    },
    {
      "epoch": 0.001298382042354372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 357760
    },
    {
      "epoch": 0.0012984981767052086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 357792
    },
    {
      "epoch": 0.0012986143110560453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 357824
    },
    {
      "epoch": 0.001298730445406882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 357856
    },
    {
      "epoch": 0.0012988465797577188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 357888
    },
    {
      "epoch": 0.0012989627141085556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 357920
    },
    {
      "epoch": 0.001299078848459392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 357952
    },
    {
      "epoch": 0.0012991949828102289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7468,
      "step": 357984
    },
    {
      "epoch": 0.0012993111171610656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 358016
    },
    {
      "epoch": 0.0012994272515119024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 358048
    },
    {
      "epoch": 0.001299543385862739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 358080
    },
    {
      "epoch": 0.0012996595202135757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 358112
    },
    {
      "epoch": 0.0012997756545644124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 358144
    },
    {
      "epoch": 0.0012998917889152492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 358176
    },
    {
      "epoch": 0.001300007923266086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 358208
    },
    {
      "epoch": 0.0013001240576169225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 358240
    },
    {
      "epoch": 0.0013002401919677592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 358272
    },
    {
      "epoch": 0.001300356326318596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 358304
    },
    {
      "epoch": 0.0013004724606694327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 358336
    },
    {
      "epoch": 0.0013005885950202693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 358368
    },
    {
      "epoch": 0.001300704729371106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 358400
    },
    {
      "epoch": 0.0013008208637219428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 358432
    },
    {
      "epoch": 0.0013009369980727795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 358464
    },
    {
      "epoch": 0.0013010531324236163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 358496
    },
    {
      "epoch": 0.0013011692667744528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 358528
    },
    {
      "epoch": 0.0013012854011252896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 358560
    },
    {
      "epoch": 0.0013014015354761263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 358592
    },
    {
      "epoch": 0.001301517669826963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 358624
    },
    {
      "epoch": 0.0013016338041777996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 358656
    },
    {
      "epoch": 0.0013017499385286364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 358688
    },
    {
      "epoch": 0.0013018660728794731,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 358720
    },
    {
      "epoch": 0.0013019822072303099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 358752
    },
    {
      "epoch": 0.0013020983415811466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 358784
    },
    {
      "epoch": 0.0013022144759319832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 358816
    },
    {
      "epoch": 0.00130233061028282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 358848
    },
    {
      "epoch": 0.0013024467446336567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 358880
    },
    {
      "epoch": 0.0013025628789844934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 358912
    },
    {
      "epoch": 0.00130267901333533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 358944
    },
    {
      "epoch": 0.0013027951476861667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 358976
    },
    {
      "epoch": 0.0013029112820370035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 359008
    },
    {
      "epoch": 0.0013030274163878402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7446,
      "step": 359040
    },
    {
      "epoch": 0.001303143550738677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 359072
    },
    {
      "epoch": 0.0013032596850895135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 359104
    },
    {
      "epoch": 0.0013033758194403503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 359136
    },
    {
      "epoch": 0.001303491953791187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 359168
    },
    {
      "epoch": 0.0013036080881420238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 359200
    },
    {
      "epoch": 0.0013037242224928603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 359232
    },
    {
      "epoch": 0.001303840356843697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 359264
    },
    {
      "epoch": 0.0013039564911945338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 359296
    },
    {
      "epoch": 0.0013040726255453706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 359328
    },
    {
      "epoch": 0.0013041887598962073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 359360
    },
    {
      "epoch": 0.0013043048942470439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 359392
    },
    {
      "epoch": 0.0013044210285978806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 359424
    },
    {
      "epoch": 0.0013045371629487174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 359456
    },
    {
      "epoch": 0.0013046532972995541,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.755,
      "step": 359488
    },
    {
      "epoch": 0.0013047694316503907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 359520
    },
    {
      "epoch": 0.0013048855660012274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 359552
    },
    {
      "epoch": 0.0013050017003520642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 359584
    },
    {
      "epoch": 0.001305117834702901,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 359616
    },
    {
      "epoch": 0.0013052339690537377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 359648
    },
    {
      "epoch": 0.0013053501034045742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 359680
    },
    {
      "epoch": 0.001305466237755411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 359712
    },
    {
      "epoch": 0.0013055823721062477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 359744
    },
    {
      "epoch": 0.0013056985064570845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 359776
    },
    {
      "epoch": 0.001305814640807921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 359808
    },
    {
      "epoch": 0.0013059307751587578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 359840
    },
    {
      "epoch": 0.0013060469095095945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 359872
    },
    {
      "epoch": 0.0013061630438604313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 359904
    },
    {
      "epoch": 0.001306279178211268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7503,
      "step": 359936
    },
    {
      "epoch": 0.0013063953125621046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 359968
    },
    {
      "epoch": 0.0013065114469129413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 360000
    },
    {
      "epoch": 0.001306627581263778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 360032
    },
    {
      "epoch": 0.0013067437156146148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7495,
      "step": 360064
    },
    {
      "epoch": 0.0013068598499654514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 360096
    },
    {
      "epoch": 0.0013069759843162881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 360128
    },
    {
      "epoch": 0.0013070921186671249,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 360160
    },
    {
      "epoch": 0.0013072082530179616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 360192
    },
    {
      "epoch": 0.0013073243873687984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 360224
    },
    {
      "epoch": 0.001307440521719635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 360256
    },
    {
      "epoch": 0.0013075566560704717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 360288
    },
    {
      "epoch": 0.0013076727904213084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 360320
    },
    {
      "epoch": 0.0013077889247721452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 360352
    },
    {
      "epoch": 0.0013079050591229817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 360384
    },
    {
      "epoch": 0.0013080211934738185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.695,
      "step": 360416
    },
    {
      "epoch": 0.0013081373278246552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 360448
    },
    {
      "epoch": 0.001308253462175492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 360480
    },
    {
      "epoch": 0.0013083695965263287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 360512
    },
    {
      "epoch": 0.0013084857308771653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 360544
    },
    {
      "epoch": 0.001308601865228002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 360576
    },
    {
      "epoch": 0.0013087179995788388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 360608
    },
    {
      "epoch": 0.0013088341339296755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 360640
    },
    {
      "epoch": 0.001308950268280512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 360672
    },
    {
      "epoch": 0.0013090664026313488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 360704
    },
    {
      "epoch": 0.0013091825369821856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 360736
    },
    {
      "epoch": 0.0013092986713330223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 360768
    },
    {
      "epoch": 0.001309414805683859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 360800
    },
    {
      "epoch": 0.0013095309400346956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 360832
    },
    {
      "epoch": 0.0013096470743855324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 360864
    },
    {
      "epoch": 0.0013097632087363691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 360896
    },
    {
      "epoch": 0.001309879343087206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7471,
      "step": 360928
    },
    {
      "epoch": 0.0013099954774380424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 360960
    },
    {
      "epoch": 0.0013101116117888792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 360992
    },
    {
      "epoch": 0.001310227746139716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 361024
    },
    {
      "epoch": 0.0013103438804905527,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 361056
    },
    {
      "epoch": 0.0013104600148413895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 361088
    },
    {
      "epoch": 0.001310576149192226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 361120
    },
    {
      "epoch": 0.0013106922835430627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 361152
    },
    {
      "epoch": 0.0013108084178938995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 361184
    },
    {
      "epoch": 0.0013109245522447363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7539,
      "step": 361216
    },
    {
      "epoch": 0.0013110406865955728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 361248
    },
    {
      "epoch": 0.0013111568209464095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 361280
    },
    {
      "epoch": 0.0013112729552972463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 361312
    },
    {
      "epoch": 0.001311389089648083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 361344
    },
    {
      "epoch": 0.0013115052239989198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 361376
    },
    {
      "epoch": 0.0013116213583497563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 361408
    },
    {
      "epoch": 0.001311737492700593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 361440
    },
    {
      "epoch": 0.0013118536270514299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 361472
    },
    {
      "epoch": 0.0013119697614022666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 361504
    },
    {
      "epoch": 0.0013120858957531031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 361536
    },
    {
      "epoch": 0.00131220203010394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 361568
    },
    {
      "epoch": 0.0013123181644547767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 361600
    },
    {
      "epoch": 0.0013124342988056134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 361632
    },
    {
      "epoch": 0.0013125504331564502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 361664
    },
    {
      "epoch": 0.0013126665675072867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 361696
    },
    {
      "epoch": 0.0013127827018581235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 361728
    },
    {
      "epoch": 0.0013128988362089602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.749,
      "step": 361760
    },
    {
      "epoch": 0.001313014970559797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7505,
      "step": 361792
    },
    {
      "epoch": 0.0013131311049106335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 361824
    },
    {
      "epoch": 0.0013132472392614703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 361856
    },
    {
      "epoch": 0.001313363373612307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 361888
    },
    {
      "epoch": 0.0013134795079631438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7477,
      "step": 361920
    },
    {
      "epoch": 0.0013135956423139805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 361952
    },
    {
      "epoch": 0.001313711776664817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7419,
      "step": 361984
    },
    {
      "epoch": 0.0013138279110156538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 362016
    },
    {
      "epoch": 0.0013139440453664906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 362048
    },
    {
      "epoch": 0.0013140601797173273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 362080
    },
    {
      "epoch": 0.0013141763140681639,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 362112
    },
    {
      "epoch": 0.0013142924484190006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 362144
    },
    {
      "epoch": 0.0013144085827698374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 362176
    },
    {
      "epoch": 0.0013145247171206741,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 362208
    },
    {
      "epoch": 0.0013146408514715109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 362240
    },
    {
      "epoch": 0.0013147569858223474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 362272
    },
    {
      "epoch": 0.0013148731201731842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 362304
    },
    {
      "epoch": 0.001314989254524021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 362336
    },
    {
      "epoch": 0.0013151053888748577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 362368
    },
    {
      "epoch": 0.0013152215232256942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 362400
    },
    {
      "epoch": 0.001315337657576531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 362432
    },
    {
      "epoch": 0.0013154537919273677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 362464
    },
    {
      "epoch": 0.0013155699262782045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 362496
    },
    {
      "epoch": 0.0013156860606290412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 362528
    },
    {
      "epoch": 0.0013158021949798778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 362560
    },
    {
      "epoch": 0.0013159183293307145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 362592
    },
    {
      "epoch": 0.0013160344636815513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 362624
    },
    {
      "epoch": 0.001316150598032388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 362656
    },
    {
      "epoch": 0.0013162667323832246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 362688
    },
    {
      "epoch": 0.0013163828667340613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 362720
    },
    {
      "epoch": 0.001316499001084898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 362752
    },
    {
      "epoch": 0.0013166151354357348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 362784
    },
    {
      "epoch": 0.0013167312697865716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 362816
    },
    {
      "epoch": 0.0013168474041374081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 362848
    },
    {
      "epoch": 0.0013169635384882449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 362880
    },
    {
      "epoch": 0.0013170796728390816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 362912
    },
    {
      "epoch": 0.0013171958071899184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 362944
    },
    {
      "epoch": 0.001317311941540755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 362976
    },
    {
      "epoch": 0.0013174280758915917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 363008
    },
    {
      "epoch": 0.0013175442102424284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 363040
    },
    {
      "epoch": 0.0013176603445932652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 363072
    },
    {
      "epoch": 0.001317776478944102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 363104
    },
    {
      "epoch": 0.0013178926132949385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 363136
    },
    {
      "epoch": 0.0013180087476457752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6896,
      "step": 363168
    },
    {
      "epoch": 0.001318124881996612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 363200
    },
    {
      "epoch": 0.0013182410163474487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 363232
    },
    {
      "epoch": 0.0013183571506982853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 363264
    },
    {
      "epoch": 0.001318473285049122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 363296
    },
    {
      "epoch": 0.0013185894193999588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 363328
    },
    {
      "epoch": 0.0013187055537507955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 363360
    },
    {
      "epoch": 0.0013188216881016323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7481,
      "step": 363392
    },
    {
      "epoch": 0.0013189378224524688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 363424
    },
    {
      "epoch": 0.0013190539568033056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 363456
    },
    {
      "epoch": 0.0013191700911541423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 363488
    },
    {
      "epoch": 0.001319286225504979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 363520
    },
    {
      "epoch": 0.0013194023598558156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 363552
    },
    {
      "epoch": 0.0013195184942066524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 363584
    },
    {
      "epoch": 0.0013196346285574891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 363616
    },
    {
      "epoch": 0.0013197507629083259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 363648
    },
    {
      "epoch": 0.0013198668972591626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 363680
    },
    {
      "epoch": 0.0013199830316099992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 363712
    },
    {
      "epoch": 0.001320099165960836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 363744
    },
    {
      "epoch": 0.0013202153003116727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 363776
    },
    {
      "epoch": 0.0013203314346625094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 363808
    },
    {
      "epoch": 0.001320447569013346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 363840
    },
    {
      "epoch": 0.0013205637033641827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 363872
    },
    {
      "epoch": 0.0013206798377150195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 363904
    },
    {
      "epoch": 0.0013207959720658562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 363936
    },
    {
      "epoch": 0.001320912106416693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 363968
    },
    {
      "epoch": 0.0013210282407675295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 364000
    },
    {
      "epoch": 0.0013211443751183663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 364032
    },
    {
      "epoch": 0.001321260509469203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 364064
    },
    {
      "epoch": 0.0013213766438200398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 364096
    },
    {
      "epoch": 0.0013214927781708763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 364128
    },
    {
      "epoch": 0.001321608912521713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 364160
    },
    {
      "epoch": 0.0013217250468725498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 364192
    },
    {
      "epoch": 0.0013218411812233866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 364224
    },
    {
      "epoch": 0.0013219573155742233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 364256
    },
    {
      "epoch": 0.0013220734499250599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 364288
    },
    {
      "epoch": 0.0013221895842758966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7511,
      "step": 364320
    },
    {
      "epoch": 0.0013223057186267334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 364352
    },
    {
      "epoch": 0.0013224218529775701,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 364384
    },
    {
      "epoch": 0.0013225379873284067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 364416
    },
    {
      "epoch": 0.0013226541216792434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 364448
    },
    {
      "epoch": 0.0013227702560300802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 364480
    },
    {
      "epoch": 0.001322886390380917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 364512
    },
    {
      "epoch": 0.0013230025247317537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 364544
    },
    {
      "epoch": 0.0013231186590825902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 364576
    },
    {
      "epoch": 0.001323234793433427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 364608
    },
    {
      "epoch": 0.0013233509277842637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 364640
    },
    {
      "epoch": 0.0013234670621351005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 364672
    },
    {
      "epoch": 0.001323583196485937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 364704
    },
    {
      "epoch": 0.0013236993308367738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 364736
    },
    {
      "epoch": 0.0013238154651876105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 364768
    },
    {
      "epoch": 0.0013239315995384473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 364800
    },
    {
      "epoch": 0.001324047733889284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 364832
    },
    {
      "epoch": 0.0013241638682401206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 364864
    },
    {
      "epoch": 0.0013242800025909573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 364896
    },
    {
      "epoch": 0.001324396136941794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 364928
    },
    {
      "epoch": 0.0013245122712926308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 364960
    },
    {
      "epoch": 0.0013246284056434674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7491,
      "step": 364992
    },
    {
      "epoch": 0.0013247445399943041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7483,
      "step": 365024
    },
    {
      "epoch": 0.001324860674345141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 365056
    },
    {
      "epoch": 0.0013249768086959776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 365088
    },
    {
      "epoch": 0.0013250929430468144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 365120
    },
    {
      "epoch": 0.001325209077397651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 365152
    },
    {
      "epoch": 0.0013253252117484877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 365184
    },
    {
      "epoch": 0.0013254413460993244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 365216
    },
    {
      "epoch": 0.0013255574804501612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 365248
    },
    {
      "epoch": 0.0013256736148009977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 365280
    },
    {
      "epoch": 0.0013257897491518345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 365312
    },
    {
      "epoch": 0.0013259058835026712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 365344
    },
    {
      "epoch": 0.001326022017853508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 365376
    },
    {
      "epoch": 0.0013261381522043448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 365408
    },
    {
      "epoch": 0.0013262542865551813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 365440
    },
    {
      "epoch": 0.001326370420906018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 365472
    },
    {
      "epoch": 0.0013264865552568548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 365504
    },
    {
      "epoch": 0.0013266026896076916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 365536
    },
    {
      "epoch": 0.001326718823958528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 365568
    },
    {
      "epoch": 0.0013268349583093648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 365600
    },
    {
      "epoch": 0.0013269510926602016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7409,
      "step": 365632
    },
    {
      "epoch": 0.0013270672270110384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 365664
    },
    {
      "epoch": 0.0013271833613618751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 365696
    },
    {
      "epoch": 0.0013272994957127116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 365728
    },
    {
      "epoch": 0.0013274156300635484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 365760
    },
    {
      "epoch": 0.0013275317644143852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 365792
    },
    {
      "epoch": 0.001327647898765222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 365824
    },
    {
      "epoch": 0.0013277640331160584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 365856
    },
    {
      "epoch": 0.0013278801674668952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 365888
    },
    {
      "epoch": 0.001327996301817732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 365920
    },
    {
      "epoch": 0.0013281124361685687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 365952
    },
    {
      "epoch": 0.0013282285705194055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 365984
    },
    {
      "epoch": 0.001328344704870242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 366016
    },
    {
      "epoch": 0.0013284608392210788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7451,
      "step": 366048
    },
    {
      "epoch": 0.0013285769735719155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7529,
      "step": 366080
    },
    {
      "epoch": 0.0013286931079227523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 366112
    },
    {
      "epoch": 0.0013288092422735888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 366144
    },
    {
      "epoch": 0.0013289253766244256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 366176
    },
    {
      "epoch": 0.0013290415109752623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 366208
    },
    {
      "epoch": 0.001329157645326099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 366240
    },
    {
      "epoch": 0.0013292737796769358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 366272
    },
    {
      "epoch": 0.0013293899140277724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 366304
    },
    {
      "epoch": 0.0013295060483786091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7538,
      "step": 366336
    },
    {
      "epoch": 0.0013296221827294459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 366368
    },
    {
      "epoch": 0.0013297383170802826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 366400
    },
    {
      "epoch": 0.0013298544514311192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 366432
    },
    {
      "epoch": 0.001329970585781956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 366464
    },
    {
      "epoch": 0.0013300867201327927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7469,
      "step": 366496
    },
    {
      "epoch": 0.0013302028544836294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7476,
      "step": 366528
    },
    {
      "epoch": 0.0013303189888344662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 366560
    },
    {
      "epoch": 0.0013304351231853027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 366592
    },
    {
      "epoch": 0.0013305512575361395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7532,
      "step": 366624
    },
    {
      "epoch": 0.0013306673918869762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 366656
    },
    {
      "epoch": 0.001330783526237813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 366688
    },
    {
      "epoch": 0.0013308996605886495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 366720
    },
    {
      "epoch": 0.0013310157949394863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 366752
    },
    {
      "epoch": 0.001331131929290323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 366784
    },
    {
      "epoch": 0.0013312480636411598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 366816
    },
    {
      "epoch": 0.0013313641979919965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 366848
    },
    {
      "epoch": 0.001331480332342833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 366880
    },
    {
      "epoch": 0.0013315964666936698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 366912
    },
    {
      "epoch": 0.0013317126010445066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 366944
    },
    {
      "epoch": 0.0013318287353953433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 366976
    },
    {
      "epoch": 0.0013319448697461799,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 367008
    },
    {
      "epoch": 0.0013320610040970166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 367040
    },
    {
      "epoch": 0.0013321771384478534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 367072
    },
    {
      "epoch": 0.0013322932727986901,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 367104
    },
    {
      "epoch": 0.0013324094071495269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 367136
    },
    {
      "epoch": 0.0013325255415003634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7487,
      "step": 367168
    },
    {
      "epoch": 0.0013326416758512002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 367200
    },
    {
      "epoch": 0.001332757810202037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 367232
    },
    {
      "epoch": 0.0013328739445528737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 367264
    },
    {
      "epoch": 0.0013329900789037102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 367296
    },
    {
      "epoch": 0.001333106213254547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 367328
    },
    {
      "epoch": 0.0013332223476053837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7588,
      "step": 367360
    },
    {
      "epoch": 0.0013333384819562205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7451,
      "step": 367392
    },
    {
      "epoch": 0.0013334546163070572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 367424
    },
    {
      "epoch": 0.0013335707506578938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 367456
    },
    {
      "epoch": 0.0013336868850087305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 367488
    },
    {
      "epoch": 0.0013338030193595673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 367520
    },
    {
      "epoch": 0.001333919153710404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 367552
    },
    {
      "epoch": 0.0013340352880612406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 367584
    },
    {
      "epoch": 0.0013341514224120773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 367616
    },
    {
      "epoch": 0.001334267556762914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 367648
    },
    {
      "epoch": 0.0013343836911137508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 367680
    },
    {
      "epoch": 0.0013344998254645876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 367712
    },
    {
      "epoch": 0.0013346159598154241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 367744
    },
    {
      "epoch": 0.0013347320941662609,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 367776
    },
    {
      "epoch": 0.0013348482285170976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 367808
    },
    {
      "epoch": 0.0013349643628679344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 367840
    },
    {
      "epoch": 0.001335080497218771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 367872
    },
    {
      "epoch": 0.0013351966315696077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 367904
    },
    {
      "epoch": 0.0013353127659204444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 367936
    },
    {
      "epoch": 0.0013354289002712812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 367968
    },
    {
      "epoch": 0.001335545034622118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 368000
    },
    {
      "epoch": 0.0013356611689729545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 368032
    },
    {
      "epoch": 0.0013357773033237912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 368064
    },
    {
      "epoch": 0.001335893437674628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 368096
    },
    {
      "epoch": 0.0013360095720254647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 368128
    },
    {
      "epoch": 0.0013361257063763013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 368160
    },
    {
      "epoch": 0.001336241840727138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 368192
    },
    {
      "epoch": 0.0013363579750779748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 368224
    },
    {
      "epoch": 0.0013364741094288115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7573,
      "step": 368256
    },
    {
      "epoch": 0.0013365902437796483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 368288
    },
    {
      "epoch": 0.0013367063781304848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 368320
    },
    {
      "epoch": 0.0013368225124813216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 368352
    },
    {
      "epoch": 0.0013369386468321583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 368384
    },
    {
      "epoch": 0.001337054781182995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 368416
    },
    {
      "epoch": 0.0013371709155338316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 368448
    },
    {
      "epoch": 0.0013372870498846684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 368480
    },
    {
      "epoch": 0.0013374031842355051,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 368512
    },
    {
      "epoch": 0.0013375193185863419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 368544
    },
    {
      "epoch": 0.0013376354529371786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 368576
    },
    {
      "epoch": 0.0013377515872880152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 368608
    },
    {
      "epoch": 0.001337867721638852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 368640
    },
    {
      "epoch": 0.0013379838559896887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 368672
    },
    {
      "epoch": 0.0013380999903405254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 368704
    },
    {
      "epoch": 0.001338216124691362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 368736
    },
    {
      "epoch": 0.0013383322590421987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 368768
    },
    {
      "epoch": 0.0013384483933930355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 368800
    },
    {
      "epoch": 0.0013385645277438722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 368832
    },
    {
      "epoch": 0.001338680662094709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6894,
      "step": 368864
    },
    {
      "epoch": 0.0013387967964455455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 368896
    },
    {
      "epoch": 0.0013389129307963823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 368928
    },
    {
      "epoch": 0.001339029065147219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 368960
    },
    {
      "epoch": 0.0013391451994980558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 368992
    },
    {
      "epoch": 0.0013392613338488923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 369024
    },
    {
      "epoch": 0.001339377468199729,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 369056
    },
    {
      "epoch": 0.0013394936025505658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 369088
    },
    {
      "epoch": 0.0013396097369014026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 369120
    },
    {
      "epoch": 0.0013397258712522394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 369152
    },
    {
      "epoch": 0.0013398420056030759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 369184
    },
    {
      "epoch": 0.0013399581399539126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 369216
    },
    {
      "epoch": 0.0013400742743047494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 369248
    },
    {
      "epoch": 0.0013401904086555862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6856,
      "step": 369280
    },
    {
      "epoch": 0.0013403065430064227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 369312
    },
    {
      "epoch": 0.0013404226773572594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 369344
    },
    {
      "epoch": 0.0013405388117080962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 369376
    },
    {
      "epoch": 0.001340654946058933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 369408
    },
    {
      "epoch": 0.0013407710804097697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 369440
    },
    {
      "epoch": 0.0013408872147606062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 369472
    },
    {
      "epoch": 0.001341003349111443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 369504
    },
    {
      "epoch": 0.0013411194834622798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 369536
    },
    {
      "epoch": 0.0013412356178131165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 369568
    },
    {
      "epoch": 0.001341351752163953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 369600
    },
    {
      "epoch": 0.0013414678865147898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 369632
    },
    {
      "epoch": 0.0013415840208656265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 369664
    },
    {
      "epoch": 0.0013417001552164633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 369696
    },
    {
      "epoch": 0.0013418162895673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 369728
    },
    {
      "epoch": 0.0013419324239181366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 369760
    },
    {
      "epoch": 0.0013420485582689733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 369792
    },
    {
      "epoch": 0.00134216469261981,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 369824
    },
    {
      "epoch": 0.0013422808269706469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 369856
    },
    {
      "epoch": 0.0013423969613214834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7428,
      "step": 369888
    },
    {
      "epoch": 0.0013425130956723201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 369920
    },
    {
      "epoch": 0.001342629230023157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7464,
      "step": 369952
    },
    {
      "epoch": 0.0013427453643739937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 369984
    },
    {
      "epoch": 0.0013428614987248304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 370016
    },
    {
      "epoch": 0.001342977633075667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 370048
    },
    {
      "epoch": 0.0013430937674265037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 370080
    },
    {
      "epoch": 0.0013432099017773405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 370112
    },
    {
      "epoch": 0.0013433260361281772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 370144
    },
    {
      "epoch": 0.0013434421704790137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 370176
    },
    {
      "epoch": 0.0013435583048298505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 370208
    },
    {
      "epoch": 0.0013436744391806873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 370240
    },
    {
      "epoch": 0.001343790573531524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 370272
    },
    {
      "epoch": 0.0013439067078823608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 370304
    },
    {
      "epoch": 0.0013440228422331973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 370336
    },
    {
      "epoch": 0.001344138976584034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 370368
    },
    {
      "epoch": 0.0013442551109348708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 370400
    },
    {
      "epoch": 0.0013443712452857076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 370432
    },
    {
      "epoch": 0.001344487379636544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 370464
    },
    {
      "epoch": 0.0013446035139873809,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 370496
    },
    {
      "epoch": 0.0013447196483382176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 370528
    },
    {
      "epoch": 0.0013448357826890544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 370560
    },
    {
      "epoch": 0.0013449519170398911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 370592
    },
    {
      "epoch": 0.0013450680513907277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 370624
    },
    {
      "epoch": 0.0013451841857415644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 370656
    },
    {
      "epoch": 0.0013453003200924012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 370688
    },
    {
      "epoch": 0.001345416454443238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 370720
    },
    {
      "epoch": 0.0013455325887940745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 370752
    },
    {
      "epoch": 0.0013456487231449112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 370784
    },
    {
      "epoch": 0.001345764857495748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 370816
    },
    {
      "epoch": 0.0013458809918465847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 370848
    },
    {
      "epoch": 0.0013459971261974215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 370880
    },
    {
      "epoch": 0.001346113260548258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7476,
      "step": 370912
    },
    {
      "epoch": 0.0013462293948990948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 370944
    },
    {
      "epoch": 0.0013463455292499315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 370976
    },
    {
      "epoch": 0.0013464616636007683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 371008
    },
    {
      "epoch": 0.0013465777979516048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 371040
    },
    {
      "epoch": 0.0013466939323024416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 371072
    },
    {
      "epoch": 0.0013468100666532783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 371104
    },
    {
      "epoch": 0.001346926201004115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 371136
    },
    {
      "epoch": 0.0013470423353549518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 371168
    },
    {
      "epoch": 0.0013471584697057884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 371200
    },
    {
      "epoch": 0.0013472746040566251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 371232
    },
    {
      "epoch": 0.0013473907384074619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 371264
    },
    {
      "epoch": 0.0013475068727582986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 371296
    },
    {
      "epoch": 0.0013476230071091352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 371328
    },
    {
      "epoch": 0.001347739141459972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 371360
    },
    {
      "epoch": 0.0013478552758108087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 371392
    },
    {
      "epoch": 0.0013479714101616454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 371424
    },
    {
      "epoch": 0.0013480875445124822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 371456
    },
    {
      "epoch": 0.0013482036788633187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 371488
    },
    {
      "epoch": 0.0013483198132141555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 371520
    },
    {
      "epoch": 0.0013484359475649922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 371552
    },
    {
      "epoch": 0.001348552081915829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 371584
    },
    {
      "epoch": 0.0013486682162666655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 371616
    },
    {
      "epoch": 0.0013487843506175023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7484,
      "step": 371648
    },
    {
      "epoch": 0.001348900484968339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 371680
    },
    {
      "epoch": 0.0013490166193191758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 371712
    },
    {
      "epoch": 0.0013491327536700125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 371744
    },
    {
      "epoch": 0.001349248888020849,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7605,
      "step": 371776
    },
    {
      "epoch": 0.0013493650223716858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 371808
    },
    {
      "epoch": 0.0013494811567225226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 371840
    },
    {
      "epoch": 0.0013495972910733593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 371872
    },
    {
      "epoch": 0.0013497134254241959,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 371904
    },
    {
      "epoch": 0.0013498295597750326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 371936
    },
    {
      "epoch": 0.0013499456941258694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 371968
    },
    {
      "epoch": 0.0013500618284767061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 372000
    },
    {
      "epoch": 0.0013501779628275429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 372032
    },
    {
      "epoch": 0.0013502940971783794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7551,
      "step": 372064
    },
    {
      "epoch": 0.0013504102315292162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 372096
    },
    {
      "epoch": 0.001350526365880053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 372128
    },
    {
      "epoch": 0.0013506425002308897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 372160
    },
    {
      "epoch": 0.0013507586345817262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7529,
      "step": 372192
    },
    {
      "epoch": 0.001350874768932563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.75,
      "step": 372224
    },
    {
      "epoch": 0.0013509909032833997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 372256
    },
    {
      "epoch": 0.0013511070376342365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 372288
    },
    {
      "epoch": 0.0013512231719850732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 372320
    },
    {
      "epoch": 0.0013513393063359098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 372352
    },
    {
      "epoch": 0.0013514554406867465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 372384
    },
    {
      "epoch": 0.0013515715750375833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 372416
    },
    {
      "epoch": 0.00135168770938842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 372448
    },
    {
      "epoch": 0.0013518038437392566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 372480
    },
    {
      "epoch": 0.0013519199780900933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 372512
    },
    {
      "epoch": 0.00135203611244093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 372544
    },
    {
      "epoch": 0.0013521522467917668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 372576
    },
    {
      "epoch": 0.0013522683811426036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 372608
    },
    {
      "epoch": 0.0013523845154934401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 372640
    },
    {
      "epoch": 0.0013525006498442769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 372672
    },
    {
      "epoch": 0.0013526167841951136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 372704
    },
    {
      "epoch": 0.0013527329185459504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 372736
    },
    {
      "epoch": 0.001352849052896787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 372768
    },
    {
      "epoch": 0.0013529651872476237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 372800
    },
    {
      "epoch": 0.0013530813215984604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 372832
    },
    {
      "epoch": 0.0013531974559492972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 372864
    },
    {
      "epoch": 0.001353313590300134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 372896
    },
    {
      "epoch": 0.0013534297246509705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 372928
    },
    {
      "epoch": 0.0013535458590018072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 372960
    },
    {
      "epoch": 0.001353661993352644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 372992
    },
    {
      "epoch": 0.0013537781277034807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 373024
    },
    {
      "epoch": 0.0013538942620543173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 373056
    },
    {
      "epoch": 0.001354010396405154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 373088
    },
    {
      "epoch": 0.0013541265307559908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 373120
    },
    {
      "epoch": 0.0013542426651068275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 373152
    },
    {
      "epoch": 0.0013543587994576643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 373184
    },
    {
      "epoch": 0.0013544749338085008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 373216
    },
    {
      "epoch": 0.0013545910681593376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 373248
    },
    {
      "epoch": 0.0013547072025101743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 373280
    },
    {
      "epoch": 0.001354823336861011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 373312
    },
    {
      "epoch": 0.0013549394712118476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 373344
    },
    {
      "epoch": 0.0013550556055626844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7474,
      "step": 373376
    },
    {
      "epoch": 0.0013551717399135211,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 373408
    },
    {
      "epoch": 0.001355287874264358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 373440
    },
    {
      "epoch": 0.0013554040086151947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 373472
    },
    {
      "epoch": 0.0013555201429660312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 373504
    },
    {
      "epoch": 0.001355636277316868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 373536
    },
    {
      "epoch": 0.0013557524116677047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 373568
    },
    {
      "epoch": 0.0013558685460185415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 373600
    },
    {
      "epoch": 0.001355984680369378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 373632
    },
    {
      "epoch": 0.0013561008147202147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 373664
    },
    {
      "epoch": 0.0013562169490710515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 373696
    },
    {
      "epoch": 0.0013563330834218883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 373728
    },
    {
      "epoch": 0.001356449217772725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 373760
    },
    {
      "epoch": 0.0013565653521235615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6894,
      "step": 373792
    },
    {
      "epoch": 0.0013566814864743983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 373824
    },
    {
      "epoch": 0.001356797620825235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 373856
    },
    {
      "epoch": 0.0013569137551760718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 373888
    },
    {
      "epoch": 0.0013570298895269083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 373920
    },
    {
      "epoch": 0.001357146023877745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 373952
    },
    {
      "epoch": 0.0013572621582285819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 373984
    },
    {
      "epoch": 0.0013573782925794186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 374016
    },
    {
      "epoch": 0.0013574944269302554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 374048
    },
    {
      "epoch": 0.001357610561281092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 374080
    },
    {
      "epoch": 0.0013577266956319287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 374112
    },
    {
      "epoch": 0.0013578428299827654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 374144
    },
    {
      "epoch": 0.0013579589643336022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 374176
    },
    {
      "epoch": 0.0013580750986844387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 374208
    },
    {
      "epoch": 0.0013581912330352755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 374240
    },
    {
      "epoch": 0.0013583073673861122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 374272
    },
    {
      "epoch": 0.001358423501736949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 374304
    },
    {
      "epoch": 0.0013585396360877857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 374336
    },
    {
      "epoch": 0.0013586557704386223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 374368
    },
    {
      "epoch": 0.001358771904789459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 374400
    },
    {
      "epoch": 0.0013588880391402958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 374432
    },
    {
      "epoch": 0.0013590041734911325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 374464
    },
    {
      "epoch": 0.001359120307841969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 374496
    },
    {
      "epoch": 0.0013592364421928058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 374528
    },
    {
      "epoch": 0.0013593525765436426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 374560
    },
    {
      "epoch": 0.0013594687108944793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 374592
    },
    {
      "epoch": 0.001359584845245316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 374624
    },
    {
      "epoch": 0.0013597009795961526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 374656
    },
    {
      "epoch": 0.0013598171139469894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 374688
    },
    {
      "epoch": 0.0013599332482978261,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 374720
    },
    {
      "epoch": 0.0013600493826486629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 374752
    },
    {
      "epoch": 0.0013601655169994994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 374784
    },
    {
      "epoch": 0.0013602816513503362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7439,
      "step": 374816
    },
    {
      "epoch": 0.001360397785701173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.754,
      "step": 374848
    },
    {
      "epoch": 0.0013605139200520097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 374880
    },
    {
      "epoch": 0.0013606300544028464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 374912
    },
    {
      "epoch": 0.001360746188753683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 374944
    },
    {
      "epoch": 0.0013608623231045197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 374976
    },
    {
      "epoch": 0.0013609784574553565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 375008
    },
    {
      "epoch": 0.0013610945918061932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 375040
    },
    {
      "epoch": 0.0013612107261570298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 375072
    },
    {
      "epoch": 0.0013613268605078665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 375104
    },
    {
      "epoch": 0.0013614429948587033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 375136
    },
    {
      "epoch": 0.00136155912920954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 375168
    },
    {
      "epoch": 0.0013616752635603768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 375200
    },
    {
      "epoch": 0.0013617913979112133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 375232
    },
    {
      "epoch": 0.00136190753226205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 375264
    },
    {
      "epoch": 0.0013620236666128868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7464,
      "step": 375296
    },
    {
      "epoch": 0.0013621398009637236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 375328
    },
    {
      "epoch": 0.0013622559353145601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 375360
    },
    {
      "epoch": 0.0013623720696653969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 375392
    },
    {
      "epoch": 0.0013624882040162336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 375424
    },
    {
      "epoch": 0.0013626043383670704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 375456
    },
    {
      "epoch": 0.0013627204727179071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 375488
    },
    {
      "epoch": 0.0013628366070687437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 375520
    },
    {
      "epoch": 0.0013629527414195804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 375552
    },
    {
      "epoch": 0.0013630688757704172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.698,
      "step": 375584
    },
    {
      "epoch": 0.001363185010121254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 375616
    },
    {
      "epoch": 0.0013633011444720905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 375648
    },
    {
      "epoch": 0.0013634172788229272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 375680
    },
    {
      "epoch": 0.001363533413173764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 375712
    },
    {
      "epoch": 0.0013636495475246007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 375744
    },
    {
      "epoch": 0.0013637656818754375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 375776
    },
    {
      "epoch": 0.001363881816226274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 375808
    },
    {
      "epoch": 0.0013639979505771108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 375840
    },
    {
      "epoch": 0.0013641140849279475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 375872
    },
    {
      "epoch": 0.0013642302192787843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 375904
    },
    {
      "epoch": 0.0013643463536296208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 375936
    },
    {
      "epoch": 0.0013644624879804576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 375968
    },
    {
      "epoch": 0.0013645786223312943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 376000
    },
    {
      "epoch": 0.001364694756682131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 376032
    },
    {
      "epoch": 0.0013648108910329678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 376064
    },
    {
      "epoch": 0.0013649270253838044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 376096
    },
    {
      "epoch": 0.0013650431597346411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 376128
    },
    {
      "epoch": 0.0013651592940854779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 376160
    },
    {
      "epoch": 0.0013652754284363146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 376192
    },
    {
      "epoch": 0.0013653915627871512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 376224
    },
    {
      "epoch": 0.001365507697137988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 376256
    },
    {
      "epoch": 0.0013656238314888247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 376288
    },
    {
      "epoch": 0.0013657399658396614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 376320
    },
    {
      "epoch": 0.0013658561001904982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 376352
    },
    {
      "epoch": 0.0013659722345413347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 376384
    },
    {
      "epoch": 0.0013660883688921715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 376416
    },
    {
      "epoch": 0.0013662045032430082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 376448
    },
    {
      "epoch": 0.001366320637593845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 376480
    },
    {
      "epoch": 0.0013664367719446815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 376512
    },
    {
      "epoch": 0.0013665529062955183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7495,
      "step": 376544
    },
    {
      "epoch": 0.001366669040646355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7507,
      "step": 376576
    },
    {
      "epoch": 0.0013667851749971918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 376608
    },
    {
      "epoch": 0.0013669013093480285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 376640
    },
    {
      "epoch": 0.001367017443698865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 376672
    },
    {
      "epoch": 0.0013671335780497018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 376704
    },
    {
      "epoch": 0.0013672497124005386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 376736
    },
    {
      "epoch": 0.0013673658467513753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 376768
    },
    {
      "epoch": 0.0013674819811022119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 376800
    },
    {
      "epoch": 0.0013675981154530486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7431,
      "step": 376832
    },
    {
      "epoch": 0.0013677142498038854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 376864
    },
    {
      "epoch": 0.0013678303841547221,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.753,
      "step": 376896
    },
    {
      "epoch": 0.001367946518505559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 376928
    },
    {
      "epoch": 0.0013680626528563954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 376960
    },
    {
      "epoch": 0.0013681787872072322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 376992
    },
    {
      "epoch": 0.001368294921558069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 377024
    },
    {
      "epoch": 0.0013684110559089057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 377056
    },
    {
      "epoch": 0.0013685271902597422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 377088
    },
    {
      "epoch": 0.001368643324610579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 377120
    },
    {
      "epoch": 0.0013687594589614157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 377152
    },
    {
      "epoch": 0.0013688755933122525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7449,
      "step": 377184
    },
    {
      "epoch": 0.001368991727663089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 377216
    },
    {
      "epoch": 0.0013691078620139258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 377248
    },
    {
      "epoch": 0.0013692239963647625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6851,
      "step": 377280
    },
    {
      "epoch": 0.0013693401307155993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 377312
    },
    {
      "epoch": 0.001369456265066436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 377344
    },
    {
      "epoch": 0.0013695723994172726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 377376
    },
    {
      "epoch": 0.0013696885337681093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 377408
    },
    {
      "epoch": 0.001369804668118946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 377440
    },
    {
      "epoch": 0.0013699208024697828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 377472
    },
    {
      "epoch": 0.0013700369368206194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 377504
    },
    {
      "epoch": 0.0013701530711714561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6867,
      "step": 377536
    },
    {
      "epoch": 0.001370269205522293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6937,
      "step": 377568
    },
    {
      "epoch": 0.0013703853398731296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 377600
    },
    {
      "epoch": 0.0013705014742239664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 377632
    },
    {
      "epoch": 0.001370617608574803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 377664
    },
    {
      "epoch": 0.0013707337429256397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.752,
      "step": 377696
    },
    {
      "epoch": 0.0013708498772764764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 377728
    },
    {
      "epoch": 0.0013709660116273132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 377760
    },
    {
      "epoch": 0.0013710821459781497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 377792
    },
    {
      "epoch": 0.0013711982803289865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 377824
    },
    {
      "epoch": 0.0013713144146798232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 377856
    },
    {
      "epoch": 0.00137143054903066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 377888
    },
    {
      "epoch": 0.0013715466833814968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 377920
    },
    {
      "epoch": 0.0013716628177323333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 377952
    },
    {
      "epoch": 0.00137177895208317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 377984
    },
    {
      "epoch": 0.0013718950864340068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 378016
    },
    {
      "epoch": 0.0013720112207848436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 378048
    },
    {
      "epoch": 0.00137212735513568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 378080
    },
    {
      "epoch": 0.0013722434894865168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.687,
      "step": 378112
    },
    {
      "epoch": 0.0013723596238373536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 378144
    },
    {
      "epoch": 0.0013724757581881904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 378176
    },
    {
      "epoch": 0.001372591892539027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 378208
    },
    {
      "epoch": 0.0013727080268898636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 378240
    },
    {
      "epoch": 0.0013728241612407004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 378272
    },
    {
      "epoch": 0.0013729402955915372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 378304
    },
    {
      "epoch": 0.001373056429942374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7491,
      "step": 378336
    },
    {
      "epoch": 0.0013731725642932104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 378368
    },
    {
      "epoch": 0.0013732886986440472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6903,
      "step": 378400
    },
    {
      "epoch": 0.001373404832994884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 378432
    },
    {
      "epoch": 0.0013735209673457207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 378464
    },
    {
      "epoch": 0.0013736371016965575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 378496
    },
    {
      "epoch": 0.001373753236047394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 378528
    },
    {
      "epoch": 0.0013738693703982308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 378560
    },
    {
      "epoch": 0.0013739855047490675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 378592
    },
    {
      "epoch": 0.0013741016390999043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 378624
    },
    {
      "epoch": 0.0013742177734507408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 378656
    },
    {
      "epoch": 0.0013743339078015776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 378688
    },
    {
      "epoch": 0.0013744500421524143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 378720
    },
    {
      "epoch": 0.001374566176503251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 378752
    },
    {
      "epoch": 0.0013746823108540878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7522,
      "step": 378784
    },
    {
      "epoch": 0.0013747984452049244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 378816
    },
    {
      "epoch": 0.001374914579555761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 378848
    },
    {
      "epoch": 0.0013750307139065979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 378880
    },
    {
      "epoch": 0.0013751468482574346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 378912
    },
    {
      "epoch": 0.0013752629826082712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 378944
    },
    {
      "epoch": 0.001375379116959108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 378976
    },
    {
      "epoch": 0.0013754952513099447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 379008
    },
    {
      "epoch": 0.0013756113856607814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 379040
    },
    {
      "epoch": 0.0013757275200116182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7484,
      "step": 379072
    },
    {
      "epoch": 0.0013758436543624547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7504,
      "step": 379104
    },
    {
      "epoch": 0.0013759597887132915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 379136
    },
    {
      "epoch": 0.0013760759230641282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 379168
    },
    {
      "epoch": 0.001376192057414965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7412,
      "step": 379200
    },
    {
      "epoch": 0.0013763081917658015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 379232
    },
    {
      "epoch": 0.0013764243261166383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 379264
    },
    {
      "epoch": 0.001376540460467475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 379296
    },
    {
      "epoch": 0.0013766565948183118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 379328
    },
    {
      "epoch": 0.0013767727291691485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 379360
    },
    {
      "epoch": 0.001376888863519985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 379392
    },
    {
      "epoch": 0.0013770049978708218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 379424
    },
    {
      "epoch": 0.0013771211322216586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 379456
    },
    {
      "epoch": 0.0013772372665724953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 379488
    },
    {
      "epoch": 0.0013773534009233319,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 379520
    },
    {
      "epoch": 0.0013774695352741686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 379552
    },
    {
      "epoch": 0.0013775856696250054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 379584
    },
    {
      "epoch": 0.0013777018039758421,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 379616
    },
    {
      "epoch": 0.0013778179383266789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 379648
    },
    {
      "epoch": 0.0013779340726775154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7543,
      "step": 379680
    },
    {
      "epoch": 0.0013780502070283522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 379712
    },
    {
      "epoch": 0.001378166341379189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 379744
    },
    {
      "epoch": 0.0013782824757300257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7536,
      "step": 379776
    },
    {
      "epoch": 0.0013783986100808622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 379808
    },
    {
      "epoch": 0.001378514744431699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 379840
    },
    {
      "epoch": 0.0013786308787825357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 379872
    },
    {
      "epoch": 0.0013787470131333725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 379904
    },
    {
      "epoch": 0.0013788631474842092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 379936
    },
    {
      "epoch": 0.0013789792818350458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 379968
    },
    {
      "epoch": 0.0013790954161858825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 380000
    },
    {
      "epoch": 0.0013792115505367193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 380032
    },
    {
      "epoch": 0.001379327684887556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 380064
    },
    {
      "epoch": 0.0013794438192383926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 380096
    },
    {
      "epoch": 0.0013795599535892293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 380128
    },
    {
      "epoch": 0.001379676087940066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 380160
    },
    {
      "epoch": 0.0013797922222909028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 380192
    },
    {
      "epoch": 0.0013799083566417396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 380224
    },
    {
      "epoch": 0.0013800244909925761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 380256
    },
    {
      "epoch": 0.0013801406253434129,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 380288
    },
    {
      "epoch": 0.0013802567596942496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 380320
    },
    {
      "epoch": 0.0013803728940450864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 380352
    },
    {
      "epoch": 0.001380489028395923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 380384
    },
    {
      "epoch": 0.0013806051627467597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 380416
    },
    {
      "epoch": 0.0013807212970975964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6927,
      "step": 380448
    },
    {
      "epoch": 0.0013808374314484332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 380480
    },
    {
      "epoch": 0.00138095356579927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 380512
    },
    {
      "epoch": 0.0013810697001501065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 380544
    },
    {
      "epoch": 0.0013811858345009432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 380576
    },
    {
      "epoch": 0.00138130196885178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 380608
    },
    {
      "epoch": 0.0013814181032026167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 380640
    },
    {
      "epoch": 0.0013815342375534533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 380672
    },
    {
      "epoch": 0.00138165037190429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 380704
    },
    {
      "epoch": 0.0013817665062551268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6868,
      "step": 380736
    },
    {
      "epoch": 0.0013818826406059635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 380768
    },
    {
      "epoch": 0.0013819987749568003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 380800
    },
    {
      "epoch": 0.0013821149093076368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 380832
    },
    {
      "epoch": 0.0013822310436584736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 380864
    },
    {
      "epoch": 0.0013823471780093103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 380896
    },
    {
      "epoch": 0.001382463312360147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 380928
    },
    {
      "epoch": 0.0013825794467109836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.751,
      "step": 380960
    },
    {
      "epoch": 0.0013826955810618204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 380992
    },
    {
      "epoch": 0.0013828117154126571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 381024
    },
    {
      "epoch": 0.0013829278497634939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 381056
    },
    {
      "epoch": 0.0013830439841143306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 381088
    },
    {
      "epoch": 0.0013831601184651672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 381120
    },
    {
      "epoch": 0.001383276252816004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 381152
    },
    {
      "epoch": 0.0013833923871668407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 381184
    },
    {
      "epoch": 0.0013835085215176774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7485,
      "step": 381216
    },
    {
      "epoch": 0.001383624655868514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7492,
      "step": 381248
    },
    {
      "epoch": 0.0013837407902193507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 381280
    },
    {
      "epoch": 0.0013838569245701875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 381312
    },
    {
      "epoch": 0.0013839730589210242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 381344
    },
    {
      "epoch": 0.001384089193271861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 381376
    },
    {
      "epoch": 0.0013842053276226975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 381408
    },
    {
      "epoch": 0.0013843214619735343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 381440
    },
    {
      "epoch": 0.001384437596324371,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 381472
    },
    {
      "epoch": 0.0013845537306752078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.756,
      "step": 381504
    },
    {
      "epoch": 0.0013846698650260443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 381536
    },
    {
      "epoch": 0.001384785999376881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 381568
    },
    {
      "epoch": 0.0013849021337277178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 381600
    },
    {
      "epoch": 0.0013850182680785546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 381632
    },
    {
      "epoch": 0.0013851344024293913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 381664
    },
    {
      "epoch": 0.0013852505367802279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 381696
    },
    {
      "epoch": 0.0013853666711310646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 381728
    },
    {
      "epoch": 0.0013854828054819014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 381760
    },
    {
      "epoch": 0.0013855989398327381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 381792
    },
    {
      "epoch": 0.0013857150741835747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 381824
    },
    {
      "epoch": 0.0013858312085344114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 381856
    },
    {
      "epoch": 0.0013859473428852482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 381888
    },
    {
      "epoch": 0.001386063477236085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 381920
    },
    {
      "epoch": 0.0013861796115869217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 381952
    },
    {
      "epoch": 0.0013862957459377582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 381984
    },
    {
      "epoch": 0.001386411880288595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 382016
    },
    {
      "epoch": 0.0013865280146394317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7447,
      "step": 382048
    },
    {
      "epoch": 0.0013866441489902685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 382080
    },
    {
      "epoch": 0.001386760283341105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 382112
    },
    {
      "epoch": 0.0013868764176919418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 382144
    },
    {
      "epoch": 0.0013869925520427785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 382176
    },
    {
      "epoch": 0.0013871086863936153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 382208
    },
    {
      "epoch": 0.001387224820744452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 382240
    },
    {
      "epoch": 0.0013873409550952886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 382272
    },
    {
      "epoch": 0.0013874570894461253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 382304
    },
    {
      "epoch": 0.001387573223796962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 382336
    },
    {
      "epoch": 0.0013876893581477989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 382368
    },
    {
      "epoch": 0.0013878054924986354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 382400
    },
    {
      "epoch": 0.0013879216268494721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 382432
    },
    {
      "epoch": 0.001388037761200309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 382464
    },
    {
      "epoch": 0.0013881538955511457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 382496
    },
    {
      "epoch": 0.0013882700299019824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 382528
    },
    {
      "epoch": 0.001388386164252819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 382560
    },
    {
      "epoch": 0.0013885022986036557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 382592
    },
    {
      "epoch": 0.0013886184329544925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 382624
    },
    {
      "epoch": 0.0013887345673053292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 382656
    },
    {
      "epoch": 0.0013888507016561657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 382688
    },
    {
      "epoch": 0.0013889668360070025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 382720
    },
    {
      "epoch": 0.0013890829703578393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 382752
    },
    {
      "epoch": 0.001389199104708676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 382784
    },
    {
      "epoch": 0.0013893152390595128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 382816
    },
    {
      "epoch": 0.0013894313734103493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 382848
    },
    {
      "epoch": 0.001389547507761186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 382880
    },
    {
      "epoch": 0.0013896636421120228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6884,
      "step": 382912
    },
    {
      "epoch": 0.0013897797764628596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6917,
      "step": 382944
    },
    {
      "epoch": 0.001389895910813696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 382976
    },
    {
      "epoch": 0.0013900120451645329,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 383008
    },
    {
      "epoch": 0.0013901281795153696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 383040
    },
    {
      "epoch": 0.0013902443138662064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 383072
    },
    {
      "epoch": 0.0013903604482170431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 383104
    },
    {
      "epoch": 0.0013904765825678797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 383136
    },
    {
      "epoch": 0.0013905927169187164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 383168
    },
    {
      "epoch": 0.0013907088512695532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 383200
    },
    {
      "epoch": 0.00139082498562039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 383232
    },
    {
      "epoch": 0.0013909411199712265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 383264
    },
    {
      "epoch": 0.0013910572543220632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 383296
    },
    {
      "epoch": 0.0013911733886729,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 383328
    },
    {
      "epoch": 0.0013912895230237367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 383360
    },
    {
      "epoch": 0.0013914056573745735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 383392
    },
    {
      "epoch": 0.00139152179172541,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 383424
    },
    {
      "epoch": 0.0013916379260762468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 383456
    },
    {
      "epoch": 0.0013917540604270835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 383488
    },
    {
      "epoch": 0.0013918701947779203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 383520
    },
    {
      "epoch": 0.0013919863291287568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 383552
    },
    {
      "epoch": 0.0013921024634795936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 383584
    },
    {
      "epoch": 0.0013922185978304303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 383616
    },
    {
      "epoch": 0.001392334732181267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 383648
    },
    {
      "epoch": 0.0013924508665321038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 383680
    },
    {
      "epoch": 0.0013925670008829404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 383712
    },
    {
      "epoch": 0.0013926831352337771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 383744
    },
    {
      "epoch": 0.0013927992695846139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 383776
    },
    {
      "epoch": 0.0013929154039354506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 383808
    },
    {
      "epoch": 0.0013930315382862872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 383840
    },
    {
      "epoch": 0.001393147672637124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 383872
    },
    {
      "epoch": 0.0013932638069879607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 383904
    },
    {
      "epoch": 0.0013933799413387974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 383936
    },
    {
      "epoch": 0.0013934960756896342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 383968
    },
    {
      "epoch": 0.0013936122100404707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 384000
    },
    {
      "epoch": 0.0013937283443913075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 384032
    },
    {
      "epoch": 0.0013938444787421442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 384064
    },
    {
      "epoch": 0.001393960613092981,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 384096
    },
    {
      "epoch": 0.0013940767474438175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 384128
    },
    {
      "epoch": 0.0013941928817946543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 384160
    },
    {
      "epoch": 0.001394309016145491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 384192
    },
    {
      "epoch": 0.0013944251504963278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 384224
    },
    {
      "epoch": 0.0013945412848471645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.688,
      "step": 384256
    },
    {
      "epoch": 0.001394657419198001,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 384288
    },
    {
      "epoch": 0.0013947735535488378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 384320
    },
    {
      "epoch": 0.0013948896878996746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 384352
    },
    {
      "epoch": 0.0013950058222505113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 384384
    },
    {
      "epoch": 0.0013951219566013479,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 384416
    },
    {
      "epoch": 0.0013952380909521846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 384448
    },
    {
      "epoch": 0.0013953542253030214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7412,
      "step": 384480
    },
    {
      "epoch": 0.0013954703596538581,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 384512
    },
    {
      "epoch": 0.0013955864940046949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 384544
    },
    {
      "epoch": 0.0013957026283555314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 384576
    },
    {
      "epoch": 0.0013958187627063682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 384608
    },
    {
      "epoch": 0.001395934897057205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 384640
    },
    {
      "epoch": 0.0013960510314080417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 384672
    },
    {
      "epoch": 0.0013961671657588782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.749,
      "step": 384704
    },
    {
      "epoch": 0.001396283300109715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7628,
      "step": 384736
    },
    {
      "epoch": 0.0013963994344605517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 384768
    },
    {
      "epoch": 0.0013965155688113885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 384800
    },
    {
      "epoch": 0.0013966317031622252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 384832
    },
    {
      "epoch": 0.0013967478375130618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 384864
    },
    {
      "epoch": 0.0013968639718638985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 384896
    },
    {
      "epoch": 0.0013969801062147353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 384928
    },
    {
      "epoch": 0.001397096240565572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 384960
    },
    {
      "epoch": 0.0013972123749164086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 384992
    },
    {
      "epoch": 0.0013973285092672453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 385024
    },
    {
      "epoch": 0.001397444643618082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 385056
    },
    {
      "epoch": 0.0013975607779689188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6904,
      "step": 385088
    },
    {
      "epoch": 0.0013976769123197556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6774,
      "step": 385120
    },
    {
      "epoch": 0.0013977930466705921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 385152
    },
    {
      "epoch": 0.0013979091810214289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 385184
    },
    {
      "epoch": 0.0013980253153722656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 385216
    },
    {
      "epoch": 0.0013981414497231024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 385248
    },
    {
      "epoch": 0.001398257584073939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 385280
    },
    {
      "epoch": 0.0013983737184247757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 385312
    },
    {
      "epoch": 0.0013984898527756124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 385344
    },
    {
      "epoch": 0.0013986059871264492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 385376
    },
    {
      "epoch": 0.001398722121477286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 385408
    },
    {
      "epoch": 0.0013988382558281225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 385440
    },
    {
      "epoch": 0.0013989543901789592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 385472
    },
    {
      "epoch": 0.001399070524529796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 385504
    },
    {
      "epoch": 0.0013991866588806327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 385536
    },
    {
      "epoch": 0.0013993027932314693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 385568
    },
    {
      "epoch": 0.001399418927582306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7539,
      "step": 385600
    },
    {
      "epoch": 0.0013995350619331428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 385632
    },
    {
      "epoch": 0.0013996511962839795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 385664
    },
    {
      "epoch": 0.0013997673306348163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 385696
    },
    {
      "epoch": 0.0013998834649856528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 385728
    },
    {
      "epoch": 0.0013999995993364896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 385760
    },
    {
      "epoch": 0.0014001157336873263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 385792
    },
    {
      "epoch": 0.001400231868038163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 385824
    },
    {
      "epoch": 0.0014003480023889996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 385856
    },
    {
      "epoch": 0.0014004641367398364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 385888
    },
    {
      "epoch": 0.0014005802710906731,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 385920
    },
    {
      "epoch": 0.00140069640544151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 385952
    },
    {
      "epoch": 0.0014008125397923466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 385984
    },
    {
      "epoch": 0.0014009286741431832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 386016
    },
    {
      "epoch": 0.00140104480849402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 386048
    },
    {
      "epoch": 0.0014011609428448567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 386080
    },
    {
      "epoch": 0.0014012770771956934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 386112
    },
    {
      "epoch": 0.00140139321154653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7431,
      "step": 386144
    },
    {
      "epoch": 0.0014015093458973667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 386176
    },
    {
      "epoch": 0.0014016254802482035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 386208
    },
    {
      "epoch": 0.0014017416145990402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 386240
    },
    {
      "epoch": 0.001401857748949877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 386272
    },
    {
      "epoch": 0.0014019738833007135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 386304
    },
    {
      "epoch": 0.0014020900176515503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 386336
    },
    {
      "epoch": 0.001402206152002387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 386368
    },
    {
      "epoch": 0.0014023222863532238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 386400
    },
    {
      "epoch": 0.0014024384207040603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 386432
    },
    {
      "epoch": 0.001402554555054897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 386464
    },
    {
      "epoch": 0.0014026706894057338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 386496
    },
    {
      "epoch": 0.0014027868237565706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 386528
    },
    {
      "epoch": 0.0014029029581074074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 386560
    },
    {
      "epoch": 0.001403019092458244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 386592
    },
    {
      "epoch": 0.0014031352268090806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 386624
    },
    {
      "epoch": 0.0014032513611599174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.748,
      "step": 386656
    },
    {
      "epoch": 0.0014033674955107542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.747,
      "step": 386688
    },
    {
      "epoch": 0.0014034836298615907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 386720
    },
    {
      "epoch": 0.0014035997642124274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 386752
    },
    {
      "epoch": 0.0014037158985632642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 386784
    },
    {
      "epoch": 0.001403832032914101,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 386816
    },
    {
      "epoch": 0.0014039481672649377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 386848
    },
    {
      "epoch": 0.0014040643016157742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 386880
    },
    {
      "epoch": 0.001404180435966611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 386912
    },
    {
      "epoch": 0.0014042965703174478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 386944
    },
    {
      "epoch": 0.0014044127046682845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 386976
    },
    {
      "epoch": 0.001404528839019121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 387008
    },
    {
      "epoch": 0.0014046449733699578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 387040
    },
    {
      "epoch": 0.0014047611077207946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 387072
    },
    {
      "epoch": 0.0014048772420716313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 387104
    },
    {
      "epoch": 0.001404993376422468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 387136
    },
    {
      "epoch": 0.0014051095107733046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 387168
    },
    {
      "epoch": 0.0014052256451241414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 387200
    },
    {
      "epoch": 0.001405341779474978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 387232
    },
    {
      "epoch": 0.0014054579138258149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 387264
    },
    {
      "epoch": 0.0014055740481766514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.688,
      "step": 387296
    },
    {
      "epoch": 0.0014056901825274882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 387328
    },
    {
      "epoch": 0.001405806316878325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 387360
    },
    {
      "epoch": 0.0014059224512291617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 387392
    },
    {
      "epoch": 0.0014060385855799984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 387424
    },
    {
      "epoch": 0.001406154719930835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 387456
    },
    {
      "epoch": 0.0014062708542816717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6898,
      "step": 387488
    },
    {
      "epoch": 0.0014063869886325085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 387520
    },
    {
      "epoch": 0.0014065031229833452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7567,
      "step": 387552
    },
    {
      "epoch": 0.0014066192573341818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 387584
    },
    {
      "epoch": 0.0014067353916850185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 387616
    },
    {
      "epoch": 0.0014068515260358553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 387648
    },
    {
      "epoch": 0.001406967660386692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 387680
    },
    {
      "epoch": 0.0014070837947375288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 387712
    },
    {
      "epoch": 0.0014071999290883653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 387744
    },
    {
      "epoch": 0.001407316063439202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 387776
    },
    {
      "epoch": 0.0014074321977900388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 387808
    },
    {
      "epoch": 0.0014075483321408756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 387840
    },
    {
      "epoch": 0.001407664466491712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 387872
    },
    {
      "epoch": 0.0014077806008425489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 387904
    },
    {
      "epoch": 0.0014078967351933856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 387936
    },
    {
      "epoch": 0.0014080128695442224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 387968
    },
    {
      "epoch": 0.0014081290038950591,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 388000
    },
    {
      "epoch": 0.0014082451382458957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 388032
    },
    {
      "epoch": 0.0014083612725967324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 388064
    },
    {
      "epoch": 0.0014084774069475692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 388096
    },
    {
      "epoch": 0.001408593541298406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 388128
    },
    {
      "epoch": 0.0014087096756492425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 388160
    },
    {
      "epoch": 0.0014088258100000792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 388192
    },
    {
      "epoch": 0.001408941944350916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 388224
    },
    {
      "epoch": 0.0014090580787017527,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 388256
    },
    {
      "epoch": 0.0014091742130525895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 388288
    },
    {
      "epoch": 0.001409290347403426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 388320
    },
    {
      "epoch": 0.0014094064817542628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 388352
    },
    {
      "epoch": 0.0014095226161050995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 388384
    },
    {
      "epoch": 0.0014096387504559363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 388416
    },
    {
      "epoch": 0.0014097548848067728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 388448
    },
    {
      "epoch": 0.0014098710191576096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 388480
    },
    {
      "epoch": 0.0014099871535084463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 388512
    },
    {
      "epoch": 0.001410103287859283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 388544
    },
    {
      "epoch": 0.0014102194222101198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6743,
      "step": 388576
    },
    {
      "epoch": 0.0014103355565609564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 388608
    },
    {
      "epoch": 0.0014104516909117931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 388640
    },
    {
      "epoch": 0.0014105678252626299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 388672
    },
    {
      "epoch": 0.0014106839596134666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 388704
    },
    {
      "epoch": 0.0014108000939643032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 388736
    },
    {
      "epoch": 0.00141091622831514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 388768
    },
    {
      "epoch": 0.0014110323626659767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 388800
    },
    {
      "epoch": 0.0014111484970168134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 388832
    },
    {
      "epoch": 0.0014112646313676502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 388864
    },
    {
      "epoch": 0.0014113807657184867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 388896
    },
    {
      "epoch": 0.0014114969000693235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 388928
    },
    {
      "epoch": 0.0014116130344201602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 388960
    },
    {
      "epoch": 0.001411729168770997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 388992
    },
    {
      "epoch": 0.0014118453031218335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 389024
    },
    {
      "epoch": 0.0014119614374726703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 389056
    },
    {
      "epoch": 0.001412077571823507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 389088
    },
    {
      "epoch": 0.0014121937061743438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 389120
    },
    {
      "epoch": 0.0014123098405251805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 389152
    },
    {
      "epoch": 0.001412425974876017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 389184
    },
    {
      "epoch": 0.0014125421092268538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 389216
    },
    {
      "epoch": 0.0014126582435776906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 389248
    },
    {
      "epoch": 0.0014127743779285273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 389280
    },
    {
      "epoch": 0.0014128905122793639,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 389312
    },
    {
      "epoch": 0.0014130066466302006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 389344
    },
    {
      "epoch": 0.0014131227809810374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 389376
    },
    {
      "epoch": 0.0014132389153318741,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 389408
    },
    {
      "epoch": 0.0014133550496827109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 389440
    },
    {
      "epoch": 0.0014134711840335474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 389472
    },
    {
      "epoch": 0.0014135873183843842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 389504
    },
    {
      "epoch": 0.001413703452735221,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 389536
    },
    {
      "epoch": 0.0014138195870860577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 389568
    },
    {
      "epoch": 0.0014139357214368942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 389600
    },
    {
      "epoch": 0.001414051855787731,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7662,
      "step": 389632
    },
    {
      "epoch": 0.0014141679901385677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 389664
    },
    {
      "epoch": 0.0014142841244894045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 389696
    },
    {
      "epoch": 0.0014144002588402412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 389728
    },
    {
      "epoch": 0.0014145163931910778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 389760
    },
    {
      "epoch": 0.0014146325275419145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 389792
    },
    {
      "epoch": 0.0014147486618927513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 389824
    },
    {
      "epoch": 0.001414864796243588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 389856
    },
    {
      "epoch": 0.0014149809305944246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 389888
    },
    {
      "epoch": 0.0014150970649452613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 389920
    },
    {
      "epoch": 0.001415213199296098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 389952
    },
    {
      "epoch": 0.0014153293336469348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 389984
    },
    {
      "epoch": 0.0014154454679977716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 390016
    },
    {
      "epoch": 0.0014155616023486081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 390048
    },
    {
      "epoch": 0.0014156777366994449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 390080
    },
    {
      "epoch": 0.0014157938710502816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 390112
    },
    {
      "epoch": 0.0014159100054011184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 390144
    },
    {
      "epoch": 0.001416026139751955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 390176
    },
    {
      "epoch": 0.0014161422741027917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 390208
    },
    {
      "epoch": 0.0014162584084536284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 390240
    },
    {
      "epoch": 0.0014163745428044652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 390272
    },
    {
      "epoch": 0.001416490677155302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 390304
    },
    {
      "epoch": 0.0014166068115061385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 390336
    },
    {
      "epoch": 0.0014167229458569752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 390368
    },
    {
      "epoch": 0.001416839080207812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 390400
    },
    {
      "epoch": 0.0014169552145586487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 390432
    },
    {
      "epoch": 0.0014170713489094853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 390464
    },
    {
      "epoch": 0.001417187483260322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 390496
    },
    {
      "epoch": 0.0014173036176111588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 390528
    },
    {
      "epoch": 0.0014174197519619955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 390560
    },
    {
      "epoch": 0.0014175358863128323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 390592
    },
    {
      "epoch": 0.0014176520206636688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 390624
    },
    {
      "epoch": 0.0014177681550145056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 390656
    },
    {
      "epoch": 0.0014178842893653423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 390688
    },
    {
      "epoch": 0.001418000423716179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 390720
    },
    {
      "epoch": 0.0014181165580670156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 390752
    },
    {
      "epoch": 0.0014182326924178524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 390784
    },
    {
      "epoch": 0.0014183488267686891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 390816
    },
    {
      "epoch": 0.001418464961119526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 390848
    },
    {
      "epoch": 0.0014185810954703627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 390880
    },
    {
      "epoch": 0.0014186972298211992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 390912
    },
    {
      "epoch": 0.001418813364172036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 390944
    },
    {
      "epoch": 0.0014189294985228727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 390976
    },
    {
      "epoch": 0.0014190456328737095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 391008
    },
    {
      "epoch": 0.001419161767224546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 391040
    },
    {
      "epoch": 0.0014192779015753827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7523,
      "step": 391072
    },
    {
      "epoch": 0.0014193940359262195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 391104
    },
    {
      "epoch": 0.0014195101702770563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 391136
    },
    {
      "epoch": 0.001419626304627893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 391168
    },
    {
      "epoch": 0.0014197424389787295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 391200
    },
    {
      "epoch": 0.0014198585733295663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 391232
    },
    {
      "epoch": 0.001419974707680403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 391264
    },
    {
      "epoch": 0.0014200908420312398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 391296
    },
    {
      "epoch": 0.0014202069763820763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 391328
    },
    {
      "epoch": 0.001420323110732913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 391360
    },
    {
      "epoch": 0.0014204392450837499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 391392
    },
    {
      "epoch": 0.0014205553794345866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 391424
    },
    {
      "epoch": 0.0014206715137854234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 391456
    },
    {
      "epoch": 0.00142078764813626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.747,
      "step": 391488
    },
    {
      "epoch": 0.0014209037824870967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 391520
    },
    {
      "epoch": 0.0014210199168379334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 391552
    },
    {
      "epoch": 0.0014211360511887702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 391584
    },
    {
      "epoch": 0.0014212521855396067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 391616
    },
    {
      "epoch": 0.0014213683198904435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6993,
      "step": 391648
    },
    {
      "epoch": 0.0014214844542412802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 391680
    },
    {
      "epoch": 0.001421600588592117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 391712
    },
    {
      "epoch": 0.0014217167229429537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7479,
      "step": 391744
    },
    {
      "epoch": 0.0014218328572937903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7539,
      "step": 391776
    },
    {
      "epoch": 0.001421948991644627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 391808
    },
    {
      "epoch": 0.0014220651259954638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 391840
    },
    {
      "epoch": 0.0014221812603463005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 391872
    },
    {
      "epoch": 0.001422297394697137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 391904
    },
    {
      "epoch": 0.0014224135290479738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7725,
      "step": 391936
    },
    {
      "epoch": 0.0014225296633988106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 391968
    },
    {
      "epoch": 0.0014226457977496473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 392000
    },
    {
      "epoch": 0.001422761932100484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 392032
    },
    {
      "epoch": 0.0014228780664513206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 392064
    },
    {
      "epoch": 0.0014229942008021574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 392096
    },
    {
      "epoch": 0.0014231103351529941,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6918,
      "step": 392128
    },
    {
      "epoch": 0.0014232264695038309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 392160
    },
    {
      "epoch": 0.0014233426038546674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 392192
    },
    {
      "epoch": 0.0014234587382055042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 392224
    },
    {
      "epoch": 0.001423574872556341,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 392256
    },
    {
      "epoch": 0.0014236910069071777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6934,
      "step": 392288
    },
    {
      "epoch": 0.0014238071412580144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 392320
    },
    {
      "epoch": 0.001423923275608851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 392352
    },
    {
      "epoch": 0.0014240394099596877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 392384
    },
    {
      "epoch": 0.0014241555443105245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 392416
    },
    {
      "epoch": 0.0014242716786613612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 392448
    },
    {
      "epoch": 0.0014243878130121978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 392480
    },
    {
      "epoch": 0.0014245039473630345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 392512
    },
    {
      "epoch": 0.0014246200817138713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 392544
    },
    {
      "epoch": 0.001424736216064708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 392576
    },
    {
      "epoch": 0.0014248523504155448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7531,
      "step": 392608
    },
    {
      "epoch": 0.0014249684847663813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 392640
    },
    {
      "epoch": 0.001425084619117218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 392672
    },
    {
      "epoch": 0.0014252007534680548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 392704
    },
    {
      "epoch": 0.0014253168878188916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6818,
      "step": 392736
    },
    {
      "epoch": 0.0014254330221697281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 392768
    },
    {
      "epoch": 0.0014255491565205649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 392800
    },
    {
      "epoch": 0.0014256652908714016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 392832
    },
    {
      "epoch": 0.0014257814252222384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 392864
    },
    {
      "epoch": 0.0014258975595730751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 392896
    },
    {
      "epoch": 0.0014260136939239117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 392928
    },
    {
      "epoch": 0.0014261298282747484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 392960
    },
    {
      "epoch": 0.0014262459626255852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 392992
    },
    {
      "epoch": 0.001426362096976422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 393024
    },
    {
      "epoch": 0.0014264782313272585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 393056
    },
    {
      "epoch": 0.0014265943656780952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 393088
    },
    {
      "epoch": 0.001426710500028932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 393120
    },
    {
      "epoch": 0.0014268266343797687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 393152
    },
    {
      "epoch": 0.0014269427687306055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 393184
    },
    {
      "epoch": 0.001427058903081442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 393216
    },
    {
      "epoch": 0.0014271750374322788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 393248
    },
    {
      "epoch": 0.0014272911717831155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 393280
    },
    {
      "epoch": 0.0014274073061339523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 393312
    },
    {
      "epoch": 0.0014275234404847888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 393344
    },
    {
      "epoch": 0.0014276395748356256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 393376
    },
    {
      "epoch": 0.0014277557091864623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 393408
    },
    {
      "epoch": 0.001427871843537299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 393440
    },
    {
      "epoch": 0.0014279879778881358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 393472
    },
    {
      "epoch": 0.0014281041122389724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 393504
    },
    {
      "epoch": 0.0014282202465898091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 393536
    },
    {
      "epoch": 0.0014283363809406459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 393568
    },
    {
      "epoch": 0.0014284525152914826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 393600
    },
    {
      "epoch": 0.0014285686496423192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 393632
    },
    {
      "epoch": 0.001428684783993156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 393664
    },
    {
      "epoch": 0.0014288009183439927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7575,
      "step": 393696
    },
    {
      "epoch": 0.0014289170526948294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 393728
    },
    {
      "epoch": 0.0014290331870456662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 393760
    },
    {
      "epoch": 0.0014291493213965027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 393792
    },
    {
      "epoch": 0.0014292654557473395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 393824
    },
    {
      "epoch": 0.0014293815900981762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 393856
    },
    {
      "epoch": 0.001429497724449013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 393888
    },
    {
      "epoch": 0.0014296138587998495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 393920
    },
    {
      "epoch": 0.0014297299931506863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 393952
    },
    {
      "epoch": 0.001429846127501523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 393984
    },
    {
      "epoch": 0.0014299622618523598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 394016
    },
    {
      "epoch": 0.0014300783962031965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 394048
    },
    {
      "epoch": 0.001430194530554033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 394080
    },
    {
      "epoch": 0.0014303106649048698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7457,
      "step": 394112
    },
    {
      "epoch": 0.0014304267992557066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 394144
    },
    {
      "epoch": 0.0014305429336065433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6936,
      "step": 394176
    },
    {
      "epoch": 0.0014306590679573799,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 394208
    },
    {
      "epoch": 0.0014307752023082166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 394240
    },
    {
      "epoch": 0.0014308913366590534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 394272
    },
    {
      "epoch": 0.0014310074710098901,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 394304
    },
    {
      "epoch": 0.001431123605360727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 394336
    },
    {
      "epoch": 0.0014312397397115634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 394368
    },
    {
      "epoch": 0.0014313558740624002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 394400
    },
    {
      "epoch": 0.001431472008413237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 394432
    },
    {
      "epoch": 0.0014315881427640737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 394464
    },
    {
      "epoch": 0.0014317042771149102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 394496
    },
    {
      "epoch": 0.001431820411465747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 394528
    },
    {
      "epoch": 0.0014319365458165837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7745,
      "step": 394560
    },
    {
      "epoch": 0.0014320526801674205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7434,
      "step": 394592
    },
    {
      "epoch": 0.0014321688145182573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 394624
    },
    {
      "epoch": 0.0014322849488690938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 394656
    },
    {
      "epoch": 0.0014324010832199305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.692,
      "step": 394688
    },
    {
      "epoch": 0.0014325172175707673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6901,
      "step": 394720
    },
    {
      "epoch": 0.001432633351921604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 394752
    },
    {
      "epoch": 0.0014327494862724406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 394784
    },
    {
      "epoch": 0.0014328656206232773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 394816
    },
    {
      "epoch": 0.001432981754974114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 394848
    },
    {
      "epoch": 0.0014330978893249509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 394880
    },
    {
      "epoch": 0.0014332140236757876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 394912
    },
    {
      "epoch": 0.0014333301580266241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 394944
    },
    {
      "epoch": 0.001433446292377461,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 394976
    },
    {
      "epoch": 0.0014335624267282977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 395008
    },
    {
      "epoch": 0.0014336785610791344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 395040
    },
    {
      "epoch": 0.001433794695429971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 395072
    },
    {
      "epoch": 0.0014339108297808077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6914,
      "step": 395104
    },
    {
      "epoch": 0.0014340269641316445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 395136
    },
    {
      "epoch": 0.0014341430984824812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 395168
    },
    {
      "epoch": 0.001434259232833318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 395200
    },
    {
      "epoch": 0.0014343753671841545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 395232
    },
    {
      "epoch": 0.0014344915015349912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 395264
    },
    {
      "epoch": 0.001434607635885828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 395296
    },
    {
      "epoch": 0.0014347237702366648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 395328
    },
    {
      "epoch": 0.0014348399045875013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6943,
      "step": 395360
    },
    {
      "epoch": 0.001434956038938338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 395392
    },
    {
      "epoch": 0.0014350721732891748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 395424
    },
    {
      "epoch": 0.0014351883076400116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 395456
    },
    {
      "epoch": 0.0014353044419908483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 395488
    },
    {
      "epoch": 0.0014354205763416848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 395520
    },
    {
      "epoch": 0.0014355367106925216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 395552
    },
    {
      "epoch": 0.0014356528450433584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 395584
    },
    {
      "epoch": 0.0014357689793941951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6944,
      "step": 395616
    },
    {
      "epoch": 0.0014358851137450316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 395648
    },
    {
      "epoch": 0.0014360012480958684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 395680
    },
    {
      "epoch": 0.0014361173824467052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 395712
    },
    {
      "epoch": 0.001436233516797542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 395744
    },
    {
      "epoch": 0.0014363496511483787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 395776
    },
    {
      "epoch": 0.0014364657854992152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 395808
    },
    {
      "epoch": 0.001436581919850052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 395840
    },
    {
      "epoch": 0.0014366980542008887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7494,
      "step": 395872
    },
    {
      "epoch": 0.0014368141885517255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 395904
    },
    {
      "epoch": 0.001436930322902562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 395936
    },
    {
      "epoch": 0.0014370464572533988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 395968
    },
    {
      "epoch": 0.0014371625916042355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 396000
    },
    {
      "epoch": 0.0014372787259550723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 396032
    },
    {
      "epoch": 0.001437394860305909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 396064
    },
    {
      "epoch": 0.0014375109946567456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 396096
    },
    {
      "epoch": 0.0014376271290075823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 396128
    },
    {
      "epoch": 0.001437743263358419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 396160
    },
    {
      "epoch": 0.0014378593977092558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 396192
    },
    {
      "epoch": 0.0014379755320600924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 396224
    },
    {
      "epoch": 0.0014380916664109291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 396256
    },
    {
      "epoch": 0.0014382078007617659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7524,
      "step": 396288
    },
    {
      "epoch": 0.0014383239351126026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7616,
      "step": 396320
    },
    {
      "epoch": 0.0014384400694634394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 396352
    },
    {
      "epoch": 0.001438556203814276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 396384
    },
    {
      "epoch": 0.0014386723381651127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 396416
    },
    {
      "epoch": 0.0014387884725159494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 396448
    },
    {
      "epoch": 0.0014389046068667862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 396480
    },
    {
      "epoch": 0.0014390207412176227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 396512
    },
    {
      "epoch": 0.0014391368755684595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 396544
    },
    {
      "epoch": 0.0014392530099192962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 396576
    },
    {
      "epoch": 0.001439369144270133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 396608
    },
    {
      "epoch": 0.0014394852786209697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 396640
    },
    {
      "epoch": 0.0014396014129718063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 396672
    },
    {
      "epoch": 0.001439717547322643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 396704
    },
    {
      "epoch": 0.0014398336816734798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7464,
      "step": 396736
    },
    {
      "epoch": 0.0014399498160243165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 396768
    },
    {
      "epoch": 0.001440065950375153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 396800
    },
    {
      "epoch": 0.0014401820847259898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 396832
    },
    {
      "epoch": 0.0014402982190768266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 396864
    },
    {
      "epoch": 0.0014404143534276633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 396896
    },
    {
      "epoch": 0.0014405304877785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 396928
    },
    {
      "epoch": 0.0014406466221293366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 396960
    },
    {
      "epoch": 0.0014407627564801734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 396992
    },
    {
      "epoch": 0.0014408788908310101,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 397024
    },
    {
      "epoch": 0.0014409950251818469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 397056
    },
    {
      "epoch": 0.0014411111595326834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6956,
      "step": 397088
    },
    {
      "epoch": 0.0014412272938835202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 397120
    },
    {
      "epoch": 0.001441343428234357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 397152
    },
    {
      "epoch": 0.0014414595625851937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 397184
    },
    {
      "epoch": 0.0014415756969360304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 397216
    },
    {
      "epoch": 0.001441691831286867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 397248
    },
    {
      "epoch": 0.0014418079656377037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 397280
    },
    {
      "epoch": 0.0014419240999885405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 397312
    },
    {
      "epoch": 0.0014420402343393772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 397344
    },
    {
      "epoch": 0.0014421563686902138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 397376
    },
    {
      "epoch": 0.0014422725030410505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 397408
    },
    {
      "epoch": 0.0014423886373918873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 397440
    },
    {
      "epoch": 0.001442504771742724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 397472
    },
    {
      "epoch": 0.0014426209060935608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 397504
    },
    {
      "epoch": 0.0014427370404443973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 397536
    },
    {
      "epoch": 0.001442853174795234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 397568
    },
    {
      "epoch": 0.0014429693091460708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 397600
    },
    {
      "epoch": 0.0014430854434969076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 397632
    },
    {
      "epoch": 0.0014432015778477441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 397664
    },
    {
      "epoch": 0.0014433177121985809,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 397696
    },
    {
      "epoch": 0.0014434338465494176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 397728
    },
    {
      "epoch": 0.0014435499809002544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 397760
    },
    {
      "epoch": 0.0014436661152510911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 397792
    },
    {
      "epoch": 0.0014437822496019277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 397824
    },
    {
      "epoch": 0.0014438983839527644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 397856
    },
    {
      "epoch": 0.0014440145183036012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 397888
    },
    {
      "epoch": 0.001444130652654438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 397920
    },
    {
      "epoch": 0.0014442467870052745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 397952
    },
    {
      "epoch": 0.0014443629213561112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 397984
    },
    {
      "epoch": 0.001444479055706948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 398016
    },
    {
      "epoch": 0.0014445951900577847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 398048
    },
    {
      "epoch": 0.0014447113244086215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 398080
    },
    {
      "epoch": 0.001444827458759458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 398112
    },
    {
      "epoch": 0.0014449435931102948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 398144
    },
    {
      "epoch": 0.0014450597274611315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 398176
    },
    {
      "epoch": 0.0014451758618119683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 398208
    },
    {
      "epoch": 0.0014452919961628048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 398240
    },
    {
      "epoch": 0.0014454081305136416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 398272
    },
    {
      "epoch": 0.0014455242648644783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 398304
    },
    {
      "epoch": 0.001445640399215315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 398336
    },
    {
      "epoch": 0.0014457565335661518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 398368
    },
    {
      "epoch": 0.0014458726679169884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 398400
    },
    {
      "epoch": 0.0014459888022678251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 398432
    },
    {
      "epoch": 0.0014461049366186619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 398464
    },
    {
      "epoch": 0.0014462210709694986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 398496
    },
    {
      "epoch": 0.0014463372053203352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 398528
    },
    {
      "epoch": 0.001446453339671172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 398560
    },
    {
      "epoch": 0.0014465694740220087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 398592
    },
    {
      "epoch": 0.0014466856083728454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 398624
    },
    {
      "epoch": 0.001446801742723682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 398656
    },
    {
      "epoch": 0.0014469178770745187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 398688
    },
    {
      "epoch": 0.0014470340114253555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 398720
    },
    {
      "epoch": 0.0014471501457761922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 398752
    },
    {
      "epoch": 0.001447266280127029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 398784
    },
    {
      "epoch": 0.0014473824144778655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7548,
      "step": 398816
    },
    {
      "epoch": 0.0014474985488287023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 398848
    },
    {
      "epoch": 0.001447614683179539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 398880
    },
    {
      "epoch": 0.0014477308175303758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 398912
    },
    {
      "epoch": 0.0014478469518812123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 398944
    },
    {
      "epoch": 0.001447963086232049,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 398976
    },
    {
      "epoch": 0.0014480792205828858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 399008
    },
    {
      "epoch": 0.0014481953549337226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 399040
    },
    {
      "epoch": 0.0014483114892845594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 399072
    },
    {
      "epoch": 0.0014484276236353959,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 399104
    },
    {
      "epoch": 0.0014485437579862326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 399136
    },
    {
      "epoch": 0.0014486598923370694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 399168
    },
    {
      "epoch": 0.0014487760266879062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 399200
    },
    {
      "epoch": 0.0014488921610387427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 399232
    },
    {
      "epoch": 0.0014490082953895794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 399264
    },
    {
      "epoch": 0.0014491244297404162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 399296
    },
    {
      "epoch": 0.001449240564091253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 399328
    },
    {
      "epoch": 0.0014493566984420897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7574,
      "step": 399360
    },
    {
      "epoch": 0.0014494728327929262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 399392
    },
    {
      "epoch": 0.001449588967143763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 399424
    },
    {
      "epoch": 0.0014497051014945998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 399456
    },
    {
      "epoch": 0.0014498212358454365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 399488
    },
    {
      "epoch": 0.001449937370196273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7557,
      "step": 399520
    },
    {
      "epoch": 0.0014500535045471098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 399552
    },
    {
      "epoch": 0.0014501696388979466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 399584
    },
    {
      "epoch": 0.0014502857732487833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 399616
    },
    {
      "epoch": 0.00145040190759962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 399648
    },
    {
      "epoch": 0.0014505180419504566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 399680
    },
    {
      "epoch": 0.0014506341763012934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 399712
    },
    {
      "epoch": 0.00145075031065213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 399744
    },
    {
      "epoch": 0.0014508664450029669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 399776
    },
    {
      "epoch": 0.0014509825793538034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 399808
    },
    {
      "epoch": 0.0014510987137046402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7439,
      "step": 399840
    },
    {
      "epoch": 0.001451214848055477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 399872
    },
    {
      "epoch": 0.0014513309824063137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 399904
    },
    {
      "epoch": 0.0014514471167571504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 399936
    },
    {
      "epoch": 0.001451563251107987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 399968
    },
    {
      "epoch": 0.0014516793854588237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 400000
    },
    {
      "epoch": 0.0014517955198096605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 400032
    },
    {
      "epoch": 0.0014519116541604972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 400064
    },
    {
      "epoch": 0.0014520277885113337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 400096
    },
    {
      "epoch": 0.0014521439228621705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 400128
    },
    {
      "epoch": 0.0014522600572130073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 400160
    },
    {
      "epoch": 0.001452376191563844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 400192
    },
    {
      "epoch": 0.0014524923259146808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 400224
    },
    {
      "epoch": 0.0014526084602655173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 400256
    },
    {
      "epoch": 0.001452724594616354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 400288
    },
    {
      "epoch": 0.0014528407289671908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 400320
    },
    {
      "epoch": 0.0014529568633180276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 400352
    },
    {
      "epoch": 0.001453072997668864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 400384
    },
    {
      "epoch": 0.0014531891320197009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 400416
    },
    {
      "epoch": 0.0014533052663705376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 400448
    },
    {
      "epoch": 0.0014534214007213744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 400480
    },
    {
      "epoch": 0.0014535375350722111,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 400512
    },
    {
      "epoch": 0.0014536536694230477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 400544
    },
    {
      "epoch": 0.0014537698037738844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 400576
    },
    {
      "epoch": 0.0014538859381247212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 400608
    },
    {
      "epoch": 0.001454002072475558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 400640
    },
    {
      "epoch": 0.0014541182068263945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 400672
    },
    {
      "epoch": 0.0014542343411772312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7539,
      "step": 400704
    },
    {
      "epoch": 0.001454350475528068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7553,
      "step": 400736
    },
    {
      "epoch": 0.0014544666098789047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 400768
    },
    {
      "epoch": 0.0014545827442297415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 400800
    },
    {
      "epoch": 0.001454698878580578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 400832
    },
    {
      "epoch": 0.0014548150129314148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 400864
    },
    {
      "epoch": 0.0014549311472822515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 400896
    },
    {
      "epoch": 0.0014550472816330883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 400928
    },
    {
      "epoch": 0.0014551634159839248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 400960
    },
    {
      "epoch": 0.0014552795503347616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 400992
    },
    {
      "epoch": 0.0014553956846855983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 401024
    },
    {
      "epoch": 0.001455511819036435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 401056
    },
    {
      "epoch": 0.0014556279533872718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 401088
    },
    {
      "epoch": 0.0014557440877381084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 401120
    },
    {
      "epoch": 0.0014558602220889451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 401152
    },
    {
      "epoch": 0.0014559763564397819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 401184
    },
    {
      "epoch": 0.0014560924907906186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 401216
    },
    {
      "epoch": 0.0014562086251414552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 401248
    },
    {
      "epoch": 0.001456324759492292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 401280
    },
    {
      "epoch": 0.0014564408938431287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 401312
    },
    {
      "epoch": 0.0014565570281939654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 401344
    },
    {
      "epoch": 0.0014566731625448022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 401376
    },
    {
      "epoch": 0.0014567892968956387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 401408
    },
    {
      "epoch": 0.0014569054312464755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 401440
    },
    {
      "epoch": 0.0014570215655973122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 401472
    },
    {
      "epoch": 0.001457137699948149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 401504
    },
    {
      "epoch": 0.0014572538342989855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7488,
      "step": 401536
    },
    {
      "epoch": 0.0014573699686498223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 401568
    },
    {
      "epoch": 0.001457486103000659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 401600
    },
    {
      "epoch": 0.0014576022373514958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 401632
    },
    {
      "epoch": 0.0014577183717023325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 401664
    },
    {
      "epoch": 0.001457834506053169,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 401696
    },
    {
      "epoch": 0.0014579506404040058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 401728
    },
    {
      "epoch": 0.0014580667747548426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 401760
    },
    {
      "epoch": 0.0014581829091056793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 401792
    },
    {
      "epoch": 0.0014582990434565159,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7479,
      "step": 401824
    },
    {
      "epoch": 0.0014584151778073526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 401856
    },
    {
      "epoch": 0.0014585313121581894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 401888
    },
    {
      "epoch": 0.0014586474465090261,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 401920
    },
    {
      "epoch": 0.0014587635808598629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 401952
    },
    {
      "epoch": 0.0014588797152106994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 401984
    },
    {
      "epoch": 0.0014589958495615362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 402016
    },
    {
      "epoch": 0.001459111983912373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 402048
    },
    {
      "epoch": 0.0014592281182632097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 402080
    },
    {
      "epoch": 0.0014593442526140462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6885,
      "step": 402112
    },
    {
      "epoch": 0.001459460386964883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 402144
    },
    {
      "epoch": 0.0014595765213157197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 402176
    },
    {
      "epoch": 0.0014596926556665565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 402208
    },
    {
      "epoch": 0.0014598087900173932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 402240
    },
    {
      "epoch": 0.0014599249243682298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 402272
    },
    {
      "epoch": 0.0014600410587190665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6957,
      "step": 402304
    },
    {
      "epoch": 0.0014601571930699033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 402336
    },
    {
      "epoch": 0.00146027332742074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 402368
    },
    {
      "epoch": 0.0014603894617715766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 402400
    },
    {
      "epoch": 0.0014605055961224133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 402432
    },
    {
      "epoch": 0.00146062173047325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 402464
    },
    {
      "epoch": 0.0014607378648240868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 402496
    },
    {
      "epoch": 0.0014608539991749236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 402528
    },
    {
      "epoch": 0.0014609701335257601,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 402560
    },
    {
      "epoch": 0.0014610862678765969,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 402592
    },
    {
      "epoch": 0.0014612024022274336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 402624
    },
    {
      "epoch": 0.0014613185365782704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6862,
      "step": 402656
    },
    {
      "epoch": 0.001461434670929107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 402688
    },
    {
      "epoch": 0.0014615508052799437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.694,
      "step": 402720
    },
    {
      "epoch": 0.0014616669396307804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 402752
    },
    {
      "epoch": 0.0014617830739816172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 402784
    },
    {
      "epoch": 0.001461899208332454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 402816
    },
    {
      "epoch": 0.0014620153426832905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 402848
    },
    {
      "epoch": 0.0014621314770341272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 402880
    },
    {
      "epoch": 0.001462247611384964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 402912
    },
    {
      "epoch": 0.0014623637457358007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 402944
    },
    {
      "epoch": 0.0014624798800866373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 402976
    },
    {
      "epoch": 0.001462596014437474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 403008
    },
    {
      "epoch": 0.0014627121487883108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 403040
    },
    {
      "epoch": 0.0014628282831391475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 403072
    },
    {
      "epoch": 0.0014629444174899843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 403104
    },
    {
      "epoch": 0.0014630605518408208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 403136
    },
    {
      "epoch": 0.0014631766861916576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 403168
    },
    {
      "epoch": 0.0014632928205424943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 403200
    },
    {
      "epoch": 0.001463408954893331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 403232
    },
    {
      "epoch": 0.0014635250892441676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 403264
    },
    {
      "epoch": 0.0014636412235950044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 403296
    },
    {
      "epoch": 0.0014637573579458411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 403328
    },
    {
      "epoch": 0.001463873492296678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 403360
    },
    {
      "epoch": 0.0014639896266475147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6943,
      "step": 403392
    },
    {
      "epoch": 0.0014641057609983512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 403424
    },
    {
      "epoch": 0.001464221895349188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 403456
    },
    {
      "epoch": 0.0014643380297000247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 403488
    },
    {
      "epoch": 0.0014644541640508615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 403520
    },
    {
      "epoch": 0.001464570298401698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6953,
      "step": 403552
    },
    {
      "epoch": 0.0014646864327525347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 403584
    },
    {
      "epoch": 0.0014648025671033715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 403616
    },
    {
      "epoch": 0.0014649187014542083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 403648
    },
    {
      "epoch": 0.001465034835805045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 403680
    },
    {
      "epoch": 0.0014651509701558815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 403712
    },
    {
      "epoch": 0.0014652671045067183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 403744
    },
    {
      "epoch": 0.001465383238857555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 403776
    },
    {
      "epoch": 0.0014654993732083918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6842,
      "step": 403808
    },
    {
      "epoch": 0.0014656155075592283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 403840
    },
    {
      "epoch": 0.001465731641910065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 403872
    },
    {
      "epoch": 0.0014658477762609019,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 403904
    },
    {
      "epoch": 0.0014659639106117386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 403936
    },
    {
      "epoch": 0.0014660800449625754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 403968
    },
    {
      "epoch": 0.001466196179313412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 404000
    },
    {
      "epoch": 0.0014663123136642487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 404032
    },
    {
      "epoch": 0.0014664284480150854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 404064
    },
    {
      "epoch": 0.0014665445823659222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 404096
    },
    {
      "epoch": 0.0014666607167167587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 404128
    },
    {
      "epoch": 0.0014667768510675955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 404160
    },
    {
      "epoch": 0.0014668929854184322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 404192
    },
    {
      "epoch": 0.001467009119769269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7487,
      "step": 404224
    },
    {
      "epoch": 0.0014671252541201057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 404256
    },
    {
      "epoch": 0.0014672413884709423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 404288
    },
    {
      "epoch": 0.001467357522821779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 404320
    },
    {
      "epoch": 0.0014674736571726158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 404352
    },
    {
      "epoch": 0.0014675897915234525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 404384
    },
    {
      "epoch": 0.001467705925874289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 404416
    },
    {
      "epoch": 0.0014678220602251258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 404448
    },
    {
      "epoch": 0.0014679381945759626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 404480
    },
    {
      "epoch": 0.0014680543289267993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 404512
    },
    {
      "epoch": 0.001468170463277636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 404544
    },
    {
      "epoch": 0.0014682865976284726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 404576
    },
    {
      "epoch": 0.0014684027319793094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 404608
    },
    {
      "epoch": 0.0014685188663301461,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 404640
    },
    {
      "epoch": 0.0014686350006809829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 404672
    },
    {
      "epoch": 0.0014687511350318194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6891,
      "step": 404704
    },
    {
      "epoch": 0.0014688672693826562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 404736
    },
    {
      "epoch": 0.001468983403733493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 404768
    },
    {
      "epoch": 0.0014690995380843297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 404800
    },
    {
      "epoch": 0.0014692156724351664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 404832
    },
    {
      "epoch": 0.001469331806786003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 404864
    },
    {
      "epoch": 0.0014694479411368397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 404896
    },
    {
      "epoch": 0.0014695640754876765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 404928
    },
    {
      "epoch": 0.0014696802098385132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 404960
    },
    {
      "epoch": 0.0014697963441893498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 404992
    },
    {
      "epoch": 0.0014699124785401865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 405024
    },
    {
      "epoch": 0.0014700286128910233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 405056
    },
    {
      "epoch": 0.00147014474724186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7446,
      "step": 405088
    },
    {
      "epoch": 0.0014702608815926968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 405120
    },
    {
      "epoch": 0.0014703770159435333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 405152
    },
    {
      "epoch": 0.00147049315029437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 405184
    },
    {
      "epoch": 0.0014706092846452068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 405216
    },
    {
      "epoch": 0.0014707254189960436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 405248
    },
    {
      "epoch": 0.0014708415533468801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 405280
    },
    {
      "epoch": 0.0014709576876977169,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 405312
    },
    {
      "epoch": 0.0014710738220485536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 405344
    },
    {
      "epoch": 0.0014711899563993904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 405376
    },
    {
      "epoch": 0.0014713060907502271,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 405408
    },
    {
      "epoch": 0.0014714222251010637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 405440
    },
    {
      "epoch": 0.0014715383594519004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 405472
    },
    {
      "epoch": 0.0014716544938027372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 405504
    },
    {
      "epoch": 0.001471770628153574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 405536
    },
    {
      "epoch": 0.0014718867625044105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 405568
    },
    {
      "epoch": 0.0014720028968552472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 405600
    },
    {
      "epoch": 0.001472119031206084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 405632
    },
    {
      "epoch": 0.0014722351655569207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 405664
    },
    {
      "epoch": 0.0014723512999077575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 405696
    },
    {
      "epoch": 0.001472467434258594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 405728
    },
    {
      "epoch": 0.0014725835686094308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 405760
    },
    {
      "epoch": 0.0014726997029602675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 405792
    },
    {
      "epoch": 0.0014728158373111043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 405824
    },
    {
      "epoch": 0.0014729319716619408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 405856
    },
    {
      "epoch": 0.0014730481060127776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7482,
      "step": 405888
    },
    {
      "epoch": 0.0014731642403636143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 405920
    },
    {
      "epoch": 0.001473280374714451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 405952
    },
    {
      "epoch": 0.0014733965090652878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 405984
    },
    {
      "epoch": 0.0014735126434161244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 406016
    },
    {
      "epoch": 0.0014736287777669611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 406048
    },
    {
      "epoch": 0.0014737449121177979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 406080
    },
    {
      "epoch": 0.0014738610464686346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 406112
    },
    {
      "epoch": 0.0014739771808194712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 406144
    },
    {
      "epoch": 0.001474093315170308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 406176
    },
    {
      "epoch": 0.0014742094495211447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 406208
    },
    {
      "epoch": 0.0014743255838719814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 406240
    },
    {
      "epoch": 0.0014744417182228182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 406272
    },
    {
      "epoch": 0.0014745578525736547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 406304
    },
    {
      "epoch": 0.0014746739869244915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 406336
    },
    {
      "epoch": 0.0014747901212753282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 406368
    },
    {
      "epoch": 0.001474906255626165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7585,
      "step": 406400
    },
    {
      "epoch": 0.0014750223899770015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 406432
    },
    {
      "epoch": 0.0014751385243278383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 406464
    },
    {
      "epoch": 0.001475254658678675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 406496
    },
    {
      "epoch": 0.0014753707930295118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 406528
    },
    {
      "epoch": 0.0014754869273803485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 406560
    },
    {
      "epoch": 0.001475603061731185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 406592
    },
    {
      "epoch": 0.0014757191960820218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 406624
    },
    {
      "epoch": 0.0014758353304328586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 406656
    },
    {
      "epoch": 0.0014759514647836953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 406688
    },
    {
      "epoch": 0.0014760675991345319,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 406720
    },
    {
      "epoch": 0.0014761837334853686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 406752
    },
    {
      "epoch": 0.0014762998678362054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 406784
    },
    {
      "epoch": 0.0014764160021870421,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 406816
    },
    {
      "epoch": 0.001476532136537879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 406848
    },
    {
      "epoch": 0.0014766482708887154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 406880
    },
    {
      "epoch": 0.0014767644052395522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 406912
    },
    {
      "epoch": 0.001476880539590389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 406944
    },
    {
      "epoch": 0.0014769966739412257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 406976
    },
    {
      "epoch": 0.0014771128082920622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 407008
    },
    {
      "epoch": 0.001477228942642899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 407040
    },
    {
      "epoch": 0.0014773450769937357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 407072
    },
    {
      "epoch": 0.0014774612113445725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6997,
      "step": 407104
    },
    {
      "epoch": 0.0014775773456954092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 407136
    },
    {
      "epoch": 0.0014776934800462458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 407168
    },
    {
      "epoch": 0.0014778096143970825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 407200
    },
    {
      "epoch": 0.0014779257487479193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 407232
    },
    {
      "epoch": 0.001478041883098756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.743,
      "step": 407264
    },
    {
      "epoch": 0.0014781580174495926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 407296
    },
    {
      "epoch": 0.0014782741518004293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6861,
      "step": 407328
    },
    {
      "epoch": 0.001478390286151266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 407360
    },
    {
      "epoch": 0.0014785064205021028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 407392
    },
    {
      "epoch": 0.0014786225548529396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 407424
    },
    {
      "epoch": 0.0014787386892037761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 407456
    },
    {
      "epoch": 0.001478854823554613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 407488
    },
    {
      "epoch": 0.0014789709579054496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 407520
    },
    {
      "epoch": 0.0014790870922562864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6875,
      "step": 407552
    },
    {
      "epoch": 0.001479203226607123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 407584
    },
    {
      "epoch": 0.0014793193609579597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 407616
    },
    {
      "epoch": 0.0014794354953087964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 407648
    },
    {
      "epoch": 0.0014795516296596332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 407680
    },
    {
      "epoch": 0.00147966776401047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 407712
    },
    {
      "epoch": 0.0014797838983613065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 407744
    },
    {
      "epoch": 0.0014799000327121432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 407776
    },
    {
      "epoch": 0.00148001616706298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 407808
    },
    {
      "epoch": 0.0014801323014138168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 407840
    },
    {
      "epoch": 0.0014802484357646533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7011,
      "step": 407872
    },
    {
      "epoch": 0.00148036457011549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 407904
    },
    {
      "epoch": 0.0014804807044663268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 407936
    },
    {
      "epoch": 0.0014805968388171636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 407968
    },
    {
      "epoch": 0.0014807129731680003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 408000
    },
    {
      "epoch": 0.0014808291075188368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 408032
    },
    {
      "epoch": 0.0014809452418696736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 408064
    },
    {
      "epoch": 0.0014810613762205104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 408096
    },
    {
      "epoch": 0.001481177510571347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 408128
    },
    {
      "epoch": 0.0014812936449221836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 408160
    },
    {
      "epoch": 0.0014814097792730204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 408192
    },
    {
      "epoch": 0.0014815259136238572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 408224
    },
    {
      "epoch": 0.001481642047974694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 408256
    },
    {
      "epoch": 0.0014817581823255307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7286,
      "step": 408288
    },
    {
      "epoch": 0.0014818743166763672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 408320
    },
    {
      "epoch": 0.001481990451027204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 408352
    },
    {
      "epoch": 0.0014821065853780407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 408384
    },
    {
      "epoch": 0.0014822227197288775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 408416
    },
    {
      "epoch": 0.001482338854079714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 408448
    },
    {
      "epoch": 0.0014824549884305508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 408480
    },
    {
      "epoch": 0.0014825711227813875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 408512
    },
    {
      "epoch": 0.0014826872571322243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 408544
    },
    {
      "epoch": 0.001482803391483061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7439,
      "step": 408576
    },
    {
      "epoch": 0.0014829195258338976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 408608
    },
    {
      "epoch": 0.0014830356601847343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 408640
    },
    {
      "epoch": 0.001483151794535571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 408672
    },
    {
      "epoch": 0.0014832679288864078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 408704
    },
    {
      "epoch": 0.0014833840632372444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 408736
    },
    {
      "epoch": 0.001483500197588081,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 408768
    },
    {
      "epoch": 0.0014836163319389179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 408800
    },
    {
      "epoch": 0.0014837324662897546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 408832
    },
    {
      "epoch": 0.0014838486006405914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 408864
    },
    {
      "epoch": 0.001483964734991428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 408896
    },
    {
      "epoch": 0.0014840808693422647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 408928
    },
    {
      "epoch": 0.0014841970036931014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 408960
    },
    {
      "epoch": 0.0014843131380439382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 408992
    },
    {
      "epoch": 0.0014844292723947747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 409024
    },
    {
      "epoch": 0.0014845454067456115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 409056
    },
    {
      "epoch": 0.0014846615410964482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 409088
    },
    {
      "epoch": 0.001484777675447285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 409120
    },
    {
      "epoch": 0.0014848938097981217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 409152
    },
    {
      "epoch": 0.0014850099441489583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 409184
    },
    {
      "epoch": 0.001485126078499795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 409216
    },
    {
      "epoch": 0.0014852422128506318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 409248
    },
    {
      "epoch": 0.0014853583472014685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7419,
      "step": 409280
    },
    {
      "epoch": 0.001485474481552305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 409312
    },
    {
      "epoch": 0.0014855906159031418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 409344
    },
    {
      "epoch": 0.0014857067502539786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7612,
      "step": 409376
    },
    {
      "epoch": 0.0014858228846048153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7412,
      "step": 409408
    },
    {
      "epoch": 0.001485939018955652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 409440
    },
    {
      "epoch": 0.0014860551533064886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 409472
    },
    {
      "epoch": 0.0014861712876573254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6922,
      "step": 409504
    },
    {
      "epoch": 0.0014862874220081621,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 409536
    },
    {
      "epoch": 0.0014864035563589989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 409568
    },
    {
      "epoch": 0.0014865196907098354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 409600
    },
    {
      "epoch": 0.0014866358250606722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 409632
    },
    {
      "epoch": 0.001486751959411509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 409664
    },
    {
      "epoch": 0.0014868680937623457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 409696
    },
    {
      "epoch": 0.0014869842281131824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6911,
      "step": 409728
    },
    {
      "epoch": 0.001487100362464019,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 409760
    },
    {
      "epoch": 0.0014872164968148557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 409792
    },
    {
      "epoch": 0.0014873326311656925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 409824
    },
    {
      "epoch": 0.0014874487655165292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 409856
    },
    {
      "epoch": 0.0014875648998673658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 409888
    },
    {
      "epoch": 0.0014876810342182025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 409920
    },
    {
      "epoch": 0.0014877971685690393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 409952
    },
    {
      "epoch": 0.001487913302919876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 409984
    },
    {
      "epoch": 0.0014880294372707128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 410016
    },
    {
      "epoch": 0.0014881455716215493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 410048
    },
    {
      "epoch": 0.001488261705972386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 410080
    },
    {
      "epoch": 0.0014883778403232228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 410112
    },
    {
      "epoch": 0.0014884939746740596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 410144
    },
    {
      "epoch": 0.0014886101090248961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 410176
    },
    {
      "epoch": 0.0014887262433757329,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 410208
    },
    {
      "epoch": 0.0014888423777265696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 410240
    },
    {
      "epoch": 0.0014889585120774064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 410272
    },
    {
      "epoch": 0.0014890746464282431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 410304
    },
    {
      "epoch": 0.0014891907807790797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 410336
    },
    {
      "epoch": 0.0014893069151299164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 410368
    },
    {
      "epoch": 0.0014894230494807532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 410400
    },
    {
      "epoch": 0.00148953918383159,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 410432
    },
    {
      "epoch": 0.0014896553181824265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7495,
      "step": 410464
    },
    {
      "epoch": 0.0014897714525332632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 410496
    },
    {
      "epoch": 0.0014898875868841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 410528
    },
    {
      "epoch": 0.0014900037212349367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 410560
    },
    {
      "epoch": 0.0014901198555857735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 410592
    },
    {
      "epoch": 0.00149023598993661,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 410624
    },
    {
      "epoch": 0.0014903521242874468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 410656
    },
    {
      "epoch": 0.0014904682586382835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 410688
    },
    {
      "epoch": 0.0014905843929891203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 410720
    },
    {
      "epoch": 0.0014907005273399568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 410752
    },
    {
      "epoch": 0.0014908166616907936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 410784
    },
    {
      "epoch": 0.0014909327960416303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7535,
      "step": 410816
    },
    {
      "epoch": 0.001491048930392467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 410848
    },
    {
      "epoch": 0.0014911650647433038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 410880
    },
    {
      "epoch": 0.0014912811990941404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 410912
    },
    {
      "epoch": 0.0014913973334449771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 410944
    },
    {
      "epoch": 0.0014915134677958139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 410976
    },
    {
      "epoch": 0.0014916296021466506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7571,
      "step": 411008
    },
    {
      "epoch": 0.0014917457364974872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7449,
      "step": 411040
    },
    {
      "epoch": 0.001491861870848324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 411072
    },
    {
      "epoch": 0.0014919780051991607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7501,
      "step": 411104
    },
    {
      "epoch": 0.0014920941395499974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 411136
    },
    {
      "epoch": 0.0014922102739008342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 411168
    },
    {
      "epoch": 0.0014923264082516707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 411200
    },
    {
      "epoch": 0.0014924425426025075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.755,
      "step": 411232
    },
    {
      "epoch": 0.0014925586769533442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 411264
    },
    {
      "epoch": 0.001492674811304181,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 411296
    },
    {
      "epoch": 0.0014927909456550175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 411328
    },
    {
      "epoch": 0.0014929070800058543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 411360
    },
    {
      "epoch": 0.001493023214356691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 411392
    },
    {
      "epoch": 0.0014931393487075278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 411424
    },
    {
      "epoch": 0.0014932554830583645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 411456
    },
    {
      "epoch": 0.001493371617409201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 411488
    },
    {
      "epoch": 0.0014934877517600378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 411520
    },
    {
      "epoch": 0.0014936038861108746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7575,
      "step": 411552
    },
    {
      "epoch": 0.0014937200204617113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 411584
    },
    {
      "epoch": 0.0014938361548125479,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 411616
    },
    {
      "epoch": 0.0014939522891633846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7449,
      "step": 411648
    },
    {
      "epoch": 0.0014940684235142214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 411680
    },
    {
      "epoch": 0.0014941845578650581,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 411712
    },
    {
      "epoch": 0.001494300692215895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 411744
    },
    {
      "epoch": 0.0014944168265667314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 411776
    },
    {
      "epoch": 0.0014945329609175682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 411808
    },
    {
      "epoch": 0.001494649095268405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6927,
      "step": 411840
    },
    {
      "epoch": 0.0014947652296192417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 411872
    },
    {
      "epoch": 0.0014948813639700782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 411904
    },
    {
      "epoch": 0.001494997498320915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 411936
    },
    {
      "epoch": 0.0014951136326717517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 411968
    },
    {
      "epoch": 0.0014952297670225885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 412000
    },
    {
      "epoch": 0.0014953459013734253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 412032
    },
    {
      "epoch": 0.0014954620357242618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 412064
    },
    {
      "epoch": 0.0014955781700750985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7479,
      "step": 412096
    },
    {
      "epoch": 0.0014956943044259353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 412128
    },
    {
      "epoch": 0.001495810438776772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 412160
    },
    {
      "epoch": 0.0014959265731276086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 412192
    },
    {
      "epoch": 0.0014960427074784453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 412224
    },
    {
      "epoch": 0.001496158841829282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 412256
    },
    {
      "epoch": 0.0014962749761801189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 412288
    },
    {
      "epoch": 0.0014963911105309556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 412320
    },
    {
      "epoch": 0.0014965072448817921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 412352
    },
    {
      "epoch": 0.001496623379232629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 412384
    },
    {
      "epoch": 0.0014967395135834657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 412416
    },
    {
      "epoch": 0.0014968556479343024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 412448
    },
    {
      "epoch": 0.001496971782285139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 412480
    },
    {
      "epoch": 0.0014970879166359757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 412512
    },
    {
      "epoch": 0.0014972040509868125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 412544
    },
    {
      "epoch": 0.0014973201853376492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 412576
    },
    {
      "epoch": 0.001497436319688486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 412608
    },
    {
      "epoch": 0.0014975524540393225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 412640
    },
    {
      "epoch": 0.0014976685883901593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 412672
    },
    {
      "epoch": 0.001497784722740996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 412704
    },
    {
      "epoch": 0.0014979008570918328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 412736
    },
    {
      "epoch": 0.0014980169914426693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 412768
    },
    {
      "epoch": 0.001498133125793506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 412800
    },
    {
      "epoch": 0.0014982492601443428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 412832
    },
    {
      "epoch": 0.0014983653944951796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 412864
    },
    {
      "epoch": 0.0014984815288460163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 412896
    },
    {
      "epoch": 0.0014985976631968529,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 412928
    },
    {
      "epoch": 0.0014987137975476896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 412960
    },
    {
      "epoch": 0.0014988299318985264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 412992
    },
    {
      "epoch": 0.0014989460662493631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 413024
    },
    {
      "epoch": 0.0014990622006001997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 413056
    },
    {
      "epoch": 0.0014991783349510364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 413088
    },
    {
      "epoch": 0.0014992944693018732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 413120
    },
    {
      "epoch": 0.00149941060365271,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 413152
    },
    {
      "epoch": 0.0014995267380035467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 413184
    },
    {
      "epoch": 0.0014996428723543832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 413216
    },
    {
      "epoch": 0.00149975900670522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6932,
      "step": 413248
    },
    {
      "epoch": 0.0014998751410560567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 413280
    },
    {
      "epoch": 0.0014999912754068935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 413312
    },
    {
      "epoch": 0.00150010740975773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 413344
    },
    {
      "epoch": 0.0015002235441085668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7408,
      "step": 413376
    },
    {
      "epoch": 0.0015003396784594035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 413408
    },
    {
      "epoch": 0.0015004558128102403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 413440
    },
    {
      "epoch": 0.001500571947161077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 413472
    },
    {
      "epoch": 0.0015006880815119136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 413504
    },
    {
      "epoch": 0.0015008042158627503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 413536
    },
    {
      "epoch": 0.001500920350213587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 413568
    },
    {
      "epoch": 0.0015010364845644238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 413600
    },
    {
      "epoch": 0.0015011526189152604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 413632
    },
    {
      "epoch": 0.0015012687532660971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 413664
    },
    {
      "epoch": 0.0015013848876169339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 413696
    },
    {
      "epoch": 0.0015015010219677706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 413728
    },
    {
      "epoch": 0.0015016171563186074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 413760
    },
    {
      "epoch": 0.001501733290669444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 413792
    },
    {
      "epoch": 0.0015018494250202807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 413824
    },
    {
      "epoch": 0.0015019655593711174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 413856
    },
    {
      "epoch": 0.0015020816937219542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 413888
    },
    {
      "epoch": 0.0015021978280727907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 413920
    },
    {
      "epoch": 0.0015023139624236275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 413952
    },
    {
      "epoch": 0.0015024300967744642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 413984
    },
    {
      "epoch": 0.001502546231125301,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 414016
    },
    {
      "epoch": 0.0015026623654761377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 414048
    },
    {
      "epoch": 0.0015027784998269743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 414080
    },
    {
      "epoch": 0.001502894634177811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 414112
    },
    {
      "epoch": 0.0015030107685286478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 414144
    },
    {
      "epoch": 0.0015031269028794845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7434,
      "step": 414176
    },
    {
      "epoch": 0.001503243037230321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 414208
    },
    {
      "epoch": 0.0015033591715811578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 414240
    },
    {
      "epoch": 0.0015034753059319946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7699,
      "step": 414272
    },
    {
      "epoch": 0.0015035914402828313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7541,
      "step": 414304
    },
    {
      "epoch": 0.001503707574633668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 414336
    },
    {
      "epoch": 0.0015038237089845046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 414368
    },
    {
      "epoch": 0.0015039398433353414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6907,
      "step": 414400
    },
    {
      "epoch": 0.0015040559776861781,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 414432
    },
    {
      "epoch": 0.0015041721120370149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 414464
    },
    {
      "epoch": 0.0015042882463878514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 414496
    },
    {
      "epoch": 0.0015044043807386882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7406,
      "step": 414528
    },
    {
      "epoch": 0.001504520515089525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 414560
    },
    {
      "epoch": 0.0015046366494403617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 414592
    },
    {
      "epoch": 0.0015047527837911984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 414624
    },
    {
      "epoch": 0.001504868918142035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 414656
    },
    {
      "epoch": 0.0015049850524928717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 414688
    },
    {
      "epoch": 0.0015051011868437085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 414720
    },
    {
      "epoch": 0.0015052173211945452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 414752
    },
    {
      "epoch": 0.0015053334555453818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 414784
    },
    {
      "epoch": 0.0015054495898962185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 414816
    },
    {
      "epoch": 0.0015055657242470553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 414848
    },
    {
      "epoch": 0.001505681858597892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 414880
    },
    {
      "epoch": 0.0015057979929487288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6915,
      "step": 414912
    },
    {
      "epoch": 0.0015059141272995653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 414944
    },
    {
      "epoch": 0.001506030261650402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 414976
    },
    {
      "epoch": 0.0015061463960012388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 415008
    },
    {
      "epoch": 0.0015062625303520756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 415040
    },
    {
      "epoch": 0.0015063786647029121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 415072
    },
    {
      "epoch": 0.0015064947990537489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 415104
    },
    {
      "epoch": 0.0015066109334045856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7381,
      "step": 415136
    },
    {
      "epoch": 0.0015067270677554224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 415168
    },
    {
      "epoch": 0.0015068432021062591,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7471,
      "step": 415200
    },
    {
      "epoch": 0.0015069593364570957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 415232
    },
    {
      "epoch": 0.0015070754708079324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 415264
    },
    {
      "epoch": 0.0015071916051587692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 415296
    },
    {
      "epoch": 0.001507307739509606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 415328
    },
    {
      "epoch": 0.0015074238738604425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 415360
    },
    {
      "epoch": 0.0015075400082112792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 415392
    },
    {
      "epoch": 0.001507656142562116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 415424
    },
    {
      "epoch": 0.0015077722769129527,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 415456
    },
    {
      "epoch": 0.0015078884112637895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 415488
    },
    {
      "epoch": 0.001508004545614626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 415520
    },
    {
      "epoch": 0.0015081206799654628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 415552
    },
    {
      "epoch": 0.0015082368143162995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 415584
    },
    {
      "epoch": 0.0015083529486671363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 415616
    },
    {
      "epoch": 0.0015084690830179728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 415648
    },
    {
      "epoch": 0.0015085852173688096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 415680
    },
    {
      "epoch": 0.0015087013517196463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 415712
    },
    {
      "epoch": 0.001508817486070483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.761,
      "step": 415744
    },
    {
      "epoch": 0.0015089336204213199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 415776
    },
    {
      "epoch": 0.0015090497547721564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 415808
    },
    {
      "epoch": 0.0015091658891229931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 415840
    },
    {
      "epoch": 0.00150928202347383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 415872
    },
    {
      "epoch": 0.0015093981578246666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 415904
    },
    {
      "epoch": 0.0015095142921755032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 415936
    },
    {
      "epoch": 0.00150963042652634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 415968
    },
    {
      "epoch": 0.0015097465608771767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 416000
    },
    {
      "epoch": 0.0015098626952280134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7778,
      "step": 416032
    },
    {
      "epoch": 0.0015099788295788502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 416064
    },
    {
      "epoch": 0.0015100949639296867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 416096
    },
    {
      "epoch": 0.0015102110982805235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 416128
    },
    {
      "epoch": 0.0015103272326313602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 416160
    },
    {
      "epoch": 0.001510443366982197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 416192
    },
    {
      "epoch": 0.0015105595013330335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 416224
    },
    {
      "epoch": 0.0015106756356838703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 416256
    },
    {
      "epoch": 0.001510791770034707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7563,
      "step": 416288
    },
    {
      "epoch": 0.0015109079043855438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 416320
    },
    {
      "epoch": 0.0015110240387363806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 416352
    },
    {
      "epoch": 0.001511140173087217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 416384
    },
    {
      "epoch": 0.0015112563074380538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7459,
      "step": 416416
    },
    {
      "epoch": 0.0015113724417888906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 416448
    },
    {
      "epoch": 0.0015114885761397274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 416480
    },
    {
      "epoch": 0.001511604710490564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 416512
    },
    {
      "epoch": 0.0015117208448414006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 416544
    },
    {
      "epoch": 0.0015118369791922374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 416576
    },
    {
      "epoch": 0.0015119531135430742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 416608
    },
    {
      "epoch": 0.001512069247893911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7428,
      "step": 416640
    },
    {
      "epoch": 0.0015121853822447474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 416672
    },
    {
      "epoch": 0.0015123015165955842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 416704
    },
    {
      "epoch": 0.001512417650946421,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 416736
    },
    {
      "epoch": 0.0015125337852972577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 416768
    },
    {
      "epoch": 0.0015126499196480942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 416800
    },
    {
      "epoch": 0.001512766053998931,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 416832
    },
    {
      "epoch": 0.0015128821883497678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 416864
    },
    {
      "epoch": 0.0015129983227006045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 416896
    },
    {
      "epoch": 0.0015131144570514413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 416928
    },
    {
      "epoch": 0.0015132305914022778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 416960
    },
    {
      "epoch": 0.0015133467257531146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 416992
    },
    {
      "epoch": 0.0015134628601039513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 417024
    },
    {
      "epoch": 0.001513578994454788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 417056
    },
    {
      "epoch": 0.0015136951288056246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7006,
      "step": 417088
    },
    {
      "epoch": 0.0015138112631564614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 417120
    },
    {
      "epoch": 0.0015139273975072981,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 417152
    },
    {
      "epoch": 0.0015140435318581349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 417184
    },
    {
      "epoch": 0.0015141596662089716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 417216
    },
    {
      "epoch": 0.0015142758005598082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 417248
    },
    {
      "epoch": 0.001514391934910645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 417280
    },
    {
      "epoch": 0.0015145080692614817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 417312
    },
    {
      "epoch": 0.0015146242036123184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 417344
    },
    {
      "epoch": 0.001514740337963155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7408,
      "step": 417376
    },
    {
      "epoch": 0.0015148564723139917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6913,
      "step": 417408
    },
    {
      "epoch": 0.0015149726066648285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 417440
    },
    {
      "epoch": 0.0015150887410156652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 417472
    },
    {
      "epoch": 0.001515204875366502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 417504
    },
    {
      "epoch": 0.0015153210097173385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 417536
    },
    {
      "epoch": 0.0015154371440681753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6919,
      "step": 417568
    },
    {
      "epoch": 0.001515553278419012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 417600
    },
    {
      "epoch": 0.0015156694127698488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 417632
    },
    {
      "epoch": 0.0015157855471206853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 417664
    },
    {
      "epoch": 0.001515901681471522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 417696
    },
    {
      "epoch": 0.0015160178158223588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 417728
    },
    {
      "epoch": 0.0015161339501731956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 417760
    },
    {
      "epoch": 0.0015162500845240323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 417792
    },
    {
      "epoch": 0.0015163662188748689,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 417824
    },
    {
      "epoch": 0.0015164823532257056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 417856
    },
    {
      "epoch": 0.0015165984875765424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 417888
    },
    {
      "epoch": 0.0015167146219273791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 417920
    },
    {
      "epoch": 0.0015168307562782157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 417952
    },
    {
      "epoch": 0.0015169468906290524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 417984
    },
    {
      "epoch": 0.0015170630249798892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 418016
    },
    {
      "epoch": 0.001517179159330726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7502,
      "step": 418048
    },
    {
      "epoch": 0.0015172952936815627,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 418080
    },
    {
      "epoch": 0.0015174114280323992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 418112
    },
    {
      "epoch": 0.001517527562383236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 418144
    },
    {
      "epoch": 0.0015176436967340727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 418176
    },
    {
      "epoch": 0.0015177598310849095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 418208
    },
    {
      "epoch": 0.001517875965435746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7491,
      "step": 418240
    },
    {
      "epoch": 0.0015179920997865828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7381,
      "step": 418272
    },
    {
      "epoch": 0.0015181082341374195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 418304
    },
    {
      "epoch": 0.0015182243684882563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 418336
    },
    {
      "epoch": 0.001518340502839093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 418368
    },
    {
      "epoch": 0.0015184566371899296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 418400
    },
    {
      "epoch": 0.0015185727715407663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 418432
    },
    {
      "epoch": 0.001518688905891603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 418464
    },
    {
      "epoch": 0.0015188050402424398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 418496
    },
    {
      "epoch": 0.0015189211745932764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7469,
      "step": 418528
    },
    {
      "epoch": 0.0015190373089441131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 418560
    },
    {
      "epoch": 0.0015191534432949499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 418592
    },
    {
      "epoch": 0.0015192695776457866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 418624
    },
    {
      "epoch": 0.0015193857119966234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 418656
    },
    {
      "epoch": 0.00151950184634746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 418688
    },
    {
      "epoch": 0.0015196179806982967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 418720
    },
    {
      "epoch": 0.0015197341150491334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 418752
    },
    {
      "epoch": 0.0015198502493999702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 418784
    },
    {
      "epoch": 0.0015199663837508067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 418816
    },
    {
      "epoch": 0.0015200825181016435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 418848
    },
    {
      "epoch": 0.0015201986524524802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6966,
      "step": 418880
    },
    {
      "epoch": 0.001520314786803317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 418912
    },
    {
      "epoch": 0.0015204309211541537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 418944
    },
    {
      "epoch": 0.0015205470555049903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 418976
    },
    {
      "epoch": 0.001520663189855827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 419008
    },
    {
      "epoch": 0.0015207793242066638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 419040
    },
    {
      "epoch": 0.0015208954585575005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 419072
    },
    {
      "epoch": 0.001521011592908337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7585,
      "step": 419104
    },
    {
      "epoch": 0.0015211277272591738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.766,
      "step": 419136
    },
    {
      "epoch": 0.0015212438616100106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 419168
    },
    {
      "epoch": 0.0015213599959608473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 419200
    },
    {
      "epoch": 0.001521476130311684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 419232
    },
    {
      "epoch": 0.0015215922646625206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 419264
    },
    {
      "epoch": 0.0015217083990133574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 419296
    },
    {
      "epoch": 0.0015218245333641941,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 419328
    },
    {
      "epoch": 0.0015219406677150309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6958,
      "step": 419360
    },
    {
      "epoch": 0.0015220568020658674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 419392
    },
    {
      "epoch": 0.0015221729364167042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 419424
    },
    {
      "epoch": 0.001522289070767541,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 419456
    },
    {
      "epoch": 0.0015224052051183777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 419488
    },
    {
      "epoch": 0.0015225213394692144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 419520
    },
    {
      "epoch": 0.001522637473820051,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 419552
    },
    {
      "epoch": 0.0015227536081708877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 419584
    },
    {
      "epoch": 0.0015228697425217245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 419616
    },
    {
      "epoch": 0.0015229858768725612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 419648
    },
    {
      "epoch": 0.0015231020112233978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 419680
    },
    {
      "epoch": 0.0015232181455742345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 419712
    },
    {
      "epoch": 0.0015233342799250713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 419744
    },
    {
      "epoch": 0.001523450414275908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 419776
    },
    {
      "epoch": 0.0015235665486267448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 419808
    },
    {
      "epoch": 0.0015236826829775813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 419840
    },
    {
      "epoch": 0.001523798817328418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 419872
    },
    {
      "epoch": 0.0015239149516792548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 419904
    },
    {
      "epoch": 0.0015240310860300916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 419936
    },
    {
      "epoch": 0.0015241472203809281,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 419968
    },
    {
      "epoch": 0.0015242633547317649,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 420000
    },
    {
      "epoch": 0.0015243794890826016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 420032
    },
    {
      "epoch": 0.0015244956234334384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 420064
    },
    {
      "epoch": 0.001524611757784275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 420096
    },
    {
      "epoch": 0.0015247278921351117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 420128
    },
    {
      "epoch": 0.0015248440264859484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 420160
    },
    {
      "epoch": 0.0015249601608367852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 420192
    },
    {
      "epoch": 0.001525076295187622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 420224
    },
    {
      "epoch": 0.0015251924295384585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 420256
    },
    {
      "epoch": 0.0015253085638892952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 420288
    },
    {
      "epoch": 0.001525424698240132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 420320
    },
    {
      "epoch": 0.0015255408325909688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 420352
    },
    {
      "epoch": 0.0015256569669418053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 420384
    },
    {
      "epoch": 0.001525773101292642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 420416
    },
    {
      "epoch": 0.0015258892356434788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 420448
    },
    {
      "epoch": 0.0015260053699943156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 420480
    },
    {
      "epoch": 0.0015261215043451523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 420512
    },
    {
      "epoch": 0.0015262376386959888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 420544
    },
    {
      "epoch": 0.0015263537730468256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 420576
    },
    {
      "epoch": 0.0015264699073976624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 420608
    },
    {
      "epoch": 0.001526586041748499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 420640
    },
    {
      "epoch": 0.0015267021760993356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7574,
      "step": 420672
    },
    {
      "epoch": 0.0015268183104501724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 420704
    },
    {
      "epoch": 0.0015269344448010091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 420736
    },
    {
      "epoch": 0.001527050579151846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 420768
    },
    {
      "epoch": 0.0015271667135026827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 420800
    },
    {
      "epoch": 0.0015272828478535192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 420832
    },
    {
      "epoch": 0.001527398982204356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 420864
    },
    {
      "epoch": 0.0015275151165551927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7493,
      "step": 420896
    },
    {
      "epoch": 0.0015276312509060295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 420928
    },
    {
      "epoch": 0.001527747385256866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7476,
      "step": 420960
    },
    {
      "epoch": 0.0015278635196077027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 420992
    },
    {
      "epoch": 0.0015279796539585395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 421024
    },
    {
      "epoch": 0.0015280957883093763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 421056
    },
    {
      "epoch": 0.001528211922660213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 421088
    },
    {
      "epoch": 0.0015283280570110495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 421120
    },
    {
      "epoch": 0.0015284441913618863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 421152
    },
    {
      "epoch": 0.001528560325712723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 421184
    },
    {
      "epoch": 0.0015286764600635598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 421216
    },
    {
      "epoch": 0.0015287925944143963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 421248
    },
    {
      "epoch": 0.001528908728765233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7487,
      "step": 421280
    },
    {
      "epoch": 0.0015290248631160699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 421312
    },
    {
      "epoch": 0.0015291409974669066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 421344
    },
    {
      "epoch": 0.0015292571318177434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 421376
    },
    {
      "epoch": 0.00152937326616858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 421408
    },
    {
      "epoch": 0.0015294894005194167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 421440
    },
    {
      "epoch": 0.0015296055348702534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 421472
    },
    {
      "epoch": 0.0015297216692210902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 421504
    },
    {
      "epoch": 0.0015298378035719267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.746,
      "step": 421536
    },
    {
      "epoch": 0.0015299539379227635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.751,
      "step": 421568
    },
    {
      "epoch": 0.0015300700722736002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 421600
    },
    {
      "epoch": 0.001530186206624437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 421632
    },
    {
      "epoch": 0.0015303023409752737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 421664
    },
    {
      "epoch": 0.0015304184753261103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 421696
    },
    {
      "epoch": 0.001530534609676947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 421728
    },
    {
      "epoch": 0.0015306507440277838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 421760
    },
    {
      "epoch": 0.0015307668783786205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 421792
    },
    {
      "epoch": 0.001530883012729457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 421824
    },
    {
      "epoch": 0.0015309991470802938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 421856
    },
    {
      "epoch": 0.0015311152814311306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 421888
    },
    {
      "epoch": 0.0015312314157819673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 421920
    },
    {
      "epoch": 0.001531347550132804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 421952
    },
    {
      "epoch": 0.0015314636844836406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 421984
    },
    {
      "epoch": 0.0015315798188344774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 422016
    },
    {
      "epoch": 0.0015316959531853141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 422048
    },
    {
      "epoch": 0.0015318120875361509,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7535,
      "step": 422080
    },
    {
      "epoch": 0.0015319282218869874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 422112
    },
    {
      "epoch": 0.0015320443562378242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 422144
    },
    {
      "epoch": 0.001532160490588661,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 422176
    },
    {
      "epoch": 0.0015322766249394977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 422208
    },
    {
      "epoch": 0.0015323927592903344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 422240
    },
    {
      "epoch": 0.001532508893641171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 422272
    },
    {
      "epoch": 0.0015326250279920077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 422304
    },
    {
      "epoch": 0.0015327411623428445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 422336
    },
    {
      "epoch": 0.0015328572966936812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 422368
    },
    {
      "epoch": 0.0015329734310445178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 422400
    },
    {
      "epoch": 0.0015330895653953545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 422432
    },
    {
      "epoch": 0.0015332056997461913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 422464
    },
    {
      "epoch": 0.001533321834097028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 422496
    },
    {
      "epoch": 0.0015334379684478648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 422528
    },
    {
      "epoch": 0.0015335541027987013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 422560
    },
    {
      "epoch": 0.001533670237149538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 422592
    },
    {
      "epoch": 0.0015337863715003748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 422624
    },
    {
      "epoch": 0.0015339025058512116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 422656
    },
    {
      "epoch": 0.0015340186402020481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 422688
    },
    {
      "epoch": 0.0015341347745528849,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 422720
    },
    {
      "epoch": 0.0015342509089037216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 422752
    },
    {
      "epoch": 0.0015343670432545584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 422784
    },
    {
      "epoch": 0.0015344831776053951,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 422816
    },
    {
      "epoch": 0.0015345993119562317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 422848
    },
    {
      "epoch": 0.0015347154463070684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 422880
    },
    {
      "epoch": 0.0015348315806579052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 422912
    },
    {
      "epoch": 0.001534947715008742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 422944
    },
    {
      "epoch": 0.0015350638493595785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 422976
    },
    {
      "epoch": 0.0015351799837104152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 423008
    },
    {
      "epoch": 0.001535296118061252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 423040
    },
    {
      "epoch": 0.0015354122524120887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 423072
    },
    {
      "epoch": 0.0015355283867629255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 423104
    },
    {
      "epoch": 0.001535644521113762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 423136
    },
    {
      "epoch": 0.0015357606554645988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 423168
    },
    {
      "epoch": 0.0015358767898154355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 423200
    },
    {
      "epoch": 0.0015359929241662723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 423232
    },
    {
      "epoch": 0.0015361090585171088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 423264
    },
    {
      "epoch": 0.0015362251928679456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 423296
    },
    {
      "epoch": 0.0015363413272187823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 423328
    },
    {
      "epoch": 0.001536457461569619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 423360
    },
    {
      "epoch": 0.0015365735959204558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 423392
    },
    {
      "epoch": 0.0015366897302712924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 423424
    },
    {
      "epoch": 0.0015368058646221291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7688,
      "step": 423456
    },
    {
      "epoch": 0.0015369219989729659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7484,
      "step": 423488
    },
    {
      "epoch": 0.0015370381333238026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 423520
    },
    {
      "epoch": 0.0015371542676746392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 423552
    },
    {
      "epoch": 0.001537270402025476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 423584
    },
    {
      "epoch": 0.0015373865363763127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 423616
    },
    {
      "epoch": 0.0015375026707271494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 423648
    },
    {
      "epoch": 0.0015376188050779862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 423680
    },
    {
      "epoch": 0.0015377349394288227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 423712
    },
    {
      "epoch": 0.0015378510737796595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 423744
    },
    {
      "epoch": 0.0015379672081304962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 423776
    },
    {
      "epoch": 0.001538083342481333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 423808
    },
    {
      "epoch": 0.0015381994768321695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 423840
    },
    {
      "epoch": 0.0015383156111830063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 423872
    },
    {
      "epoch": 0.001538431745533843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 423904
    },
    {
      "epoch": 0.0015385478798846798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 423936
    },
    {
      "epoch": 0.0015386640142355165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 423968
    },
    {
      "epoch": 0.001538780148586353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 424000
    },
    {
      "epoch": 0.0015388962829371898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 424032
    },
    {
      "epoch": 0.0015390124172880266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 424064
    },
    {
      "epoch": 0.0015391285516388633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 424096
    },
    {
      "epoch": 0.0015392446859896999,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 424128
    },
    {
      "epoch": 0.0015393608203405366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7505,
      "step": 424160
    },
    {
      "epoch": 0.0015394769546913734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7615,
      "step": 424192
    },
    {
      "epoch": 0.0015395930890422101,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 424224
    },
    {
      "epoch": 0.001539709223393047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 424256
    },
    {
      "epoch": 0.0015398253577438834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6853,
      "step": 424288
    },
    {
      "epoch": 0.0015399414920947202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 424320
    },
    {
      "epoch": 0.001540057626445557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 424352
    },
    {
      "epoch": 0.0015401737607963937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 424384
    },
    {
      "epoch": 0.0015402898951472302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 424416
    },
    {
      "epoch": 0.001540406029498067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 424448
    },
    {
      "epoch": 0.0015405221638489037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 424480
    },
    {
      "epoch": 0.0015406382981997405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 424512
    },
    {
      "epoch": 0.0015407544325505773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 424544
    },
    {
      "epoch": 0.0015408705669014138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 424576
    },
    {
      "epoch": 0.0015409867012522505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 424608
    },
    {
      "epoch": 0.0015411028356030873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 424640
    },
    {
      "epoch": 0.001541218969953924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 424672
    },
    {
      "epoch": 0.0015413351043047606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 424704
    },
    {
      "epoch": 0.0015414512386555973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 424736
    },
    {
      "epoch": 0.001541567373006434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 424768
    },
    {
      "epoch": 0.0015416835073572709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 424800
    },
    {
      "epoch": 0.0015417996417081076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 424832
    },
    {
      "epoch": 0.0015419157760589441,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 424864
    },
    {
      "epoch": 0.001542031910409781,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 424896
    },
    {
      "epoch": 0.0015421480447606177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 424928
    },
    {
      "epoch": 0.0015422641791114544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 424960
    },
    {
      "epoch": 0.001542380313462291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 424992
    },
    {
      "epoch": 0.0015424964478131277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 425024
    },
    {
      "epoch": 0.0015426125821639645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7615,
      "step": 425056
    },
    {
      "epoch": 0.0015427287165148012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 425088
    },
    {
      "epoch": 0.001542844850865638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 425120
    },
    {
      "epoch": 0.0015429609852164745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 425152
    },
    {
      "epoch": 0.0015430771195673113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 425184
    },
    {
      "epoch": 0.001543193253918148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 425216
    },
    {
      "epoch": 0.0015433093882689848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.753,
      "step": 425248
    },
    {
      "epoch": 0.0015434255226198213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 425280
    },
    {
      "epoch": 0.001543541656970658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 425312
    },
    {
      "epoch": 0.0015436577913214948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 425344
    },
    {
      "epoch": 0.0015437739256723316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 425376
    },
    {
      "epoch": 0.0015438900600231683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 425408
    },
    {
      "epoch": 0.0015440061943740048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 425440
    },
    {
      "epoch": 0.0015441223287248416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 425472
    },
    {
      "epoch": 0.0015442384630756784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 425504
    },
    {
      "epoch": 0.0015443545974265151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 425536
    },
    {
      "epoch": 0.0015444707317773516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 425568
    },
    {
      "epoch": 0.0015445868661281884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7498,
      "step": 425600
    },
    {
      "epoch": 0.0015447030004790252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 425632
    },
    {
      "epoch": 0.001544819134829862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 425664
    },
    {
      "epoch": 0.0015449352691806987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 425696
    },
    {
      "epoch": 0.0015450514035315352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 425728
    },
    {
      "epoch": 0.001545167537882372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 425760
    },
    {
      "epoch": 0.0015452836722332087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 425792
    },
    {
      "epoch": 0.0015453998065840455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7381,
      "step": 425824
    },
    {
      "epoch": 0.001545515940934882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 425856
    },
    {
      "epoch": 0.0015456320752857188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7498,
      "step": 425888
    },
    {
      "epoch": 0.0015457482096365555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 425920
    },
    {
      "epoch": 0.0015458643439873923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 425952
    },
    {
      "epoch": 0.001545980478338229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 425984
    },
    {
      "epoch": 0.0015460966126890656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 426016
    },
    {
      "epoch": 0.0015462127470399023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 426048
    },
    {
      "epoch": 0.001546328881390739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 426080
    },
    {
      "epoch": 0.0015464450157415758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 426112
    },
    {
      "epoch": 0.0015465611500924124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7587,
      "step": 426144
    },
    {
      "epoch": 0.0015466772844432491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 426176
    },
    {
      "epoch": 0.0015467934187940859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 426208
    },
    {
      "epoch": 0.0015469095531449226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 426240
    },
    {
      "epoch": 0.0015470256874957594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 426272
    },
    {
      "epoch": 0.001547141821846596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 426304
    },
    {
      "epoch": 0.0015472579561974327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 426336
    },
    {
      "epoch": 0.0015473740905482694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 426368
    },
    {
      "epoch": 0.0015474902248991062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 426400
    },
    {
      "epoch": 0.0015476063592499427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7515,
      "step": 426432
    },
    {
      "epoch": 0.0015477224936007795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 426464
    },
    {
      "epoch": 0.0015478386279516162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 426496
    },
    {
      "epoch": 0.001547954762302453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 426528
    },
    {
      "epoch": 0.0015480708966532897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 426560
    },
    {
      "epoch": 0.0015481870310041263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 426592
    },
    {
      "epoch": 0.001548303165354963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 426624
    },
    {
      "epoch": 0.0015484192997057998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 426656
    },
    {
      "epoch": 0.0015485354340566365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 426688
    },
    {
      "epoch": 0.001548651568407473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 426720
    },
    {
      "epoch": 0.0015487677027583098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 426752
    },
    {
      "epoch": 0.0015488838371091466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 426784
    },
    {
      "epoch": 0.0015489999714599833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 426816
    },
    {
      "epoch": 0.00154911610581082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 426848
    },
    {
      "epoch": 0.0015492322401616566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 426880
    },
    {
      "epoch": 0.0015493483745124934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 426912
    },
    {
      "epoch": 0.0015494645088633301,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 426944
    },
    {
      "epoch": 0.0015495806432141669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 426976
    },
    {
      "epoch": 0.0015496967775650034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7558,
      "step": 427008
    },
    {
      "epoch": 0.0015498129119158402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 427040
    },
    {
      "epoch": 0.001549929046266677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 427072
    },
    {
      "epoch": 0.0015500451806175137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 427104
    },
    {
      "epoch": 0.0015501613149683504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 427136
    },
    {
      "epoch": 0.001550277449319187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 427168
    },
    {
      "epoch": 0.0015503935836700237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 427200
    },
    {
      "epoch": 0.0015505097180208605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 427232
    },
    {
      "epoch": 0.0015506258523716972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 427264
    },
    {
      "epoch": 0.0015507419867225338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 427296
    },
    {
      "epoch": 0.0015508581210733705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 427328
    },
    {
      "epoch": 0.0015509742554242073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 427360
    },
    {
      "epoch": 0.001551090389775044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 427392
    },
    {
      "epoch": 0.0015512065241258808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 427424
    },
    {
      "epoch": 0.0015513226584767173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 427456
    },
    {
      "epoch": 0.001551438792827554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 427488
    },
    {
      "epoch": 0.0015515549271783908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 427520
    },
    {
      "epoch": 0.0015516710615292276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 427552
    },
    {
      "epoch": 0.0015517871958800641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 427584
    },
    {
      "epoch": 0.0015519033302309009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 427616
    },
    {
      "epoch": 0.0015520194645817376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 427648
    },
    {
      "epoch": 0.0015521355989325744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 427680
    },
    {
      "epoch": 0.0015522517332834111,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 427712
    },
    {
      "epoch": 0.0015523678676342477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 427744
    },
    {
      "epoch": 0.0015524840019850844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 427776
    },
    {
      "epoch": 0.0015526001363359212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 427808
    },
    {
      "epoch": 0.001552716270686758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 427840
    },
    {
      "epoch": 0.0015528324050375945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 427872
    },
    {
      "epoch": 0.0015529485393884312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 427904
    },
    {
      "epoch": 0.001553064673739268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 427936
    },
    {
      "epoch": 0.0015531808080901047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 427968
    },
    {
      "epoch": 0.0015532969424409415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 428000
    },
    {
      "epoch": 0.001553413076791778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 428032
    },
    {
      "epoch": 0.0015535292111426148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 428064
    },
    {
      "epoch": 0.0015536453454934515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 428096
    },
    {
      "epoch": 0.0015537614798442883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 428128
    },
    {
      "epoch": 0.0015538776141951248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 428160
    },
    {
      "epoch": 0.0015539937485459616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 428192
    },
    {
      "epoch": 0.0015541098828967983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 428224
    },
    {
      "epoch": 0.001554226017247635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 428256
    },
    {
      "epoch": 0.0015543421515984718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 428288
    },
    {
      "epoch": 0.0015544582859493084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7516,
      "step": 428320
    },
    {
      "epoch": 0.0015545744203001451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 428352
    },
    {
      "epoch": 0.001554690554650982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 428384
    },
    {
      "epoch": 0.0015548066890018186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 428416
    },
    {
      "epoch": 0.0015549228233526552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6918,
      "step": 428448
    },
    {
      "epoch": 0.001555038957703492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 428480
    },
    {
      "epoch": 0.0015551550920543287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 428512
    },
    {
      "epoch": 0.0015552712264051654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7308,
      "step": 428544
    },
    {
      "epoch": 0.0015553873607560022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 428576
    },
    {
      "epoch": 0.0015555034951068387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 428608
    },
    {
      "epoch": 0.0015556196294576755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 428640
    },
    {
      "epoch": 0.0015557357638085122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 428672
    },
    {
      "epoch": 0.001555851898159349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 428704
    },
    {
      "epoch": 0.0015559680325101855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 428736
    },
    {
      "epoch": 0.0015560841668610223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 428768
    },
    {
      "epoch": 0.001556200301211859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 428800
    },
    {
      "epoch": 0.0015563164355626958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 428832
    },
    {
      "epoch": 0.0015564325699135326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 428864
    },
    {
      "epoch": 0.001556548704264369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 428896
    },
    {
      "epoch": 0.0015566648386152058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 428928
    },
    {
      "epoch": 0.0015567809729660426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 428960
    },
    {
      "epoch": 0.0015568971073168794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 428992
    },
    {
      "epoch": 0.0015570132416677159,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 429024
    },
    {
      "epoch": 0.0015571293760185526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 429056
    },
    {
      "epoch": 0.0015572455103693894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.757,
      "step": 429088
    },
    {
      "epoch": 0.0015573616447202262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7608,
      "step": 429120
    },
    {
      "epoch": 0.001557477779071063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 429152
    },
    {
      "epoch": 0.0015575939134218994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 429184
    },
    {
      "epoch": 0.0015577100477727362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 429216
    },
    {
      "epoch": 0.001557826182123573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6867,
      "step": 429248
    },
    {
      "epoch": 0.0015579423164744097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 429280
    },
    {
      "epoch": 0.0015580584508252462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 429312
    },
    {
      "epoch": 0.001558174585176083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 429344
    },
    {
      "epoch": 0.0015582907195269198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 429376
    },
    {
      "epoch": 0.0015584068538777565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 429408
    },
    {
      "epoch": 0.0015585229882285933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 429440
    },
    {
      "epoch": 0.0015586391225794298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 429472
    },
    {
      "epoch": 0.0015587552569302666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 429504
    },
    {
      "epoch": 0.0015588713912811033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 429536
    },
    {
      "epoch": 0.00155898752563194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 429568
    },
    {
      "epoch": 0.0015591036599827766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 429600
    },
    {
      "epoch": 0.0015592197943336134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 429632
    },
    {
      "epoch": 0.00155933592868445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 429664
    },
    {
      "epoch": 0.0015594520630352869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 429696
    },
    {
      "epoch": 0.0015595681973861236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 429728
    },
    {
      "epoch": 0.0015596843317369602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 429760
    },
    {
      "epoch": 0.001559800466087797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 429792
    },
    {
      "epoch": 0.0015599166004386337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 429824
    },
    {
      "epoch": 0.0015600327347894704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 429856
    },
    {
      "epoch": 0.001560148869140307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 429888
    },
    {
      "epoch": 0.0015602650034911437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 429920
    },
    {
      "epoch": 0.0015603811378419805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7452,
      "step": 429952
    },
    {
      "epoch": 0.0015604972721928172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7406,
      "step": 429984
    },
    {
      "epoch": 0.001560613406543654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 430016
    },
    {
      "epoch": 0.0015607295408944905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 430048
    },
    {
      "epoch": 0.0015608456752453273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 430080
    },
    {
      "epoch": 0.001560961809596164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 430112
    },
    {
      "epoch": 0.0015610779439470008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 430144
    },
    {
      "epoch": 0.0015611940782978373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 430176
    },
    {
      "epoch": 0.001561310212648674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 430208
    },
    {
      "epoch": 0.0015614263469995108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 430240
    },
    {
      "epoch": 0.0015615424813503476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 430272
    },
    {
      "epoch": 0.0015616586157011843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 430304
    },
    {
      "epoch": 0.0015617747500520209,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 430336
    },
    {
      "epoch": 0.0015618908844028576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 430368
    },
    {
      "epoch": 0.0015620070187536944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 430400
    },
    {
      "epoch": 0.0015621231531045311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 430432
    },
    {
      "epoch": 0.0015622392874553677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 430464
    },
    {
      "epoch": 0.0015623554218062044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 430496
    },
    {
      "epoch": 0.0015624715561570412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7506,
      "step": 430528
    },
    {
      "epoch": 0.001562587690507878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 430560
    },
    {
      "epoch": 0.0015627038248587147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 430592
    },
    {
      "epoch": 0.0015628199592095512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 430624
    },
    {
      "epoch": 0.001562936093560388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 430656
    },
    {
      "epoch": 0.0015630522279112247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 430688
    },
    {
      "epoch": 0.0015631683622620615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 430720
    },
    {
      "epoch": 0.001563284496612898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 430752
    },
    {
      "epoch": 0.0015634006309637348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 430784
    },
    {
      "epoch": 0.0015635167653145715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 430816
    },
    {
      "epoch": 0.0015636328996654083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 430848
    },
    {
      "epoch": 0.001563749034016245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 430880
    },
    {
      "epoch": 0.0015638651683670816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 430912
    },
    {
      "epoch": 0.0015639813027179183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7507,
      "step": 430944
    },
    {
      "epoch": 0.001564097437068755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 430976
    },
    {
      "epoch": 0.0015642135714195918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 431008
    },
    {
      "epoch": 0.0015643297057704284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 431040
    },
    {
      "epoch": 0.0015644458401212651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 431072
    },
    {
      "epoch": 0.0015645619744721019,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 431104
    },
    {
      "epoch": 0.0015646781088229386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 431136
    },
    {
      "epoch": 0.0015647942431737754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 431168
    },
    {
      "epoch": 0.001564910377524612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 431200
    },
    {
      "epoch": 0.0015650265118754487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 431232
    },
    {
      "epoch": 0.0015651426462262854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 431264
    },
    {
      "epoch": 0.0015652587805771222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 431296
    },
    {
      "epoch": 0.0015653749149279587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 431328
    },
    {
      "epoch": 0.0015654910492787955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 431360
    },
    {
      "epoch": 0.0015656071836296322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7578,
      "step": 431392
    },
    {
      "epoch": 0.001565723317980469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 431424
    },
    {
      "epoch": 0.0015658394523313057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7599,
      "step": 431456
    },
    {
      "epoch": 0.0015659555866821423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 431488
    },
    {
      "epoch": 0.001566071721032979,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 431520
    },
    {
      "epoch": 0.0015661878553838158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 431552
    },
    {
      "epoch": 0.0015663039897346525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 431584
    },
    {
      "epoch": 0.001566420124085489,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 431616
    },
    {
      "epoch": 0.0015665362584363258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 431648
    },
    {
      "epoch": 0.0015666523927871626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 431680
    },
    {
      "epoch": 0.0015667685271379993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 431712
    },
    {
      "epoch": 0.001566884661488836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 431744
    },
    {
      "epoch": 0.0015670007958396726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 431776
    },
    {
      "epoch": 0.0015671169301905094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 431808
    },
    {
      "epoch": 0.0015672330645413461,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 431840
    },
    {
      "epoch": 0.0015673491988921829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 431872
    },
    {
      "epoch": 0.0015674653332430194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 431904
    },
    {
      "epoch": 0.0015675814675938562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 431936
    },
    {
      "epoch": 0.001567697601944693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 431968
    },
    {
      "epoch": 0.0015678137362955297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 432000
    },
    {
      "epoch": 0.0015679298706463664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 432032
    },
    {
      "epoch": 0.001568046004997203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 432064
    },
    {
      "epoch": 0.0015681621393480397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 432096
    },
    {
      "epoch": 0.0015682782736988765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7404,
      "step": 432128
    },
    {
      "epoch": 0.0015683944080497132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 432160
    },
    {
      "epoch": 0.0015685105424005498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 432192
    },
    {
      "epoch": 0.0015686266767513865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 432224
    },
    {
      "epoch": 0.0015687428111022233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 432256
    },
    {
      "epoch": 0.00156885894545306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 432288
    },
    {
      "epoch": 0.0015689750798038968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 432320
    },
    {
      "epoch": 0.0015690912141547333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 432352
    },
    {
      "epoch": 0.00156920734850557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 432384
    },
    {
      "epoch": 0.0015693234828564068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 432416
    },
    {
      "epoch": 0.0015694396172072436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 432448
    },
    {
      "epoch": 0.0015695557515580801,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7009,
      "step": 432480
    },
    {
      "epoch": 0.0015696718859089169,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6927,
      "step": 432512
    },
    {
      "epoch": 0.0015697880202597536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 432544
    },
    {
      "epoch": 0.0015699041546105904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 432576
    },
    {
      "epoch": 0.0015700202889614271,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 432608
    },
    {
      "epoch": 0.0015701364233122637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 432640
    },
    {
      "epoch": 0.0015702525576631004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 432672
    },
    {
      "epoch": 0.0015703686920139372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 432704
    },
    {
      "epoch": 0.001570484826364774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 432736
    },
    {
      "epoch": 0.0015706009607156105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 432768
    },
    {
      "epoch": 0.0015707170950664472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 432800
    },
    {
      "epoch": 0.001570833229417284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 432832
    },
    {
      "epoch": 0.0015709493637681207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 432864
    },
    {
      "epoch": 0.0015710654981189575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 432896
    },
    {
      "epoch": 0.001571181632469794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 432928
    },
    {
      "epoch": 0.0015712977668206308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 432960
    },
    {
      "epoch": 0.0015714139011714675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 432992
    },
    {
      "epoch": 0.0015715300355223043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 433024
    },
    {
      "epoch": 0.0015716461698731408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 433056
    },
    {
      "epoch": 0.0015717623042239776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 433088
    },
    {
      "epoch": 0.0015718784385748143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 433120
    },
    {
      "epoch": 0.001571994572925651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 433152
    },
    {
      "epoch": 0.0015721107072764879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 433184
    },
    {
      "epoch": 0.0015722268416273244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 433216
    },
    {
      "epoch": 0.0015723429759781611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 433248
    },
    {
      "epoch": 0.001572459110328998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 433280
    },
    {
      "epoch": 0.0015725752446798347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 433312
    },
    {
      "epoch": 0.0015726913790306712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 433344
    },
    {
      "epoch": 0.001572807513381508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 433376
    },
    {
      "epoch": 0.0015729236477323447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 433408
    },
    {
      "epoch": 0.0015730397820831815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 433440
    },
    {
      "epoch": 0.0015731559164340182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 433472
    },
    {
      "epoch": 0.0015732720507848547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 433504
    },
    {
      "epoch": 0.0015733881851356915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 433536
    },
    {
      "epoch": 0.0015735043194865283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 433568
    },
    {
      "epoch": 0.001573620453837365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 433600
    },
    {
      "epoch": 0.0015737365881882015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 433632
    },
    {
      "epoch": 0.0015738527225390383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 433664
    },
    {
      "epoch": 0.001573968856889875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 433696
    },
    {
      "epoch": 0.0015740849912407118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 433728
    },
    {
      "epoch": 0.0015742011255915486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 433760
    },
    {
      "epoch": 0.001574317259942385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 433792
    },
    {
      "epoch": 0.0015744333942932219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 433824
    },
    {
      "epoch": 0.0015745495286440586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 433856
    },
    {
      "epoch": 0.0015746656629948954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 433888
    },
    {
      "epoch": 0.001574781797345732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 433920
    },
    {
      "epoch": 0.0015748979316965687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 433952
    },
    {
      "epoch": 0.0015750140660474054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 433984
    },
    {
      "epoch": 0.0015751302003982422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7613,
      "step": 434016
    },
    {
      "epoch": 0.001575246334749079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 434048
    },
    {
      "epoch": 0.0015753624690999155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 434080
    },
    {
      "epoch": 0.0015754786034507522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 434112
    },
    {
      "epoch": 0.001575594737801589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 434144
    },
    {
      "epoch": 0.0015757108721524257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 434176
    },
    {
      "epoch": 0.0015758270065032623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 434208
    },
    {
      "epoch": 0.001575943140854099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 434240
    },
    {
      "epoch": 0.0015760592752049358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 434272
    },
    {
      "epoch": 0.0015761754095557725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 434304
    },
    {
      "epoch": 0.0015762915439066093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 434336
    },
    {
      "epoch": 0.0015764076782574458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 434368
    },
    {
      "epoch": 0.0015765238126082826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 434400
    },
    {
      "epoch": 0.0015766399469591193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 434432
    },
    {
      "epoch": 0.001576756081309956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 434464
    },
    {
      "epoch": 0.0015768722156607926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 434496
    },
    {
      "epoch": 0.0015769883500116294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 434528
    },
    {
      "epoch": 0.0015771044843624661,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 434560
    },
    {
      "epoch": 0.0015772206187133029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 434592
    },
    {
      "epoch": 0.0015773367530641396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6964,
      "step": 434624
    },
    {
      "epoch": 0.0015774528874149762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 434656
    },
    {
      "epoch": 0.001577569021765813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 434688
    },
    {
      "epoch": 0.0015776851561166497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 434720
    },
    {
      "epoch": 0.0015778012904674864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 434752
    },
    {
      "epoch": 0.001577917424818323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 434784
    },
    {
      "epoch": 0.0015780335591691597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 434816
    },
    {
      "epoch": 0.0015781496935199965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 434848
    },
    {
      "epoch": 0.0015782658278708332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 434880
    },
    {
      "epoch": 0.00157838196222167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 434912
    },
    {
      "epoch": 0.0015784980965725065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 434944
    },
    {
      "epoch": 0.0015786142309233433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 434976
    },
    {
      "epoch": 0.00157873036527418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 435008
    },
    {
      "epoch": 0.0015788464996250168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 435040
    },
    {
      "epoch": 0.0015789626339758533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 435072
    },
    {
      "epoch": 0.00157907876832669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 435104
    },
    {
      "epoch": 0.0015791949026775268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 435136
    },
    {
      "epoch": 0.0015793110370283636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 435168
    },
    {
      "epoch": 0.0015794271713792003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 435200
    },
    {
      "epoch": 0.0015795433057300369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 435232
    },
    {
      "epoch": 0.0015796594400808736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 435264
    },
    {
      "epoch": 0.0015797755744317104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 435296
    },
    {
      "epoch": 0.0015798917087825471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 435328
    },
    {
      "epoch": 0.0015800078431333837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 435360
    },
    {
      "epoch": 0.0015801239774842204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 435392
    },
    {
      "epoch": 0.0015802401118350572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 435424
    },
    {
      "epoch": 0.001580356246185894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 435456
    },
    {
      "epoch": 0.0015804723805367307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 435488
    },
    {
      "epoch": 0.0015805885148875672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 435520
    },
    {
      "epoch": 0.001580704649238404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 435552
    },
    {
      "epoch": 0.0015808207835892407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 435584
    },
    {
      "epoch": 0.0015809369179400775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 435616
    },
    {
      "epoch": 0.001581053052290914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 435648
    },
    {
      "epoch": 0.0015811691866417508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 435680
    },
    {
      "epoch": 0.0015812853209925875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 435712
    },
    {
      "epoch": 0.0015814014553434243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7492,
      "step": 435744
    },
    {
      "epoch": 0.001581517589694261,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7657,
      "step": 435776
    },
    {
      "epoch": 0.0015816337240450976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 435808
    },
    {
      "epoch": 0.0015817498583959343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 435840
    },
    {
      "epoch": 0.001581865992746771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 435872
    },
    {
      "epoch": 0.0015819821270976078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 435904
    },
    {
      "epoch": 0.0015820982614484444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 435936
    },
    {
      "epoch": 0.0015822143957992811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 435968
    },
    {
      "epoch": 0.0015823305301501179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 436000
    },
    {
      "epoch": 0.0015824466645009546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 436032
    },
    {
      "epoch": 0.0015825627988517914,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 436064
    },
    {
      "epoch": 0.001582678933202628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 436096
    },
    {
      "epoch": 0.0015827950675534647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 436128
    },
    {
      "epoch": 0.0015829112019043014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7498,
      "step": 436160
    },
    {
      "epoch": 0.0015830273362551382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 436192
    },
    {
      "epoch": 0.0015831434706059747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7417,
      "step": 436224
    },
    {
      "epoch": 0.0015832596049568115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 436256
    },
    {
      "epoch": 0.0015833757393076482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 436288
    },
    {
      "epoch": 0.001583491873658485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 436320
    },
    {
      "epoch": 0.0015836080080093217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 436352
    },
    {
      "epoch": 0.0015837241423601583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 436384
    },
    {
      "epoch": 0.001583840276710995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 436416
    },
    {
      "epoch": 0.0015839564110618318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 436448
    },
    {
      "epoch": 0.0015840725454126685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 436480
    },
    {
      "epoch": 0.001584188679763505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 436512
    },
    {
      "epoch": 0.0015843048141143418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 436544
    },
    {
      "epoch": 0.0015844209484651786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 436576
    },
    {
      "epoch": 0.0015845370828160153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 436608
    },
    {
      "epoch": 0.001584653217166852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 436640
    },
    {
      "epoch": 0.0015847693515176886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 436672
    },
    {
      "epoch": 0.0015848854858685254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 436704
    },
    {
      "epoch": 0.0015850016202193621,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 436736
    },
    {
      "epoch": 0.001585117754570199,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 436768
    },
    {
      "epoch": 0.0015852338889210354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 436800
    },
    {
      "epoch": 0.0015853500232718722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 436832
    },
    {
      "epoch": 0.001585466157622709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 436864
    },
    {
      "epoch": 0.0015855822919735457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 436896
    },
    {
      "epoch": 0.0015856984263243824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 436928
    },
    {
      "epoch": 0.001585814560675219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 436960
    },
    {
      "epoch": 0.0015859306950260557,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 436992
    },
    {
      "epoch": 0.0015860468293768925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 437024
    },
    {
      "epoch": 0.0015861629637277292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 437056
    },
    {
      "epoch": 0.0015862790980785658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 437088
    },
    {
      "epoch": 0.0015863952324294025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 437120
    },
    {
      "epoch": 0.0015865113667802393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6904,
      "step": 437152
    },
    {
      "epoch": 0.001586627501131076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 437184
    },
    {
      "epoch": 0.0015867436354819128,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 437216
    },
    {
      "epoch": 0.0015868597698327493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 437248
    },
    {
      "epoch": 0.001586975904183586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 437280
    },
    {
      "epoch": 0.0015870920385344228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 437312
    },
    {
      "epoch": 0.0015872081728852596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 437344
    },
    {
      "epoch": 0.0015873243072360961,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 437376
    },
    {
      "epoch": 0.001587440441586933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 437408
    },
    {
      "epoch": 0.0015875565759377696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 437440
    },
    {
      "epoch": 0.0015876727102886064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 437472
    },
    {
      "epoch": 0.0015877888446394432,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 437504
    },
    {
      "epoch": 0.0015879049789902797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7635,
      "step": 437536
    },
    {
      "epoch": 0.0015880211133411164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 437568
    },
    {
      "epoch": 0.0015881372476919532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 437600
    },
    {
      "epoch": 0.00158825338204279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 437632
    },
    {
      "epoch": 0.0015883695163936265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 437664
    },
    {
      "epoch": 0.0015884856507444632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 437696
    },
    {
      "epoch": 0.0015886017850953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 437728
    },
    {
      "epoch": 0.0015887179194461368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 437760
    },
    {
      "epoch": 0.0015888340537969735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 437792
    },
    {
      "epoch": 0.00158895018814781,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 437824
    },
    {
      "epoch": 0.0015890663224986468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 437856
    },
    {
      "epoch": 0.0015891824568494836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 437888
    },
    {
      "epoch": 0.0015892985912003203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 437920
    },
    {
      "epoch": 0.0015894147255511568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 437952
    },
    {
      "epoch": 0.0015895308599019936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 437984
    },
    {
      "epoch": 0.0015896469942528304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 438016
    },
    {
      "epoch": 0.0015897631286036671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 438048
    },
    {
      "epoch": 0.0015898792629545039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 438080
    },
    {
      "epoch": 0.0015899953973053404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 438112
    },
    {
      "epoch": 0.0015901115316561772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 438144
    },
    {
      "epoch": 0.001590227666007014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 438176
    },
    {
      "epoch": 0.0015903438003578507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 438208
    },
    {
      "epoch": 0.0015904599347086872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 438240
    },
    {
      "epoch": 0.001590576069059524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7494,
      "step": 438272
    },
    {
      "epoch": 0.0015906922034103607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 438304
    },
    {
      "epoch": 0.0015908083377611975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 438336
    },
    {
      "epoch": 0.0015909244721120342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 438368
    },
    {
      "epoch": 0.0015910406064628708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 438400
    },
    {
      "epoch": 0.0015911567408137075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 438432
    },
    {
      "epoch": 0.0015912728751645443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 438464
    },
    {
      "epoch": 0.001591389009515381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 438496
    },
    {
      "epoch": 0.0015915051438662176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 438528
    },
    {
      "epoch": 0.0015916212782170543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 438560
    },
    {
      "epoch": 0.001591737412567891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 438592
    },
    {
      "epoch": 0.0015918535469187278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 438624
    },
    {
      "epoch": 0.0015919696812695646,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 438656
    },
    {
      "epoch": 0.001592085815620401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 438688
    },
    {
      "epoch": 0.0015922019499712379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 438720
    },
    {
      "epoch": 0.0015923180843220746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 438752
    },
    {
      "epoch": 0.0015924342186729114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 438784
    },
    {
      "epoch": 0.001592550353023748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 438816
    },
    {
      "epoch": 0.0015926664873745847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7519,
      "step": 438848
    },
    {
      "epoch": 0.0015927826217254214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 438880
    },
    {
      "epoch": 0.0015928987560762582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 438912
    },
    {
      "epoch": 0.001593014890427095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 438944
    },
    {
      "epoch": 0.0015931310247779315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 438976
    },
    {
      "epoch": 0.0015932471591287682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 439008
    },
    {
      "epoch": 0.001593363293479605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7013,
      "step": 439040
    },
    {
      "epoch": 0.0015934794278304417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 439072
    },
    {
      "epoch": 0.0015935955621812783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 439104
    },
    {
      "epoch": 0.001593711696532115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 439136
    },
    {
      "epoch": 0.0015938278308829518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 439168
    },
    {
      "epoch": 0.0015939439652337885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 439200
    },
    {
      "epoch": 0.0015940600995846253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 439232
    },
    {
      "epoch": 0.0015941762339354618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 439264
    },
    {
      "epoch": 0.0015942923682862986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 439296
    },
    {
      "epoch": 0.0015944085026371353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 439328
    },
    {
      "epoch": 0.001594524636987972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 439360
    },
    {
      "epoch": 0.0015946407713388086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 439392
    },
    {
      "epoch": 0.0015947569056896454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 439424
    },
    {
      "epoch": 0.0015948730400404821,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 439456
    },
    {
      "epoch": 0.0015949891743913189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 439488
    },
    {
      "epoch": 0.0015951053087421556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 439520
    },
    {
      "epoch": 0.0015952214430929922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 439552
    },
    {
      "epoch": 0.001595337577443829,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 439584
    },
    {
      "epoch": 0.0015954537117946657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 439616
    },
    {
      "epoch": 0.0015955698461455024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6965,
      "step": 439648
    },
    {
      "epoch": 0.001595685980496339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 439680
    },
    {
      "epoch": 0.0015958021148471757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 439712
    },
    {
      "epoch": 0.0015959182491980125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 439744
    },
    {
      "epoch": 0.0015960343835488492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 439776
    },
    {
      "epoch": 0.001596150517899686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 439808
    },
    {
      "epoch": 0.0015962666522505225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 439840
    },
    {
      "epoch": 0.0015963827866013593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7425,
      "step": 439872
    },
    {
      "epoch": 0.001596498920952196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 439904
    },
    {
      "epoch": 0.0015966150553030328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 439936
    },
    {
      "epoch": 0.0015967311896538693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 439968
    },
    {
      "epoch": 0.001596847324004706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 440000
    },
    {
      "epoch": 0.0015969634583555428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 440032
    },
    {
      "epoch": 0.0015970795927063796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 440064
    },
    {
      "epoch": 0.0015971957270572163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 440096
    },
    {
      "epoch": 0.0015973118614080529,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 440128
    },
    {
      "epoch": 0.0015974279957588896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7535,
      "step": 440160
    },
    {
      "epoch": 0.0015975441301097264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 440192
    },
    {
      "epoch": 0.0015976602644605631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 440224
    },
    {
      "epoch": 0.0015977763988113997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 440256
    },
    {
      "epoch": 0.0015978925331622364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 440288
    },
    {
      "epoch": 0.0015980086675130732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 440320
    },
    {
      "epoch": 0.00159812480186391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 440352
    },
    {
      "epoch": 0.0015982409362147467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 440384
    },
    {
      "epoch": 0.0015983570705655832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7476,
      "step": 440416
    },
    {
      "epoch": 0.00159847320491642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 440448
    },
    {
      "epoch": 0.0015985893392672567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7499,
      "step": 440480
    },
    {
      "epoch": 0.0015987054736180935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 440512
    },
    {
      "epoch": 0.00159882160796893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 440544
    },
    {
      "epoch": 0.0015989377423197668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 440576
    },
    {
      "epoch": 0.0015990538766706035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 440608
    },
    {
      "epoch": 0.0015991700110214403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 440640
    },
    {
      "epoch": 0.001599286145372277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 440672
    },
    {
      "epoch": 0.0015994022797231136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7444,
      "step": 440704
    },
    {
      "epoch": 0.0015995184140739503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 440736
    },
    {
      "epoch": 0.001599634548424787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 440768
    },
    {
      "epoch": 0.0015997506827756238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 440800
    },
    {
      "epoch": 0.0015998668171264604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 440832
    },
    {
      "epoch": 0.0015999829514772971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 440864
    },
    {
      "epoch": 0.0016000990858281339,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 440896
    },
    {
      "epoch": 0.0016002152201789706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 440928
    },
    {
      "epoch": 0.0016003313545298074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 440960
    },
    {
      "epoch": 0.001600447488880644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 440992
    },
    {
      "epoch": 0.0016005636232314807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.758,
      "step": 441024
    },
    {
      "epoch": 0.0016006797575823174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7417,
      "step": 441056
    },
    {
      "epoch": 0.0016007958919331542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 441088
    },
    {
      "epoch": 0.0016009120262839907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 441120
    },
    {
      "epoch": 0.0016010281606348275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 441152
    },
    {
      "epoch": 0.0016011442949856642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 441184
    },
    {
      "epoch": 0.001601260429336501,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 441216
    },
    {
      "epoch": 0.0016013765636873378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 441248
    },
    {
      "epoch": 0.0016014926980381743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7381,
      "step": 441280
    },
    {
      "epoch": 0.001601608832389011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7609,
      "step": 441312
    },
    {
      "epoch": 0.0016017249667398478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 441344
    },
    {
      "epoch": 0.0016018411010906845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 441376
    },
    {
      "epoch": 0.001601957235441521,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 441408
    },
    {
      "epoch": 0.0016020733697923578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 441440
    },
    {
      "epoch": 0.0016021895041431946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 441472
    },
    {
      "epoch": 0.0016023056384940313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 441504
    },
    {
      "epoch": 0.001602421772844868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6979,
      "step": 441536
    },
    {
      "epoch": 0.0016025379071957046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 441568
    },
    {
      "epoch": 0.0016026540415465414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 441600
    },
    {
      "epoch": 0.0016027701758973781,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 441632
    },
    {
      "epoch": 0.001602886310248215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 441664
    },
    {
      "epoch": 0.0016030024445990514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 441696
    },
    {
      "epoch": 0.0016031185789498882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 441728
    },
    {
      "epoch": 0.001603234713300725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 441760
    },
    {
      "epoch": 0.0016033508476515617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 441792
    },
    {
      "epoch": 0.0016034669820023982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 441824
    },
    {
      "epoch": 0.001603583116353235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 441856
    },
    {
      "epoch": 0.0016036992507040717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 441888
    },
    {
      "epoch": 0.0016038153850549085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 441920
    },
    {
      "epoch": 0.0016039315194057453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 441952
    },
    {
      "epoch": 0.0016040476537565818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 441984
    },
    {
      "epoch": 0.0016041637881074185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 442016
    },
    {
      "epoch": 0.0016042799224582553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 442048
    },
    {
      "epoch": 0.001604396056809092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6858,
      "step": 442080
    },
    {
      "epoch": 0.0016045121911599286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 442112
    },
    {
      "epoch": 0.0016046283255107653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 442144
    },
    {
      "epoch": 0.001604744459861602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 442176
    },
    {
      "epoch": 0.0016048605942124389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 442208
    },
    {
      "epoch": 0.0016049767285632756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 442240
    },
    {
      "epoch": 0.0016050928629141121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 442272
    },
    {
      "epoch": 0.001605208997264949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 442304
    },
    {
      "epoch": 0.0016053251316157857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 442336
    },
    {
      "epoch": 0.0016054412659666224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 442368
    },
    {
      "epoch": 0.001605557400317459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 442400
    },
    {
      "epoch": 0.0016056735346682957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 442432
    },
    {
      "epoch": 0.0016057896690191325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 442464
    },
    {
      "epoch": 0.0016059058033699692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 442496
    },
    {
      "epoch": 0.001606021937720806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 442528
    },
    {
      "epoch": 0.0016061380720716425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 442560
    },
    {
      "epoch": 0.0016062542064224793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 442592
    },
    {
      "epoch": 0.001606370340773316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 442624
    },
    {
      "epoch": 0.0016064864751241528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 442656
    },
    {
      "epoch": 0.0016066026094749893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 442688
    },
    {
      "epoch": 0.001606718743825826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 442720
    },
    {
      "epoch": 0.0016068348781766628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 442752
    },
    {
      "epoch": 0.0016069510125274996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 442784
    },
    {
      "epoch": 0.0016070671468783363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 442816
    },
    {
      "epoch": 0.0016071832812291729,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 442848
    },
    {
      "epoch": 0.0016072994155800096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 442880
    },
    {
      "epoch": 0.0016074155499308464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 442912
    },
    {
      "epoch": 0.0016075316842816831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 442944
    },
    {
      "epoch": 0.0016076478186325197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 442976
    },
    {
      "epoch": 0.0016077639529833564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 443008
    },
    {
      "epoch": 0.0016078800873341932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 443040
    },
    {
      "epoch": 0.00160799622168503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 443072
    },
    {
      "epoch": 0.0016081123560358667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 443104
    },
    {
      "epoch": 0.0016082284903867032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 443136
    },
    {
      "epoch": 0.00160834462473754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 443168
    },
    {
      "epoch": 0.0016084607590883767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7621,
      "step": 443200
    },
    {
      "epoch": 0.0016085768934392135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 443232
    },
    {
      "epoch": 0.00160869302779005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 443264
    },
    {
      "epoch": 0.0016088091621408868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 443296
    },
    {
      "epoch": 0.0016089252964917235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 443328
    },
    {
      "epoch": 0.0016090414308425603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 443360
    },
    {
      "epoch": 0.001609157565193397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 443392
    },
    {
      "epoch": 0.0016092736995442336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 443424
    },
    {
      "epoch": 0.0016093898338950703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 443456
    },
    {
      "epoch": 0.001609505968245907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 443488
    },
    {
      "epoch": 0.0016096221025967438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 443520
    },
    {
      "epoch": 0.0016097382369475804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 443552
    },
    {
      "epoch": 0.0016098543712984171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 443584
    },
    {
      "epoch": 0.0016099705056492539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 443616
    },
    {
      "epoch": 0.0016100866400000906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7397,
      "step": 443648
    },
    {
      "epoch": 0.0016102027743509274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 443680
    },
    {
      "epoch": 0.001610318908701764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 443712
    },
    {
      "epoch": 0.0016104350430526007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 443744
    },
    {
      "epoch": 0.0016105511774034374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 443776
    },
    {
      "epoch": 0.0016106673117542742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 443808
    },
    {
      "epoch": 0.0016107834461051107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 443840
    },
    {
      "epoch": 0.0016108995804559475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 443872
    },
    {
      "epoch": 0.0016110157148067842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7473,
      "step": 443904
    },
    {
      "epoch": 0.001611131849157621,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 443936
    },
    {
      "epoch": 0.0016112479835084577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 443968
    },
    {
      "epoch": 0.0016113641178592943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 444000
    },
    {
      "epoch": 0.001611480252210131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 444032
    },
    {
      "epoch": 0.0016115963865609678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 444064
    },
    {
      "epoch": 0.0016117125209118045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 444096
    },
    {
      "epoch": 0.001611828655262641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 444128
    },
    {
      "epoch": 0.0016119447896134778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 444160
    },
    {
      "epoch": 0.0016120609239643146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 444192
    },
    {
      "epoch": 0.0016121770583151513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 444224
    },
    {
      "epoch": 0.001612293192665988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 444256
    },
    {
      "epoch": 0.0016124093270168246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 444288
    },
    {
      "epoch": 0.0016125254613676614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 444320
    },
    {
      "epoch": 0.0016126415957184981,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 444352
    },
    {
      "epoch": 0.0016127577300693349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 444384
    },
    {
      "epoch": 0.0016128738644201714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 444416
    },
    {
      "epoch": 0.0016129899987710082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 444448
    },
    {
      "epoch": 0.001613106133121845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 444480
    },
    {
      "epoch": 0.0016132222674726817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 444512
    },
    {
      "epoch": 0.0016133384018235184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.758,
      "step": 444544
    },
    {
      "epoch": 0.001613454536174355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 444576
    },
    {
      "epoch": 0.0016135706705251917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 444608
    },
    {
      "epoch": 0.0016136868048760285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 444640
    },
    {
      "epoch": 0.0016138029392268652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 444672
    },
    {
      "epoch": 0.0016139190735777018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 444704
    },
    {
      "epoch": 0.0016140352079285385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 444736
    },
    {
      "epoch": 0.0016141513422793753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 444768
    },
    {
      "epoch": 0.001614267476630212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 444800
    },
    {
      "epoch": 0.0016143836109810488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 444832
    },
    {
      "epoch": 0.0016144997453318853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 444864
    },
    {
      "epoch": 0.001614615879682722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 444896
    },
    {
      "epoch": 0.0016147320140335588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 444928
    },
    {
      "epoch": 0.0016148481483843956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 444960
    },
    {
      "epoch": 0.0016149642827352321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 444992
    },
    {
      "epoch": 0.0016150804170860689,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 445024
    },
    {
      "epoch": 0.0016151965514369056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 445056
    },
    {
      "epoch": 0.0016153126857877424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 445088
    },
    {
      "epoch": 0.0016154288201385791,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.748,
      "step": 445120
    },
    {
      "epoch": 0.0016155449544894157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 445152
    },
    {
      "epoch": 0.0016156610888402524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 445184
    },
    {
      "epoch": 0.0016157772231910892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 445216
    },
    {
      "epoch": 0.001615893357541926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 445248
    },
    {
      "epoch": 0.0016160094918927625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 445280
    },
    {
      "epoch": 0.0016161256262435992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 445312
    },
    {
      "epoch": 0.001616241760594436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 445344
    },
    {
      "epoch": 0.0016163578949452727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 445376
    },
    {
      "epoch": 0.0016164740292961095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 445408
    },
    {
      "epoch": 0.001616590163646946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 445440
    },
    {
      "epoch": 0.0016167062979977828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 445472
    },
    {
      "epoch": 0.0016168224323486195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 445504
    },
    {
      "epoch": 0.0016169385666994563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 445536
    },
    {
      "epoch": 0.0016170547010502928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 445568
    },
    {
      "epoch": 0.0016171708354011296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 445600
    },
    {
      "epoch": 0.0016172869697519663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7491,
      "step": 445632
    },
    {
      "epoch": 0.001617403104102803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 445664
    },
    {
      "epoch": 0.0016175192384536399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 445696
    },
    {
      "epoch": 0.0016176353728044764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 445728
    },
    {
      "epoch": 0.0016177515071553131,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 445760
    },
    {
      "epoch": 0.00161786764150615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 445792
    },
    {
      "epoch": 0.0016179837758569867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 445824
    },
    {
      "epoch": 0.0016180999102078232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 445856
    },
    {
      "epoch": 0.00161821604455866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 445888
    },
    {
      "epoch": 0.0016183321789094967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 445920
    },
    {
      "epoch": 0.0016184483132603335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 445952
    },
    {
      "epoch": 0.0016185644476111702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 445984
    },
    {
      "epoch": 0.0016186805819620067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 446016
    },
    {
      "epoch": 0.0016187967163128435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7406,
      "step": 446048
    },
    {
      "epoch": 0.0016189128506636803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 446080
    },
    {
      "epoch": 0.001619028985014517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 446112
    },
    {
      "epoch": 0.0016191451193653535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 446144
    },
    {
      "epoch": 0.0016192612537161903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 446176
    },
    {
      "epoch": 0.001619377388067027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7417,
      "step": 446208
    },
    {
      "epoch": 0.0016194935224178638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 446240
    },
    {
      "epoch": 0.0016196096567687006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 446272
    },
    {
      "epoch": 0.001619725791119537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 446304
    },
    {
      "epoch": 0.0016198419254703738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 446336
    },
    {
      "epoch": 0.0016199580598212106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 446368
    },
    {
      "epoch": 0.0016200741941720474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 446400
    },
    {
      "epoch": 0.001620190328522884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 446432
    },
    {
      "epoch": 0.0016203064628737206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 446464
    },
    {
      "epoch": 0.0016204225972245574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6881,
      "step": 446496
    },
    {
      "epoch": 0.0016205387315753942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 446528
    },
    {
      "epoch": 0.001620654865926231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 446560
    },
    {
      "epoch": 0.0016207710002770674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 446592
    },
    {
      "epoch": 0.0016208871346279042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 446624
    },
    {
      "epoch": 0.001621003268978741,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 446656
    },
    {
      "epoch": 0.0016211194033295777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 446688
    },
    {
      "epoch": 0.0016212355376804142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7533,
      "step": 446720
    },
    {
      "epoch": 0.001621351672031251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 446752
    },
    {
      "epoch": 0.0016214678063820878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 446784
    },
    {
      "epoch": 0.0016215839407329245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 446816
    },
    {
      "epoch": 0.0016217000750837613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 446848
    },
    {
      "epoch": 0.0016218162094345978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 446880
    },
    {
      "epoch": 0.0016219323437854346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 446912
    },
    {
      "epoch": 0.0016220484781362713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 446944
    },
    {
      "epoch": 0.001622164612487108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 446976
    },
    {
      "epoch": 0.0016222807468379446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 447008
    },
    {
      "epoch": 0.0016223968811887814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 447040
    },
    {
      "epoch": 0.0016225130155396181,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 447072
    },
    {
      "epoch": 0.0016226291498904549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 447104
    },
    {
      "epoch": 0.0016227452842412916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 447136
    },
    {
      "epoch": 0.0016228614185921282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 447168
    },
    {
      "epoch": 0.001622977552942965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 447200
    },
    {
      "epoch": 0.0016230936872938017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 447232
    },
    {
      "epoch": 0.0016232098216446384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7384,
      "step": 447264
    },
    {
      "epoch": 0.001623325955995475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 447296
    },
    {
      "epoch": 0.0016234420903463117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 447328
    },
    {
      "epoch": 0.0016235582246971485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 447360
    },
    {
      "epoch": 0.0016236743590479852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 447392
    },
    {
      "epoch": 0.001623790493398822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 447424
    },
    {
      "epoch": 0.0016239066277496585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 447456
    },
    {
      "epoch": 0.0016240227621004953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 447488
    },
    {
      "epoch": 0.001624138896451332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 447520
    },
    {
      "epoch": 0.0016242550308021688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 447552
    },
    {
      "epoch": 0.0016243711651530053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 447584
    },
    {
      "epoch": 0.001624487299503842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 447616
    },
    {
      "epoch": 0.0016246034338546788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 447648
    },
    {
      "epoch": 0.0016247195682055156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 447680
    },
    {
      "epoch": 0.0016248357025563523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 447712
    },
    {
      "epoch": 0.0016249518369071889,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 447744
    },
    {
      "epoch": 0.0016250679712580256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7008,
      "step": 447776
    },
    {
      "epoch": 0.0016251841056088624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 447808
    },
    {
      "epoch": 0.0016253002399596991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 447840
    },
    {
      "epoch": 0.0016254163743105357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 447872
    },
    {
      "epoch": 0.0016255325086613724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 447904
    },
    {
      "epoch": 0.0016256486430122092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 447936
    },
    {
      "epoch": 0.001625764777363046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 447968
    },
    {
      "epoch": 0.0016258809117138827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 448000
    },
    {
      "epoch": 0.0016259970460647192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 448032
    },
    {
      "epoch": 0.001626113180415556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7583,
      "step": 448064
    },
    {
      "epoch": 0.0016262293147663927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 448096
    },
    {
      "epoch": 0.0016263454491172295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 448128
    },
    {
      "epoch": 0.001626461583468066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 448160
    },
    {
      "epoch": 0.0016265777178189028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6929,
      "step": 448192
    },
    {
      "epoch": 0.0016266938521697395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 448224
    },
    {
      "epoch": 0.0016268099865205763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 448256
    },
    {
      "epoch": 0.001626926120871413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6969,
      "step": 448288
    },
    {
      "epoch": 0.0016270422552222496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 448320
    },
    {
      "epoch": 0.0016271583895730863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 448352
    },
    {
      "epoch": 0.001627274523923923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 448384
    },
    {
      "epoch": 0.0016273906582747598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 448416
    },
    {
      "epoch": 0.0016275067926255964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 448448
    },
    {
      "epoch": 0.0016276229269764331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 448480
    },
    {
      "epoch": 0.0016277390613272699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 448512
    },
    {
      "epoch": 0.0016278551956781066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 448544
    },
    {
      "epoch": 0.0016279713300289434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 448576
    },
    {
      "epoch": 0.00162808746437978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 448608
    },
    {
      "epoch": 0.0016282035987306167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 448640
    },
    {
      "epoch": 0.0016283197330814534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 448672
    },
    {
      "epoch": 0.0016284358674322902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 448704
    },
    {
      "epoch": 0.0016285520017831267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 448736
    },
    {
      "epoch": 0.0016286681361339635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 448768
    },
    {
      "epoch": 0.0016287842704848002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7469,
      "step": 448800
    },
    {
      "epoch": 0.001628900404835637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 448832
    },
    {
      "epoch": 0.0016290165391864737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7555,
      "step": 448864
    },
    {
      "epoch": 0.0016291326735373103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 448896
    },
    {
      "epoch": 0.001629248807888147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 448928
    },
    {
      "epoch": 0.0016293649422389838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7026,
      "step": 448960
    },
    {
      "epoch": 0.0016294810765898205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 448992
    },
    {
      "epoch": 0.001629597210940657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 449024
    },
    {
      "epoch": 0.0016297133452914938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 449056
    },
    {
      "epoch": 0.0016298294796423306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 449088
    },
    {
      "epoch": 0.0016299456139931673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 449120
    },
    {
      "epoch": 0.001630061748344004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 449152
    },
    {
      "epoch": 0.0016301778826948406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6882,
      "step": 449184
    },
    {
      "epoch": 0.0016302940170456774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 449216
    },
    {
      "epoch": 0.0016304101513965141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 449248
    },
    {
      "epoch": 0.001630526285747351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 449280
    },
    {
      "epoch": 0.0016306424200981874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 449312
    },
    {
      "epoch": 0.0016307585544490242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 449344
    },
    {
      "epoch": 0.001630874688799861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 449376
    },
    {
      "epoch": 0.0016309908231506977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 449408
    },
    {
      "epoch": 0.0016311069575015344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 449440
    },
    {
      "epoch": 0.001631223091852371,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 449472
    },
    {
      "epoch": 0.0016313392262032077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 449504
    },
    {
      "epoch": 0.0016314553605540445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 449536
    },
    {
      "epoch": 0.0016315714949048812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 449568
    },
    {
      "epoch": 0.0016316876292557178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 449600
    },
    {
      "epoch": 0.0016318037636065545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 449632
    },
    {
      "epoch": 0.0016319198979573913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 449664
    },
    {
      "epoch": 0.001632036032308228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 449696
    },
    {
      "epoch": 0.0016321521666590648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 449728
    },
    {
      "epoch": 0.0016322683010099013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 449760
    },
    {
      "epoch": 0.001632384435360738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 449792
    },
    {
      "epoch": 0.0016325005697115748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 449824
    },
    {
      "epoch": 0.0016326167040624116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 449856
    },
    {
      "epoch": 0.0016327328384132481,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 449888
    },
    {
      "epoch": 0.0016328489727640849,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 449920
    },
    {
      "epoch": 0.0016329651071149216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 449952
    },
    {
      "epoch": 0.0016330812414657584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 449984
    },
    {
      "epoch": 0.0016331973758165952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 450016
    },
    {
      "epoch": 0.0016333135101674317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 450048
    },
    {
      "epoch": 0.0016334296445182684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 450080
    },
    {
      "epoch": 0.0016335457788691052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 450112
    },
    {
      "epoch": 0.001633661913219942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 450144
    },
    {
      "epoch": 0.0016337780475707785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 450176
    },
    {
      "epoch": 0.0016338941819216152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 450208
    },
    {
      "epoch": 0.001634010316272452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 450240
    },
    {
      "epoch": 0.0016341264506232888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 450272
    },
    {
      "epoch": 0.0016342425849741255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 450304
    },
    {
      "epoch": 0.001634358719324962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 450336
    },
    {
      "epoch": 0.0016344748536757988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 450368
    },
    {
      "epoch": 0.0016345909880266356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 450400
    },
    {
      "epoch": 0.0016347071223774723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 450432
    },
    {
      "epoch": 0.0016348232567283088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7451,
      "step": 450464
    },
    {
      "epoch": 0.0016349393910791456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7602,
      "step": 450496
    },
    {
      "epoch": 0.0016350555254299824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7652,
      "step": 450528
    },
    {
      "epoch": 0.001635171659780819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7546,
      "step": 450560
    },
    {
      "epoch": 0.0016352877941316559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 450592
    },
    {
      "epoch": 0.0016354039284824924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 450624
    },
    {
      "epoch": 0.0016355200628333292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 450656
    },
    {
      "epoch": 0.001635636197184166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7474,
      "step": 450688
    },
    {
      "epoch": 0.0016357523315350027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 450720
    },
    {
      "epoch": 0.0016358684658858392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 450752
    },
    {
      "epoch": 0.001635984600236676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 450784
    },
    {
      "epoch": 0.0016361007345875127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 450816
    },
    {
      "epoch": 0.0016362168689383495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 450848
    },
    {
      "epoch": 0.0016363330032891862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 450880
    },
    {
      "epoch": 0.0016364491376400227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 450912
    },
    {
      "epoch": 0.0016365652719908595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 450944
    },
    {
      "epoch": 0.0016366814063416963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7543,
      "step": 450976
    },
    {
      "epoch": 0.001636797540692533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 451008
    },
    {
      "epoch": 0.0016369136750433695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 451040
    },
    {
      "epoch": 0.0016370298093942063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7432,
      "step": 451072
    },
    {
      "epoch": 0.001637145943745043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 451104
    },
    {
      "epoch": 0.0016372620780958798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 451136
    },
    {
      "epoch": 0.0016373782124467166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 451168
    },
    {
      "epoch": 0.001637494346797553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 451200
    },
    {
      "epoch": 0.0016376104811483899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 451232
    },
    {
      "epoch": 0.0016377266154992266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 451264
    },
    {
      "epoch": 0.0016378427498500634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 451296
    },
    {
      "epoch": 0.0016379588842009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7396,
      "step": 451328
    },
    {
      "epoch": 0.0016380750185517367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 451360
    },
    {
      "epoch": 0.0016381911529025734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 451392
    },
    {
      "epoch": 0.0016383072872534102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 451424
    },
    {
      "epoch": 0.001638423421604247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 451456
    },
    {
      "epoch": 0.0016385395559550835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 451488
    },
    {
      "epoch": 0.0016386556903059202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 451520
    },
    {
      "epoch": 0.001638771824656757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 451552
    },
    {
      "epoch": 0.0016388879590075937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 451584
    },
    {
      "epoch": 0.0016390040933584303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 451616
    },
    {
      "epoch": 0.001639120227709267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 451648
    },
    {
      "epoch": 0.0016392363620601038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 451680
    },
    {
      "epoch": 0.0016393524964109405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 451712
    },
    {
      "epoch": 0.0016394686307617773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 451744
    },
    {
      "epoch": 0.0016395847651126138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 451776
    },
    {
      "epoch": 0.0016397008994634506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 451808
    },
    {
      "epoch": 0.0016398170338142873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 451840
    },
    {
      "epoch": 0.001639933168165124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 451872
    },
    {
      "epoch": 0.0016400493025159606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 451904
    },
    {
      "epoch": 0.0016401654368667974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 451936
    },
    {
      "epoch": 0.0016402815712176341,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 451968
    },
    {
      "epoch": 0.0016403977055684709,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 452000
    },
    {
      "epoch": 0.0016405138399193076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 452032
    },
    {
      "epoch": 0.0016406299742701442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 452064
    },
    {
      "epoch": 0.001640746108620981,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 452096
    },
    {
      "epoch": 0.0016408622429718177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 452128
    },
    {
      "epoch": 0.0016409783773226544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 452160
    },
    {
      "epoch": 0.001641094511673491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 452192
    },
    {
      "epoch": 0.0016412106460243277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 452224
    },
    {
      "epoch": 0.0016413267803751645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 452256
    },
    {
      "epoch": 0.0016414429147260012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 452288
    },
    {
      "epoch": 0.001641559049076838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 452320
    },
    {
      "epoch": 0.0016416751834276745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 452352
    },
    {
      "epoch": 0.0016417913177785113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 452384
    },
    {
      "epoch": 0.001641907452129348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7519,
      "step": 452416
    },
    {
      "epoch": 0.0016420235864801848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7506,
      "step": 452448
    },
    {
      "epoch": 0.0016421397208310213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 452480
    },
    {
      "epoch": 0.001642255855181858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 452512
    },
    {
      "epoch": 0.0016423719895326948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 452544
    },
    {
      "epoch": 0.0016424881238835316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 452576
    },
    {
      "epoch": 0.0016426042582343683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 452608
    },
    {
      "epoch": 0.0016427203925852049,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 452640
    },
    {
      "epoch": 0.0016428365269360416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 452672
    },
    {
      "epoch": 0.0016429526612868784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 452704
    },
    {
      "epoch": 0.0016430687956377151,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 452736
    },
    {
      "epoch": 0.0016431849299885517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 452768
    },
    {
      "epoch": 0.0016433010643393884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 452800
    },
    {
      "epoch": 0.0016434171986902252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 452832
    },
    {
      "epoch": 0.001643533333041062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7451,
      "step": 452864
    },
    {
      "epoch": 0.0016436494673918987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 452896
    },
    {
      "epoch": 0.0016437656017427352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 452928
    },
    {
      "epoch": 0.001643881736093572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 452960
    },
    {
      "epoch": 0.0016439978704444087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 452992
    },
    {
      "epoch": 0.0016441140047952455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7234,
      "step": 453024
    },
    {
      "epoch": 0.001644230139146082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 453056
    },
    {
      "epoch": 0.0016443462734969188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 453088
    },
    {
      "epoch": 0.0016444624078477555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 453120
    },
    {
      "epoch": 0.0016445785421985923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 453152
    },
    {
      "epoch": 0.001644694676549429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 453184
    },
    {
      "epoch": 0.0016448108109002656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 453216
    },
    {
      "epoch": 0.0016449269452511023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 453248
    },
    {
      "epoch": 0.001645043079601939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 453280
    },
    {
      "epoch": 0.0016451592139527758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 453312
    },
    {
      "epoch": 0.0016452753483036124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 453344
    },
    {
      "epoch": 0.0016453914826544491,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 453376
    },
    {
      "epoch": 0.0016455076170052859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 453408
    },
    {
      "epoch": 0.0016456237513561226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 453440
    },
    {
      "epoch": 0.0016457398857069594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 453472
    },
    {
      "epoch": 0.001645856020057796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 453504
    },
    {
      "epoch": 0.0016459721544086327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 453536
    },
    {
      "epoch": 0.0016460882887594694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 453568
    },
    {
      "epoch": 0.0016462044231103062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 453600
    },
    {
      "epoch": 0.0016463205574611427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 453632
    },
    {
      "epoch": 0.0016464366918119795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 453664
    },
    {
      "epoch": 0.0016465528261628162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 453696
    },
    {
      "epoch": 0.001646668960513653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 453728
    },
    {
      "epoch": 0.0016467850948644897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7845,
      "step": 453760
    },
    {
      "epoch": 0.0016469012292153263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 453792
    },
    {
      "epoch": 0.001647017363566163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 453824
    },
    {
      "epoch": 0.0016471334979169998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 453856
    },
    {
      "epoch": 0.0016472496322678365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 453888
    },
    {
      "epoch": 0.001647365766618673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 453920
    },
    {
      "epoch": 0.0016474819009695098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 453952
    },
    {
      "epoch": 0.0016475980353203466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 453984
    },
    {
      "epoch": 0.0016477141696711833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 454016
    },
    {
      "epoch": 0.00164783030402202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 454048
    },
    {
      "epoch": 0.0016479464383728566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 454080
    },
    {
      "epoch": 0.0016480625727236934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6994,
      "step": 454112
    },
    {
      "epoch": 0.0016481787070745301,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 454144
    },
    {
      "epoch": 0.001648294841425367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 454176
    },
    {
      "epoch": 0.0016484109757762034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 454208
    },
    {
      "epoch": 0.0016485271101270402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 454240
    },
    {
      "epoch": 0.001648643244477877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 454272
    },
    {
      "epoch": 0.0016487593788287137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 454304
    },
    {
      "epoch": 0.0016488755131795505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 454336
    },
    {
      "epoch": 0.001648991647530387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 454368
    },
    {
      "epoch": 0.0016491077818812237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 454400
    },
    {
      "epoch": 0.0016492239162320605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 454432
    },
    {
      "epoch": 0.0016493400505828973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 454464
    },
    {
      "epoch": 0.0016494561849337338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 454496
    },
    {
      "epoch": 0.0016495723192845705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 454528
    },
    {
      "epoch": 0.0016496884536354073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 454560
    },
    {
      "epoch": 0.001649804587986244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 454592
    },
    {
      "epoch": 0.0016499207223370808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 454624
    },
    {
      "epoch": 0.0016500368566879173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7522,
      "step": 454656
    },
    {
      "epoch": 0.001650152991038754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.681,
      "step": 454688
    },
    {
      "epoch": 0.0016502691253895909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 454720
    },
    {
      "epoch": 0.0016503852597404276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 454752
    },
    {
      "epoch": 0.0016505013940912641,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 454784
    },
    {
      "epoch": 0.001650617528442101,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 454816
    },
    {
      "epoch": 0.0016507336627929377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7514,
      "step": 454848
    },
    {
      "epoch": 0.0016508497971437744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7463,
      "step": 454880
    },
    {
      "epoch": 0.0016509659314946112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 454912
    },
    {
      "epoch": 0.0016510820658454477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 454944
    },
    {
      "epoch": 0.0016511982001962845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 454976
    },
    {
      "epoch": 0.0016513143345471212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 455008
    },
    {
      "epoch": 0.001651430468897958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 455040
    },
    {
      "epoch": 0.0016515466032487945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 455072
    },
    {
      "epoch": 0.0016516627375996313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7477,
      "step": 455104
    },
    {
      "epoch": 0.001651778871950468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 455136
    },
    {
      "epoch": 0.0016518950063013048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 455168
    },
    {
      "epoch": 0.0016520111406521415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 455200
    },
    {
      "epoch": 0.001652127275002978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 455232
    },
    {
      "epoch": 0.0016522434093538148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 455264
    },
    {
      "epoch": 0.0016523595437046516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 455296
    },
    {
      "epoch": 0.0016524756780554883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 455328
    },
    {
      "epoch": 0.0016525918124063249,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 455360
    },
    {
      "epoch": 0.0016527079467571616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7659,
      "step": 455392
    },
    {
      "epoch": 0.0016528240811079984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 455424
    },
    {
      "epoch": 0.0016529402154588351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 455456
    },
    {
      "epoch": 0.0016530563498096719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7449,
      "step": 455488
    },
    {
      "epoch": 0.0016531724841605084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 455520
    },
    {
      "epoch": 0.0016532886185113452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 455552
    },
    {
      "epoch": 0.001653404752862182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 455584
    },
    {
      "epoch": 0.0016535208872130187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 455616
    },
    {
      "epoch": 0.0016536370215638552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 455648
    },
    {
      "epoch": 0.001653753155914692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 455680
    },
    {
      "epoch": 0.0016538692902655287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7437,
      "step": 455712
    },
    {
      "epoch": 0.0016539854246163655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 455744
    },
    {
      "epoch": 0.0016541015589672022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 455776
    },
    {
      "epoch": 0.0016542176933180388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7516,
      "step": 455808
    },
    {
      "epoch": 0.0016543338276688755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 455840
    },
    {
      "epoch": 0.0016544499620197123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7297,
      "step": 455872
    },
    {
      "epoch": 0.001654566096370549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7559,
      "step": 455904
    },
    {
      "epoch": 0.0016546822307213856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7594,
      "step": 455936
    },
    {
      "epoch": 0.0016547983650722223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 455968
    },
    {
      "epoch": 0.001654914499423059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 456000
    },
    {
      "epoch": 0.0016550306337738958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 456032
    },
    {
      "epoch": 0.0016551467681247326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 456064
    },
    {
      "epoch": 0.0016552629024755691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 456096
    },
    {
      "epoch": 0.0016553790368264059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 456128
    },
    {
      "epoch": 0.0016554951711772426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6904,
      "step": 456160
    },
    {
      "epoch": 0.0016556113055280794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 456192
    },
    {
      "epoch": 0.001655727439878916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 456224
    },
    {
      "epoch": 0.0016558435742297527,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 456256
    },
    {
      "epoch": 0.0016559597085805894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 456288
    },
    {
      "epoch": 0.0016560758429314262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 456320
    },
    {
      "epoch": 0.001656191977282263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 456352
    },
    {
      "epoch": 0.0016563081116330995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 456384
    },
    {
      "epoch": 0.0016564242459839362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7071,
      "step": 456416
    },
    {
      "epoch": 0.001656540380334773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 456448
    },
    {
      "epoch": 0.0016566565146856097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 456480
    },
    {
      "epoch": 0.0016567726490364463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 456512
    },
    {
      "epoch": 0.001656888783387283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6971,
      "step": 456544
    },
    {
      "epoch": 0.0016570049177381198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7045,
      "step": 456576
    },
    {
      "epoch": 0.0016571210520889565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 456608
    },
    {
      "epoch": 0.0016572371864397933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 456640
    },
    {
      "epoch": 0.0016573533207906298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 456672
    },
    {
      "epoch": 0.0016574694551414666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 456704
    },
    {
      "epoch": 0.0016575855894923033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 456736
    },
    {
      "epoch": 0.00165770172384314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 456768
    },
    {
      "epoch": 0.0016578178581939766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 456800
    },
    {
      "epoch": 0.0016579339925448134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7499,
      "step": 456832
    },
    {
      "epoch": 0.0016580501268956501,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 456864
    },
    {
      "epoch": 0.0016581662612464869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 456896
    },
    {
      "epoch": 0.0016582823955973236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 456928
    },
    {
      "epoch": 0.0016583985299481602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 456960
    },
    {
      "epoch": 0.001658514664298997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7028,
      "step": 456992
    },
    {
      "epoch": 0.0016586307986498337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 457024
    },
    {
      "epoch": 0.0016587469330006704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 457056
    },
    {
      "epoch": 0.001658863067351507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 457088
    },
    {
      "epoch": 0.0016589792017023437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 457120
    },
    {
      "epoch": 0.0016590953360531805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 457152
    },
    {
      "epoch": 0.0016592114704040172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 457184
    },
    {
      "epoch": 0.001659327604754854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 457216
    },
    {
      "epoch": 0.0016594437391056905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7417,
      "step": 457248
    },
    {
      "epoch": 0.0016595598734565273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 457280
    },
    {
      "epoch": 0.001659676007807364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 457312
    },
    {
      "epoch": 0.0016597921421582008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 457344
    },
    {
      "epoch": 0.0016599082765090373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 457376
    },
    {
      "epoch": 0.001660024410859874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 457408
    },
    {
      "epoch": 0.0016601405452107108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 457440
    },
    {
      "epoch": 0.0016602566795615476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 457472
    },
    {
      "epoch": 0.0016603728139123843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 457504
    },
    {
      "epoch": 0.0016604889482632209,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 457536
    },
    {
      "epoch": 0.0016606050826140576,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7469,
      "step": 457568
    },
    {
      "epoch": 0.0016607212169648944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.706,
      "step": 457600
    },
    {
      "epoch": 0.0016608373513157311,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 457632
    },
    {
      "epoch": 0.0016609534856665677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 457664
    },
    {
      "epoch": 0.0016610696200174044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 457696
    },
    {
      "epoch": 0.0016611857543682412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 457728
    },
    {
      "epoch": 0.001661301888719078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 457760
    },
    {
      "epoch": 0.0016614180230699147,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 457792
    },
    {
      "epoch": 0.0016615341574207512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 457824
    },
    {
      "epoch": 0.001661650291771588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 457856
    },
    {
      "epoch": 0.0016617664261224247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 457888
    },
    {
      "epoch": 0.0016618825604732615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 457920
    },
    {
      "epoch": 0.001661998694824098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 457952
    },
    {
      "epoch": 0.0016621148291749348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 457984
    },
    {
      "epoch": 0.0016622309635257715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 458016
    },
    {
      "epoch": 0.0016623470978766083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 458048
    },
    {
      "epoch": 0.001662463232227445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 458080
    },
    {
      "epoch": 0.0016625793665782816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7472,
      "step": 458112
    },
    {
      "epoch": 0.0016626955009291183,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 458144
    },
    {
      "epoch": 0.001662811635279955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 458176
    },
    {
      "epoch": 0.0016629277696307918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 458208
    },
    {
      "epoch": 0.0016630439039816284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 458240
    },
    {
      "epoch": 0.0016631600383324651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 458272
    },
    {
      "epoch": 0.001663276172683302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 458304
    },
    {
      "epoch": 0.0016633923070341386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 458336
    },
    {
      "epoch": 0.0016635084413849754,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7397,
      "step": 458368
    },
    {
      "epoch": 0.001663624575735812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 458400
    },
    {
      "epoch": 0.0016637407100866487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 458432
    },
    {
      "epoch": 0.0016638568444374854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 458464
    },
    {
      "epoch": 0.0016639729787883222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 458496
    },
    {
      "epoch": 0.0016640891131391587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 458528
    },
    {
      "epoch": 0.0016642052474899955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7662,
      "step": 458560
    },
    {
      "epoch": 0.0016643213818408322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 458592
    },
    {
      "epoch": 0.001664437516191669,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 458624
    },
    {
      "epoch": 0.0016645536505425058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7529,
      "step": 458656
    },
    {
      "epoch": 0.0016646697848933423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 458688
    },
    {
      "epoch": 0.001664785919244179,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7406,
      "step": 458720
    },
    {
      "epoch": 0.0016649020535950158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 458752
    },
    {
      "epoch": 0.0016650181879458526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 458784
    },
    {
      "epoch": 0.001665134322296689,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 458816
    },
    {
      "epoch": 0.0016652504566475258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 458848
    },
    {
      "epoch": 0.0016653665909983626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 458880
    },
    {
      "epoch": 0.0016654827253491994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 458912
    },
    {
      "epoch": 0.001665598859700036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6945,
      "step": 458944
    },
    {
      "epoch": 0.0016657149940508726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 458976
    },
    {
      "epoch": 0.0016658311284017094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 459008
    },
    {
      "epoch": 0.0016659472627525462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 459040
    },
    {
      "epoch": 0.001666063397103383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6956,
      "step": 459072
    },
    {
      "epoch": 0.0016661795314542194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 459104
    },
    {
      "epoch": 0.0016662956658050562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 459136
    },
    {
      "epoch": 0.001666411800155893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 459168
    },
    {
      "epoch": 0.0016665279345067297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 459200
    },
    {
      "epoch": 0.0016666440688575665,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 459232
    },
    {
      "epoch": 0.001666760203208403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 459264
    },
    {
      "epoch": 0.0016668763375592398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 459296
    },
    {
      "epoch": 0.0016669924719100765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 459328
    },
    {
      "epoch": 0.0016671086062609133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 459360
    },
    {
      "epoch": 0.0016672247406117498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 459392
    },
    {
      "epoch": 0.0016673408749625866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 459424
    },
    {
      "epoch": 0.0016674570093134233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.752,
      "step": 459456
    },
    {
      "epoch": 0.00166757314366426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 459488
    },
    {
      "epoch": 0.0016676892780150968,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 459520
    },
    {
      "epoch": 0.0016678054123659334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 459552
    },
    {
      "epoch": 0.00166792154671677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 459584
    },
    {
      "epoch": 0.0016680376810676069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 459616
    },
    {
      "epoch": 0.0016681538154184436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6908,
      "step": 459648
    },
    {
      "epoch": 0.0016682699497692802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7137,
      "step": 459680
    },
    {
      "epoch": 0.001668386084120117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 459712
    },
    {
      "epoch": 0.0016685022184709537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.743,
      "step": 459744
    },
    {
      "epoch": 0.0016686183528217904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7496,
      "step": 459776
    },
    {
      "epoch": 0.0016687344871726272,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 459808
    },
    {
      "epoch": 0.0016688506215234637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 459840
    },
    {
      "epoch": 0.0016689667558743005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7456,
      "step": 459872
    },
    {
      "epoch": 0.0016690828902251372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 459904
    },
    {
      "epoch": 0.001669199024575974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 459936
    },
    {
      "epoch": 0.0016693151589268105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 459968
    },
    {
      "epoch": 0.0016694312932776473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 460000
    },
    {
      "epoch": 0.001669547427628484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 460032
    },
    {
      "epoch": 0.0016696635619793208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 460064
    },
    {
      "epoch": 0.0016697796963301575,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 460096
    },
    {
      "epoch": 0.001669895830680994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7506,
      "step": 460128
    },
    {
      "epoch": 0.0016700119650318308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 460160
    },
    {
      "epoch": 0.0016701280993826676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 460192
    },
    {
      "epoch": 0.0016702442337335043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 460224
    },
    {
      "epoch": 0.0016703603680843409,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 460256
    },
    {
      "epoch": 0.0016704765024351776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7549,
      "step": 460288
    },
    {
      "epoch": 0.0016705926367860144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7539,
      "step": 460320
    },
    {
      "epoch": 0.0016707087711368511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 460352
    },
    {
      "epoch": 0.0016708249054876879,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 460384
    },
    {
      "epoch": 0.0016709410398385244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 460416
    },
    {
      "epoch": 0.0016710571741893612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 460448
    },
    {
      "epoch": 0.001671173308540198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 460480
    },
    {
      "epoch": 0.0016712894428910347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 460512
    },
    {
      "epoch": 0.0016714055772418712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 460544
    },
    {
      "epoch": 0.001671521711592708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 460576
    },
    {
      "epoch": 0.0016716378459435447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 460608
    },
    {
      "epoch": 0.0016717539802943815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 460640
    },
    {
      "epoch": 0.0016718701146452182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 460672
    },
    {
      "epoch": 0.0016719862489960548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 460704
    },
    {
      "epoch": 0.0016721023833468915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7586,
      "step": 460736
    },
    {
      "epoch": 0.0016722185176977283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 460768
    },
    {
      "epoch": 0.001672334652048565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 460800
    },
    {
      "epoch": 0.0016724507863994016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7591,
      "step": 460832
    },
    {
      "epoch": 0.0016725669207502383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 460864
    },
    {
      "epoch": 0.001672683055101075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 460896
    },
    {
      "epoch": 0.0016727991894519118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 460928
    },
    {
      "epoch": 0.0016729153238027486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 460960
    },
    {
      "epoch": 0.0016730314581535851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7524,
      "step": 460992
    },
    {
      "epoch": 0.0016731475925044219,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 461024
    },
    {
      "epoch": 0.0016732637268552586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7589,
      "step": 461056
    },
    {
      "epoch": 0.0016733798612060954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 461088
    },
    {
      "epoch": 0.001673495995556932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 461120
    },
    {
      "epoch": 0.0016736121299077687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 461152
    },
    {
      "epoch": 0.0016737282642586054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 461184
    },
    {
      "epoch": 0.0016738443986094422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7528,
      "step": 461216
    },
    {
      "epoch": 0.001673960532960279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 461248
    },
    {
      "epoch": 0.0016740766673111155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 461280
    },
    {
      "epoch": 0.0016741928016619522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7109,
      "step": 461312
    },
    {
      "epoch": 0.001674308936012789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 461344
    },
    {
      "epoch": 0.0016744250703636257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 461376
    },
    {
      "epoch": 0.0016745412047144623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6977,
      "step": 461408
    },
    {
      "epoch": 0.001674657339065299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 461440
    },
    {
      "epoch": 0.0016747734734161358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 461472
    },
    {
      "epoch": 0.0016748896077669725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 461504
    },
    {
      "epoch": 0.0016750057421178093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 461536
    },
    {
      "epoch": 0.0016751218764686458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 461568
    },
    {
      "epoch": 0.0016752380108194826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 461600
    },
    {
      "epoch": 0.0016753541451703193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7478,
      "step": 461632
    },
    {
      "epoch": 0.001675470279521156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 461664
    },
    {
      "epoch": 0.0016755864138719926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 461696
    },
    {
      "epoch": 0.0016757025482228294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 461728
    },
    {
      "epoch": 0.0016758186825736661,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 461760
    },
    {
      "epoch": 0.0016759348169245029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 461792
    },
    {
      "epoch": 0.0016760509512753396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 461824
    },
    {
      "epoch": 0.0016761670856261762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6996,
      "step": 461856
    },
    {
      "epoch": 0.001676283219977013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7408,
      "step": 461888
    },
    {
      "epoch": 0.0016763993543278497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 461920
    },
    {
      "epoch": 0.0016765154886786864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 461952
    },
    {
      "epoch": 0.001676631623029523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 461984
    },
    {
      "epoch": 0.0016767477573803597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 462016
    },
    {
      "epoch": 0.0016768638917311965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 462048
    },
    {
      "epoch": 0.0016769800260820332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 462080
    },
    {
      "epoch": 0.00167709616043287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 462112
    },
    {
      "epoch": 0.0016772122947837065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 462144
    },
    {
      "epoch": 0.0016773284291345433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 462176
    },
    {
      "epoch": 0.00167744456348538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 462208
    },
    {
      "epoch": 0.0016775606978362168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 462240
    },
    {
      "epoch": 0.0016776768321870533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 462272
    },
    {
      "epoch": 0.00167779296653789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 462304
    },
    {
      "epoch": 0.0016779091008887268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7004,
      "step": 462336
    },
    {
      "epoch": 0.0016780252352395636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 462368
    },
    {
      "epoch": 0.0016781413695904003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 462400
    },
    {
      "epoch": 0.0016782575039412369,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 462432
    },
    {
      "epoch": 0.0016783736382920736,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 462464
    },
    {
      "epoch": 0.0016784897726429104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 462496
    },
    {
      "epoch": 0.0016786059069937471,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 462528
    },
    {
      "epoch": 0.0016787220413445837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6894,
      "step": 462560
    },
    {
      "epoch": 0.0016788381756954204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 462592
    },
    {
      "epoch": 0.0016789543100462572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 462624
    },
    {
      "epoch": 0.001679070444397094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 462656
    },
    {
      "epoch": 0.0016791865787479307,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7453,
      "step": 462688
    },
    {
      "epoch": 0.0016793027130987672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 462720
    },
    {
      "epoch": 0.001679418847449604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 462752
    },
    {
      "epoch": 0.0016795349818004407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 462784
    },
    {
      "epoch": 0.0016796511161512775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 462816
    },
    {
      "epoch": 0.001679767250502114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 462848
    },
    {
      "epoch": 0.0016798833848529508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 462880
    },
    {
      "epoch": 0.0016799995192037875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 462912
    },
    {
      "epoch": 0.0016801156535546243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.761,
      "step": 462944
    },
    {
      "epoch": 0.001680231787905461,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 462976
    },
    {
      "epoch": 0.0016803479222562976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 463008
    },
    {
      "epoch": 0.0016804640566071343,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 463040
    },
    {
      "epoch": 0.001680580190957971,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 463072
    },
    {
      "epoch": 0.0016806963253088079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7084,
      "step": 463104
    },
    {
      "epoch": 0.0016808124596596444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 463136
    },
    {
      "epoch": 0.0016809285940104811,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 463168
    },
    {
      "epoch": 0.001681044728361318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 463200
    },
    {
      "epoch": 0.0016811608627121547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 463232
    },
    {
      "epoch": 0.0016812769970629912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 463264
    },
    {
      "epoch": 0.001681393131413828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 463296
    },
    {
      "epoch": 0.0016815092657646647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 463328
    },
    {
      "epoch": 0.0016816254001155015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 463360
    },
    {
      "epoch": 0.0016817415344663382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 463392
    },
    {
      "epoch": 0.0016818576688171747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 463424
    },
    {
      "epoch": 0.0016819738031680115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7047,
      "step": 463456
    },
    {
      "epoch": 0.0016820899375188483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 463488
    },
    {
      "epoch": 0.001682206071869685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7554,
      "step": 463520
    },
    {
      "epoch": 0.0016823222062205215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 463552
    },
    {
      "epoch": 0.0016824383405713583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 463584
    },
    {
      "epoch": 0.001682554474922195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7573,
      "step": 463616
    },
    {
      "epoch": 0.0016826706092730318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7552,
      "step": 463648
    },
    {
      "epoch": 0.0016827867436238686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 463680
    },
    {
      "epoch": 0.001682902877974705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 463712
    },
    {
      "epoch": 0.0016830190123255419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 463744
    },
    {
      "epoch": 0.0016831351466763786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 463776
    },
    {
      "epoch": 0.0016832512810272154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 463808
    },
    {
      "epoch": 0.001683367415378052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 463840
    },
    {
      "epoch": 0.0016834835497288887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 463872
    },
    {
      "epoch": 0.0016835996840797254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 463904
    },
    {
      "epoch": 0.0016837158184305622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 463936
    },
    {
      "epoch": 0.001683831952781399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 463968
    },
    {
      "epoch": 0.0016839480871322355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 464000
    },
    {
      "epoch": 0.0016840642214830722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6901,
      "step": 464032
    },
    {
      "epoch": 0.001684180355833909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 464064
    },
    {
      "epoch": 0.0016842964901847457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 464096
    },
    {
      "epoch": 0.0016844126245355823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 464128
    },
    {
      "epoch": 0.001684528758886419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 464160
    },
    {
      "epoch": 0.0016846448932372558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 464192
    },
    {
      "epoch": 0.0016847610275880925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 464224
    },
    {
      "epoch": 0.0016848771619389293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 464256
    },
    {
      "epoch": 0.0016849932962897658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 464288
    },
    {
      "epoch": 0.0016851094306406026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 464320
    },
    {
      "epoch": 0.0016852255649914393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6836,
      "step": 464352
    },
    {
      "epoch": 0.001685341699342276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 464384
    },
    {
      "epoch": 0.0016854578336931126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 464416
    },
    {
      "epoch": 0.0016855739680439494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 464448
    },
    {
      "epoch": 0.0016856901023947861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 464480
    },
    {
      "epoch": 0.0016858062367456229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 464512
    },
    {
      "epoch": 0.0016859223710964596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 464544
    },
    {
      "epoch": 0.0016860385054472962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 464576
    },
    {
      "epoch": 0.001686154639798133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 464608
    },
    {
      "epoch": 0.0016862707741489697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 464640
    },
    {
      "epoch": 0.0016863869084998064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 464672
    },
    {
      "epoch": 0.001686503042850643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7471,
      "step": 464704
    },
    {
      "epoch": 0.0016866191772014797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 464736
    },
    {
      "epoch": 0.0016867353115523165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 464768
    },
    {
      "epoch": 0.0016868514459031532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 464800
    },
    {
      "epoch": 0.00168696758025399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 464832
    },
    {
      "epoch": 0.0016870837146048265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 464864
    },
    {
      "epoch": 0.0016871998489556633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 464896
    },
    {
      "epoch": 0.0016873159833065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 464928
    },
    {
      "epoch": 0.0016874321176573368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 464960
    },
    {
      "epoch": 0.0016875482520081733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 464992
    },
    {
      "epoch": 0.00168766438635901,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 465024
    },
    {
      "epoch": 0.0016877805207098468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 465056
    },
    {
      "epoch": 0.0016878966550606836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 465088
    },
    {
      "epoch": 0.0016880127894115203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 465120
    },
    {
      "epoch": 0.0016881289237623569,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7583,
      "step": 465152
    },
    {
      "epoch": 0.0016882450581131936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 465184
    },
    {
      "epoch": 0.0016883611924640304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 465216
    },
    {
      "epoch": 0.0016884773268148671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 465248
    },
    {
      "epoch": 0.0016885934611657037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 465280
    },
    {
      "epoch": 0.0016887095955165404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 465312
    },
    {
      "epoch": 0.0016888257298673772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7483,
      "step": 465344
    },
    {
      "epoch": 0.001688941864218214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 465376
    },
    {
      "epoch": 0.0016890579985690507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 465408
    },
    {
      "epoch": 0.0016891741329198872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 465440
    },
    {
      "epoch": 0.001689290267270724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 465472
    },
    {
      "epoch": 0.0016894064016215607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 465504
    },
    {
      "epoch": 0.0016895225359723975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 465536
    },
    {
      "epoch": 0.001689638670323234,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7451,
      "step": 465568
    },
    {
      "epoch": 0.0016897548046740708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7528,
      "step": 465600
    },
    {
      "epoch": 0.0016898709390249075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 465632
    },
    {
      "epoch": 0.0016899870733757443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 465664
    },
    {
      "epoch": 0.001690103207726581,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 465696
    },
    {
      "epoch": 0.0016902193420774176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 465728
    },
    {
      "epoch": 0.0016903354764282543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 465760
    },
    {
      "epoch": 0.001690451610779091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 465792
    },
    {
      "epoch": 0.0016905677451299278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 465824
    },
    {
      "epoch": 0.0016906838794807644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 465856
    },
    {
      "epoch": 0.0016908000138316011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 465888
    },
    {
      "epoch": 0.0016909161481824379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 465920
    },
    {
      "epoch": 0.0016910322825332746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7463,
      "step": 465952
    },
    {
      "epoch": 0.0016911484168841114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 465984
    },
    {
      "epoch": 0.001691264551234948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 466016
    },
    {
      "epoch": 0.0016913806855857847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 466048
    },
    {
      "epoch": 0.0016914968199366214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 466080
    },
    {
      "epoch": 0.0016916129542874582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 466112
    },
    {
      "epoch": 0.0016917290886382947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6968,
      "step": 466144
    },
    {
      "epoch": 0.0016918452229891315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7037,
      "step": 466176
    },
    {
      "epoch": 0.0016919613573399682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 466208
    },
    {
      "epoch": 0.001692077491690805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.731,
      "step": 466240
    },
    {
      "epoch": 0.0016921936260416417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 466272
    },
    {
      "epoch": 0.0016923097603924783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 466304
    },
    {
      "epoch": 0.001692425894743315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 466336
    },
    {
      "epoch": 0.0016925420290941518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 466368
    },
    {
      "epoch": 0.0016926581634449885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 466400
    },
    {
      "epoch": 0.001692774297795825,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 466432
    },
    {
      "epoch": 0.0016928904321466618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7588,
      "step": 466464
    },
    {
      "epoch": 0.0016930065664974986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 466496
    },
    {
      "epoch": 0.0016931227008483353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 466528
    },
    {
      "epoch": 0.001693238835199172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 466560
    },
    {
      "epoch": 0.0016933549695500086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 466592
    },
    {
      "epoch": 0.0016934711039008454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 466624
    },
    {
      "epoch": 0.0016935872382516821,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 466656
    },
    {
      "epoch": 0.001693703372602519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 466688
    },
    {
      "epoch": 0.0016938195069533554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6691,
      "step": 466720
    },
    {
      "epoch": 0.0016939356413041922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 466752
    },
    {
      "epoch": 0.001694051775655029,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 466784
    },
    {
      "epoch": 0.0016941679100058657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 466816
    },
    {
      "epoch": 0.0016942840443567025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 466848
    },
    {
      "epoch": 0.001694400178707539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 466880
    },
    {
      "epoch": 0.0016945163130583757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 466912
    },
    {
      "epoch": 0.0016946324474092125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 466944
    },
    {
      "epoch": 0.0016947485817600492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 466976
    },
    {
      "epoch": 0.0016948647161108858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 467008
    },
    {
      "epoch": 0.0016949808504617225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 467040
    },
    {
      "epoch": 0.0016950969848125593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7003,
      "step": 467072
    },
    {
      "epoch": 0.001695213119163396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 467104
    },
    {
      "epoch": 0.0016953292535142328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 467136
    },
    {
      "epoch": 0.0016954453878650693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6921,
      "step": 467168
    },
    {
      "epoch": 0.001695561522215906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 467200
    },
    {
      "epoch": 0.0016956776565667428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 467232
    },
    {
      "epoch": 0.0016957937909175796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 467264
    },
    {
      "epoch": 0.0016959099252684161,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 467296
    },
    {
      "epoch": 0.001696026059619253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7401,
      "step": 467328
    },
    {
      "epoch": 0.0016961421939700896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 467360
    },
    {
      "epoch": 0.0016962583283209264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 467392
    },
    {
      "epoch": 0.0016963744626717632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 467424
    },
    {
      "epoch": 0.0016964905970225997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6924,
      "step": 467456
    },
    {
      "epoch": 0.0016966067313734364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.69,
      "step": 467488
    },
    {
      "epoch": 0.0016967228657242732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 467520
    },
    {
      "epoch": 0.00169683900007511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 467552
    },
    {
      "epoch": 0.0016969551344259465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 467584
    },
    {
      "epoch": 0.0016970712687767832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 467616
    },
    {
      "epoch": 0.00169718740312762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 467648
    },
    {
      "epoch": 0.0016973035374784568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 467680
    },
    {
      "epoch": 0.0016974196718292935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 467712
    },
    {
      "epoch": 0.00169753580618013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 467744
    },
    {
      "epoch": 0.0016976519405309668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 467776
    },
    {
      "epoch": 0.0016977680748818036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 467808
    },
    {
      "epoch": 0.0016978842092326403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 467840
    },
    {
      "epoch": 0.0016980003435834768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7479,
      "step": 467872
    },
    {
      "epoch": 0.0016981164779343136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6852,
      "step": 467904
    },
    {
      "epoch": 0.0016982326122851504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 467936
    },
    {
      "epoch": 0.0016983487466359871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 467968
    },
    {
      "epoch": 0.0016984648809868239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 468000
    },
    {
      "epoch": 0.0016985810153376604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 468032
    },
    {
      "epoch": 0.0016986971496884972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 468064
    },
    {
      "epoch": 0.001698813284039334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 468096
    },
    {
      "epoch": 0.0016989294183901707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 468128
    },
    {
      "epoch": 0.0016990455527410072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7053,
      "step": 468160
    },
    {
      "epoch": 0.001699161687091844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 468192
    },
    {
      "epoch": 0.0016992778214426807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 468224
    },
    {
      "epoch": 0.0016993939557935175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 468256
    },
    {
      "epoch": 0.0016995100901443542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 468288
    },
    {
      "epoch": 0.0016996262244951908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 468320
    },
    {
      "epoch": 0.0016997423588460275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6951,
      "step": 468352
    },
    {
      "epoch": 0.0016998584931968643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 468384
    },
    {
      "epoch": 0.001699974627547701,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 468416
    },
    {
      "epoch": 0.0017000907618985376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 468448
    },
    {
      "epoch": 0.0017002068962493743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 468480
    },
    {
      "epoch": 0.001700323030600211,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 468512
    },
    {
      "epoch": 0.0017004391649510478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7468,
      "step": 468544
    },
    {
      "epoch": 0.0017005552993018846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 468576
    },
    {
      "epoch": 0.001700671433652721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 468608
    },
    {
      "epoch": 0.0017007875680035579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 468640
    },
    {
      "epoch": 0.0017009037023543946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 468672
    },
    {
      "epoch": 0.0017010198367052314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6833,
      "step": 468704
    },
    {
      "epoch": 0.001701135971056068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 468736
    },
    {
      "epoch": 0.0017012521054069047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 468768
    },
    {
      "epoch": 0.0017013682397577414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6937,
      "step": 468800
    },
    {
      "epoch": 0.0017014843741085782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 468832
    },
    {
      "epoch": 0.001701600508459415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 468864
    },
    {
      "epoch": 0.0017017166428102515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 468896
    },
    {
      "epoch": 0.0017018327771610882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 468928
    },
    {
      "epoch": 0.001701948911511925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 468960
    },
    {
      "epoch": 0.0017020650458627617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 468992
    },
    {
      "epoch": 0.0017021811802135983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 469024
    },
    {
      "epoch": 0.001702297314564435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 469056
    },
    {
      "epoch": 0.0017024134489152718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 469088
    },
    {
      "epoch": 0.0017025295832661085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 469120
    },
    {
      "epoch": 0.0017026457176169453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 469152
    },
    {
      "epoch": 0.0017027618519677818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 469184
    },
    {
      "epoch": 0.0017028779863186186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 469216
    },
    {
      "epoch": 0.0017029941206694553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6894,
      "step": 469248
    },
    {
      "epoch": 0.001703110255020292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 469280
    },
    {
      "epoch": 0.0017032263893711286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 469312
    },
    {
      "epoch": 0.0017033425237219654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 469344
    },
    {
      "epoch": 0.0017034586580728021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 469376
    },
    {
      "epoch": 0.0017035747924236389,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 469408
    },
    {
      "epoch": 0.0017036909267744756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 469440
    },
    {
      "epoch": 0.0017038070611253122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 469472
    },
    {
      "epoch": 0.001703923195476149,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 469504
    },
    {
      "epoch": 0.0017040393298269857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 469536
    },
    {
      "epoch": 0.0017041554641778224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 469568
    },
    {
      "epoch": 0.001704271598528659,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 469600
    },
    {
      "epoch": 0.0017043877328794957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 469632
    },
    {
      "epoch": 0.0017045038672303325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 469664
    },
    {
      "epoch": 0.0017046200015811692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7023,
      "step": 469696
    },
    {
      "epoch": 0.001704736135932006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 469728
    },
    {
      "epoch": 0.0017048522702828425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 469760
    },
    {
      "epoch": 0.0017049684046336793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 469792
    },
    {
      "epoch": 0.001705084538984516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 469824
    },
    {
      "epoch": 0.0017052006733353528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 469856
    },
    {
      "epoch": 0.0017053168076861893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 469888
    },
    {
      "epoch": 0.001705432942037026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 469920
    },
    {
      "epoch": 0.0017055490763878628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 469952
    },
    {
      "epoch": 0.0017056652107386996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 469984
    },
    {
      "epoch": 0.0017057813450895363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7486,
      "step": 470016
    },
    {
      "epoch": 0.0017058974794403729,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 470048
    },
    {
      "epoch": 0.0017060136137912096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 470080
    },
    {
      "epoch": 0.0017061297481420464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 470112
    },
    {
      "epoch": 0.0017062458824928831,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6948,
      "step": 470144
    },
    {
      "epoch": 0.0017063620168437197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 470176
    },
    {
      "epoch": 0.0017064781511945564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 470208
    },
    {
      "epoch": 0.0017065942855453932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 470240
    },
    {
      "epoch": 0.00170671041989623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 470272
    },
    {
      "epoch": 0.0017068265542470667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 470304
    },
    {
      "epoch": 0.0017069426885979032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 470336
    },
    {
      "epoch": 0.00170705882294874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 470368
    },
    {
      "epoch": 0.0017071749572995767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 470400
    },
    {
      "epoch": 0.0017072910916504135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 470432
    },
    {
      "epoch": 0.00170740722600125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 470464
    },
    {
      "epoch": 0.0017075233603520868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 470496
    },
    {
      "epoch": 0.0017076394947029235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 470528
    },
    {
      "epoch": 0.0017077556290537603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 470560
    },
    {
      "epoch": 0.001707871763404597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 470592
    },
    {
      "epoch": 0.0017079878977554336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 470624
    },
    {
      "epoch": 0.0017081040321062703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 470656
    },
    {
      "epoch": 0.001708220166457107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 470688
    },
    {
      "epoch": 0.0017083363008079438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 470720
    },
    {
      "epoch": 0.0017084524351587804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 470752
    },
    {
      "epoch": 0.0017085685695096171,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 470784
    },
    {
      "epoch": 0.0017086847038604539,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7402,
      "step": 470816
    },
    {
      "epoch": 0.0017088008382112906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7531,
      "step": 470848
    },
    {
      "epoch": 0.0017089169725621274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 470880
    },
    {
      "epoch": 0.001709033106912964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 470912
    },
    {
      "epoch": 0.0017091492412638007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 470944
    },
    {
      "epoch": 0.0017092653756146374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 470976
    },
    {
      "epoch": 0.0017093815099654742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6902,
      "step": 471008
    },
    {
      "epoch": 0.0017094976443163107,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6901,
      "step": 471040
    },
    {
      "epoch": 0.0017096137786671475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 471072
    },
    {
      "epoch": 0.0017097299130179842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 471104
    },
    {
      "epoch": 0.001709846047368821,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 471136
    },
    {
      "epoch": 0.0017099621817196578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 471168
    },
    {
      "epoch": 0.0017100783160704943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 471200
    },
    {
      "epoch": 0.001710194450421331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 471232
    },
    {
      "epoch": 0.0017103105847721678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 471264
    },
    {
      "epoch": 0.0017104267191230046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 471296
    },
    {
      "epoch": 0.001710542853473841,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6886,
      "step": 471328
    },
    {
      "epoch": 0.0017106589878246778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 471360
    },
    {
      "epoch": 0.0017107751221755146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 471392
    },
    {
      "epoch": 0.0017108912565263514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 471424
    },
    {
      "epoch": 0.001711007390877188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6963,
      "step": 471456
    },
    {
      "epoch": 0.0017111235252280246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 471488
    },
    {
      "epoch": 0.0017112396595788614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7474,
      "step": 471520
    },
    {
      "epoch": 0.0017113557939296982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 471552
    },
    {
      "epoch": 0.001711471928280535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 471584
    },
    {
      "epoch": 0.0017115880626313714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 471616
    },
    {
      "epoch": 0.0017117041969822082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 471648
    },
    {
      "epoch": 0.001711820331333045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 471680
    },
    {
      "epoch": 0.0017119364656838817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 471712
    },
    {
      "epoch": 0.0017120526000347185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 471744
    },
    {
      "epoch": 0.001712168734385555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6897,
      "step": 471776
    },
    {
      "epoch": 0.0017122848687363917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 471808
    },
    {
      "epoch": 0.0017124010030872285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 471840
    },
    {
      "epoch": 0.0017125171374380653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 471872
    },
    {
      "epoch": 0.0017126332717889018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 471904
    },
    {
      "epoch": 0.0017127494061397385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 471936
    },
    {
      "epoch": 0.0017128655404905753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 471968
    },
    {
      "epoch": 0.001712981674841412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 472000
    },
    {
      "epoch": 0.0017130978091922488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 472032
    },
    {
      "epoch": 0.0017132139435430853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 472064
    },
    {
      "epoch": 0.001713330077893922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 472096
    },
    {
      "epoch": 0.0017134462122447589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 472128
    },
    {
      "epoch": 0.0017135623465955956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 472160
    },
    {
      "epoch": 0.0017136784809464321,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 472192
    },
    {
      "epoch": 0.001713794615297269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 472224
    },
    {
      "epoch": 0.0017139107496481057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 472256
    },
    {
      "epoch": 0.0017140268839989424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 472288
    },
    {
      "epoch": 0.0017141430183497792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6892,
      "step": 472320
    },
    {
      "epoch": 0.0017142591527006157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 472352
    },
    {
      "epoch": 0.0017143752870514525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 472384
    },
    {
      "epoch": 0.0017144914214022892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 472416
    },
    {
      "epoch": 0.001714607555753126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 472448
    },
    {
      "epoch": 0.0017147236901039625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6941,
      "step": 472480
    },
    {
      "epoch": 0.0017148398244547993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 472512
    },
    {
      "epoch": 0.001714955958805636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 472544
    },
    {
      "epoch": 0.0017150720931564728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 472576
    },
    {
      "epoch": 0.0017151882275073095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 472608
    },
    {
      "epoch": 0.001715304361858146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 472640
    },
    {
      "epoch": 0.0017154204962089828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 472672
    },
    {
      "epoch": 0.0017155366305598196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 472704
    },
    {
      "epoch": 0.0017156527649106563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 472736
    },
    {
      "epoch": 0.0017157688992614929,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 472768
    },
    {
      "epoch": 0.0017158850336123296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 472800
    },
    {
      "epoch": 0.0017160011679631664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 472832
    },
    {
      "epoch": 0.0017161173023140031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6949,
      "step": 472864
    },
    {
      "epoch": 0.0017162334366648399,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 472896
    },
    {
      "epoch": 0.0017163495710156764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 472928
    },
    {
      "epoch": 0.0017164657053665132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 472960
    },
    {
      "epoch": 0.00171658183971735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 472992
    },
    {
      "epoch": 0.0017166979740681867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 473024
    },
    {
      "epoch": 0.0017168141084190232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 473056
    },
    {
      "epoch": 0.00171693024276986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6799,
      "step": 473088
    },
    {
      "epoch": 0.0017170463771206967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 473120
    },
    {
      "epoch": 0.0017171625114715335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 473152
    },
    {
      "epoch": 0.0017172786458223702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 473184
    },
    {
      "epoch": 0.0017173947801732068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 473216
    },
    {
      "epoch": 0.0017175109145240435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 473248
    },
    {
      "epoch": 0.0017176270488748803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7418,
      "step": 473280
    },
    {
      "epoch": 0.001717743183225717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 473312
    },
    {
      "epoch": 0.0017178593175765536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 473344
    },
    {
      "epoch": 0.0017179754519273903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 473376
    },
    {
      "epoch": 0.001718091586278227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 473408
    },
    {
      "epoch": 0.0017182077206290638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 473440
    },
    {
      "epoch": 0.0017183238549799006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7658,
      "step": 473472
    },
    {
      "epoch": 0.0017184399893307371,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 473504
    },
    {
      "epoch": 0.0017185561236815739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 473536
    },
    {
      "epoch": 0.0017186722580324106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6866,
      "step": 473568
    },
    {
      "epoch": 0.0017187883923832474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 473600
    },
    {
      "epoch": 0.001718904526734084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 473632
    },
    {
      "epoch": 0.0017190206610849207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6914,
      "step": 473664
    },
    {
      "epoch": 0.0017191367954357574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6959,
      "step": 473696
    },
    {
      "epoch": 0.0017192529297865942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 473728
    },
    {
      "epoch": 0.001719369064137431,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 473760
    },
    {
      "epoch": 0.0017194851984882675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 473792
    },
    {
      "epoch": 0.0017196013328391042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 473824
    },
    {
      "epoch": 0.001719717467189941,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 473856
    },
    {
      "epoch": 0.0017198336015407777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 473888
    },
    {
      "epoch": 0.0017199497358916143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 473920
    },
    {
      "epoch": 0.001720065870242451,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 473952
    },
    {
      "epoch": 0.0017201820045932878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6984,
      "step": 473984
    },
    {
      "epoch": 0.0017202981389441245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7012,
      "step": 474016
    },
    {
      "epoch": 0.0017204142732949613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 474048
    },
    {
      "epoch": 0.0017205304076457978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 474080
    },
    {
      "epoch": 0.0017206465419966346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 474112
    },
    {
      "epoch": 0.0017207626763474713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 474144
    },
    {
      "epoch": 0.001720878810698308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 474176
    },
    {
      "epoch": 0.0017209949450491446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 474208
    },
    {
      "epoch": 0.0017211110793999814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 474240
    },
    {
      "epoch": 0.0017212272137508181,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6925,
      "step": 474272
    },
    {
      "epoch": 0.0017213433481016549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 474304
    },
    {
      "epoch": 0.0017214594824524916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.747,
      "step": 474336
    },
    {
      "epoch": 0.0017215756168033282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7562,
      "step": 474368
    },
    {
      "epoch": 0.001721691751154165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 474400
    },
    {
      "epoch": 0.0017218078855050017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6804,
      "step": 474432
    },
    {
      "epoch": 0.0017219240198558384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 474464
    },
    {
      "epoch": 0.001722040154206675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 474496
    },
    {
      "epoch": 0.0017221562885575117,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 474528
    },
    {
      "epoch": 0.0017222724229083485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 474560
    },
    {
      "epoch": 0.0017223885572591852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 474592
    },
    {
      "epoch": 0.001722504691610022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 474624
    },
    {
      "epoch": 0.0017226208259608585,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 474656
    },
    {
      "epoch": 0.0017227369603116953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 474688
    },
    {
      "epoch": 0.001722853094662532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 474720
    },
    {
      "epoch": 0.0017229692290133688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 474752
    },
    {
      "epoch": 0.0017230853633642053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 474784
    },
    {
      "epoch": 0.001723201497715042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 474816
    },
    {
      "epoch": 0.0017233176320658788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 474848
    },
    {
      "epoch": 0.0017234337664167156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 474880
    },
    {
      "epoch": 0.0017235499007675523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 474912
    },
    {
      "epoch": 0.0017236660351183889,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.747,
      "step": 474944
    },
    {
      "epoch": 0.0017237821694692256,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 474976
    },
    {
      "epoch": 0.0017238983038200624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 475008
    },
    {
      "epoch": 0.0017240144381708991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 475040
    },
    {
      "epoch": 0.0017241305725217357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 475072
    },
    {
      "epoch": 0.0017242467068725724,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 475104
    },
    {
      "epoch": 0.0017243628412234092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 475136
    },
    {
      "epoch": 0.001724478975574246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 475168
    },
    {
      "epoch": 0.0017245951099250827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 475200
    },
    {
      "epoch": 0.0017247112442759192,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7694,
      "step": 475232
    },
    {
      "epoch": 0.001724827378626756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 475264
    },
    {
      "epoch": 0.0017249435129775927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 475296
    },
    {
      "epoch": 0.0017250596473284295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 475328
    },
    {
      "epoch": 0.001725175781679266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 475360
    },
    {
      "epoch": 0.0017252919160301028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 475392
    },
    {
      "epoch": 0.0017254080503809395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 475424
    },
    {
      "epoch": 0.0017255241847317763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 475456
    },
    {
      "epoch": 0.001725640319082613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 475488
    },
    {
      "epoch": 0.0017257564534334496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 475520
    },
    {
      "epoch": 0.0017258725877842863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 475552
    },
    {
      "epoch": 0.001725988722135123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 475584
    },
    {
      "epoch": 0.0017261048564859599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 475616
    },
    {
      "epoch": 0.0017262209908367964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7481,
      "step": 475648
    },
    {
      "epoch": 0.0017263371251876331,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 475680
    },
    {
      "epoch": 0.00172645325953847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 475712
    },
    {
      "epoch": 0.0017265693938893067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 475744
    },
    {
      "epoch": 0.0017266855282401434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 475776
    },
    {
      "epoch": 0.00172680166259098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 475808
    },
    {
      "epoch": 0.0017269177969418167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 475840
    },
    {
      "epoch": 0.0017270339312926535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 475872
    },
    {
      "epoch": 0.0017271500656434902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 475904
    },
    {
      "epoch": 0.0017272661999943267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 475936
    },
    {
      "epoch": 0.0017273823343451635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 475968
    },
    {
      "epoch": 0.0017274984686960003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 476000
    },
    {
      "epoch": 0.001727614603046837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 476032
    },
    {
      "epoch": 0.0017277307373976738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 476064
    },
    {
      "epoch": 0.0017278468717485103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 476096
    },
    {
      "epoch": 0.001727963006099347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 476128
    },
    {
      "epoch": 0.0017280791404501838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 476160
    },
    {
      "epoch": 0.0017281952748010206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6974,
      "step": 476192
    },
    {
      "epoch": 0.001728311409151857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 476224
    },
    {
      "epoch": 0.0017284275435026939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 476256
    },
    {
      "epoch": 0.0017285436778535306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6933,
      "step": 476288
    },
    {
      "epoch": 0.0017286598122043674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 476320
    },
    {
      "epoch": 0.0017287759465552041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 476352
    },
    {
      "epoch": 0.0017288920809060406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 476384
    },
    {
      "epoch": 0.0017290082152568774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 476416
    },
    {
      "epoch": 0.0017291243496077142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 476448
    },
    {
      "epoch": 0.001729240483958551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 476480
    },
    {
      "epoch": 0.0017293566183093874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 476512
    },
    {
      "epoch": 0.0017294727526602242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 476544
    },
    {
      "epoch": 0.001729588887011061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 476576
    },
    {
      "epoch": 0.0017297050213618977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6923,
      "step": 476608
    },
    {
      "epoch": 0.0017298211557127345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.688,
      "step": 476640
    },
    {
      "epoch": 0.001729937290063571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6999,
      "step": 476672
    },
    {
      "epoch": 0.0017300534244144078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 476704
    },
    {
      "epoch": 0.0017301695587652445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 476736
    },
    {
      "epoch": 0.0017302856931160813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 476768
    },
    {
      "epoch": 0.0017304018274669178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 476800
    },
    {
      "epoch": 0.0017305179618177546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 476832
    },
    {
      "epoch": 0.0017306340961685913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 476864
    },
    {
      "epoch": 0.001730750230519428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 476896
    },
    {
      "epoch": 0.0017308663648702648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 476928
    },
    {
      "epoch": 0.0017309824992211014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 476960
    },
    {
      "epoch": 0.0017310986335719381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 476992
    },
    {
      "epoch": 0.0017312147679227749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 477024
    },
    {
      "epoch": 0.0017313309022736116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 477056
    },
    {
      "epoch": 0.0017314470366244482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 477088
    },
    {
      "epoch": 0.001731563170975285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 477120
    },
    {
      "epoch": 0.0017316793053261217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 477152
    },
    {
      "epoch": 0.0017317954396769584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6829,
      "step": 477184
    },
    {
      "epoch": 0.0017319115740277952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 477216
    },
    {
      "epoch": 0.0017320277083786317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 477248
    },
    {
      "epoch": 0.0017321438427294685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 477280
    },
    {
      "epoch": 0.0017322599770803052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 477312
    },
    {
      "epoch": 0.001732376111431142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 477344
    },
    {
      "epoch": 0.0017324922457819785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7031,
      "step": 477376
    },
    {
      "epoch": 0.0017326083801328153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 477408
    },
    {
      "epoch": 0.001732724514483652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 477440
    },
    {
      "epoch": 0.0017328406488344888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 477472
    },
    {
      "epoch": 0.0017329567831853255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 477504
    },
    {
      "epoch": 0.001733072917536162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.69,
      "step": 477536
    },
    {
      "epoch": 0.0017331890518869988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 477568
    },
    {
      "epoch": 0.0017333051862378356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 477600
    },
    {
      "epoch": 0.0017334213205886723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 477632
    },
    {
      "epoch": 0.0017335374549395089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 477664
    },
    {
      "epoch": 0.0017336535892903456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 477696
    },
    {
      "epoch": 0.0017337697236411824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 477728
    },
    {
      "epoch": 0.0017338858579920191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 477760
    },
    {
      "epoch": 0.0017340019923428559,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 477792
    },
    {
      "epoch": 0.0017341181266936924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 477824
    },
    {
      "epoch": 0.0017342342610445292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7536,
      "step": 477856
    },
    {
      "epoch": 0.001734350395395366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 477888
    },
    {
      "epoch": 0.0017344665297462027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 477920
    },
    {
      "epoch": 0.0017345826640970392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7073,
      "step": 477952
    },
    {
      "epoch": 0.001734698798447876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 477984
    },
    {
      "epoch": 0.0017348149327987127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 478016
    },
    {
      "epoch": 0.0017349310671495495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 478048
    },
    {
      "epoch": 0.0017350472015003862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.705,
      "step": 478080
    },
    {
      "epoch": 0.0017351633358512228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7014,
      "step": 478112
    },
    {
      "epoch": 0.0017352794702020595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 478144
    },
    {
      "epoch": 0.0017353956045528963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 478176
    },
    {
      "epoch": 0.001735511738903733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.709,
      "step": 478208
    },
    {
      "epoch": 0.0017356278732545696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 478240
    },
    {
      "epoch": 0.0017357440076054063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 478272
    },
    {
      "epoch": 0.001735860141956243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 478304
    },
    {
      "epoch": 0.0017359762763070798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 478336
    },
    {
      "epoch": 0.0017360924106579166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 478368
    },
    {
      "epoch": 0.0017362085450087531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7446,
      "step": 478400
    },
    {
      "epoch": 0.0017363246793595899,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 478432
    },
    {
      "epoch": 0.0017364408137104266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 478464
    },
    {
      "epoch": 0.0017365569480612634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 478496
    },
    {
      "epoch": 0.0017366730824121,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 478528
    },
    {
      "epoch": 0.0017367892167629367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 478560
    },
    {
      "epoch": 0.0017369053511137734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 478592
    },
    {
      "epoch": 0.0017370214854646102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6926,
      "step": 478624
    },
    {
      "epoch": 0.001737137619815447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 478656
    },
    {
      "epoch": 0.0017372537541662835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 478688
    },
    {
      "epoch": 0.0017373698885171202,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 478720
    },
    {
      "epoch": 0.001737486022867957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 478752
    },
    {
      "epoch": 0.0017376021572187937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 478784
    },
    {
      "epoch": 0.0017377182915696303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 478816
    },
    {
      "epoch": 0.001737834425920467,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7021,
      "step": 478848
    },
    {
      "epoch": 0.0017379505602713038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 478880
    },
    {
      "epoch": 0.0017380666946221405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 478912
    },
    {
      "epoch": 0.0017381828289729773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 478944
    },
    {
      "epoch": 0.0017382989633238138,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 478976
    },
    {
      "epoch": 0.0017384150976746506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.699,
      "step": 479008
    },
    {
      "epoch": 0.0017385312320254873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 479040
    },
    {
      "epoch": 0.001738647366376324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 479072
    },
    {
      "epoch": 0.0017387635007271606,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 479104
    },
    {
      "epoch": 0.0017388796350779974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 479136
    },
    {
      "epoch": 0.0017389957694288341,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 479168
    },
    {
      "epoch": 0.001739111903779671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 479200
    },
    {
      "epoch": 0.0017392280381305076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 479232
    },
    {
      "epoch": 0.0017393441724813442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 479264
    },
    {
      "epoch": 0.001739460306832181,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 479296
    },
    {
      "epoch": 0.0017395764411830177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 479328
    },
    {
      "epoch": 0.0017396925755338544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6899,
      "step": 479360
    },
    {
      "epoch": 0.001739808709884691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 479392
    },
    {
      "epoch": 0.0017399248442355277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 479424
    },
    {
      "epoch": 0.0017400409785863645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 479456
    },
    {
      "epoch": 0.0017401571129372012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 479488
    },
    {
      "epoch": 0.001740273247288038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 479520
    },
    {
      "epoch": 0.0017403893816388745,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 479552
    },
    {
      "epoch": 0.0017405055159897113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 479584
    },
    {
      "epoch": 0.001740621650340548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7369,
      "step": 479616
    },
    {
      "epoch": 0.0017407377846913848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 479648
    },
    {
      "epoch": 0.0017408539190422213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 479680
    },
    {
      "epoch": 0.001740970053393058,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 479712
    },
    {
      "epoch": 0.0017410861877438948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 479744
    },
    {
      "epoch": 0.0017412023220947316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 479776
    },
    {
      "epoch": 0.0017413184564455684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 479808
    },
    {
      "epoch": 0.0017414345907964049,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 479840
    },
    {
      "epoch": 0.0017415507251472416,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 479872
    },
    {
      "epoch": 0.0017416668594980784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6943,
      "step": 479904
    },
    {
      "epoch": 0.0017417829938489152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 479936
    },
    {
      "epoch": 0.0017418991281997517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 479968
    },
    {
      "epoch": 0.0017420152625505884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 480000
    },
    {
      "epoch": 0.0017421313969014252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7543,
      "step": 480032
    },
    {
      "epoch": 0.001742247531252262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 480064
    },
    {
      "epoch": 0.0017423636656030987,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 480096
    },
    {
      "epoch": 0.0017424797999539352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 480128
    },
    {
      "epoch": 0.001742595934304772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 480160
    },
    {
      "epoch": 0.0017427120686556088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 480192
    },
    {
      "epoch": 0.0017428282030064455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.697,
      "step": 480224
    },
    {
      "epoch": 0.001742944337357282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 480256
    },
    {
      "epoch": 0.0017430604717081188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 480288
    },
    {
      "epoch": 0.0017431766060589556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 480320
    },
    {
      "epoch": 0.0017432927404097923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 480352
    },
    {
      "epoch": 0.001743408874760629,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 480384
    },
    {
      "epoch": 0.0017435250091114656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 480416
    },
    {
      "epoch": 0.0017436411434623024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 480448
    },
    {
      "epoch": 0.001743757277813139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 480480
    },
    {
      "epoch": 0.0017438734121639759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 480512
    },
    {
      "epoch": 0.0017439895465148124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 480544
    },
    {
      "epoch": 0.0017441056808656492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7381,
      "step": 480576
    },
    {
      "epoch": 0.001744221815216486,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 480608
    },
    {
      "epoch": 0.0017443379495673227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 480640
    },
    {
      "epoch": 0.0017444540839181594,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 480672
    },
    {
      "epoch": 0.001744570218268996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 480704
    },
    {
      "epoch": 0.0017446863526198327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 480736
    },
    {
      "epoch": 0.0017448024869706695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7454,
      "step": 480768
    },
    {
      "epoch": 0.0017449186213215062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6942,
      "step": 480800
    },
    {
      "epoch": 0.0017450347556723428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 480832
    },
    {
      "epoch": 0.0017451508900231795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 480864
    },
    {
      "epoch": 0.0017452670243740163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 480896
    },
    {
      "epoch": 0.001745383158724853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 480928
    },
    {
      "epoch": 0.0017454992930756898,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6923,
      "step": 480960
    },
    {
      "epoch": 0.0017456154274265263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6975,
      "step": 480992
    },
    {
      "epoch": 0.001745731561777363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 481024
    },
    {
      "epoch": 0.0017458476961281998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 481056
    },
    {
      "epoch": 0.0017459638304790366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 481088
    },
    {
      "epoch": 0.001746079964829873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 481120
    },
    {
      "epoch": 0.0017461960991807099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 481152
    },
    {
      "epoch": 0.0017463122335315466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 481184
    },
    {
      "epoch": 0.0017464283678823834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7074,
      "step": 481216
    },
    {
      "epoch": 0.0017465445022332201,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 481248
    },
    {
      "epoch": 0.0017466606365840567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 481280
    },
    {
      "epoch": 0.0017467767709348934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 481312
    },
    {
      "epoch": 0.0017468929052857302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 481344
    },
    {
      "epoch": 0.001747009039636567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 481376
    },
    {
      "epoch": 0.0017471251739874035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 481408
    },
    {
      "epoch": 0.0017472413083382402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 481440
    },
    {
      "epoch": 0.001747357442689077,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 481472
    },
    {
      "epoch": 0.0017474735770399137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6936,
      "step": 481504
    },
    {
      "epoch": 0.0017475897113907505,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6716,
      "step": 481536
    },
    {
      "epoch": 0.001747705845741587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6952,
      "step": 481568
    },
    {
      "epoch": 0.0017478219800924238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 481600
    },
    {
      "epoch": 0.0017479381144432605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7015,
      "step": 481632
    },
    {
      "epoch": 0.0017480542487940973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 481664
    },
    {
      "epoch": 0.0017481703831449338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 481696
    },
    {
      "epoch": 0.0017482865174957706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 481728
    },
    {
      "epoch": 0.0017484026518466073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 481760
    },
    {
      "epoch": 0.001748518786197444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 481792
    },
    {
      "epoch": 0.0017486349205482808,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 481824
    },
    {
      "epoch": 0.0017487510548991174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 481856
    },
    {
      "epoch": 0.0017488671892499541,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 481888
    },
    {
      "epoch": 0.0017489833236007909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 481920
    },
    {
      "epoch": 0.0017490994579516276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 481952
    },
    {
      "epoch": 0.0017492155923024642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.686,
      "step": 481984
    },
    {
      "epoch": 0.001749331726653301,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 482016
    },
    {
      "epoch": 0.0017494478610041377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7317,
      "step": 482048
    },
    {
      "epoch": 0.0017495639953549744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 482080
    },
    {
      "epoch": 0.0017496801297058112,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 482112
    },
    {
      "epoch": 0.0017497962640566477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 482144
    },
    {
      "epoch": 0.0017499123984074845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 482176
    },
    {
      "epoch": 0.0017500285327583212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 482208
    },
    {
      "epoch": 0.001750144667109158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 482240
    },
    {
      "epoch": 0.0017502608014599945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 482272
    },
    {
      "epoch": 0.0017503769358108313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 482304
    },
    {
      "epoch": 0.001750493070161668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 482336
    },
    {
      "epoch": 0.0017506092045125048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 482368
    },
    {
      "epoch": 0.0017507253388633415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 482400
    },
    {
      "epoch": 0.001750841473214178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 482432
    },
    {
      "epoch": 0.0017509576075650148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 482464
    },
    {
      "epoch": 0.0017510737419158516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 482496
    },
    {
      "epoch": 0.0017511898762666883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 482528
    },
    {
      "epoch": 0.0017513060106175249,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7086,
      "step": 482560
    },
    {
      "epoch": 0.0017514221449683616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6937,
      "step": 482592
    },
    {
      "epoch": 0.0017515382793191984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7311,
      "step": 482624
    },
    {
      "epoch": 0.0017516544136700351,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7604,
      "step": 482656
    },
    {
      "epoch": 0.0017517705480208719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 482688
    },
    {
      "epoch": 0.0017518866823717084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6871,
      "step": 482720
    },
    {
      "epoch": 0.0017520028167225452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 482752
    },
    {
      "epoch": 0.001752118951073382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 482784
    },
    {
      "epoch": 0.0017522350854242187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 482816
    },
    {
      "epoch": 0.0017523512197750552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.696,
      "step": 482848
    },
    {
      "epoch": 0.001752467354125892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 482880
    },
    {
      "epoch": 0.0017525834884767287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 482912
    },
    {
      "epoch": 0.0017526996228275655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 482944
    },
    {
      "epoch": 0.0017528157571784022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 482976
    },
    {
      "epoch": 0.0017529318915292388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7,
      "step": 483008
    },
    {
      "epoch": 0.0017530480258800755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 483040
    },
    {
      "epoch": 0.0017531641602309123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 483072
    },
    {
      "epoch": 0.001753280294581749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 483104
    },
    {
      "epoch": 0.0017533964289325856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 483136
    },
    {
      "epoch": 0.0017535125632834223,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 483168
    },
    {
      "epoch": 0.001753628697634259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 483200
    },
    {
      "epoch": 0.0017537448319850958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 483232
    },
    {
      "epoch": 0.0017538609663359326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 483264
    },
    {
      "epoch": 0.0017539771006867691,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 483296
    },
    {
      "epoch": 0.0017540932350376059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 483328
    },
    {
      "epoch": 0.0017542093693884426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7434,
      "step": 483360
    },
    {
      "epoch": 0.0017543255037392794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 483392
    },
    {
      "epoch": 0.001754441638090116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 483424
    },
    {
      "epoch": 0.0017545577724409527,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 483456
    },
    {
      "epoch": 0.0017546739067917894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 483488
    },
    {
      "epoch": 0.0017547900411426262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 483520
    },
    {
      "epoch": 0.001754906175493463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 483552
    },
    {
      "epoch": 0.0017550223098442995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 483584
    },
    {
      "epoch": 0.0017551384441951362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7085,
      "step": 483616
    },
    {
      "epoch": 0.001755254578545973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 483648
    },
    {
      "epoch": 0.0017553707128968097,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 483680
    },
    {
      "epoch": 0.0017554868472476463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6989,
      "step": 483712
    },
    {
      "epoch": 0.001755602981598483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 483744
    },
    {
      "epoch": 0.0017557191159493198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 483776
    },
    {
      "epoch": 0.0017558352503001565,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7419,
      "step": 483808
    },
    {
      "epoch": 0.0017559513846509933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.729,
      "step": 483840
    },
    {
      "epoch": 0.0017560675190018298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 483872
    },
    {
      "epoch": 0.0017561836533526666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 483904
    },
    {
      "epoch": 0.0017562997877035033,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 483936
    },
    {
      "epoch": 0.00175641592205434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 483968
    },
    {
      "epoch": 0.0017565320564051766,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 484000
    },
    {
      "epoch": 0.0017566481907560134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 484032
    },
    {
      "epoch": 0.0017567643251068501,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 484064
    },
    {
      "epoch": 0.001756880459457687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 484096
    },
    {
      "epoch": 0.0017569965938085237,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 484128
    },
    {
      "epoch": 0.0017571127281593602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 484160
    },
    {
      "epoch": 0.001757228862510197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 484192
    },
    {
      "epoch": 0.0017573449968610337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 484224
    },
    {
      "epoch": 0.0017574611312118705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7207,
      "step": 484256
    },
    {
      "epoch": 0.001757577265562707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 484288
    },
    {
      "epoch": 0.0017576933999135437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 484320
    },
    {
      "epoch": 0.0017578095342643805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 484352
    },
    {
      "epoch": 0.0017579256686152173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 484384
    },
    {
      "epoch": 0.001758041802966054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7542,
      "step": 484416
    },
    {
      "epoch": 0.0017581579373168905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 484448
    },
    {
      "epoch": 0.0017582740716677273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 484480
    },
    {
      "epoch": 0.001758390206018564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 484512
    },
    {
      "epoch": 0.0017585063403694008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 484544
    },
    {
      "epoch": 0.0017586224747202373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 484576
    },
    {
      "epoch": 0.001758738609071074,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 484608
    },
    {
      "epoch": 0.0017588547434219109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 484640
    },
    {
      "epoch": 0.0017589708777727476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7553,
      "step": 484672
    },
    {
      "epoch": 0.0017590870121235844,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 484704
    },
    {
      "epoch": 0.001759203146474421,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 484736
    },
    {
      "epoch": 0.0017593192808252577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 484768
    },
    {
      "epoch": 0.0017594354151760944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 484800
    },
    {
      "epoch": 0.0017595515495269312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 484832
    },
    {
      "epoch": 0.0017596676838777677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7502,
      "step": 484864
    },
    {
      "epoch": 0.0017597838182286045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7481,
      "step": 484896
    },
    {
      "epoch": 0.0017598999525794412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 484928
    },
    {
      "epoch": 0.001760016086930278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 484960
    },
    {
      "epoch": 0.0017601322212811145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7324,
      "step": 484992
    },
    {
      "epoch": 0.0017602483556319513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 485024
    },
    {
      "epoch": 0.001760364489982788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 485056
    },
    {
      "epoch": 0.0017604806243336248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7496,
      "step": 485088
    },
    {
      "epoch": 0.0017605967586844615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 485120
    },
    {
      "epoch": 0.001760712893035298,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 485152
    },
    {
      "epoch": 0.0017608290273861348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 485184
    },
    {
      "epoch": 0.0017609451617369716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 485216
    },
    {
      "epoch": 0.0017610612960878083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 485248
    },
    {
      "epoch": 0.0017611774304386449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 485280
    },
    {
      "epoch": 0.0017612935647894816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 485312
    },
    {
      "epoch": 0.0017614096991403184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 485344
    },
    {
      "epoch": 0.0017615258334911551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 485376
    },
    {
      "epoch": 0.0017616419678419919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7422,
      "step": 485408
    },
    {
      "epoch": 0.0017617581021928284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7457,
      "step": 485440
    },
    {
      "epoch": 0.0017618742365436652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 485472
    },
    {
      "epoch": 0.001761990370894502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 485504
    },
    {
      "epoch": 0.0017621065052453387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 485536
    },
    {
      "epoch": 0.0017622226395961752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 485568
    },
    {
      "epoch": 0.001762338773947012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.759,
      "step": 485600
    },
    {
      "epoch": 0.0017624549082978487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 485632
    },
    {
      "epoch": 0.0017625710426486855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 485664
    },
    {
      "epoch": 0.0017626871769995222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7603,
      "step": 485696
    },
    {
      "epoch": 0.0017628033113503588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 485728
    },
    {
      "epoch": 0.0017629194457011955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 485760
    },
    {
      "epoch": 0.0017630355800520323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7089,
      "step": 485792
    },
    {
      "epoch": 0.001763151714402869,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7093,
      "step": 485824
    },
    {
      "epoch": 0.0017632678487537056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 485856
    },
    {
      "epoch": 0.0017633839831045423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 485888
    },
    {
      "epoch": 0.001763500117455379,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 485920
    },
    {
      "epoch": 0.0017636162518062158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 485952
    },
    {
      "epoch": 0.0017637323861570526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 485984
    },
    {
      "epoch": 0.0017638485205078891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 486016
    },
    {
      "epoch": 0.0017639646548587259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7118,
      "step": 486048
    },
    {
      "epoch": 0.0017640807892095626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 486080
    },
    {
      "epoch": 0.0017641969235603994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 486112
    },
    {
      "epoch": 0.001764313057911236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 486144
    },
    {
      "epoch": 0.0017644291922620727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.751,
      "step": 486176
    },
    {
      "epoch": 0.0017645453266129094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 486208
    },
    {
      "epoch": 0.0017646614609637462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6893,
      "step": 486240
    },
    {
      "epoch": 0.001764777595314583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 486272
    },
    {
      "epoch": 0.0017648937296654195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7259,
      "step": 486304
    },
    {
      "epoch": 0.0017650098640162562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 486336
    },
    {
      "epoch": 0.001765125998367093,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 486368
    },
    {
      "epoch": 0.0017652421327179297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 486400
    },
    {
      "epoch": 0.0017653582670687663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 486432
    },
    {
      "epoch": 0.001765474401419603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7091,
      "step": 486464
    },
    {
      "epoch": 0.0017655905357704398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 486496
    },
    {
      "epoch": 0.0017657066701212765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 486528
    },
    {
      "epoch": 0.0017658228044721133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 486560
    },
    {
      "epoch": 0.0017659389388229498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 486592
    },
    {
      "epoch": 0.0017660550731737866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7489,
      "step": 486624
    },
    {
      "epoch": 0.0017661712075246233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 486656
    },
    {
      "epoch": 0.00176628734187546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7457,
      "step": 486688
    },
    {
      "epoch": 0.0017664034762262966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 486720
    },
    {
      "epoch": 0.0017665196105771334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 486752
    },
    {
      "epoch": 0.0017666357449279701,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 486784
    },
    {
      "epoch": 0.0017667518792788069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 486816
    },
    {
      "epoch": 0.0017668680136296436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 486848
    },
    {
      "epoch": 0.0017669841479804802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7094,
      "step": 486880
    },
    {
      "epoch": 0.001767100282331317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 486912
    },
    {
      "epoch": 0.0017672164166821537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 486944
    },
    {
      "epoch": 0.0017673325510329904,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 486976
    },
    {
      "epoch": 0.001767448685383827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 487008
    },
    {
      "epoch": 0.0017675648197346637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 487040
    },
    {
      "epoch": 0.0017676809540855005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7441,
      "step": 487072
    },
    {
      "epoch": 0.0017677970884363372,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 487104
    },
    {
      "epoch": 0.001767913222787174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7251,
      "step": 487136
    },
    {
      "epoch": 0.0017680293571380105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 487168
    },
    {
      "epoch": 0.0017681454914888473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 487200
    },
    {
      "epoch": 0.001768261625839684,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6955,
      "step": 487232
    },
    {
      "epoch": 0.0017683777601905208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 487264
    },
    {
      "epoch": 0.0017684938945413573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 487296
    },
    {
      "epoch": 0.001768610028892194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 487328
    },
    {
      "epoch": 0.0017687261632430308,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7648,
      "step": 487360
    },
    {
      "epoch": 0.0017688422975938676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 487392
    },
    {
      "epoch": 0.0017689584319447043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 487424
    },
    {
      "epoch": 0.0017690745662955409,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 487456
    },
    {
      "epoch": 0.0017691907006463776,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7319,
      "step": 487488
    },
    {
      "epoch": 0.0017693068349972144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 487520
    },
    {
      "epoch": 0.0017694229693480511,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 487552
    },
    {
      "epoch": 0.0017695391036988877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7516,
      "step": 487584
    },
    {
      "epoch": 0.0017696552380497244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7427,
      "step": 487616
    },
    {
      "epoch": 0.0017697713724005612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 487648
    },
    {
      "epoch": 0.001769887506751398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 487680
    },
    {
      "epoch": 0.0017700036411022347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 487712
    },
    {
      "epoch": 0.0017701197754530712,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 487744
    },
    {
      "epoch": 0.001770235909803908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 487776
    },
    {
      "epoch": 0.0017703520441547447,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 487808
    },
    {
      "epoch": 0.0017704681785055815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 487840
    },
    {
      "epoch": 0.001770584312856418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 487872
    },
    {
      "epoch": 0.0017707004472072548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7473,
      "step": 487904
    },
    {
      "epoch": 0.0017708165815580915,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7386,
      "step": 487936
    },
    {
      "epoch": 0.0017709327159089283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 487968
    },
    {
      "epoch": 0.001771048850259765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6978,
      "step": 488000
    },
    {
      "epoch": 0.0017711649846106016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 488032
    },
    {
      "epoch": 0.0017712811189614383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 488064
    },
    {
      "epoch": 0.001771397253312275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 488096
    },
    {
      "epoch": 0.0017715133876631118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 488128
    },
    {
      "epoch": 0.0017716295220139484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7623,
      "step": 488160
    },
    {
      "epoch": 0.0017717456563647851,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 488192
    },
    {
      "epoch": 0.001771861790715622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 488224
    },
    {
      "epoch": 0.0017719779250664586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 488256
    },
    {
      "epoch": 0.0017720940594172954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7416,
      "step": 488288
    },
    {
      "epoch": 0.001772210193768132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7421,
      "step": 488320
    },
    {
      "epoch": 0.0017723263281189687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 488352
    },
    {
      "epoch": 0.0017724424624698054,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 488384
    },
    {
      "epoch": 0.0017725585968206422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 488416
    },
    {
      "epoch": 0.0017726747311714787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 488448
    },
    {
      "epoch": 0.0017727908655223155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 488480
    },
    {
      "epoch": 0.0017729069998731522,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 488512
    },
    {
      "epoch": 0.001773023134223989,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6962,
      "step": 488544
    },
    {
      "epoch": 0.0017731392685748258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 488576
    },
    {
      "epoch": 0.0017732554029256623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 488608
    },
    {
      "epoch": 0.001773371537276499,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7171,
      "step": 488640
    },
    {
      "epoch": 0.0017734876716273358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 488672
    },
    {
      "epoch": 0.0017736038059781726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 488704
    },
    {
      "epoch": 0.001773719940329009,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 488736
    },
    {
      "epoch": 0.0017738360746798458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 488768
    },
    {
      "epoch": 0.0017739522090306826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 488800
    },
    {
      "epoch": 0.0017740683433815194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 488832
    },
    {
      "epoch": 0.0017741844777323561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 488864
    },
    {
      "epoch": 0.0017743006120831926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 488896
    },
    {
      "epoch": 0.0017744167464340294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 488928
    },
    {
      "epoch": 0.0017745328807848662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 488960
    },
    {
      "epoch": 0.001774649015135703,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 488992
    },
    {
      "epoch": 0.0017747651494865394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 489024
    },
    {
      "epoch": 0.0017748812838373762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 489056
    },
    {
      "epoch": 0.001774997418188213,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 489088
    },
    {
      "epoch": 0.0017751135525390497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7381,
      "step": 489120
    },
    {
      "epoch": 0.0017752296868898865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 489152
    },
    {
      "epoch": 0.001775345821240723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 489184
    },
    {
      "epoch": 0.0017754619555915598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 489216
    },
    {
      "epoch": 0.0017755780899423965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 489248
    },
    {
      "epoch": 0.0017756942242932333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 489280
    },
    {
      "epoch": 0.0017758103586440698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 489312
    },
    {
      "epoch": 0.0017759264929949066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 489344
    },
    {
      "epoch": 0.0017760426273457433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 489376
    },
    {
      "epoch": 0.00177615876169658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 489408
    },
    {
      "epoch": 0.0017762748960474168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 489440
    },
    {
      "epoch": 0.0017763910303982534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 489472
    },
    {
      "epoch": 0.00177650716474909,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 489504
    },
    {
      "epoch": 0.0017766232990999269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7387,
      "step": 489536
    },
    {
      "epoch": 0.0017767394334507636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 489568
    },
    {
      "epoch": 0.0017768555678016002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 489600
    },
    {
      "epoch": 0.001776971702152437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 489632
    },
    {
      "epoch": 0.0017770878365032737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7434,
      "step": 489664
    },
    {
      "epoch": 0.0017772039708541104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 489696
    },
    {
      "epoch": 0.0017773201052049472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7477,
      "step": 489728
    },
    {
      "epoch": 0.0017774362395557837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 489760
    },
    {
      "epoch": 0.0017775523739066205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 489792
    },
    {
      "epoch": 0.0017776685082574572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 489824
    },
    {
      "epoch": 0.001777784642608294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 489856
    },
    {
      "epoch": 0.0017779007769591305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 489888
    },
    {
      "epoch": 0.0017780169113099673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7552,
      "step": 489920
    },
    {
      "epoch": 0.001778133045660804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7526,
      "step": 489952
    },
    {
      "epoch": 0.0017782491800116408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7469,
      "step": 489984
    },
    {
      "epoch": 0.0017783653143624775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7438,
      "step": 490016
    },
    {
      "epoch": 0.001778481448713314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 490048
    },
    {
      "epoch": 0.0017785975830641508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 490080
    },
    {
      "epoch": 0.0017787137174149876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 490112
    },
    {
      "epoch": 0.0017788298517658243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7477,
      "step": 490144
    },
    {
      "epoch": 0.0017789459861166609,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 490176
    },
    {
      "epoch": 0.0017790621204674976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 490208
    },
    {
      "epoch": 0.0017791782548183344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 490240
    },
    {
      "epoch": 0.0017792943891691711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 490272
    },
    {
      "epoch": 0.0017794105235200079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 490304
    },
    {
      "epoch": 0.0017795266578708444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 490336
    },
    {
      "epoch": 0.0017796427922216812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 490368
    },
    {
      "epoch": 0.001779758926572518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 490400
    },
    {
      "epoch": 0.0017798750609233547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 490432
    },
    {
      "epoch": 0.0017799911952741912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7511,
      "step": 490464
    },
    {
      "epoch": 0.001780107329625028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 490496
    },
    {
      "epoch": 0.0017802234639758647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7279,
      "step": 490528
    },
    {
      "epoch": 0.0017803395983267015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7522,
      "step": 490560
    },
    {
      "epoch": 0.0017804557326775382,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7581,
      "step": 490592
    },
    {
      "epoch": 0.0017805718670283748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7562,
      "step": 490624
    },
    {
      "epoch": 0.0017806880013792115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 490656
    },
    {
      "epoch": 0.0017808041357300483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 490688
    },
    {
      "epoch": 0.001780920270080885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.71,
      "step": 490720
    },
    {
      "epoch": 0.0017810364044317216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7002,
      "step": 490752
    },
    {
      "epoch": 0.0017811525387825583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7095,
      "step": 490784
    },
    {
      "epoch": 0.001781268673133395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 490816
    },
    {
      "epoch": 0.0017813848074842318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 490848
    },
    {
      "epoch": 0.0017815009418350686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 490880
    },
    {
      "epoch": 0.0017816170761859051,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 490912
    },
    {
      "epoch": 0.0017817332105367419,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 490944
    },
    {
      "epoch": 0.0017818493448875786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 490976
    },
    {
      "epoch": 0.0017819654792384154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7435,
      "step": 491008
    },
    {
      "epoch": 0.001782081613589252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 491040
    },
    {
      "epoch": 0.0017821977479400887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 491072
    },
    {
      "epoch": 0.0017823138822909254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 491104
    },
    {
      "epoch": 0.0017824300166417622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7581,
      "step": 491136
    },
    {
      "epoch": 0.001782546150992599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 491168
    },
    {
      "epoch": 0.0017826622853434355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 491200
    },
    {
      "epoch": 0.0017827784196942722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.713,
      "step": 491232
    },
    {
      "epoch": 0.001782894554045109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6973,
      "step": 491264
    },
    {
      "epoch": 0.0017830106883959457,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 491296
    },
    {
      "epoch": 0.0017831268227467823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 491328
    },
    {
      "epoch": 0.001783242957097619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 491360
    },
    {
      "epoch": 0.0017833590914484558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 491392
    },
    {
      "epoch": 0.0017834752257992925,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7325,
      "step": 491424
    },
    {
      "epoch": 0.0017835913601501293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 491456
    },
    {
      "epoch": 0.0017837074945009658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 491488
    },
    {
      "epoch": 0.0017838236288518026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 491520
    },
    {
      "epoch": 0.0017839397632026393,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 491552
    },
    {
      "epoch": 0.001784055897553476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 491584
    },
    {
      "epoch": 0.0017841720319043126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 491616
    },
    {
      "epoch": 0.0017842881662551494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 491648
    },
    {
      "epoch": 0.0017844043006059861,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 491680
    },
    {
      "epoch": 0.0017845204349568229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 491712
    },
    {
      "epoch": 0.0017846365693076596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 491744
    },
    {
      "epoch": 0.0017847527036584962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 491776
    },
    {
      "epoch": 0.001784868838009333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 491808
    },
    {
      "epoch": 0.0017849849723601697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 491840
    },
    {
      "epoch": 0.0017851011067110064,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 491872
    },
    {
      "epoch": 0.001785217241061843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 491904
    },
    {
      "epoch": 0.0017853333754126797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 491936
    },
    {
      "epoch": 0.0017854495097635165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 491968
    },
    {
      "epoch": 0.0017855656441143532,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 492000
    },
    {
      "epoch": 0.00178568177846519,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 492032
    },
    {
      "epoch": 0.0017857979128160265,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 492064
    },
    {
      "epoch": 0.0017859140471668633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7277,
      "step": 492096
    },
    {
      "epoch": 0.0017860301815177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 492128
    },
    {
      "epoch": 0.0017861463158685368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7022,
      "step": 492160
    },
    {
      "epoch": 0.0017862624502193733,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 492192
    },
    {
      "epoch": 0.00178637858457021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7545,
      "step": 492224
    },
    {
      "epoch": 0.0017864947189210468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 492256
    },
    {
      "epoch": 0.0017866108532718836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 492288
    },
    {
      "epoch": 0.0017867269876227204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 492320
    },
    {
      "epoch": 0.0017868431219735569,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7059,
      "step": 492352
    },
    {
      "epoch": 0.0017869592563243936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 492384
    },
    {
      "epoch": 0.0017870753906752304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 492416
    },
    {
      "epoch": 0.0017871915250260671,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 492448
    },
    {
      "epoch": 0.0017873076593769037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 492480
    },
    {
      "epoch": 0.0017874237937277404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 492512
    },
    {
      "epoch": 0.0017875399280785772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 492544
    },
    {
      "epoch": 0.001787656062429414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 492576
    },
    {
      "epoch": 0.0017877721967802507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 492608
    },
    {
      "epoch": 0.0017878883311310872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7478,
      "step": 492640
    },
    {
      "epoch": 0.001788004465481924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 492672
    },
    {
      "epoch": 0.0017881205998327607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7226,
      "step": 492704
    },
    {
      "epoch": 0.0017882367341835975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 492736
    },
    {
      "epoch": 0.001788352868534434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7515,
      "step": 492768
    },
    {
      "epoch": 0.0017884690028852708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 492800
    },
    {
      "epoch": 0.0017885851372361075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7133,
      "step": 492832
    },
    {
      "epoch": 0.0017887012715869443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7018,
      "step": 492864
    },
    {
      "epoch": 0.001788817405937781,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 492896
    },
    {
      "epoch": 0.0017889335402886176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 492928
    },
    {
      "epoch": 0.0017890496746394543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 492960
    },
    {
      "epoch": 0.001789165808990291,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 492992
    },
    {
      "epoch": 0.0017892819433411279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 493024
    },
    {
      "epoch": 0.0017893980776919644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 493056
    },
    {
      "epoch": 0.0017895142120428011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 493088
    },
    {
      "epoch": 0.001789630346393638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 493120
    },
    {
      "epoch": 0.0017897464807444747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 493152
    },
    {
      "epoch": 0.0017898626150953114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7774,
      "step": 493184
    },
    {
      "epoch": 0.001789978749446148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7494,
      "step": 493216
    },
    {
      "epoch": 0.0017900948837969847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 493248
    },
    {
      "epoch": 0.0017902110181478215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 493280
    },
    {
      "epoch": 0.0017903271524986582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 493312
    },
    {
      "epoch": 0.0017904432868494947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 493344
    },
    {
      "epoch": 0.0017905594212003315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 493376
    },
    {
      "epoch": 0.0017906755555511683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7077,
      "step": 493408
    },
    {
      "epoch": 0.001790791689902005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 493440
    },
    {
      "epoch": 0.0017909078242528418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 493472
    },
    {
      "epoch": 0.0017910239586036783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7347,
      "step": 493504
    },
    {
      "epoch": 0.001791140092954515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 493536
    },
    {
      "epoch": 0.0017912562273053518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 493568
    },
    {
      "epoch": 0.0017913723616561886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 493600
    },
    {
      "epoch": 0.001791488496007025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 493632
    },
    {
      "epoch": 0.0017916046303578619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 493664
    },
    {
      "epoch": 0.0017917207647086986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 493696
    },
    {
      "epoch": 0.0017918368990595354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 493728
    },
    {
      "epoch": 0.0017919530334103721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 493760
    },
    {
      "epoch": 0.0017920691677612087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 493792
    },
    {
      "epoch": 0.0017921853021120454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 493824
    },
    {
      "epoch": 0.0017923014364628822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 493856
    },
    {
      "epoch": 0.001792417570813719,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 493888
    },
    {
      "epoch": 0.0017925337051645555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 493920
    },
    {
      "epoch": 0.0017926498395153922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 493952
    },
    {
      "epoch": 0.001792765973866229,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 493984
    },
    {
      "epoch": 0.0017928821082170657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 494016
    },
    {
      "epoch": 0.0017929982425679025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7405,
      "step": 494048
    },
    {
      "epoch": 0.001793114376918739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 494080
    },
    {
      "epoch": 0.0017932305112695758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7517,
      "step": 494112
    },
    {
      "epoch": 0.0017933466456204125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6946,
      "step": 494144
    },
    {
      "epoch": 0.0017934627799712493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 494176
    },
    {
      "epoch": 0.0017935789143220858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 494208
    },
    {
      "epoch": 0.0017936950486729226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7153,
      "step": 494240
    },
    {
      "epoch": 0.0017938111830237593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 494272
    },
    {
      "epoch": 0.001793927317374596,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 494304
    },
    {
      "epoch": 0.0017940434517254328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7241,
      "step": 494336
    },
    {
      "epoch": 0.0017941595860762694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 494368
    },
    {
      "epoch": 0.0017942757204271061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7524,
      "step": 494400
    },
    {
      "epoch": 0.0017943918547779429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 494432
    },
    {
      "epoch": 0.0017945079891287796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.744,
      "step": 494464
    },
    {
      "epoch": 0.0017946241234796162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 494496
    },
    {
      "epoch": 0.001794740257830453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7389,
      "step": 494528
    },
    {
      "epoch": 0.0017948563921812897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 494560
    },
    {
      "epoch": 0.0017949725265321264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 494592
    },
    {
      "epoch": 0.0017950886608829632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 494624
    },
    {
      "epoch": 0.0017952047952337997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7732,
      "step": 494656
    },
    {
      "epoch": 0.0017953209295846365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 494688
    },
    {
      "epoch": 0.0017954370639354732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 494720
    },
    {
      "epoch": 0.00179555319828631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 494752
    },
    {
      "epoch": 0.0017956693326371465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 494784
    },
    {
      "epoch": 0.0017957854669879833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 494816
    },
    {
      "epoch": 0.00179590160133882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 494848
    },
    {
      "epoch": 0.0017960177356896568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 494880
    },
    {
      "epoch": 0.0017961338700404935,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7509,
      "step": 494912
    },
    {
      "epoch": 0.00179625000439133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7927,
      "step": 494944
    },
    {
      "epoch": 0.0017963661387421668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 494976
    },
    {
      "epoch": 0.0017964822730930036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 495008
    },
    {
      "epoch": 0.0017965984074438403,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 495040
    },
    {
      "epoch": 0.0017967145417946769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 495072
    },
    {
      "epoch": 0.0017968306761455136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 495104
    },
    {
      "epoch": 0.0017969468104963504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.735,
      "step": 495136
    },
    {
      "epoch": 0.0017970629448471871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 495168
    },
    {
      "epoch": 0.0017971790791980239,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7697,
      "step": 495200
    },
    {
      "epoch": 0.0017972952135488604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 495232
    },
    {
      "epoch": 0.0017974113478996972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 495264
    },
    {
      "epoch": 0.001797527482250534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 495296
    },
    {
      "epoch": 0.0017976436166013707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 495328
    },
    {
      "epoch": 0.0017977597509522072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7448,
      "step": 495360
    },
    {
      "epoch": 0.001797875885303044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7575,
      "step": 495392
    },
    {
      "epoch": 0.0017979920196538807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 495424
    },
    {
      "epoch": 0.0017981081540047175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7367,
      "step": 495456
    },
    {
      "epoch": 0.0017982242883555542,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7551,
      "step": 495488
    },
    {
      "epoch": 0.0017983404227063908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 495520
    },
    {
      "epoch": 0.0017984565570572275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 495552
    },
    {
      "epoch": 0.0017985726914080643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 495584
    },
    {
      "epoch": 0.001798688825758901,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 495616
    },
    {
      "epoch": 0.0017988049601097376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 495648
    },
    {
      "epoch": 0.0017989210944605743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 495680
    },
    {
      "epoch": 0.001799037228811411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 495712
    },
    {
      "epoch": 0.0017991533631622478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7285,
      "step": 495744
    },
    {
      "epoch": 0.0017992694975130846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6954,
      "step": 495776
    },
    {
      "epoch": 0.0017993856318639211,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 495808
    },
    {
      "epoch": 0.0017995017662147579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 495840
    },
    {
      "epoch": 0.0017996179005655946,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7142,
      "step": 495872
    },
    {
      "epoch": 0.0017997340349164314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 495904
    },
    {
      "epoch": 0.001799850169267268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 495936
    },
    {
      "epoch": 0.0017999663036181047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 495968
    },
    {
      "epoch": 0.0018000824379689414,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7252,
      "step": 496000
    },
    {
      "epoch": 0.0018001985723197782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 496032
    },
    {
      "epoch": 0.001800314706670615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7469,
      "step": 496064
    },
    {
      "epoch": 0.0018004308410214515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7139,
      "step": 496096
    },
    {
      "epoch": 0.0018005469753722882,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 496128
    },
    {
      "epoch": 0.001800663109723125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 496160
    },
    {
      "epoch": 0.0018007792440739617,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 496192
    },
    {
      "epoch": 0.0018008953784247983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 496224
    },
    {
      "epoch": 0.001801011512775635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7383,
      "step": 496256
    },
    {
      "epoch": 0.0018011276471264718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 496288
    },
    {
      "epoch": 0.0018012437814773085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6886,
      "step": 496320
    },
    {
      "epoch": 0.0018013599158281453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7123,
      "step": 496352
    },
    {
      "epoch": 0.0018014760501789818,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 496384
    },
    {
      "epoch": 0.0018015921845298186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 496416
    },
    {
      "epoch": 0.0018017083188806553,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 496448
    },
    {
      "epoch": 0.001801824453231492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 496480
    },
    {
      "epoch": 0.0018019405875823286,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 496512
    },
    {
      "epoch": 0.0018020567219331654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 496544
    },
    {
      "epoch": 0.0018021728562840021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7483,
      "step": 496576
    },
    {
      "epoch": 0.001802288990634839,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 496608
    },
    {
      "epoch": 0.0018024051249856757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 496640
    },
    {
      "epoch": 0.0018025212593365122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 496672
    },
    {
      "epoch": 0.001802637393687349,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.756,
      "step": 496704
    },
    {
      "epoch": 0.0018027535280381857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 496736
    },
    {
      "epoch": 0.0018028696623890225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7001,
      "step": 496768
    },
    {
      "epoch": 0.001802985796739859,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 496800
    },
    {
      "epoch": 0.0018031019310906957,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 496832
    },
    {
      "epoch": 0.0018032180654415325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 496864
    },
    {
      "epoch": 0.0018033341997923693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 496896
    },
    {
      "epoch": 0.001803450334143206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 496928
    },
    {
      "epoch": 0.0018035664684940425,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 496960
    },
    {
      "epoch": 0.0018036826028448793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 496992
    },
    {
      "epoch": 0.001803798737195716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 497024
    },
    {
      "epoch": 0.0018039148715465528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 497056
    },
    {
      "epoch": 0.0018040310058973893,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 497088
    },
    {
      "epoch": 0.001804147140248226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7406,
      "step": 497120
    },
    {
      "epoch": 0.0018042632745990628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7519,
      "step": 497152
    },
    {
      "epoch": 0.0018043794089498996,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 497184
    },
    {
      "epoch": 0.0018044955433007364,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 497216
    },
    {
      "epoch": 0.001804611677651573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 497248
    },
    {
      "epoch": 0.0018047278120024096,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 497280
    },
    {
      "epoch": 0.0018048439463532464,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7025,
      "step": 497312
    },
    {
      "epoch": 0.0018049600807040832,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 497344
    },
    {
      "epoch": 0.0018050762150549197,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 497376
    },
    {
      "epoch": 0.0018051923494057564,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 497408
    },
    {
      "epoch": 0.0018053084837565932,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.736,
      "step": 497440
    },
    {
      "epoch": 0.00180542461810743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 497472
    },
    {
      "epoch": 0.0018055407524582667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 497504
    },
    {
      "epoch": 0.0018056568868091032,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 497536
    },
    {
      "epoch": 0.00180577302115994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 497568
    },
    {
      "epoch": 0.0018058891555107768,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 497600
    },
    {
      "epoch": 0.0018060052898616135,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 497632
    },
    {
      "epoch": 0.00180612142421245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 497664
    },
    {
      "epoch": 0.0018062375585632868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 497696
    },
    {
      "epoch": 0.0018063536929141236,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 497728
    },
    {
      "epoch": 0.0018064698272649603,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 497760
    },
    {
      "epoch": 0.001806585961615797,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 497792
    },
    {
      "epoch": 0.0018067020959666336,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 497824
    },
    {
      "epoch": 0.0018068182303174704,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 497856
    },
    {
      "epoch": 0.0018069343646683071,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 497888
    },
    {
      "epoch": 0.0018070504990191439,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 497920
    },
    {
      "epoch": 0.0018071666333699804,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 497952
    },
    {
      "epoch": 0.0018072827677208172,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.737,
      "step": 497984
    },
    {
      "epoch": 0.001807398902071654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7673,
      "step": 498016
    },
    {
      "epoch": 0.0018075150364224907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 498048
    },
    {
      "epoch": 0.0018076311707733274,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7394,
      "step": 498080
    },
    {
      "epoch": 0.001807747305124164,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7293,
      "step": 498112
    },
    {
      "epoch": 0.0018078634394750007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7538,
      "step": 498144
    },
    {
      "epoch": 0.0018079795738258375,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.765,
      "step": 498176
    },
    {
      "epoch": 0.0018080957081766742,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 498208
    },
    {
      "epoch": 0.0018082118425275108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6947,
      "step": 498240
    },
    {
      "epoch": 0.0018083279768783475,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 498272
    },
    {
      "epoch": 0.0018084441112291843,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6916,
      "step": 498304
    },
    {
      "epoch": 0.001808560245580021,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 498336
    },
    {
      "epoch": 0.0018086763799308578,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 498368
    },
    {
      "epoch": 0.0018087925142816943,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7019,
      "step": 498400
    },
    {
      "epoch": 0.001808908648632531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 498432
    },
    {
      "epoch": 0.0018090247829833678,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 498464
    },
    {
      "epoch": 0.0018091409173342046,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 498496
    },
    {
      "epoch": 0.0018092570516850411,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7083,
      "step": 498528
    },
    {
      "epoch": 0.0018093731860358779,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6987,
      "step": 498560
    },
    {
      "epoch": 0.0018094893203867146,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 498592
    },
    {
      "epoch": 0.0018096054547375514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 498624
    },
    {
      "epoch": 0.0018097215890883881,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 498656
    },
    {
      "epoch": 0.0018098377234392247,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 498688
    },
    {
      "epoch": 0.0018099538577900614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7483,
      "step": 498720
    },
    {
      "epoch": 0.0018100699921408982,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 498752
    },
    {
      "epoch": 0.001810186126491735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 498784
    },
    {
      "epoch": 0.0018103022608425715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6928,
      "step": 498816
    },
    {
      "epoch": 0.0018104183951934082,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 498848
    },
    {
      "epoch": 0.001810534529544245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 498880
    },
    {
      "epoch": 0.0018106506638950817,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7465,
      "step": 498912
    },
    {
      "epoch": 0.0018107667982459185,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 498944
    },
    {
      "epoch": 0.001810882932596755,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.743,
      "step": 498976
    },
    {
      "epoch": 0.0018109990669475918,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 499008
    },
    {
      "epoch": 0.0018111152012984285,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 499040
    },
    {
      "epoch": 0.0018112313356492653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6982,
      "step": 499072
    },
    {
      "epoch": 0.0018113474700001018,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6908,
      "step": 499104
    },
    {
      "epoch": 0.0018114636043509386,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 499136
    },
    {
      "epoch": 0.0018115797387017753,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 499168
    },
    {
      "epoch": 0.001811695873052612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 499200
    },
    {
      "epoch": 0.0018118120074034488,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.745,
      "step": 499232
    },
    {
      "epoch": 0.0018119281417542854,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 499264
    },
    {
      "epoch": 0.0018120442761051221,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 499296
    },
    {
      "epoch": 0.0018121604104559589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.733,
      "step": 499328
    },
    {
      "epoch": 0.0018122765448067956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 499360
    },
    {
      "epoch": 0.0018123926791576322,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7032,
      "step": 499392
    },
    {
      "epoch": 0.001812508813508469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 499424
    },
    {
      "epoch": 0.0018126249478593057,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 499456
    },
    {
      "epoch": 0.0018127410822101424,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 499488
    },
    {
      "epoch": 0.0018128572165609792,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7479,
      "step": 499520
    },
    {
      "epoch": 0.0018129733509118157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 499552
    },
    {
      "epoch": 0.0018130894852626525,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7502,
      "step": 499584
    },
    {
      "epoch": 0.0018132056196134892,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 499616
    },
    {
      "epoch": 0.001813321753964326,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 499648
    },
    {
      "epoch": 0.0018134378883151625,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.74,
      "step": 499680
    },
    {
      "epoch": 0.0018135540226659993,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 499712
    },
    {
      "epoch": 0.001813670157016836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.734,
      "step": 499744
    },
    {
      "epoch": 0.0018137862913676728,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7657,
      "step": 499776
    },
    {
      "epoch": 0.0018139024257185095,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7429,
      "step": 499808
    },
    {
      "epoch": 0.001814018560069346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 499840
    },
    {
      "epoch": 0.0018141346944201828,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7443,
      "step": 499872
    },
    {
      "epoch": 0.0018142508287710196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7147,
      "step": 499904
    },
    {
      "epoch": 0.0018143669631218563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 499936
    },
    {
      "epoch": 0.0018144830974726929,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 499968
    },
    {
      "epoch": 0.0018145992318235296,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7408,
      "step": 500000
    },
    {
      "epoch": 0.0018147153661743664,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7204,
      "step": 500032
    },
    {
      "epoch": 0.0018148315005252031,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7423,
      "step": 500064
    },
    {
      "epoch": 0.00181494763487604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7375,
      "step": 500096
    },
    {
      "epoch": 0.0018150637692268764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7126,
      "step": 500128
    },
    {
      "epoch": 0.0018151799035777132,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 500160
    },
    {
      "epoch": 0.00181529603792855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7587,
      "step": 500192
    },
    {
      "epoch": 0.0018154121722793867,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7501,
      "step": 500224
    },
    {
      "epoch": 0.0018155283066302232,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7186,
      "step": 500256
    },
    {
      "epoch": 0.00181564444098106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 500288
    },
    {
      "epoch": 0.0018157605753318967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 500320
    },
    {
      "epoch": 0.0018158767096827335,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 500352
    },
    {
      "epoch": 0.0018159928440335702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 500384
    },
    {
      "epoch": 0.0018161089783844068,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 500416
    },
    {
      "epoch": 0.0018162251127352435,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.749,
      "step": 500448
    },
    {
      "epoch": 0.0018163412470860803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 500480
    },
    {
      "epoch": 0.001816457381436917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.764,
      "step": 500512
    },
    {
      "epoch": 0.0018165735157877536,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 500544
    },
    {
      "epoch": 0.0018166896501385903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 500576
    },
    {
      "epoch": 0.001816805784489427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 500608
    },
    {
      "epoch": 0.0018169219188402638,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 500640
    },
    {
      "epoch": 0.0018170380531911006,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7397,
      "step": 500672
    },
    {
      "epoch": 0.0018171541875419371,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 500704
    },
    {
      "epoch": 0.0018172703218927739,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 500736
    },
    {
      "epoch": 0.0018173864562436106,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 500768
    },
    {
      "epoch": 0.0018175025905944474,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 500800
    },
    {
      "epoch": 0.001817618724945284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6879,
      "step": 500832
    },
    {
      "epoch": 0.0018177348592961207,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 500864
    },
    {
      "epoch": 0.0018178509936469574,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 500896
    },
    {
      "epoch": 0.0018179671279977942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 500928
    },
    {
      "epoch": 0.001818083262348631,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 500960
    },
    {
      "epoch": 0.0018181993966994675,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7352,
      "step": 500992
    },
    {
      "epoch": 0.0018183155310503042,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 501024
    },
    {
      "epoch": 0.001818431665401141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7356,
      "step": 501056
    },
    {
      "epoch": 0.0018185477997519778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 501088
    },
    {
      "epoch": 0.0018186639341028143,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 501120
    },
    {
      "epoch": 0.001818780068453651,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 501152
    },
    {
      "epoch": 0.0018188962028044878,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 501184
    },
    {
      "epoch": 0.0018190123371553246,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 501216
    },
    {
      "epoch": 0.0018191284715061613,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.701,
      "step": 501248
    },
    {
      "epoch": 0.0018192446058569978,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 501280
    },
    {
      "epoch": 0.0018193607402078346,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 501312
    },
    {
      "epoch": 0.0018194768745586714,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7154,
      "step": 501344
    },
    {
      "epoch": 0.001819593008909508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 501376
    },
    {
      "epoch": 0.0018197091432603446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 501408
    },
    {
      "epoch": 0.0018198252776111814,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7456,
      "step": 501440
    },
    {
      "epoch": 0.0018199414119620182,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7299,
      "step": 501472
    },
    {
      "epoch": 0.001820057546312855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 501504
    },
    {
      "epoch": 0.0018201736806636917,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 501536
    },
    {
      "epoch": 0.0018202898150145282,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7148,
      "step": 501568
    },
    {
      "epoch": 0.001820405949365365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 501600
    },
    {
      "epoch": 0.0018205220837162017,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 501632
    },
    {
      "epoch": 0.0018206382180670385,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 501664
    },
    {
      "epoch": 0.001820754352417875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 501696
    },
    {
      "epoch": 0.0018208704867687118,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 501728
    },
    {
      "epoch": 0.0018209866211195485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 501760
    },
    {
      "epoch": 0.0018211027554703853,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7212,
      "step": 501792
    },
    {
      "epoch": 0.001821218889821222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 501824
    },
    {
      "epoch": 0.0018213350241720586,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 501856
    },
    {
      "epoch": 0.0018214511585228953,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 501888
    },
    {
      "epoch": 0.001821567292873732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 501920
    },
    {
      "epoch": 0.0018216834272245688,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7618,
      "step": 501952
    },
    {
      "epoch": 0.0018217995615754053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 501984
    },
    {
      "epoch": 0.001821915695926242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 502016
    },
    {
      "epoch": 0.0018220318302770789,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 502048
    },
    {
      "epoch": 0.0018221479646279156,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 502080
    },
    {
      "epoch": 0.0018222640989787524,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 502112
    },
    {
      "epoch": 0.001822380233329589,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 502144
    },
    {
      "epoch": 0.0018224963676804257,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 502176
    },
    {
      "epoch": 0.0018226125020312624,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7485,
      "step": 502208
    },
    {
      "epoch": 0.0018227286363820992,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 502240
    },
    {
      "epoch": 0.0018228447707329357,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7132,
      "step": 502272
    },
    {
      "epoch": 0.0018229609050837725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 502304
    },
    {
      "epoch": 0.0018230770394346092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6985,
      "step": 502336
    },
    {
      "epoch": 0.001823193173785446,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7501,
      "step": 502368
    },
    {
      "epoch": 0.0018233093081362827,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7589,
      "step": 502400
    },
    {
      "epoch": 0.0018234254424871193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 502432
    },
    {
      "epoch": 0.001823541576837956,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 502464
    },
    {
      "epoch": 0.0018236577111887928,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 502496
    },
    {
      "epoch": 0.0018237738455396295,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 502528
    },
    {
      "epoch": 0.001823889979890466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 502560
    },
    {
      "epoch": 0.0018240061142413028,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 502592
    },
    {
      "epoch": 0.0018241222485921396,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 502624
    },
    {
      "epoch": 0.0018242383829429763,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 502656
    },
    {
      "epoch": 0.001824354517293813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 502688
    },
    {
      "epoch": 0.0018244706516446496,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 502720
    },
    {
      "epoch": 0.0018245867859954864,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 502752
    },
    {
      "epoch": 0.0018247029203463231,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 502784
    },
    {
      "epoch": 0.0018248190546971599,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 502816
    },
    {
      "epoch": 0.0018249351890479964,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 502848
    },
    {
      "epoch": 0.0018250513233988332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7042,
      "step": 502880
    },
    {
      "epoch": 0.00182516745774967,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 502912
    },
    {
      "epoch": 0.0018252835921005067,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 502944
    },
    {
      "epoch": 0.0018253997264513434,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 502976
    },
    {
      "epoch": 0.00182551586080218,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7302,
      "step": 503008
    },
    {
      "epoch": 0.0018256319951530167,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7517,
      "step": 503040
    },
    {
      "epoch": 0.0018257481295038535,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7719,
      "step": 503072
    },
    {
      "epoch": 0.0018258642638546902,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7472,
      "step": 503104
    },
    {
      "epoch": 0.0018259803982055268,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 503136
    },
    {
      "epoch": 0.0018260965325563635,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6983,
      "step": 503168
    },
    {
      "epoch": 0.0018262126669072003,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7041,
      "step": 503200
    },
    {
      "epoch": 0.001826328801258037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 503232
    },
    {
      "epoch": 0.0018264449356088738,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 503264
    },
    {
      "epoch": 0.0018265610699597103,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7467,
      "step": 503296
    },
    {
      "epoch": 0.001826677204310547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7103,
      "step": 503328
    },
    {
      "epoch": 0.0018267933386613838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7055,
      "step": 503360
    },
    {
      "epoch": 0.0018269094730122206,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7082,
      "step": 503392
    },
    {
      "epoch": 0.0018270256073630571,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 503424
    },
    {
      "epoch": 0.0018271417417138939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 503456
    },
    {
      "epoch": 0.0018272578760647306,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6935,
      "step": 503488
    },
    {
      "epoch": 0.0018273740104155674,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 503520
    },
    {
      "epoch": 0.0018274901447664041,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 503552
    },
    {
      "epoch": 0.0018276062791172407,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 503584
    },
    {
      "epoch": 0.0018277224134680774,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7289,
      "step": 503616
    },
    {
      "epoch": 0.0018278385478189142,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 503648
    },
    {
      "epoch": 0.001827954682169751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 503680
    },
    {
      "epoch": 0.0018280708165205875,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 503712
    },
    {
      "epoch": 0.0018281869508714242,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 503744
    },
    {
      "epoch": 0.001828303085222261,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 503776
    },
    {
      "epoch": 0.0018284192195730977,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 503808
    },
    {
      "epoch": 0.0018285353539239345,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 503840
    },
    {
      "epoch": 0.001828651488274771,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 503872
    },
    {
      "epoch": 0.0018287676226256078,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 503904
    },
    {
      "epoch": 0.0018288837569764445,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 503936
    },
    {
      "epoch": 0.0018289998913272813,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7545,
      "step": 503968
    },
    {
      "epoch": 0.0018291160256781178,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 504000
    },
    {
      "epoch": 0.0018292321600289546,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 504032
    },
    {
      "epoch": 0.0018293482943797913,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 504064
    },
    {
      "epoch": 0.001829464428730628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 504096
    },
    {
      "epoch": 0.0018295805630814648,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 504128
    },
    {
      "epoch": 0.0018296966974323014,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.761,
      "step": 504160
    },
    {
      "epoch": 0.0018298128317831381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 504192
    },
    {
      "epoch": 0.0018299289661339749,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 504224
    },
    {
      "epoch": 0.0018300451004848116,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7292,
      "step": 504256
    },
    {
      "epoch": 0.0018301612348356482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 504288
    },
    {
      "epoch": 0.001830277369186485,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7239,
      "step": 504320
    },
    {
      "epoch": 0.0018303935035373217,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7057,
      "step": 504352
    },
    {
      "epoch": 0.0018305096378881584,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 504384
    },
    {
      "epoch": 0.0018306257722389952,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7172,
      "step": 504416
    },
    {
      "epoch": 0.0018307419065898317,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 504448
    },
    {
      "epoch": 0.0018308580409406685,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 504480
    },
    {
      "epoch": 0.0018309741752915052,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 504512
    },
    {
      "epoch": 0.001831090309642342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7584,
      "step": 504544
    },
    {
      "epoch": 0.0018312064439931785,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7474,
      "step": 504576
    },
    {
      "epoch": 0.0018313225783440153,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 504608
    },
    {
      "epoch": 0.001831438712694852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7205,
      "step": 504640
    },
    {
      "epoch": 0.0018315548470456888,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 504672
    },
    {
      "epoch": 0.0018316709813965255,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 504704
    },
    {
      "epoch": 0.001831787115747362,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 504736
    },
    {
      "epoch": 0.0018319032500981988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 504768
    },
    {
      "epoch": 0.0018320193844490356,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7456,
      "step": 504800
    },
    {
      "epoch": 0.0018321355187998723,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7395,
      "step": 504832
    },
    {
      "epoch": 0.0018322516531507089,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7225,
      "step": 504864
    },
    {
      "epoch": 0.0018323677875015456,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 504896
    },
    {
      "epoch": 0.0018324839218523824,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7411,
      "step": 504928
    },
    {
      "epoch": 0.0018326000562032191,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 504960
    },
    {
      "epoch": 0.001832716190554056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 504992
    },
    {
      "epoch": 0.0018328323249048924,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7568,
      "step": 505024
    },
    {
      "epoch": 0.0018329484592557292,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 505056
    },
    {
      "epoch": 0.001833064593606566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 505088
    },
    {
      "epoch": 0.0018331807279574027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 505120
    },
    {
      "epoch": 0.0018332968623082392,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 505152
    },
    {
      "epoch": 0.001833412996659076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 505184
    },
    {
      "epoch": 0.0018335291310099127,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7617,
      "step": 505216
    },
    {
      "epoch": 0.0018336452653607495,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 505248
    },
    {
      "epoch": 0.0018337613997115863,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 505280
    },
    {
      "epoch": 0.0018338775340624228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 505312
    },
    {
      "epoch": 0.0018339936684132595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 505344
    },
    {
      "epoch": 0.0018341098027640963,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 505376
    },
    {
      "epoch": 0.001834225937114933,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 505408
    },
    {
      "epoch": 0.0018343420714657696,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7501,
      "step": 505440
    },
    {
      "epoch": 0.0018344582058166063,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7466,
      "step": 505472
    },
    {
      "epoch": 0.001834574340167443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6992,
      "step": 505504
    },
    {
      "epoch": 0.0018346904745182799,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7179,
      "step": 505536
    },
    {
      "epoch": 0.0018348066088691166,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7221,
      "step": 505568
    },
    {
      "epoch": 0.0018349227432199531,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 505600
    },
    {
      "epoch": 0.00183503887757079,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 505632
    },
    {
      "epoch": 0.0018351550119216267,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7117,
      "step": 505664
    },
    {
      "epoch": 0.0018352711462724634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 505696
    },
    {
      "epoch": 0.0018353872806233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 505728
    },
    {
      "epoch": 0.0018355034149741367,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7485,
      "step": 505760
    },
    {
      "epoch": 0.0018356195493249735,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 505792
    },
    {
      "epoch": 0.0018357356836758102,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7124,
      "step": 505824
    },
    {
      "epoch": 0.001835851818026647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 505856
    },
    {
      "epoch": 0.0018359679523774835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 505888
    },
    {
      "epoch": 0.0018360840867283203,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7566,
      "step": 505920
    },
    {
      "epoch": 0.001836200221079157,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 505952
    },
    {
      "epoch": 0.0018363163554299938,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 505984
    },
    {
      "epoch": 0.0018364324897808303,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7363,
      "step": 506016
    },
    {
      "epoch": 0.001836548624131667,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 506048
    },
    {
      "epoch": 0.0018366647584825038,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 506080
    },
    {
      "epoch": 0.0018367808928333406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7072,
      "step": 506112
    },
    {
      "epoch": 0.0018368970271841773,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 506144
    },
    {
      "epoch": 0.0018370131615350139,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 506176
    },
    {
      "epoch": 0.0018371292958858506,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 506208
    },
    {
      "epoch": 0.0018372454302366874,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 506240
    },
    {
      "epoch": 0.0018373615645875241,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 506272
    },
    {
      "epoch": 0.0018374776989383607,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 506304
    },
    {
      "epoch": 0.0018375938332891974,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 506336
    },
    {
      "epoch": 0.0018377099676400342,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 506368
    },
    {
      "epoch": 0.001837826101990871,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 506400
    },
    {
      "epoch": 0.0018379422363417075,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 506432
    },
    {
      "epoch": 0.0018380583706925442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 506464
    },
    {
      "epoch": 0.001838174505043381,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 506496
    },
    {
      "epoch": 0.0018382906393942177,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 506528
    },
    {
      "epoch": 0.0018384067737450545,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7478,
      "step": 506560
    },
    {
      "epoch": 0.001838522908095891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7316,
      "step": 506592
    },
    {
      "epoch": 0.0018386390424467278,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7056,
      "step": 506624
    },
    {
      "epoch": 0.0018387551767975645,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7151,
      "step": 506656
    },
    {
      "epoch": 0.0018388713111484013,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 506688
    },
    {
      "epoch": 0.0018389874454992378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 506720
    },
    {
      "epoch": 0.0018391035798500746,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 506752
    },
    {
      "epoch": 0.0018392197142009113,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.741,
      "step": 506784
    },
    {
      "epoch": 0.001839335848551748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 506816
    },
    {
      "epoch": 0.0018394519829025848,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 506848
    },
    {
      "epoch": 0.0018395681172534214,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 506880
    },
    {
      "epoch": 0.0018396842516042581,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 506912
    },
    {
      "epoch": 0.0018398003859550949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6874,
      "step": 506944
    },
    {
      "epoch": 0.0018399165203059316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7136,
      "step": 506976
    },
    {
      "epoch": 0.0018400326546567682,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7105,
      "step": 507008
    },
    {
      "epoch": 0.001840148789007605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7278,
      "step": 507040
    },
    {
      "epoch": 0.0018402649233584417,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7525,
      "step": 507072
    },
    {
      "epoch": 0.0018403810577092784,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7451,
      "step": 507104
    },
    {
      "epoch": 0.0018404971920601152,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7016,
      "step": 507136
    },
    {
      "epoch": 0.0018406133264109517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 507168
    },
    {
      "epoch": 0.0018407294607617885,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 507200
    },
    {
      "epoch": 0.0018408455951126252,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7385,
      "step": 507232
    },
    {
      "epoch": 0.001840961729463462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 507264
    },
    {
      "epoch": 0.0018410778638142985,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7371,
      "step": 507296
    },
    {
      "epoch": 0.0018411939981651353,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7426,
      "step": 507328
    },
    {
      "epoch": 0.001841310132515972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6988,
      "step": 507360
    },
    {
      "epoch": 0.0018414262668668088,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 507392
    },
    {
      "epoch": 0.0018415424012176455,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 507424
    },
    {
      "epoch": 0.001841658535568482,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 507456
    },
    {
      "epoch": 0.0018417746699193188,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 507488
    },
    {
      "epoch": 0.0018418908042701556,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 507520
    },
    {
      "epoch": 0.0018420069386209923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 507552
    },
    {
      "epoch": 0.0018421230729718289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 507584
    },
    {
      "epoch": 0.0018422392073226656,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 507616
    },
    {
      "epoch": 0.0018423553416735024,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7457,
      "step": 507648
    },
    {
      "epoch": 0.0018424714760243391,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 507680
    },
    {
      "epoch": 0.0018425876103751759,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 507712
    },
    {
      "epoch": 0.0018427037447260124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 507744
    },
    {
      "epoch": 0.0018428198790768492,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 507776
    },
    {
      "epoch": 0.001842936013427686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 507808
    },
    {
      "epoch": 0.0018430521477785227,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 507840
    },
    {
      "epoch": 0.0018431682821293592,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7163,
      "step": 507872
    },
    {
      "epoch": 0.001843284416480196,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 507904
    },
    {
      "epoch": 0.0018434005508310327,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7493,
      "step": 507936
    },
    {
      "epoch": 0.0018435166851818695,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7152,
      "step": 507968
    },
    {
      "epoch": 0.0018436328195327062,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7498,
      "step": 508000
    },
    {
      "epoch": 0.0018437489538835428,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7342,
      "step": 508032
    },
    {
      "epoch": 0.0018438650882343795,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7403,
      "step": 508064
    },
    {
      "epoch": 0.0018439812225852163,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 508096
    },
    {
      "epoch": 0.001844097356936053,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 508128
    },
    {
      "epoch": 0.0018442134912868896,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7063,
      "step": 508160
    },
    {
      "epoch": 0.0018443296256377263,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 508192
    },
    {
      "epoch": 0.001844445759988563,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7115,
      "step": 508224
    },
    {
      "epoch": 0.0018445618943393998,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 508256
    },
    {
      "epoch": 0.0018446780286902366,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 508288
    },
    {
      "epoch": 0.0018447941630410731,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 508320
    },
    {
      "epoch": 0.0018449102973919099,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 508352
    },
    {
      "epoch": 0.0018450264317427466,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7092,
      "step": 508384
    },
    {
      "epoch": 0.0018451425660935834,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7005,
      "step": 508416
    },
    {
      "epoch": 0.00184525870044442,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 508448
    },
    {
      "epoch": 0.0018453748347952567,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 508480
    },
    {
      "epoch": 0.0018454909691460934,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.723,
      "step": 508512
    },
    {
      "epoch": 0.0018456071034969302,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 508544
    },
    {
      "epoch": 0.001845723237847767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 508576
    },
    {
      "epoch": 0.0018458393721986035,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7076,
      "step": 508608
    },
    {
      "epoch": 0.0018459555065494402,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7275,
      "step": 508640
    },
    {
      "epoch": 0.001846071640900277,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7066,
      "step": 508672
    },
    {
      "epoch": 0.0018461877752511137,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 508704
    },
    {
      "epoch": 0.0018463039096019503,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7036,
      "step": 508736
    },
    {
      "epoch": 0.001846420043952787,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 508768
    },
    {
      "epoch": 0.0018465361783036238,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7112,
      "step": 508800
    },
    {
      "epoch": 0.0018466523126544605,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7344,
      "step": 508832
    },
    {
      "epoch": 0.0018467684470052973,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 508864
    },
    {
      "epoch": 0.0018468845813561338,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 508896
    },
    {
      "epoch": 0.0018470007157069706,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 508928
    },
    {
      "epoch": 0.0018471168500578073,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 508960
    },
    {
      "epoch": 0.001847232984408644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7462,
      "step": 508992
    },
    {
      "epoch": 0.0018473491187594806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7096,
      "step": 509024
    },
    {
      "epoch": 0.0018474652531103174,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.726,
      "step": 509056
    },
    {
      "epoch": 0.0018475813874611541,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 509088
    },
    {
      "epoch": 0.001847697521811991,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 509120
    },
    {
      "epoch": 0.0018478136561628276,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 509152
    },
    {
      "epoch": 0.0018479297905136642,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 509184
    },
    {
      "epoch": 0.001848045924864501,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7492,
      "step": 509216
    },
    {
      "epoch": 0.0018481620592153377,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.75,
      "step": 509248
    },
    {
      "epoch": 0.0018482781935661744,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7336,
      "step": 509280
    },
    {
      "epoch": 0.001848394327917011,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 509312
    },
    {
      "epoch": 0.0018485104622678477,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 509344
    },
    {
      "epoch": 0.0018486265966186845,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 509376
    },
    {
      "epoch": 0.0018487427309695212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7498,
      "step": 509408
    },
    {
      "epoch": 0.001848858865320358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7417,
      "step": 509440
    },
    {
      "epoch": 0.0018489749996711945,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 509472
    },
    {
      "epoch": 0.0018490911340220313,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 509504
    },
    {
      "epoch": 0.001849207268372868,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 509536
    },
    {
      "epoch": 0.0018493234027237048,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7351,
      "step": 509568
    },
    {
      "epoch": 0.0018494395370745413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 509600
    },
    {
      "epoch": 0.001849555671425378,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.712,
      "step": 509632
    },
    {
      "epoch": 0.0018496718057762148,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 509664
    },
    {
      "epoch": 0.0018497879401270516,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 509696
    },
    {
      "epoch": 0.0018499040744778884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7541,
      "step": 509728
    },
    {
      "epoch": 0.001850020208828725,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 509760
    },
    {
      "epoch": 0.0018501363431795616,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7461,
      "step": 509792
    },
    {
      "epoch": 0.0018502524775303984,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 509824
    },
    {
      "epoch": 0.0018503686118812352,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 509856
    },
    {
      "epoch": 0.0018504847462320717,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 509888
    },
    {
      "epoch": 0.0018506008805829084,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7052,
      "step": 509920
    },
    {
      "epoch": 0.0018507170149337452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7428,
      "step": 509952
    },
    {
      "epoch": 0.001850833149284582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 509984
    },
    {
      "epoch": 0.0018509492836354187,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 510016
    },
    {
      "epoch": 0.0018510654179862552,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 510048
    },
    {
      "epoch": 0.001851181552337092,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7709,
      "step": 510080
    },
    {
      "epoch": 0.0018512976866879288,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 510112
    },
    {
      "epoch": 0.0018514138210387655,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7458,
      "step": 510144
    },
    {
      "epoch": 0.001851529955389602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7173,
      "step": 510176
    },
    {
      "epoch": 0.0018516460897404388,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 510208
    },
    {
      "epoch": 0.0018517622240912756,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 510240
    },
    {
      "epoch": 0.0018518783584421123,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.75,
      "step": 510272
    },
    {
      "epoch": 0.001851994492792949,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7581,
      "step": 510304
    },
    {
      "epoch": 0.0018521106271437856,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7572,
      "step": 510336
    },
    {
      "epoch": 0.0018522267614946224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7358,
      "step": 510368
    },
    {
      "epoch": 0.001852342895845459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7067,
      "step": 510400
    },
    {
      "epoch": 0.0018524590301962959,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7044,
      "step": 510432
    },
    {
      "epoch": 0.0018525751645471324,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 510464
    },
    {
      "epoch": 0.0018526912988979692,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 510496
    },
    {
      "epoch": 0.001852807433248806,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7119,
      "step": 510528
    },
    {
      "epoch": 0.0018529235675996427,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 510560
    },
    {
      "epoch": 0.0018530397019504794,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7313,
      "step": 510592
    },
    {
      "epoch": 0.001853155836301316,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 510624
    },
    {
      "epoch": 0.0018532719706521527,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 510656
    },
    {
      "epoch": 0.0018533881050029895,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 510688
    },
    {
      "epoch": 0.0018535042393538262,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 510720
    },
    {
      "epoch": 0.0018536203737046628,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 510752
    },
    {
      "epoch": 0.0018537365080554995,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7062,
      "step": 510784
    },
    {
      "epoch": 0.0018538526424063363,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 510816
    },
    {
      "epoch": 0.001853968776757173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 510848
    },
    {
      "epoch": 0.0018540849111080098,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7314,
      "step": 510880
    },
    {
      "epoch": 0.0018542010454588463,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6976,
      "step": 510912
    },
    {
      "epoch": 0.001854317179809683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7177,
      "step": 510944
    },
    {
      "epoch": 0.0018544333141605198,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 510976
    },
    {
      "epoch": 0.0018545494485113566,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 511008
    },
    {
      "epoch": 0.001854665582862193,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7231,
      "step": 511040
    },
    {
      "epoch": 0.0018547817172130299,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 511072
    },
    {
      "epoch": 0.0018548978515638666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7235,
      "step": 511104
    },
    {
      "epoch": 0.0018550139859147034,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 511136
    },
    {
      "epoch": 0.0018551301202655401,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7586,
      "step": 511168
    },
    {
      "epoch": 0.0018552462546163767,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 511200
    },
    {
      "epoch": 0.0018553623889672134,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7114,
      "step": 511232
    },
    {
      "epoch": 0.0018554785233180502,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7169,
      "step": 511264
    },
    {
      "epoch": 0.001855594657668887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 511296
    },
    {
      "epoch": 0.0018557107920197235,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.73,
      "step": 511328
    },
    {
      "epoch": 0.0018558269263705602,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 511360
    },
    {
      "epoch": 0.001855943060721397,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6981,
      "step": 511392
    },
    {
      "epoch": 0.0018560591950722337,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 511424
    },
    {
      "epoch": 0.0018561753294230705,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7174,
      "step": 511456
    },
    {
      "epoch": 0.001856291463773907,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 511488
    },
    {
      "epoch": 0.0018564075981247438,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7291,
      "step": 511520
    },
    {
      "epoch": 0.0018565237324755805,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 511552
    },
    {
      "epoch": 0.0018566398668264173,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7373,
      "step": 511584
    },
    {
      "epoch": 0.0018567560011772538,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7491,
      "step": 511616
    },
    {
      "epoch": 0.0018568721355280906,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 511648
    },
    {
      "epoch": 0.0018569882698789273,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 511680
    },
    {
      "epoch": 0.001857104404229764,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 511712
    },
    {
      "epoch": 0.0018572205385806008,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 511744
    },
    {
      "epoch": 0.0018573366729314374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 511776
    },
    {
      "epoch": 0.0018574528072822741,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 511808
    },
    {
      "epoch": 0.0018575689416331109,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 511840
    },
    {
      "epoch": 0.0018576850759839476,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 511872
    },
    {
      "epoch": 0.0018578012103347842,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 511904
    },
    {
      "epoch": 0.001857917344685621,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7097,
      "step": 511936
    },
    {
      "epoch": 0.0018580334790364577,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7338,
      "step": 511968
    },
    {
      "epoch": 0.0018581496133872944,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 512000
    },
    {
      "epoch": 0.0018582657477381312,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7455,
      "step": 512032
    },
    {
      "epoch": 0.0018583818820889677,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 512064
    },
    {
      "epoch": 0.0018584980164398045,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7287,
      "step": 512096
    },
    {
      "epoch": 0.0018586141507906412,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 512128
    },
    {
      "epoch": 0.001858730285141478,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7198,
      "step": 512160
    },
    {
      "epoch": 0.0018588464194923145,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7069,
      "step": 512192
    },
    {
      "epoch": 0.0018589625538431513,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7353,
      "step": 512224
    },
    {
      "epoch": 0.001859078688193988,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7587,
      "step": 512256
    },
    {
      "epoch": 0.0018591948225448248,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 512288
    },
    {
      "epoch": 0.0018593109568956615,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 512320
    },
    {
      "epoch": 0.001859427091246498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7178,
      "step": 512352
    },
    {
      "epoch": 0.0018595432255973348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 512384
    },
    {
      "epoch": 0.0018596593599481716,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7068,
      "step": 512416
    },
    {
      "epoch": 0.0018597754942990083,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7227,
      "step": 512448
    },
    {
      "epoch": 0.0018598916286498449,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7328,
      "step": 512480
    },
    {
      "epoch": 0.0018600077630006816,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 512512
    },
    {
      "epoch": 0.0018601238973515184,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6972,
      "step": 512544
    },
    {
      "epoch": 0.0018602400317023551,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7262,
      "step": 512576
    },
    {
      "epoch": 0.0018603561660531919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7196,
      "step": 512608
    },
    {
      "epoch": 0.0018604723004040284,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 512640
    },
    {
      "epoch": 0.0018605884347548652,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7166,
      "step": 512672
    },
    {
      "epoch": 0.001860704569105702,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 512704
    },
    {
      "epoch": 0.0018608207034565387,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7269,
      "step": 512736
    },
    {
      "epoch": 0.0018609368378073752,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7379,
      "step": 512768
    },
    {
      "epoch": 0.001861052972158212,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7529,
      "step": 512800
    },
    {
      "epoch": 0.0018611691065090487,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7343,
      "step": 512832
    },
    {
      "epoch": 0.0018612852408598855,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 512864
    },
    {
      "epoch": 0.0018614013752107222,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 512896
    },
    {
      "epoch": 0.0018615175095615588,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7585,
      "step": 512928
    },
    {
      "epoch": 0.0018616336439123955,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7414,
      "step": 512960
    },
    {
      "epoch": 0.0018617497782632323,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 512992
    },
    {
      "epoch": 0.001861865912614069,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7236,
      "step": 513024
    },
    {
      "epoch": 0.0018619820469649056,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7064,
      "step": 513056
    },
    {
      "epoch": 0.0018620981813157423,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6957,
      "step": 513088
    },
    {
      "epoch": 0.001862214315666579,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 513120
    },
    {
      "epoch": 0.0018623304500174158,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.702,
      "step": 513152
    },
    {
      "epoch": 0.0018624465843682526,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7121,
      "step": 513184
    },
    {
      "epoch": 0.0018625627187190891,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7106,
      "step": 513216
    },
    {
      "epoch": 0.0018626788530699259,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 513248
    },
    {
      "epoch": 0.0018627949874207626,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7162,
      "step": 513280
    },
    {
      "epoch": 0.0018629111217715994,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 513312
    },
    {
      "epoch": 0.001863027256122436,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7372,
      "step": 513344
    },
    {
      "epoch": 0.0018631433904732727,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7203,
      "step": 513376
    },
    {
      "epoch": 0.0018632595248241094,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7007,
      "step": 513408
    },
    {
      "epoch": 0.0018633756591749462,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6998,
      "step": 513440
    },
    {
      "epoch": 0.001863491793525783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7088,
      "step": 513472
    },
    {
      "epoch": 0.0018636079278766195,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 513504
    },
    {
      "epoch": 0.0018637240622274562,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7246,
      "step": 513536
    },
    {
      "epoch": 0.001863840196578293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 513568
    },
    {
      "epoch": 0.0018639563309291297,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 513600
    },
    {
      "epoch": 0.0018640724652799663,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 513632
    },
    {
      "epoch": 0.001864188599630803,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 513664
    },
    {
      "epoch": 0.0018643047339816398,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.711,
      "step": 513696
    },
    {
      "epoch": 0.0018644208683324765,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7176,
      "step": 513728
    },
    {
      "epoch": 0.0018645370026833133,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7393,
      "step": 513760
    },
    {
      "epoch": 0.0018646531370341498,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7562,
      "step": 513792
    },
    {
      "epoch": 0.0018647692713849866,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7531,
      "step": 513824
    },
    {
      "epoch": 0.0018648854057358233,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7398,
      "step": 513856
    },
    {
      "epoch": 0.00186500154008666,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 513888
    },
    {
      "epoch": 0.0018651176744374966,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.707,
      "step": 513920
    },
    {
      "epoch": 0.0018652338087883334,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 513952
    },
    {
      "epoch": 0.0018653499431391701,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6961,
      "step": 513984
    },
    {
      "epoch": 0.001865466077490007,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 514016
    },
    {
      "epoch": 0.0018655822118408437,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7257,
      "step": 514048
    },
    {
      "epoch": 0.0018656983461916802,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7267,
      "step": 514080
    },
    {
      "epoch": 0.001865814480542517,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7488,
      "step": 514112
    },
    {
      "epoch": 0.0018659306148933537,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 514144
    },
    {
      "epoch": 0.0018660467492441905,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 514176
    },
    {
      "epoch": 0.001866162883595027,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7415,
      "step": 514208
    },
    {
      "epoch": 0.0018662790179458637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 514240
    },
    {
      "epoch": 0.0018663951522967005,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7445,
      "step": 514272
    },
    {
      "epoch": 0.0018665112866475373,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 514304
    },
    {
      "epoch": 0.001866627420998374,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 514336
    },
    {
      "epoch": 0.0018667435553492105,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7335,
      "step": 514368
    },
    {
      "epoch": 0.0018668596897000473,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7496,
      "step": 514400
    },
    {
      "epoch": 0.001866975824050884,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7135,
      "step": 514432
    },
    {
      "epoch": 0.0018670919584017208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7357,
      "step": 514464
    },
    {
      "epoch": 0.0018672080927525573,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 514496
    },
    {
      "epoch": 0.001867324227103394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 514528
    },
    {
      "epoch": 0.0018674403614542309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 514560
    },
    {
      "epoch": 0.0018675564958050676,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 514592
    },
    {
      "epoch": 0.0018676726301559044,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.758,
      "step": 514624
    },
    {
      "epoch": 0.001867788764506741,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7657,
      "step": 514656
    },
    {
      "epoch": 0.0018679048988575777,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7471,
      "step": 514688
    },
    {
      "epoch": 0.0018680210332084144,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 514720
    },
    {
      "epoch": 0.0018681371675592512,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 514752
    },
    {
      "epoch": 0.0018682533019100877,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7213,
      "step": 514784
    },
    {
      "epoch": 0.0018683694362609245,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 514816
    },
    {
      "epoch": 0.0018684855706117612,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7265,
      "step": 514848
    },
    {
      "epoch": 0.001868601704962598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.727,
      "step": 514880
    },
    {
      "epoch": 0.0018687178393134347,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 514912
    },
    {
      "epoch": 0.0018688339736642713,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7242,
      "step": 514944
    },
    {
      "epoch": 0.001868950108015108,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7229,
      "step": 514976
    },
    {
      "epoch": 0.0018690662423659448,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7266,
      "step": 515008
    },
    {
      "epoch": 0.0018691823767167815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 515040
    },
    {
      "epoch": 0.001869298511067618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.739,
      "step": 515072
    },
    {
      "epoch": 0.0018694146454184548,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7477,
      "step": 515104
    },
    {
      "epoch": 0.0018695307797692916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7497,
      "step": 515136
    },
    {
      "epoch": 0.0018696469141201283,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7079,
      "step": 515168
    },
    {
      "epoch": 0.001869763048470965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7364,
      "step": 515200
    },
    {
      "epoch": 0.0018698791828218016,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 515232
    },
    {
      "epoch": 0.0018699953171726384,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.746,
      "step": 515264
    },
    {
      "epoch": 0.0018701114515234751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7599,
      "step": 515296
    },
    {
      "epoch": 0.0018702275858743119,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 515328
    },
    {
      "epoch": 0.0018703437202251484,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.74,
      "step": 515360
    },
    {
      "epoch": 0.0018704598545759852,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 515392
    },
    {
      "epoch": 0.001870575988926822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 515424
    },
    {
      "epoch": 0.0018706921232776587,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7243,
      "step": 515456
    },
    {
      "epoch": 0.0018708082576284954,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 515488
    },
    {
      "epoch": 0.001870924391979332,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7284,
      "step": 515520
    },
    {
      "epoch": 0.0018710405263301687,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 515552
    },
    {
      "epoch": 0.0018711566606810055,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7224,
      "step": 515584
    },
    {
      "epoch": 0.0018712727950318422,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7128,
      "step": 515616
    },
    {
      "epoch": 0.0018713889293826788,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7233,
      "step": 515648
    },
    {
      "epoch": 0.0018715050637335155,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7256,
      "step": 515680
    },
    {
      "epoch": 0.0018716211980843523,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 515712
    },
    {
      "epoch": 0.001871737332435189,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 515744
    },
    {
      "epoch": 0.0018718534667860258,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 515776
    },
    {
      "epoch": 0.0018719696011368623,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7268,
      "step": 515808
    },
    {
      "epoch": 0.001872085735487699,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 515840
    },
    {
      "epoch": 0.0018722018698385358,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7219,
      "step": 515872
    },
    {
      "epoch": 0.0018723180041893726,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.732,
      "step": 515904
    },
    {
      "epoch": 0.0018724341385402091,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 515936
    },
    {
      "epoch": 0.0018725502728910459,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 515968
    },
    {
      "epoch": 0.0018726664072418826,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7424,
      "step": 516000
    },
    {
      "epoch": 0.0018727825415927194,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 516032
    },
    {
      "epoch": 0.0018728986759435561,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7046,
      "step": 516064
    },
    {
      "epoch": 0.0018730148102943927,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 516096
    },
    {
      "epoch": 0.0018731309446452294,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 516128
    },
    {
      "epoch": 0.0018732470789960662,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 516160
    },
    {
      "epoch": 0.001873363213346903,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 516192
    },
    {
      "epoch": 0.0018734793476977395,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7475,
      "step": 516224
    },
    {
      "epoch": 0.0018735954820485762,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7388,
      "step": 516256
    },
    {
      "epoch": 0.001873711616399413,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7628,
      "step": 516288
    },
    {
      "epoch": 0.0018738277507502497,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7078,
      "step": 516320
    },
    {
      "epoch": 0.0018739438851010865,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7102,
      "step": 516352
    },
    {
      "epoch": 0.001874060019451923,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7376,
      "step": 516384
    },
    {
      "epoch": 0.0018741761538027598,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7374,
      "step": 516416
    },
    {
      "epoch": 0.0018742922881535965,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.756,
      "step": 516448
    },
    {
      "epoch": 0.0018744084225044333,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.72,
      "step": 516480
    },
    {
      "epoch": 0.0018745245568552698,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7141,
      "step": 516512
    },
    {
      "epoch": 0.0018746406912061066,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 516544
    },
    {
      "epoch": 0.0018747568255569433,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7408,
      "step": 516576
    },
    {
      "epoch": 0.00187487295990778,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7206,
      "step": 516608
    },
    {
      "epoch": 0.0018749890942586168,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 516640
    },
    {
      "epoch": 0.0018751052286094534,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.704,
      "step": 516672
    },
    {
      "epoch": 0.0018752213629602901,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.708,
      "step": 516704
    },
    {
      "epoch": 0.0018753374973111269,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7244,
      "step": 516736
    },
    {
      "epoch": 0.0018754536316619636,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 516768
    },
    {
      "epoch": 0.0018755697660128002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 516800
    },
    {
      "epoch": 0.001875685900363637,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7366,
      "step": 516832
    },
    {
      "epoch": 0.0018758020347144737,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 516864
    },
    {
      "epoch": 0.0018759181690653104,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 516896
    },
    {
      "epoch": 0.0018760343034161472,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7143,
      "step": 516928
    },
    {
      "epoch": 0.0018761504377669837,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 516960
    },
    {
      "epoch": 0.0018762665721178205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 516992
    },
    {
      "epoch": 0.0018763827064686572,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 517024
    },
    {
      "epoch": 0.001876498840819494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 517056
    },
    {
      "epoch": 0.0018766149751703305,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 517088
    },
    {
      "epoch": 0.0018767311095211673,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7476,
      "step": 517120
    },
    {
      "epoch": 0.001876847243872004,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7303,
      "step": 517152
    },
    {
      "epoch": 0.0018769633782228408,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7464,
      "step": 517184
    },
    {
      "epoch": 0.0018770795125736775,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7043,
      "step": 517216
    },
    {
      "epoch": 0.001877195646924514,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7065,
      "step": 517248
    },
    {
      "epoch": 0.0018773117812753508,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7333,
      "step": 517280
    },
    {
      "epoch": 0.0018774279156261876,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7577,
      "step": 517312
    },
    {
      "epoch": 0.0018775440499770243,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7238,
      "step": 517344
    },
    {
      "epoch": 0.0018776601843278609,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7295,
      "step": 517376
    },
    {
      "epoch": 0.0018777763186786976,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 517408
    },
    {
      "epoch": 0.0018778924530295344,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7208,
      "step": 517440
    },
    {
      "epoch": 0.0018780085873803711,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7058,
      "step": 517472
    },
    {
      "epoch": 0.001878124721731208,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7038,
      "step": 517504
    },
    {
      "epoch": 0.0018782408560820444,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 517536
    },
    {
      "epoch": 0.0018783569904328812,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7122,
      "step": 517568
    },
    {
      "epoch": 0.001878473124783718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 517600
    },
    {
      "epoch": 0.0018785892591345547,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7399,
      "step": 517632
    },
    {
      "epoch": 0.0018787053934853912,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 517664
    },
    {
      "epoch": 0.001878821527836228,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7195,
      "step": 517696
    },
    {
      "epoch": 0.0018789376621870647,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7468,
      "step": 517728
    },
    {
      "epoch": 0.0018790537965379015,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7407,
      "step": 517760
    },
    {
      "epoch": 0.0018791699308887383,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7261,
      "step": 517792
    },
    {
      "epoch": 0.0018792860652395748,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 517824
    },
    {
      "epoch": 0.0018794021995904115,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 517856
    },
    {
      "epoch": 0.0018795183339412483,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7568,
      "step": 517888
    },
    {
      "epoch": 0.001879634468292085,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7341,
      "step": 517920
    },
    {
      "epoch": 0.0018797506026429216,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7027,
      "step": 517952
    },
    {
      "epoch": 0.0018798667369937583,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.718,
      "step": 517984
    },
    {
      "epoch": 0.001879982871344595,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 518016
    },
    {
      "epoch": 0.0018800990056954318,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 518048
    },
    {
      "epoch": 0.0018802151400462686,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7099,
      "step": 518080
    },
    {
      "epoch": 0.0018803312743971051,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7149,
      "step": 518112
    },
    {
      "epoch": 0.001880447408747942,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.717,
      "step": 518144
    },
    {
      "epoch": 0.0018805635430987786,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7488,
      "step": 518176
    },
    {
      "epoch": 0.0018806796774496154,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7446,
      "step": 518208
    },
    {
      "epoch": 0.001880795811800452,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 518240
    },
    {
      "epoch": 0.0018809119461512887,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6967,
      "step": 518272
    },
    {
      "epoch": 0.0018810280805021254,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7051,
      "step": 518304
    },
    {
      "epoch": 0.0018811442148529622,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 518336
    },
    {
      "epoch": 0.001881260349203799,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 518368
    },
    {
      "epoch": 0.0018813764835546355,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7061,
      "step": 518400
    },
    {
      "epoch": 0.0018814926179054722,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7161,
      "step": 518432
    },
    {
      "epoch": 0.001881608752256309,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7408,
      "step": 518464
    },
    {
      "epoch": 0.0018817248866071458,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7189,
      "step": 518496
    },
    {
      "epoch": 0.0018818410209579823,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 518528
    },
    {
      "epoch": 0.001881957155308819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 518560
    },
    {
      "epoch": 0.0018820732896596558,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7305,
      "step": 518592
    },
    {
      "epoch": 0.0018821894240104926,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 518624
    },
    {
      "epoch": 0.0018823055583613293,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7127,
      "step": 518656
    },
    {
      "epoch": 0.0018824216927121658,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7113,
      "step": 518688
    },
    {
      "epoch": 0.0018825378270630026,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 518720
    },
    {
      "epoch": 0.0018826539614138394,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7453,
      "step": 518752
    },
    {
      "epoch": 0.0018827700957646761,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.738,
      "step": 518784
    },
    {
      "epoch": 0.0018828862301155126,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6991,
      "step": 518816
    },
    {
      "epoch": 0.0018830023644663494,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7167,
      "step": 518848
    },
    {
      "epoch": 0.0018831184988171862,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7223,
      "step": 518880
    },
    {
      "epoch": 0.001883234633168023,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7281,
      "step": 518912
    },
    {
      "epoch": 0.0018833507675188597,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7274,
      "step": 518944
    },
    {
      "epoch": 0.0018834669018696962,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7272,
      "step": 518976
    },
    {
      "epoch": 0.001883583036220533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7337,
      "step": 519008
    },
    {
      "epoch": 0.0018836991705713697,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7329,
      "step": 519040
    },
    {
      "epoch": 0.0018838153049222065,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7531,
      "step": 519072
    },
    {
      "epoch": 0.001883931439273043,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7247,
      "step": 519104
    },
    {
      "epoch": 0.0018840475736238798,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 519136
    },
    {
      "epoch": 0.0018841637079747165,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 519168
    },
    {
      "epoch": 0.0018842798423255533,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7209,
      "step": 519200
    },
    {
      "epoch": 0.00188439597667639,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7199,
      "step": 519232
    },
    {
      "epoch": 0.0018845121110272266,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7183,
      "step": 519264
    },
    {
      "epoch": 0.0018846282453780633,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7346,
      "step": 519296
    },
    {
      "epoch": 0.0018847443797289,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7412,
      "step": 519328
    },
    {
      "epoch": 0.0018848605140797368,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 519360
    },
    {
      "epoch": 0.0018849766484305734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 519392
    },
    {
      "epoch": 0.00188509278278141,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 519424
    },
    {
      "epoch": 0.0018852089171322469,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7228,
      "step": 519456
    },
    {
      "epoch": 0.0018853250514830836,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7542,
      "step": 519488
    },
    {
      "epoch": 0.0018854411858339204,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7377,
      "step": 519520
    },
    {
      "epoch": 0.001885557320184757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7368,
      "step": 519552
    },
    {
      "epoch": 0.0018856734545355937,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7332,
      "step": 519584
    },
    {
      "epoch": 0.0018857895888864304,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7516,
      "step": 519616
    },
    {
      "epoch": 0.0018859057232372672,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 519648
    },
    {
      "epoch": 0.0018860218575881037,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7185,
      "step": 519680
    },
    {
      "epoch": 0.0018861379919389405,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.728,
      "step": 519712
    },
    {
      "epoch": 0.0018862541262897772,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7442,
      "step": 519744
    },
    {
      "epoch": 0.001886370260640614,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7197,
      "step": 519776
    },
    {
      "epoch": 0.0018864863949914507,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7507,
      "step": 519808
    },
    {
      "epoch": 0.0018866025293422873,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 519840
    },
    {
      "epoch": 0.001886718663693124,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 519872
    },
    {
      "epoch": 0.0018868347980439608,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7264,
      "step": 519904
    },
    {
      "epoch": 0.0018869509323947975,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7506,
      "step": 519936
    },
    {
      "epoch": 0.001887067066745634,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7391,
      "step": 519968
    },
    {
      "epoch": 0.0018871832010964708,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7271,
      "step": 520000
    },
    {
      "epoch": 0.0018872993354473076,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7327,
      "step": 520032
    },
    {
      "epoch": 0.0018874154697981443,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7468,
      "step": 520064
    },
    {
      "epoch": 0.001887531604148981,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7217,
      "step": 520096
    },
    {
      "epoch": 0.0018876477384998176,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7098,
      "step": 520128
    },
    {
      "epoch": 0.0018877638728506544,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7315,
      "step": 520160
    },
    {
      "epoch": 0.0018878800072014911,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7326,
      "step": 520192
    },
    {
      "epoch": 0.0018879961415523279,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7545,
      "step": 520224
    },
    {
      "epoch": 0.0018881122759031644,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7288,
      "step": 520256
    },
    {
      "epoch": 0.0018882284102540012,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7182,
      "step": 520288
    },
    {
      "epoch": 0.001888344544604838,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.714,
      "step": 520320
    },
    {
      "epoch": 0.0018884606789556747,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7348,
      "step": 520352
    },
    {
      "epoch": 0.0018885768133065114,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7382,
      "step": 520384
    },
    {
      "epoch": 0.001888692947657348,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7104,
      "step": 520416
    },
    {
      "epoch": 0.0018888090820081847,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.703,
      "step": 520448
    },
    {
      "epoch": 0.0018889252163590215,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7048,
      "step": 520480
    },
    {
      "epoch": 0.0018890413507098582,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7159,
      "step": 520512
    },
    {
      "epoch": 0.0018891574850606948,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7108,
      "step": 520544
    },
    {
      "epoch": 0.0018892736194115315,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7144,
      "step": 520576
    },
    {
      "epoch": 0.0018893897537623683,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7345,
      "step": 520608
    },
    {
      "epoch": 0.001889505888113205,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7255,
      "step": 520640
    },
    {
      "epoch": 0.0018896220224640418,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7188,
      "step": 520672
    },
    {
      "epoch": 0.0018897381568148783,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7215,
      "step": 520704
    },
    {
      "epoch": 0.001889854291165715,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7298,
      "step": 520736
    },
    {
      "epoch": 0.0018899704255165518,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7282,
      "step": 520768
    },
    {
      "epoch": 0.0018900865598673886,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7359,
      "step": 520800
    },
    {
      "epoch": 0.0018902026942182251,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7354,
      "step": 520832
    },
    {
      "epoch": 0.0018903188285690619,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7318,
      "step": 520864
    },
    {
      "epoch": 0.0018904349629198986,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7331,
      "step": 520896
    },
    {
      "epoch": 0.0018905510972707354,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7157,
      "step": 520928
    },
    {
      "epoch": 0.0018906672316215721,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7107,
      "step": 520960
    },
    {
      "epoch": 0.0018907833659724087,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6986,
      "step": 520992
    },
    {
      "epoch": 0.0018908995003232454,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7034,
      "step": 521024
    },
    {
      "epoch": 0.0018910156346740822,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7024,
      "step": 521056
    },
    {
      "epoch": 0.001891131769024919,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7145,
      "step": 521088
    },
    {
      "epoch": 0.0018912479033757555,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7218,
      "step": 521120
    },
    {
      "epoch": 0.0018913640377265922,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 521152
    },
    {
      "epoch": 0.001891480172077429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 521184
    },
    {
      "epoch": 0.0018915963064282657,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7355,
      "step": 521216
    },
    {
      "epoch": 0.0018917124407791025,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7381,
      "step": 521248
    },
    {
      "epoch": 0.001891828575129939,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7138,
      "step": 521280
    },
    {
      "epoch": 0.0018919447094807758,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.722,
      "step": 521312
    },
    {
      "epoch": 0.0018920608438316125,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7306,
      "step": 521344
    },
    {
      "epoch": 0.0018921769781824493,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7175,
      "step": 521376
    },
    {
      "epoch": 0.0018922931125332858,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 521408
    },
    {
      "epoch": 0.0018924092468841226,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7322,
      "step": 521440
    },
    {
      "epoch": 0.0018925253812349593,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7304,
      "step": 521472
    },
    {
      "epoch": 0.001892641515585796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7312,
      "step": 521504
    },
    {
      "epoch": 0.0018927576499366328,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7253,
      "step": 521536
    },
    {
      "epoch": 0.0018928737842874694,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 521568
    },
    {
      "epoch": 0.0018929899186383061,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7158,
      "step": 521600
    },
    {
      "epoch": 0.0018931060529891429,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7165,
      "step": 521632
    },
    {
      "epoch": 0.0018932221873399796,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7258,
      "step": 521664
    },
    {
      "epoch": 0.0018933383216908162,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7672,
      "step": 521696
    },
    {
      "epoch": 0.001893454456041653,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7075,
      "step": 521728
    },
    {
      "epoch": 0.0018935705903924897,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7049,
      "step": 521760
    },
    {
      "epoch": 0.0018936867247433264,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7168,
      "step": 521792
    },
    {
      "epoch": 0.0018938028590941632,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.719,
      "step": 521824
    },
    {
      "epoch": 0.0018939189934449997,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7301,
      "step": 521856
    },
    {
      "epoch": 0.0018940351277958365,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7202,
      "step": 521888
    },
    {
      "epoch": 0.0018941512621466732,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7111,
      "step": 521920
    },
    {
      "epoch": 0.00189426739649751,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7134,
      "step": 521952
    },
    {
      "epoch": 0.0018943835308483465,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7334,
      "step": 521984
    },
    {
      "epoch": 0.0018944996651991833,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7365,
      "step": 522016
    },
    {
      "epoch": 0.00189461579955002,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7087,
      "step": 522048
    },
    {
      "epoch": 0.0018947319339008568,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.724,
      "step": 522080
    },
    {
      "epoch": 0.0018948480682516936,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.762,
      "step": 522112
    },
    {
      "epoch": 0.00189496420260253,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7361,
      "step": 522144
    },
    {
      "epoch": 0.0018950803369533668,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.6995,
      "step": 522176
    },
    {
      "epoch": 0.0018951964713042036,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7214,
      "step": 522208
    },
    {
      "epoch": 0.0018953126056550404,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7476,
      "step": 522240
    },
    {
      "epoch": 0.0018954287400058769,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7216,
      "step": 522272
    },
    {
      "epoch": 0.0018955448743567136,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7033,
      "step": 522304
    },
    {
      "epoch": 0.0018956610087075504,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7192,
      "step": 522336
    },
    {
      "epoch": 0.0018957771430583872,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 522368
    },
    {
      "epoch": 0.001895893277409224,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7276,
      "step": 522400
    },
    {
      "epoch": 0.0018960094117600604,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7339,
      "step": 522432
    },
    {
      "epoch": 0.0018961255461108972,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7039,
      "step": 522464
    },
    {
      "epoch": 0.001896241680461734,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7193,
      "step": 522496
    },
    {
      "epoch": 0.0018963578148125707,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.743,
      "step": 522528
    },
    {
      "epoch": 0.0018964739491634072,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7378,
      "step": 522560
    },
    {
      "epoch": 0.001896590083514244,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7321,
      "step": 522592
    },
    {
      "epoch": 0.0018967062178650807,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7191,
      "step": 522624
    },
    {
      "epoch": 0.0018968223522159175,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7237,
      "step": 522656
    },
    {
      "epoch": 0.0018969384865667543,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 522688
    },
    {
      "epoch": 0.0018970546209175908,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7392,
      "step": 522720
    },
    {
      "epoch": 0.0018971707552684275,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7129,
      "step": 522752
    },
    {
      "epoch": 0.0018972868896192643,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.742,
      "step": 522784
    },
    {
      "epoch": 0.001897403023970101,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7413,
      "step": 522816
    },
    {
      "epoch": 0.0018975191583209376,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7433,
      "step": 522848
    },
    {
      "epoch": 0.0018976352926717743,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7081,
      "step": 522880
    },
    {
      "epoch": 0.001897751427022611,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7029,
      "step": 522912
    },
    {
      "epoch": 0.0018978675613734479,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.715,
      "step": 522944
    },
    {
      "epoch": 0.0018979836957242846,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7323,
      "step": 522976
    },
    {
      "epoch": 0.0018980998300751211,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7164,
      "step": 523008
    },
    {
      "epoch": 0.001898215964425958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7155,
      "step": 523040
    },
    {
      "epoch": 0.0018983320987767947,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7245,
      "step": 523072
    },
    {
      "epoch": 0.0018984482331276314,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7146,
      "step": 523104
    },
    {
      "epoch": 0.001898564367478468,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7116,
      "step": 523136
    },
    {
      "epoch": 0.0018986805018293047,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7131,
      "step": 523168
    },
    {
      "epoch": 0.0018987966361801415,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7054,
      "step": 523200
    },
    {
      "epoch": 0.0018989127705309782,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7232,
      "step": 523232
    },
    {
      "epoch": 0.001899028904881815,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7309,
      "step": 523264
    },
    {
      "epoch": 0.0018991450392326515,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7349,
      "step": 523296
    },
    {
      "epoch": 0.0018992611735834883,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.721,
      "step": 523328
    },
    {
      "epoch": 0.001899377307934325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7017,
      "step": 523360
    },
    {
      "epoch": 0.0018994934422851618,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7294,
      "step": 523392
    },
    {
      "epoch": 0.0018996095766359983,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7273,
      "step": 523424
    },
    {
      "epoch": 0.001899725710986835,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7296,
      "step": 523456
    },
    {
      "epoch": 0.0018998418453376718,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7254,
      "step": 523488
    },
    {
      "epoch": 0.0018999579796885086,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7156,
      "step": 523520
    },
    {
      "epoch": 0.0019000741140393453,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7101,
      "step": 523552
    },
    {
      "epoch": 0.0019001902483901819,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7201,
      "step": 523584
    },
    {
      "epoch": 0.0019003063827410186,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7307,
      "step": 523616
    },
    {
      "epoch": 0.0019004225170918554,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7184,
      "step": 523648
    },
    {
      "epoch": 0.0019005386514426921,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7181,
      "step": 523680
    },
    {
      "epoch": 0.0019006547857935287,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 523712
    },
    {
      "epoch": 0.0019007709201443654,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7125,
      "step": 523744
    },
    {
      "epoch": 0.0019008870544952022,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.716,
      "step": 523776
    },
    {
      "epoch": 0.001901003188846039,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7263,
      "step": 523808
    },
    {
      "epoch": 0.0019011193231968757,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7194,
      "step": 523840
    },
    {
      "epoch": 0.0019012354575477122,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7673,
      "step": 523872
    },
    {
      "epoch": 0.001901351591898549,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7211,
      "step": 523904
    },
    {
      "epoch": 0.0019014677262493857,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7035,
      "step": 523936
    },
    {
      "epoch": 0.0019015838606002225,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.725,
      "step": 523968
    },
    {
      "epoch": 0.001901699994951059,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7248,
      "step": 524000
    },
    {
      "epoch": 0.0019018161293018958,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7362,
      "step": 524032
    },
    {
      "epoch": 0.0019019322636527325,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7283,
      "step": 524064
    },
    {
      "epoch": 0.0019020483980035693,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 524096
    },
    {
      "epoch": 0.001902164532354406,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7496,
      "step": 524128
    },
    {
      "epoch": 0.0019022806667052426,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.753,
      "step": 524160
    },
    {
      "epoch": 0.0019023968010560793,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7249,
      "step": 524192
    },
    {
      "epoch": 0.001902512935406916,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7187,
      "step": 524224
    },
    {
      "epoch": 0.0019026290697577528,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7436,
      "step": 524256
    },
    {
      "epoch": 0.0019027452041085894,
      "grad_norm": 1.2276249554308492,
      "learning_rate": 0.002,
      "loss": 2.7222,
      "step": 524288
    },
    {
      "epoch": 0.0019027452041085894,
      "eval_loss": 2.5859375,
      "eval_runtime": 24835.5419,
      "eval_samples_per_second": 214.58,
      "eval_steps_per_second": 0.419,
      "step": 524288
    }
  ],
  "logging_steps": 32,
  "max_steps": 275542936,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 262144,
  "total_flos": 4.092632087624103e+19,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
