{
  "best_metric": 1.7109375,
  "best_model_checkpoint": "./output/checkpoint-262144",
  "epoch": 0.0009513726020542947,
  "eval_steps": 262144,
  "global_step": 262144,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.161343508367059e-07,
      "grad_norm": 409572.6672130356,
      "learning_rate": 0.0001,
      "loss": 93.4514,
      "step": 32
    },
    {
      "epoch": 2.322687016734118e-07,
      "grad_norm": 298229.6124800487,
      "learning_rate": 0.0001,
      "loss": 29.8428,
      "step": 64
    },
    {
      "epoch": 3.484030525101177e-07,
      "grad_norm": 116628.18664456718,
      "learning_rate": 0.0001,
      "loss": 24.1542,
      "step": 96
    },
    {
      "epoch": 4.645374033468236e-07,
      "grad_norm": 139092.62571394647,
      "learning_rate": 0.0001,
      "loss": 21.9257,
      "step": 128
    },
    {
      "epoch": 5.806717541835295e-07,
      "grad_norm": 76551.02225313519,
      "learning_rate": 0.0001,
      "loss": 19.5443,
      "step": 160
    },
    {
      "epoch": 6.968061050202354e-07,
      "grad_norm": 77397.12640660505,
      "learning_rate": 0.0001,
      "loss": 18.6562,
      "step": 192
    },
    {
      "epoch": 8.129404558569413e-07,
      "grad_norm": 95565.95644893634,
      "learning_rate": 0.0001,
      "loss": 16.8551,
      "step": 224
    },
    {
      "epoch": 9.290748066936471e-07,
      "grad_norm": 46406.155755459855,
      "learning_rate": 0.0001,
      "loss": 15.6748,
      "step": 256
    },
    {
      "epoch": 1.045209157530353e-06,
      "grad_norm": 56922.60166225715,
      "learning_rate": 0.0001,
      "loss": 15.2214,
      "step": 288
    },
    {
      "epoch": 1.161343508367059e-06,
      "grad_norm": 58975.64263320918,
      "learning_rate": 0.0001,
      "loss": 14.0696,
      "step": 320
    },
    {
      "epoch": 1.2774778592037649e-06,
      "grad_norm": 65675.34386662926,
      "learning_rate": 0.0001,
      "loss": 13.6104,
      "step": 352
    },
    {
      "epoch": 1.3936122100404707e-06,
      "grad_norm": 54383.54637204161,
      "learning_rate": 0.0001,
      "loss": 12.461,
      "step": 384
    },
    {
      "epoch": 1.5097465608771768e-06,
      "grad_norm": 52796.19152931393,
      "learning_rate": 0.0001,
      "loss": 12.1938,
      "step": 416
    },
    {
      "epoch": 1.6258809117138826e-06,
      "grad_norm": 59098.14467476962,
      "learning_rate": 0.0001,
      "loss": 12.2385,
      "step": 448
    },
    {
      "epoch": 1.7420152625505885e-06,
      "grad_norm": 39881.043165895244,
      "learning_rate": 0.0001,
      "loss": 11.5189,
      "step": 480
    },
    {
      "epoch": 1.8581496133872943e-06,
      "grad_norm": 46619.10501071422,
      "learning_rate": 0.0001,
      "loss": 11.2951,
      "step": 512
    },
    {
      "epoch": 1.974283964224e-06,
      "grad_norm": 34572.07248632919,
      "learning_rate": 0.0001,
      "loss": 10.9112,
      "step": 544
    },
    {
      "epoch": 2.090418315060706e-06,
      "grad_norm": 46754.31603606238,
      "learning_rate": 0.0001,
      "loss": 10.7267,
      "step": 576
    },
    {
      "epoch": 2.2065526658974122e-06,
      "grad_norm": 32216.06530909695,
      "learning_rate": 0.0001,
      "loss": 10.3765,
      "step": 608
    },
    {
      "epoch": 2.322687016734118e-06,
      "grad_norm": 39603.068921486374,
      "learning_rate": 0.0001,
      "loss": 10.1508,
      "step": 640
    },
    {
      "epoch": 2.438821367570824e-06,
      "grad_norm": 40816.37112728176,
      "learning_rate": 0.0001,
      "loss": 9.8823,
      "step": 672
    },
    {
      "epoch": 2.5549557184075298e-06,
      "grad_norm": 35320.400507355516,
      "learning_rate": 0.0001,
      "loss": 9.5353,
      "step": 704
    },
    {
      "epoch": 2.6710900692442356e-06,
      "grad_norm": 37960.90167527637,
      "learning_rate": 0.0001,
      "loss": 9.2725,
      "step": 736
    },
    {
      "epoch": 2.7872244200809414e-06,
      "grad_norm": 44359.30657708707,
      "learning_rate": 0.0001,
      "loss": 9.181,
      "step": 768
    },
    {
      "epoch": 2.9033587709176473e-06,
      "grad_norm": 33022.60225966451,
      "learning_rate": 0.0001,
      "loss": 8.9102,
      "step": 800
    },
    {
      "epoch": 3.0194931217543536e-06,
      "grad_norm": 32567.61557130027,
      "learning_rate": 0.0001,
      "loss": 8.7962,
      "step": 832
    },
    {
      "epoch": 3.1356274725910594e-06,
      "grad_norm": 39935.8979365683,
      "learning_rate": 0.0001,
      "loss": 8.5775,
      "step": 864
    },
    {
      "epoch": 3.2517618234277652e-06,
      "grad_norm": 45957.19573690283,
      "learning_rate": 0.0001,
      "loss": 8.8212,
      "step": 896
    },
    {
      "epoch": 3.367896174264471e-06,
      "grad_norm": 34437.40036646204,
      "learning_rate": 0.0001,
      "loss": 8.1587,
      "step": 928
    },
    {
      "epoch": 3.484030525101177e-06,
      "grad_norm": 32333.58532547852,
      "learning_rate": 0.0001,
      "loss": 8.0502,
      "step": 960
    },
    {
      "epoch": 3.6001648759378828e-06,
      "grad_norm": 27926.411978626973,
      "learning_rate": 0.0001,
      "loss": 7.6843,
      "step": 992
    },
    {
      "epoch": 3.7162992267745886e-06,
      "grad_norm": 46014.227669276384,
      "learning_rate": 0.0001,
      "loss": 7.5791,
      "step": 1024
    },
    {
      "epoch": 3.8324335776112944e-06,
      "grad_norm": 26273.29929795647,
      "learning_rate": 0.0001,
      "loss": 7.6431,
      "step": 1056
    },
    {
      "epoch": 3.948567928448e-06,
      "grad_norm": 30534.236129302466,
      "learning_rate": 0.0001,
      "loss": 7.1365,
      "step": 1088
    },
    {
      "epoch": 4.064702279284706e-06,
      "grad_norm": 23060.03321766905,
      "learning_rate": 0.0001,
      "loss": 7.2321,
      "step": 1120
    },
    {
      "epoch": 4.180836630121412e-06,
      "grad_norm": 24888.293633754805,
      "learning_rate": 0.0001,
      "loss": 7.1035,
      "step": 1152
    },
    {
      "epoch": 4.296970980958119e-06,
      "grad_norm": 30571.920907918102,
      "learning_rate": 0.0001,
      "loss": 6.8501,
      "step": 1184
    },
    {
      "epoch": 4.4131053317948245e-06,
      "grad_norm": 23334.540149743687,
      "learning_rate": 0.0001,
      "loss": 6.5739,
      "step": 1216
    },
    {
      "epoch": 4.52923968263153e-06,
      "grad_norm": 37391.70378573301,
      "learning_rate": 0.0001,
      "loss": 6.5454,
      "step": 1248
    },
    {
      "epoch": 4.645374033468236e-06,
      "grad_norm": 27152.180317609855,
      "learning_rate": 0.0001,
      "loss": 6.4446,
      "step": 1280
    },
    {
      "epoch": 4.761508384304942e-06,
      "grad_norm": 30626.713372479262,
      "learning_rate": 0.0001,
      "loss": 6.3004,
      "step": 1312
    },
    {
      "epoch": 4.877642735141648e-06,
      "grad_norm": 31318.10952148932,
      "learning_rate": 0.0001,
      "loss": 6.191,
      "step": 1344
    },
    {
      "epoch": 4.993777085978354e-06,
      "grad_norm": 24971.72232746472,
      "learning_rate": 0.0001,
      "loss": 6.0902,
      "step": 1376
    },
    {
      "epoch": 5.1099114368150595e-06,
      "grad_norm": 19424.006074957862,
      "learning_rate": 0.0001,
      "loss": 6.0366,
      "step": 1408
    },
    {
      "epoch": 5.226045787651765e-06,
      "grad_norm": 35026.2539247348,
      "learning_rate": 0.0001,
      "loss": 6.0009,
      "step": 1440
    },
    {
      "epoch": 5.342180138488471e-06,
      "grad_norm": 26796.96822403609,
      "learning_rate": 0.0001,
      "loss": 5.9144,
      "step": 1472
    },
    {
      "epoch": 5.458314489325177e-06,
      "grad_norm": 18411.54549460745,
      "learning_rate": 0.0001,
      "loss": 5.6386,
      "step": 1504
    },
    {
      "epoch": 5.574448840161883e-06,
      "grad_norm": 16159.86540785535,
      "learning_rate": 0.0001,
      "loss": 5.5917,
      "step": 1536
    },
    {
      "epoch": 5.690583190998589e-06,
      "grad_norm": 16693.284952339367,
      "learning_rate": 0.0001,
      "loss": 5.5403,
      "step": 1568
    },
    {
      "epoch": 5.806717541835295e-06,
      "grad_norm": 15610.660428053643,
      "learning_rate": 0.0001,
      "loss": 5.4474,
      "step": 1600
    },
    {
      "epoch": 5.922851892672001e-06,
      "grad_norm": 13846.19155219225,
      "learning_rate": 0.0001,
      "loss": 5.2932,
      "step": 1632
    },
    {
      "epoch": 6.038986243508707e-06,
      "grad_norm": 15878.182169253507,
      "learning_rate": 0.0001,
      "loss": 5.2903,
      "step": 1664
    },
    {
      "epoch": 6.155120594345413e-06,
      "grad_norm": 13490.37138109993,
      "learning_rate": 0.0001,
      "loss": 5.2357,
      "step": 1696
    },
    {
      "epoch": 6.271254945182119e-06,
      "grad_norm": 17380.39524867027,
      "learning_rate": 0.0001,
      "loss": 5.1873,
      "step": 1728
    },
    {
      "epoch": 6.387389296018825e-06,
      "grad_norm": 11603.54118362149,
      "learning_rate": 0.0001,
      "loss": 5.2631,
      "step": 1760
    },
    {
      "epoch": 6.5035236468555305e-06,
      "grad_norm": 8962.60179858505,
      "learning_rate": 0.0001,
      "loss": 4.9992,
      "step": 1792
    },
    {
      "epoch": 6.619657997692236e-06,
      "grad_norm": 14465.668252797726,
      "learning_rate": 0.0001,
      "loss": 4.8946,
      "step": 1824
    },
    {
      "epoch": 6.735792348528942e-06,
      "grad_norm": 19733.17369304796,
      "learning_rate": 0.0001,
      "loss": 4.9184,
      "step": 1856
    },
    {
      "epoch": 6.851926699365648e-06,
      "grad_norm": 12558.132385032417,
      "learning_rate": 0.0001,
      "loss": 4.8565,
      "step": 1888
    },
    {
      "epoch": 6.968061050202354e-06,
      "grad_norm": 14083.85621909,
      "learning_rate": 0.0001,
      "loss": 4.8656,
      "step": 1920
    },
    {
      "epoch": 7.08419540103906e-06,
      "grad_norm": 16702.6788420301,
      "learning_rate": 0.0001,
      "loss": 4.7063,
      "step": 1952
    },
    {
      "epoch": 7.2003297518757655e-06,
      "grad_norm": 13509.329887155765,
      "learning_rate": 0.0001,
      "loss": 4.5831,
      "step": 1984
    },
    {
      "epoch": 7.316464102712471e-06,
      "grad_norm": 10776.858818783887,
      "learning_rate": 0.0001,
      "loss": 4.6689,
      "step": 2016
    },
    {
      "epoch": 7.432598453549177e-06,
      "grad_norm": 15217.366091081596,
      "learning_rate": 0.0001,
      "loss": 4.6723,
      "step": 2048
    },
    {
      "epoch": 7.548732804385883e-06,
      "grad_norm": 14957.827248634743,
      "learning_rate": 0.0001,
      "loss": 4.436,
      "step": 2080
    },
    {
      "epoch": 7.664867155222589e-06,
      "grad_norm": 13795.876847812175,
      "learning_rate": 0.0001,
      "loss": 4.4759,
      "step": 2112
    },
    {
      "epoch": 7.781001506059296e-06,
      "grad_norm": 11286.501229344725,
      "learning_rate": 0.0001,
      "loss": 4.4257,
      "step": 2144
    },
    {
      "epoch": 7.897135856896e-06,
      "grad_norm": 17644.0152743076,
      "learning_rate": 0.0001,
      "loss": 4.3807,
      "step": 2176
    },
    {
      "epoch": 8.013270207732707e-06,
      "grad_norm": 12903.463469162069,
      "learning_rate": 0.0001,
      "loss": 4.4125,
      "step": 2208
    },
    {
      "epoch": 8.129404558569412e-06,
      "grad_norm": 11965.748451308844,
      "learning_rate": 0.0001,
      "loss": 4.3511,
      "step": 2240
    },
    {
      "epoch": 8.245538909406119e-06,
      "grad_norm": 12105.225462584329,
      "learning_rate": 0.0001,
      "loss": 4.3268,
      "step": 2272
    },
    {
      "epoch": 8.361673260242824e-06,
      "grad_norm": 7619.67606758975,
      "learning_rate": 0.0001,
      "loss": 4.2662,
      "step": 2304
    },
    {
      "epoch": 8.47780761107953e-06,
      "grad_norm": 13129.423406989357,
      "learning_rate": 0.0001,
      "loss": 4.2016,
      "step": 2336
    },
    {
      "epoch": 8.593941961916237e-06,
      "grad_norm": 13801.409927974752,
      "learning_rate": 0.0001,
      "loss": 4.2434,
      "step": 2368
    },
    {
      "epoch": 8.710076312752942e-06,
      "grad_norm": 9483.954330868532,
      "learning_rate": 0.0001,
      "loss": 4.117,
      "step": 2400
    },
    {
      "epoch": 8.826210663589649e-06,
      "grad_norm": 11325.919565315657,
      "learning_rate": 0.0001,
      "loss": 4.0331,
      "step": 2432
    },
    {
      "epoch": 8.942345014426354e-06,
      "grad_norm": 9246.819885776948,
      "learning_rate": 0.0001,
      "loss": 4.1495,
      "step": 2464
    },
    {
      "epoch": 9.05847936526306e-06,
      "grad_norm": 22537.95500927269,
      "learning_rate": 0.0001,
      "loss": 4.0052,
      "step": 2496
    },
    {
      "epoch": 9.174613716099766e-06,
      "grad_norm": 25145.246111342793,
      "learning_rate": 0.0001,
      "loss": 3.9738,
      "step": 2528
    },
    {
      "epoch": 9.290748066936472e-06,
      "grad_norm": 16548.68197923931,
      "learning_rate": 0.0001,
      "loss": 3.9415,
      "step": 2560
    },
    {
      "epoch": 9.406882417773177e-06,
      "grad_norm": 27505.274839564863,
      "learning_rate": 0.0001,
      "loss": 4.0287,
      "step": 2592
    },
    {
      "epoch": 9.523016768609884e-06,
      "grad_norm": 34385.620715642166,
      "learning_rate": 0.0001,
      "loss": 4.0115,
      "step": 2624
    },
    {
      "epoch": 9.639151119446589e-06,
      "grad_norm": 17275.651420424063,
      "learning_rate": 0.0001,
      "loss": 3.8689,
      "step": 2656
    },
    {
      "epoch": 9.755285470283296e-06,
      "grad_norm": 21617.040176675437,
      "learning_rate": 0.0001,
      "loss": 3.937,
      "step": 2688
    },
    {
      "epoch": 9.87141982112e-06,
      "grad_norm": 24796.554075112937,
      "learning_rate": 0.0001,
      "loss": 3.9091,
      "step": 2720
    },
    {
      "epoch": 9.987554171956707e-06,
      "grad_norm": 25231.23207455395,
      "learning_rate": 0.0001,
      "loss": 3.9054,
      "step": 2752
    },
    {
      "epoch": 1.0103688522793412e-05,
      "grad_norm": 17232.006760676486,
      "learning_rate": 0.0001,
      "loss": 3.8413,
      "step": 2784
    },
    {
      "epoch": 1.0219822873630119e-05,
      "grad_norm": 21326.664061685784,
      "learning_rate": 0.0001,
      "loss": 3.835,
      "step": 2816
    },
    {
      "epoch": 1.0335957224466826e-05,
      "grad_norm": 21415.05984114917,
      "learning_rate": 0.0001,
      "loss": 3.7126,
      "step": 2848
    },
    {
      "epoch": 1.045209157530353e-05,
      "grad_norm": 22420.70578728511,
      "learning_rate": 0.0001,
      "loss": 3.7849,
      "step": 2880
    },
    {
      "epoch": 1.0568225926140237e-05,
      "grad_norm": 22287.552220914702,
      "learning_rate": 0.0001,
      "loss": 3.763,
      "step": 2912
    },
    {
      "epoch": 1.0684360276976942e-05,
      "grad_norm": 28384.378062589287,
      "learning_rate": 0.0001,
      "loss": 3.7441,
      "step": 2944
    },
    {
      "epoch": 1.0800494627813649e-05,
      "grad_norm": 24712.028791663386,
      "learning_rate": 0.0001,
      "loss": 3.6634,
      "step": 2976
    },
    {
      "epoch": 1.0916628978650354e-05,
      "grad_norm": 20253.55057761478,
      "learning_rate": 0.0001,
      "loss": 3.6061,
      "step": 3008
    },
    {
      "epoch": 1.103276332948706e-05,
      "grad_norm": 23643.859477674112,
      "learning_rate": 0.0001,
      "loss": 3.6196,
      "step": 3040
    },
    {
      "epoch": 1.1148897680323766e-05,
      "grad_norm": 29160.01148833793,
      "learning_rate": 0.0001,
      "loss": 3.5903,
      "step": 3072
    },
    {
      "epoch": 1.1265032031160472e-05,
      "grad_norm": 20934.49555160095,
      "learning_rate": 0.0001,
      "loss": 3.6281,
      "step": 3104
    },
    {
      "epoch": 1.1381166381997177e-05,
      "grad_norm": 20391.92585314099,
      "learning_rate": 0.0001,
      "loss": 3.5786,
      "step": 3136
    },
    {
      "epoch": 1.1497300732833884e-05,
      "grad_norm": 25167.143898344922,
      "learning_rate": 0.0001,
      "loss": 3.534,
      "step": 3168
    },
    {
      "epoch": 1.161343508367059e-05,
      "grad_norm": 21929.786410268567,
      "learning_rate": 0.0001,
      "loss": 3.5526,
      "step": 3200
    },
    {
      "epoch": 1.1729569434507296e-05,
      "grad_norm": 23073.222943490146,
      "learning_rate": 0.0001,
      "loss": 3.4931,
      "step": 3232
    },
    {
      "epoch": 1.1845703785344003e-05,
      "grad_norm": 20149.36386092623,
      "learning_rate": 0.0001,
      "loss": 3.4955,
      "step": 3264
    },
    {
      "epoch": 1.1961838136180708e-05,
      "grad_norm": 21224.364725475294,
      "learning_rate": 0.0001,
      "loss": 3.4365,
      "step": 3296
    },
    {
      "epoch": 1.2077972487017414e-05,
      "grad_norm": 18944.67766946696,
      "learning_rate": 0.0001,
      "loss": 3.3932,
      "step": 3328
    },
    {
      "epoch": 1.219410683785412e-05,
      "grad_norm": 20244.963521824386,
      "learning_rate": 0.0001,
      "loss": 3.4429,
      "step": 3360
    },
    {
      "epoch": 1.2310241188690826e-05,
      "grad_norm": 24120.8867167026,
      "learning_rate": 0.0001,
      "loss": 3.397,
      "step": 3392
    },
    {
      "epoch": 1.2426375539527531e-05,
      "grad_norm": 15817.65191170927,
      "learning_rate": 0.0001,
      "loss": 3.3637,
      "step": 3424
    },
    {
      "epoch": 1.2542509890364238e-05,
      "grad_norm": 18846.486569119457,
      "learning_rate": 0.0001,
      "loss": 3.4019,
      "step": 3456
    },
    {
      "epoch": 1.2658644241200943e-05,
      "grad_norm": 28008.362536928144,
      "learning_rate": 0.0001,
      "loss": 3.3812,
      "step": 3488
    },
    {
      "epoch": 1.277477859203765e-05,
      "grad_norm": 19973.044935612597,
      "learning_rate": 0.0001,
      "loss": 3.3781,
      "step": 3520
    },
    {
      "epoch": 1.2890912942874354e-05,
      "grad_norm": 22302.577250174476,
      "learning_rate": 0.0001,
      "loss": 3.3601,
      "step": 3552
    },
    {
      "epoch": 1.3007047293711061e-05,
      "grad_norm": 17686.60589259567,
      "learning_rate": 0.0001,
      "loss": 3.2783,
      "step": 3584
    },
    {
      "epoch": 1.3123181644547766e-05,
      "grad_norm": 23607.784139982305,
      "learning_rate": 0.0001,
      "loss": 3.2929,
      "step": 3616
    },
    {
      "epoch": 1.3239315995384473e-05,
      "grad_norm": 18854.86547286933,
      "learning_rate": 0.0001,
      "loss": 3.3439,
      "step": 3648
    },
    {
      "epoch": 1.3355450346221178e-05,
      "grad_norm": 18161.94493990112,
      "learning_rate": 0.0001,
      "loss": 3.2751,
      "step": 3680
    },
    {
      "epoch": 1.3471584697057884e-05,
      "grad_norm": 18281.619293705906,
      "learning_rate": 0.0001,
      "loss": 3.3619,
      "step": 3712
    },
    {
      "epoch": 1.3587719047894591e-05,
      "grad_norm": 15669.935736945446,
      "learning_rate": 0.0001,
      "loss": 3.212,
      "step": 3744
    },
    {
      "epoch": 1.3703853398731296e-05,
      "grad_norm": 12464.307100677519,
      "learning_rate": 0.0001,
      "loss": 3.1625,
      "step": 3776
    },
    {
      "epoch": 1.3819987749568003e-05,
      "grad_norm": 17099.399755546976,
      "learning_rate": 0.0001,
      "loss": 3.202,
      "step": 3808
    },
    {
      "epoch": 1.3936122100404708e-05,
      "grad_norm": 22267.673677328756,
      "learning_rate": 0.0001,
      "loss": 3.2204,
      "step": 3840
    },
    {
      "epoch": 1.4052256451241414e-05,
      "grad_norm": 24745.124287422765,
      "learning_rate": 0.0001,
      "loss": 3.1489,
      "step": 3872
    },
    {
      "epoch": 1.416839080207812e-05,
      "grad_norm": 15584.810040549099,
      "learning_rate": 0.0001,
      "loss": 3.1428,
      "step": 3904
    },
    {
      "epoch": 1.4284525152914826e-05,
      "grad_norm": 14626.249553456963,
      "learning_rate": 0.0001,
      "loss": 3.0737,
      "step": 3936
    },
    {
      "epoch": 1.4400659503751531e-05,
      "grad_norm": 16616.824726764135,
      "learning_rate": 0.0001,
      "loss": 3.1624,
      "step": 3968
    },
    {
      "epoch": 1.4516793854588238e-05,
      "grad_norm": 21450.52055312411,
      "learning_rate": 0.0001,
      "loss": 3.1398,
      "step": 4000
    },
    {
      "epoch": 1.4632928205424943e-05,
      "grad_norm": 16308.334065746874,
      "learning_rate": 0.0001,
      "loss": 3.1596,
      "step": 4032
    },
    {
      "epoch": 1.474906255626165e-05,
      "grad_norm": 20303.278565295805,
      "learning_rate": 0.0001,
      "loss": 3.1389,
      "step": 4064
    },
    {
      "epoch": 1.4865196907098354e-05,
      "grad_norm": 11624.364326706214,
      "learning_rate": 0.0001,
      "loss": 3.1285,
      "step": 4096
    },
    {
      "epoch": 1.4981331257935061e-05,
      "grad_norm": 18035.782877380178,
      "learning_rate": 0.0001,
      "loss": 3.0929,
      "step": 4128
    },
    {
      "epoch": 1.5097465608771766e-05,
      "grad_norm": 16843.670087008948,
      "learning_rate": 0.0001,
      "loss": 3.0646,
      "step": 4160
    },
    {
      "epoch": 1.5213599959608473e-05,
      "grad_norm": 22057.891014328638,
      "learning_rate": 0.0001,
      "loss": 3.0149,
      "step": 4192
    },
    {
      "epoch": 1.5329734310445178e-05,
      "grad_norm": 16503.23586452063,
      "learning_rate": 0.0001,
      "loss": 3.0159,
      "step": 4224
    },
    {
      "epoch": 1.5445868661281884e-05,
      "grad_norm": 18436.948730741755,
      "learning_rate": 0.0001,
      "loss": 3.0184,
      "step": 4256
    },
    {
      "epoch": 1.556200301211859e-05,
      "grad_norm": 22615.68265606855,
      "learning_rate": 0.0001,
      "loss": 3.013,
      "step": 4288
    },
    {
      "epoch": 1.5678137362955298e-05,
      "grad_norm": 26030.19515869983,
      "learning_rate": 0.0001,
      "loss": 3.0253,
      "step": 4320
    },
    {
      "epoch": 1.5794271713792e-05,
      "grad_norm": 10740.673582229374,
      "learning_rate": 0.0001,
      "loss": 2.9711,
      "step": 4352
    },
    {
      "epoch": 1.5910406064628708e-05,
      "grad_norm": 16481.400547283596,
      "learning_rate": 0.0001,
      "loss": 2.9474,
      "step": 4384
    },
    {
      "epoch": 1.6026540415465414e-05,
      "grad_norm": 14825.707200670056,
      "learning_rate": 0.0001,
      "loss": 2.9869,
      "step": 4416
    },
    {
      "epoch": 1.614267476630212e-05,
      "grad_norm": 16911.679987511587,
      "learning_rate": 0.0001,
      "loss": 2.9715,
      "step": 4448
    },
    {
      "epoch": 1.6258809117138824e-05,
      "grad_norm": 19450.93190569542,
      "learning_rate": 0.0001,
      "loss": 2.9727,
      "step": 4480
    },
    {
      "epoch": 1.637494346797553e-05,
      "grad_norm": 12378.442551468259,
      "learning_rate": 0.0001,
      "loss": 2.9479,
      "step": 4512
    },
    {
      "epoch": 1.6491077818812238e-05,
      "grad_norm": 13415.24976472671,
      "learning_rate": 0.0001,
      "loss": 2.9736,
      "step": 4544
    },
    {
      "epoch": 1.6607212169648945e-05,
      "grad_norm": 19899.070631564682,
      "learning_rate": 0.0001,
      "loss": 2.9914,
      "step": 4576
    },
    {
      "epoch": 1.6723346520485648e-05,
      "grad_norm": 19685.893528107885,
      "learning_rate": 0.0001,
      "loss": 2.999,
      "step": 4608
    },
    {
      "epoch": 1.6839480871322355e-05,
      "grad_norm": 16442.116317554744,
      "learning_rate": 0.0001,
      "loss": 2.9148,
      "step": 4640
    },
    {
      "epoch": 1.695561522215906e-05,
      "grad_norm": 13557.160690941153,
      "learning_rate": 0.0001,
      "loss": 2.8907,
      "step": 4672
    },
    {
      "epoch": 1.7071749572995768e-05,
      "grad_norm": 13852.497247788935,
      "learning_rate": 0.0001,
      "loss": 2.9094,
      "step": 4704
    },
    {
      "epoch": 1.7187883923832475e-05,
      "grad_norm": 18497.08204014893,
      "learning_rate": 0.0001,
      "loss": 2.856,
      "step": 4736
    },
    {
      "epoch": 1.7304018274669178e-05,
      "grad_norm": 15590.688310655178,
      "learning_rate": 0.0001,
      "loss": 2.8007,
      "step": 4768
    },
    {
      "epoch": 1.7420152625505885e-05,
      "grad_norm": 21210.639665036037,
      "learning_rate": 0.0001,
      "loss": 2.8586,
      "step": 4800
    },
    {
      "epoch": 1.753628697634259e-05,
      "grad_norm": 19541.73216478007,
      "learning_rate": 0.0001,
      "loss": 2.8502,
      "step": 4832
    },
    {
      "epoch": 1.7652421327179298e-05,
      "grad_norm": 14852.01015014466,
      "learning_rate": 0.0001,
      "loss": 2.8688,
      "step": 4864
    },
    {
      "epoch": 1.7768555678016e-05,
      "grad_norm": 16210.071128776703,
      "learning_rate": 0.0001,
      "loss": 2.8488,
      "step": 4896
    },
    {
      "epoch": 1.7884690028852708e-05,
      "grad_norm": 15208.27445833353,
      "learning_rate": 0.0001,
      "loss": 2.7921,
      "step": 4928
    },
    {
      "epoch": 1.8000824379689415e-05,
      "grad_norm": 15372.000439110065,
      "learning_rate": 0.0001,
      "loss": 2.7917,
      "step": 4960
    },
    {
      "epoch": 1.811695873052612e-05,
      "grad_norm": 12801.62220189301,
      "learning_rate": 0.0001,
      "loss": 2.7533,
      "step": 4992
    },
    {
      "epoch": 1.8233093081362825e-05,
      "grad_norm": 13743.575080742274,
      "learning_rate": 0.0001,
      "loss": 2.8254,
      "step": 5024
    },
    {
      "epoch": 1.834922743219953e-05,
      "grad_norm": 15653.647370501229,
      "learning_rate": 0.0001,
      "loss": 2.7967,
      "step": 5056
    },
    {
      "epoch": 1.8465361783036238e-05,
      "grad_norm": 9654.409044576472,
      "learning_rate": 0.0001,
      "loss": 2.7101,
      "step": 5088
    },
    {
      "epoch": 1.8581496133872945e-05,
      "grad_norm": 16286.731654939244,
      "learning_rate": 0.0001,
      "loss": 2.7445,
      "step": 5120
    },
    {
      "epoch": 1.869763048470965e-05,
      "grad_norm": 16090.246486614182,
      "learning_rate": 0.0001,
      "loss": 2.782,
      "step": 5152
    },
    {
      "epoch": 1.8813764835546355e-05,
      "grad_norm": 16758.283802346825,
      "learning_rate": 0.0001,
      "loss": 2.7765,
      "step": 5184
    },
    {
      "epoch": 1.892989918638306e-05,
      "grad_norm": 18782.056916110123,
      "learning_rate": 0.0001,
      "loss": 2.7318,
      "step": 5216
    },
    {
      "epoch": 1.9046033537219768e-05,
      "grad_norm": 15958.73619682962,
      "learning_rate": 0.0001,
      "loss": 2.739,
      "step": 5248
    },
    {
      "epoch": 1.9162167888056475e-05,
      "grad_norm": 10285.049343586057,
      "learning_rate": 0.0001,
      "loss": 2.704,
      "step": 5280
    },
    {
      "epoch": 1.9278302238893178e-05,
      "grad_norm": 14528.576874560014,
      "learning_rate": 0.0001,
      "loss": 2.7101,
      "step": 5312
    },
    {
      "epoch": 1.9394436589729885e-05,
      "grad_norm": 13850.896144293334,
      "learning_rate": 0.0001,
      "loss": 2.7152,
      "step": 5344
    },
    {
      "epoch": 1.951057094056659e-05,
      "grad_norm": 13609.22900093903,
      "learning_rate": 0.0001,
      "loss": 2.6749,
      "step": 5376
    },
    {
      "epoch": 1.9626705291403298e-05,
      "grad_norm": 13022.109861692921,
      "learning_rate": 0.0001,
      "loss": 2.7144,
      "step": 5408
    },
    {
      "epoch": 1.974283964224e-05,
      "grad_norm": 14125.93711050704,
      "learning_rate": 0.0001,
      "loss": 2.6897,
      "step": 5440
    },
    {
      "epoch": 1.9858973993076708e-05,
      "grad_norm": 15328.001304801615,
      "learning_rate": 0.0001,
      "loss": 2.758,
      "step": 5472
    },
    {
      "epoch": 1.9975108343913415e-05,
      "grad_norm": 20883.58771858897,
      "learning_rate": 0.0001,
      "loss": 2.6389,
      "step": 5504
    },
    {
      "epoch": 2.009124269475012e-05,
      "grad_norm": 36168.23822084786,
      "learning_rate": 0.0001,
      "loss": 2.5778,
      "step": 5536
    },
    {
      "epoch": 2.0207377045586825e-05,
      "grad_norm": 28756.544994140033,
      "learning_rate": 0.0001,
      "loss": 2.6195,
      "step": 5568
    },
    {
      "epoch": 2.032351139642353e-05,
      "grad_norm": 24967.321522341957,
      "learning_rate": 0.0001,
      "loss": 2.6376,
      "step": 5600
    },
    {
      "epoch": 2.0439645747260238e-05,
      "grad_norm": 24962.49010014826,
      "learning_rate": 0.0001,
      "loss": 2.6018,
      "step": 5632
    },
    {
      "epoch": 2.0555780098096945e-05,
      "grad_norm": 19619.67176076093,
      "learning_rate": 0.0001,
      "loss": 2.5701,
      "step": 5664
    },
    {
      "epoch": 2.067191444893365e-05,
      "grad_norm": 28947.78526934315,
      "learning_rate": 0.0001,
      "loss": 2.6023,
      "step": 5696
    },
    {
      "epoch": 2.0788048799770355e-05,
      "grad_norm": 15386.200960601029,
      "learning_rate": 0.0001,
      "loss": 2.5938,
      "step": 5728
    },
    {
      "epoch": 2.090418315060706e-05,
      "grad_norm": 16819.837157356786,
      "learning_rate": 0.0001,
      "loss": 2.623,
      "step": 5760
    },
    {
      "epoch": 2.1020317501443768e-05,
      "grad_norm": 14677.377422414402,
      "learning_rate": 0.0001,
      "loss": 2.5902,
      "step": 5792
    },
    {
      "epoch": 2.1136451852280475e-05,
      "grad_norm": 8028.8776301547905,
      "learning_rate": 0.0001,
      "loss": 2.5836,
      "step": 5824
    },
    {
      "epoch": 2.1252586203117178e-05,
      "grad_norm": 12440.009967841666,
      "learning_rate": 0.0001,
      "loss": 2.6106,
      "step": 5856
    },
    {
      "epoch": 2.1368720553953885e-05,
      "grad_norm": 9361.396744076175,
      "learning_rate": 0.0001,
      "loss": 2.5657,
      "step": 5888
    },
    {
      "epoch": 2.148485490479059e-05,
      "grad_norm": 11840.09720188141,
      "learning_rate": 0.0001,
      "loss": 2.5423,
      "step": 5920
    },
    {
      "epoch": 2.1600989255627298e-05,
      "grad_norm": 17109.72881141019,
      "learning_rate": 0.0001,
      "loss": 2.5264,
      "step": 5952
    },
    {
      "epoch": 2.1717123606464e-05,
      "grad_norm": 13527.633717690615,
      "learning_rate": 0.0001,
      "loss": 2.5181,
      "step": 5984
    },
    {
      "epoch": 2.1833257957300708e-05,
      "grad_norm": 14498.226098388726,
      "learning_rate": 0.0001,
      "loss": 2.5279,
      "step": 6016
    },
    {
      "epoch": 2.1949392308137415e-05,
      "grad_norm": 17539.361105809985,
      "learning_rate": 0.0001,
      "loss": 2.6562,
      "step": 6048
    },
    {
      "epoch": 2.206552665897412e-05,
      "grad_norm": 18467.02275679542,
      "learning_rate": 0.0001,
      "loss": 2.4887,
      "step": 6080
    },
    {
      "epoch": 2.2181661009810828e-05,
      "grad_norm": 19486.593776234982,
      "learning_rate": 0.0001,
      "loss": 2.4612,
      "step": 6112
    },
    {
      "epoch": 2.229779536064753e-05,
      "grad_norm": 8924.183709169147,
      "learning_rate": 0.0001,
      "loss": 2.4452,
      "step": 6144
    },
    {
      "epoch": 2.2413929711484238e-05,
      "grad_norm": 8827.689675107526,
      "learning_rate": 0.0001,
      "loss": 2.458,
      "step": 6176
    },
    {
      "epoch": 2.2530064062320945e-05,
      "grad_norm": 8147.702191415688,
      "learning_rate": 0.0001,
      "loss": 2.4801,
      "step": 6208
    },
    {
      "epoch": 2.264619841315765e-05,
      "grad_norm": 10896.875699024928,
      "learning_rate": 0.0001,
      "loss": 2.4671,
      "step": 6240
    },
    {
      "epoch": 2.2762332763994355e-05,
      "grad_norm": 9675.749996770277,
      "learning_rate": 0.0001,
      "loss": 2.4609,
      "step": 6272
    },
    {
      "epoch": 2.287846711483106e-05,
      "grad_norm": 18169.082970805102,
      "learning_rate": 0.0001,
      "loss": 2.5135,
      "step": 6304
    },
    {
      "epoch": 2.299460146566777e-05,
      "grad_norm": 16604.1614061054,
      "learning_rate": 0.0001,
      "loss": 2.4967,
      "step": 6336
    },
    {
      "epoch": 2.3110735816504475e-05,
      "grad_norm": 14649.379099470394,
      "learning_rate": 0.0001,
      "loss": 2.4842,
      "step": 6368
    },
    {
      "epoch": 2.322687016734118e-05,
      "grad_norm": 9441.90928785063,
      "learning_rate": 0.0001,
      "loss": 2.421,
      "step": 6400
    },
    {
      "epoch": 2.3343004518177885e-05,
      "grad_norm": 13269.950828846353,
      "learning_rate": 0.0001,
      "loss": 2.4117,
      "step": 6432
    },
    {
      "epoch": 2.345913886901459e-05,
      "grad_norm": 9358.628532001898,
      "learning_rate": 0.0001,
      "loss": 2.4475,
      "step": 6464
    },
    {
      "epoch": 2.35752732198513e-05,
      "grad_norm": 9726.558178513096,
      "learning_rate": 0.0001,
      "loss": 2.4237,
      "step": 6496
    },
    {
      "epoch": 2.3691407570688005e-05,
      "grad_norm": 13905.706706241146,
      "learning_rate": 0.0001,
      "loss": 2.3553,
      "step": 6528
    },
    {
      "epoch": 2.380754192152471e-05,
      "grad_norm": 15080.404934881557,
      "learning_rate": 0.0001,
      "loss": 2.3781,
      "step": 6560
    },
    {
      "epoch": 2.3923676272361415e-05,
      "grad_norm": 8294.456401717956,
      "learning_rate": 0.0001,
      "loss": 2.3736,
      "step": 6592
    },
    {
      "epoch": 2.4039810623198122e-05,
      "grad_norm": 7958.516319013237,
      "learning_rate": 0.0001,
      "loss": 2.3908,
      "step": 6624
    },
    {
      "epoch": 2.415594497403483e-05,
      "grad_norm": 14199.886214332844,
      "learning_rate": 0.0001,
      "loss": 2.4256,
      "step": 6656
    },
    {
      "epoch": 2.4272079324871532e-05,
      "grad_norm": 16220.851765551648,
      "learning_rate": 0.0001,
      "loss": 2.3841,
      "step": 6688
    },
    {
      "epoch": 2.438821367570824e-05,
      "grad_norm": 23313.716037989314,
      "learning_rate": 0.0001,
      "loss": 2.3728,
      "step": 6720
    },
    {
      "epoch": 2.4504348026544945e-05,
      "grad_norm": 16561.34982421421,
      "learning_rate": 0.0001,
      "loss": 2.3761,
      "step": 6752
    },
    {
      "epoch": 2.4620482377381652e-05,
      "grad_norm": 15540.790166526283,
      "learning_rate": 0.0001,
      "loss": 2.3652,
      "step": 6784
    },
    {
      "epoch": 2.4736616728218355e-05,
      "grad_norm": 10463.79887994795,
      "learning_rate": 0.0001,
      "loss": 2.3655,
      "step": 6816
    },
    {
      "epoch": 2.4852751079055062e-05,
      "grad_norm": 13617.547760151238,
      "learning_rate": 0.0001,
      "loss": 2.3176,
      "step": 6848
    },
    {
      "epoch": 2.496888542989177e-05,
      "grad_norm": 10771.971755440134,
      "learning_rate": 0.0001,
      "loss": 2.3197,
      "step": 6880
    },
    {
      "epoch": 2.5085019780728475e-05,
      "grad_norm": 10909.340768350761,
      "learning_rate": 0.0001,
      "loss": 2.4162,
      "step": 6912
    },
    {
      "epoch": 2.520115413156518e-05,
      "grad_norm": 9485.776338813814,
      "learning_rate": 0.0001,
      "loss": 2.35,
      "step": 6944
    },
    {
      "epoch": 2.5317288482401885e-05,
      "grad_norm": 15615.700880844253,
      "learning_rate": 0.0001,
      "loss": 2.3233,
      "step": 6976
    },
    {
      "epoch": 2.5433422833238592e-05,
      "grad_norm": 9280.945722284987,
      "learning_rate": 0.0001,
      "loss": 2.2792,
      "step": 7008
    },
    {
      "epoch": 2.55495571840753e-05,
      "grad_norm": 8999.512542354725,
      "learning_rate": 0.0001,
      "loss": 2.286,
      "step": 7040
    },
    {
      "epoch": 2.5665691534912005e-05,
      "grad_norm": 12004.975301932112,
      "learning_rate": 0.0001,
      "loss": 2.3358,
      "step": 7072
    },
    {
      "epoch": 2.578182588574871e-05,
      "grad_norm": 8446.11603046039,
      "learning_rate": 0.0001,
      "loss": 2.2862,
      "step": 7104
    },
    {
      "epoch": 2.5897960236585415e-05,
      "grad_norm": 8882.275243427215,
      "learning_rate": 0.0001,
      "loss": 2.2808,
      "step": 7136
    },
    {
      "epoch": 2.6014094587422122e-05,
      "grad_norm": 15635.375659062369,
      "learning_rate": 0.0001,
      "loss": 2.3125,
      "step": 7168
    },
    {
      "epoch": 2.613022893825883e-05,
      "grad_norm": 16112.291333016543,
      "learning_rate": 0.0001,
      "loss": 2.3014,
      "step": 7200
    },
    {
      "epoch": 2.6246363289095532e-05,
      "grad_norm": 7468.061696317191,
      "learning_rate": 0.0001,
      "loss": 2.2694,
      "step": 7232
    },
    {
      "epoch": 2.636249763993224e-05,
      "grad_norm": 16851.66383476718,
      "learning_rate": 0.0001,
      "loss": 2.2969,
      "step": 7264
    },
    {
      "epoch": 2.6478631990768945e-05,
      "grad_norm": 13156.936649539663,
      "learning_rate": 0.0001,
      "loss": 2.2344,
      "step": 7296
    },
    {
      "epoch": 2.6594766341605652e-05,
      "grad_norm": 10638.894232954852,
      "learning_rate": 0.0001,
      "loss": 2.2271,
      "step": 7328
    },
    {
      "epoch": 2.6710900692442355e-05,
      "grad_norm": 9921.911055839999,
      "learning_rate": 0.0001,
      "loss": 2.236,
      "step": 7360
    },
    {
      "epoch": 2.6827035043279062e-05,
      "grad_norm": 5554.782624009692,
      "learning_rate": 0.0001,
      "loss": 2.2672,
      "step": 7392
    },
    {
      "epoch": 2.694316939411577e-05,
      "grad_norm": 8417.736988050885,
      "learning_rate": 0.0001,
      "loss": 2.2132,
      "step": 7424
    },
    {
      "epoch": 2.7059303744952475e-05,
      "grad_norm": 14542.259900029294,
      "learning_rate": 0.0001,
      "loss": 2.2076,
      "step": 7456
    },
    {
      "epoch": 2.7175438095789182e-05,
      "grad_norm": 9094.134758183429,
      "learning_rate": 0.0001,
      "loss": 2.2154,
      "step": 7488
    },
    {
      "epoch": 2.7291572446625885e-05,
      "grad_norm": 8125.027138416216,
      "learning_rate": 0.0001,
      "loss": 2.2301,
      "step": 7520
    },
    {
      "epoch": 2.7407706797462592e-05,
      "grad_norm": 8850.744488459713,
      "learning_rate": 0.0001,
      "loss": 2.2534,
      "step": 7552
    },
    {
      "epoch": 2.75238411482993e-05,
      "grad_norm": 9008.208395680022,
      "learning_rate": 0.0001,
      "loss": 2.2496,
      "step": 7584
    },
    {
      "epoch": 2.7639975499136005e-05,
      "grad_norm": 10577.522772369719,
      "learning_rate": 0.0001,
      "loss": 2.2281,
      "step": 7616
    },
    {
      "epoch": 2.775610984997271e-05,
      "grad_norm": 9065.412001117213,
      "learning_rate": 0.0001,
      "loss": 2.2434,
      "step": 7648
    },
    {
      "epoch": 2.7872244200809415e-05,
      "grad_norm": 7972.46555339062,
      "learning_rate": 0.0001,
      "loss": 2.1984,
      "step": 7680
    },
    {
      "epoch": 2.7988378551646122e-05,
      "grad_norm": 12220.363333387433,
      "learning_rate": 0.0001,
      "loss": 2.1947,
      "step": 7712
    },
    {
      "epoch": 2.810451290248283e-05,
      "grad_norm": 8995.745313758054,
      "learning_rate": 0.0001,
      "loss": 2.186,
      "step": 7744
    },
    {
      "epoch": 2.8220647253319532e-05,
      "grad_norm": 20462.785538630855,
      "learning_rate": 0.0001,
      "loss": 2.1525,
      "step": 7776
    },
    {
      "epoch": 2.833678160415624e-05,
      "grad_norm": 3465.70768386487,
      "learning_rate": 0.0001,
      "loss": 2.1905,
      "step": 7808
    },
    {
      "epoch": 2.8452915954992945e-05,
      "grad_norm": 4099.091042536138,
      "learning_rate": 0.0001,
      "loss": 2.1887,
      "step": 7840
    },
    {
      "epoch": 2.8569050305829652e-05,
      "grad_norm": 6118.072127925593,
      "learning_rate": 0.0001,
      "loss": 2.1552,
      "step": 7872
    },
    {
      "epoch": 2.868518465666636e-05,
      "grad_norm": 5525.807497551828,
      "learning_rate": 0.0001,
      "loss": 2.1347,
      "step": 7904
    },
    {
      "epoch": 2.8801319007503062e-05,
      "grad_norm": 4024.7498524753064,
      "learning_rate": 0.0001,
      "loss": 2.142,
      "step": 7936
    },
    {
      "epoch": 2.891745335833977e-05,
      "grad_norm": 6497.287376282505,
      "learning_rate": 0.0001,
      "loss": 2.1351,
      "step": 7968
    },
    {
      "epoch": 2.9033587709176475e-05,
      "grad_norm": 4541.016588413876,
      "learning_rate": 0.0001,
      "loss": 2.1781,
      "step": 8000
    },
    {
      "epoch": 2.9149722060013182e-05,
      "grad_norm": 4106.9287186412175,
      "learning_rate": 0.0001,
      "loss": 2.1514,
      "step": 8032
    },
    {
      "epoch": 2.9265856410849885e-05,
      "grad_norm": 6898.322078592736,
      "learning_rate": 0.0001,
      "loss": 2.1373,
      "step": 8064
    },
    {
      "epoch": 2.9381990761686592e-05,
      "grad_norm": 3538.59166901184,
      "learning_rate": 0.0001,
      "loss": 2.1532,
      "step": 8096
    },
    {
      "epoch": 2.94981251125233e-05,
      "grad_norm": 4148.488821245635,
      "learning_rate": 0.0001,
      "loss": 2.1128,
      "step": 8128
    },
    {
      "epoch": 2.9614259463360005e-05,
      "grad_norm": 4816.19460777905,
      "learning_rate": 0.0001,
      "loss": 2.1709,
      "step": 8160
    },
    {
      "epoch": 2.973039381419671e-05,
      "grad_norm": 6746.929894403825,
      "learning_rate": 0.0001,
      "loss": 2.1399,
      "step": 8192
    },
    {
      "epoch": 2.9846528165033415e-05,
      "grad_norm": 5197.007865108538,
      "learning_rate": 0.0001,
      "loss": 2.1039,
      "step": 8224
    },
    {
      "epoch": 2.9962662515870122e-05,
      "grad_norm": 4412.275150078472,
      "learning_rate": 0.0001,
      "loss": 2.1142,
      "step": 8256
    },
    {
      "epoch": 3.007879686670683e-05,
      "grad_norm": 5089.480769194438,
      "learning_rate": 0.0001,
      "loss": 2.098,
      "step": 8288
    },
    {
      "epoch": 3.0194931217543532e-05,
      "grad_norm": 6679.571075301168,
      "learning_rate": 0.0001,
      "loss": 2.0827,
      "step": 8320
    },
    {
      "epoch": 3.031106556838024e-05,
      "grad_norm": 2953.162499761908,
      "learning_rate": 0.0001,
      "loss": 2.0654,
      "step": 8352
    },
    {
      "epoch": 3.0427199919216946e-05,
      "grad_norm": 3350.802739643144,
      "learning_rate": 0.0001,
      "loss": 2.065,
      "step": 8384
    },
    {
      "epoch": 3.054333427005365e-05,
      "grad_norm": 7150.920989634832,
      "learning_rate": 0.0001,
      "loss": 2.101,
      "step": 8416
    },
    {
      "epoch": 3.0659468620890356e-05,
      "grad_norm": 6751.544906538058,
      "learning_rate": 0.0001,
      "loss": 2.0943,
      "step": 8448
    },
    {
      "epoch": 3.077560297172706e-05,
      "grad_norm": 4109.740867743367,
      "learning_rate": 0.0001,
      "loss": 2.087,
      "step": 8480
    },
    {
      "epoch": 3.089173732256377e-05,
      "grad_norm": 5390.4381605301805,
      "learning_rate": 0.0001,
      "loss": 2.0832,
      "step": 8512
    },
    {
      "epoch": 3.1007871673400476e-05,
      "grad_norm": 3120.081168815965,
      "learning_rate": 0.0001,
      "loss": 2.0723,
      "step": 8544
    },
    {
      "epoch": 3.112400602423718e-05,
      "grad_norm": 3607.5452180118273,
      "learning_rate": 0.0001,
      "loss": 2.0624,
      "step": 8576
    },
    {
      "epoch": 3.124014037507389e-05,
      "grad_norm": 6521.729026876233,
      "learning_rate": 0.0001,
      "loss": 2.137,
      "step": 8608
    },
    {
      "epoch": 3.1356274725910596e-05,
      "grad_norm": 3980.838950522867,
      "learning_rate": 0.0001,
      "loss": 2.0233,
      "step": 8640
    },
    {
      "epoch": 3.1472409076747296e-05,
      "grad_norm": 3767.3602827444047,
      "learning_rate": 0.0001,
      "loss": 2.0331,
      "step": 8672
    },
    {
      "epoch": 3.1588543427584e-05,
      "grad_norm": 4976.906418650044,
      "learning_rate": 0.0001,
      "loss": 2.0633,
      "step": 8704
    },
    {
      "epoch": 3.170467777842071e-05,
      "grad_norm": 5064.9089207013385,
      "learning_rate": 0.0001,
      "loss": 2.037,
      "step": 8736
    },
    {
      "epoch": 3.1820812129257416e-05,
      "grad_norm": 3535.76165769131,
      "learning_rate": 0.0001,
      "loss": 2.0191,
      "step": 8768
    },
    {
      "epoch": 3.193694648009412e-05,
      "grad_norm": 8398.897963423535,
      "learning_rate": 0.0001,
      "loss": 2.0463,
      "step": 8800
    },
    {
      "epoch": 3.205308083093083e-05,
      "grad_norm": 8621.803494629183,
      "learning_rate": 0.0001,
      "loss": 2.035,
      "step": 8832
    },
    {
      "epoch": 3.2169215181767536e-05,
      "grad_norm": 10823.170838529715,
      "learning_rate": 0.0001,
      "loss": 2.0283,
      "step": 8864
    },
    {
      "epoch": 3.228534953260424e-05,
      "grad_norm": 5418.9871055391895,
      "learning_rate": 0.0001,
      "loss": 2.0488,
      "step": 8896
    },
    {
      "epoch": 3.240148388344095e-05,
      "grad_norm": 5566.006804702991,
      "learning_rate": 0.0001,
      "loss": 2.0316,
      "step": 8928
    },
    {
      "epoch": 3.251761823427765e-05,
      "grad_norm": 10668.832340045465,
      "learning_rate": 0.0001,
      "loss": 2.0054,
      "step": 8960
    },
    {
      "epoch": 3.2633752585114356e-05,
      "grad_norm": 5487.171675827174,
      "learning_rate": 0.0001,
      "loss": 2.0139,
      "step": 8992
    },
    {
      "epoch": 3.274988693595106e-05,
      "grad_norm": 9551.782137381484,
      "learning_rate": 0.0001,
      "loss": 2.0085,
      "step": 9024
    },
    {
      "epoch": 3.286602128678777e-05,
      "grad_norm": 5860.839999522253,
      "learning_rate": 0.0001,
      "loss": 2.0109,
      "step": 9056
    },
    {
      "epoch": 3.2982155637624476e-05,
      "grad_norm": 12797.789809181897,
      "learning_rate": 0.0001,
      "loss": 1.9805,
      "step": 9088
    },
    {
      "epoch": 3.309828998846118e-05,
      "grad_norm": 8774.981153825916,
      "learning_rate": 0.0001,
      "loss": 1.9852,
      "step": 9120
    },
    {
      "epoch": 3.321442433929789e-05,
      "grad_norm": 7168.795191662265,
      "learning_rate": 0.0001,
      "loss": 2.002,
      "step": 9152
    },
    {
      "epoch": 3.3330558690134596e-05,
      "grad_norm": 14469.226810717979,
      "learning_rate": 0.0001,
      "loss": 1.9887,
      "step": 9184
    },
    {
      "epoch": 3.3446693040971296e-05,
      "grad_norm": 7456.30746415409,
      "learning_rate": 0.0001,
      "loss": 1.9512,
      "step": 9216
    },
    {
      "epoch": 3.3562827391808e-05,
      "grad_norm": 5534.825245660427,
      "learning_rate": 0.0001,
      "loss": 1.9451,
      "step": 9248
    },
    {
      "epoch": 3.367896174264471e-05,
      "grad_norm": 5095.24101490793,
      "learning_rate": 0.0001,
      "loss": 1.958,
      "step": 9280
    },
    {
      "epoch": 3.3795096093481416e-05,
      "grad_norm": 10993.327635434141,
      "learning_rate": 0.0001,
      "loss": 1.9985,
      "step": 9312
    },
    {
      "epoch": 3.391123044431812e-05,
      "grad_norm": 10343.740909361564,
      "learning_rate": 0.0001,
      "loss": 1.9928,
      "step": 9344
    },
    {
      "epoch": 3.402736479515483e-05,
      "grad_norm": 8252.799873376307,
      "learning_rate": 0.0001,
      "loss": 1.9843,
      "step": 9376
    },
    {
      "epoch": 3.4143499145991536e-05,
      "grad_norm": 6626.456519136001,
      "learning_rate": 0.0001,
      "loss": 2.0028,
      "step": 9408
    },
    {
      "epoch": 3.425963349682824e-05,
      "grad_norm": 4892.352195008041,
      "learning_rate": 0.0001,
      "loss": 1.989,
      "step": 9440
    },
    {
      "epoch": 3.437576784766495e-05,
      "grad_norm": 11357.49318731911,
      "learning_rate": 0.0001,
      "loss": 1.9949,
      "step": 9472
    },
    {
      "epoch": 3.449190219850165e-05,
      "grad_norm": 6459.340107549687,
      "learning_rate": 0.0001,
      "loss": 1.9793,
      "step": 9504
    },
    {
      "epoch": 3.4608036549338356e-05,
      "grad_norm": 5264.228219786828,
      "learning_rate": 0.0001,
      "loss": 1.9243,
      "step": 9536
    },
    {
      "epoch": 3.472417090017506e-05,
      "grad_norm": 9461.105960721505,
      "learning_rate": 0.0001,
      "loss": 1.9282,
      "step": 9568
    },
    {
      "epoch": 3.484030525101177e-05,
      "grad_norm": 7046.354163679257,
      "learning_rate": 0.0001,
      "loss": 1.9469,
      "step": 9600
    },
    {
      "epoch": 3.4956439601848476e-05,
      "grad_norm": 8711.152435240701,
      "learning_rate": 0.0001,
      "loss": 1.9176,
      "step": 9632
    },
    {
      "epoch": 3.507257395268518e-05,
      "grad_norm": 6237.586592585309,
      "learning_rate": 0.0001,
      "loss": 1.8922,
      "step": 9664
    },
    {
      "epoch": 3.518870830352189e-05,
      "grad_norm": 7333.434393243046,
      "learning_rate": 0.0001,
      "loss": 1.9049,
      "step": 9696
    },
    {
      "epoch": 3.5304842654358596e-05,
      "grad_norm": 5894.741385336595,
      "learning_rate": 0.0001,
      "loss": 1.9283,
      "step": 9728
    },
    {
      "epoch": 3.54209770051953e-05,
      "grad_norm": 8139.839679010884,
      "learning_rate": 0.0001,
      "loss": 1.953,
      "step": 9760
    },
    {
      "epoch": 3.5537111356032e-05,
      "grad_norm": 13547.099099069144,
      "learning_rate": 0.0001,
      "loss": 1.9172,
      "step": 9792
    },
    {
      "epoch": 3.565324570686871e-05,
      "grad_norm": 11891.228195607046,
      "learning_rate": 0.0001,
      "loss": 1.8989,
      "step": 9824
    },
    {
      "epoch": 3.5769380057705416e-05,
      "grad_norm": 11841.149944156608,
      "learning_rate": 0.0001,
      "loss": 1.914,
      "step": 9856
    },
    {
      "epoch": 3.588551440854212e-05,
      "grad_norm": 9462.854537611789,
      "learning_rate": 0.0001,
      "loss": 1.905,
      "step": 9888
    },
    {
      "epoch": 3.600164875937883e-05,
      "grad_norm": 12326.463401965708,
      "learning_rate": 0.0001,
      "loss": 1.9209,
      "step": 9920
    },
    {
      "epoch": 3.6117783110215536e-05,
      "grad_norm": 20852.57144814519,
      "learning_rate": 0.0001,
      "loss": 1.9202,
      "step": 9952
    },
    {
      "epoch": 3.623391746105224e-05,
      "grad_norm": 15668.550571128142,
      "learning_rate": 0.0001,
      "loss": 1.8957,
      "step": 9984
    },
    {
      "epoch": 3.635005181188895e-05,
      "grad_norm": 12767.800828647038,
      "learning_rate": 9.99800059980007e-07,
      "loss": 1.9118,
      "step": 10016
    },
    {
      "epoch": 3.646618616272565e-05,
      "grad_norm": 8591.33272548561,
      "learning_rate": 9.982048454657787e-07,
      "loss": 1.903,
      "step": 10048
    },
    {
      "epoch": 3.6582320513562356e-05,
      "grad_norm": 5879.11591993218,
      "learning_rate": 9.966172423210911e-07,
      "loss": 1.844,
      "step": 10080
    },
    {
      "epoch": 3.669845486439906e-05,
      "grad_norm": 5426.173237190275,
      "learning_rate": 9.950371902099892e-07,
      "loss": 1.8407,
      "step": 10112
    },
    {
      "epoch": 3.681458921523577e-05,
      "grad_norm": 5429.302625568039,
      "learning_rate": 9.934646294640047e-07,
      "loss": 1.8543,
      "step": 10144
    },
    {
      "epoch": 3.6930723566072476e-05,
      "grad_norm": 4471.081692387202,
      "learning_rate": 9.918995010726928e-07,
      "loss": 1.8578,
      "step": 10176
    },
    {
      "epoch": 3.704685791690918e-05,
      "grad_norm": 4939.131401370083,
      "learning_rate": 9.903417466743302e-07,
      "loss": 1.8649,
      "step": 10208
    },
    {
      "epoch": 3.716299226774589e-05,
      "grad_norm": 4566.186264269122,
      "learning_rate": 9.88791308546776e-07,
      "loss": 1.8767,
      "step": 10240
    },
    {
      "epoch": 3.7279126618582596e-05,
      "grad_norm": 4359.418195126501,
      "learning_rate": 9.872481295984873e-07,
      "loss": 1.8524,
      "step": 10272
    },
    {
      "epoch": 3.73952609694193e-05,
      "grad_norm": 5109.624643748306,
      "learning_rate": 9.85712153359689e-07,
      "loss": 1.8732,
      "step": 10304
    },
    {
      "epoch": 3.7511395320256e-05,
      "grad_norm": 5342.124343367534,
      "learning_rate": 9.841833239736953e-07,
      "loss": 1.8943,
      "step": 10336
    },
    {
      "epoch": 3.762752967109271e-05,
      "grad_norm": 5553.342416959357,
      "learning_rate": 9.82661586188378e-07,
      "loss": 1.8693,
      "step": 10368
    },
    {
      "epoch": 3.7743664021929416e-05,
      "grad_norm": 5692.346704128272,
      "learning_rate": 9.811468853477788e-07,
      "loss": 1.8477,
      "step": 10400
    },
    {
      "epoch": 3.785979837276612e-05,
      "grad_norm": 4949.559980442706,
      "learning_rate": 9.796391673838652e-07,
      "loss": 1.8173,
      "step": 10432
    },
    {
      "epoch": 3.797593272360283e-05,
      "grad_norm": 4947.146702898551,
      "learning_rate": 9.781383788084238e-07,
      "loss": 1.8383,
      "step": 10464
    },
    {
      "epoch": 3.8092067074439536e-05,
      "grad_norm": 4645.385236985196,
      "learning_rate": 9.7664446670509e-07,
      "loss": 1.8722,
      "step": 10496
    },
    {
      "epoch": 3.820820142527624e-05,
      "grad_norm": 5407.556102344201,
      "learning_rate": 9.751573787215115e-07,
      "loss": 1.8371,
      "step": 10528
    },
    {
      "epoch": 3.832433577611295e-05,
      "grad_norm": 5508.991286978044,
      "learning_rate": 9.736770630616433e-07,
      "loss": 1.8478,
      "step": 10560
    },
    {
      "epoch": 3.844047012694965e-05,
      "grad_norm": 5839.9708047215445,
      "learning_rate": 9.722034684781694e-07,
      "loss": 1.8643,
      "step": 10592
    },
    {
      "epoch": 3.8556604477786356e-05,
      "grad_norm": 5747.7397296676545,
      "learning_rate": 9.707365442650517e-07,
      "loss": 1.8792,
      "step": 10624
    },
    {
      "epoch": 3.867273882862306e-05,
      "grad_norm": 4278.391344886533,
      "learning_rate": 9.692762402502016e-07,
      "loss": 1.8666,
      "step": 10656
    },
    {
      "epoch": 3.878887317945977e-05,
      "grad_norm": 6041.881660542517,
      "learning_rate": 9.678225067882721e-07,
      "loss": 1.8605,
      "step": 10688
    },
    {
      "epoch": 3.8905007530296476e-05,
      "grad_norm": 4696.7383895635485,
      "learning_rate": 9.663752947535696e-07,
      "loss": 1.8468,
      "step": 10720
    },
    {
      "epoch": 3.902114188113318e-05,
      "grad_norm": 5702.050201462628,
      "learning_rate": 9.649345555330812e-07,
      "loss": 1.8642,
      "step": 10752
    },
    {
      "epoch": 3.913727623196989e-05,
      "grad_norm": 10613.930468963888,
      "learning_rate": 9.635002410196155e-07,
      "loss": 1.8508,
      "step": 10784
    },
    {
      "epoch": 3.9253410582806596e-05,
      "grad_norm": 10906.772666559067,
      "learning_rate": 9.620723036050563e-07,
      "loss": 1.8499,
      "step": 10816
    },
    {
      "epoch": 3.93695449336433e-05,
      "grad_norm": 9533.134217034814,
      "learning_rate": 9.60650696173725e-07,
      "loss": 1.8496,
      "step": 10848
    },
    {
      "epoch": 3.948567928448e-05,
      "grad_norm": 9724.994807196557,
      "learning_rate": 9.59235372095852e-07,
      "loss": 1.8233,
      "step": 10880
    },
    {
      "epoch": 3.960181363531671e-05,
      "grad_norm": 11131.417340123406,
      "learning_rate": 9.578262852211515e-07,
      "loss": 1.8475,
      "step": 10912
    },
    {
      "epoch": 3.9717947986153416e-05,
      "grad_norm": 9726.443954498478,
      "learning_rate": 9.564233898725014e-07,
      "loss": 1.8898,
      "step": 10944
    },
    {
      "epoch": 3.983408233699012e-05,
      "grad_norm": 10848.796430941084,
      "learning_rate": 9.550266408397246e-07,
      "loss": 1.841,
      "step": 10976
    },
    {
      "epoch": 3.995021668782683e-05,
      "grad_norm": 8704.614638224946,
      "learning_rate": 9.536359933734689e-07,
      "loss": 1.825,
      "step": 11008
    },
    {
      "epoch": 4.0066351038663536e-05,
      "grad_norm": 11354.616506073642,
      "learning_rate": 9.522514031791859e-07,
      "loss": 1.8374,
      "step": 11040
    },
    {
      "epoch": 4.018248538950024e-05,
      "grad_norm": 10805.221052805908,
      "learning_rate": 9.508728264112049e-07,
      "loss": 1.864,
      "step": 11072
    },
    {
      "epoch": 4.029861974033695e-05,
      "grad_norm": 9809.475215321154,
      "learning_rate": 9.495002196669013e-07,
      "loss": 1.8677,
      "step": 11104
    },
    {
      "epoch": 4.041475409117365e-05,
      "grad_norm": 10952.251549339067,
      "learning_rate": 9.481335399809572e-07,
      "loss": 1.8795,
      "step": 11136
    },
    {
      "epoch": 4.0530888442010356e-05,
      "grad_norm": 10235.984368882164,
      "learning_rate": 9.467727448197129e-07,
      "loss": 1.8802,
      "step": 11168
    },
    {
      "epoch": 4.064702279284706e-05,
      "grad_norm": 12907.898976983048,
      "learning_rate": 9.454177920756062e-07,
      "loss": 1.8878,
      "step": 11200
    },
    {
      "epoch": 4.076315714368377e-05,
      "grad_norm": 10790.377194519198,
      "learning_rate": 9.440686400617013e-07,
      "loss": 1.8543,
      "step": 11232
    },
    {
      "epoch": 4.0879291494520476e-05,
      "grad_norm": 12416.475828511082,
      "learning_rate": 9.427252475063007e-07,
      "loss": 1.8699,
      "step": 11264
    },
    {
      "epoch": 4.099542584535718e-05,
      "grad_norm": 8186.713015612554,
      "learning_rate": 9.413875735476427e-07,
      "loss": 1.8331,
      "step": 11296
    },
    {
      "epoch": 4.111156019619389e-05,
      "grad_norm": 11959.511026793696,
      "learning_rate": 9.400555777286816e-07,
      "loss": 1.8138,
      "step": 11328
    },
    {
      "epoch": 4.1227694547030596e-05,
      "grad_norm": 10862.981174613164,
      "learning_rate": 9.387292199919476e-07,
      "loss": 1.8518,
      "step": 11360
    },
    {
      "epoch": 4.13438288978673e-05,
      "grad_norm": 8790.45470951304,
      "learning_rate": 9.374084606744877e-07,
      "loss": 1.8502,
      "step": 11392
    },
    {
      "epoch": 4.1459963248704e-05,
      "grad_norm": 11112.632451404123,
      "learning_rate": 9.360932605028841e-07,
      "loss": 1.8204,
      "step": 11424
    },
    {
      "epoch": 4.157609759954071e-05,
      "grad_norm": 10016.66511369927,
      "learning_rate": 9.34783580588349e-07,
      "loss": 1.8267,
      "step": 11456
    },
    {
      "epoch": 4.1692231950377416e-05,
      "grad_norm": 9884.840312316634,
      "learning_rate": 9.334793824218948e-07,
      "loss": 1.8503,
      "step": 11488
    },
    {
      "epoch": 4.180836630121412e-05,
      "grad_norm": 9103.180652936642,
      "learning_rate": 9.321806278695785e-07,
      "loss": 1.8392,
      "step": 11520
    },
    {
      "epoch": 4.192450065205083e-05,
      "grad_norm": 8728.402602996724,
      "learning_rate": 9.308872791678188e-07,
      "loss": 1.8663,
      "step": 11552
    },
    {
      "epoch": 4.2040635002887536e-05,
      "grad_norm": 10994.572206320718,
      "learning_rate": 9.295992989187835e-07,
      "loss": 1.8632,
      "step": 11584
    },
    {
      "epoch": 4.215676935372424e-05,
      "grad_norm": 10237.175782411867,
      "learning_rate": 9.283166500858485e-07,
      "loss": 1.8473,
      "step": 11616
    },
    {
      "epoch": 4.227290370456095e-05,
      "grad_norm": 9769.24817987546,
      "learning_rate": 9.270392959891236e-07,
      "loss": 1.8459,
      "step": 11648
    },
    {
      "epoch": 4.2389038055397656e-05,
      "grad_norm": 11205.546394531593,
      "learning_rate": 9.257672003010474e-07,
      "loss": 1.8572,
      "step": 11680
    },
    {
      "epoch": 4.2505172406234356e-05,
      "grad_norm": 10854.247095031513,
      "learning_rate": 9.245003270420486e-07,
      "loss": 1.87,
      "step": 11712
    },
    {
      "epoch": 4.262130675707106e-05,
      "grad_norm": 9792.301465947625,
      "learning_rate": 9.232386405762704e-07,
      "loss": 1.8551,
      "step": 11744
    },
    {
      "epoch": 4.273744110790777e-05,
      "grad_norm": 9167.511003538528,
      "learning_rate": 9.219821056073614e-07,
      "loss": 1.8452,
      "step": 11776
    },
    {
      "epoch": 4.2853575458744476e-05,
      "grad_norm": 9576.716556315114,
      "learning_rate": 9.207697168976312e-07,
      "loss": 1.836,
      "step": 11808
    },
    {
      "epoch": 4.296970980958118e-05,
      "grad_norm": 9821.707947195335,
      "learning_rate": 9.195232220821039e-07,
      "loss": 1.874,
      "step": 11840
    },
    {
      "epoch": 4.308584416041789e-05,
      "grad_norm": 9465.380076890731,
      "learning_rate": 9.182817759372944e-07,
      "loss": 1.8436,
      "step": 11872
    },
    {
      "epoch": 4.3201978511254596e-05,
      "grad_norm": 8536.898968595095,
      "learning_rate": 9.170453444739675e-07,
      "loss": 1.8259,
      "step": 11904
    },
    {
      "epoch": 4.33181128620913e-05,
      "grad_norm": 10005.187254619475,
      "learning_rate": 9.158138940223838e-07,
      "loss": 1.8477,
      "step": 11936
    },
    {
      "epoch": 4.3434247212928e-05,
      "grad_norm": 10467.600775726976,
      "learning_rate": 9.145873912284495e-07,
      "loss": 1.8491,
      "step": 11968
    },
    {
      "epoch": 4.355038156376471e-05,
      "grad_norm": 8635.849350237648,
      "learning_rate": 9.133658030499223e-07,
      "loss": 1.8573,
      "step": 12000
    },
    {
      "epoch": 4.3666515914601416e-05,
      "grad_norm": 11022.999954640298,
      "learning_rate": 9.121490967526715e-07,
      "loss": 1.8669,
      "step": 12032
    },
    {
      "epoch": 4.378265026543812e-05,
      "grad_norm": 13902.190331023381,
      "learning_rate": 9.109372399069946e-07,
      "loss": 1.8693,
      "step": 12064
    },
    {
      "epoch": 4.389878461627483e-05,
      "grad_norm": 10048.817442863612,
      "learning_rate": 9.097302003839872e-07,
      "loss": 1.8379,
      "step": 12096
    },
    {
      "epoch": 4.4014918967111537e-05,
      "grad_norm": 10540.281495292238,
      "learning_rate": 9.085279463519642e-07,
      "loss": 1.8508,
      "step": 12128
    },
    {
      "epoch": 4.413105331794824e-05,
      "grad_norm": 12325.637833394263,
      "learning_rate": 9.073304462729352e-07,
      "loss": 1.8605,
      "step": 12160
    },
    {
      "epoch": 4.424718766878495e-05,
      "grad_norm": 9590.65659900301,
      "learning_rate": 9.061376688991289e-07,
      "loss": 1.8393,
      "step": 12192
    },
    {
      "epoch": 4.4363322019621657e-05,
      "grad_norm": 10535.104935405247,
      "learning_rate": 9.049495832695686e-07,
      "loss": 1.8237,
      "step": 12224
    },
    {
      "epoch": 4.4479456370458356e-05,
      "grad_norm": 9897.198997696267,
      "learning_rate": 9.037661587066969e-07,
      "loss": 1.8319,
      "step": 12256
    },
    {
      "epoch": 4.459559072129506e-05,
      "grad_norm": 12743.76616232423,
      "learning_rate": 9.025873648130485e-07,
      "loss": 1.8783,
      "step": 12288
    },
    {
      "epoch": 4.471172507213177e-05,
      "grad_norm": 9500.560194009615,
      "learning_rate": 9.01413171467971e-07,
      "loss": 1.8395,
      "step": 12320
    },
    {
      "epoch": 4.4827859422968477e-05,
      "grad_norm": 9775.840321936525,
      "learning_rate": 9.002435488243919e-07,
      "loss": 1.8486,
      "step": 12352
    },
    {
      "epoch": 4.494399377380518e-05,
      "grad_norm": 13272.169980828305,
      "learning_rate": 8.990784673056328e-07,
      "loss": 1.836,
      "step": 12384
    },
    {
      "epoch": 4.506012812464189e-05,
      "grad_norm": 9083.409271853823,
      "learning_rate": 8.979178976022672e-07,
      "loss": 1.8349,
      "step": 12416
    },
    {
      "epoch": 4.51762624754786e-05,
      "grad_norm": 10255.60042123327,
      "learning_rate": 8.967618106690242e-07,
      "loss": 1.8592,
      "step": 12448
    },
    {
      "epoch": 4.52923968263153e-05,
      "grad_norm": 8655.596224408808,
      "learning_rate": 8.956101777217353e-07,
      "loss": 1.8648,
      "step": 12480
    },
    {
      "epoch": 4.5408531177152e-05,
      "grad_norm": 11037.237516697736,
      "learning_rate": 8.944629702343244e-07,
      "loss": 1.8437,
      "step": 12512
    },
    {
      "epoch": 4.552466552798871e-05,
      "grad_norm": 11907.569357345772,
      "learning_rate": 8.933201599358393e-07,
      "loss": 1.8523,
      "step": 12544
    },
    {
      "epoch": 4.5640799878825417e-05,
      "grad_norm": 9795.927214919473,
      "learning_rate": 8.92181718807527e-07,
      "loss": 1.8342,
      "step": 12576
    },
    {
      "epoch": 4.575693422966212e-05,
      "grad_norm": 10163.15246367976,
      "learning_rate": 8.910476190799474e-07,
      "loss": 1.8638,
      "step": 12608
    },
    {
      "epoch": 4.587306858049883e-05,
      "grad_norm": 10120.788408024348,
      "learning_rate": 8.89917833230128e-07,
      "loss": 1.8319,
      "step": 12640
    },
    {
      "epoch": 4.598920293133554e-05,
      "grad_norm": 9238.27132097775,
      "learning_rate": 8.887923339787595e-07,
      "loss": 1.8218,
      "step": 12672
    },
    {
      "epoch": 4.610533728217224e-05,
      "grad_norm": 10092.7399649451,
      "learning_rate": 8.876710942874287e-07,
      "loss": 1.8437,
      "step": 12704
    },
    {
      "epoch": 4.622147163300895e-05,
      "grad_norm": 9822.811104770364,
      "learning_rate": 8.865540873558902e-07,
      "loss": 1.8597,
      "step": 12736
    },
    {
      "epoch": 4.633760598384566e-05,
      "grad_norm": 10649.300070896677,
      "learning_rate": 8.854412866193763e-07,
      "loss": 1.8351,
      "step": 12768
    },
    {
      "epoch": 4.645374033468236e-05,
      "grad_norm": 10707.643344826161,
      "learning_rate": 8.843672471386924e-07,
      "loss": 1.8422,
      "step": 12800
    },
    {
      "epoch": 4.656987468551906e-05,
      "grad_norm": 10933.47675718936,
      "learning_rate": 8.832626506143878e-07,
      "loss": 1.8385,
      "step": 12832
    },
    {
      "epoch": 4.668600903635577e-05,
      "grad_norm": 11129.902335600254,
      "learning_rate": 8.821621827824619e-07,
      "loss": 1.8515,
      "step": 12864
    },
    {
      "epoch": 4.680214338719248e-05,
      "grad_norm": 10370.75059964321,
      "learning_rate": 8.810658179868875e-07,
      "loss": 1.8939,
      "step": 12896
    },
    {
      "epoch": 4.691827773802918e-05,
      "grad_norm": 8360.058133769167,
      "learning_rate": 8.799735307942847e-07,
      "loss": 1.8831,
      "step": 12928
    },
    {
      "epoch": 4.703441208886589e-05,
      "grad_norm": 10362.950159100448,
      "learning_rate": 8.78885295991442e-07,
      "loss": 1.8513,
      "step": 12960
    },
    {
      "epoch": 4.71505464397026e-05,
      "grad_norm": 10459.178744050607,
      "learning_rate": 8.778010885828722e-07,
      "loss": 1.8448,
      "step": 12992
    },
    {
      "epoch": 4.7266680790539303e-05,
      "grad_norm": 10409.371738966767,
      "learning_rate": 8.76720883788401e-07,
      "loss": 1.8348,
      "step": 13024
    },
    {
      "epoch": 4.738281514137601e-05,
      "grad_norm": 11323.327779411844,
      "learning_rate": 8.75644657040788e-07,
      "loss": 1.8508,
      "step": 13056
    },
    {
      "epoch": 4.749894949221271e-05,
      "grad_norm": 9978.762047468614,
      "learning_rate": 8.745723839833802e-07,
      "loss": 1.8447,
      "step": 13088
    },
    {
      "epoch": 4.761508384304942e-05,
      "grad_norm": 9750.57772647344,
      "learning_rate": 8.735040404677968e-07,
      "loss": 1.8243,
      "step": 13120
    },
    {
      "epoch": 4.7731218193886123e-05,
      "grad_norm": 11339.081267898207,
      "learning_rate": 8.724396025516448e-07,
      "loss": 1.837,
      "step": 13152
    },
    {
      "epoch": 4.784735254472283e-05,
      "grad_norm": 9787.396282975365,
      "learning_rate": 8.713790464962657e-07,
      "loss": 1.8558,
      "step": 13184
    },
    {
      "epoch": 4.796348689555954e-05,
      "grad_norm": 10000.983651621473,
      "learning_rate": 8.703223487645115e-07,
      "loss": 1.8207,
      "step": 13216
    },
    {
      "epoch": 4.8079621246396243e-05,
      "grad_norm": 9318.210557827077,
      "learning_rate": 8.692694860185511e-07,
      "loss": 1.8101,
      "step": 13248
    },
    {
      "epoch": 4.819575559723295e-05,
      "grad_norm": 9185.179257913262,
      "learning_rate": 8.682204351177055e-07,
      "loss": 1.827,
      "step": 13280
    },
    {
      "epoch": 4.831188994806966e-05,
      "grad_norm": 12183.286092019673,
      "learning_rate": 8.671751731163108e-07,
      "loss": 1.8354,
      "step": 13312
    },
    {
      "epoch": 4.842802429890636e-05,
      "grad_norm": 10089.568870868567,
      "learning_rate": 8.661336772616117e-07,
      "loss": 1.8477,
      "step": 13344
    },
    {
      "epoch": 4.8544158649743063e-05,
      "grad_norm": 9205.619370797382,
      "learning_rate": 8.650959249916797e-07,
      "loss": 1.8573,
      "step": 13376
    },
    {
      "epoch": 4.866029300057977e-05,
      "grad_norm": 11243.981323356953,
      "learning_rate": 8.640618939333617e-07,
      "loss": 1.8604,
      "step": 13408
    },
    {
      "epoch": 4.877642735141648e-05,
      "grad_norm": 10507.317926093225,
      "learning_rate": 8.630315619002528e-07,
      "loss": 1.8577,
      "step": 13440
    },
    {
      "epoch": 4.8892561702253184e-05,
      "grad_norm": 9744.97234475296,
      "learning_rate": 8.62004906890698e-07,
      "loss": 1.8551,
      "step": 13472
    },
    {
      "epoch": 4.900869605308989e-05,
      "grad_norm": 9449.173297172616,
      "learning_rate": 8.609819070858184e-07,
      "loss": 1.8723,
      "step": 13504
    },
    {
      "epoch": 4.91248304039266e-05,
      "grad_norm": 11593.991374845851,
      "learning_rate": 8.599625408475633e-07,
      "loss": 1.8466,
      "step": 13536
    },
    {
      "epoch": 4.9240964754763304e-05,
      "grad_norm": 9916.692492963568,
      "learning_rate": 8.589467867167886e-07,
      "loss": 1.8214,
      "step": 13568
    },
    {
      "epoch": 4.935709910560001e-05,
      "grad_norm": 10185.534644779329,
      "learning_rate": 8.579346234113591e-07,
      "loss": 1.848,
      "step": 13600
    },
    {
      "epoch": 4.947323345643671e-05,
      "grad_norm": 10435.034067984638,
      "learning_rate": 8.569260298242756e-07,
      "loss": 1.8516,
      "step": 13632
    },
    {
      "epoch": 4.958936780727342e-05,
      "grad_norm": 9962.973652479464,
      "learning_rate": 8.55920985021826e-07,
      "loss": 1.8131,
      "step": 13664
    },
    {
      "epoch": 4.9705502158110124e-05,
      "grad_norm": 9643.938925563558,
      "learning_rate": 8.549194682417608e-07,
      "loss": 1.8476,
      "step": 13696
    },
    {
      "epoch": 4.982163650894683e-05,
      "grad_norm": 9899.39594116732,
      "learning_rate": 8.53921458891492e-07,
      "loss": 1.8503,
      "step": 13728
    },
    {
      "epoch": 4.993777085978354e-05,
      "grad_norm": 9742.34078648453,
      "learning_rate": 8.529269365463138e-07,
      "loss": 1.8477,
      "step": 13760
    },
    {
      "epoch": 5.0053905210620244e-05,
      "grad_norm": 9106.185590026156,
      "learning_rate": 8.519667991599624e-07,
      "loss": 1.8581,
      "step": 13792
    },
    {
      "epoch": 5.017003956145695e-05,
      "grad_norm": 8776.47514666338,
      "learning_rate": 8.509790828082779e-07,
      "loss": 1.8451,
      "step": 13824
    },
    {
      "epoch": 5.028617391229366e-05,
      "grad_norm": 12213.201218353852,
      "learning_rate": 8.499947937978321e-07,
      "loss": 1.8363,
      "step": 13856
    },
    {
      "epoch": 5.040230826313036e-05,
      "grad_norm": 10391.334466756423,
      "learning_rate": 8.490139123531099e-07,
      "loss": 1.8426,
      "step": 13888
    },
    {
      "epoch": 5.0518442613967064e-05,
      "grad_norm": 10180.276617066946,
      "learning_rate": 8.480364188579739e-07,
      "loss": 1.8353,
      "step": 13920
    },
    {
      "epoch": 5.063457696480377e-05,
      "grad_norm": 9448.887553569466,
      "learning_rate": 8.470622938540152e-07,
      "loss": 1.8449,
      "step": 13952
    },
    {
      "epoch": 5.075071131564048e-05,
      "grad_norm": 8310.823906208096,
      "learning_rate": 8.460915180389269e-07,
      "loss": 1.8267,
      "step": 13984
    },
    {
      "epoch": 5.0866845666477184e-05,
      "grad_norm": 11446.211600350573,
      "learning_rate": 8.451240722648987e-07,
      "loss": 1.8425,
      "step": 14016
    },
    {
      "epoch": 5.098298001731389e-05,
      "grad_norm": 10985.48797277572,
      "learning_rate": 8.441599375370293e-07,
      "loss": 1.8694,
      "step": 14048
    },
    {
      "epoch": 5.10991143681506e-05,
      "grad_norm": 10063.695841985687,
      "learning_rate": 8.431990950117611e-07,
      "loss": 1.8633,
      "step": 14080
    },
    {
      "epoch": 5.1215248718987304e-05,
      "grad_norm": 9363.139857974995,
      "learning_rate": 8.42241525995332e-07,
      "loss": 1.8181,
      "step": 14112
    },
    {
      "epoch": 5.133138306982401e-05,
      "grad_norm": 11386.258560211952,
      "learning_rate": 8.412872119422498e-07,
      "loss": 1.8254,
      "step": 14144
    },
    {
      "epoch": 5.144751742066071e-05,
      "grad_norm": 10122.297466484573,
      "learning_rate": 8.403361344537815e-07,
      "loss": 1.8182,
      "step": 14176
    },
    {
      "epoch": 5.156365177149742e-05,
      "grad_norm": 10054.888363378283,
      "learning_rate": 8.393882752764651e-07,
      "loss": 1.8366,
      "step": 14208
    },
    {
      "epoch": 5.1679786122334124e-05,
      "grad_norm": 10648.396217271407,
      "learning_rate": 8.384436163006371e-07,
      "loss": 1.8451,
      "step": 14240
    },
    {
      "epoch": 5.179592047317083e-05,
      "grad_norm": 9742.041367187885,
      "learning_rate": 8.375021395589801e-07,
      "loss": 1.8477,
      "step": 14272
    },
    {
      "epoch": 5.191205482400754e-05,
      "grad_norm": 11393.923907065555,
      "learning_rate": 8.365638272250869e-07,
      "loss": 1.871,
      "step": 14304
    },
    {
      "epoch": 5.2028189174844244e-05,
      "grad_norm": 12283.75138139811,
      "learning_rate": 8.35628661612043e-07,
      "loss": 1.8563,
      "step": 14336
    },
    {
      "epoch": 5.214432352568095e-05,
      "grad_norm": 10091.717296872717,
      "learning_rate": 8.346966251710266e-07,
      "loss": 1.8389,
      "step": 14368
    },
    {
      "epoch": 5.226045787651766e-05,
      "grad_norm": 9875.999189955415,
      "learning_rate": 8.33767700489925e-07,
      "loss": 1.8469,
      "step": 14400
    },
    {
      "epoch": 5.2376592227354364e-05,
      "grad_norm": 9718.941917719232,
      "learning_rate": 8.328418702919685e-07,
      "loss": 1.8305,
      "step": 14432
    },
    {
      "epoch": 5.2492726578191064e-05,
      "grad_norm": 10965.842968053117,
      "learning_rate": 8.319191174343811e-07,
      "loss": 1.8198,
      "step": 14464
    },
    {
      "epoch": 5.260886092902777e-05,
      "grad_norm": 9495.057029844527,
      "learning_rate": 8.30999424907047e-07,
      "loss": 1.845,
      "step": 14496
    },
    {
      "epoch": 5.272499527986448e-05,
      "grad_norm": 10146.88636971953,
      "learning_rate": 8.300827758311938e-07,
      "loss": 1.8446,
      "step": 14528
    },
    {
      "epoch": 5.2841129630701184e-05,
      "grad_norm": 9964.878022334244,
      "learning_rate": 8.291691534580914e-07,
      "loss": 1.8098,
      "step": 14560
    },
    {
      "epoch": 5.295726398153789e-05,
      "grad_norm": 10013.951068384546,
      "learning_rate": 8.282585411677661e-07,
      "loss": 1.8378,
      "step": 14592
    },
    {
      "epoch": 5.30733983323746e-05,
      "grad_norm": 11259.349537162438,
      "learning_rate": 8.273509224677321e-07,
      "loss": 1.8737,
      "step": 14624
    },
    {
      "epoch": 5.3189532683211304e-05,
      "grad_norm": 9513.798978326167,
      "learning_rate": 8.264462809917356e-07,
      "loss": 1.8634,
      "step": 14656
    },
    {
      "epoch": 5.330566703404801e-05,
      "grad_norm": 10844.11895914094,
      "learning_rate": 8.255446004985157e-07,
      "loss": 1.865,
      "step": 14688
    },
    {
      "epoch": 5.342180138488471e-05,
      "grad_norm": 13146.547835838883,
      "learning_rate": 8.246458648705801e-07,
      "loss": 1.8544,
      "step": 14720
    },
    {
      "epoch": 5.353793573572142e-05,
      "grad_norm": 10484.733091500231,
      "learning_rate": 8.237500581129945e-07,
      "loss": 1.8336,
      "step": 14752
    },
    {
      "epoch": 5.3654070086558124e-05,
      "grad_norm": 10235.834504328408,
      "learning_rate": 8.228571643521874e-07,
      "loss": 1.8367,
      "step": 14784
    },
    {
      "epoch": 5.377020443739483e-05,
      "grad_norm": 9137.068785994774,
      "learning_rate": 8.219949365267865e-07,
      "loss": 1.8337,
      "step": 14816
    },
    {
      "epoch": 5.388633878823154e-05,
      "grad_norm": 11032.85652041211,
      "learning_rate": 8.211077318035777e-07,
      "loss": 1.8443,
      "step": 14848
    },
    {
      "epoch": 5.4002473139068244e-05,
      "grad_norm": 9758.641093922863,
      "learning_rate": 8.202233936559833e-07,
      "loss": 1.8274,
      "step": 14880
    },
    {
      "epoch": 5.411860748990495e-05,
      "grad_norm": 11350.098501775215,
      "learning_rate": 8.193419066805777e-07,
      "loss": 1.8449,
      "step": 14912
    },
    {
      "epoch": 5.423474184074166e-05,
      "grad_norm": 9721.275327856936,
      "learning_rate": 8.18463255589564e-07,
      "loss": 1.8568,
      "step": 14944
    },
    {
      "epoch": 5.4350876191578364e-05,
      "grad_norm": 10579.480232979313,
      "learning_rate": 8.175874252096607e-07,
      "loss": 1.8188,
      "step": 14976
    },
    {
      "epoch": 5.4467010542415064e-05,
      "grad_norm": 9646.387095695465,
      "learning_rate": 8.167144004810015e-07,
      "loss": 1.8026,
      "step": 15008
    },
    {
      "epoch": 5.458314489325177e-05,
      "grad_norm": 9625.590475394223,
      "learning_rate": 8.15844166456047e-07,
      "loss": 1.8186,
      "step": 15040
    },
    {
      "epoch": 5.469927924408848e-05,
      "grad_norm": 8958.9258284685,
      "learning_rate": 8.149767082985105e-07,
      "loss": 1.8177,
      "step": 15072
    },
    {
      "epoch": 5.4815413594925184e-05,
      "grad_norm": 10157.979031283732,
      "learning_rate": 8.141120112822951e-07,
      "loss": 1.8307,
      "step": 15104
    },
    {
      "epoch": 5.493154794576189e-05,
      "grad_norm": 12084.653077353938,
      "learning_rate": 8.132500607904444e-07,
      "loss": 1.8526,
      "step": 15136
    },
    {
      "epoch": 5.50476822965986e-05,
      "grad_norm": 9715.074780978272,
      "learning_rate": 8.123908423141034e-07,
      "loss": 1.8456,
      "step": 15168
    },
    {
      "epoch": 5.5163816647435304e-05,
      "grad_norm": 9888.716094620171,
      "learning_rate": 8.115343414514944e-07,
      "loss": 1.8592,
      "step": 15200
    },
    {
      "epoch": 5.527995099827201e-05,
      "grad_norm": 10567.146256203707,
      "learning_rate": 8.106805439069019e-07,
      "loss": 1.8738,
      "step": 15232
    },
    {
      "epoch": 5.539608534910871e-05,
      "grad_norm": 10053.140205925709,
      "learning_rate": 8.098294354896712e-07,
      "loss": 1.87,
      "step": 15264
    },
    {
      "epoch": 5.551221969994542e-05,
      "grad_norm": 10255.628893441883,
      "learning_rate": 8.08981002113217e-07,
      "loss": 1.8666,
      "step": 15296
    },
    {
      "epoch": 5.5628354050782124e-05,
      "grad_norm": 8537.280421773668,
      "learning_rate": 8.081352297940449e-07,
      "loss": 1.813,
      "step": 15328
    },
    {
      "epoch": 5.574448840161883e-05,
      "grad_norm": 10372.16650464116,
      "learning_rate": 8.072921046507833e-07,
      "loss": 1.8191,
      "step": 15360
    },
    {
      "epoch": 5.586062275245554e-05,
      "grad_norm": 9814.089361728882,
      "learning_rate": 8.064516129032258e-07,
      "loss": 1.8485,
      "step": 15392
    },
    {
      "epoch": 5.5976757103292244e-05,
      "grad_norm": 9922.589682134396,
      "learning_rate": 8.056137408713863e-07,
      "loss": 1.8245,
      "step": 15424
    },
    {
      "epoch": 5.609289145412895e-05,
      "grad_norm": 9747.338918904996,
      "learning_rate": 8.047784749745631e-07,
      "loss": 1.8233,
      "step": 15456
    },
    {
      "epoch": 5.620902580496566e-05,
      "grad_norm": 10137.664326658287,
      "learning_rate": 8.039458017304144e-07,
      "loss": 1.8531,
      "step": 15488
    },
    {
      "epoch": 5.6325160155802364e-05,
      "grad_norm": 11155.779488677606,
      "learning_rate": 8.031157077540445e-07,
      "loss": 1.8401,
      "step": 15520
    },
    {
      "epoch": 5.6441294506639064e-05,
      "grad_norm": 10539.273789023608,
      "learning_rate": 8.022881797571e-07,
      "loss": 1.8519,
      "step": 15552
    },
    {
      "epoch": 5.655742885747577e-05,
      "grad_norm": 10395.860329958266,
      "learning_rate": 8.014632045468768e-07,
      "loss": 1.8421,
      "step": 15584
    },
    {
      "epoch": 5.667356320831248e-05,
      "grad_norm": 10671.360831684027,
      "learning_rate": 8.006407690254357e-07,
      "loss": 1.8293,
      "step": 15616
    },
    {
      "epoch": 5.6789697559149184e-05,
      "grad_norm": 10531.457638902604,
      "learning_rate": 7.9982086018873e-07,
      "loss": 1.8527,
      "step": 15648
    },
    {
      "epoch": 5.690583190998589e-05,
      "grad_norm": 9329.291077032594,
      "learning_rate": 7.990034651257415e-07,
      "loss": 1.827,
      "step": 15680
    },
    {
      "epoch": 5.70219662608226e-05,
      "grad_norm": 10526.398054415386,
      "learning_rate": 7.981885710176262e-07,
      "loss": 1.8368,
      "step": 15712
    },
    {
      "epoch": 5.7138100611659304e-05,
      "grad_norm": 11449.263731786425,
      "learning_rate": 7.97376165136871e-07,
      "loss": 1.8452,
      "step": 15744
    },
    {
      "epoch": 5.725423496249601e-05,
      "grad_norm": 10360.951886771792,
      "learning_rate": 7.965662348464577e-07,
      "loss": 1.8271,
      "step": 15776
    },
    {
      "epoch": 5.737036931333272e-05,
      "grad_norm": 11274.664340901683,
      "learning_rate": 7.957839637921133e-07,
      "loss": 1.8259,
      "step": 15808
    },
    {
      "epoch": 5.748650366416942e-05,
      "grad_norm": 9451.442535401673,
      "learning_rate": 7.949788707361304e-07,
      "loss": 1.8746,
      "step": 15840
    },
    {
      "epoch": 5.7602638015006124e-05,
      "grad_norm": 9722.536706024823,
      "learning_rate": 7.94176216279449e-07,
      "loss": 1.8408,
      "step": 15872
    },
    {
      "epoch": 5.771877236584283e-05,
      "grad_norm": 10514.799284817565,
      "learning_rate": 7.933759881361526e-07,
      "loss": 1.8149,
      "step": 15904
    },
    {
      "epoch": 5.783490671667954e-05,
      "grad_norm": 9785.8394632244,
      "learning_rate": 7.925781741068082e-07,
      "loss": 1.8063,
      "step": 15936
    },
    {
      "epoch": 5.7951041067516244e-05,
      "grad_norm": 11188.475141859146,
      "learning_rate": 7.917827620776833e-07,
      "loss": 1.8257,
      "step": 15968
    },
    {
      "epoch": 5.806717541835295e-05,
      "grad_norm": 8854.495355467752,
      "learning_rate": 7.909897400199752e-07,
      "loss": 1.8387,
      "step": 16000
    },
    {
      "epoch": 5.818330976918966e-05,
      "grad_norm": 10208.538680927843,
      "learning_rate": 7.901990959890454e-07,
      "loss": 1.8509,
      "step": 16032
    },
    {
      "epoch": 5.8299444120026364e-05,
      "grad_norm": 11389.02840456551,
      "learning_rate": 7.894108181236647e-07,
      "loss": 1.8537,
      "step": 16064
    },
    {
      "epoch": 5.8415578470863064e-05,
      "grad_norm": 11821.753592424433,
      "learning_rate": 7.886248946452661e-07,
      "loss": 1.8477,
      "step": 16096
    },
    {
      "epoch": 5.853171282169977e-05,
      "grad_norm": 9960.057228751248,
      "learning_rate": 7.878413138572051e-07,
      "loss": 1.8504,
      "step": 16128
    },
    {
      "epoch": 5.864784717253648e-05,
      "grad_norm": 9402.50232650862,
      "learning_rate": 7.870600641440283e-07,
      "loss": 1.8616,
      "step": 16160
    },
    {
      "epoch": 5.8763981523373184e-05,
      "grad_norm": 10499.561514653838,
      "learning_rate": 7.862811339707515e-07,
      "loss": 1.831,
      "step": 16192
    },
    {
      "epoch": 5.888011587420989e-05,
      "grad_norm": 8773.538966688413,
      "learning_rate": 7.855045118821426e-07,
      "loss": 1.8114,
      "step": 16224
    },
    {
      "epoch": 5.89962502250466e-05,
      "grad_norm": 10786.308729125085,
      "learning_rate": 7.847301865020156e-07,
      "loss": 1.835,
      "step": 16256
    },
    {
      "epoch": 5.9112384575883304e-05,
      "grad_norm": 11223.256746595438,
      "learning_rate": 7.839581465325298e-07,
      "loss": 1.8481,
      "step": 16288
    },
    {
      "epoch": 5.922851892672001e-05,
      "grad_norm": 10730.557115080279,
      "learning_rate": 7.831883807534978e-07,
      "loss": 1.8226,
      "step": 16320
    },
    {
      "epoch": 5.934465327755672e-05,
      "grad_norm": 9943.939159105912,
      "learning_rate": 7.824208780217005e-07,
      "loss": 1.8335,
      "step": 16352
    },
    {
      "epoch": 5.946078762839342e-05,
      "grad_norm": 11142.48356516625,
      "learning_rate": 7.816556272702094e-07,
      "loss": 1.8274,
      "step": 16384
    },
    {
      "epoch": 5.9576921979230124e-05,
      "grad_norm": 9560.365212689314,
      "learning_rate": 7.808926175077171e-07,
      "loss": 1.8367,
      "step": 16416
    },
    {
      "epoch": 5.969305633006683e-05,
      "grad_norm": 9511.074387260358,
      "learning_rate": 7.801318378178729e-07,
      "loss": 1.869,
      "step": 16448
    },
    {
      "epoch": 5.980919068090354e-05,
      "grad_norm": 10697.237493857936,
      "learning_rate": 7.793732773586288e-07,
      "loss": 1.8722,
      "step": 16480
    },
    {
      "epoch": 5.9925325031740244e-05,
      "grad_norm": 10463.648789977615,
      "learning_rate": 7.786169253615883e-07,
      "loss": 1.8414,
      "step": 16512
    },
    {
      "epoch": 6.004145938257695e-05,
      "grad_norm": 9787.47791823818,
      "learning_rate": 7.778627711313662e-07,
      "loss": 1.823,
      "step": 16544
    },
    {
      "epoch": 6.015759373341366e-05,
      "grad_norm": 13504.253848324977,
      "learning_rate": 7.771108040449519e-07,
      "loss": 1.8412,
      "step": 16576
    },
    {
      "epoch": 6.0273728084250364e-05,
      "grad_norm": 11198.751716151224,
      "learning_rate": 7.763610135510823e-07,
      "loss": 1.847,
      "step": 16608
    },
    {
      "epoch": 6.0389862435087064e-05,
      "grad_norm": 10099.112238211832,
      "learning_rate": 7.756133891696192e-07,
      "loss": 1.8478,
      "step": 16640
    },
    {
      "epoch": 6.050599678592377e-05,
      "grad_norm": 10329.066366327597,
      "learning_rate": 7.748679204909338e-07,
      "loss": 1.8096,
      "step": 16672
    },
    {
      "epoch": 6.062213113676048e-05,
      "grad_norm": 9459.262761970407,
      "learning_rate": 7.74124597175299e-07,
      "loss": 1.813,
      "step": 16704
    },
    {
      "epoch": 6.0738265487597184e-05,
      "grad_norm": 11328.58949737345,
      "learning_rate": 7.733834089522855e-07,
      "loss": 1.8446,
      "step": 16736
    },
    {
      "epoch": 6.085439983843389e-05,
      "grad_norm": 9825.471693511716,
      "learning_rate": 7.726443456201674e-07,
      "loss": 1.8182,
      "step": 16768
    },
    {
      "epoch": 6.09705341892706e-05,
      "grad_norm": 9695.014491995358,
      "learning_rate": 7.719303947779722e-07,
      "loss": 1.8008,
      "step": 16800
    },
    {
      "epoch": 6.10866685401073e-05,
      "grad_norm": 10248.851838132894,
      "learning_rate": 7.711954852744357e-07,
      "loss": 1.821,
      "step": 16832
    },
    {
      "epoch": 6.120280289094401e-05,
      "grad_norm": 9570.309921836388,
      "learning_rate": 7.704626707744303e-07,
      "loss": 1.8228,
      "step": 16864
    },
    {
      "epoch": 6.131893724178071e-05,
      "grad_norm": 10187.21787339409,
      "learning_rate": 7.697319413431387e-07,
      "loss": 1.8494,
      "step": 16896
    },
    {
      "epoch": 6.143507159261742e-05,
      "grad_norm": 9596.036994509765,
      "learning_rate": 7.690032871115763e-07,
      "loss": 1.8511,
      "step": 16928
    },
    {
      "epoch": 6.155120594345412e-05,
      "grad_norm": 12223.484773173319,
      "learning_rate": 7.6827669827603e-07,
      "loss": 1.849,
      "step": 16960
    },
    {
      "epoch": 6.166734029429084e-05,
      "grad_norm": 9314.470140593077,
      "learning_rate": 7.675521650975067e-07,
      "loss": 1.8517,
      "step": 16992
    },
    {
      "epoch": 6.178347464512754e-05,
      "grad_norm": 10020.939377124281,
      "learning_rate": 7.668296779011828e-07,
      "loss": 1.8601,
      "step": 17024
    },
    {
      "epoch": 6.189960899596424e-05,
      "grad_norm": 10780.405929277431,
      "learning_rate": 7.661092270758628e-07,
      "loss": 1.882,
      "step": 17056
    },
    {
      "epoch": 6.201574334680095e-05,
      "grad_norm": 12654.138295435214,
      "learning_rate": 7.65390803073442e-07,
      "loss": 1.8515,
      "step": 17088
    },
    {
      "epoch": 6.213187769763765e-05,
      "grad_norm": 9672.622188424399,
      "learning_rate": 7.646743964083746e-07,
      "loss": 1.8097,
      "step": 17120
    },
    {
      "epoch": 6.224801204847436e-05,
      "grad_norm": 9810.011722724901,
      "learning_rate": 7.639599976571487e-07,
      "loss": 1.8297,
      "step": 17152
    },
    {
      "epoch": 6.236414639931106e-05,
      "grad_norm": 10393.672017145818,
      "learning_rate": 7.632475974577645e-07,
      "loss": 1.864,
      "step": 17184
    },
    {
      "epoch": 6.248028075014778e-05,
      "grad_norm": 9233.280565432851,
      "learning_rate": 7.625371865092198e-07,
      "loss": 1.8252,
      "step": 17216
    },
    {
      "epoch": 6.259641510098448e-05,
      "grad_norm": 10490.23803352431,
      "learning_rate": 7.618287555709996e-07,
      "loss": 1.8134,
      "step": 17248
    },
    {
      "epoch": 6.271254945182119e-05,
      "grad_norm": 9467.084556504184,
      "learning_rate": 7.611222954625718e-07,
      "loss": 1.8157,
      "step": 17280
    },
    {
      "epoch": 6.282868380265789e-05,
      "grad_norm": 9787.419271697723,
      "learning_rate": 7.604177970628867e-07,
      "loss": 1.8208,
      "step": 17312
    },
    {
      "epoch": 6.294481815349459e-05,
      "grad_norm": 8547.034222465709,
      "learning_rate": 7.597152513098829e-07,
      "loss": 1.8348,
      "step": 17344
    },
    {
      "epoch": 6.30609525043313e-05,
      "grad_norm": 11207.975196260919,
      "learning_rate": 7.590146491999979e-07,
      "loss": 1.8595,
      "step": 17376
    },
    {
      "epoch": 6.3177086855168e-05,
      "grad_norm": 10282.676986077118,
      "learning_rate": 7.58315981787683e-07,
      "loss": 1.8324,
      "step": 17408
    },
    {
      "epoch": 6.329322120600472e-05,
      "grad_norm": 11692.007697568455,
      "learning_rate": 7.576192401849239e-07,
      "loss": 1.8385,
      "step": 17440
    },
    {
      "epoch": 6.340935555684142e-05,
      "grad_norm": 10377.755826767172,
      "learning_rate": 7.569244155607655e-07,
      "loss": 1.8342,
      "step": 17472
    },
    {
      "epoch": 6.352548990767813e-05,
      "grad_norm": 10370.098360189262,
      "learning_rate": 7.562314991408423e-07,
      "loss": 1.853,
      "step": 17504
    },
    {
      "epoch": 6.364162425851483e-05,
      "grad_norm": 10076.938920128474,
      "learning_rate": 7.555404822069124e-07,
      "loss": 1.8134,
      "step": 17536
    },
    {
      "epoch": 6.375775860935154e-05,
      "grad_norm": 11114.18589011359,
      "learning_rate": 7.548513560963972e-07,
      "loss": 1.8192,
      "step": 17568
    },
    {
      "epoch": 6.387389296018824e-05,
      "grad_norm": 10159.216308357649,
      "learning_rate": 7.541641122019254e-07,
      "loss": 1.8348,
      "step": 17600
    },
    {
      "epoch": 6.399002731102494e-05,
      "grad_norm": 11721.444450237352,
      "learning_rate": 7.534787419708808e-07,
      "loss": 1.8587,
      "step": 17632
    },
    {
      "epoch": 6.410616166186166e-05,
      "grad_norm": 9942.804332782578,
      "learning_rate": 7.527952369049562e-07,
      "loss": 1.8218,
      "step": 17664
    },
    {
      "epoch": 6.422229601269836e-05,
      "grad_norm": 8899.077705020898,
      "learning_rate": 7.521135885597101e-07,
      "loss": 1.8196,
      "step": 17696
    },
    {
      "epoch": 6.433843036353507e-05,
      "grad_norm": 10059.72703407006,
      "learning_rate": 7.514337885441285e-07,
      "loss": 1.8129,
      "step": 17728
    },
    {
      "epoch": 6.445456471437177e-05,
      "grad_norm": 10693.657372480193,
      "learning_rate": 7.50755828520192e-07,
      "loss": 1.835,
      "step": 17760
    },
    {
      "epoch": 6.457069906520848e-05,
      "grad_norm": 9693.828449070057,
      "learning_rate": 7.500797002024448e-07,
      "loss": 1.8512,
      "step": 17792
    },
    {
      "epoch": 6.468683341604518e-05,
      "grad_norm": 9696.752033541952,
      "learning_rate": 7.494264398639804e-07,
      "loss": 1.8553,
      "step": 17824
    },
    {
      "epoch": 6.48029677668819e-05,
      "grad_norm": 11527.948906895796,
      "learning_rate": 7.487538937058607e-07,
      "loss": 1.8356,
      "step": 17856
    },
    {
      "epoch": 6.49191021177186e-05,
      "grad_norm": 11557.3832678509,
      "learning_rate": 7.480831549620176e-07,
      "loss": 1.8498,
      "step": 17888
    },
    {
      "epoch": 6.50352364685553e-05,
      "grad_norm": 8576.247780935437,
      "learning_rate": 7.474142155514584e-07,
      "loss": 1.8311,
      "step": 17920
    },
    {
      "epoch": 6.515137081939201e-05,
      "grad_norm": 9429.940614871337,
      "learning_rate": 7.467470674436817e-07,
      "loss": 1.8488,
      "step": 17952
    },
    {
      "epoch": 6.526750517022871e-05,
      "grad_norm": 9439.95021173311,
      "learning_rate": 7.460817026582743e-07,
      "loss": 1.8383,
      "step": 17984
    },
    {
      "epoch": 6.538363952106542e-05,
      "grad_norm": 10815.249049374685,
      "learning_rate": 7.454181132645084e-07,
      "loss": 1.8222,
      "step": 18016
    },
    {
      "epoch": 6.549977387190212e-05,
      "grad_norm": 11703.916267643066,
      "learning_rate": 7.44756291380946e-07,
      "loss": 1.8407,
      "step": 18048
    },
    {
      "epoch": 6.561590822273884e-05,
      "grad_norm": 10474.987732689715,
      "learning_rate": 7.440962291750448e-07,
      "loss": 1.8509,
      "step": 18080
    },
    {
      "epoch": 6.573204257357554e-05,
      "grad_norm": 10403.290729379814,
      "learning_rate": 7.4343791886277e-07,
      "loss": 1.8033,
      "step": 18112
    },
    {
      "epoch": 6.584817692441225e-05,
      "grad_norm": 8467.3945225199,
      "learning_rate": 7.427813527082075e-07,
      "loss": 1.8106,
      "step": 18144
    },
    {
      "epoch": 6.596431127524895e-05,
      "grad_norm": 9975.289669979515,
      "learning_rate": 7.421265230231825e-07,
      "loss": 1.8213,
      "step": 18176
    },
    {
      "epoch": 6.608044562608565e-05,
      "grad_norm": 12565.74430744156,
      "learning_rate": 7.41473422166882e-07,
      "loss": 1.8386,
      "step": 18208
    },
    {
      "epoch": 6.619657997692236e-05,
      "grad_norm": 10356.472662060187,
      "learning_rate": 7.408220425454791e-07,
      "loss": 1.8615,
      "step": 18240
    },
    {
      "epoch": 6.631271432775906e-05,
      "grad_norm": 10067.479922999599,
      "learning_rate": 7.401723766117641e-07,
      "loss": 1.845,
      "step": 18272
    },
    {
      "epoch": 6.642884867859578e-05,
      "grad_norm": 11924.747712215969,
      "learning_rate": 7.395244168647747e-07,
      "loss": 1.8468,
      "step": 18304
    },
    {
      "epoch": 6.654498302943248e-05,
      "grad_norm": 10046.46186475617,
      "learning_rate": 7.388781558494347e-07,
      "loss": 1.8466,
      "step": 18336
    },
    {
      "epoch": 6.666111738026919e-05,
      "grad_norm": 10064.420301239412,
      "learning_rate": 7.382335861561919e-07,
      "loss": 1.8285,
      "step": 18368
    },
    {
      "epoch": 6.677725173110589e-05,
      "grad_norm": 9436.642835246017,
      "learning_rate": 7.375907004206624e-07,
      "loss": 1.8417,
      "step": 18400
    },
    {
      "epoch": 6.689338608194259e-05,
      "grad_norm": 8490.434146732427,
      "learning_rate": 7.369494913232764e-07,
      "loss": 1.8248,
      "step": 18432
    },
    {
      "epoch": 6.70095204327793e-05,
      "grad_norm": 8812.895778346638,
      "learning_rate": 7.363099515889292e-07,
      "loss": 1.7926,
      "step": 18464
    },
    {
      "epoch": 6.7125654783616e-05,
      "grad_norm": 9861.36613253965,
      "learning_rate": 7.356720739866336e-07,
      "loss": 1.8236,
      "step": 18496
    },
    {
      "epoch": 6.724178913445272e-05,
      "grad_norm": 11360.484144612852,
      "learning_rate": 7.35035851329177e-07,
      "loss": 1.8223,
      "step": 18528
    },
    {
      "epoch": 6.735792348528942e-05,
      "grad_norm": 9853.054957727578,
      "learning_rate": 7.344012764727807e-07,
      "loss": 1.7958,
      "step": 18560
    },
    {
      "epoch": 6.747405783612613e-05,
      "grad_norm": 10109.564184474028,
      "learning_rate": 7.337683423167642e-07,
      "loss": 1.8302,
      "step": 18592
    },
    {
      "epoch": 6.759019218696283e-05,
      "grad_norm": 10030.009571281575,
      "learning_rate": 7.331370418032097e-07,
      "loss": 1.8345,
      "step": 18624
    },
    {
      "epoch": 6.770632653779954e-05,
      "grad_norm": 9457.217243988847,
      "learning_rate": 7.325073679166336e-07,
      "loss": 1.8299,
      "step": 18656
    },
    {
      "epoch": 6.782246088863624e-05,
      "grad_norm": 9344.07309474835,
      "learning_rate": 7.318793136836571e-07,
      "loss": 1.8423,
      "step": 18688
    },
    {
      "epoch": 6.793859523947294e-05,
      "grad_norm": 8840.458924739145,
      "learning_rate": 7.312528721726834e-07,
      "loss": 1.8565,
      "step": 18720
    },
    {
      "epoch": 6.805472959030966e-05,
      "grad_norm": 9568.016304333934,
      "learning_rate": 7.306280364935755e-07,
      "loss": 1.8388,
      "step": 18752
    },
    {
      "epoch": 6.817086394114636e-05,
      "grad_norm": 11001.828302604981,
      "learning_rate": 7.300047997973381e-07,
      "loss": 1.8537,
      "step": 18784
    },
    {
      "epoch": 6.828699829198307e-05,
      "grad_norm": 10325.192298451395,
      "learning_rate": 7.294025576341292e-07,
      "loss": 1.8483,
      "step": 18816
    },
    {
      "epoch": 6.840313264281977e-05,
      "grad_norm": 10203.376499963137,
      "learning_rate": 7.287824490777102e-07,
      "loss": 1.8554,
      "step": 18848
    },
    {
      "epoch": 6.851926699365649e-05,
      "grad_norm": 8726.489213882063,
      "learning_rate": 7.281639194105152e-07,
      "loss": 1.8184,
      "step": 18880
    },
    {
      "epoch": 6.863540134449318e-05,
      "grad_norm": 10406.902324899567,
      "learning_rate": 7.2754696194375e-07,
      "loss": 1.8388,
      "step": 18912
    },
    {
      "epoch": 6.87515356953299e-05,
      "grad_norm": 9840.30324735981,
      "learning_rate": 7.269315700282241e-07,
      "loss": 1.8435,
      "step": 18944
    },
    {
      "epoch": 6.88676700461666e-05,
      "grad_norm": 11232.637624351637,
      "learning_rate": 7.263177370540502e-07,
      "loss": 1.8291,
      "step": 18976
    },
    {
      "epoch": 6.89838043970033e-05,
      "grad_norm": 9581.670000579232,
      "learning_rate": 7.257054564503449e-07,
      "loss": 1.8029,
      "step": 19008
    },
    {
      "epoch": 6.909993874784001e-05,
      "grad_norm": 10435.535252204365,
      "learning_rate": 7.250947216849342e-07,
      "loss": 1.8063,
      "step": 19040
    },
    {
      "epoch": 6.921607309867671e-05,
      "grad_norm": 10252.21000565244,
      "learning_rate": 7.244855262640613e-07,
      "loss": 1.8118,
      "step": 19072
    },
    {
      "epoch": 6.933220744951343e-05,
      "grad_norm": 10808.259434340018,
      "learning_rate": 7.23877863732095e-07,
      "loss": 1.8229,
      "step": 19104
    },
    {
      "epoch": 6.944834180035012e-05,
      "grad_norm": 10387.904119696139,
      "learning_rate": 7.232717276712438e-07,
      "loss": 1.8383,
      "step": 19136
    },
    {
      "epoch": 6.956447615118684e-05,
      "grad_norm": 9871.576773747951,
      "learning_rate": 7.226671117012703e-07,
      "loss": 1.8422,
      "step": 19168
    },
    {
      "epoch": 6.968061050202354e-05,
      "grad_norm": 9855.15459036539,
      "learning_rate": 7.220640094792102e-07,
      "loss": 1.8569,
      "step": 19200
    },
    {
      "epoch": 6.979674485286025e-05,
      "grad_norm": 9580.75560694458,
      "learning_rate": 7.214624146990914e-07,
      "loss": 1.8503,
      "step": 19232
    },
    {
      "epoch": 6.991287920369695e-05,
      "grad_norm": 10986.95799573294,
      "learning_rate": 7.208623210916582e-07,
      "loss": 1.8224,
      "step": 19264
    },
    {
      "epoch": 7.002901355453365e-05,
      "grad_norm": 9189.476372459967,
      "learning_rate": 7.202637224240964e-07,
      "loss": 1.837,
      "step": 19296
    },
    {
      "epoch": 7.014514790537037e-05,
      "grad_norm": 8908.679587907514,
      "learning_rate": 7.196666124997617e-07,
      "loss": 1.8134,
      "step": 19328
    },
    {
      "epoch": 7.026128225620706e-05,
      "grad_norm": 8469.993270363324,
      "learning_rate": 7.190709851579098e-07,
      "loss": 1.8053,
      "step": 19360
    },
    {
      "epoch": 7.037741660704378e-05,
      "grad_norm": 11294.72540613538,
      "learning_rate": 7.184768342734299e-07,
      "loss": 1.8434,
      "step": 19392
    },
    {
      "epoch": 7.049355095788048e-05,
      "grad_norm": 8739.458678888526,
      "learning_rate": 7.178841537565801e-07,
      "loss": 1.8448,
      "step": 19424
    },
    {
      "epoch": 7.060968530871719e-05,
      "grad_norm": 9997.669028328553,
      "learning_rate": 7.172929375527248e-07,
      "loss": 1.7981,
      "step": 19456
    },
    {
      "epoch": 7.072581965955389e-05,
      "grad_norm": 9537.678229003115,
      "learning_rate": 7.167031796420754e-07,
      "loss": 1.8115,
      "step": 19488
    },
    {
      "epoch": 7.08419540103906e-05,
      "grad_norm": 10215.158148555507,
      "learning_rate": 7.161148740394328e-07,
      "loss": 1.8417,
      "step": 19520
    },
    {
      "epoch": 7.09580883612273e-05,
      "grad_norm": 9932.44330464564,
      "learning_rate": 7.155280147939323e-07,
      "loss": 1.8335,
      "step": 19552
    },
    {
      "epoch": 7.1074222712064e-05,
      "grad_norm": 8079.167655148642,
      "learning_rate": 7.149425959887901e-07,
      "loss": 1.8551,
      "step": 19584
    },
    {
      "epoch": 7.119035706290072e-05,
      "grad_norm": 9116.314935323375,
      "learning_rate": 7.143586117410541e-07,
      "loss": 1.8428,
      "step": 19616
    },
    {
      "epoch": 7.130649141373742e-05,
      "grad_norm": 10908.382189857486,
      "learning_rate": 7.137760562013538e-07,
      "loss": 1.8369,
      "step": 19648
    },
    {
      "epoch": 7.142262576457413e-05,
      "grad_norm": 10207.783598803415,
      "learning_rate": 7.131949235536565e-07,
      "loss": 1.8301,
      "step": 19680
    },
    {
      "epoch": 7.153876011541083e-05,
      "grad_norm": 10042.24865256781,
      "learning_rate": 7.126152080150215e-07,
      "loss": 1.8254,
      "step": 19712
    },
    {
      "epoch": 7.165489446624755e-05,
      "grad_norm": 11390.728422712922,
      "learning_rate": 7.120369038353586e-07,
      "loss": 1.8389,
      "step": 19744
    },
    {
      "epoch": 7.177102881708425e-05,
      "grad_norm": 9124.144233844618,
      "learning_rate": 7.114600052971897e-07,
      "loss": 1.8266,
      "step": 19776
    },
    {
      "epoch": 7.188716316792095e-05,
      "grad_norm": 11722.444284363222,
      "learning_rate": 7.109024699115808e-07,
      "loss": 1.822,
      "step": 19808
    },
    {
      "epoch": 7.200329751875766e-05,
      "grad_norm": 10412.45235283216,
      "learning_rate": 7.103283221464939e-07,
      "loss": 1.8499,
      "step": 19840
    },
    {
      "epoch": 7.211943186959436e-05,
      "grad_norm": 9714.525207131845,
      "learning_rate": 7.097555632389488e-07,
      "loss": 1.8177,
      "step": 19872
    },
    {
      "epoch": 7.223556622043107e-05,
      "grad_norm": 10906.382901769037,
      "learning_rate": 7.091841875985731e-07,
      "loss": 1.7975,
      "step": 19904
    },
    {
      "epoch": 7.235170057126777e-05,
      "grad_norm": 10662.471008166916,
      "learning_rate": 7.086141896664472e-07,
      "loss": 1.8136,
      "step": 19936
    },
    {
      "epoch": 7.246783492210449e-05,
      "grad_norm": 10660.904089241212,
      "learning_rate": 7.080455639148767e-07,
      "loss": 1.8324,
      "step": 19968
    },
    {
      "epoch": 7.258396927294119e-05,
      "grad_norm": 9109.2303736375,
      "learning_rate": 7.07478304847167e-07,
      "loss": 1.8381,
      "step": 20000
    },
    {
      "epoch": 7.27001036237779e-05,
      "grad_norm": 9800.843637156957,
      "learning_rate": 7.069124069974004e-07,
      "loss": 1.8574,
      "step": 20032
    },
    {
      "epoch": 7.28162379746146e-05,
      "grad_norm": 10333.174342862892,
      "learning_rate": 7.063478649302151e-07,
      "loss": 1.8383,
      "step": 20064
    },
    {
      "epoch": 7.29323723254513e-05,
      "grad_norm": 9600.482175390984,
      "learning_rate": 7.057846732405855e-07,
      "loss": 1.8332,
      "step": 20096
    },
    {
      "epoch": 7.304850667628801e-05,
      "grad_norm": 11628.136050115685,
      "learning_rate": 7.052228265536053e-07,
      "loss": 1.8423,
      "step": 20128
    },
    {
      "epoch": 7.316464102712471e-05,
      "grad_norm": 11883.980477937517,
      "learning_rate": 7.046623195242716e-07,
      "loss": 1.8345,
      "step": 20160
    },
    {
      "epoch": 7.328077537796143e-05,
      "grad_norm": 9516.219102143456,
      "learning_rate": 7.041031468372717e-07,
      "loss": 1.8391,
      "step": 20192
    },
    {
      "epoch": 7.339690972879813e-05,
      "grad_norm": 9625.18488134124,
      "learning_rate": 7.03545303206771e-07,
      "loss": 1.79,
      "step": 20224
    },
    {
      "epoch": 7.351304407963484e-05,
      "grad_norm": 10843.474258741982,
      "learning_rate": 7.029887833762038e-07,
      "loss": 1.8028,
      "step": 20256
    },
    {
      "epoch": 7.362917843047154e-05,
      "grad_norm": 11747.40499003929,
      "learning_rate": 7.024335821180645e-07,
      "loss": 1.8235,
      "step": 20288
    },
    {
      "epoch": 7.374531278130825e-05,
      "grad_norm": 11240.494295181152,
      "learning_rate": 7.018796942337019e-07,
      "loss": 1.8229,
      "step": 20320
    },
    {
      "epoch": 7.386144713214495e-05,
      "grad_norm": 8726.174075733305,
      "learning_rate": 7.013271145531147e-07,
      "loss": 1.8064,
      "step": 20352
    },
    {
      "epoch": 7.397758148298165e-05,
      "grad_norm": 9509.669815508843,
      "learning_rate": 7.007758379347481e-07,
      "loss": 1.8095,
      "step": 20384
    },
    {
      "epoch": 7.409371583381837e-05,
      "grad_norm": 9760.982634960477,
      "learning_rate": 7.002258592652943e-07,
      "loss": 1.8306,
      "step": 20416
    },
    {
      "epoch": 7.420985018465507e-05,
      "grad_norm": 9773.473691579673,
      "learning_rate": 6.996771734594916e-07,
      "loss": 1.8512,
      "step": 20448
    },
    {
      "epoch": 7.432598453549178e-05,
      "grad_norm": 9974.754232561321,
      "learning_rate": 6.991297754599284e-07,
      "loss": 1.851,
      "step": 20480
    },
    {
      "epoch": 7.444211888632848e-05,
      "grad_norm": 8764.707981444675,
      "learning_rate": 6.985836602368465e-07,
      "loss": 1.8431,
      "step": 20512
    },
    {
      "epoch": 7.455825323716519e-05,
      "grad_norm": 10192.356940374488,
      "learning_rate": 6.980388227879474e-07,
      "loss": 1.8458,
      "step": 20544
    },
    {
      "epoch": 7.467438758800189e-05,
      "grad_norm": 10582.607996141594,
      "learning_rate": 6.974952581381996e-07,
      "loss": 1.8545,
      "step": 20576
    },
    {
      "epoch": 7.47905219388386e-05,
      "grad_norm": 10109.603355226158,
      "learning_rate": 6.96952961339648e-07,
      "loss": 1.8513,
      "step": 20608
    },
    {
      "epoch": 7.49066562896753e-05,
      "grad_norm": 12423.808675281507,
      "learning_rate": 6.964119274712248e-07,
      "loss": 1.8552,
      "step": 20640
    },
    {
      "epoch": 7.5022790640512e-05,
      "grad_norm": 9557.386148942607,
      "learning_rate": 6.958721516385616e-07,
      "loss": 1.793,
      "step": 20672
    },
    {
      "epoch": 7.513892499134872e-05,
      "grad_norm": 9636.104088271359,
      "learning_rate": 6.953336289738034e-07,
      "loss": 1.8099,
      "step": 20704
    },
    {
      "epoch": 7.525505934218542e-05,
      "grad_norm": 11636.78959163566,
      "learning_rate": 6.947963546354251e-07,
      "loss": 1.8415,
      "step": 20736
    },
    {
      "epoch": 7.537119369302213e-05,
      "grad_norm": 9679.29553221721,
      "learning_rate": 6.942603238080475e-07,
      "loss": 1.8323,
      "step": 20768
    },
    {
      "epoch": 7.548732804385883e-05,
      "grad_norm": 10367.763307483441,
      "learning_rate": 6.93725531702257e-07,
      "loss": 1.7936,
      "step": 20800
    },
    {
      "epoch": 7.560346239469555e-05,
      "grad_norm": 9518.885649066282,
      "learning_rate": 6.932086286156005e-07,
      "loss": 1.7916,
      "step": 20832
    },
    {
      "epoch": 7.571959674553225e-05,
      "grad_norm": 9613.921572386576,
      "learning_rate": 6.92676261346048e-07,
      "loss": 1.8178,
      "step": 20864
    },
    {
      "epoch": 7.583573109636895e-05,
      "grad_norm": 9675.007493537149,
      "learning_rate": 6.921451187307275e-07,
      "loss": 1.8266,
      "step": 20896
    },
    {
      "epoch": 7.595186544720566e-05,
      "grad_norm": 10495.563062551717,
      "learning_rate": 6.916151960815171e-07,
      "loss": 1.8565,
      "step": 20928
    },
    {
      "epoch": 7.606799979804236e-05,
      "grad_norm": 10547.095713986862,
      "learning_rate": 6.91086488735382e-07,
      "loss": 1.827,
      "step": 20960
    },
    {
      "epoch": 7.618413414887907e-05,
      "grad_norm": 11326.475003283238,
      "learning_rate": 6.905589920542014e-07,
      "loss": 1.8199,
      "step": 20992
    },
    {
      "epoch": 7.630026849971577e-05,
      "grad_norm": 10804.737294353807,
      "learning_rate": 6.900327014245992e-07,
      "loss": 1.8286,
      "step": 21024
    },
    {
      "epoch": 7.641640285055249e-05,
      "grad_norm": 11150.293628420734,
      "learning_rate": 6.895076122577726e-07,
      "loss": 1.8413,
      "step": 21056
    },
    {
      "epoch": 7.653253720138919e-05,
      "grad_norm": 9852.172247783734,
      "learning_rate": 6.889837199893257e-07,
      "loss": 1.8372,
      "step": 21088
    },
    {
      "epoch": 7.66486715522259e-05,
      "grad_norm": 10127.001036832176,
      "learning_rate": 6.884610200791017e-07,
      "loss": 1.7996,
      "step": 21120
    },
    {
      "epoch": 7.67648059030626e-05,
      "grad_norm": 10781.32858232231,
      "learning_rate": 6.879395080110184e-07,
      "loss": 1.8211,
      "step": 21152
    },
    {
      "epoch": 7.68809402538993e-05,
      "grad_norm": 10880.824233485255,
      "learning_rate": 6.874191792929041e-07,
      "loss": 1.8584,
      "step": 21184
    },
    {
      "epoch": 7.699707460473601e-05,
      "grad_norm": 9850.554197607362,
      "learning_rate": 6.869000294563347e-07,
      "loss": 1.8171,
      "step": 21216
    },
    {
      "epoch": 7.711320895557271e-05,
      "grad_norm": 10489.938036041965,
      "learning_rate": 6.86382054056473e-07,
      "loss": 1.8052,
      "step": 21248
    },
    {
      "epoch": 7.722934330640943e-05,
      "grad_norm": 9980.868198708968,
      "learning_rate": 6.858652486719089e-07,
      "loss": 1.8153,
      "step": 21280
    },
    {
      "epoch": 7.734547765724613e-05,
      "grad_norm": 9805.363328301506,
      "learning_rate": 6.853496089045006e-07,
      "loss": 1.8184,
      "step": 21312
    },
    {
      "epoch": 7.746161200808284e-05,
      "grad_norm": 10012.720109940155,
      "learning_rate": 6.848351303792173e-07,
      "loss": 1.8531,
      "step": 21344
    },
    {
      "epoch": 7.757774635891954e-05,
      "grad_norm": 9377.096778854317,
      "learning_rate": 6.843218087439837e-07,
      "loss": 1.8767,
      "step": 21376
    },
    {
      "epoch": 7.769388070975625e-05,
      "grad_norm": 10370.026518770335,
      "learning_rate": 6.838096396695252e-07,
      "loss": 1.8248,
      "step": 21408
    },
    {
      "epoch": 7.781001506059295e-05,
      "grad_norm": 10640.04896605274,
      "learning_rate": 6.832986188492143e-07,
      "loss": 1.8282,
      "step": 21440
    },
    {
      "epoch": 7.792614941142965e-05,
      "grad_norm": 10540.862583299338,
      "learning_rate": 6.82788741998919e-07,
      "loss": 1.8368,
      "step": 21472
    },
    {
      "epoch": 7.804228376226637e-05,
      "grad_norm": 9207.784098250784,
      "learning_rate": 6.822800048568512e-07,
      "loss": 1.8501,
      "step": 21504
    },
    {
      "epoch": 7.815841811310307e-05,
      "grad_norm": 9471.371495195403,
      "learning_rate": 6.817724031834181e-07,
      "loss": 1.8246,
      "step": 21536
    },
    {
      "epoch": 7.827455246393978e-05,
      "grad_norm": 10538.809420423162,
      "learning_rate": 6.812659327610731e-07,
      "loss": 1.7954,
      "step": 21568
    },
    {
      "epoch": 7.839068681477648e-05,
      "grad_norm": 9807.573604108205,
      "learning_rate": 6.807605893941687e-07,
      "loss": 1.7999,
      "step": 21600
    },
    {
      "epoch": 7.850682116561319e-05,
      "grad_norm": 9287.58838450542,
      "learning_rate": 6.802563689088104e-07,
      "loss": 1.8306,
      "step": 21632
    },
    {
      "epoch": 7.862295551644989e-05,
      "grad_norm": 9912.536002456687,
      "learning_rate": 6.797532671527126e-07,
      "loss": 1.8285,
      "step": 21664
    },
    {
      "epoch": 7.87390898672866e-05,
      "grad_norm": 9210.251896663847,
      "learning_rate": 6.79251279995054e-07,
      "loss": 1.808,
      "step": 21696
    },
    {
      "epoch": 7.88552242181233e-05,
      "grad_norm": 12123.555914004768,
      "learning_rate": 6.787504033263361e-07,
      "loss": 1.8219,
      "step": 21728
    },
    {
      "epoch": 7.897135856896e-05,
      "grad_norm": 9807.218158071126,
      "learning_rate": 6.78250633058241e-07,
      "loss": 1.8351,
      "step": 21760
    },
    {
      "epoch": 7.908749291979672e-05,
      "grad_norm": 11971.852154115502,
      "learning_rate": 6.777519651234921e-07,
      "loss": 1.8646,
      "step": 21792
    },
    {
      "epoch": 7.920362727063342e-05,
      "grad_norm": 9597.808916622585,
      "learning_rate": 6.772699279427875e-07,
      "loss": 1.8244,
      "step": 21824
    },
    {
      "epoch": 7.931976162147013e-05,
      "grad_norm": 11953.409890069026,
      "learning_rate": 6.767734184214317e-07,
      "loss": 1.8183,
      "step": 21856
    },
    {
      "epoch": 7.943589597230683e-05,
      "grad_norm": 11684.465584698344,
      "learning_rate": 6.762779992812971e-07,
      "loss": 1.8236,
      "step": 21888
    },
    {
      "epoch": 7.955203032314355e-05,
      "grad_norm": 11367.67381657303,
      "learning_rate": 6.75783666537253e-07,
      "loss": 1.8171,
      "step": 21920
    },
    {
      "epoch": 7.966816467398025e-05,
      "grad_norm": 9820.829598358787,
      "learning_rate": 6.752904162245299e-07,
      "loss": 1.8471,
      "step": 21952
    },
    {
      "epoch": 7.978429902481696e-05,
      "grad_norm": 11060.802321712472,
      "learning_rate": 6.747982443985856e-07,
      "loss": 1.8269,
      "step": 21984
    },
    {
      "epoch": 7.990043337565366e-05,
      "grad_norm": 10338.662195854935,
      "learning_rate": 6.743071471349733e-07,
      "loss": 1.7938,
      "step": 22016
    },
    {
      "epoch": 8.001656772649036e-05,
      "grad_norm": 10262.771360602359,
      "learning_rate": 6.738171205292091e-07,
      "loss": 1.8081,
      "step": 22048
    },
    {
      "epoch": 8.013270207732707e-05,
      "grad_norm": 9494.751708180684,
      "learning_rate": 6.73328160696643e-07,
      "loss": 1.8378,
      "step": 22080
    },
    {
      "epoch": 8.024883642816377e-05,
      "grad_norm": 8903.589500869859,
      "learning_rate": 6.728402637723285e-07,
      "loss": 1.8112,
      "step": 22112
    },
    {
      "epoch": 8.036497077900049e-05,
      "grad_norm": 9273.72632764198,
      "learning_rate": 6.723534259108946e-07,
      "loss": 1.8039,
      "step": 22144
    },
    {
      "epoch": 8.048110512983719e-05,
      "grad_norm": 9279.34081710549,
      "learning_rate": 6.71867643286419e-07,
      "loss": 1.8159,
      "step": 22176
    },
    {
      "epoch": 8.05972394806739e-05,
      "grad_norm": 8600.778569408702,
      "learning_rate": 6.713829120923008e-07,
      "loss": 1.8179,
      "step": 22208
    },
    {
      "epoch": 8.07133738315106e-05,
      "grad_norm": 9026.444039598318,
      "learning_rate": 6.708992285411361e-07,
      "loss": 1.8435,
      "step": 22240
    },
    {
      "epoch": 8.08295081823473e-05,
      "grad_norm": 10772.361301033308,
      "learning_rate": 6.704165888645932e-07,
      "loss": 1.8677,
      "step": 22272
    },
    {
      "epoch": 8.094564253318401e-05,
      "grad_norm": 11429.946981504332,
      "learning_rate": 6.699349893132901e-07,
      "loss": 1.8584,
      "step": 22304
    },
    {
      "epoch": 8.106177688402071e-05,
      "grad_norm": 9526.454954493827,
      "learning_rate": 6.694544261566708e-07,
      "loss": 1.8525,
      "step": 22336
    },
    {
      "epoch": 8.117791123485743e-05,
      "grad_norm": 10018.029247312068,
      "learning_rate": 6.689748956828851e-07,
      "loss": 1.8584,
      "step": 22368
    },
    {
      "epoch": 8.129404558569413e-05,
      "grad_norm": 9340.112097828376,
      "learning_rate": 6.684963941986679e-07,
      "loss": 1.8334,
      "step": 22400
    },
    {
      "epoch": 8.141017993653084e-05,
      "grad_norm": 10896.034508021714,
      "learning_rate": 6.680189180292189e-07,
      "loss": 1.8025,
      "step": 22432
    },
    {
      "epoch": 8.152631428736754e-05,
      "grad_norm": 12488.20323345196,
      "learning_rate": 6.675424635180858e-07,
      "loss": 1.7878,
      "step": 22464
    },
    {
      "epoch": 8.164244863820425e-05,
      "grad_norm": 10414.84286967403,
      "learning_rate": 6.670670270270451e-07,
      "loss": 1.8014,
      "step": 22496
    },
    {
      "epoch": 8.175858298904095e-05,
      "grad_norm": 9674.779894137126,
      "learning_rate": 6.665926049359858e-07,
      "loss": 1.844,
      "step": 22528
    },
    {
      "epoch": 8.187471733987765e-05,
      "grad_norm": 9994.867432837715,
      "learning_rate": 6.661191936427943e-07,
      "loss": 1.8023,
      "step": 22560
    },
    {
      "epoch": 8.199085169071437e-05,
      "grad_norm": 9604.15847432767,
      "learning_rate": 6.656467895632386e-07,
      "loss": 1.8117,
      "step": 22592
    },
    {
      "epoch": 8.210698604155107e-05,
      "grad_norm": 8753.63238890005,
      "learning_rate": 6.651753891308553e-07,
      "loss": 1.8248,
      "step": 22624
    },
    {
      "epoch": 8.222312039238778e-05,
      "grad_norm": 11232.192484105675,
      "learning_rate": 6.647049887968355e-07,
      "loss": 1.824,
      "step": 22656
    },
    {
      "epoch": 8.233925474322448e-05,
      "grad_norm": 9399.19305046981,
      "learning_rate": 6.642355850299131e-07,
      "loss": 1.8199,
      "step": 22688
    },
    {
      "epoch": 8.245538909406119e-05,
      "grad_norm": 10364.010420681754,
      "learning_rate": 6.637671743162539e-07,
      "loss": 1.8406,
      "step": 22720
    },
    {
      "epoch": 8.257152344489789e-05,
      "grad_norm": 10003.482893472654,
      "learning_rate": 6.632997531593441e-07,
      "loss": 1.8143,
      "step": 22752
    },
    {
      "epoch": 8.26876577957346e-05,
      "grad_norm": 11921.320899967419,
      "learning_rate": 6.628333180798819e-07,
      "loss": 1.8219,
      "step": 22784
    },
    {
      "epoch": 8.28037921465713e-05,
      "grad_norm": 9374.686448089877,
      "learning_rate": 6.623823961658694e-07,
      "loss": 1.8135,
      "step": 22816
    },
    {
      "epoch": 8.2919926497408e-05,
      "grad_norm": 10769.455696552172,
      "learning_rate": 6.619178923246148e-07,
      "loss": 1.8389,
      "step": 22848
    },
    {
      "epoch": 8.303606084824472e-05,
      "grad_norm": 9501.043521634872,
      "learning_rate": 6.614543643319656e-07,
      "loss": 1.837,
      "step": 22880
    },
    {
      "epoch": 8.315219519908142e-05,
      "grad_norm": 10485.194609543496,
      "learning_rate": 6.609918087758631e-07,
      "loss": 1.8379,
      "step": 22912
    },
    {
      "epoch": 8.326832954991813e-05,
      "grad_norm": 10328.163437901241,
      "learning_rate": 6.605302222609272e-07,
      "loss": 1.8316,
      "step": 22944
    },
    {
      "epoch": 8.338446390075483e-05,
      "grad_norm": 10912.83473713407,
      "learning_rate": 6.600696014083522e-07,
      "loss": 1.8439,
      "step": 22976
    },
    {
      "epoch": 8.350059825159155e-05,
      "grad_norm": 11341.068203657009,
      "learning_rate": 6.596099428558033e-07,
      "loss": 1.8021,
      "step": 23008
    },
    {
      "epoch": 8.361673260242825e-05,
      "grad_norm": 8816.414917640843,
      "learning_rate": 6.591512432573124e-07,
      "loss": 1.8033,
      "step": 23040
    },
    {
      "epoch": 8.373286695326496e-05,
      "grad_norm": 10981.835547849003,
      "learning_rate": 6.586934992831776e-07,
      "loss": 1.8115,
      "step": 23072
    },
    {
      "epoch": 8.384900130410166e-05,
      "grad_norm": 10136.117797263409,
      "learning_rate": 6.582367076198594e-07,
      "loss": 1.8239,
      "step": 23104
    },
    {
      "epoch": 8.396513565493836e-05,
      "grad_norm": 10567.363152650712,
      "learning_rate": 6.57780864969882e-07,
      "loss": 1.845,
      "step": 23136
    },
    {
      "epoch": 8.408127000577507e-05,
      "grad_norm": 10657.302472952524,
      "learning_rate": 6.573259680517322e-07,
      "loss": 1.8387,
      "step": 23168
    },
    {
      "epoch": 8.419740435661177e-05,
      "grad_norm": 9586.484131317384,
      "learning_rate": 6.568720135997607e-07,
      "loss": 1.8576,
      "step": 23200
    },
    {
      "epoch": 8.431353870744849e-05,
      "grad_norm": 11441.311113679236,
      "learning_rate": 6.564189983640828e-07,
      "loss": 1.846,
      "step": 23232
    },
    {
      "epoch": 8.442967305828519e-05,
      "grad_norm": 10023.286886046913,
      "learning_rate": 6.559669191104821e-07,
      "loss": 1.8115,
      "step": 23264
    },
    {
      "epoch": 8.45458074091219e-05,
      "grad_norm": 9895.608520955142,
      "learning_rate": 6.555157726203121e-07,
      "loss": 1.827,
      "step": 23296
    },
    {
      "epoch": 8.46619417599586e-05,
      "grad_norm": 8148.121869486244,
      "learning_rate": 6.550655556904012e-07,
      "loss": 1.8036,
      "step": 23328
    },
    {
      "epoch": 8.477807611079531e-05,
      "grad_norm": 10024.908079379084,
      "learning_rate": 6.546162651329555e-07,
      "loss": 1.7869,
      "step": 23360
    },
    {
      "epoch": 8.489421046163201e-05,
      "grad_norm": 10684.895039259862,
      "learning_rate": 6.54167897775466e-07,
      "loss": 1.8116,
      "step": 23392
    },
    {
      "epoch": 8.501034481246871e-05,
      "grad_norm": 10937.597176711162,
      "learning_rate": 6.537204504606135e-07,
      "loss": 1.8309,
      "step": 23424
    },
    {
      "epoch": 8.512647916330543e-05,
      "grad_norm": 10567.737411574912,
      "learning_rate": 6.532739200461747e-07,
      "loss": 1.7999,
      "step": 23456
    },
    {
      "epoch": 8.524261351414213e-05,
      "grad_norm": 9334.583011575825,
      "learning_rate": 6.528283034049304e-07,
      "loss": 1.8304,
      "step": 23488
    },
    {
      "epoch": 8.535874786497884e-05,
      "grad_norm": 8820.839642573716,
      "learning_rate": 6.523835974245735e-07,
      "loss": 1.8579,
      "step": 23520
    },
    {
      "epoch": 8.547488221581554e-05,
      "grad_norm": 9662.568499110368,
      "learning_rate": 6.519397990076168e-07,
      "loss": 1.8239,
      "step": 23552
    },
    {
      "epoch": 8.559101656665225e-05,
      "grad_norm": 8812.673941545778,
      "learning_rate": 6.514969050713037e-07,
      "loss": 1.8279,
      "step": 23584
    },
    {
      "epoch": 8.570715091748895e-05,
      "grad_norm": 10091.299817169243,
      "learning_rate": 6.51054912547517e-07,
      "loss": 1.8264,
      "step": 23616
    },
    {
      "epoch": 8.582328526832565e-05,
      "grad_norm": 11852.490033743965,
      "learning_rate": 6.506138183826906e-07,
      "loss": 1.8167,
      "step": 23648
    },
    {
      "epoch": 8.593941961916237e-05,
      "grad_norm": 10132.01125147421,
      "learning_rate": 6.501736195377201e-07,
      "loss": 1.8161,
      "step": 23680
    },
    {
      "epoch": 8.605555396999907e-05,
      "grad_norm": 11128.630284091569,
      "learning_rate": 6.497343129878754e-07,
      "loss": 1.8256,
      "step": 23712
    },
    {
      "epoch": 8.617168832083578e-05,
      "grad_norm": 9554.086978879772,
      "learning_rate": 6.492958957227136e-07,
      "loss": 1.833,
      "step": 23744
    },
    {
      "epoch": 8.628782267167248e-05,
      "grad_norm": 9098.860807815448,
      "learning_rate": 6.488583647459915e-07,
      "loss": 1.8137,
      "step": 23776
    },
    {
      "epoch": 8.640395702250919e-05,
      "grad_norm": 10414.489713855404,
      "learning_rate": 6.484217170755803e-07,
      "loss": 1.8177,
      "step": 23808
    },
    {
      "epoch": 8.652009137334589e-05,
      "grad_norm": 10100.458009417196,
      "learning_rate": 6.479995541764601e-07,
      "loss": 1.8289,
      "step": 23840
    },
    {
      "epoch": 8.66362257241826e-05,
      "grad_norm": 10588.226763722054,
      "learning_rate": 6.475646368546418e-07,
      "loss": 1.8343,
      "step": 23872
    },
    {
      "epoch": 8.67523600750193e-05,
      "grad_norm": 11458.251524556441,
      "learning_rate": 6.47130594068258e-07,
      "loss": 1.7897,
      "step": 23904
    },
    {
      "epoch": 8.6868494425856e-05,
      "grad_norm": 10498.756878792841,
      "learning_rate": 6.466974228903602e-07,
      "loss": 1.805,
      "step": 23936
    },
    {
      "epoch": 8.698462877669272e-05,
      "grad_norm": 10467.186059299796,
      "learning_rate": 6.462651204076959e-07,
      "loss": 1.8164,
      "step": 23968
    },
    {
      "epoch": 8.710076312752942e-05,
      "grad_norm": 12636.328738996941,
      "learning_rate": 6.458336837206266e-07,
      "loss": 1.8232,
      "step": 24000
    },
    {
      "epoch": 8.721689747836613e-05,
      "grad_norm": 8687.609222334992,
      "learning_rate": 6.45403109943046e-07,
      "loss": 1.8471,
      "step": 24032
    },
    {
      "epoch": 8.733303182920283e-05,
      "grad_norm": 10040.285902303778,
      "learning_rate": 6.449733962022988e-07,
      "loss": 1.8566,
      "step": 24064
    },
    {
      "epoch": 8.744916618003955e-05,
      "grad_norm": 10391.039697739587,
      "learning_rate": 6.445445396391006e-07,
      "loss": 1.8576,
      "step": 24096
    },
    {
      "epoch": 8.756530053087625e-05,
      "grad_norm": 9506.86835924428,
      "learning_rate": 6.441165374074573e-07,
      "loss": 1.858,
      "step": 24128
    },
    {
      "epoch": 8.768143488171296e-05,
      "grad_norm": 10222.495390069882,
      "learning_rate": 6.436893866745868e-07,
      "loss": 1.8219,
      "step": 24160
    },
    {
      "epoch": 8.779756923254966e-05,
      "grad_norm": 9951.272682426103,
      "learning_rate": 6.432630846208391e-07,
      "loss": 1.8189,
      "step": 24192
    },
    {
      "epoch": 8.791370358338636e-05,
      "grad_norm": 10867.954729386758,
      "learning_rate": 6.428376284396189e-07,
      "loss": 1.7973,
      "step": 24224
    },
    {
      "epoch": 8.802983793422307e-05,
      "grad_norm": 9856.404821231725,
      "learning_rate": 6.424130153373078e-07,
      "loss": 1.7946,
      "step": 24256
    },
    {
      "epoch": 8.814597228505977e-05,
      "grad_norm": 9200.823224038162,
      "learning_rate": 6.419892425331871e-07,
      "loss": 1.8159,
      "step": 24288
    },
    {
      "epoch": 8.826210663589649e-05,
      "grad_norm": 10446.998994926726,
      "learning_rate": 6.415663072593617e-07,
      "loss": 1.8383,
      "step": 24320
    },
    {
      "epoch": 8.837824098673319e-05,
      "grad_norm": 9157.736073943166,
      "learning_rate": 6.411442067606836e-07,
      "loss": 1.7908,
      "step": 24352
    },
    {
      "epoch": 8.84943753375699e-05,
      "grad_norm": 10952.61576062997,
      "learning_rate": 6.407229382946771e-07,
      "loss": 1.8069,
      "step": 24384
    },
    {
      "epoch": 8.86105096884066e-05,
      "grad_norm": 10972.993484004262,
      "learning_rate": 6.403024991314634e-07,
      "loss": 1.8177,
      "step": 24416
    },
    {
      "epoch": 8.872664403924331e-05,
      "grad_norm": 9286.426223257255,
      "learning_rate": 6.398828865536871e-07,
      "loss": 1.8283,
      "step": 24448
    },
    {
      "epoch": 8.884277839008001e-05,
      "grad_norm": 8841.434046578643,
      "learning_rate": 6.394640978564413e-07,
      "loss": 1.8301,
      "step": 24480
    },
    {
      "epoch": 8.895891274091671e-05,
      "grad_norm": 9618.25826228429,
      "learning_rate": 6.390461303471954e-07,
      "loss": 1.8198,
      "step": 24512
    },
    {
      "epoch": 8.907504709175343e-05,
      "grad_norm": 9465.522383894087,
      "learning_rate": 6.386289813457219e-07,
      "loss": 1.8221,
      "step": 24544
    },
    {
      "epoch": 8.919118144259013e-05,
      "grad_norm": 10284.765918580744,
      "learning_rate": 6.382126481840243e-07,
      "loss": 1.8228,
      "step": 24576
    },
    {
      "epoch": 8.930731579342684e-05,
      "grad_norm": 11123.510417130017,
      "learning_rate": 6.377971282062655e-07,
      "loss": 1.8272,
      "step": 24608
    },
    {
      "epoch": 8.942345014426354e-05,
      "grad_norm": 10683.602295106273,
      "learning_rate": 6.373824187686963e-07,
      "loss": 1.8429,
      "step": 24640
    },
    {
      "epoch": 8.953958449510025e-05,
      "grad_norm": 9080.780032574294,
      "learning_rate": 6.369685172395854e-07,
      "loss": 1.8233,
      "step": 24672
    },
    {
      "epoch": 8.965571884593695e-05,
      "grad_norm": 9489.327162660164,
      "learning_rate": 6.365554209991489e-07,
      "loss": 1.825,
      "step": 24704
    },
    {
      "epoch": 8.977185319677367e-05,
      "grad_norm": 8918.65987690976,
      "learning_rate": 6.361431274394804e-07,
      "loss": 1.8577,
      "step": 24736
    },
    {
      "epoch": 8.988798754761037e-05,
      "grad_norm": 10470.835687756733,
      "learning_rate": 6.35731633964482e-07,
      "loss": 1.8296,
      "step": 24768
    },
    {
      "epoch": 9.000412189844707e-05,
      "grad_norm": 10647.722667312479,
      "learning_rate": 6.353209379897959e-07,
      "loss": 1.7959,
      "step": 24800
    },
    {
      "epoch": 9.012025624928378e-05,
      "grad_norm": 7669.2718037633795,
      "learning_rate": 6.349238343433312e-07,
      "loss": 1.7966,
      "step": 24832
    },
    {
      "epoch": 9.023639060012048e-05,
      "grad_norm": 9242.668878630242,
      "learning_rate": 6.345147009400132e-07,
      "loss": 1.8224,
      "step": 24864
    },
    {
      "epoch": 9.03525249509572e-05,
      "grad_norm": 10174.51993953523,
      "learning_rate": 6.341063574331939e-07,
      "loss": 1.8289,
      "step": 24896
    },
    {
      "epoch": 9.046865930179389e-05,
      "grad_norm": 10530.515466965517,
      "learning_rate": 6.33698801284441e-07,
      "loss": 1.8567,
      "step": 24928
    },
    {
      "epoch": 9.05847936526306e-05,
      "grad_norm": 10975.549371216002,
      "learning_rate": 6.33292029966728e-07,
      "loss": 1.8311,
      "step": 24960
    },
    {
      "epoch": 9.07009280034673e-05,
      "grad_norm": 10410.917634867736,
      "learning_rate": 6.328860409643689e-07,
      "loss": 1.8151,
      "step": 24992
    },
    {
      "epoch": 9.0817062354304e-05,
      "grad_norm": 8765.683088042826,
      "learning_rate": 6.324808317729518e-07,
      "loss": 1.8377,
      "step": 25024
    },
    {
      "epoch": 9.093319670514072e-05,
      "grad_norm": 10911.573672023665,
      "learning_rate": 6.320763998992752e-07,
      "loss": 1.8278,
      "step": 25056
    },
    {
      "epoch": 9.104933105597742e-05,
      "grad_norm": 11607.040966585755,
      "learning_rate": 6.316727428612831e-07,
      "loss": 1.8237,
      "step": 25088
    },
    {
      "epoch": 9.116546540681413e-05,
      "grad_norm": 10276.1511277326,
      "learning_rate": 6.312698581880009e-07,
      "loss": 1.7881,
      "step": 25120
    },
    {
      "epoch": 9.128159975765083e-05,
      "grad_norm": 9577.261090729438,
      "learning_rate": 6.308677434194719e-07,
      "loss": 1.7972,
      "step": 25152
    },
    {
      "epoch": 9.139773410848755e-05,
      "grad_norm": 11208.320926882849,
      "learning_rate": 6.304663961066943e-07,
      "loss": 1.8313,
      "step": 25184
    },
    {
      "epoch": 9.151386845932425e-05,
      "grad_norm": 9344.652802539,
      "learning_rate": 6.300658138115584e-07,
      "loss": 1.8318,
      "step": 25216
    },
    {
      "epoch": 9.163000281016096e-05,
      "grad_norm": 9900.941268384537,
      "learning_rate": 6.29665994106785e-07,
      "loss": 1.8029,
      "step": 25248
    },
    {
      "epoch": 9.174613716099766e-05,
      "grad_norm": 10792.874037993772,
      "learning_rate": 6.292669345758626e-07,
      "loss": 1.8137,
      "step": 25280
    },
    {
      "epoch": 9.186227151183436e-05,
      "grad_norm": 10326.61609628246,
      "learning_rate": 6.288686328129869e-07,
      "loss": 1.8353,
      "step": 25312
    },
    {
      "epoch": 9.197840586267107e-05,
      "grad_norm": 9563.111522930181,
      "learning_rate": 6.284710864229996e-07,
      "loss": 1.834,
      "step": 25344
    },
    {
      "epoch": 9.209454021350777e-05,
      "grad_norm": 9581.99300772026,
      "learning_rate": 6.280742930213278e-07,
      "loss": 1.8364,
      "step": 25376
    },
    {
      "epoch": 9.221067456434449e-05,
      "grad_norm": 11107.688688471604,
      "learning_rate": 6.276782502339241e-07,
      "loss": 1.8148,
      "step": 25408
    },
    {
      "epoch": 9.232680891518119e-05,
      "grad_norm": 9992.266809888535,
      "learning_rate": 6.272829556972067e-07,
      "loss": 1.8069,
      "step": 25440
    },
    {
      "epoch": 9.24429432660179e-05,
      "grad_norm": 9248.785866263745,
      "learning_rate": 6.268884070580009e-07,
      "loss": 1.8427,
      "step": 25472
    },
    {
      "epoch": 9.25590776168546e-05,
      "grad_norm": 9096.220533826123,
      "learning_rate": 6.264946019734787e-07,
      "loss": 1.8289,
      "step": 25504
    },
    {
      "epoch": 9.267521196769131e-05,
      "grad_norm": 9847.94313549789,
      "learning_rate": 6.261015381111024e-07,
      "loss": 1.8251,
      "step": 25536
    },
    {
      "epoch": 9.279134631852801e-05,
      "grad_norm": 9536.904004969327,
      "learning_rate": 6.257092131485649e-07,
      "loss": 1.8007,
      "step": 25568
    },
    {
      "epoch": 9.290748066936471e-05,
      "grad_norm": 9450.986086118211,
      "learning_rate": 6.253176247737328e-07,
      "loss": 1.8024,
      "step": 25600
    },
    {
      "epoch": 9.302361502020143e-05,
      "grad_norm": 10581.813833176238,
      "learning_rate": 6.249267706845892e-07,
      "loss": 1.8343,
      "step": 25632
    },
    {
      "epoch": 9.313974937103813e-05,
      "grad_norm": 10098.268366408174,
      "learning_rate": 6.245366485891767e-07,
      "loss": 1.8364,
      "step": 25664
    },
    {
      "epoch": 9.325588372187484e-05,
      "grad_norm": 8270.402892241707,
      "learning_rate": 6.241472562055411e-07,
      "loss": 1.7897,
      "step": 25696
    },
    {
      "epoch": 9.337201807271154e-05,
      "grad_norm": 10001.841630419869,
      "learning_rate": 6.237585912616748e-07,
      "loss": 1.7969,
      "step": 25728
    },
    {
      "epoch": 9.348815242354825e-05,
      "grad_norm": 10843.947712894967,
      "learning_rate": 6.233706514954623e-07,
      "loss": 1.8137,
      "step": 25760
    },
    {
      "epoch": 9.360428677438495e-05,
      "grad_norm": 10066.68634655913,
      "learning_rate": 6.229834346546233e-07,
      "loss": 1.8476,
      "step": 25792
    },
    {
      "epoch": 9.372042112522167e-05,
      "grad_norm": 8930.57400170896,
      "learning_rate": 6.226090056150256e-07,
      "loss": 1.8473,
      "step": 25824
    },
    {
      "epoch": 9.383655547605837e-05,
      "grad_norm": 11784.06177852102,
      "learning_rate": 6.222232054892992e-07,
      "loss": 1.8396,
      "step": 25856
    },
    {
      "epoch": 9.395268982689507e-05,
      "grad_norm": 11187.366982449445,
      "learning_rate": 6.218381216599018e-07,
      "loss": 1.8321,
      "step": 25888
    },
    {
      "epoch": 9.406882417773178e-05,
      "grad_norm": 9475.73374467645,
      "learning_rate": 6.214537519130523e-07,
      "loss": 1.8286,
      "step": 25920
    },
    {
      "epoch": 9.418495852856848e-05,
      "grad_norm": 10803.581350644794,
      "learning_rate": 6.210700940445369e-07,
      "loss": 1.8323,
      "step": 25952
    },
    {
      "epoch": 9.43010928794052e-05,
      "grad_norm": 9534.030941841966,
      "learning_rate": 6.206871458596551e-07,
      "loss": 1.8328,
      "step": 25984
    },
    {
      "epoch": 9.44172272302419e-05,
      "grad_norm": 9087.20242979103,
      "learning_rate": 6.203049051731675e-07,
      "loss": 1.7802,
      "step": 26016
    },
    {
      "epoch": 9.453336158107861e-05,
      "grad_norm": 11261.359243004372,
      "learning_rate": 6.199233698092436e-07,
      "loss": 1.7969,
      "step": 26048
    },
    {
      "epoch": 9.464949593191531e-05,
      "grad_norm": 11805.873284090423,
      "learning_rate": 6.195425376014091e-07,
      "loss": 1.8557,
      "step": 26080
    },
    {
      "epoch": 9.476563028275202e-05,
      "grad_norm": 8539.696833026333,
      "learning_rate": 6.19162406392495e-07,
      "loss": 1.802,
      "step": 26112
    },
    {
      "epoch": 9.488176463358872e-05,
      "grad_norm": 9686.564509670083,
      "learning_rate": 6.187829740345857e-07,
      "loss": 1.7816,
      "step": 26144
    },
    {
      "epoch": 9.499789898442542e-05,
      "grad_norm": 10326.910477001338,
      "learning_rate": 6.184042383889684e-07,
      "loss": 1.794,
      "step": 26176
    },
    {
      "epoch": 9.511403333526213e-05,
      "grad_norm": 11024.947800329941,
      "learning_rate": 6.180261973260823e-07,
      "loss": 1.8007,
      "step": 26208
    },
    {
      "epoch": 9.523016768609883e-05,
      "grad_norm": 10201.96383055733,
      "learning_rate": 6.176488487254683e-07,
      "loss": 1.827,
      "step": 26240
    },
    {
      "epoch": 9.534630203693555e-05,
      "grad_norm": 9346.182536201612,
      "learning_rate": 6.17272190475719e-07,
      "loss": 1.8443,
      "step": 26272
    },
    {
      "epoch": 9.546243638777225e-05,
      "grad_norm": 9466.492697932006,
      "learning_rate": 6.168962204744293e-07,
      "loss": 1.8189,
      "step": 26304
    },
    {
      "epoch": 9.557857073860896e-05,
      "grad_norm": 10795.54408077703,
      "learning_rate": 6.16520936628147e-07,
      "loss": 1.8273,
      "step": 26336
    },
    {
      "epoch": 9.569470508944566e-05,
      "grad_norm": 10499.855903773156,
      "learning_rate": 6.161463368523236e-07,
      "loss": 1.8231,
      "step": 26368
    },
    {
      "epoch": 9.581083944028236e-05,
      "grad_norm": 10628.057301313349,
      "learning_rate": 6.15772419071266e-07,
      "loss": 1.8374,
      "step": 26400
    },
    {
      "epoch": 9.592697379111907e-05,
      "grad_norm": 10937.085900732425,
      "learning_rate": 6.153991812180881e-07,
      "loss": 1.8423,
      "step": 26432
    },
    {
      "epoch": 9.604310814195577e-05,
      "grad_norm": 10977.404975676172,
      "learning_rate": 6.150266212346628e-07,
      "loss": 1.8114,
      "step": 26464
    },
    {
      "epoch": 9.615924249279249e-05,
      "grad_norm": 9933.136060680938,
      "learning_rate": 6.146547370715743e-07,
      "loss": 1.8194,
      "step": 26496
    },
    {
      "epoch": 9.627537684362919e-05,
      "grad_norm": 9607.565664620774,
      "learning_rate": 6.142835266880706e-07,
      "loss": 1.8353,
      "step": 26528
    },
    {
      "epoch": 9.63915111944659e-05,
      "grad_norm": 9564.294432941722,
      "learning_rate": 6.139129880520171e-07,
      "loss": 1.8126,
      "step": 26560
    },
    {
      "epoch": 9.65076455453026e-05,
      "grad_norm": 9553.58759838418,
      "learning_rate": 6.135431191398489e-07,
      "loss": 1.8177,
      "step": 26592
    },
    {
      "epoch": 9.662377989613931e-05,
      "grad_norm": 11203.33146880873,
      "learning_rate": 6.131739179365251e-07,
      "loss": 1.7975,
      "step": 26624
    },
    {
      "epoch": 9.673991424697601e-05,
      "grad_norm": 10717.056685489724,
      "learning_rate": 6.128053824354824e-07,
      "loss": 1.839,
      "step": 26656
    },
    {
      "epoch": 9.685604859781271e-05,
      "grad_norm": 8998.928714019241,
      "learning_rate": 6.1243751063859e-07,
      "loss": 1.8308,
      "step": 26688
    },
    {
      "epoch": 9.697218294864943e-05,
      "grad_norm": 9320.658453135165,
      "learning_rate": 6.120703005561024e-07,
      "loss": 1.8233,
      "step": 26720
    },
    {
      "epoch": 9.708831729948613e-05,
      "grad_norm": 9967.832161508339,
      "learning_rate": 6.117037502066161e-07,
      "loss": 1.8054,
      "step": 26752
    },
    {
      "epoch": 9.720445165032284e-05,
      "grad_norm": 8828.435648516672,
      "learning_rate": 6.113378576170238e-07,
      "loss": 1.8166,
      "step": 26784
    },
    {
      "epoch": 9.732058600115954e-05,
      "grad_norm": 9766.506540211807,
      "learning_rate": 6.109726208224694e-07,
      "loss": 1.8162,
      "step": 26816
    },
    {
      "epoch": 9.743672035199625e-05,
      "grad_norm": 10922.250592254326,
      "learning_rate": 6.106194212060931e-07,
      "loss": 1.8246,
      "step": 26848
    },
    {
      "epoch": 9.755285470283295e-05,
      "grad_norm": 10155.30836557906,
      "learning_rate": 6.102554697976864e-07,
      "loss": 1.8348,
      "step": 26880
    },
    {
      "epoch": 9.766898905366967e-05,
      "grad_norm": 9236.065612586346,
      "learning_rate": 6.098921683993328e-07,
      "loss": 1.8032,
      "step": 26912
    },
    {
      "epoch": 9.778512340450637e-05,
      "grad_norm": 10647.197753399718,
      "learning_rate": 6.095295150784911e-07,
      "loss": 1.8162,
      "step": 26944
    },
    {
      "epoch": 9.790125775534307e-05,
      "grad_norm": 10387.463212931249,
      "learning_rate": 6.091675079106547e-07,
      "loss": 1.8289,
      "step": 26976
    },
    {
      "epoch": 9.801739210617978e-05,
      "grad_norm": 9049.57910623472,
      "learning_rate": 6.088061449793081e-07,
      "loss": 1.8087,
      "step": 27008
    },
    {
      "epoch": 9.813352645701648e-05,
      "grad_norm": 8923.101478746054,
      "learning_rate": 6.084454243758851e-07,
      "loss": 1.801,
      "step": 27040
    },
    {
      "epoch": 9.82496608078532e-05,
      "grad_norm": 9719.127841529815,
      "learning_rate": 6.080853441997254e-07,
      "loss": 1.8162,
      "step": 27072
    },
    {
      "epoch": 9.83657951586899e-05,
      "grad_norm": 10303.620043460454,
      "learning_rate": 6.077259025580334e-07,
      "loss": 1.8049,
      "step": 27104
    },
    {
      "epoch": 9.848192950952661e-05,
      "grad_norm": 10867.274359286233,
      "learning_rate": 6.073670975658364e-07,
      "loss": 1.8115,
      "step": 27136
    },
    {
      "epoch": 9.859806386036331e-05,
      "grad_norm": 9624.104321961602,
      "learning_rate": 6.070089273459422e-07,
      "loss": 1.8432,
      "step": 27168
    },
    {
      "epoch": 9.871419821120002e-05,
      "grad_norm": 10044.808211210408,
      "learning_rate": 6.06651390028899e-07,
      "loss": 1.8347,
      "step": 27200
    },
    {
      "epoch": 9.883033256203672e-05,
      "grad_norm": 10667.64847564823,
      "learning_rate": 6.062944837529535e-07,
      "loss": 1.8323,
      "step": 27232
    },
    {
      "epoch": 9.894646691287342e-05,
      "grad_norm": 10416.763892879593,
      "learning_rate": 6.059382066640109e-07,
      "loss": 1.8294,
      "step": 27264
    },
    {
      "epoch": 9.906260126371013e-05,
      "grad_norm": 8913.168348011834,
      "learning_rate": 6.055825569155937e-07,
      "loss": 1.8273,
      "step": 27296
    },
    {
      "epoch": 9.917873561454683e-05,
      "grad_norm": 9320.67261521399,
      "learning_rate": 6.052275326688025e-07,
      "loss": 1.8147,
      "step": 27328
    },
    {
      "epoch": 9.929486996538355e-05,
      "grad_norm": 10167.694330574655,
      "learning_rate": 6.048731320922748e-07,
      "loss": 1.79,
      "step": 27360
    },
    {
      "epoch": 9.941100431622025e-05,
      "grad_norm": 9335.766492366869,
      "learning_rate": 6.045193533621463e-07,
      "loss": 1.8076,
      "step": 27392
    },
    {
      "epoch": 9.952713866705696e-05,
      "grad_norm": 10254.399348572299,
      "learning_rate": 6.041661946620115e-07,
      "loss": 1.8305,
      "step": 27424
    },
    {
      "epoch": 9.964327301789366e-05,
      "grad_norm": 9819.619748238727,
      "learning_rate": 6.038136541828836e-07,
      "loss": 1.8043,
      "step": 27456
    },
    {
      "epoch": 9.975940736873037e-05,
      "grad_norm": 9660.278774445384,
      "learning_rate": 6.034617301231568e-07,
      "loss": 1.8181,
      "step": 27488
    },
    {
      "epoch": 9.987554171956707e-05,
      "grad_norm": 10349.591682767006,
      "learning_rate": 6.031104206885666e-07,
      "loss": 1.8375,
      "step": 27520
    },
    {
      "epoch": 9.999167607040377e-05,
      "grad_norm": 10678.933935557425,
      "learning_rate": 6.02759724092152e-07,
      "loss": 1.8021,
      "step": 27552
    },
    {
      "epoch": 0.00010010781042124049,
      "grad_norm": 10322.505897310013,
      "learning_rate": 6.024096385542169e-07,
      "loss": 1.8239,
      "step": 27584
    },
    {
      "epoch": 0.00010022394477207719,
      "grad_norm": 10581.927990682983,
      "learning_rate": 6.020601623022926e-07,
      "loss": 1.8479,
      "step": 27616
    },
    {
      "epoch": 0.0001003400791229139,
      "grad_norm": 9497.727096521567,
      "learning_rate": 6.017112935711001e-07,
      "loss": 1.8252,
      "step": 27648
    },
    {
      "epoch": 0.0001004562134737506,
      "grad_norm": 9824.134974642804,
      "learning_rate": 6.013630306025118e-07,
      "loss": 1.8344,
      "step": 27680
    },
    {
      "epoch": 0.00010057234782458731,
      "grad_norm": 9568.909342239585,
      "learning_rate": 6.010153716455158e-07,
      "loss": 1.7946,
      "step": 27712
    },
    {
      "epoch": 0.00010068848217542401,
      "grad_norm": 10589.896127913626,
      "learning_rate": 6.006683149561772e-07,
      "loss": 1.8191,
      "step": 27744
    },
    {
      "epoch": 0.00010080461652626071,
      "grad_norm": 9776.206421715942,
      "learning_rate": 6.003218587976029e-07,
      "loss": 1.8177,
      "step": 27776
    },
    {
      "epoch": 0.00010092075087709743,
      "grad_norm": 9343.83433072312,
      "learning_rate": 5.99976001439904e-07,
      "loss": 1.8233,
      "step": 27808
    },
    {
      "epoch": 0.00010103688522793413,
      "grad_norm": 10391.749900762625,
      "learning_rate": 5.996415215231645e-07,
      "loss": 1.812,
      "step": 27840
    },
    {
      "epoch": 0.00010115301957877084,
      "grad_norm": 9674.23754101583,
      "learning_rate": 5.992968380262321e-07,
      "loss": 1.8172,
      "step": 27872
    },
    {
      "epoch": 0.00010126915392960754,
      "grad_norm": 9598.452531528194,
      "learning_rate": 5.989527482354798e-07,
      "loss": 1.7834,
      "step": 27904
    },
    {
      "epoch": 0.00010138528828044425,
      "grad_norm": 10634.280229521883,
      "learning_rate": 5.986092504484634e-07,
      "loss": 1.7897,
      "step": 27936
    },
    {
      "epoch": 0.00010150142263128095,
      "grad_norm": 9966.763265975569,
      "learning_rate": 5.982663429695645e-07,
      "loss": 1.7926,
      "step": 27968
    },
    {
      "epoch": 0.00010161755698211767,
      "grad_norm": 10382.707354057515,
      "learning_rate": 5.979240241099565e-07,
      "loss": 1.7965,
      "step": 28000
    },
    {
      "epoch": 0.00010173369133295437,
      "grad_norm": 9271.245655250432,
      "learning_rate": 5.975822921875696e-07,
      "loss": 1.8218,
      "step": 28032
    },
    {
      "epoch": 0.00010184982568379107,
      "grad_norm": 10747.162323143724,
      "learning_rate": 5.972411455270552e-07,
      "loss": 1.8353,
      "step": 28064
    },
    {
      "epoch": 0.00010196596003462778,
      "grad_norm": 10634.958297990643,
      "learning_rate": 5.969005824597525e-07,
      "loss": 1.8348,
      "step": 28096
    },
    {
      "epoch": 0.00010208209438546448,
      "grad_norm": 8977.986411217162,
      "learning_rate": 5.965606013236534e-07,
      "loss": 1.8473,
      "step": 28128
    },
    {
      "epoch": 0.0001021982287363012,
      "grad_norm": 11054.349189346245,
      "learning_rate": 5.96221200463369e-07,
      "loss": 1.8249,
      "step": 28160
    },
    {
      "epoch": 0.0001023143630871379,
      "grad_norm": 10556.71227229387,
      "learning_rate": 5.958823782300952e-07,
      "loss": 1.8434,
      "step": 28192
    },
    {
      "epoch": 0.00010243049743797461,
      "grad_norm": 9464.946698212305,
      "learning_rate": 5.955441329815798e-07,
      "loss": 1.8329,
      "step": 28224
    },
    {
      "epoch": 0.00010254663178881131,
      "grad_norm": 8801.115383859025,
      "learning_rate": 5.952064630820885e-07,
      "loss": 1.8167,
      "step": 28256
    },
    {
      "epoch": 0.00010266276613964802,
      "grad_norm": 9872.487224605562,
      "learning_rate": 5.948693669023713e-07,
      "loss": 1.8057,
      "step": 28288
    },
    {
      "epoch": 0.00010277890049048472,
      "grad_norm": 8041.694411005681,
      "learning_rate": 5.945328428196306e-07,
      "loss": 1.8267,
      "step": 28320
    },
    {
      "epoch": 0.00010289503484132142,
      "grad_norm": 11768.666874374514,
      "learning_rate": 5.941968892174876e-07,
      "loss": 1.7963,
      "step": 28352
    },
    {
      "epoch": 0.00010301116919215813,
      "grad_norm": 9610.858026211812,
      "learning_rate": 5.938615044859497e-07,
      "loss": 1.8149,
      "step": 28384
    },
    {
      "epoch": 0.00010312730354299483,
      "grad_norm": 9430.116648271112,
      "learning_rate": 5.935266870213785e-07,
      "loss": 1.8249,
      "step": 28416
    },
    {
      "epoch": 0.00010324343789383155,
      "grad_norm": 10204.771041037618,
      "learning_rate": 5.931924352264574e-07,
      "loss": 1.8172,
      "step": 28448
    },
    {
      "epoch": 0.00010335957224466825,
      "grad_norm": 9427.780756890776,
      "learning_rate": 5.928587475101592e-07,
      "loss": 1.8094,
      "step": 28480
    },
    {
      "epoch": 0.00010347570659550496,
      "grad_norm": 10505.791831175791,
      "learning_rate": 5.925256222877149e-07,
      "loss": 1.8151,
      "step": 28512
    },
    {
      "epoch": 0.00010359184094634166,
      "grad_norm": 9433.338115428705,
      "learning_rate": 5.921930579805819e-07,
      "loss": 1.8207,
      "step": 28544
    },
    {
      "epoch": 0.00010370797529717837,
      "grad_norm": 10281.159078625327,
      "learning_rate": 5.918610530164123e-07,
      "loss": 1.8038,
      "step": 28576
    },
    {
      "epoch": 0.00010382410964801507,
      "grad_norm": 11015.064593546422,
      "learning_rate": 5.915296058290223e-07,
      "loss": 1.8118,
      "step": 28608
    },
    {
      "epoch": 0.00010394024399885177,
      "grad_norm": 10194.089660190359,
      "learning_rate": 5.9119871485836e-07,
      "loss": 1.8298,
      "step": 28640
    },
    {
      "epoch": 0.00010405637834968849,
      "grad_norm": 9345.202726533009,
      "learning_rate": 5.908683785504763e-07,
      "loss": 1.8165,
      "step": 28672
    },
    {
      "epoch": 0.00010417251270052519,
      "grad_norm": 9563.472277368717,
      "learning_rate": 5.905385953574926e-07,
      "loss": 1.8013,
      "step": 28704
    },
    {
      "epoch": 0.0001042886470513619,
      "grad_norm": 10887.8806018435,
      "learning_rate": 5.902093637375712e-07,
      "loss": 1.8276,
      "step": 28736
    },
    {
      "epoch": 0.0001044047814021986,
      "grad_norm": 10303.87732846233,
      "learning_rate": 5.898806821548847e-07,
      "loss": 1.8277,
      "step": 28768
    },
    {
      "epoch": 0.00010452091575303531,
      "grad_norm": 9883.094454673597,
      "learning_rate": 5.895525490795865e-07,
      "loss": 1.7952,
      "step": 28800
    },
    {
      "epoch": 0.00010463705010387201,
      "grad_norm": 8770.06681844557,
      "learning_rate": 5.892351917887075e-07,
      "loss": 1.8047,
      "step": 28832
    },
    {
      "epoch": 0.00010475318445470873,
      "grad_norm": 9838.825742943109,
      "learning_rate": 5.88908134139538e-07,
      "loss": 1.8124,
      "step": 28864
    },
    {
      "epoch": 0.00010486931880554543,
      "grad_norm": 10311.595123936935,
      "learning_rate": 5.885816204909636e-07,
      "loss": 1.8008,
      "step": 28896
    },
    {
      "epoch": 0.00010498545315638213,
      "grad_norm": 10055.1543001587,
      "learning_rate": 5.882556493365786e-07,
      "loss": 1.8328,
      "step": 28928
    },
    {
      "epoch": 0.00010510158750721884,
      "grad_norm": 10068.699320170406,
      "learning_rate": 5.879302191758113e-07,
      "loss": 1.8252,
      "step": 28960
    },
    {
      "epoch": 0.00010521772185805554,
      "grad_norm": 9545.43545366056,
      "learning_rate": 5.876053285138941e-07,
      "loss": 1.8268,
      "step": 28992
    },
    {
      "epoch": 0.00010533385620889225,
      "grad_norm": 9198.93059001969,
      "learning_rate": 5.872809758618353e-07,
      "loss": 1.8599,
      "step": 29024
    },
    {
      "epoch": 0.00010544999055972895,
      "grad_norm": 11306.784600406962,
      "learning_rate": 5.869571597363899e-07,
      "loss": 1.8207,
      "step": 29056
    },
    {
      "epoch": 0.00010556612491056567,
      "grad_norm": 10713.41327495584,
      "learning_rate": 5.86633878660032e-07,
      "loss": 1.8226,
      "step": 29088
    },
    {
      "epoch": 0.00010568225926140237,
      "grad_norm": 10617.089996792907,
      "learning_rate": 5.863111311609255e-07,
      "loss": 1.7971,
      "step": 29120
    },
    {
      "epoch": 0.00010579839361223907,
      "grad_norm": 11615.44239364132,
      "learning_rate": 5.859889157728963e-07,
      "loss": 1.8049,
      "step": 29152
    },
    {
      "epoch": 0.00010591452796307578,
      "grad_norm": 9863.957116695105,
      "learning_rate": 5.856672310354047e-07,
      "loss": 1.8148,
      "step": 29184
    },
    {
      "epoch": 0.00010603066231391248,
      "grad_norm": 9821.45294750222,
      "learning_rate": 5.853460754935173e-07,
      "loss": 1.8399,
      "step": 29216
    },
    {
      "epoch": 0.0001061467966647492,
      "grad_norm": 9277.485435181237,
      "learning_rate": 5.850254476978789e-07,
      "loss": 1.7856,
      "step": 29248
    },
    {
      "epoch": 0.0001062629310155859,
      "grad_norm": 9352.403541336313,
      "learning_rate": 5.847053462046862e-07,
      "loss": 1.7817,
      "step": 29280
    },
    {
      "epoch": 0.00010637906536642261,
      "grad_norm": 10201.314327085505,
      "learning_rate": 5.843857695756592e-07,
      "loss": 1.8159,
      "step": 29312
    },
    {
      "epoch": 0.00010649519971725931,
      "grad_norm": 10583.878306178694,
      "learning_rate": 5.840667163780147e-07,
      "loss": 1.8299,
      "step": 29344
    },
    {
      "epoch": 0.00010661133406809602,
      "grad_norm": 9847.323291128409,
      "learning_rate": 5.837481851844397e-07,
      "loss": 1.8343,
      "step": 29376
    },
    {
      "epoch": 0.00010672746841893272,
      "grad_norm": 10284.015558136813,
      "learning_rate": 5.834301745730637e-07,
      "loss": 1.8356,
      "step": 29408
    },
    {
      "epoch": 0.00010684360276976942,
      "grad_norm": 7760.81065353356,
      "learning_rate": 5.831126831274325e-07,
      "loss": 1.8142,
      "step": 29440
    },
    {
      "epoch": 0.00010695973712060613,
      "grad_norm": 8680.066704812814,
      "learning_rate": 5.827957094364819e-07,
      "loss": 1.7879,
      "step": 29472
    },
    {
      "epoch": 0.00010707587147144283,
      "grad_norm": 8510.314330270063,
      "learning_rate": 5.82479252094511e-07,
      "loss": 1.7955,
      "step": 29504
    },
    {
      "epoch": 0.00010719200582227955,
      "grad_norm": 10768.028138893398,
      "learning_rate": 5.821633097011562e-07,
      "loss": 1.7994,
      "step": 29536
    },
    {
      "epoch": 0.00010730814017311625,
      "grad_norm": 8783.590951313705,
      "learning_rate": 5.818478808613652e-07,
      "loss": 1.7886,
      "step": 29568
    },
    {
      "epoch": 0.00010742427452395296,
      "grad_norm": 10031.081696407422,
      "learning_rate": 5.815329641853709e-07,
      "loss": 1.799,
      "step": 29600
    },
    {
      "epoch": 0.00010754040887478966,
      "grad_norm": 9586.13926458405,
      "learning_rate": 5.812185582886661e-07,
      "loss": 1.8028,
      "step": 29632
    },
    {
      "epoch": 0.00010765654322562637,
      "grad_norm": 9876.175778103587,
      "learning_rate": 5.809046617919773e-07,
      "loss": 1.8127,
      "step": 29664
    },
    {
      "epoch": 0.00010777267757646307,
      "grad_norm": 10081.437893475315,
      "learning_rate": 5.8059127332124e-07,
      "loss": 1.7909,
      "step": 29696
    },
    {
      "epoch": 0.00010788881192729977,
      "grad_norm": 9867.2303104772,
      "learning_rate": 5.80278391507573e-07,
      "loss": 1.8026,
      "step": 29728
    },
    {
      "epoch": 0.00010800494627813649,
      "grad_norm": 9560.255906616727,
      "learning_rate": 5.799660149872534e-07,
      "loss": 1.7963,
      "step": 29760
    },
    {
      "epoch": 0.00010812108062897319,
      "grad_norm": 9160.12030488683,
      "learning_rate": 5.796541424016917e-07,
      "loss": 1.7912,
      "step": 29792
    },
    {
      "epoch": 0.0001082372149798099,
      "grad_norm": 11495.369415551637,
      "learning_rate": 5.793427723974065e-07,
      "loss": 1.7943,
      "step": 29824
    },
    {
      "epoch": 0.0001083533493306466,
      "grad_norm": 9330.545536033784,
      "learning_rate": 5.790416107014432e-07,
      "loss": 1.7948,
      "step": 29856
    },
    {
      "epoch": 0.00010846948368148331,
      "grad_norm": 10055.99025456966,
      "learning_rate": 5.787312262183146e-07,
      "loss": 1.8185,
      "step": 29888
    },
    {
      "epoch": 0.00010858561803232001,
      "grad_norm": 12463.569633134803,
      "learning_rate": 5.784213403281707e-07,
      "loss": 1.8378,
      "step": 29920
    },
    {
      "epoch": 0.00010870175238315673,
      "grad_norm": 9531.082834599645,
      "learning_rate": 5.781119516975626e-07,
      "loss": 1.8596,
      "step": 29952
    },
    {
      "epoch": 0.00010881788673399343,
      "grad_norm": 8837.851492302867,
      "learning_rate": 5.778030589980296e-07,
      "loss": 1.8576,
      "step": 29984
    },
    {
      "epoch": 0.00010893402108483013,
      "grad_norm": 9628.93649371518,
      "learning_rate": 5.774946609060736e-07,
      "loss": 1.8492,
      "step": 30016
    },
    {
      "epoch": 0.00010905015543566684,
      "grad_norm": 8674.553129700687,
      "learning_rate": 5.771867561031365e-07,
      "loss": 1.8294,
      "step": 30048
    },
    {
      "epoch": 0.00010916628978650354,
      "grad_norm": 10394.689894364334,
      "learning_rate": 5.76879343275576e-07,
      "loss": 1.8455,
      "step": 30080
    },
    {
      "epoch": 0.00010928242413734025,
      "grad_norm": 8721.357405817056,
      "learning_rate": 5.765724211146422e-07,
      "loss": 1.8297,
      "step": 30112
    },
    {
      "epoch": 0.00010939855848817695,
      "grad_norm": 9805.411057166344,
      "learning_rate": 5.762659883164542e-07,
      "loss": 1.8129,
      "step": 30144
    },
    {
      "epoch": 0.00010951469283901367,
      "grad_norm": 10842.715158114226,
      "learning_rate": 5.759600435819766e-07,
      "loss": 1.8165,
      "step": 30176
    },
    {
      "epoch": 0.00010963082718985037,
      "grad_norm": 10060.295224296353,
      "learning_rate": 5.756545856169969e-07,
      "loss": 1.7894,
      "step": 30208
    },
    {
      "epoch": 0.00010974696154068708,
      "grad_norm": 11656.060054752636,
      "learning_rate": 5.753496131321016e-07,
      "loss": 1.794,
      "step": 30240
    },
    {
      "epoch": 0.00010986309589152378,
      "grad_norm": 9521.39044467771,
      "learning_rate": 5.750451248426545e-07,
      "loss": 1.7977,
      "step": 30272
    },
    {
      "epoch": 0.00010997923024236048,
      "grad_norm": 9850.21218045581,
      "learning_rate": 5.747411194687732e-07,
      "loss": 1.7862,
      "step": 30304
    },
    {
      "epoch": 0.0001100953645931972,
      "grad_norm": 11501.547026378668,
      "learning_rate": 5.744375957353065e-07,
      "loss": 1.7906,
      "step": 30336
    },
    {
      "epoch": 0.0001102114989440339,
      "grad_norm": 8614.893382973465,
      "learning_rate": 5.741345523718123e-07,
      "loss": 1.8089,
      "step": 30368
    },
    {
      "epoch": 0.00011032763329487061,
      "grad_norm": 9836.919436490267,
      "learning_rate": 5.738319881125352e-07,
      "loss": 1.7918,
      "step": 30400
    },
    {
      "epoch": 0.00011044376764570731,
      "grad_norm": 9720.766739306113,
      "learning_rate": 5.735299016963839e-07,
      "loss": 1.7975,
      "step": 30432
    },
    {
      "epoch": 0.00011055990199654402,
      "grad_norm": 9746.82327735555,
      "learning_rate": 5.732282918669097e-07,
      "loss": 1.8028,
      "step": 30464
    },
    {
      "epoch": 0.00011067603634738072,
      "grad_norm": 10341.197996363864,
      "learning_rate": 5.729271573722839e-07,
      "loss": 1.8042,
      "step": 30496
    },
    {
      "epoch": 0.00011079217069821742,
      "grad_norm": 11195.446485067043,
      "learning_rate": 5.726264969652766e-07,
      "loss": 1.792,
      "step": 30528
    },
    {
      "epoch": 0.00011090830504905413,
      "grad_norm": 11468.455606575804,
      "learning_rate": 5.723263094032348e-07,
      "loss": 1.8182,
      "step": 30560
    },
    {
      "epoch": 0.00011102443939989083,
      "grad_norm": 10042.38437822413,
      "learning_rate": 5.720265934480606e-07,
      "loss": 1.8064,
      "step": 30592
    },
    {
      "epoch": 0.00011114057375072755,
      "grad_norm": 9493.605321478242,
      "learning_rate": 5.7172734786619e-07,
      "loss": 1.8043,
      "step": 30624
    },
    {
      "epoch": 0.00011125670810156425,
      "grad_norm": 8690.676613474925,
      "learning_rate": 5.714285714285715e-07,
      "loss": 1.8112,
      "step": 30656
    },
    {
      "epoch": 0.00011137284245240096,
      "grad_norm": 10781.732792088664,
      "learning_rate": 5.711302629106447e-07,
      "loss": 1.8281,
      "step": 30688
    },
    {
      "epoch": 0.00011148897680323766,
      "grad_norm": 8434.466432442541,
      "learning_rate": 5.708324210923199e-07,
      "loss": 1.8185,
      "step": 30720
    },
    {
      "epoch": 0.00011160511115407437,
      "grad_norm": 10168.407151565087,
      "learning_rate": 5.705350447579562e-07,
      "loss": 1.8281,
      "step": 30752
    },
    {
      "epoch": 0.00011172124550491107,
      "grad_norm": 8919.43316584636,
      "learning_rate": 5.702381326963412e-07,
      "loss": 1.8406,
      "step": 30784
    },
    {
      "epoch": 0.00011183737985574777,
      "grad_norm": 9213.6746198246,
      "learning_rate": 5.699416837006704e-07,
      "loss": 1.8263,
      "step": 30816
    },
    {
      "epoch": 0.00011195351420658449,
      "grad_norm": 8766.959450117241,
      "learning_rate": 5.696549391872194e-07,
      "loss": 1.8364,
      "step": 30848
    },
    {
      "epoch": 0.00011206964855742119,
      "grad_norm": 11739.940033918401,
      "learning_rate": 5.693593983428462e-07,
      "loss": 1.8431,
      "step": 30880
    },
    {
      "epoch": 0.0001121857829082579,
      "grad_norm": 9159.25040601031,
      "learning_rate": 5.690643170074818e-07,
      "loss": 1.8205,
      "step": 30912
    },
    {
      "epoch": 0.0001123019172590946,
      "grad_norm": 10638.06711766757,
      "learning_rate": 5.687696939916126e-07,
      "loss": 1.8257,
      "step": 30944
    },
    {
      "epoch": 0.00011241805160993131,
      "grad_norm": 10273.723570351694,
      "learning_rate": 5.684755281100316e-07,
      "loss": 1.7919,
      "step": 30976
    },
    {
      "epoch": 0.00011253418596076801,
      "grad_norm": 9909.799997981796,
      "learning_rate": 5.681818181818182e-07,
      "loss": 1.781,
      "step": 31008
    },
    {
      "epoch": 0.00011265032031160473,
      "grad_norm": 9949.925426856224,
      "learning_rate": 5.678885630303183e-07,
      "loss": 1.7824,
      "step": 31040
    },
    {
      "epoch": 0.00011276645466244143,
      "grad_norm": 10411.511609751968,
      "learning_rate": 5.675957614831248e-07,
      "loss": 1.7935,
      "step": 31072
    },
    {
      "epoch": 0.00011288258901327813,
      "grad_norm": 8970.69941531874,
      "learning_rate": 5.673034123720573e-07,
      "loss": 1.7927,
      "step": 31104
    },
    {
      "epoch": 0.00011299872336411484,
      "grad_norm": 10463.243474181416,
      "learning_rate": 5.670115145331432e-07,
      "loss": 1.7998,
      "step": 31136
    },
    {
      "epoch": 0.00011311485771495154,
      "grad_norm": 9645.06599251659,
      "learning_rate": 5.667200668065976e-07,
      "loss": 1.8327,
      "step": 31168
    },
    {
      "epoch": 0.00011323099206578825,
      "grad_norm": 9949.81326457939,
      "learning_rate": 5.664290680368047e-07,
      "loss": 1.8344,
      "step": 31200
    },
    {
      "epoch": 0.00011334712641662495,
      "grad_norm": 10443.76742368385,
      "learning_rate": 5.661385170722979e-07,
      "loss": 1.8079,
      "step": 31232
    },
    {
      "epoch": 0.00011346326076746167,
      "grad_norm": 9768.62487763759,
      "learning_rate": 5.658484127657409e-07,
      "loss": 1.7901,
      "step": 31264
    },
    {
      "epoch": 0.00011357939511829837,
      "grad_norm": 10363.358335983563,
      "learning_rate": 5.655587539739085e-07,
      "loss": 1.7843,
      "step": 31296
    },
    {
      "epoch": 0.00011369552946913508,
      "grad_norm": 8673.214859554673,
      "learning_rate": 5.652695395576682e-07,
      "loss": 1.7807,
      "step": 31328
    },
    {
      "epoch": 0.00011381166381997178,
      "grad_norm": 9922.225254447714,
      "learning_rate": 5.649807683819609e-07,
      "loss": 1.7811,
      "step": 31360
    },
    {
      "epoch": 0.00011392779817080848,
      "grad_norm": 9649.803314057754,
      "learning_rate": 5.646924393157821e-07,
      "loss": 1.7988,
      "step": 31392
    },
    {
      "epoch": 0.0001140439325216452,
      "grad_norm": 9243.401862950674,
      "learning_rate": 5.644045512321636e-07,
      "loss": 1.7973,
      "step": 31424
    },
    {
      "epoch": 0.0001141600668724819,
      "grad_norm": 11036.068684092175,
      "learning_rate": 5.641171030081553e-07,
      "loss": 1.8256,
      "step": 31456
    },
    {
      "epoch": 0.00011427620122331861,
      "grad_norm": 9690.055520996772,
      "learning_rate": 5.638300935248058e-07,
      "loss": 1.8336,
      "step": 31488
    },
    {
      "epoch": 0.00011439233557415531,
      "grad_norm": 9032.625642635701,
      "learning_rate": 5.635435216671452e-07,
      "loss": 1.828,
      "step": 31520
    },
    {
      "epoch": 0.00011450846992499202,
      "grad_norm": 10251.346838342755,
      "learning_rate": 5.632573863241661e-07,
      "loss": 1.8296,
      "step": 31552
    },
    {
      "epoch": 0.00011462460427582872,
      "grad_norm": 10590.269590525068,
      "learning_rate": 5.629716863888063e-07,
      "loss": 1.8232,
      "step": 31584
    },
    {
      "epoch": 0.00011474073862666544,
      "grad_norm": 10748.524922053257,
      "learning_rate": 5.626864207579296e-07,
      "loss": 1.8249,
      "step": 31616
    },
    {
      "epoch": 0.00011485687297750213,
      "grad_norm": 10836.755603039132,
      "learning_rate": 5.624015883323094e-07,
      "loss": 1.8261,
      "step": 31648
    },
    {
      "epoch": 0.00011497300732833883,
      "grad_norm": 9098.921254742234,
      "learning_rate": 5.621171880166099e-07,
      "loss": 1.8233,
      "step": 31680
    },
    {
      "epoch": 0.00011508914167917555,
      "grad_norm": 11223.916606960334,
      "learning_rate": 5.618332187193684e-07,
      "loss": 1.8097,
      "step": 31712
    },
    {
      "epoch": 0.00011520527603001225,
      "grad_norm": 8170.470733072851,
      "learning_rate": 5.615496793529785e-07,
      "loss": 1.8122,
      "step": 31744
    },
    {
      "epoch": 0.00011532141038084896,
      "grad_norm": 11828.51554507158,
      "learning_rate": 5.612665688336716e-07,
      "loss": 1.8242,
      "step": 31776
    },
    {
      "epoch": 0.00011543754473168566,
      "grad_norm": 11255.78162545809,
      "learning_rate": 5.609838860815003e-07,
      "loss": 1.8279,
      "step": 31808
    },
    {
      "epoch": 0.00011555367908252238,
      "grad_norm": 9290.609129653449,
      "learning_rate": 5.607104440741931e-07,
      "loss": 1.7856,
      "step": 31840
    },
    {
      "epoch": 0.00011566981343335907,
      "grad_norm": 9148.542178948514,
      "learning_rate": 5.604286003472598e-07,
      "loss": 1.7806,
      "step": 31872
    },
    {
      "epoch": 0.00011578594778419577,
      "grad_norm": 10851.14980082756,
      "learning_rate": 5.601471812037091e-07,
      "loss": 1.7871,
      "step": 31904
    },
    {
      "epoch": 0.00011590208213503249,
      "grad_norm": 8783.167651821295,
      "learning_rate": 5.598661855785879e-07,
      "loss": 1.7927,
      "step": 31936
    },
    {
      "epoch": 0.00011601821648586919,
      "grad_norm": 8861.202062925775,
      "learning_rate": 5.59585612410679e-07,
      "loss": 1.8091,
      "step": 31968
    },
    {
      "epoch": 0.0001161343508367059,
      "grad_norm": 9985.073159471593,
      "learning_rate": 5.593054606424843e-07,
      "loss": 1.7977,
      "step": 32000
    },
    {
      "epoch": 0.0001162504851875426,
      "grad_norm": 9696.654268354627,
      "learning_rate": 5.59025729220208e-07,
      "loss": 1.7811,
      "step": 32032
    },
    {
      "epoch": 0.00011636661953837932,
      "grad_norm": 10531.521447540237,
      "learning_rate": 5.5874641709374e-07,
      "loss": 1.789,
      "step": 32064
    },
    {
      "epoch": 0.00011648275388921602,
      "grad_norm": 10203.760483272821,
      "learning_rate": 5.584675232166391e-07,
      "loss": 1.8147,
      "step": 32096
    },
    {
      "epoch": 0.00011659888824005273,
      "grad_norm": 9268.749322319598,
      "learning_rate": 5.581890465461167e-07,
      "loss": 1.7855,
      "step": 32128
    },
    {
      "epoch": 0.00011671502259088943,
      "grad_norm": 7885.6871609264335,
      "learning_rate": 5.579109860430209e-07,
      "loss": 1.7932,
      "step": 32160
    },
    {
      "epoch": 0.00011683115694172613,
      "grad_norm": 11367.42327882621,
      "learning_rate": 5.576333406718192e-07,
      "loss": 1.8091,
      "step": 32192
    },
    {
      "epoch": 0.00011694729129256284,
      "grad_norm": 12152.906154496546,
      "learning_rate": 5.573561094005829e-07,
      "loss": 1.8246,
      "step": 32224
    },
    {
      "epoch": 0.00011706342564339954,
      "grad_norm": 10691.624104877612,
      "learning_rate": 5.570792912009714e-07,
      "loss": 1.8142,
      "step": 32256
    },
    {
      "epoch": 0.00011717955999423626,
      "grad_norm": 9662.98101001963,
      "learning_rate": 5.568028850482151e-07,
      "loss": 1.8181,
      "step": 32288
    },
    {
      "epoch": 0.00011729569434507296,
      "grad_norm": 10701.256748625368,
      "learning_rate": 5.565268899211007e-07,
      "loss": 1.834,
      "step": 32320
    },
    {
      "epoch": 0.00011741182869590967,
      "grad_norm": 9266.966925591134,
      "learning_rate": 5.562513048019543e-07,
      "loss": 1.8442,
      "step": 32352
    },
    {
      "epoch": 0.00011752796304674637,
      "grad_norm": 12549.634416986019,
      "learning_rate": 5.559761286766264e-07,
      "loss": 1.8518,
      "step": 32384
    },
    {
      "epoch": 0.00011764409739758308,
      "grad_norm": 11049.114806173387,
      "learning_rate": 5.557013605344756e-07,
      "loss": 1.8337,
      "step": 32416
    },
    {
      "epoch": 0.00011776023174841978,
      "grad_norm": 11342.197494312997,
      "learning_rate": 5.554269993683536e-07,
      "loss": 1.8078,
      "step": 32448
    },
    {
      "epoch": 0.00011787636609925648,
      "grad_norm": 8692.961175571878,
      "learning_rate": 5.551530441745892e-07,
      "loss": 1.7825,
      "step": 32480
    },
    {
      "epoch": 0.0001179925004500932,
      "grad_norm": 10400.918805567131,
      "learning_rate": 5.548794939529733e-07,
      "loss": 1.7939,
      "step": 32512
    },
    {
      "epoch": 0.0001181086348009299,
      "grad_norm": 10179.525529217952,
      "learning_rate": 5.546063477067431e-07,
      "loss": 1.7923,
      "step": 32544
    },
    {
      "epoch": 0.00011822476915176661,
      "grad_norm": 10993.163148066165,
      "learning_rate": 5.543336044425674e-07,
      "loss": 1.7934,
      "step": 32576
    },
    {
      "epoch": 0.00011834090350260331,
      "grad_norm": 9228.172083354319,
      "learning_rate": 5.540612631705309e-07,
      "loss": 1.7943,
      "step": 32608
    },
    {
      "epoch": 0.00011845703785344002,
      "grad_norm": 10436.660193759306,
      "learning_rate": 5.537893229041196e-07,
      "loss": 1.7828,
      "step": 32640
    },
    {
      "epoch": 0.00011857317220427672,
      "grad_norm": 10005.745149662767,
      "learning_rate": 5.535177826602049e-07,
      "loss": 1.8153,
      "step": 32672
    },
    {
      "epoch": 0.00011868930655511344,
      "grad_norm": 10548.689018072342,
      "learning_rate": 5.532466414590304e-07,
      "loss": 1.8111,
      "step": 32704
    },
    {
      "epoch": 0.00011880544090595014,
      "grad_norm": 9772.495177793642,
      "learning_rate": 5.529758983241947e-07,
      "loss": 1.8061,
      "step": 32736
    },
    {
      "epoch": 0.00011892157525678684,
      "grad_norm": 8976.58275737488,
      "learning_rate": 5.527055522826388e-07,
      "loss": 1.7943,
      "step": 32768
    },
    {
      "epoch": 0.00011903770960762355,
      "grad_norm": 9656.50060839847,
      "learning_rate": 5.524356023646303e-07,
      "loss": 1.7719,
      "step": 32800
    },
    {
      "epoch": 0.00011915384395846025,
      "grad_norm": 10163.183261163798,
      "learning_rate": 5.521660476037487e-07,
      "loss": 1.794,
      "step": 32832
    },
    {
      "epoch": 0.00011926997830929696,
      "grad_norm": 9550.67945226935,
      "learning_rate": 5.519052923473255e-07,
      "loss": 1.7799,
      "step": 32864
    },
    {
      "epoch": 0.00011938611266013366,
      "grad_norm": 9671.507535022656,
      "learning_rate": 5.516365127405419e-07,
      "loss": 1.7894,
      "step": 32896
    },
    {
      "epoch": 0.00011950224701097038,
      "grad_norm": 10049.430730145863,
      "learning_rate": 5.51368125441196e-07,
      "loss": 1.8324,
      "step": 32928
    },
    {
      "epoch": 0.00011961838136180708,
      "grad_norm": 9547.929409039427,
      "learning_rate": 5.511001294958703e-07,
      "loss": 1.8449,
      "step": 32960
    },
    {
      "epoch": 0.00011973451571264378,
      "grad_norm": 10570.067265632702,
      "learning_rate": 5.508325239543881e-07,
      "loss": 1.8204,
      "step": 32992
    },
    {
      "epoch": 0.00011985065006348049,
      "grad_norm": 9558.26009271562,
      "learning_rate": 5.50565307869799e-07,
      "loss": 1.8326,
      "step": 33024
    },
    {
      "epoch": 0.00011996678441431719,
      "grad_norm": 9977.276983225433,
      "learning_rate": 5.502984802983655e-07,
      "loss": 1.8118,
      "step": 33056
    },
    {
      "epoch": 0.0001200829187651539,
      "grad_norm": 8847.00101729394,
      "learning_rate": 5.500320402995483e-07,
      "loss": 1.809,
      "step": 33088
    },
    {
      "epoch": 0.0001201990531159906,
      "grad_norm": 9641.611483564353,
      "learning_rate": 5.49765986935993e-07,
      "loss": 1.8127,
      "step": 33120
    },
    {
      "epoch": 0.00012031518746682732,
      "grad_norm": 10371.755299851613,
      "learning_rate": 5.495003192735157e-07,
      "loss": 1.8181,
      "step": 33152
    },
    {
      "epoch": 0.00012043132181766402,
      "grad_norm": 8656.88696934412,
      "learning_rate": 5.492350363810898e-07,
      "loss": 1.8057,
      "step": 33184
    },
    {
      "epoch": 0.00012054745616850073,
      "grad_norm": 9893.605106330047,
      "learning_rate": 5.489701373308314e-07,
      "loss": 1.8099,
      "step": 33216
    },
    {
      "epoch": 0.00012066359051933743,
      "grad_norm": 8800.088635917255,
      "learning_rate": 5.487056211979868e-07,
      "loss": 1.7805,
      "step": 33248
    },
    {
      "epoch": 0.00012077972487017413,
      "grad_norm": 8772.321471537623,
      "learning_rate": 5.484414870609185e-07,
      "loss": 1.7914,
      "step": 33280
    },
    {
      "epoch": 0.00012089585922101084,
      "grad_norm": 9293.978265522252,
      "learning_rate": 5.481777340010908e-07,
      "loss": 1.8055,
      "step": 33312
    },
    {
      "epoch": 0.00012101199357184754,
      "grad_norm": 10624.6000395309,
      "learning_rate": 5.479143611030578e-07,
      "loss": 1.8121,
      "step": 33344
    },
    {
      "epoch": 0.00012112812792268426,
      "grad_norm": 9534.01379273179,
      "learning_rate": 5.476513674544496e-07,
      "loss": 1.7911,
      "step": 33376
    },
    {
      "epoch": 0.00012124426227352096,
      "grad_norm": 10173.44798974271,
      "learning_rate": 5.473887521459583e-07,
      "loss": 1.7846,
      "step": 33408
    },
    {
      "epoch": 0.00012136039662435767,
      "grad_norm": 10246.452068887063,
      "learning_rate": 5.471265142713258e-07,
      "loss": 1.8124,
      "step": 33440
    },
    {
      "epoch": 0.00012147653097519437,
      "grad_norm": 10662.671710223472,
      "learning_rate": 5.4686465292733e-07,
      "loss": 1.8128,
      "step": 33472
    },
    {
      "epoch": 0.00012159266532603108,
      "grad_norm": 12042.39893044571,
      "learning_rate": 5.466031672137719e-07,
      "loss": 1.8252,
      "step": 33504
    },
    {
      "epoch": 0.00012170879967686778,
      "grad_norm": 9873.41612614398,
      "learning_rate": 5.46342056233463e-07,
      "loss": 1.8331,
      "step": 33536
    },
    {
      "epoch": 0.00012182493402770448,
      "grad_norm": 10790.770315413076,
      "learning_rate": 5.460813190922116e-07,
      "loss": 1.7804,
      "step": 33568
    },
    {
      "epoch": 0.0001219410683785412,
      "grad_norm": 9059.212990100188,
      "learning_rate": 5.458209548988108e-07,
      "loss": 1.7938,
      "step": 33600
    },
    {
      "epoch": 0.0001220572027293779,
      "grad_norm": 9084.657616002927,
      "learning_rate": 5.455609627650249e-07,
      "loss": 1.7938,
      "step": 33632
    },
    {
      "epoch": 0.0001221733370802146,
      "grad_norm": 9513.983182663294,
      "learning_rate": 5.453013418055771e-07,
      "loss": 1.7886,
      "step": 33664
    },
    {
      "epoch": 0.00012228947143105132,
      "grad_norm": 9836.94169953243,
      "learning_rate": 5.450420911381371e-07,
      "loss": 1.8054,
      "step": 33696
    },
    {
      "epoch": 0.00012240560578188802,
      "grad_norm": 9729.546443694075,
      "learning_rate": 5.44783209883308e-07,
      "loss": 1.8183,
      "step": 33728
    },
    {
      "epoch": 0.00012252174013272472,
      "grad_norm": 8775.219541413195,
      "learning_rate": 5.445246971646138e-07,
      "loss": 1.8105,
      "step": 33760
    },
    {
      "epoch": 0.00012263787448356142,
      "grad_norm": 9295.936531624988,
      "learning_rate": 5.442665521084873e-07,
      "loss": 1.8188,
      "step": 33792
    },
    {
      "epoch": 0.00012275400883439812,
      "grad_norm": 10940.787266006044,
      "learning_rate": 5.440087738442574e-07,
      "loss": 1.8157,
      "step": 33824
    },
    {
      "epoch": 0.00012287014318523485,
      "grad_norm": 10552.453648322746,
      "learning_rate": 5.437594001094744e-07,
      "loss": 1.8052,
      "step": 33856
    },
    {
      "epoch": 0.00012298627753607155,
      "grad_norm": 9902.140778639738,
      "learning_rate": 5.435023414335118e-07,
      "loss": 1.8144,
      "step": 33888
    },
    {
      "epoch": 0.00012310241188690825,
      "grad_norm": 10707.862718582079,
      "learning_rate": 5.432456469815829e-07,
      "loss": 1.8204,
      "step": 33920
    },
    {
      "epoch": 0.00012321854623774495,
      "grad_norm": 10494.653877093802,
      "learning_rate": 5.429893158943901e-07,
      "loss": 1.8204,
      "step": 33952
    },
    {
      "epoch": 0.00012333468058858168,
      "grad_norm": 11241.862923910789,
      "learning_rate": 5.427333473154712e-07,
      "loss": 1.7875,
      "step": 33984
    },
    {
      "epoch": 0.00012345081493941838,
      "grad_norm": 9081.825587402567,
      "learning_rate": 5.424777403911877e-07,
      "loss": 1.7832,
      "step": 34016
    },
    {
      "epoch": 0.00012356694929025508,
      "grad_norm": 9423.152869395679,
      "learning_rate": 5.422224942707123e-07,
      "loss": 1.7953,
      "step": 34048
    },
    {
      "epoch": 0.00012368308364109178,
      "grad_norm": 9773.522701666989,
      "learning_rate": 5.419676081060178e-07,
      "loss": 1.8032,
      "step": 34080
    },
    {
      "epoch": 0.00012379921799192848,
      "grad_norm": 8816.419794905414,
      "learning_rate": 5.417130810518645e-07,
      "loss": 1.8157,
      "step": 34112
    },
    {
      "epoch": 0.0001239153523427652,
      "grad_norm": 8872.46707517137,
      "learning_rate": 5.41458912265789e-07,
      "loss": 1.8096,
      "step": 34144
    },
    {
      "epoch": 0.0001240314866936019,
      "grad_norm": 8920.463328773903,
      "learning_rate": 5.412051009080921e-07,
      "loss": 1.7865,
      "step": 34176
    },
    {
      "epoch": 0.0001241476210444386,
      "grad_norm": 9532.572580368847,
      "learning_rate": 5.409516461418276e-07,
      "loss": 1.8067,
      "step": 34208
    },
    {
      "epoch": 0.0001242637553952753,
      "grad_norm": 9504.629503562988,
      "learning_rate": 5.406985471327902e-07,
      "loss": 1.823,
      "step": 34240
    },
    {
      "epoch": 0.00012437988974611203,
      "grad_norm": 9704.495659229284,
      "learning_rate": 5.404458030495039e-07,
      "loss": 1.8136,
      "step": 34272
    },
    {
      "epoch": 0.00012449602409694873,
      "grad_norm": 9979.85090068985,
      "learning_rate": 5.401934130632113e-07,
      "loss": 1.7986,
      "step": 34304
    },
    {
      "epoch": 0.00012461215844778543,
      "grad_norm": 9182.268565011589,
      "learning_rate": 5.399413763478615e-07,
      "loss": 1.783,
      "step": 34336
    },
    {
      "epoch": 0.00012472829279862213,
      "grad_norm": 10230.90738888785,
      "learning_rate": 5.39689692080099e-07,
      "loss": 1.8075,
      "step": 34368
    },
    {
      "epoch": 0.00012484442714945883,
      "grad_norm": 11646.107504226466,
      "learning_rate": 5.394383594392522e-07,
      "loss": 1.7869,
      "step": 34400
    },
    {
      "epoch": 0.00012496056150029556,
      "grad_norm": 9571.761593353649,
      "learning_rate": 5.391873776073225e-07,
      "loss": 1.7882,
      "step": 34432
    },
    {
      "epoch": 0.00012507669585113226,
      "grad_norm": 9274.247139256102,
      "learning_rate": 5.389367457689729e-07,
      "loss": 1.8004,
      "step": 34464
    },
    {
      "epoch": 0.00012519283020196896,
      "grad_norm": 9723.652194520328,
      "learning_rate": 5.386864631115168e-07,
      "loss": 1.8163,
      "step": 34496
    },
    {
      "epoch": 0.00012530896455280566,
      "grad_norm": 9197.315804081101,
      "learning_rate": 5.384365288249069e-07,
      "loss": 1.8153,
      "step": 34528
    },
    {
      "epoch": 0.00012542509890364238,
      "grad_norm": 9670.798260743524,
      "learning_rate": 5.381869421017248e-07,
      "loss": 1.8326,
      "step": 34560
    },
    {
      "epoch": 0.00012554123325447908,
      "grad_norm": 9104.74250047743,
      "learning_rate": 5.37937702137169e-07,
      "loss": 1.8039,
      "step": 34592
    },
    {
      "epoch": 0.00012565736760531578,
      "grad_norm": 10501.331629845807,
      "learning_rate": 5.37688808129045e-07,
      "loss": 1.8222,
      "step": 34624
    },
    {
      "epoch": 0.00012577350195615248,
      "grad_norm": 9026.963387540685,
      "learning_rate": 5.374402592777538e-07,
      "loss": 1.8342,
      "step": 34656
    },
    {
      "epoch": 0.00012588963630698918,
      "grad_norm": 10691.265032726482,
      "learning_rate": 5.371920547862814e-07,
      "loss": 1.8403,
      "step": 34688
    },
    {
      "epoch": 0.0001260057706578259,
      "grad_norm": 8968.398073234706,
      "learning_rate": 5.369441938601881e-07,
      "loss": 1.8208,
      "step": 34720
    },
    {
      "epoch": 0.0001261219050086626,
      "grad_norm": 9965.9238407686,
      "learning_rate": 5.366966757075974e-07,
      "loss": 1.7938,
      "step": 34752
    },
    {
      "epoch": 0.0001262380393594993,
      "grad_norm": 7794.61827160253,
      "learning_rate": 5.364494995391862e-07,
      "loss": 1.766,
      "step": 34784
    },
    {
      "epoch": 0.000126354173710336,
      "grad_norm": 11785.600875644823,
      "learning_rate": 5.362026645681733e-07,
      "loss": 1.7832,
      "step": 34816
    },
    {
      "epoch": 0.00012647030806117274,
      "grad_norm": 10136.13239850388,
      "learning_rate": 5.359638678202763e-07,
      "loss": 1.7912,
      "step": 34848
    },
    {
      "epoch": 0.00012658644241200944,
      "grad_norm": 10028.032808083548,
      "learning_rate": 5.357177022921594e-07,
      "loss": 1.7986,
      "step": 34880
    },
    {
      "epoch": 0.00012670257676284614,
      "grad_norm": 9763.269227057093,
      "learning_rate": 5.354718756405593e-07,
      "loss": 1.7873,
      "step": 34912
    },
    {
      "epoch": 0.00012681871111368284,
      "grad_norm": 8561.631094598739,
      "learning_rate": 5.352263870886816e-07,
      "loss": 1.7864,
      "step": 34944
    },
    {
      "epoch": 0.00012693484546451954,
      "grad_norm": 9396.92013374595,
      "learning_rate": 5.349812358622224e-07,
      "loss": 1.8073,
      "step": 34976
    },
    {
      "epoch": 0.00012705097981535626,
      "grad_norm": 10369.76836771198,
      "learning_rate": 5.347364211893584e-07,
      "loss": 1.7988,
      "step": 35008
    },
    {
      "epoch": 0.00012716711416619296,
      "grad_norm": 8767.541616667697,
      "learning_rate": 5.34491942300736e-07,
      "loss": 1.8077,
      "step": 35040
    },
    {
      "epoch": 0.00012728324851702966,
      "grad_norm": 10053.234205965759,
      "learning_rate": 5.342477984294618e-07,
      "loss": 1.7877,
      "step": 35072
    },
    {
      "epoch": 0.00012739938286786636,
      "grad_norm": 9637.1439752657,
      "learning_rate": 5.340039888110923e-07,
      "loss": 1.7836,
      "step": 35104
    },
    {
      "epoch": 0.0001275155172187031,
      "grad_norm": 10628.680915334697,
      "learning_rate": 5.337605126836239e-07,
      "loss": 1.7993,
      "step": 35136
    },
    {
      "epoch": 0.0001276316515695398,
      "grad_norm": 10524.607356096472,
      "learning_rate": 5.335173692874823e-07,
      "loss": 1.8044,
      "step": 35168
    },
    {
      "epoch": 0.0001277477859203765,
      "grad_norm": 9187.142754959237,
      "learning_rate": 5.332745578655137e-07,
      "loss": 1.8083,
      "step": 35200
    },
    {
      "epoch": 0.0001278639202712132,
      "grad_norm": 11053.249476963778,
      "learning_rate": 5.330320776629739e-07,
      "loss": 1.8321,
      "step": 35232
    },
    {
      "epoch": 0.0001279800546220499,
      "grad_norm": 8618.088303098317,
      "learning_rate": 5.327899279275186e-07,
      "loss": 1.8352,
      "step": 35264
    },
    {
      "epoch": 0.00012809618897288662,
      "grad_norm": 8650.504378358524,
      "learning_rate": 5.325481079091945e-07,
      "loss": 1.8271,
      "step": 35296
    },
    {
      "epoch": 0.00012821232332372332,
      "grad_norm": 9901.661274755868,
      "learning_rate": 5.323066168604281e-07,
      "loss": 1.8172,
      "step": 35328
    },
    {
      "epoch": 0.00012832845767456002,
      "grad_norm": 9310.255904109188,
      "learning_rate": 5.32065454036017e-07,
      "loss": 1.8042,
      "step": 35360
    },
    {
      "epoch": 0.00012844459202539672,
      "grad_norm": 9143.634835228275,
      "learning_rate": 5.318246186931198e-07,
      "loss": 1.8033,
      "step": 35392
    },
    {
      "epoch": 0.00012856072637623344,
      "grad_norm": 8824.745322104201,
      "learning_rate": 5.315841100912473e-07,
      "loss": 1.8146,
      "step": 35424
    },
    {
      "epoch": 0.00012867686072707014,
      "grad_norm": 10104.371628161744,
      "learning_rate": 5.313439274922516e-07,
      "loss": 1.8217,
      "step": 35456
    },
    {
      "epoch": 0.00012879299507790684,
      "grad_norm": 9235.457108340657,
      "learning_rate": 5.311040701603173e-07,
      "loss": 1.8263,
      "step": 35488
    },
    {
      "epoch": 0.00012890912942874354,
      "grad_norm": 10607.588604390727,
      "learning_rate": 5.308645373619525e-07,
      "loss": 1.7899,
      "step": 35520
    },
    {
      "epoch": 0.00012902526377958024,
      "grad_norm": 11341.193587978296,
      "learning_rate": 5.306253283659786e-07,
      "loss": 1.7765,
      "step": 35552
    },
    {
      "epoch": 0.00012914139813041697,
      "grad_norm": 10145.023607661049,
      "learning_rate": 5.303864424435213e-07,
      "loss": 1.7763,
      "step": 35584
    },
    {
      "epoch": 0.00012925753248125367,
      "grad_norm": 9289.70607715874,
      "learning_rate": 5.30147878868001e-07,
      "loss": 1.787,
      "step": 35616
    },
    {
      "epoch": 0.00012937366683209037,
      "grad_norm": 9923.036833550503,
      "learning_rate": 5.299096369151238e-07,
      "loss": 1.7826,
      "step": 35648
    },
    {
      "epoch": 0.00012948980118292707,
      "grad_norm": 10214.920655590038,
      "learning_rate": 5.296717158628724e-07,
      "loss": 1.7813,
      "step": 35680
    },
    {
      "epoch": 0.0001296059355337638,
      "grad_norm": 9477.113695635397,
      "learning_rate": 5.294341149914966e-07,
      "loss": 1.7779,
      "step": 35712
    },
    {
      "epoch": 0.0001297220698846005,
      "grad_norm": 10719.827890409435,
      "learning_rate": 5.29196833583504e-07,
      "loss": 1.804,
      "step": 35744
    },
    {
      "epoch": 0.0001298382042354372,
      "grad_norm": 9178.578212337683,
      "learning_rate": 5.289598709236513e-07,
      "loss": 1.8221,
      "step": 35776
    },
    {
      "epoch": 0.0001299543385862739,
      "grad_norm": 9756.75632574679,
      "learning_rate": 5.28723226298935e-07,
      "loss": 1.8139,
      "step": 35808
    },
    {
      "epoch": 0.0001300704729371106,
      "grad_norm": 10201.26737224351,
      "learning_rate": 5.284868989985828e-07,
      "loss": 1.797,
      "step": 35840
    },
    {
      "epoch": 0.00013018660728794732,
      "grad_norm": 9764.11060977906,
      "learning_rate": 5.282582588624333e-07,
      "loss": 1.811,
      "step": 35872
    },
    {
      "epoch": 0.00013030274163878402,
      "grad_norm": 9763.852825601172,
      "learning_rate": 5.280225542258686e-07,
      "loss": 1.8111,
      "step": 35904
    },
    {
      "epoch": 0.00013041887598962072,
      "grad_norm": 9507.237032913401,
      "learning_rate": 5.27787164816617e-07,
      "loss": 1.7874,
      "step": 35936
    },
    {
      "epoch": 0.00013053501034045742,
      "grad_norm": 10122.094743678308,
      "learning_rate": 5.275520899326718e-07,
      "loss": 1.8122,
      "step": 35968
    },
    {
      "epoch": 0.00013065114469129415,
      "grad_norm": 9845.089029561896,
      "learning_rate": 5.273173288742128e-07,
      "loss": 1.8148,
      "step": 36000
    },
    {
      "epoch": 0.00013076727904213085,
      "grad_norm": 9872.12905102035,
      "learning_rate": 5.270828809435982e-07,
      "loss": 1.8124,
      "step": 36032
    },
    {
      "epoch": 0.00013088341339296755,
      "grad_norm": 10383.39193134883,
      "learning_rate": 5.268487454453553e-07,
      "loss": 1.8386,
      "step": 36064
    },
    {
      "epoch": 0.00013099954774380425,
      "grad_norm": 10928.88750056473,
      "learning_rate": 5.266149216861721e-07,
      "loss": 1.8496,
      "step": 36096
    },
    {
      "epoch": 0.00013111568209464095,
      "grad_norm": 10804.480366958885,
      "learning_rate": 5.263814089748883e-07,
      "loss": 1.8061,
      "step": 36128
    },
    {
      "epoch": 0.00013123181644547768,
      "grad_norm": 9299.121786491454,
      "learning_rate": 5.261482066224877e-07,
      "loss": 1.8061,
      "step": 36160
    },
    {
      "epoch": 0.00013134795079631438,
      "grad_norm": 10260.993031865873,
      "learning_rate": 5.259153139420881e-07,
      "loss": 1.8105,
      "step": 36192
    },
    {
      "epoch": 0.00013146408514715108,
      "grad_norm": 9651.759839531856,
      "learning_rate": 5.256827302489347e-07,
      "loss": 1.798,
      "step": 36224
    },
    {
      "epoch": 0.00013158021949798778,
      "grad_norm": 9012.44572799193,
      "learning_rate": 5.254504548603896e-07,
      "loss": 1.8008,
      "step": 36256
    },
    {
      "epoch": 0.0001316963538488245,
      "grad_norm": 10047.174926316353,
      "learning_rate": 5.252184870959254e-07,
      "loss": 1.7734,
      "step": 36288
    },
    {
      "epoch": 0.0001318124881996612,
      "grad_norm": 8267.63146251694,
      "learning_rate": 5.249868262771151e-07,
      "loss": 1.777,
      "step": 36320
    },
    {
      "epoch": 0.0001319286225504979,
      "grad_norm": 8835.988456307534,
      "learning_rate": 5.247554717276248e-07,
      "loss": 1.7944,
      "step": 36352
    },
    {
      "epoch": 0.0001320447569013346,
      "grad_norm": 10059.45058141845,
      "learning_rate": 5.245244227732052e-07,
      "loss": 1.8259,
      "step": 36384
    },
    {
      "epoch": 0.0001321608912521713,
      "grad_norm": 10215.203081681733,
      "learning_rate": 5.242936787416832e-07,
      "loss": 1.7902,
      "step": 36416
    },
    {
      "epoch": 0.00013227702560300803,
      "grad_norm": 10589.26409152213,
      "learning_rate": 5.240632389629537e-07,
      "loss": 1.7939,
      "step": 36448
    },
    {
      "epoch": 0.00013239315995384473,
      "grad_norm": 9930.997130197953,
      "learning_rate": 5.238331027689714e-07,
      "loss": 1.8072,
      "step": 36480
    },
    {
      "epoch": 0.00013250929430468143,
      "grad_norm": 9522.016593138242,
      "learning_rate": 5.236032694937432e-07,
      "loss": 1.8057,
      "step": 36512
    },
    {
      "epoch": 0.00013262542865551813,
      "grad_norm": 9539.63081046641,
      "learning_rate": 5.233737384733188e-07,
      "loss": 1.7882,
      "step": 36544
    },
    {
      "epoch": 0.00013274156300635483,
      "grad_norm": 10920.089102200585,
      "learning_rate": 5.231445090457846e-07,
      "loss": 1.7872,
      "step": 36576
    },
    {
      "epoch": 0.00013285769735719156,
      "grad_norm": 9876.935962129146,
      "learning_rate": 5.229155805512536e-07,
      "loss": 1.7694,
      "step": 36608
    },
    {
      "epoch": 0.00013297383170802826,
      "grad_norm": 8165.817656548546,
      "learning_rate": 5.226869523318588e-07,
      "loss": 1.7909,
      "step": 36640
    },
    {
      "epoch": 0.00013308996605886496,
      "grad_norm": 10663.324059597928,
      "learning_rate": 5.224586237317451e-07,
      "loss": 1.8105,
      "step": 36672
    },
    {
      "epoch": 0.00013320610040970166,
      "grad_norm": 8540.328916382554,
      "learning_rate": 5.222305940970607e-07,
      "loss": 1.8111,
      "step": 36704
    },
    {
      "epoch": 0.00013332223476053838,
      "grad_norm": 9084.050087928841,
      "learning_rate": 5.220028627759501e-07,
      "loss": 1.8036,
      "step": 36736
    },
    {
      "epoch": 0.00013343836911137508,
      "grad_norm": 9954.136426631896,
      "learning_rate": 5.217754291185455e-07,
      "loss": 1.8101,
      "step": 36768
    },
    {
      "epoch": 0.00013355450346221178,
      "grad_norm": 10751.017533238424,
      "learning_rate": 5.215482924769597e-07,
      "loss": 1.8231,
      "step": 36800
    },
    {
      "epoch": 0.00013367063781304848,
      "grad_norm": 10517.416602949605,
      "learning_rate": 5.213214522052779e-07,
      "loss": 1.8134,
      "step": 36832
    },
    {
      "epoch": 0.00013378677216388518,
      "grad_norm": 10342.300034325053,
      "learning_rate": 5.211019827066655e-07,
      "loss": 1.8096,
      "step": 36864
    },
    {
      "epoch": 0.0001339029065147219,
      "grad_norm": 10278.419528312706,
      "learning_rate": 5.208757240332232e-07,
      "loss": 1.8139,
      "step": 36896
    },
    {
      "epoch": 0.0001340190408655586,
      "grad_norm": 10677.809419539197,
      "learning_rate": 5.206497598236704e-07,
      "loss": 1.8159,
      "step": 36928
    },
    {
      "epoch": 0.0001341351752163953,
      "grad_norm": 9587.099352776104,
      "learning_rate": 5.204240894398452e-07,
      "loss": 1.8381,
      "step": 36960
    },
    {
      "epoch": 0.000134251309567232,
      "grad_norm": 9781.598540116027,
      "learning_rate": 5.201987122455202e-07,
      "loss": 1.831,
      "step": 36992
    },
    {
      "epoch": 0.00013436744391806874,
      "grad_norm": 9405.350392196986,
      "learning_rate": 5.199736276063948e-07,
      "loss": 1.8147,
      "step": 37024
    },
    {
      "epoch": 0.00013448357826890544,
      "grad_norm": 8526.987041153516,
      "learning_rate": 5.19748834890088e-07,
      "loss": 1.791,
      "step": 37056
    },
    {
      "epoch": 0.00013459971261974214,
      "grad_norm": 10087.934377264753,
      "learning_rate": 5.195243334661312e-07,
      "loss": 1.786,
      "step": 37088
    },
    {
      "epoch": 0.00013471584697057884,
      "grad_norm": 11440.525774631164,
      "learning_rate": 5.193001227059598e-07,
      "loss": 1.7845,
      "step": 37120
    },
    {
      "epoch": 0.00013483198132141554,
      "grad_norm": 9066.434028878168,
      "learning_rate": 5.190762019829071e-07,
      "loss": 1.7771,
      "step": 37152
    },
    {
      "epoch": 0.00013494811567225226,
      "grad_norm": 8858.133099022614,
      "learning_rate": 5.188525706721958e-07,
      "loss": 1.774,
      "step": 37184
    },
    {
      "epoch": 0.00013506425002308896,
      "grad_norm": 9525.947092021876,
      "learning_rate": 5.186292281509316e-07,
      "loss": 1.7846,
      "step": 37216
    },
    {
      "epoch": 0.00013518038437392566,
      "grad_norm": 10042.790050578575,
      "learning_rate": 5.18406173798095e-07,
      "loss": 1.8192,
      "step": 37248
    },
    {
      "epoch": 0.00013529651872476236,
      "grad_norm": 10213.965733249745,
      "learning_rate": 5.18183406994535e-07,
      "loss": 1.8017,
      "step": 37280
    },
    {
      "epoch": 0.0001354126530755991,
      "grad_norm": 9189.48834266631,
      "learning_rate": 5.179609271229609e-07,
      "loss": 1.8129,
      "step": 37312
    },
    {
      "epoch": 0.0001355287874264358,
      "grad_norm": 10606.450395867601,
      "learning_rate": 5.177387335679361e-07,
      "loss": 1.7714,
      "step": 37344
    },
    {
      "epoch": 0.0001356449217772725,
      "grad_norm": 8959.763724563276,
      "learning_rate": 5.175168257158703e-07,
      "loss": 1.7747,
      "step": 37376
    },
    {
      "epoch": 0.0001357610561281092,
      "grad_norm": 9045.634416667523,
      "learning_rate": 5.172952029550126e-07,
      "loss": 1.7751,
      "step": 37408
    },
    {
      "epoch": 0.0001358771904789459,
      "grad_norm": 9448.961424410621,
      "learning_rate": 5.17073864675444e-07,
      "loss": 1.7735,
      "step": 37440
    },
    {
      "epoch": 0.00013599332482978262,
      "grad_norm": 9154.839594444024,
      "learning_rate": 5.168528102690716e-07,
      "loss": 1.7962,
      "step": 37472
    },
    {
      "epoch": 0.00013610945918061932,
      "grad_norm": 9757.293989626427,
      "learning_rate": 5.166320391296196e-07,
      "loss": 1.8143,
      "step": 37504
    },
    {
      "epoch": 0.00013622559353145602,
      "grad_norm": 9583.803942068096,
      "learning_rate": 5.164115506526242e-07,
      "loss": 1.8156,
      "step": 37536
    },
    {
      "epoch": 0.00013634172788229272,
      "grad_norm": 10468.593792864445,
      "learning_rate": 5.161913442354258e-07,
      "loss": 1.8221,
      "step": 37568
    },
    {
      "epoch": 0.00013645786223312944,
      "grad_norm": 10525.196150191216,
      "learning_rate": 5.159714192771618e-07,
      "loss": 1.8399,
      "step": 37600
    },
    {
      "epoch": 0.00013657399658396614,
      "grad_norm": 9312.44897972601,
      "learning_rate": 5.157517751787604e-07,
      "loss": 1.8475,
      "step": 37632
    },
    {
      "epoch": 0.00013669013093480284,
      "grad_norm": 10995.919788721632,
      "learning_rate": 5.155324113429333e-07,
      "loss": 1.8329,
      "step": 37664
    },
    {
      "epoch": 0.00013680626528563954,
      "grad_norm": 9409.946120993467,
      "learning_rate": 5.153133271741687e-07,
      "loss": 1.8158,
      "step": 37696
    },
    {
      "epoch": 0.00013692239963647624,
      "grad_norm": 9624.85573917864,
      "learning_rate": 5.150945220787257e-07,
      "loss": 1.816,
      "step": 37728
    },
    {
      "epoch": 0.00013703853398731297,
      "grad_norm": 9577.610557962775,
      "learning_rate": 5.148759954646255e-07,
      "loss": 1.8064,
      "step": 37760
    },
    {
      "epoch": 0.00013715466833814967,
      "grad_norm": 9115.252272976322,
      "learning_rate": 5.146577467416465e-07,
      "loss": 1.7828,
      "step": 37792
    },
    {
      "epoch": 0.00013727080268898637,
      "grad_norm": 10480.795198838683,
      "learning_rate": 5.144397753213169e-07,
      "loss": 1.8067,
      "step": 37824
    },
    {
      "epoch": 0.00013738693703982307,
      "grad_norm": 9934.34164904751,
      "learning_rate": 5.14228879393665e-07,
      "loss": 1.7747,
      "step": 37856
    },
    {
      "epoch": 0.0001375030713906598,
      "grad_norm": 10789.343538881316,
      "learning_rate": 5.140114521999305e-07,
      "loss": 1.7872,
      "step": 37888
    },
    {
      "epoch": 0.0001376192057414965,
      "grad_norm": 8397.037691948275,
      "learning_rate": 5.137943005720629e-07,
      "loss": 1.7958,
      "step": 37920
    },
    {
      "epoch": 0.0001377353400923332,
      "grad_norm": 10017.169560309938,
      "learning_rate": 5.135774239284693e-07,
      "loss": 1.7707,
      "step": 37952
    },
    {
      "epoch": 0.0001378514744431699,
      "grad_norm": 11404.494552587588,
      "learning_rate": 5.133608216892743e-07,
      "loss": 1.7849,
      "step": 37984
    },
    {
      "epoch": 0.0001379676087940066,
      "grad_norm": 9464.45434243306,
      "learning_rate": 5.131444932763126e-07,
      "loss": 1.7922,
      "step": 38016
    },
    {
      "epoch": 0.00013808374314484332,
      "grad_norm": 9724.235085599279,
      "learning_rate": 5.12928438113123e-07,
      "loss": 1.798,
      "step": 38048
    },
    {
      "epoch": 0.00013819987749568002,
      "grad_norm": 10962.019704415789,
      "learning_rate": 5.127126556249418e-07,
      "loss": 1.7995,
      "step": 38080
    },
    {
      "epoch": 0.00013831601184651672,
      "grad_norm": 10222.165621823979,
      "learning_rate": 5.124971452386966e-07,
      "loss": 1.788,
      "step": 38112
    },
    {
      "epoch": 0.00013843214619735342,
      "grad_norm": 9690.273886738185,
      "learning_rate": 5.122819063829999e-07,
      "loss": 1.7788,
      "step": 38144
    },
    {
      "epoch": 0.00013854828054819015,
      "grad_norm": 9930.969136997657,
      "learning_rate": 5.120669384881421e-07,
      "loss": 1.7896,
      "step": 38176
    },
    {
      "epoch": 0.00013866441489902685,
      "grad_norm": 9068.446724770456,
      "learning_rate": 5.118522409860862e-07,
      "loss": 1.8128,
      "step": 38208
    },
    {
      "epoch": 0.00013878054924986355,
      "grad_norm": 10765.338266863702,
      "learning_rate": 5.116378133104606e-07,
      "loss": 1.8326,
      "step": 38240
    },
    {
      "epoch": 0.00013889668360070025,
      "grad_norm": 9745.511582261857,
      "learning_rate": 5.114236548965533e-07,
      "loss": 1.8047,
      "step": 38272
    },
    {
      "epoch": 0.00013901281795153695,
      "grad_norm": 10723.35637755269,
      "learning_rate": 5.112097651813055e-07,
      "loss": 1.7938,
      "step": 38304
    },
    {
      "epoch": 0.00013912895230237368,
      "grad_norm": 8198.645619856978,
      "learning_rate": 5.109961436033055e-07,
      "loss": 1.8162,
      "step": 38336
    },
    {
      "epoch": 0.00013924508665321038,
      "grad_norm": 10059.390737017824,
      "learning_rate": 5.107827896027823e-07,
      "loss": 1.8069,
      "step": 38368
    },
    {
      "epoch": 0.00013936122100404708,
      "grad_norm": 10858.433404501775,
      "learning_rate": 5.105697026215995e-07,
      "loss": 1.8164,
      "step": 38400
    },
    {
      "epoch": 0.00013947735535488378,
      "grad_norm": 8943.205745145306,
      "learning_rate": 5.103568821032497e-07,
      "loss": 1.8267,
      "step": 38432
    },
    {
      "epoch": 0.0001395934897057205,
      "grad_norm": 9247.66705715555,
      "learning_rate": 5.101443274928474e-07,
      "loss": 1.8088,
      "step": 38464
    },
    {
      "epoch": 0.0001397096240565572,
      "grad_norm": 8950.790467886063,
      "learning_rate": 5.099320382371235e-07,
      "loss": 1.8232,
      "step": 38496
    },
    {
      "epoch": 0.0001398257584073939,
      "grad_norm": 9236.24739815906,
      "learning_rate": 5.097200137844197e-07,
      "loss": 1.8159,
      "step": 38528
    },
    {
      "epoch": 0.0001399418927582306,
      "grad_norm": 8704.177847447741,
      "learning_rate": 5.095082535846815e-07,
      "loss": 1.7829,
      "step": 38560
    },
    {
      "epoch": 0.0001400580271090673,
      "grad_norm": 9678.680695218745,
      "learning_rate": 5.092967570894525e-07,
      "loss": 1.782,
      "step": 38592
    },
    {
      "epoch": 0.00014017416145990403,
      "grad_norm": 8952.346731444219,
      "learning_rate": 5.090855237518695e-07,
      "loss": 1.779,
      "step": 38624
    },
    {
      "epoch": 0.00014029029581074073,
      "grad_norm": 10617.411643145424,
      "learning_rate": 5.088745530266547e-07,
      "loss": 1.8049,
      "step": 38656
    },
    {
      "epoch": 0.00014040643016157743,
      "grad_norm": 10830.907348878947,
      "learning_rate": 5.086638443701114e-07,
      "loss": 1.7939,
      "step": 38688
    },
    {
      "epoch": 0.00014052256451241413,
      "grad_norm": 11389.804388135908,
      "learning_rate": 5.084533972401172e-07,
      "loss": 1.7765,
      "step": 38720
    },
    {
      "epoch": 0.00014063869886325086,
      "grad_norm": 8654.964355790265,
      "learning_rate": 5.082432110961187e-07,
      "loss": 1.7968,
      "step": 38752
    },
    {
      "epoch": 0.00014075483321408756,
      "grad_norm": 8510.807893496363,
      "learning_rate": 5.080332853991251e-07,
      "loss": 1.8042,
      "step": 38784
    },
    {
      "epoch": 0.00014087096756492426,
      "grad_norm": 9154.983014730286,
      "learning_rate": 5.078236196117028e-07,
      "loss": 1.8248,
      "step": 38816
    },
    {
      "epoch": 0.00014098710191576096,
      "grad_norm": 10810.65788932385,
      "learning_rate": 5.076142131979696e-07,
      "loss": 1.8261,
      "step": 38848
    },
    {
      "epoch": 0.00014110323626659766,
      "grad_norm": 9860.611948555728,
      "learning_rate": 5.074115975726061e-07,
      "loss": 1.7626,
      "step": 38880
    },
    {
      "epoch": 0.00014121937061743438,
      "grad_norm": 9071.39206516839,
      "learning_rate": 5.072027002407504e-07,
      "loss": 1.779,
      "step": 38912
    },
    {
      "epoch": 0.00014133550496827108,
      "grad_norm": 9159.410898087277,
      "learning_rate": 5.069940607007664e-07,
      "loss": 1.7906,
      "step": 38944
    },
    {
      "epoch": 0.00014145163931910778,
      "grad_norm": 9885.632503790539,
      "learning_rate": 5.067856784228719e-07,
      "loss": 1.7966,
      "step": 38976
    },
    {
      "epoch": 0.00014156777366994448,
      "grad_norm": 9532.571321527052,
      "learning_rate": 5.06577552878808e-07,
      "loss": 1.7988,
      "step": 39008
    },
    {
      "epoch": 0.0001416839080207812,
      "grad_norm": 10009.545843843265,
      "learning_rate": 5.063696835418334e-07,
      "loss": 1.8066,
      "step": 39040
    },
    {
      "epoch": 0.0001418000423716179,
      "grad_norm": 8955.200165267106,
      "learning_rate": 5.061620698867178e-07,
      "loss": 1.796,
      "step": 39072
    },
    {
      "epoch": 0.0001419161767224546,
      "grad_norm": 11524.657912493542,
      "learning_rate": 5.059547113897378e-07,
      "loss": 1.8124,
      "step": 39104
    },
    {
      "epoch": 0.0001420323110732913,
      "grad_norm": 10358.70349030225,
      "learning_rate": 5.057476075286704e-07,
      "loss": 1.814,
      "step": 39136
    },
    {
      "epoch": 0.000142148445424128,
      "grad_norm": 9843.661005946922,
      "learning_rate": 5.055407577827876e-07,
      "loss": 1.8116,
      "step": 39168
    },
    {
      "epoch": 0.00014226457977496474,
      "grad_norm": 10496.754831851606,
      "learning_rate": 5.053341616328513e-07,
      "loss": 1.8069,
      "step": 39200
    },
    {
      "epoch": 0.00014238071412580144,
      "grad_norm": 10111.633893688992,
      "learning_rate": 5.051278185611073e-07,
      "loss": 1.8034,
      "step": 39232
    },
    {
      "epoch": 0.00014249684847663814,
      "grad_norm": 9307.878383391137,
      "learning_rate": 5.049217280512801e-07,
      "loss": 1.8145,
      "step": 39264
    },
    {
      "epoch": 0.00014261298282747484,
      "grad_norm": 8759.9964611865,
      "learning_rate": 5.047158895885676e-07,
      "loss": 1.7964,
      "step": 39296
    },
    {
      "epoch": 0.00014272911717831154,
      "grad_norm": 10278.685713650359,
      "learning_rate": 5.045103026596355e-07,
      "loss": 1.7833,
      "step": 39328
    },
    {
      "epoch": 0.00014284525152914826,
      "grad_norm": 9890.985036890916,
      "learning_rate": 5.043049667526119e-07,
      "loss": 1.7938,
      "step": 39360
    },
    {
      "epoch": 0.00014296138587998496,
      "grad_norm": 10696.772223432637,
      "learning_rate": 5.040998813570823e-07,
      "loss": 1.8041,
      "step": 39392
    },
    {
      "epoch": 0.00014307752023082166,
      "grad_norm": 9447.086111600762,
      "learning_rate": 5.038950459640838e-07,
      "loss": 1.8195,
      "step": 39424
    },
    {
      "epoch": 0.00014319365458165836,
      "grad_norm": 9480.944151296326,
      "learning_rate": 5.036904600661e-07,
      "loss": 1.8047,
      "step": 39456
    },
    {
      "epoch": 0.0001433097889324951,
      "grad_norm": 10462.966978825843,
      "learning_rate": 5.034861231570557e-07,
      "loss": 1.7836,
      "step": 39488
    },
    {
      "epoch": 0.0001434259232833318,
      "grad_norm": 9080.353737602958,
      "learning_rate": 5.032820347323117e-07,
      "loss": 1.809,
      "step": 39520
    },
    {
      "epoch": 0.0001435420576341685,
      "grad_norm": 9895.220563484172,
      "learning_rate": 5.030781942886598e-07,
      "loss": 1.7979,
      "step": 39552
    },
    {
      "epoch": 0.0001436581919850052,
      "grad_norm": 10133.763762788236,
      "learning_rate": 5.028746013243171e-07,
      "loss": 1.7986,
      "step": 39584
    },
    {
      "epoch": 0.0001437743263358419,
      "grad_norm": 11658.413614210125,
      "learning_rate": 5.026712553389207e-07,
      "loss": 1.782,
      "step": 39616
    },
    {
      "epoch": 0.00014389046068667862,
      "grad_norm": 9427.97104365515,
      "learning_rate": 5.024681558335236e-07,
      "loss": 1.7668,
      "step": 39648
    },
    {
      "epoch": 0.00014400659503751532,
      "grad_norm": 9042.502750898117,
      "learning_rate": 5.02265302310588e-07,
      "loss": 1.7647,
      "step": 39680
    },
    {
      "epoch": 0.00014412272938835202,
      "grad_norm": 10459.765771756078,
      "learning_rate": 5.020626942739816e-07,
      "loss": 1.7738,
      "step": 39712
    },
    {
      "epoch": 0.00014423886373918872,
      "grad_norm": 9894.699894387904,
      "learning_rate": 5.018603312289718e-07,
      "loss": 1.8015,
      "step": 39744
    },
    {
      "epoch": 0.00014435499809002544,
      "grad_norm": 10942.750294144522,
      "learning_rate": 5.016582126822206e-07,
      "loss": 1.8217,
      "step": 39776
    },
    {
      "epoch": 0.00014447113244086214,
      "grad_norm": 11714.93294901853,
      "learning_rate": 5.014563381417797e-07,
      "loss": 1.8101,
      "step": 39808
    },
    {
      "epoch": 0.00014458726679169884,
      "grad_norm": 9773.831899516177,
      "learning_rate": 5.012547071170856e-07,
      "loss": 1.8062,
      "step": 39840
    },
    {
      "epoch": 0.00014470340114253554,
      "grad_norm": 10003.390825115252,
      "learning_rate": 5.010596088201202e-07,
      "loss": 1.8076,
      "step": 39872
    },
    {
      "epoch": 0.00014481953549337224,
      "grad_norm": 9989.027079751062,
      "learning_rate": 5.008584557887765e-07,
      "loss": 1.8034,
      "step": 39904
    },
    {
      "epoch": 0.00014493566984420897,
      "grad_norm": 9356.380924267673,
      "learning_rate": 5.006575448249273e-07,
      "loss": 1.8076,
      "step": 39936
    },
    {
      "epoch": 0.00014505180419504567,
      "grad_norm": 10181.599776066627,
      "learning_rate": 5.004568754434552e-07,
      "loss": 1.82,
      "step": 39968
    },
    {
      "epoch": 0.00014516793854588237,
      "grad_norm": 8822.133868855086,
      "learning_rate": 5.002564471606027e-07,
      "loss": 1.8128,
      "step": 40000
    },
    {
      "epoch": 0.00014528407289671907,
      "grad_norm": 11085.410051053592,
      "learning_rate": 5.000562594939676e-07,
      "loss": 1.8198,
      "step": 40032
    },
    {
      "epoch": 0.0001454002072475558,
      "grad_norm": 10579.649143520783,
      "learning_rate": 4.998563119624979e-07,
      "loss": 1.7869,
      "step": 40064
    },
    {
      "epoch": 0.0001455163415983925,
      "grad_norm": 9860.949244367906,
      "learning_rate": 4.996566040864867e-07,
      "loss": 1.7834,
      "step": 40096
    },
    {
      "epoch": 0.0001456324759492292,
      "grad_norm": 10446.8403835801,
      "learning_rate": 4.994571353875677e-07,
      "loss": 1.7794,
      "step": 40128
    },
    {
      "epoch": 0.0001457486103000659,
      "grad_norm": 9485.708091650302,
      "learning_rate": 4.992579053887108e-07,
      "loss": 1.7831,
      "step": 40160
    },
    {
      "epoch": 0.0001458647446509026,
      "grad_norm": 10208.547888901732,
      "learning_rate": 4.990589136142163e-07,
      "loss": 1.8023,
      "step": 40192
    },
    {
      "epoch": 0.00014598087900173932,
      "grad_norm": 10451.42755799417,
      "learning_rate": 4.988601595897107e-07,
      "loss": 1.7773,
      "step": 40224
    },
    {
      "epoch": 0.00014609701335257602,
      "grad_norm": 9266.076192218581,
      "learning_rate": 4.986616428421421e-07,
      "loss": 1.7853,
      "step": 40256
    },
    {
      "epoch": 0.00014621314770341272,
      "grad_norm": 10853.0421541612,
      "learning_rate": 4.984633628997752e-07,
      "loss": 1.7934,
      "step": 40288
    },
    {
      "epoch": 0.00014632928205424942,
      "grad_norm": 9969.080599533741,
      "learning_rate": 4.982653192921868e-07,
      "loss": 1.8021,
      "step": 40320
    },
    {
      "epoch": 0.00014644541640508615,
      "grad_norm": 8676.266478157526,
      "learning_rate": 4.980675115502606e-07,
      "loss": 1.8127,
      "step": 40352
    },
    {
      "epoch": 0.00014656155075592285,
      "grad_norm": 11879.585851367041,
      "learning_rate": 4.978699392061833e-07,
      "loss": 1.808,
      "step": 40384
    },
    {
      "epoch": 0.00014667768510675955,
      "grad_norm": 10675.083231525645,
      "learning_rate": 4.976726017934395e-07,
      "loss": 1.7758,
      "step": 40416
    },
    {
      "epoch": 0.00014679381945759625,
      "grad_norm": 9832.935268779105,
      "learning_rate": 4.974754988468071e-07,
      "loss": 1.7712,
      "step": 40448
    },
    {
      "epoch": 0.00014690995380843295,
      "grad_norm": 9878.628042395361,
      "learning_rate": 4.972786299023524e-07,
      "loss": 1.7829,
      "step": 40480
    },
    {
      "epoch": 0.00014702608815926968,
      "grad_norm": 9231.069060515147,
      "learning_rate": 4.970819944974265e-07,
      "loss": 1.7994,
      "step": 40512
    },
    {
      "epoch": 0.00014714222251010638,
      "grad_norm": 9447.05403816449,
      "learning_rate": 4.968855921706597e-07,
      "loss": 1.8081,
      "step": 40544
    },
    {
      "epoch": 0.00014725835686094308,
      "grad_norm": 8465.400758381142,
      "learning_rate": 4.966894224619574e-07,
      "loss": 1.8259,
      "step": 40576
    },
    {
      "epoch": 0.00014737449121177978,
      "grad_norm": 10202.346788852063,
      "learning_rate": 4.964934849124953e-07,
      "loss": 1.8166,
      "step": 40608
    },
    {
      "epoch": 0.0001474906255626165,
      "grad_norm": 10222.85234169016,
      "learning_rate": 4.962977790647154e-07,
      "loss": 1.814,
      "step": 40640
    },
    {
      "epoch": 0.0001476067599134532,
      "grad_norm": 10527.252728038782,
      "learning_rate": 4.961023044623213e-07,
      "loss": 1.8307,
      "step": 40672
    },
    {
      "epoch": 0.0001477228942642899,
      "grad_norm": 11449.687681330002,
      "learning_rate": 4.959070606502732e-07,
      "loss": 1.8156,
      "step": 40704
    },
    {
      "epoch": 0.0001478390286151266,
      "grad_norm": 10871.91289516247,
      "learning_rate": 4.957120471747842e-07,
      "loss": 1.7922,
      "step": 40736
    },
    {
      "epoch": 0.0001479551629659633,
      "grad_norm": 8138.031088660205,
      "learning_rate": 4.955172635833157e-07,
      "loss": 1.8059,
      "step": 40768
    },
    {
      "epoch": 0.00014807129731680003,
      "grad_norm": 9389.520967546749,
      "learning_rate": 4.953227094245726e-07,
      "loss": 1.8069,
      "step": 40800
    },
    {
      "epoch": 0.00014818743166763673,
      "grad_norm": 9783.936017779348,
      "learning_rate": 4.951283842484991e-07,
      "loss": 1.76,
      "step": 40832
    },
    {
      "epoch": 0.00014830356601847343,
      "grad_norm": 9368.258749628983,
      "learning_rate": 4.949403496715409e-07,
      "loss": 1.7609,
      "step": 40864
    },
    {
      "epoch": 0.00014841970036931013,
      "grad_norm": 9174.153584936324,
      "learning_rate": 4.947464739946428e-07,
      "loss": 1.7719,
      "step": 40896
    },
    {
      "epoch": 0.00014853583472014686,
      "grad_norm": 10421.355765926044,
      "learning_rate": 4.945528259715703e-07,
      "loss": 1.786,
      "step": 40928
    },
    {
      "epoch": 0.00014865196907098356,
      "grad_norm": 10515.437128336605,
      "learning_rate": 4.943594051571437e-07,
      "loss": 1.7996,
      "step": 40960
    },
    {
      "epoch": 0.00014876810342182026,
      "grad_norm": 9140.664636666199,
      "learning_rate": 4.941662111074008e-07,
      "loss": 1.7956,
      "step": 40992
    },
    {
      "epoch": 0.00014888423777265696,
      "grad_norm": 8807.422324380726,
      "learning_rate": 4.939732433795933e-07,
      "loss": 1.794,
      "step": 41024
    },
    {
      "epoch": 0.00014900037212349366,
      "grad_norm": 11279.460359432094,
      "learning_rate": 4.937805015321818e-07,
      "loss": 1.8004,
      "step": 41056
    },
    {
      "epoch": 0.00014911650647433038,
      "grad_norm": 9397.239488275267,
      "learning_rate": 4.935879851248323e-07,
      "loss": 1.809,
      "step": 41088
    },
    {
      "epoch": 0.00014923264082516708,
      "grad_norm": 9125.297803359625,
      "learning_rate": 4.933956937184114e-07,
      "loss": 1.8032,
      "step": 41120
    },
    {
      "epoch": 0.00014934877517600378,
      "grad_norm": 9173.754520369508,
      "learning_rate": 4.932036268749824e-07,
      "loss": 1.7928,
      "step": 41152
    },
    {
      "epoch": 0.00014946490952684048,
      "grad_norm": 9113.61761322034,
      "learning_rate": 4.930117841578009e-07,
      "loss": 1.8066,
      "step": 41184
    },
    {
      "epoch": 0.0001495810438776772,
      "grad_norm": 8577.589055206598,
      "learning_rate": 4.928201651313108e-07,
      "loss": 1.782,
      "step": 41216
    },
    {
      "epoch": 0.0001496971782285139,
      "grad_norm": 11021.215722414656,
      "learning_rate": 4.926287693611402e-07,
      "loss": 1.806,
      "step": 41248
    },
    {
      "epoch": 0.0001498133125793506,
      "grad_norm": 10450.119138076847,
      "learning_rate": 4.924375964140972e-07,
      "loss": 1.8196,
      "step": 41280
    },
    {
      "epoch": 0.0001499294469301873,
      "grad_norm": 9836.991816607351,
      "learning_rate": 4.922466458581653e-07,
      "loss": 1.8143,
      "step": 41312
    },
    {
      "epoch": 0.000150045581281024,
      "grad_norm": 10159.278025529176,
      "learning_rate": 4.920559172625004e-07,
      "loss": 1.7997,
      "step": 41344
    },
    {
      "epoch": 0.00015016171563186074,
      "grad_norm": 9895.829020349936,
      "learning_rate": 4.918654101974254e-07,
      "loss": 1.8078,
      "step": 41376
    },
    {
      "epoch": 0.00015027784998269744,
      "grad_norm": 9242.73509303388,
      "learning_rate": 4.916751242344272e-07,
      "loss": 1.8034,
      "step": 41408
    },
    {
      "epoch": 0.00015039398433353414,
      "grad_norm": 8437.014045265065,
      "learning_rate": 4.914850589461523e-07,
      "loss": 1.794,
      "step": 41440
    },
    {
      "epoch": 0.00015051011868437084,
      "grad_norm": 10506.424415565934,
      "learning_rate": 4.912952139064024e-07,
      "loss": 1.7926,
      "step": 41472
    },
    {
      "epoch": 0.00015062625303520756,
      "grad_norm": 9612.212544466545,
      "learning_rate": 4.91105588690131e-07,
      "loss": 1.8053,
      "step": 41504
    },
    {
      "epoch": 0.00015074238738604426,
      "grad_norm": 10835.274615809236,
      "learning_rate": 4.909161828734387e-07,
      "loss": 1.8096,
      "step": 41536
    },
    {
      "epoch": 0.00015085852173688096,
      "grad_norm": 9587.784102700685,
      "learning_rate": 4.907269960335703e-07,
      "loss": 1.7996,
      "step": 41568
    },
    {
      "epoch": 0.00015097465608771766,
      "grad_norm": 9987.287319387584,
      "learning_rate": 4.905380277489092e-07,
      "loss": 1.785,
      "step": 41600
    },
    {
      "epoch": 0.00015109079043855436,
      "grad_norm": 9237.191239765472,
      "learning_rate": 4.903492775989753e-07,
      "loss": 1.7738,
      "step": 41632
    },
    {
      "epoch": 0.0001512069247893911,
      "grad_norm": 8488.112393223832,
      "learning_rate": 4.901607451644199e-07,
      "loss": 1.7727,
      "step": 41664
    },
    {
      "epoch": 0.0001513230591402278,
      "grad_norm": 11551.25603560063,
      "learning_rate": 4.899724300270217e-07,
      "loss": 1.788,
      "step": 41696
    },
    {
      "epoch": 0.0001514391934910645,
      "grad_norm": 8852.224127302696,
      "learning_rate": 4.897843317696839e-07,
      "loss": 1.7879,
      "step": 41728
    },
    {
      "epoch": 0.0001515553278419012,
      "grad_norm": 10863.91117415823,
      "learning_rate": 4.895964499764291e-07,
      "loss": 1.7945,
      "step": 41760
    },
    {
      "epoch": 0.0001516714621927379,
      "grad_norm": 9503.417595791527,
      "learning_rate": 4.894087842323964e-07,
      "loss": 1.8108,
      "step": 41792
    },
    {
      "epoch": 0.00015178759654357462,
      "grad_norm": 9018.78761253418,
      "learning_rate": 4.892213341238371e-07,
      "loss": 1.7961,
      "step": 41824
    },
    {
      "epoch": 0.00015190373089441132,
      "grad_norm": 9411.261658247528,
      "learning_rate": 4.890340992381108e-07,
      "loss": 1.8028,
      "step": 41856
    },
    {
      "epoch": 0.00015201986524524802,
      "grad_norm": 10006.55595097534,
      "learning_rate": 4.888529202935496e-07,
      "loss": 1.8021,
      "step": 41888
    },
    {
      "epoch": 0.00015213599959608472,
      "grad_norm": 10256.964365737067,
      "learning_rate": 4.886661079261583e-07,
      "loss": 1.7893,
      "step": 41920
    },
    {
      "epoch": 0.00015225213394692144,
      "grad_norm": 9874.413400298774,
      "learning_rate": 4.884795095630671e-07,
      "loss": 1.7886,
      "step": 41952
    },
    {
      "epoch": 0.00015236826829775814,
      "grad_norm": 9142.649178438382,
      "learning_rate": 4.882931247959974e-07,
      "loss": 1.7732,
      "step": 41984
    },
    {
      "epoch": 0.00015248440264859484,
      "grad_norm": 8692.706252945627,
      "learning_rate": 4.881069532177604e-07,
      "loss": 1.8026,
      "step": 42016
    },
    {
      "epoch": 0.00015260053699943154,
      "grad_norm": 9405.153906236728,
      "learning_rate": 4.87920994422253e-07,
      "loss": 1.799,
      "step": 42048
    },
    {
      "epoch": 0.00015271667135026824,
      "grad_norm": 9221.74229741864,
      "learning_rate": 4.877352480044544e-07,
      "loss": 1.8089,
      "step": 42080
    },
    {
      "epoch": 0.00015283280570110497,
      "grad_norm": 11429.857041975634,
      "learning_rate": 4.875497135604223e-07,
      "loss": 1.8216,
      "step": 42112
    },
    {
      "epoch": 0.00015294894005194167,
      "grad_norm": 9886.230120728527,
      "learning_rate": 4.873643906872892e-07,
      "loss": 1.8056,
      "step": 42144
    },
    {
      "epoch": 0.00015306507440277837,
      "grad_norm": 9884.176546379571,
      "learning_rate": 4.871792789832586e-07,
      "loss": 1.8079,
      "step": 42176
    },
    {
      "epoch": 0.00015318120875361507,
      "grad_norm": 9683.72459335766,
      "learning_rate": 4.869943780476017e-07,
      "loss": 1.819,
      "step": 42208
    },
    {
      "epoch": 0.0001532973431044518,
      "grad_norm": 9734.389040920852,
      "learning_rate": 4.868096874806532e-07,
      "loss": 1.8055,
      "step": 42240
    },
    {
      "epoch": 0.0001534134774552885,
      "grad_norm": 9620.132223623541,
      "learning_rate": 4.866252068838083e-07,
      "loss": 1.8087,
      "step": 42272
    },
    {
      "epoch": 0.0001535296118061252,
      "grad_norm": 9043.284137966693,
      "learning_rate": 4.864409358595189e-07,
      "loss": 1.7971,
      "step": 42304
    },
    {
      "epoch": 0.0001536457461569619,
      "grad_norm": 9398.973454585346,
      "learning_rate": 4.862568740112892e-07,
      "loss": 1.8082,
      "step": 42336
    },
    {
      "epoch": 0.0001537618805077986,
      "grad_norm": 8372.555762728607,
      "learning_rate": 4.860730209436735e-07,
      "loss": 1.7953,
      "step": 42368
    },
    {
      "epoch": 0.00015387801485863532,
      "grad_norm": 10577.102344215073,
      "learning_rate": 4.858893762622718e-07,
      "loss": 1.7904,
      "step": 42400
    },
    {
      "epoch": 0.00015399414920947202,
      "grad_norm": 7830.004980841329,
      "learning_rate": 4.85705939573726e-07,
      "loss": 1.7688,
      "step": 42432
    },
    {
      "epoch": 0.00015411028356030872,
      "grad_norm": 9423.719860012818,
      "learning_rate": 4.855227104857171e-07,
      "loss": 1.7857,
      "step": 42464
    },
    {
      "epoch": 0.00015422641791114542,
      "grad_norm": 8852.732120650664,
      "learning_rate": 4.853396886069614e-07,
      "loss": 1.7768,
      "step": 42496
    },
    {
      "epoch": 0.00015434255226198215,
      "grad_norm": 10279.898832187018,
      "learning_rate": 4.851568735472064e-07,
      "loss": 1.7976,
      "step": 42528
    },
    {
      "epoch": 0.00015445868661281885,
      "grad_norm": 9312.31013229263,
      "learning_rate": 4.849742649172284e-07,
      "loss": 1.7887,
      "step": 42560
    },
    {
      "epoch": 0.00015457482096365555,
      "grad_norm": 9781.39785511253,
      "learning_rate": 4.847918623288281e-07,
      "loss": 1.7785,
      "step": 42592
    },
    {
      "epoch": 0.00015469095531449225,
      "grad_norm": 10840.554413866479,
      "learning_rate": 4.846096653948275e-07,
      "loss": 1.7999,
      "step": 42624
    },
    {
      "epoch": 0.00015480708966532895,
      "grad_norm": 9362.187564880336,
      "learning_rate": 4.844276737290663e-07,
      "loss": 1.7683,
      "step": 42656
    },
    {
      "epoch": 0.00015492322401616568,
      "grad_norm": 10136.307414438455,
      "learning_rate": 4.84245886946399e-07,
      "loss": 1.7756,
      "step": 42688
    },
    {
      "epoch": 0.00015503935836700238,
      "grad_norm": 9818.386629176914,
      "learning_rate": 4.840643046626906e-07,
      "loss": 1.7857,
      "step": 42720
    },
    {
      "epoch": 0.00015515549271783908,
      "grad_norm": 9992.76488265385,
      "learning_rate": 4.83882926494814e-07,
      "loss": 1.7775,
      "step": 42752
    },
    {
      "epoch": 0.00015527162706867578,
      "grad_norm": 8237.222104568991,
      "learning_rate": 4.837017520606458e-07,
      "loss": 1.8046,
      "step": 42784
    },
    {
      "epoch": 0.0001553877614195125,
      "grad_norm": 10210.265912306104,
      "learning_rate": 4.835207809790639e-07,
      "loss": 1.8228,
      "step": 42816
    },
    {
      "epoch": 0.0001555038957703492,
      "grad_norm": 9467.83048010472,
      "learning_rate": 4.83340012869943e-07,
      "loss": 1.8232,
      "step": 42848
    },
    {
      "epoch": 0.0001556200301211859,
      "grad_norm": 10626.931165675254,
      "learning_rate": 4.831650869636815e-07,
      "loss": 1.8134,
      "step": 42880
    },
    {
      "epoch": 0.0001557361644720226,
      "grad_norm": 10414.579492231072,
      "learning_rate": 4.829847173495704e-07,
      "loss": 1.8244,
      "step": 42912
    },
    {
      "epoch": 0.0001558522988228593,
      "grad_norm": 10358.829084409106,
      "learning_rate": 4.828045495852676e-07,
      "loss": 1.8369,
      "step": 42944
    },
    {
      "epoch": 0.00015596843317369603,
      "grad_norm": 10029.89202334701,
      "learning_rate": 4.826245832945737e-07,
      "loss": 1.8379,
      "step": 42976
    },
    {
      "epoch": 0.00015608456752453273,
      "grad_norm": 9621.807938220341,
      "learning_rate": 4.8244481810227e-07,
      "loss": 1.7863,
      "step": 43008
    },
    {
      "epoch": 0.00015620070187536943,
      "grad_norm": 10512.363863565606,
      "learning_rate": 4.822652536341157e-07,
      "loss": 1.7954,
      "step": 43040
    },
    {
      "epoch": 0.00015631683622620613,
      "grad_norm": 8567.592660718645,
      "learning_rate": 4.820858895168439e-07,
      "loss": 1.7928,
      "step": 43072
    },
    {
      "epoch": 0.00015643297057704286,
      "grad_norm": 11134.259292831292,
      "learning_rate": 4.819067253781591e-07,
      "loss": 1.7754,
      "step": 43104
    },
    {
      "epoch": 0.00015654910492787956,
      "grad_norm": 9457.10833183167,
      "learning_rate": 4.817277608467333e-07,
      "loss": 1.8034,
      "step": 43136
    },
    {
      "epoch": 0.00015666523927871626,
      "grad_norm": 9536.631585628125,
      "learning_rate": 4.815489955522035e-07,
      "loss": 1.7621,
      "step": 43168
    },
    {
      "epoch": 0.00015678137362955296,
      "grad_norm": 9342.473762339394,
      "learning_rate": 4.813704291251676e-07,
      "loss": 1.7769,
      "step": 43200
    },
    {
      "epoch": 0.00015689750798038966,
      "grad_norm": 11379.030011384979,
      "learning_rate": 4.811920611971818e-07,
      "loss": 1.7866,
      "step": 43232
    },
    {
      "epoch": 0.00015701364233122638,
      "grad_norm": 9638.617950723019,
      "learning_rate": 4.810138914007576e-07,
      "loss": 1.7807,
      "step": 43264
    },
    {
      "epoch": 0.00015712977668206308,
      "grad_norm": 12030.539472525743,
      "learning_rate": 4.808359193693577e-07,
      "loss": 1.775,
      "step": 43296
    },
    {
      "epoch": 0.00015724591103289978,
      "grad_norm": 9291.419052007072,
      "learning_rate": 4.806581447373939e-07,
      "loss": 1.7896,
      "step": 43328
    },
    {
      "epoch": 0.00015736204538373648,
      "grad_norm": 10357.819172007205,
      "learning_rate": 4.804805671402233e-07,
      "loss": 1.7824,
      "step": 43360
    },
    {
      "epoch": 0.0001574781797345732,
      "grad_norm": 10048.464957395234,
      "learning_rate": 4.80303186214145e-07,
      "loss": 1.7918,
      "step": 43392
    },
    {
      "epoch": 0.0001575943140854099,
      "grad_norm": 9580.501761390162,
      "learning_rate": 4.801260015963979e-07,
      "loss": 1.7823,
      "step": 43424
    },
    {
      "epoch": 0.0001577104484362466,
      "grad_norm": 10074.033750191627,
      "learning_rate": 4.799490129251565e-07,
      "loss": 1.7894,
      "step": 43456
    },
    {
      "epoch": 0.0001578265827870833,
      "grad_norm": 9853.123362670336,
      "learning_rate": 4.797722198395283e-07,
      "loss": 1.7796,
      "step": 43488
    },
    {
      "epoch": 0.00015794271713792,
      "grad_norm": 8481.543019993473,
      "learning_rate": 4.79595621979551e-07,
      "loss": 1.8011,
      "step": 43520
    },
    {
      "epoch": 0.00015805885148875674,
      "grad_norm": 10121.355739227824,
      "learning_rate": 4.794192189861888e-07,
      "loss": 1.8298,
      "step": 43552
    },
    {
      "epoch": 0.00015817498583959344,
      "grad_norm": 10784.212349541342,
      "learning_rate": 4.792430105013297e-07,
      "loss": 1.8042,
      "step": 43584
    },
    {
      "epoch": 0.00015829112019043014,
      "grad_norm": 9246.48246632199,
      "learning_rate": 4.790669961677824e-07,
      "loss": 1.8032,
      "step": 43616
    },
    {
      "epoch": 0.00015840725454126684,
      "grad_norm": 9326.921035368532,
      "learning_rate": 4.788911756292733e-07,
      "loss": 1.8075,
      "step": 43648
    },
    {
      "epoch": 0.00015852338889210356,
      "grad_norm": 8212.505707760574,
      "learning_rate": 4.787155485304435e-07,
      "loss": 1.8005,
      "step": 43680
    },
    {
      "epoch": 0.00015863952324294026,
      "grad_norm": 10396.62608734199,
      "learning_rate": 4.785401145168453e-07,
      "loss": 1.8254,
      "step": 43712
    },
    {
      "epoch": 0.00015875565759377696,
      "grad_norm": 9203.541709581155,
      "learning_rate": 4.783648732349399e-07,
      "loss": 1.8275,
      "step": 43744
    },
    {
      "epoch": 0.00015887179194461366,
      "grad_norm": 9248.587351590511,
      "learning_rate": 4.781898243320941e-07,
      "loss": 1.8033,
      "step": 43776
    },
    {
      "epoch": 0.00015898792629545036,
      "grad_norm": 10988.569879652221,
      "learning_rate": 4.780149674565772e-07,
      "loss": 1.8117,
      "step": 43808
    },
    {
      "epoch": 0.0001591040606462871,
      "grad_norm": 9050.490152472406,
      "learning_rate": 4.778403022575583e-07,
      "loss": 1.8097,
      "step": 43840
    },
    {
      "epoch": 0.0001592201949971238,
      "grad_norm": 9220.06366572379,
      "learning_rate": 4.776712778010304e-07,
      "loss": 1.7659,
      "step": 43872
    },
    {
      "epoch": 0.0001593363293479605,
      "grad_norm": 10008.516373569062,
      "learning_rate": 4.77496988943325e-07,
      "loss": 1.7711,
      "step": 43904
    },
    {
      "epoch": 0.0001594524636987972,
      "grad_norm": 10308.83970192572,
      "learning_rate": 4.773228907258593e-07,
      "loss": 1.7651,
      "step": 43936
    },
    {
      "epoch": 0.00015956859804963392,
      "grad_norm": 9001.221361570884,
      "learning_rate": 4.771489828013436e-07,
      "loss": 1.7734,
      "step": 43968
    },
    {
      "epoch": 0.00015968473240047062,
      "grad_norm": 9263.143526902733,
      "learning_rate": 4.769752648233735e-07,
      "loss": 1.7822,
      "step": 44000
    },
    {
      "epoch": 0.00015980086675130732,
      "grad_norm": 10366.877543407176,
      "learning_rate": 4.768017364464267e-07,
      "loss": 1.7827,
      "step": 44032
    },
    {
      "epoch": 0.00015991700110214402,
      "grad_norm": 10993.934327619027,
      "learning_rate": 4.766283973258601e-07,
      "loss": 1.8064,
      "step": 44064
    },
    {
      "epoch": 0.00016003313545298072,
      "grad_norm": 9256.216181572252,
      "learning_rate": 4.7645524711790723e-07,
      "loss": 1.8112,
      "step": 44096
    },
    {
      "epoch": 0.00016014926980381745,
      "grad_norm": 11287.300297236712,
      "learning_rate": 4.7628228547967503e-07,
      "loss": 1.8161,
      "step": 44128
    },
    {
      "epoch": 0.00016026540415465414,
      "grad_norm": 8915.46656098266,
      "learning_rate": 4.761095120691411e-07,
      "loss": 1.7962,
      "step": 44160
    },
    {
      "epoch": 0.00016038153850549084,
      "grad_norm": 8626.647784626426,
      "learning_rate": 4.759369265451512e-07,
      "loss": 1.7628,
      "step": 44192
    },
    {
      "epoch": 0.00016049767285632754,
      "grad_norm": 9496.914867471436,
      "learning_rate": 4.7576452856741555e-07,
      "loss": 1.7681,
      "step": 44224
    },
    {
      "epoch": 0.00016061380720716427,
      "grad_norm": 9855.935470567976,
      "learning_rate": 4.755923177965072e-07,
      "loss": 1.7658,
      "step": 44256
    },
    {
      "epoch": 0.00016072994155800097,
      "grad_norm": 9674.581334610815,
      "learning_rate": 4.754202938938583e-07,
      "loss": 1.7764,
      "step": 44288
    },
    {
      "epoch": 0.00016084607590883767,
      "grad_norm": 9058.808862096606,
      "learning_rate": 4.752484565217575e-07,
      "loss": 1.8111,
      "step": 44320
    },
    {
      "epoch": 0.00016096221025967437,
      "grad_norm": 9818.080973387823,
      "learning_rate": 4.750768053433476e-07,
      "loss": 1.8201,
      "step": 44352
    },
    {
      "epoch": 0.00016107834461051107,
      "grad_norm": 8919.59147046545,
      "learning_rate": 4.7490534002262207e-07,
      "loss": 1.8087,
      "step": 44384
    },
    {
      "epoch": 0.0001611944789613478,
      "grad_norm": 9291.761835088111,
      "learning_rate": 4.747340602244231e-07,
      "loss": 1.8032,
      "step": 44416
    },
    {
      "epoch": 0.0001613106133121845,
      "grad_norm": 9926.61583824014,
      "learning_rate": 4.7456296561443806e-07,
      "loss": 1.8063,
      "step": 44448
    },
    {
      "epoch": 0.0001614267476630212,
      "grad_norm": 9939.215864443231,
      "learning_rate": 4.743920558591973e-07,
      "loss": 1.811,
      "step": 44480
    },
    {
      "epoch": 0.0001615428820138579,
      "grad_norm": 9425.86706886958,
      "learning_rate": 4.742213306260712e-07,
      "loss": 1.8052,
      "step": 44512
    },
    {
      "epoch": 0.0001616590163646946,
      "grad_norm": 11371.434562094617,
      "learning_rate": 4.7405078958326735e-07,
      "loss": 1.8043,
      "step": 44544
    },
    {
      "epoch": 0.00016177515071553133,
      "grad_norm": 9558.782244616728,
      "learning_rate": 4.738804323998283e-07,
      "loss": 1.8031,
      "step": 44576
    },
    {
      "epoch": 0.00016189128506636803,
      "grad_norm": 9193.902762157102,
      "learning_rate": 4.737102587456283e-07,
      "loss": 1.7961,
      "step": 44608
    },
    {
      "epoch": 0.00016200741941720472,
      "grad_norm": 9496.009161747897,
      "learning_rate": 4.7354026829137077e-07,
      "loss": 1.8,
      "step": 44640
    },
    {
      "epoch": 0.00016212355376804142,
      "grad_norm": 10136.621823862228,
      "learning_rate": 4.733704607085861e-07,
      "loss": 1.8174,
      "step": 44672
    },
    {
      "epoch": 0.00016223968811887815,
      "grad_norm": 9493.865598374563,
      "learning_rate": 4.732008356696281e-07,
      "loss": 1.799,
      "step": 44704
    },
    {
      "epoch": 0.00016235582246971485,
      "grad_norm": 9108.216620173238,
      "learning_rate": 4.730313928476723e-07,
      "loss": 1.7938,
      "step": 44736
    },
    {
      "epoch": 0.00016247195682055155,
      "grad_norm": 9571.692431331045,
      "learning_rate": 4.728621319167125e-07,
      "loss": 1.7701,
      "step": 44768
    },
    {
      "epoch": 0.00016258809117138825,
      "grad_norm": 9175.987576277554,
      "learning_rate": 4.726930525515586e-07,
      "loss": 1.7698,
      "step": 44800
    },
    {
      "epoch": 0.00016270422552222495,
      "grad_norm": 9116.620426451898,
      "learning_rate": 4.7252415442783404e-07,
      "loss": 1.7767,
      "step": 44832
    },
    {
      "epoch": 0.00016282035987306168,
      "grad_norm": 10518.109621029818,
      "learning_rate": 4.7235543722197265e-07,
      "loss": 1.7881,
      "step": 44864
    },
    {
      "epoch": 0.00016293649422389838,
      "grad_norm": 9790.694970225555,
      "learning_rate": 4.7219216464988567e-07,
      "loss": 1.7964,
      "step": 44896
    },
    {
      "epoch": 0.00016305262857473508,
      "grad_norm": 10531.302483548747,
      "learning_rate": 4.72023802683612e-07,
      "loss": 1.7833,
      "step": 44928
    },
    {
      "epoch": 0.00016316876292557178,
      "grad_norm": 10808.449472519173,
      "learning_rate": 4.71855620679366e-07,
      "loss": 1.7931,
      "step": 44960
    },
    {
      "epoch": 0.0001632848972764085,
      "grad_norm": 11591.646129864388,
      "learning_rate": 4.7168761831677354e-07,
      "loss": 1.7836,
      "step": 44992
    },
    {
      "epoch": 0.0001634010316272452,
      "grad_norm": 8383.13246942931,
      "learning_rate": 4.7151979527625876e-07,
      "loss": 1.764,
      "step": 45024
    },
    {
      "epoch": 0.0001635171659780819,
      "grad_norm": 10343.430572107109,
      "learning_rate": 4.7135215123904087e-07,
      "loss": 1.7843,
      "step": 45056
    },
    {
      "epoch": 0.0001636333003289186,
      "grad_norm": 10182.552332298616,
      "learning_rate": 4.711846858871321e-07,
      "loss": 1.8012,
      "step": 45088
    },
    {
      "epoch": 0.0001637494346797553,
      "grad_norm": 10221.727055639863,
      "learning_rate": 4.7101739890333483e-07,
      "loss": 1.7946,
      "step": 45120
    },
    {
      "epoch": 0.00016386556903059203,
      "grad_norm": 9929.900402320256,
      "learning_rate": 4.708502899712393e-07,
      "loss": 1.7872,
      "step": 45152
    },
    {
      "epoch": 0.00016398170338142873,
      "grad_norm": 10737.433399094962,
      "learning_rate": 4.7068335877522084e-07,
      "loss": 1.7934,
      "step": 45184
    },
    {
      "epoch": 0.00016409783773226543,
      "grad_norm": 9072.096780788883,
      "learning_rate": 4.7051660500043773e-07,
      "loss": 1.8097,
      "step": 45216
    },
    {
      "epoch": 0.00016421397208310213,
      "grad_norm": 10105.483461962618,
      "learning_rate": 4.7035002833282833e-07,
      "loss": 1.83,
      "step": 45248
    },
    {
      "epoch": 0.00016433010643393886,
      "grad_norm": 9341.065035636995,
      "learning_rate": 4.701836284591087e-07,
      "loss": 1.8398,
      "step": 45280
    },
    {
      "epoch": 0.00016444624078477556,
      "grad_norm": 9372.636448726687,
      "learning_rate": 4.700174050667704e-07,
      "loss": 1.818,
      "step": 45312
    },
    {
      "epoch": 0.00016456237513561226,
      "grad_norm": 8867.135163061404,
      "learning_rate": 4.698513578440777e-07,
      "loss": 1.8059,
      "step": 45344
    },
    {
      "epoch": 0.00016467850948644896,
      "grad_norm": 8757.201379436241,
      "learning_rate": 4.6968548648006514e-07,
      "loss": 1.7771,
      "step": 45376
    },
    {
      "epoch": 0.00016479464383728566,
      "grad_norm": 9168.40705902612,
      "learning_rate": 4.695197906645353e-07,
      "loss": 1.782,
      "step": 45408
    },
    {
      "epoch": 0.00016491077818812239,
      "grad_norm": 9734.27973709406,
      "learning_rate": 4.6935427008805626e-07,
      "loss": 1.7672,
      "step": 45440
    },
    {
      "epoch": 0.00016502691253895909,
      "grad_norm": 8986.435444602048,
      "learning_rate": 4.69188924441959e-07,
      "loss": 1.7927,
      "step": 45472
    },
    {
      "epoch": 0.00016514304688979579,
      "grad_norm": 9558.387939396476,
      "learning_rate": 4.6902375341833534e-07,
      "loss": 1.7951,
      "step": 45504
    },
    {
      "epoch": 0.00016525918124063249,
      "grad_norm": 10037.821676041072,
      "learning_rate": 4.688587567100352e-07,
      "loss": 1.7915,
      "step": 45536
    },
    {
      "epoch": 0.0001653753155914692,
      "grad_norm": 9751.48204120789,
      "learning_rate": 4.686939340106642e-07,
      "loss": 1.7946,
      "step": 45568
    },
    {
      "epoch": 0.0001654914499423059,
      "grad_norm": 9800.148264184578,
      "learning_rate": 4.685292850145818e-07,
      "loss": 1.7886,
      "step": 45600
    },
    {
      "epoch": 0.0001656075842931426,
      "grad_norm": 12365.586439793302,
      "learning_rate": 4.6836480941689804e-07,
      "loss": 1.7823,
      "step": 45632
    },
    {
      "epoch": 0.0001657237186439793,
      "grad_norm": 10127.843106999635,
      "learning_rate": 4.6820050691347205e-07,
      "loss": 1.7848,
      "step": 45664
    },
    {
      "epoch": 0.000165839852994816,
      "grad_norm": 9109.628861814295,
      "learning_rate": 4.6803637720090906e-07,
      "loss": 1.7701,
      "step": 45696
    },
    {
      "epoch": 0.00016595598734565274,
      "grad_norm": 10968.76674927496,
      "learning_rate": 4.6787241997655827e-07,
      "loss": 1.7682,
      "step": 45728
    },
    {
      "epoch": 0.00016607212169648944,
      "grad_norm": 8978.927998374862,
      "learning_rate": 4.677086349385106e-07,
      "loss": 1.7657,
      "step": 45760
    },
    {
      "epoch": 0.00016618825604732614,
      "grad_norm": 10179.626122800386,
      "learning_rate": 4.675450217855963e-07,
      "loss": 1.7731,
      "step": 45792
    },
    {
      "epoch": 0.00016630439039816284,
      "grad_norm": 10311.806728212083,
      "learning_rate": 4.6738158021738244e-07,
      "loss": 1.8113,
      "step": 45824
    },
    {
      "epoch": 0.00016642052474899957,
      "grad_norm": 10116.505622002096,
      "learning_rate": 4.672183099341708e-07,
      "loss": 1.827,
      "step": 45856
    },
    {
      "epoch": 0.00016653665909983627,
      "grad_norm": 8727.731434914802,
      "learning_rate": 4.670603049048292e-07,
      "loss": 1.8286,
      "step": 45888
    },
    {
      "epoch": 0.00016665279345067297,
      "grad_norm": 9460.304646257433,
      "learning_rate": 4.668973709659678e-07,
      "loss": 1.8109,
      "step": 45920
    },
    {
      "epoch": 0.00016676892780150967,
      "grad_norm": 8586.514077319154,
      "learning_rate": 4.667346074266851e-07,
      "loss": 1.7944,
      "step": 45952
    },
    {
      "epoch": 0.00016688506215234637,
      "grad_norm": 9343.351325942956,
      "learning_rate": 4.665720139901759e-07,
      "loss": 1.7925,
      "step": 45984
    },
    {
      "epoch": 0.0001670011965031831,
      "grad_norm": 8622.041637570535,
      "learning_rate": 4.664095903603581e-07,
      "loss": 1.7948,
      "step": 46016
    },
    {
      "epoch": 0.0001671173308540198,
      "grad_norm": 8339.587399865775,
      "learning_rate": 4.6624733624187085e-07,
      "loss": 1.7931,
      "step": 46048
    },
    {
      "epoch": 0.0001672334652048565,
      "grad_norm": 9917.453402965904,
      "learning_rate": 4.6608525134007197e-07,
      "loss": 1.8126,
      "step": 46080
    },
    {
      "epoch": 0.0001673495995556932,
      "grad_norm": 11015.313522546692,
      "learning_rate": 4.6592333536103557e-07,
      "loss": 1.8015,
      "step": 46112
    },
    {
      "epoch": 0.00016746573390652992,
      "grad_norm": 8784.060109083954,
      "learning_rate": 4.657615880115504e-07,
      "loss": 1.7768,
      "step": 46144
    },
    {
      "epoch": 0.00016758186825736662,
      "grad_norm": 9605.089067780684,
      "learning_rate": 4.656000089991171e-07,
      "loss": 1.7915,
      "step": 46176
    },
    {
      "epoch": 0.00016769800260820332,
      "grad_norm": 10538.184188938812,
      "learning_rate": 4.6543859803194586e-07,
      "loss": 1.7916,
      "step": 46208
    },
    {
      "epoch": 0.00016781413695904002,
      "grad_norm": 10337.058188865922,
      "learning_rate": 4.652773548189549e-07,
      "loss": 1.7807,
      "step": 46240
    },
    {
      "epoch": 0.00016793027130987672,
      "grad_norm": 8596.774395085637,
      "learning_rate": 4.651162790697675e-07,
      "loss": 1.7822,
      "step": 46272
    },
    {
      "epoch": 0.00016804640566071345,
      "grad_norm": 9465.454135961993,
      "learning_rate": 4.649553704947102e-07,
      "loss": 1.7829,
      "step": 46304
    },
    {
      "epoch": 0.00016816254001155015,
      "grad_norm": 8359.872965542001,
      "learning_rate": 4.6479462880481076e-07,
      "loss": 1.7794,
      "step": 46336
    },
    {
      "epoch": 0.00016827867436238685,
      "grad_norm": 9875.928614565822,
      "learning_rate": 4.6463405371179554e-07,
      "loss": 1.7952,
      "step": 46368
    },
    {
      "epoch": 0.00016839480871322355,
      "grad_norm": 9956.543376091926,
      "learning_rate": 4.6447364492808766e-07,
      "loss": 1.8196,
      "step": 46400
    },
    {
      "epoch": 0.00016851094306406027,
      "grad_norm": 11409.890797023432,
      "learning_rate": 4.643134021668046e-07,
      "loss": 1.7955,
      "step": 46432
    },
    {
      "epoch": 0.00016862707741489697,
      "grad_norm": 9000.847960053541,
      "learning_rate": 4.641533251417565e-07,
      "loss": 1.793,
      "step": 46464
    },
    {
      "epoch": 0.00016874321176573367,
      "grad_norm": 9086.732636101933,
      "learning_rate": 4.639934135674433e-07,
      "loss": 1.8063,
      "step": 46496
    },
    {
      "epoch": 0.00016885934611657037,
      "grad_norm": 9412.900190695746,
      "learning_rate": 4.638336671590532e-07,
      "loss": 1.7743,
      "step": 46528
    },
    {
      "epoch": 0.00016897548046740707,
      "grad_norm": 10430.464802682573,
      "learning_rate": 4.6367408563246046e-07,
      "loss": 1.7743,
      "step": 46560
    },
    {
      "epoch": 0.0001690916148182438,
      "grad_norm": 11358.538814477855,
      "learning_rate": 4.6351466870422276e-07,
      "loss": 1.7872,
      "step": 46592
    },
    {
      "epoch": 0.0001692077491690805,
      "grad_norm": 10074.528078277413,
      "learning_rate": 4.633554160915799e-07,
      "loss": 1.7957,
      "step": 46624
    },
    {
      "epoch": 0.0001693238835199172,
      "grad_norm": 9399.266992696825,
      "learning_rate": 4.6319632751245095e-07,
      "loss": 1.8028,
      "step": 46656
    },
    {
      "epoch": 0.0001694400178707539,
      "grad_norm": 12444.566846620255,
      "learning_rate": 4.6303740268543256e-07,
      "loss": 1.8143,
      "step": 46688
    },
    {
      "epoch": 0.00016955615222159063,
      "grad_norm": 8722.553525201207,
      "learning_rate": 4.628786413297968e-07,
      "loss": 1.7941,
      "step": 46720
    },
    {
      "epoch": 0.00016967228657242733,
      "grad_norm": 10102.781993094772,
      "learning_rate": 4.6272004316548906e-07,
      "loss": 1.7932,
      "step": 46752
    },
    {
      "epoch": 0.00016978842092326403,
      "grad_norm": 8835.09094463662,
      "learning_rate": 4.625616079131259e-07,
      "loss": 1.8102,
      "step": 46784
    },
    {
      "epoch": 0.00016990455527410073,
      "grad_norm": 10521.637895308886,
      "learning_rate": 4.624033352939931e-07,
      "loss": 1.8082,
      "step": 46816
    },
    {
      "epoch": 0.00017002068962493743,
      "grad_norm": 9548.382271358849,
      "learning_rate": 4.6224522503004356e-07,
      "loss": 1.7879,
      "step": 46848
    },
    {
      "epoch": 0.00017013682397577415,
      "grad_norm": 10153.918061516943,
      "learning_rate": 4.6209221027413665e-07,
      "loss": 1.7801,
      "step": 46880
    },
    {
      "epoch": 0.00017025295832661085,
      "grad_norm": 8211.751457515016,
      "learning_rate": 4.6193441883696855e-07,
      "loss": 1.7703,
      "step": 46912
    },
    {
      "epoch": 0.00017036909267744755,
      "grad_norm": 8838.703637977687,
      "learning_rate": 4.617767889334413e-07,
      "loss": 1.7781,
      "step": 46944
    },
    {
      "epoch": 0.00017048522702828425,
      "grad_norm": 10038.956220643659,
      "learning_rate": 4.616193202881352e-07,
      "loss": 1.7832,
      "step": 46976
    },
    {
      "epoch": 0.00017060136137912098,
      "grad_norm": 9762.317757581957,
      "learning_rate": 4.6146201262628753e-07,
      "loss": 1.796,
      "step": 47008
    },
    {
      "epoch": 0.00017071749572995768,
      "grad_norm": 9866.026555812628,
      "learning_rate": 4.613048656737905e-07,
      "loss": 1.7966,
      "step": 47040
    },
    {
      "epoch": 0.00017083363008079438,
      "grad_norm": 9048.9577300372,
      "learning_rate": 4.611478791571895e-07,
      "loss": 1.8012,
      "step": 47072
    },
    {
      "epoch": 0.00017094976443163108,
      "grad_norm": 10291.782547255845,
      "learning_rate": 4.609910528036807e-07,
      "loss": 1.8113,
      "step": 47104
    },
    {
      "epoch": 0.00017106589878246778,
      "grad_norm": 10156.43234605538,
      "learning_rate": 4.608343863411092e-07,
      "loss": 1.8008,
      "step": 47136
    },
    {
      "epoch": 0.0001711820331333045,
      "grad_norm": 9523.86035176913,
      "learning_rate": 4.606778794979673e-07,
      "loss": 1.7936,
      "step": 47168
    },
    {
      "epoch": 0.0001712981674841412,
      "grad_norm": 10141.776570207016,
      "learning_rate": 4.605215320033921e-07,
      "loss": 1.7814,
      "step": 47200
    },
    {
      "epoch": 0.0001714143018349779,
      "grad_norm": 9937.079047688008,
      "learning_rate": 4.6036534358716396e-07,
      "loss": 1.7866,
      "step": 47232
    },
    {
      "epoch": 0.0001715304361858146,
      "grad_norm": 10161.899330341745,
      "learning_rate": 4.6020931397970415e-07,
      "loss": 1.8014,
      "step": 47264
    },
    {
      "epoch": 0.0001716465705366513,
      "grad_norm": 9705.337603607615,
      "learning_rate": 4.600534429120733e-07,
      "loss": 1.7601,
      "step": 47296
    },
    {
      "epoch": 0.00017176270488748803,
      "grad_norm": 7933.874841463029,
      "learning_rate": 4.5989773011596903e-07,
      "loss": 1.7799,
      "step": 47328
    },
    {
      "epoch": 0.00017187883923832473,
      "grad_norm": 9579.848224267438,
      "learning_rate": 4.5974217532372415e-07,
      "loss": 1.7836,
      "step": 47360
    },
    {
      "epoch": 0.00017199497358916143,
      "grad_norm": 9333.081698988819,
      "learning_rate": 4.595867782683051e-07,
      "loss": 1.804,
      "step": 47392
    },
    {
      "epoch": 0.00017211110793999813,
      "grad_norm": 9142.484782596031,
      "learning_rate": 4.5943153868330937e-07,
      "loss": 1.8184,
      "step": 47424
    },
    {
      "epoch": 0.00017222724229083486,
      "grad_norm": 10718.83902295393,
      "learning_rate": 4.5927645630296423e-07,
      "loss": 1.7882,
      "step": 47456
    },
    {
      "epoch": 0.00017234337664167156,
      "grad_norm": 8814.573954536883,
      "learning_rate": 4.591215308621242e-07,
      "loss": 1.7948,
      "step": 47488
    },
    {
      "epoch": 0.00017245951099250826,
      "grad_norm": 10386.650952063423,
      "learning_rate": 4.589667620962697e-07,
      "loss": 1.8005,
      "step": 47520
    },
    {
      "epoch": 0.00017257564534334496,
      "grad_norm": 10462.978925717092,
      "learning_rate": 4.588121497415048e-07,
      "loss": 1.8217,
      "step": 47552
    },
    {
      "epoch": 0.00017269177969418166,
      "grad_norm": 9928.560016437428,
      "learning_rate": 4.5865769353455556e-07,
      "loss": 1.7955,
      "step": 47584
    },
    {
      "epoch": 0.00017280791404501839,
      "grad_norm": 10077.650321379482,
      "learning_rate": 4.585033932127678e-07,
      "loss": 1.8076,
      "step": 47616
    },
    {
      "epoch": 0.00017292404839585509,
      "grad_norm": 9646.603962016892,
      "learning_rate": 4.5834924851410564e-07,
      "loss": 1.7942,
      "step": 47648
    },
    {
      "epoch": 0.00017304018274669179,
      "grad_norm": 9786.188635010058,
      "learning_rate": 4.581952591771495e-07,
      "loss": 1.7859,
      "step": 47680
    },
    {
      "epoch": 0.00017315631709752849,
      "grad_norm": 10213.597603195458,
      "learning_rate": 4.58041424941094e-07,
      "loss": 1.7759,
      "step": 47712
    },
    {
      "epoch": 0.0001732724514483652,
      "grad_norm": 10090.43398472038,
      "learning_rate": 4.578877455457463e-07,
      "loss": 1.7809,
      "step": 47744
    },
    {
      "epoch": 0.0001733885857992019,
      "grad_norm": 12012.326169397833,
      "learning_rate": 4.5773422073152446e-07,
      "loss": 1.7786,
      "step": 47776
    },
    {
      "epoch": 0.0001735047201500386,
      "grad_norm": 8474.833685683749,
      "learning_rate": 4.575808502394551e-07,
      "loss": 1.7699,
      "step": 47808
    },
    {
      "epoch": 0.0001736208545008753,
      "grad_norm": 12210.503347528309,
      "learning_rate": 4.574276338111721e-07,
      "loss": 1.7967,
      "step": 47840
    },
    {
      "epoch": 0.000173736988851712,
      "grad_norm": 8305.931856209754,
      "learning_rate": 4.5727457118891426e-07,
      "loss": 1.7905,
      "step": 47872
    },
    {
      "epoch": 0.00017385312320254874,
      "grad_norm": 8964.951645156822,
      "learning_rate": 4.571264382023913e-07,
      "loss": 1.7923,
      "step": 47904
    },
    {
      "epoch": 0.00017396925755338544,
      "grad_norm": 9588.549629636382,
      "learning_rate": 4.569736776348049e-07,
      "loss": 1.7986,
      "step": 47936
    },
    {
      "epoch": 0.00017408539190422214,
      "grad_norm": 9997.212911606914,
      "learning_rate": 4.568210701115629e-07,
      "loss": 1.7771,
      "step": 47968
    },
    {
      "epoch": 0.00017420152625505884,
      "grad_norm": 9485.62902500409,
      "learning_rate": 4.5666861537728825e-07,
      "loss": 1.7803,
      "step": 48000
    },
    {
      "epoch": 0.00017431766060589557,
      "grad_norm": 10068.763975781734,
      "learning_rate": 4.5651631317719975e-07,
      "loss": 1.794,
      "step": 48032
    },
    {
      "epoch": 0.00017443379495673227,
      "grad_norm": 9943.681410825671,
      "learning_rate": 4.563641632571109e-07,
      "loss": 1.7752,
      "step": 48064
    },
    {
      "epoch": 0.00017454992930756897,
      "grad_norm": 8601.262814261636,
      "learning_rate": 4.5621216536342764e-07,
      "loss": 1.7995,
      "step": 48096
    },
    {
      "epoch": 0.00017466606365840567,
      "grad_norm": 12189.22097592787,
      "learning_rate": 4.56060319243147e-07,
      "loss": 1.8185,
      "step": 48128
    },
    {
      "epoch": 0.00017478219800924237,
      "grad_norm": 9138.77902129163,
      "learning_rate": 4.5590862464385483e-07,
      "loss": 1.8041,
      "step": 48160
    },
    {
      "epoch": 0.0001748983323600791,
      "grad_norm": 9072.855228647706,
      "learning_rate": 4.557570813137245e-07,
      "loss": 1.8037,
      "step": 48192
    },
    {
      "epoch": 0.0001750144667109158,
      "grad_norm": 9333.765156677127,
      "learning_rate": 4.556056890015147e-07,
      "loss": 1.8078,
      "step": 48224
    },
    {
      "epoch": 0.0001751306010617525,
      "grad_norm": 9175.8553824698,
      "learning_rate": 4.5545444745656807e-07,
      "loss": 1.8015,
      "step": 48256
    },
    {
      "epoch": 0.0001752467354125892,
      "grad_norm": 9257.115965569406,
      "learning_rate": 4.5530335642880943e-07,
      "loss": 1.8107,
      "step": 48288
    },
    {
      "epoch": 0.00017536286976342592,
      "grad_norm": 10064.637002892852,
      "learning_rate": 4.5515241566874365e-07,
      "loss": 1.7991,
      "step": 48320
    },
    {
      "epoch": 0.00017547900411426262,
      "grad_norm": 9818.608760919236,
      "learning_rate": 4.550016249274546e-07,
      "loss": 1.8135,
      "step": 48352
    },
    {
      "epoch": 0.00017559513846509932,
      "grad_norm": 10826.60260654283,
      "learning_rate": 4.5485098395660253e-07,
      "loss": 1.7915,
      "step": 48384
    },
    {
      "epoch": 0.00017571127281593602,
      "grad_norm": 9417.721380461411,
      "learning_rate": 4.5470049250842316e-07,
      "loss": 1.7862,
      "step": 48416
    },
    {
      "epoch": 0.00017582740716677272,
      "grad_norm": 8970.419053756632,
      "learning_rate": 4.5455015033572566e-07,
      "loss": 1.7784,
      "step": 48448
    },
    {
      "epoch": 0.00017594354151760945,
      "grad_norm": 9542.39089536789,
      "learning_rate": 4.543999571918909e-07,
      "loss": 1.7586,
      "step": 48480
    },
    {
      "epoch": 0.00017605967586844615,
      "grad_norm": 8817.566841255019,
      "learning_rate": 4.542499128308697e-07,
      "loss": 1.7639,
      "step": 48512
    },
    {
      "epoch": 0.00017617581021928285,
      "grad_norm": 9348.475704626931,
      "learning_rate": 4.5410001700718123e-07,
      "loss": 1.7695,
      "step": 48544
    },
    {
      "epoch": 0.00017629194457011955,
      "grad_norm": 9851.197592171216,
      "learning_rate": 4.5395026947591153e-07,
      "loss": 1.7548,
      "step": 48576
    },
    {
      "epoch": 0.00017640807892095627,
      "grad_norm": 9021.802924027992,
      "learning_rate": 4.5380066999271144e-07,
      "loss": 1.7794,
      "step": 48608
    },
    {
      "epoch": 0.00017652421327179297,
      "grad_norm": 10473.277996883306,
      "learning_rate": 4.53651218313795e-07,
      "loss": 1.801,
      "step": 48640
    },
    {
      "epoch": 0.00017664034762262967,
      "grad_norm": 10262.956396672453,
      "learning_rate": 4.535019141959383e-07,
      "loss": 1.7985,
      "step": 48672
    },
    {
      "epoch": 0.00017675648197346637,
      "grad_norm": 10104.429721661683,
      "learning_rate": 4.533527573964769e-07,
      "loss": 1.7958,
      "step": 48704
    },
    {
      "epoch": 0.00017687261632430307,
      "grad_norm": 10786.141849614254,
      "learning_rate": 4.53203747673305e-07,
      "loss": 1.78,
      "step": 48736
    },
    {
      "epoch": 0.0001769887506751398,
      "grad_norm": 9684.866442032126,
      "learning_rate": 4.5305488478487336e-07,
      "loss": 1.7895,
      "step": 48768
    },
    {
      "epoch": 0.0001771048850259765,
      "grad_norm": 9617.467650062566,
      "learning_rate": 4.529061684901879e-07,
      "loss": 1.793,
      "step": 48800
    },
    {
      "epoch": 0.0001772210193768132,
      "grad_norm": 9223.923243392694,
      "learning_rate": 4.527575985488077e-07,
      "loss": 1.7941,
      "step": 48832
    },
    {
      "epoch": 0.0001773371537276499,
      "grad_norm": 9396.991007764134,
      "learning_rate": 4.526091747208436e-07,
      "loss": 1.805,
      "step": 48864
    },
    {
      "epoch": 0.00017745328807848663,
      "grad_norm": 10127.391174433818,
      "learning_rate": 4.5246552824733447e-07,
      "loss": 1.8054,
      "step": 48896
    },
    {
      "epoch": 0.00017756942242932333,
      "grad_norm": 9504.557643572898,
      "learning_rate": 4.52317391381241e-07,
      "loss": 1.8147,
      "step": 48928
    },
    {
      "epoch": 0.00017768555678016003,
      "grad_norm": 10626.605666909825,
      "learning_rate": 4.5216939991962764e-07,
      "loss": 1.8325,
      "step": 48960
    },
    {
      "epoch": 0.00017780169113099673,
      "grad_norm": 9917.045124430968,
      "learning_rate": 4.520215536247793e-07,
      "loss": 1.804,
      "step": 48992
    },
    {
      "epoch": 0.00017791782548183343,
      "grad_norm": 9705.963012499069,
      "learning_rate": 4.5187385225952426e-07,
      "loss": 1.7983,
      "step": 49024
    },
    {
      "epoch": 0.00017803395983267015,
      "grad_norm": 9135.358887312528,
      "learning_rate": 4.517262955872331e-07,
      "loss": 1.7928,
      "step": 49056
    },
    {
      "epoch": 0.00017815009418350685,
      "grad_norm": 8534.614343952513,
      "learning_rate": 4.5157888337181707e-07,
      "loss": 1.7892,
      "step": 49088
    },
    {
      "epoch": 0.00017826622853434355,
      "grad_norm": 9022.548863818914,
      "learning_rate": 4.5143161537772624e-07,
      "loss": 1.7824,
      "step": 49120
    },
    {
      "epoch": 0.00017838236288518025,
      "grad_norm": 8622.1839460777,
      "learning_rate": 4.512844913699479e-07,
      "loss": 1.7917,
      "step": 49152
    },
    {
      "epoch": 0.00017849849723601698,
      "grad_norm": 9691.026571008873,
      "learning_rate": 4.5113751111400537e-07,
      "loss": 1.7579,
      "step": 49184
    },
    {
      "epoch": 0.00017861463158685368,
      "grad_norm": 11916.507709895546,
      "learning_rate": 4.509906743759561e-07,
      "loss": 1.7746,
      "step": 49216
    },
    {
      "epoch": 0.00017873076593769038,
      "grad_norm": 10226.124583633822,
      "learning_rate": 4.5084398092239026e-07,
      "loss": 1.7901,
      "step": 49248
    },
    {
      "epoch": 0.00017884690028852708,
      "grad_norm": 8992.070284422825,
      "learning_rate": 4.5069743052042893e-07,
      "loss": 1.7968,
      "step": 49280
    },
    {
      "epoch": 0.00017896303463936378,
      "grad_norm": 10047.11043036753,
      "learning_rate": 4.50551022937723e-07,
      "loss": 1.7701,
      "step": 49312
    },
    {
      "epoch": 0.0001790791689902005,
      "grad_norm": 10337.542067629036,
      "learning_rate": 4.504047579424512e-07,
      "loss": 1.7733,
      "step": 49344
    },
    {
      "epoch": 0.0001791953033410372,
      "grad_norm": 9783.527789095302,
      "learning_rate": 4.502586353033188e-07,
      "loss": 1.7957,
      "step": 49376
    },
    {
      "epoch": 0.0001793114376918739,
      "grad_norm": 9188.580086172184,
      "learning_rate": 4.501126547895558e-07,
      "loss": 1.7978,
      "step": 49408
    },
    {
      "epoch": 0.0001794275720427106,
      "grad_norm": 9988.62713289469,
      "learning_rate": 4.49966816170916e-07,
      "loss": 1.7879,
      "step": 49440
    },
    {
      "epoch": 0.00017954370639354733,
      "grad_norm": 9209.480441371272,
      "learning_rate": 4.498211192176746e-07,
      "loss": 1.7768,
      "step": 49472
    },
    {
      "epoch": 0.00017965984074438403,
      "grad_norm": 10126.784484721691,
      "learning_rate": 4.496755637006275e-07,
      "loss": 1.7657,
      "step": 49504
    },
    {
      "epoch": 0.00017977597509522073,
      "grad_norm": 9565.786951422242,
      "learning_rate": 4.495301493910893e-07,
      "loss": 1.7893,
      "step": 49536
    },
    {
      "epoch": 0.00017989210944605743,
      "grad_norm": 10152.202913653766,
      "learning_rate": 4.493848760608918e-07,
      "loss": 1.7882,
      "step": 49568
    },
    {
      "epoch": 0.00018000824379689413,
      "grad_norm": 9524.01532968107,
      "learning_rate": 4.492397434823828e-07,
      "loss": 1.7921,
      "step": 49600
    },
    {
      "epoch": 0.00018012437814773086,
      "grad_norm": 9480.002742615638,
      "learning_rate": 4.490947514284244e-07,
      "loss": 1.803,
      "step": 49632
    },
    {
      "epoch": 0.00018024051249856756,
      "grad_norm": 10078.465557811864,
      "learning_rate": 4.489498996723915e-07,
      "loss": 1.8004,
      "step": 49664
    },
    {
      "epoch": 0.00018035664684940426,
      "grad_norm": 9406.679010150181,
      "learning_rate": 4.4880518798817036e-07,
      "loss": 1.7975,
      "step": 49696
    },
    {
      "epoch": 0.00018047278120024096,
      "grad_norm": 10679.758517869212,
      "learning_rate": 4.48660616150157e-07,
      "loss": 1.7969,
      "step": 49728
    },
    {
      "epoch": 0.0001805889155510777,
      "grad_norm": 9910.262458683927,
      "learning_rate": 4.48516183933256e-07,
      "loss": 1.799,
      "step": 49760
    },
    {
      "epoch": 0.0001807050499019144,
      "grad_norm": 9085.491951457554,
      "learning_rate": 4.483718911128787e-07,
      "loss": 1.8022,
      "step": 49792
    },
    {
      "epoch": 0.00018082118425275109,
      "grad_norm": 9340.93292985235,
      "learning_rate": 4.482277374649419e-07,
      "loss": 1.8113,
      "step": 49824
    },
    {
      "epoch": 0.00018093731860358779,
      "grad_norm": 8732.09699900316,
      "learning_rate": 4.4808372276586657e-07,
      "loss": 1.818,
      "step": 49856
    },
    {
      "epoch": 0.00018105345295442449,
      "grad_norm": 9364.789372965097,
      "learning_rate": 4.479443408191003e-07,
      "loss": 1.7989,
      "step": 49888
    },
    {
      "epoch": 0.0001811695873052612,
      "grad_norm": 9942.247834368241,
      "learning_rate": 4.478005990241576e-07,
      "loss": 1.7837,
      "step": 49920
    },
    {
      "epoch": 0.0001812857216560979,
      "grad_norm": 8577.958731539806,
      "learning_rate": 4.476569955172804e-07,
      "loss": 1.7647,
      "step": 49952
    },
    {
      "epoch": 0.0001814018560069346,
      "grad_norm": 8526.477349996305,
      "learning_rate": 4.475135300768755e-07,
      "loss": 1.79,
      "step": 49984
    },
    {
      "epoch": 0.0001815179903577713,
      "grad_norm": 8872.128267783328,
      "learning_rate": 4.473702024818462e-07,
      "loss": 1.7842,
      "step": 50016
    },
    {
      "epoch": 0.000181634124708608,
      "grad_norm": 10489.143339663158,
      "learning_rate": 4.472270125115915e-07,
      "loss": 1.7786,
      "step": 50048
    },
    {
      "epoch": 0.00018175025905944474,
      "grad_norm": 10406.077743319045,
      "learning_rate": 4.4708395994600416e-07,
      "loss": 1.7645,
      "step": 50080
    },
    {
      "epoch": 0.00018186639341028144,
      "grad_norm": 9604.146812705436,
      "learning_rate": 4.4694104456546943e-07,
      "loss": 1.7795,
      "step": 50112
    },
    {
      "epoch": 0.00018198252776111814,
      "grad_norm": 9632.673564488729,
      "learning_rate": 4.4679826615086383e-07,
      "loss": 1.7914,
      "step": 50144
    },
    {
      "epoch": 0.00018209866211195484,
      "grad_norm": 10369.06649607379,
      "learning_rate": 4.466556244835534e-07,
      "loss": 1.8019,
      "step": 50176
    },
    {
      "epoch": 0.00018221479646279157,
      "grad_norm": 9886.433532877263,
      "learning_rate": 4.4651311934539243e-07,
      "loss": 1.8053,
      "step": 50208
    },
    {
      "epoch": 0.00018233093081362827,
      "grad_norm": 9609.09173647541,
      "learning_rate": 4.463707505187224e-07,
      "loss": 1.7578,
      "step": 50240
    },
    {
      "epoch": 0.00018244706516446497,
      "grad_norm": 10642.350210362372,
      "learning_rate": 4.4622851778637e-07,
      "loss": 1.7544,
      "step": 50272
    },
    {
      "epoch": 0.00018256319951530167,
      "grad_norm": 9987.15294766231,
      "learning_rate": 4.4608642093164596e-07,
      "loss": 1.763,
      "step": 50304
    },
    {
      "epoch": 0.00018267933386613837,
      "grad_norm": 9236.389987435567,
      "learning_rate": 4.4594445973834397e-07,
      "loss": 1.7616,
      "step": 50336
    },
    {
      "epoch": 0.0001827954682169751,
      "grad_norm": 11085.58514468226,
      "learning_rate": 4.4580263399073887e-07,
      "loss": 1.7914,
      "step": 50368
    },
    {
      "epoch": 0.0001829116025678118,
      "grad_norm": 12003.359029871597,
      "learning_rate": 4.456609434735855e-07,
      "loss": 1.8005,
      "step": 50400
    },
    {
      "epoch": 0.0001830277369186485,
      "grad_norm": 8600.675438592018,
      "learning_rate": 4.455193879721174e-07,
      "loss": 1.8035,
      "step": 50432
    },
    {
      "epoch": 0.0001831438712694852,
      "grad_norm": 10633.500740583979,
      "learning_rate": 4.453779672720451e-07,
      "loss": 1.8176,
      "step": 50464
    },
    {
      "epoch": 0.00018326000562032192,
      "grad_norm": 9786.777304097606,
      "learning_rate": 4.45236681159555e-07,
      "loss": 1.8259,
      "step": 50496
    },
    {
      "epoch": 0.00018337613997115862,
      "grad_norm": 9185.612554424446,
      "learning_rate": 4.450955294213082e-07,
      "loss": 1.8026,
      "step": 50528
    },
    {
      "epoch": 0.00018349227432199532,
      "grad_norm": 10007.778574688791,
      "learning_rate": 4.4495451184443883e-07,
      "loss": 1.8179,
      "step": 50560
    },
    {
      "epoch": 0.00018360840867283202,
      "grad_norm": 10826.056992275628,
      "learning_rate": 4.448136282165527e-07,
      "loss": 1.8251,
      "step": 50592
    },
    {
      "epoch": 0.00018372454302366872,
      "grad_norm": 9404.128029753741,
      "learning_rate": 4.4467287832572624e-07,
      "loss": 1.8122,
      "step": 50624
    },
    {
      "epoch": 0.00018384067737450545,
      "grad_norm": 9730.40225273344,
      "learning_rate": 4.445322619605049e-07,
      "loss": 1.7912,
      "step": 50656
    },
    {
      "epoch": 0.00018395681172534215,
      "grad_norm": 10132.698949440864,
      "learning_rate": 4.443917789099019e-07,
      "loss": 1.7916,
      "step": 50688
    },
    {
      "epoch": 0.00018407294607617885,
      "grad_norm": 10851.131738210537,
      "learning_rate": 4.442514289633969e-07,
      "loss": 1.7737,
      "step": 50720
    },
    {
      "epoch": 0.00018418908042701555,
      "grad_norm": 9326.273639562587,
      "learning_rate": 4.4411121191093467e-07,
      "loss": 1.7688,
      "step": 50752
    },
    {
      "epoch": 0.00018430521477785227,
      "grad_norm": 10098.605547302062,
      "learning_rate": 4.439711275429239e-07,
      "loss": 1.7956,
      "step": 50784
    },
    {
      "epoch": 0.00018442134912868897,
      "grad_norm": 10777.456657300923,
      "learning_rate": 4.438311756502356e-07,
      "loss": 1.7751,
      "step": 50816
    },
    {
      "epoch": 0.00018453748347952567,
      "grad_norm": 10192.120780289057,
      "learning_rate": 4.43691356024202e-07,
      "loss": 1.7573,
      "step": 50848
    },
    {
      "epoch": 0.00018465361783036237,
      "grad_norm": 11309.406880999552,
      "learning_rate": 4.435516684566152e-07,
      "loss": 1.7652,
      "step": 50880
    },
    {
      "epoch": 0.00018476975218119907,
      "grad_norm": 9409.878851504944,
      "learning_rate": 4.434164718621521e-07,
      "loss": 1.7887,
      "step": 50912
    },
    {
      "epoch": 0.0001848858865320358,
      "grad_norm": 10285.796031421194,
      "learning_rate": 4.432770436779451e-07,
      "loss": 1.7827,
      "step": 50944
    },
    {
      "epoch": 0.0001850020208828725,
      "grad_norm": 8257.347758209049,
      "learning_rate": 4.4313774693676424e-07,
      "loss": 1.7982,
      "step": 50976
    },
    {
      "epoch": 0.0001851181552337092,
      "grad_norm": 10360.358584527854,
      "learning_rate": 4.4299858143221383e-07,
      "loss": 1.7669,
      "step": 51008
    },
    {
      "epoch": 0.0001852342895845459,
      "grad_norm": 10242.138350949961,
      "learning_rate": 4.428595469583518e-07,
      "loss": 1.7604,
      "step": 51040
    },
    {
      "epoch": 0.00018535042393538263,
      "grad_norm": 8321.451676240149,
      "learning_rate": 4.4272064330968815e-07,
      "loss": 1.7824,
      "step": 51072
    },
    {
      "epoch": 0.00018546655828621933,
      "grad_norm": 10277.107180525072,
      "learning_rate": 4.4258187028118375e-07,
      "loss": 1.7891,
      "step": 51104
    },
    {
      "epoch": 0.00018558269263705603,
      "grad_norm": 8572.04736337825,
      "learning_rate": 4.4244322766824906e-07,
      "loss": 1.7934,
      "step": 51136
    },
    {
      "epoch": 0.00018569882698789273,
      "grad_norm": 9189.107900117398,
      "learning_rate": 4.4230471526674305e-07,
      "loss": 1.8079,
      "step": 51168
    },
    {
      "epoch": 0.00018581496133872943,
      "grad_norm": 8934.084172426405,
      "learning_rate": 4.421663328729715e-07,
      "loss": 1.8125,
      "step": 51200
    },
    {
      "epoch": 0.00018593109568956615,
      "grad_norm": 8644.879177871719,
      "learning_rate": 4.4202808028368623e-07,
      "loss": 1.8053,
      "step": 51232
    },
    {
      "epoch": 0.00018604723004040285,
      "grad_norm": 8941.116037721466,
      "learning_rate": 4.418899572960836e-07,
      "loss": 1.7984,
      "step": 51264
    },
    {
      "epoch": 0.00018616336439123955,
      "grad_norm": 8711.92458645046,
      "learning_rate": 4.417519637078032e-07,
      "loss": 1.7998,
      "step": 51296
    },
    {
      "epoch": 0.00018627949874207625,
      "grad_norm": 10066.904390129073,
      "learning_rate": 4.4161409931692674e-07,
      "loss": 1.7982,
      "step": 51328
    },
    {
      "epoch": 0.00018639563309291298,
      "grad_norm": 8769.57102713696,
      "learning_rate": 4.4147636392197683e-07,
      "loss": 1.8116,
      "step": 51360
    },
    {
      "epoch": 0.00018651176744374968,
      "grad_norm": 10695.378441177292,
      "learning_rate": 4.413387573219156e-07,
      "loss": 1.8227,
      "step": 51392
    },
    {
      "epoch": 0.00018662790179458638,
      "grad_norm": 9994.516796724092,
      "learning_rate": 4.412012793161435e-07,
      "loss": 1.8094,
      "step": 51424
    },
    {
      "epoch": 0.00018674403614542308,
      "grad_norm": 10028.09064578098,
      "learning_rate": 4.410639297044983e-07,
      "loss": 1.7617,
      "step": 51456
    },
    {
      "epoch": 0.00018686017049625978,
      "grad_norm": 9796.596653940593,
      "learning_rate": 4.4092670828725353e-07,
      "loss": 1.771,
      "step": 51488
    },
    {
      "epoch": 0.0001869763048470965,
      "grad_norm": 10743.476346136757,
      "learning_rate": 4.407896148651174e-07,
      "loss": 1.7778,
      "step": 51520
    },
    {
      "epoch": 0.0001870924391979332,
      "grad_norm": 10448.406289956378,
      "learning_rate": 4.4065264923923174e-07,
      "loss": 1.7937,
      "step": 51552
    },
    {
      "epoch": 0.0001872085735487699,
      "grad_norm": 9365.918961853129,
      "learning_rate": 4.405158112111706e-07,
      "loss": 1.7666,
      "step": 51584
    },
    {
      "epoch": 0.0001873247078996066,
      "grad_norm": 8160.27511791116,
      "learning_rate": 4.4037910058293887e-07,
      "loss": 1.7672,
      "step": 51616
    },
    {
      "epoch": 0.00018744084225044333,
      "grad_norm": 10184.980215984713,
      "learning_rate": 4.402425171569716e-07,
      "loss": 1.7731,
      "step": 51648
    },
    {
      "epoch": 0.00018755697660128003,
      "grad_norm": 8477.200481290978,
      "learning_rate": 4.401060607361324e-07,
      "loss": 1.7929,
      "step": 51680
    },
    {
      "epoch": 0.00018767311095211673,
      "grad_norm": 10832.015324952232,
      "learning_rate": 4.3996973112371225e-07,
      "loss": 1.8095,
      "step": 51712
    },
    {
      "epoch": 0.00018778924530295343,
      "grad_norm": 10217.31647743183,
      "learning_rate": 4.398335281234285e-07,
      "loss": 1.7994,
      "step": 51744
    },
    {
      "epoch": 0.00018790537965379013,
      "grad_norm": 9192.97111928456,
      "learning_rate": 4.396974515394236e-07,
      "loss": 1.7839,
      "step": 51776
    },
    {
      "epoch": 0.00018802151400462686,
      "grad_norm": 10002.561971815021,
      "learning_rate": 4.395615011762637e-07,
      "loss": 1.7802,
      "step": 51808
    },
    {
      "epoch": 0.00018813764835546356,
      "grad_norm": 10043.37254113378,
      "learning_rate": 4.3942567683893806e-07,
      "loss": 1.7832,
      "step": 51840
    },
    {
      "epoch": 0.00018825378270630026,
      "grad_norm": 10034.081921132596,
      "learning_rate": 4.3928997833285704e-07,
      "loss": 1.7703,
      "step": 51872
    },
    {
      "epoch": 0.00018836991705713696,
      "grad_norm": 11291.026436954258,
      "learning_rate": 4.391586402162007e-07,
      "loss": 1.7835,
      "step": 51904
    },
    {
      "epoch": 0.0001884860514079737,
      "grad_norm": 9343.882704743248,
      "learning_rate": 4.3902318887334586e-07,
      "loss": 1.7886,
      "step": 51936
    },
    {
      "epoch": 0.0001886021857588104,
      "grad_norm": 10157.309190922564,
      "learning_rate": 4.388878627865203e-07,
      "loss": 1.8077,
      "step": 51968
    },
    {
      "epoch": 0.0001887183201096471,
      "grad_norm": 9220.96307334543,
      "learning_rate": 4.387526617627962e-07,
      "loss": 1.7941,
      "step": 52000
    },
    {
      "epoch": 0.0001888344544604838,
      "grad_norm": 8792.520685218773,
      "learning_rate": 4.386175856096614e-07,
      "loss": 1.8026,
      "step": 52032
    },
    {
      "epoch": 0.0001889505888113205,
      "grad_norm": 9711.278391643398,
      "learning_rate": 4.384826341350183e-07,
      "loss": 1.7821,
      "step": 52064
    },
    {
      "epoch": 0.00018906672316215721,
      "grad_norm": 9381.160216092676,
      "learning_rate": 4.3834780714718287e-07,
      "loss": 1.796,
      "step": 52096
    },
    {
      "epoch": 0.00018918285751299391,
      "grad_norm": 9067.560421634917,
      "learning_rate": 4.3821310445488347e-07,
      "loss": 1.8098,
      "step": 52128
    },
    {
      "epoch": 0.00018929899186383061,
      "grad_norm": 8512.369470364876,
      "learning_rate": 4.380785258672594e-07,
      "loss": 1.7924,
      "step": 52160
    },
    {
      "epoch": 0.0001894151262146673,
      "grad_norm": 9577.45415024264,
      "learning_rate": 4.3794407119386034e-07,
      "loss": 1.7858,
      "step": 52192
    },
    {
      "epoch": 0.00018953126056550404,
      "grad_norm": 9817.593187742095,
      "learning_rate": 4.378097402446446e-07,
      "loss": 1.7727,
      "step": 52224
    },
    {
      "epoch": 0.00018964739491634074,
      "grad_norm": 10464.495974484389,
      "learning_rate": 4.376755328299784e-07,
      "loss": 1.7753,
      "step": 52256
    },
    {
      "epoch": 0.00018976352926717744,
      "grad_norm": 10092.708853424832,
      "learning_rate": 4.375414487606347e-07,
      "loss": 1.7876,
      "step": 52288
    },
    {
      "epoch": 0.00018987966361801414,
      "grad_norm": 10026.747478619376,
      "learning_rate": 4.374074878477919e-07,
      "loss": 1.817,
      "step": 52320
    },
    {
      "epoch": 0.00018999579796885084,
      "grad_norm": 9336.116430293701,
      "learning_rate": 4.372736499030328e-07,
      "loss": 1.7995,
      "step": 52352
    },
    {
      "epoch": 0.00019011193231968757,
      "grad_norm": 10136.99659662565,
      "learning_rate": 4.371399347383436e-07,
      "loss": 1.7937,
      "step": 52384
    },
    {
      "epoch": 0.00019022806667052427,
      "grad_norm": 9885.791116547021,
      "learning_rate": 4.3700634216611283e-07,
      "loss": 1.7937,
      "step": 52416
    },
    {
      "epoch": 0.00019034420102136097,
      "grad_norm": 10690.513364661212,
      "learning_rate": 4.3687287199912974e-07,
      "loss": 1.7821,
      "step": 52448
    },
    {
      "epoch": 0.00019046033537219767,
      "grad_norm": 10063.775931527887,
      "learning_rate": 4.36739524050584e-07,
      "loss": 1.7812,
      "step": 52480
    },
    {
      "epoch": 0.00019057646972303437,
      "grad_norm": 9259.536489479373,
      "learning_rate": 4.3660629813406394e-07,
      "loss": 1.7614,
      "step": 52512
    },
    {
      "epoch": 0.0001906926040738711,
      "grad_norm": 8440.532210708043,
      "learning_rate": 4.364731940635558e-07,
      "loss": 1.7641,
      "step": 52544
    },
    {
      "epoch": 0.0001908087384247078,
      "grad_norm": 9194.394161661769,
      "learning_rate": 4.363402116534423e-07,
      "loss": 1.7784,
      "step": 52576
    },
    {
      "epoch": 0.0001909248727755445,
      "grad_norm": 8571.693298292934,
      "learning_rate": 4.3620735071850214e-07,
      "loss": 1.767,
      "step": 52608
    },
    {
      "epoch": 0.0001910410071263812,
      "grad_norm": 9274.767921624778,
      "learning_rate": 4.3607461107390845e-07,
      "loss": 1.7951,
      "step": 52640
    },
    {
      "epoch": 0.00019115714147721792,
      "grad_norm": 10387.0894864731,
      "learning_rate": 4.359419925352277e-07,
      "loss": 1.7913,
      "step": 52672
    },
    {
      "epoch": 0.00019127327582805462,
      "grad_norm": 9933.889771886943,
      "learning_rate": 4.3580949491841877e-07,
      "loss": 1.8031,
      "step": 52704
    },
    {
      "epoch": 0.00019138941017889132,
      "grad_norm": 8588.186537331383,
      "learning_rate": 4.3567711803983206e-07,
      "loss": 1.7911,
      "step": 52736
    },
    {
      "epoch": 0.00019150554452972802,
      "grad_norm": 9583.184856820828,
      "learning_rate": 4.3554486171620797e-07,
      "loss": 1.7877,
      "step": 52768
    },
    {
      "epoch": 0.00019162167888056472,
      "grad_norm": 9544.839233847786,
      "learning_rate": 4.354127257646762e-07,
      "loss": 1.7886,
      "step": 52800
    },
    {
      "epoch": 0.00019173781323140145,
      "grad_norm": 9917.563410435045,
      "learning_rate": 4.352807100027546e-07,
      "loss": 1.7939,
      "step": 52832
    },
    {
      "epoch": 0.00019185394758223815,
      "grad_norm": 10007.131856830907,
      "learning_rate": 4.3514881424834796e-07,
      "loss": 1.788,
      "step": 52864
    },
    {
      "epoch": 0.00019197008193307485,
      "grad_norm": 9256.75213020204,
      "learning_rate": 4.350211545055486e-07,
      "loss": 1.8044,
      "step": 52896
    },
    {
      "epoch": 0.00019208621628391155,
      "grad_norm": 9567.03005117053,
      "learning_rate": 4.3488949448528033e-07,
      "loss": 1.8245,
      "step": 52928
    },
    {
      "epoch": 0.00019220235063474827,
      "grad_norm": 10926.590227513796,
      "learning_rate": 4.347579539342021e-07,
      "loss": 1.8087,
      "step": 52960
    },
    {
      "epoch": 0.00019231848498558497,
      "grad_norm": 9995.675965136124,
      "learning_rate": 4.3462653267174455e-07,
      "loss": 1.7734,
      "step": 52992
    },
    {
      "epoch": 0.00019243461933642167,
      "grad_norm": 9121.815608748075,
      "learning_rate": 4.344952305177202e-07,
      "loss": 1.7673,
      "step": 53024
    },
    {
      "epoch": 0.00019255075368725837,
      "grad_norm": 9416.018160560227,
      "learning_rate": 4.343640472923223e-07,
      "loss": 1.7835,
      "step": 53056
    },
    {
      "epoch": 0.00019266688803809507,
      "grad_norm": 10324.691375532733,
      "learning_rate": 4.3423298281612404e-07,
      "loss": 1.7748,
      "step": 53088
    },
    {
      "epoch": 0.0001927830223889318,
      "grad_norm": 10064.864032862044,
      "learning_rate": 4.341020369100772e-07,
      "loss": 1.7742,
      "step": 53120
    },
    {
      "epoch": 0.0001928991567397685,
      "grad_norm": 9279.715728404615,
      "learning_rate": 4.339712093955114e-07,
      "loss": 1.783,
      "step": 53152
    },
    {
      "epoch": 0.0001930152910906052,
      "grad_norm": 9462.138447518088,
      "learning_rate": 4.33840500094133e-07,
      "loss": 1.786,
      "step": 53184
    },
    {
      "epoch": 0.0001931314254414419,
      "grad_norm": 13003.809903255276,
      "learning_rate": 4.3370990882802375e-07,
      "loss": 1.7977,
      "step": 53216
    },
    {
      "epoch": 0.00019324755979227863,
      "grad_norm": 10540.095824991346,
      "learning_rate": 4.3357943541964046e-07,
      "loss": 1.816,
      "step": 53248
    },
    {
      "epoch": 0.00019336369414311533,
      "grad_norm": 8929.001735916507,
      "learning_rate": 4.334490796918133e-07,
      "loss": 1.7795,
      "step": 53280
    },
    {
      "epoch": 0.00019347982849395203,
      "grad_norm": 9077.451845093974,
      "learning_rate": 4.333188414677453e-07,
      "loss": 1.7603,
      "step": 53312
    },
    {
      "epoch": 0.00019359596284478873,
      "grad_norm": 9265.941290554349,
      "learning_rate": 4.331887205710109e-07,
      "loss": 1.7655,
      "step": 53344
    },
    {
      "epoch": 0.00019371209719562543,
      "grad_norm": 10710.741057461897,
      "learning_rate": 4.330587168255553e-07,
      "loss": 1.7579,
      "step": 53376
    },
    {
      "epoch": 0.00019382823154646215,
      "grad_norm": 10638.845144093413,
      "learning_rate": 4.3292883005569325e-07,
      "loss": 1.7778,
      "step": 53408
    },
    {
      "epoch": 0.00019394436589729885,
      "grad_norm": 10944.07127169775,
      "learning_rate": 4.3279906008610825e-07,
      "loss": 1.8032,
      "step": 53440
    },
    {
      "epoch": 0.00019406050024813555,
      "grad_norm": 11281.410638745494,
      "learning_rate": 4.3266940674185135e-07,
      "loss": 1.7886,
      "step": 53472
    },
    {
      "epoch": 0.00019417663459897225,
      "grad_norm": 8056.8846336533825,
      "learning_rate": 4.325398698483402e-07,
      "loss": 1.8161,
      "step": 53504
    },
    {
      "epoch": 0.00019429276894980898,
      "grad_norm": 10319.232335789324,
      "learning_rate": 4.3241044923135824e-07,
      "loss": 1.8345,
      "step": 53536
    },
    {
      "epoch": 0.00019440890330064568,
      "grad_norm": 9423.048551291668,
      "learning_rate": 4.3228114471705354e-07,
      "loss": 1.8266,
      "step": 53568
    },
    {
      "epoch": 0.00019452503765148238,
      "grad_norm": 10035.90075678312,
      "learning_rate": 4.3215195613193773e-07,
      "loss": 1.7781,
      "step": 53600
    },
    {
      "epoch": 0.00019464117200231908,
      "grad_norm": 9973.256439097511,
      "learning_rate": 4.320228833028854e-07,
      "loss": 1.7859,
      "step": 53632
    },
    {
      "epoch": 0.00019475730635315578,
      "grad_norm": 10231.740321177038,
      "learning_rate": 4.3189392605713257e-07,
      "loss": 1.7939,
      "step": 53664
    },
    {
      "epoch": 0.0001948734407039925,
      "grad_norm": 11805.64432803225,
      "learning_rate": 4.317650842222763e-07,
      "loss": 1.7863,
      "step": 53696
    },
    {
      "epoch": 0.0001949895750548292,
      "grad_norm": 9790.67862816465,
      "learning_rate": 4.3163635762627333e-07,
      "loss": 1.7592,
      "step": 53728
    },
    {
      "epoch": 0.0001951057094056659,
      "grad_norm": 10189.359548077593,
      "learning_rate": 4.315077460974393e-07,
      "loss": 1.7634,
      "step": 53760
    },
    {
      "epoch": 0.0001952218437565026,
      "grad_norm": 8970.494300761804,
      "learning_rate": 4.313792494644477e-07,
      "loss": 1.7584,
      "step": 53792
    },
    {
      "epoch": 0.00019533797810733933,
      "grad_norm": 9432.953196109902,
      "learning_rate": 4.312508675563289e-07,
      "loss": 1.7801,
      "step": 53824
    },
    {
      "epoch": 0.00019545411245817603,
      "grad_norm": 8431.906427374535,
      "learning_rate": 4.311226002024692e-07,
      "loss": 1.7946,
      "step": 53856
    },
    {
      "epoch": 0.00019557024680901273,
      "grad_norm": 9530.420977060772,
      "learning_rate": 4.309944472326101e-07,
      "loss": 1.7722,
      "step": 53888
    },
    {
      "epoch": 0.00019568638115984943,
      "grad_norm": 9877.610338538365,
      "learning_rate": 4.3087040796082297e-07,
      "loss": 1.78,
      "step": 53920
    },
    {
      "epoch": 0.00019580251551068613,
      "grad_norm": 11473.484910871675,
      "learning_rate": 4.307424796882752e-07,
      "loss": 1.7825,
      "step": 53952
    },
    {
      "epoch": 0.00019591864986152286,
      "grad_norm": 11807.370155966146,
      "learning_rate": 4.306146652963548e-07,
      "loss": 1.7935,
      "step": 53984
    },
    {
      "epoch": 0.00019603478421235956,
      "grad_norm": 10081.823644559548,
      "learning_rate": 4.304869646162027e-07,
      "loss": 1.7872,
      "step": 54016
    },
    {
      "epoch": 0.00019615091856319626,
      "grad_norm": 9700.108968460097,
      "learning_rate": 4.303593774793103e-07,
      "loss": 1.7771,
      "step": 54048
    },
    {
      "epoch": 0.00019626705291403296,
      "grad_norm": 10116.073744294275,
      "learning_rate": 4.302319037175182e-07,
      "loss": 1.7824,
      "step": 54080
    },
    {
      "epoch": 0.0001963831872648697,
      "grad_norm": 9315.456295855829,
      "learning_rate": 4.301045431630156e-07,
      "loss": 1.8008,
      "step": 54112
    },
    {
      "epoch": 0.0001964993216157064,
      "grad_norm": 9108.241872062907,
      "learning_rate": 4.299772956483391e-07,
      "loss": 1.809,
      "step": 54144
    },
    {
      "epoch": 0.0001966154559665431,
      "grad_norm": 11911.620376758152,
      "learning_rate": 4.298501610063722e-07,
      "loss": 1.7977,
      "step": 54176
    },
    {
      "epoch": 0.0001967315903173798,
      "grad_norm": 9071.890872359521,
      "learning_rate": 4.297231390703436e-07,
      "loss": 1.7824,
      "step": 54208
    },
    {
      "epoch": 0.0001968477246682165,
      "grad_norm": 11956.59834568344,
      "learning_rate": 4.295962296738272e-07,
      "loss": 1.7862,
      "step": 54240
    },
    {
      "epoch": 0.00019696385901905321,
      "grad_norm": 9187.48474828666,
      "learning_rate": 4.2946943265074063e-07,
      "loss": 1.7879,
      "step": 54272
    },
    {
      "epoch": 0.00019707999336988991,
      "grad_norm": 9370.513966693608,
      "learning_rate": 4.293427478353443e-07,
      "loss": 1.7951,
      "step": 54304
    },
    {
      "epoch": 0.00019719612772072661,
      "grad_norm": 10541.967368570251,
      "learning_rate": 4.292161750622408e-07,
      "loss": 1.7868,
      "step": 54336
    },
    {
      "epoch": 0.00019731226207156331,
      "grad_norm": 9649.517086362404,
      "learning_rate": 4.290897141663739e-07,
      "loss": 1.7898,
      "step": 54368
    },
    {
      "epoch": 0.00019742839642240004,
      "grad_norm": 9410.02125396112,
      "learning_rate": 4.289633649830273e-07,
      "loss": 1.7952,
      "step": 54400
    },
    {
      "epoch": 0.00019754453077323674,
      "grad_norm": 9753.871949128716,
      "learning_rate": 4.2883712734782437e-07,
      "loss": 1.8051,
      "step": 54432
    },
    {
      "epoch": 0.00019766066512407344,
      "grad_norm": 10898.959491621206,
      "learning_rate": 4.2871100109672657e-07,
      "loss": 1.7978,
      "step": 54464
    },
    {
      "epoch": 0.00019777679947491014,
      "grad_norm": 8403.87303569015,
      "learning_rate": 4.2858498606603323e-07,
      "loss": 1.7679,
      "step": 54496
    },
    {
      "epoch": 0.00019789293382574684,
      "grad_norm": 9139.647367376927,
      "learning_rate": 4.2845908209237997e-07,
      "loss": 1.7563,
      "step": 54528
    },
    {
      "epoch": 0.00019800906817658357,
      "grad_norm": 9227.917858325354,
      "learning_rate": 4.2833328901273836e-07,
      "loss": 1.7611,
      "step": 54560
    },
    {
      "epoch": 0.00019812520252742027,
      "grad_norm": 10207.812400313791,
      "learning_rate": 4.2820760666441475e-07,
      "loss": 1.7713,
      "step": 54592
    },
    {
      "epoch": 0.00019824133687825697,
      "grad_norm": 9483.50757894989,
      "learning_rate": 4.2808203488504956e-07,
      "loss": 1.772,
      "step": 54624
    },
    {
      "epoch": 0.00019835747122909367,
      "grad_norm": 11349.275571594868,
      "learning_rate": 4.27956573512616e-07,
      "loss": 1.7753,
      "step": 54656
    },
    {
      "epoch": 0.0001984736055799304,
      "grad_norm": 9919.367217721097,
      "learning_rate": 4.278312223854199e-07,
      "loss": 1.8038,
      "step": 54688
    },
    {
      "epoch": 0.0001985897399307671,
      "grad_norm": 10506.167331620032,
      "learning_rate": 4.277059813420981e-07,
      "loss": 1.8053,
      "step": 54720
    },
    {
      "epoch": 0.0001987058742816038,
      "grad_norm": 9975.169973489174,
      "learning_rate": 4.2758085022161816e-07,
      "loss": 1.8105,
      "step": 54752
    },
    {
      "epoch": 0.0001988220086324405,
      "grad_norm": 10199.56253963865,
      "learning_rate": 4.2745582886327697e-07,
      "loss": 1.7975,
      "step": 54784
    },
    {
      "epoch": 0.0001989381429832772,
      "grad_norm": 11608.841113565126,
      "learning_rate": 4.273309171067003e-07,
      "loss": 1.7635,
      "step": 54816
    },
    {
      "epoch": 0.00019905427733411392,
      "grad_norm": 8826.012463168177,
      "learning_rate": 4.2720611479184177e-07,
      "loss": 1.7701,
      "step": 54848
    },
    {
      "epoch": 0.00019917041168495062,
      "grad_norm": 9075.210410783873,
      "learning_rate": 4.270814217589821e-07,
      "loss": 1.7729,
      "step": 54880
    },
    {
      "epoch": 0.00019928654603578732,
      "grad_norm": 10898.823606243015,
      "learning_rate": 4.26960729445746e-07,
      "loss": 1.79,
      "step": 54912
    },
    {
      "epoch": 0.00019940268038662402,
      "grad_norm": 8162.447549601774,
      "learning_rate": 4.268362510963233e-07,
      "loss": 1.7897,
      "step": 54944
    },
    {
      "epoch": 0.00019951881473746075,
      "grad_norm": 11021.790054251624,
      "learning_rate": 4.2671188155665045e-07,
      "loss": 1.8085,
      "step": 54976
    },
    {
      "epoch": 0.00019963494908829745,
      "grad_norm": 11369.735089262194,
      "learning_rate": 4.2658762066829746e-07,
      "loss": 1.7964,
      "step": 55008
    },
    {
      "epoch": 0.00019975108343913415,
      "grad_norm": 8627.047698952405,
      "learning_rate": 4.264634682731569e-07,
      "loss": 1.794,
      "step": 55040
    },
    {
      "epoch": 0.00019986721778997085,
      "grad_norm": 9084.457276029207,
      "learning_rate": 4.263394242134436e-07,
      "loss": 1.8115,
      "step": 55072
    },
    {
      "epoch": 0.00019998335214080755,
      "grad_norm": 10967.751820678657,
      "learning_rate": 4.262154883316931e-07,
      "loss": 1.7927,
      "step": 55104
    },
    {
      "epoch": 0.00020009948649164427,
      "grad_norm": 9387.522356830903,
      "learning_rate": 4.2609166047076134e-07,
      "loss": 1.7866,
      "step": 55136
    },
    {
      "epoch": 0.00020021562084248097,
      "grad_norm": 10834.250597064847,
      "learning_rate": 4.2596794047382387e-07,
      "loss": 1.7803,
      "step": 55168
    },
    {
      "epoch": 0.00020033175519331767,
      "grad_norm": 9002.312036360438,
      "learning_rate": 4.2584432818437433e-07,
      "loss": 1.7965,
      "step": 55200
    },
    {
      "epoch": 0.00020044788954415437,
      "grad_norm": 9700.395043502094,
      "learning_rate": 4.257208234462245e-07,
      "loss": 1.7861,
      "step": 55232
    },
    {
      "epoch": 0.00020056402389499107,
      "grad_norm": 9538.639944981674,
      "learning_rate": 4.25597426103503e-07,
      "loss": 1.786,
      "step": 55264
    },
    {
      "epoch": 0.0002006801582458278,
      "grad_norm": 10036.086388627791,
      "learning_rate": 4.254741360006543e-07,
      "loss": 1.7834,
      "step": 55296
    },
    {
      "epoch": 0.0002007962925966645,
      "grad_norm": 9091.894742021599,
      "learning_rate": 4.2535095298243836e-07,
      "loss": 1.7753,
      "step": 55328
    },
    {
      "epoch": 0.0002009124269475012,
      "grad_norm": 9400.52743201146,
      "learning_rate": 4.2522787689392943e-07,
      "loss": 1.7765,
      "step": 55360
    },
    {
      "epoch": 0.0002010285612983379,
      "grad_norm": 8514.644913324337,
      "learning_rate": 4.251049075805155e-07,
      "loss": 1.7778,
      "step": 55392
    },
    {
      "epoch": 0.00020114469564917463,
      "grad_norm": 10223.453721712638,
      "learning_rate": 4.2498204488789725e-07,
      "loss": 1.7612,
      "step": 55424
    },
    {
      "epoch": 0.00020126083000001133,
      "grad_norm": 8876.983496661465,
      "learning_rate": 4.248592886620874e-07,
      "loss": 1.765,
      "step": 55456
    },
    {
      "epoch": 0.00020137696435084803,
      "grad_norm": 9015.574413202965,
      "learning_rate": 4.247366387494098e-07,
      "loss": 1.7854,
      "step": 55488
    },
    {
      "epoch": 0.00020149309870168473,
      "grad_norm": 10208.051430121224,
      "learning_rate": 4.2461409499649876e-07,
      "loss": 1.795,
      "step": 55520
    },
    {
      "epoch": 0.00020160923305252143,
      "grad_norm": 9799.095876661275,
      "learning_rate": 4.2449165725029806e-07,
      "loss": 1.7741,
      "step": 55552
    },
    {
      "epoch": 0.00020172536740335815,
      "grad_norm": 10015.919129066488,
      "learning_rate": 4.2436932535806033e-07,
      "loss": 1.7713,
      "step": 55584
    },
    {
      "epoch": 0.00020184150175419485,
      "grad_norm": 9302.087292645667,
      "learning_rate": 4.2424709916734593e-07,
      "loss": 1.7685,
      "step": 55616
    },
    {
      "epoch": 0.00020195763610503155,
      "grad_norm": 9501.303910516703,
      "learning_rate": 4.2412497852602273e-07,
      "loss": 1.767,
      "step": 55648
    },
    {
      "epoch": 0.00020207377045586825,
      "grad_norm": 9459.505906758555,
      "learning_rate": 4.240029632822647e-07,
      "loss": 1.8032,
      "step": 55680
    },
    {
      "epoch": 0.00020218990480670498,
      "grad_norm": 9369.447262245516,
      "learning_rate": 4.238810532845515e-07,
      "loss": 1.804,
      "step": 55712
    },
    {
      "epoch": 0.00020230603915754168,
      "grad_norm": 9674.9605683951,
      "learning_rate": 4.237592483816676e-07,
      "loss": 1.7939,
      "step": 55744
    },
    {
      "epoch": 0.00020242217350837838,
      "grad_norm": 9363.207676859463,
      "learning_rate": 4.236375484227013e-07,
      "loss": 1.7983,
      "step": 55776
    },
    {
      "epoch": 0.00020253830785921508,
      "grad_norm": 10117.767342650255,
      "learning_rate": 4.235159532570442e-07,
      "loss": 1.8086,
      "step": 55808
    },
    {
      "epoch": 0.00020265444221005178,
      "grad_norm": 10605.52676673818,
      "learning_rate": 4.2339446273439045e-07,
      "loss": 1.8154,
      "step": 55840
    },
    {
      "epoch": 0.0002027705765608885,
      "grad_norm": 10322.5869819537,
      "learning_rate": 4.232730767047357e-07,
      "loss": 1.8069,
      "step": 55872
    },
    {
      "epoch": 0.0002028867109117252,
      "grad_norm": 10064.503564508286,
      "learning_rate": 4.2315558349314486e-07,
      "loss": 1.8087,
      "step": 55904
    },
    {
      "epoch": 0.0002030028452625619,
      "grad_norm": 8614.68200225638,
      "learning_rate": 4.2303440274687715e-07,
      "loss": 1.7903,
      "step": 55936
    },
    {
      "epoch": 0.0002031189796133986,
      "grad_norm": 9337.224212794721,
      "learning_rate": 4.229133260500526e-07,
      "loss": 1.7857,
      "step": 55968
    },
    {
      "epoch": 0.00020323511396423533,
      "grad_norm": 10418.659414723183,
      "learning_rate": 4.227923532538564e-07,
      "loss": 1.787,
      "step": 56000
    },
    {
      "epoch": 0.00020335124831507203,
      "grad_norm": 9568.271944295899,
      "learning_rate": 4.226714842097715e-07,
      "loss": 1.7579,
      "step": 56032
    },
    {
      "epoch": 0.00020346738266590873,
      "grad_norm": 9913.880471339162,
      "learning_rate": 4.2255071876957797e-07,
      "loss": 1.7649,
      "step": 56064
    },
    {
      "epoch": 0.00020358351701674543,
      "grad_norm": 10764.250833197822,
      "learning_rate": 4.224300567853522e-07,
      "loss": 1.7775,
      "step": 56096
    },
    {
      "epoch": 0.00020369965136758213,
      "grad_norm": 10221.11324660871,
      "learning_rate": 4.2230949810946594e-07,
      "loss": 1.7813,
      "step": 56128
    },
    {
      "epoch": 0.00020381578571841886,
      "grad_norm": 10000.340794192965,
      "learning_rate": 4.221890425945858e-07,
      "loss": 1.7584,
      "step": 56160
    },
    {
      "epoch": 0.00020393192006925556,
      "grad_norm": 10829.153521859407,
      "learning_rate": 4.220686900936724e-07,
      "loss": 1.7592,
      "step": 56192
    },
    {
      "epoch": 0.00020404805442009226,
      "grad_norm": 9651.256912962166,
      "learning_rate": 4.219484404599795e-07,
      "loss": 1.7727,
      "step": 56224
    },
    {
      "epoch": 0.00020416418877092896,
      "grad_norm": 10185.926761959365,
      "learning_rate": 4.2182829354705354e-07,
      "loss": 1.7826,
      "step": 56256
    },
    {
      "epoch": 0.0002042803231217657,
      "grad_norm": 8187.795307651505,
      "learning_rate": 4.2170824920873257e-07,
      "loss": 1.7962,
      "step": 56288
    },
    {
      "epoch": 0.0002043964574726024,
      "grad_norm": 8630.902154467978,
      "learning_rate": 4.2158830729914557e-07,
      "loss": 1.7663,
      "step": 56320
    },
    {
      "epoch": 0.0002045125918234391,
      "grad_norm": 8514.573858978498,
      "learning_rate": 4.21468467672712e-07,
      "loss": 1.7584,
      "step": 56352
    },
    {
      "epoch": 0.0002046287261742758,
      "grad_norm": 8658.057172368406,
      "learning_rate": 4.2134873018414067e-07,
      "loss": 1.7726,
      "step": 56384
    },
    {
      "epoch": 0.0002047448605251125,
      "grad_norm": 9782.135656389151,
      "learning_rate": 4.212290946884291e-07,
      "loss": 1.7807,
      "step": 56416
    },
    {
      "epoch": 0.00020486099487594921,
      "grad_norm": 9275.58645046231,
      "learning_rate": 4.2110956104086313e-07,
      "loss": 1.805,
      "step": 56448
    },
    {
      "epoch": 0.00020497712922678591,
      "grad_norm": 9782.703614032269,
      "learning_rate": 4.2099012909701545e-07,
      "loss": 1.8172,
      "step": 56480
    },
    {
      "epoch": 0.00020509326357762261,
      "grad_norm": 10168.021243093466,
      "learning_rate": 4.2087079871274584e-07,
      "loss": 1.8092,
      "step": 56512
    },
    {
      "epoch": 0.00020520939792845931,
      "grad_norm": 9889.00733137558,
      "learning_rate": 4.207515697441995e-07,
      "loss": 1.7927,
      "step": 56544
    },
    {
      "epoch": 0.00020532553227929604,
      "grad_norm": 8956.844477828115,
      "learning_rate": 4.206324420478069e-07,
      "loss": 1.8013,
      "step": 56576
    },
    {
      "epoch": 0.00020544166663013274,
      "grad_norm": 9875.56266751419,
      "learning_rate": 4.205134154802829e-07,
      "loss": 1.8115,
      "step": 56608
    },
    {
      "epoch": 0.00020555780098096944,
      "grad_norm": 10965.064705691437,
      "learning_rate": 4.2039448989862604e-07,
      "loss": 1.8024,
      "step": 56640
    },
    {
      "epoch": 0.00020567393533180614,
      "grad_norm": 10425.906195626354,
      "learning_rate": 4.202756651601177e-07,
      "loss": 1.8094,
      "step": 56672
    },
    {
      "epoch": 0.00020579006968264284,
      "grad_norm": 9588.379320823724,
      "learning_rate": 4.201569411223216e-07,
      "loss": 1.8045,
      "step": 56704
    },
    {
      "epoch": 0.00020590620403347957,
      "grad_norm": 9203.479124765809,
      "learning_rate": 4.2003831764308285e-07,
      "loss": 1.7901,
      "step": 56736
    },
    {
      "epoch": 0.00020602233838431627,
      "grad_norm": 9387.48869506643,
      "learning_rate": 4.1991979458052733e-07,
      "loss": 1.7513,
      "step": 56768
    },
    {
      "epoch": 0.00020613847273515297,
      "grad_norm": 11782.572724154941,
      "learning_rate": 4.1980137179306116e-07,
      "loss": 1.7625,
      "step": 56800
    },
    {
      "epoch": 0.00020625460708598967,
      "grad_norm": 11238.73160103043,
      "learning_rate": 4.1968304913936964e-07,
      "loss": 1.7512,
      "step": 56832
    },
    {
      "epoch": 0.0002063707414368264,
      "grad_norm": 11766.560924926196,
      "learning_rate": 4.1956482647841677e-07,
      "loss": 1.7722,
      "step": 56864
    },
    {
      "epoch": 0.0002064868757876631,
      "grad_norm": 10670.074413986062,
      "learning_rate": 4.1944670366944467e-07,
      "loss": 1.7754,
      "step": 56896
    },
    {
      "epoch": 0.0002066030101384998,
      "grad_norm": 10832.160541646343,
      "learning_rate": 4.193323672858569e-07,
      "loss": 1.7774,
      "step": 56928
    },
    {
      "epoch": 0.0002067191444893365,
      "grad_norm": 10169.25975673746,
      "learning_rate": 4.1921444065019675e-07,
      "loss": 1.7779,
      "step": 56960
    },
    {
      "epoch": 0.0002068352788401732,
      "grad_norm": 9463.377726795015,
      "learning_rate": 4.1909661345027243e-07,
      "loss": 1.8007,
      "step": 56992
    },
    {
      "epoch": 0.00020695141319100992,
      "grad_norm": 9853.658102451089,
      "learning_rate": 4.1897888554642216e-07,
      "loss": 1.7919,
      "step": 57024
    },
    {
      "epoch": 0.00020706754754184662,
      "grad_norm": 9333.261594962396,
      "learning_rate": 4.188612567992584e-07,
      "loss": 1.7919,
      "step": 57056
    },
    {
      "epoch": 0.00020718368189268332,
      "grad_norm": 10510.519777822597,
      "learning_rate": 4.187437270696678e-07,
      "loss": 1.771,
      "step": 57088
    },
    {
      "epoch": 0.00020729981624352002,
      "grad_norm": 9533.128762373872,
      "learning_rate": 4.186262962188096e-07,
      "loss": 1.7598,
      "step": 57120
    },
    {
      "epoch": 0.00020741595059435675,
      "grad_norm": 10255.6682863673,
      "learning_rate": 4.185089641081159e-07,
      "loss": 1.7514,
      "step": 57152
    },
    {
      "epoch": 0.00020753208494519345,
      "grad_norm": 9534.018355342096,
      "learning_rate": 4.183917305992902e-07,
      "loss": 1.768,
      "step": 57184
    },
    {
      "epoch": 0.00020764821929603015,
      "grad_norm": 10620.851378302967,
      "learning_rate": 4.182745955543071e-07,
      "loss": 1.7979,
      "step": 57216
    },
    {
      "epoch": 0.00020776435364686685,
      "grad_norm": 9423.482052829517,
      "learning_rate": 4.1815755883541167e-07,
      "loss": 1.8096,
      "step": 57248
    },
    {
      "epoch": 0.00020788048799770355,
      "grad_norm": 10096.999158165756,
      "learning_rate": 4.180406203051185e-07,
      "loss": 1.7954,
      "step": 57280
    },
    {
      "epoch": 0.00020799662234854028,
      "grad_norm": 7908.521480023936,
      "learning_rate": 4.179237798262112e-07,
      "loss": 1.7894,
      "step": 57312
    },
    {
      "epoch": 0.00020811275669937698,
      "grad_norm": 9954.118946446239,
      "learning_rate": 4.178070372617418e-07,
      "loss": 1.7993,
      "step": 57344
    },
    {
      "epoch": 0.00020822889105021367,
      "grad_norm": 9595.5496976463,
      "learning_rate": 4.1769039247502976e-07,
      "loss": 1.7906,
      "step": 57376
    },
    {
      "epoch": 0.00020834502540105037,
      "grad_norm": 9334.434744535954,
      "learning_rate": 4.1757384532966165e-07,
      "loss": 1.7858,
      "step": 57408
    },
    {
      "epoch": 0.0002084611597518871,
      "grad_norm": 9040.889558002575,
      "learning_rate": 4.174573956894903e-07,
      "loss": 1.8008,
      "step": 57440
    },
    {
      "epoch": 0.0002085772941027238,
      "grad_norm": 9416.563598256002,
      "learning_rate": 4.1734104341863415e-07,
      "loss": 1.7907,
      "step": 57472
    },
    {
      "epoch": 0.0002086934284535605,
      "grad_norm": 8991.255863337446,
      "learning_rate": 4.1722478838147667e-07,
      "loss": 1.8037,
      "step": 57504
    },
    {
      "epoch": 0.0002088095628043972,
      "grad_norm": 9666.472469313716,
      "learning_rate": 4.171086304426655e-07,
      "loss": 1.7897,
      "step": 57536
    },
    {
      "epoch": 0.0002089256971552339,
      "grad_norm": 9165.441615110534,
      "learning_rate": 4.16992569467112e-07,
      "loss": 1.7893,
      "step": 57568
    },
    {
      "epoch": 0.00020904183150607063,
      "grad_norm": 9065.563744191533,
      "learning_rate": 4.168766053199905e-07,
      "loss": 1.768,
      "step": 57600
    },
    {
      "epoch": 0.00020915796585690733,
      "grad_norm": 10364.93058346268,
      "learning_rate": 4.167607378667376e-07,
      "loss": 1.7879,
      "step": 57632
    },
    {
      "epoch": 0.00020927410020774403,
      "grad_norm": 10467.854316907547,
      "learning_rate": 4.166449669730516e-07,
      "loss": 1.7835,
      "step": 57664
    },
    {
      "epoch": 0.00020939023455858073,
      "grad_norm": 8776.263213919692,
      "learning_rate": 4.165292925048919e-07,
      "loss": 1.7574,
      "step": 57696
    },
    {
      "epoch": 0.00020950636890941746,
      "grad_norm": 9818.179260942428,
      "learning_rate": 4.16413714328478e-07,
      "loss": 1.7723,
      "step": 57728
    },
    {
      "epoch": 0.00020962250326025416,
      "grad_norm": 10788.721332947664,
      "learning_rate": 4.162982323102893e-07,
      "loss": 1.7677,
      "step": 57760
    },
    {
      "epoch": 0.00020973863761109086,
      "grad_norm": 9663.332551454492,
      "learning_rate": 4.1618284631706424e-07,
      "loss": 1.7749,
      "step": 57792
    },
    {
      "epoch": 0.00020985477196192756,
      "grad_norm": 9624.777503921843,
      "learning_rate": 4.1606755621579966e-07,
      "loss": 1.8018,
      "step": 57824
    },
    {
      "epoch": 0.00020997090631276425,
      "grad_norm": 10778.87971915449,
      "learning_rate": 4.159523618737501e-07,
      "loss": 1.7971,
      "step": 57856
    },
    {
      "epoch": 0.00021008704066360098,
      "grad_norm": 8333.552183792935,
      "learning_rate": 4.158372631584273e-07,
      "loss": 1.7523,
      "step": 57888
    },
    {
      "epoch": 0.00021020317501443768,
      "grad_norm": 9204.551808752016,
      "learning_rate": 4.157258523440887e-07,
      "loss": 1.7533,
      "step": 57920
    },
    {
      "epoch": 0.00021031930936527438,
      "grad_norm": 9419.998726114563,
      "learning_rate": 4.1561094150769633e-07,
      "loss": 1.7694,
      "step": 57952
    },
    {
      "epoch": 0.00021043544371611108,
      "grad_norm": 11524.507798600336,
      "learning_rate": 4.154961259062146e-07,
      "loss": 1.7762,
      "step": 57984
    },
    {
      "epoch": 0.00021055157806694778,
      "grad_norm": 9254.50376843621,
      "learning_rate": 4.1538140540816947e-07,
      "loss": 1.7829,
      "step": 58016
    },
    {
      "epoch": 0.0002106677124177845,
      "grad_norm": 9976.710981079887,
      "learning_rate": 4.1526677988234083e-07,
      "loss": 1.7848,
      "step": 58048
    },
    {
      "epoch": 0.0002107838467686212,
      "grad_norm": 9656.372196637823,
      "learning_rate": 4.1515224919776177e-07,
      "loss": 1.788,
      "step": 58080
    },
    {
      "epoch": 0.0002108999811194579,
      "grad_norm": 8512.477782643547,
      "learning_rate": 4.1503781322371823e-07,
      "loss": 1.8013,
      "step": 58112
    },
    {
      "epoch": 0.0002110161154702946,
      "grad_norm": 9556.891335575601,
      "learning_rate": 4.1492347182974813e-07,
      "loss": 1.8201,
      "step": 58144
    },
    {
      "epoch": 0.00021113224982113134,
      "grad_norm": 9834.426673680577,
      "learning_rate": 4.14809224885641e-07,
      "loss": 1.8052,
      "step": 58176
    },
    {
      "epoch": 0.00021124838417196804,
      "grad_norm": 10509.25268513418,
      "learning_rate": 4.1469507226143703e-07,
      "loss": 1.8103,
      "step": 58208
    },
    {
      "epoch": 0.00021136451852280474,
      "grad_norm": 10035.919489513655,
      "learning_rate": 4.145810138274267e-07,
      "loss": 1.8116,
      "step": 58240
    },
    {
      "epoch": 0.00021148065287364144,
      "grad_norm": 10100.117326051219,
      "learning_rate": 4.1446704945415e-07,
      "loss": 1.801,
      "step": 58272
    },
    {
      "epoch": 0.00021159678722447814,
      "grad_norm": 10061.351400284158,
      "learning_rate": 4.1435317901239616e-07,
      "loss": 1.763,
      "step": 58304
    },
    {
      "epoch": 0.00021171292157531486,
      "grad_norm": 9360.28332904512,
      "learning_rate": 4.1423940237320233e-07,
      "loss": 1.7677,
      "step": 58336
    },
    {
      "epoch": 0.00021182905592615156,
      "grad_norm": 8533.91047527451,
      "learning_rate": 4.1412571940785383e-07,
      "loss": 1.7695,
      "step": 58368
    },
    {
      "epoch": 0.00021194519027698826,
      "grad_norm": 10120.707386344098,
      "learning_rate": 4.1401212998788286e-07,
      "loss": 1.7838,
      "step": 58400
    },
    {
      "epoch": 0.00021206132462782496,
      "grad_norm": 10249.449448628935,
      "learning_rate": 4.138986339850682e-07,
      "loss": 1.8049,
      "step": 58432
    },
    {
      "epoch": 0.0002121774589786617,
      "grad_norm": 10416.306543108261,
      "learning_rate": 4.137852312714346e-07,
      "loss": 1.7724,
      "step": 58464
    },
    {
      "epoch": 0.0002122935933294984,
      "grad_norm": 8124.00160019679,
      "learning_rate": 4.1367192171925205e-07,
      "loss": 1.7633,
      "step": 58496
    },
    {
      "epoch": 0.0002124097276803351,
      "grad_norm": 9658.495535019934,
      "learning_rate": 4.1355870520103535e-07,
      "loss": 1.7632,
      "step": 58528
    },
    {
      "epoch": 0.0002125258620311718,
      "grad_norm": 9559.419752265301,
      "learning_rate": 4.1344558158954324e-07,
      "loss": 1.7779,
      "step": 58560
    },
    {
      "epoch": 0.0002126419963820085,
      "grad_norm": 9465.870905521584,
      "learning_rate": 4.1333255075777806e-07,
      "loss": 1.7662,
      "step": 58592
    },
    {
      "epoch": 0.00021275813073284522,
      "grad_norm": 8927.376098272101,
      "learning_rate": 4.132196125789852e-07,
      "loss": 1.7521,
      "step": 58624
    },
    {
      "epoch": 0.00021287426508368192,
      "grad_norm": 9817.579844340456,
      "learning_rate": 4.1310676692665197e-07,
      "loss": 1.765,
      "step": 58656
    },
    {
      "epoch": 0.00021299039943451862,
      "grad_norm": 10688.626104415853,
      "learning_rate": 4.1299401367450784e-07,
      "loss": 1.763,
      "step": 58688
    },
    {
      "epoch": 0.00021310653378535532,
      "grad_norm": 9716.226222150244,
      "learning_rate": 4.128813526965231e-07,
      "loss": 1.7996,
      "step": 58720
    },
    {
      "epoch": 0.00021322266813619204,
      "grad_norm": 10378.562135479075,
      "learning_rate": 4.127687838669086e-07,
      "loss": 1.8044,
      "step": 58752
    },
    {
      "epoch": 0.00021333880248702874,
      "grad_norm": 8820.612450391412,
      "learning_rate": 4.126563070601153e-07,
      "loss": 1.8113,
      "step": 58784
    },
    {
      "epoch": 0.00021345493683786544,
      "grad_norm": 10317.290826568766,
      "learning_rate": 4.125439221508333e-07,
      "loss": 1.8017,
      "step": 58816
    },
    {
      "epoch": 0.00021357107118870214,
      "grad_norm": 9364.99813133991,
      "learning_rate": 4.124316290139916e-07,
      "loss": 1.7998,
      "step": 58848
    },
    {
      "epoch": 0.00021368720553953884,
      "grad_norm": 9218.523417554463,
      "learning_rate": 4.123194275247572e-07,
      "loss": 1.7905,
      "step": 58880
    },
    {
      "epoch": 0.00021380333989037557,
      "grad_norm": 9630.215054711914,
      "learning_rate": 4.122073175585351e-07,
      "loss": 1.7799,
      "step": 58912
    },
    {
      "epoch": 0.00021391947424121227,
      "grad_norm": 8633.92471590991,
      "learning_rate": 4.120987981889611e-07,
      "loss": 1.7843,
      "step": 58944
    },
    {
      "epoch": 0.00021403560859204897,
      "grad_norm": 10138.615881864744,
      "learning_rate": 4.119868680454722e-07,
      "loss": 1.7891,
      "step": 58976
    },
    {
      "epoch": 0.00021415174294288567,
      "grad_norm": 8421.689497957046,
      "learning_rate": 4.1187502905649723e-07,
      "loss": 1.8008,
      "step": 59008
    },
    {
      "epoch": 0.0002142678772937224,
      "grad_norm": 9488.060497277616,
      "learning_rate": 4.117632810983782e-07,
      "loss": 1.7972,
      "step": 59040
    },
    {
      "epoch": 0.0002143840116445591,
      "grad_norm": 8933.260435025948,
      "learning_rate": 4.1165162404769174e-07,
      "loss": 1.7846,
      "step": 59072
    },
    {
      "epoch": 0.0002145001459953958,
      "grad_norm": 9723.82877265946,
      "learning_rate": 4.115400577812488e-07,
      "loss": 1.7609,
      "step": 59104
    },
    {
      "epoch": 0.0002146162803462325,
      "grad_norm": 9014.92251769254,
      "learning_rate": 4.114285821760937e-07,
      "loss": 1.768,
      "step": 59136
    },
    {
      "epoch": 0.0002147324146970692,
      "grad_norm": 10539.40415773112,
      "learning_rate": 4.11317197109504e-07,
      "loss": 1.7816,
      "step": 59168
    },
    {
      "epoch": 0.00021484854904790592,
      "grad_norm": 9178.453791352878,
      "learning_rate": 4.1120590245898947e-07,
      "loss": 1.7698,
      "step": 59200
    },
    {
      "epoch": 0.00021496468339874262,
      "grad_norm": 9326.363921700675,
      "learning_rate": 4.1109469810229194e-07,
      "loss": 1.7692,
      "step": 59232
    },
    {
      "epoch": 0.00021508081774957932,
      "grad_norm": 9524.933385593833,
      "learning_rate": 4.109835839173844e-07,
      "loss": 1.7907,
      "step": 59264
    },
    {
      "epoch": 0.00021519695210041602,
      "grad_norm": 10839.350165023732,
      "learning_rate": 4.108725597824708e-07,
      "loss": 1.7964,
      "step": 59296
    },
    {
      "epoch": 0.00021531308645125275,
      "grad_norm": 9938.20446559639,
      "learning_rate": 4.107616255759851e-07,
      "loss": 1.7886,
      "step": 59328
    },
    {
      "epoch": 0.00021542922080208945,
      "grad_norm": 9674.36974691375,
      "learning_rate": 4.106507811765909e-07,
      "loss": 1.7801,
      "step": 59360
    },
    {
      "epoch": 0.00021554535515292615,
      "grad_norm": 11475.70651419772,
      "learning_rate": 4.105400264631811e-07,
      "loss": 1.7856,
      "step": 59392
    },
    {
      "epoch": 0.00021566148950376285,
      "grad_norm": 9673.96020252306,
      "learning_rate": 4.104293613148768e-07,
      "loss": 1.7742,
      "step": 59424
    },
    {
      "epoch": 0.00021577762385459955,
      "grad_norm": 10368.027584839847,
      "learning_rate": 4.103187856110272e-07,
      "loss": 1.7651,
      "step": 59456
    },
    {
      "epoch": 0.00021589375820543628,
      "grad_norm": 9049.031881919746,
      "learning_rate": 4.1020829923120907e-07,
      "loss": 1.7782,
      "step": 59488
    },
    {
      "epoch": 0.00021600989255627298,
      "grad_norm": 10025.143190997323,
      "learning_rate": 4.100979020552258e-07,
      "loss": 1.7911,
      "step": 59520
    },
    {
      "epoch": 0.00021612602690710968,
      "grad_norm": 9043.1185992444,
      "learning_rate": 4.099875939631072e-07,
      "loss": 1.7913,
      "step": 59552
    },
    {
      "epoch": 0.00021624216125794638,
      "grad_norm": 10430.360492331989,
      "learning_rate": 4.0987737483510886e-07,
      "loss": 1.7996,
      "step": 59584
    },
    {
      "epoch": 0.0002163582956087831,
      "grad_norm": 8857.870398690648,
      "learning_rate": 4.097672445517116e-07,
      "loss": 1.7884,
      "step": 59616
    },
    {
      "epoch": 0.0002164744299596198,
      "grad_norm": 10020.25209263719,
      "learning_rate": 4.096572029936208e-07,
      "loss": 1.7905,
      "step": 59648
    },
    {
      "epoch": 0.0002165905643104565,
      "grad_norm": 8976.241306916832,
      "learning_rate": 4.095472500417661e-07,
      "loss": 1.7991,
      "step": 59680
    },
    {
      "epoch": 0.0002167066986612932,
      "grad_norm": 10150.181279169354,
      "learning_rate": 4.094373855773008e-07,
      "loss": 1.7825,
      "step": 59712
    },
    {
      "epoch": 0.0002168228330121299,
      "grad_norm": 8031.345341846532,
      "learning_rate": 4.0932760948160095e-07,
      "loss": 1.7775,
      "step": 59744
    },
    {
      "epoch": 0.00021693896736296663,
      "grad_norm": 8456.756825166489,
      "learning_rate": 4.092179216362654e-07,
      "loss": 1.7892,
      "step": 59776
    },
    {
      "epoch": 0.00021705510171380333,
      "grad_norm": 10035.245986023461,
      "learning_rate": 4.091083219231149e-07,
      "loss": 1.753,
      "step": 59808
    },
    {
      "epoch": 0.00021717123606464003,
      "grad_norm": 10899.0560141693,
      "learning_rate": 4.089988102241916e-07,
      "loss": 1.7673,
      "step": 59840
    },
    {
      "epoch": 0.00021728737041547673,
      "grad_norm": 10136.808768049243,
      "learning_rate": 4.088893864217586e-07,
      "loss": 1.7647,
      "step": 59872
    },
    {
      "epoch": 0.00021740350476631346,
      "grad_norm": 10146.152374176134,
      "learning_rate": 4.087800503982993e-07,
      "loss": 1.7778,
      "step": 59904
    },
    {
      "epoch": 0.00021751963911715016,
      "grad_norm": 8312.835256397182,
      "learning_rate": 4.086742147220762e-07,
      "loss": 1.7838,
      "step": 59936
    },
    {
      "epoch": 0.00021763577346798686,
      "grad_norm": 10902.083929231145,
      "learning_rate": 4.0856505117089494e-07,
      "loss": 1.7982,
      "step": 59968
    },
    {
      "epoch": 0.00021775190781882356,
      "grad_norm": 8901.249912231428,
      "learning_rate": 4.084559750511033e-07,
      "loss": 1.8098,
      "step": 60000
    },
    {
      "epoch": 0.00021786804216966026,
      "grad_norm": 8508.262102215704,
      "learning_rate": 4.0834698624605423e-07,
      "loss": 1.7944,
      "step": 60032
    },
    {
      "epoch": 0.00021798417652049698,
      "grad_norm": 8484.970830827882,
      "learning_rate": 4.082380846393183e-07,
      "loss": 1.7787,
      "step": 60064
    },
    {
      "epoch": 0.00021810031087133368,
      "grad_norm": 10710.223153604224,
      "learning_rate": 4.081292701146833e-07,
      "loss": 1.7895,
      "step": 60096
    },
    {
      "epoch": 0.00021821644522217038,
      "grad_norm": 10188.914564368475,
      "learning_rate": 4.08020542556154e-07,
      "loss": 1.7722,
      "step": 60128
    },
    {
      "epoch": 0.00021833257957300708,
      "grad_norm": 9626.812244974968,
      "learning_rate": 4.079119018479511e-07,
      "loss": 1.7702,
      "step": 60160
    },
    {
      "epoch": 0.0002184487139238438,
      "grad_norm": 9382.280000085268,
      "learning_rate": 4.07803347874511e-07,
      "loss": 1.7674,
      "step": 60192
    },
    {
      "epoch": 0.0002185648482746805,
      "grad_norm": 12090.624136081646,
      "learning_rate": 4.076948805204855e-07,
      "loss": 1.7656,
      "step": 60224
    },
    {
      "epoch": 0.0002186809826255172,
      "grad_norm": 10724.236103331556,
      "learning_rate": 4.075864996707407e-07,
      "loss": 1.7839,
      "step": 60256
    },
    {
      "epoch": 0.0002187971169763539,
      "grad_norm": 8483.079216888169,
      "learning_rate": 4.074782052103571e-07,
      "loss": 1.7955,
      "step": 60288
    },
    {
      "epoch": 0.0002189132513271906,
      "grad_norm": 7685.41788063603,
      "learning_rate": 4.073699970246286e-07,
      "loss": 1.7929,
      "step": 60320
    },
    {
      "epoch": 0.00021902938567802734,
      "grad_norm": 10760.387818289822,
      "learning_rate": 4.072618749990626e-07,
      "loss": 1.777,
      "step": 60352
    },
    {
      "epoch": 0.00021914552002886404,
      "grad_norm": 10380.206597173294,
      "learning_rate": 4.071538390193787e-07,
      "loss": 1.7886,
      "step": 60384
    },
    {
      "epoch": 0.00021926165437970074,
      "grad_norm": 10168.74476029367,
      "learning_rate": 4.0704588897150867e-07,
      "loss": 1.7996,
      "step": 60416
    },
    {
      "epoch": 0.00021937778873053744,
      "grad_norm": 10242.838864299292,
      "learning_rate": 4.069380247415961e-07,
      "loss": 1.7896,
      "step": 60448
    },
    {
      "epoch": 0.00021949392308137416,
      "grad_norm": 9700.408651185784,
      "learning_rate": 4.068302462159954e-07,
      "loss": 1.7857,
      "step": 60480
    },
    {
      "epoch": 0.00021961005743221086,
      "grad_norm": 9122.604014205594,
      "learning_rate": 4.0672255328127177e-07,
      "loss": 1.7924,
      "step": 60512
    },
    {
      "epoch": 0.00021972619178304756,
      "grad_norm": 9064.027471273463,
      "learning_rate": 4.0661494582420033e-07,
      "loss": 1.7864,
      "step": 60544
    },
    {
      "epoch": 0.00021984232613388426,
      "grad_norm": 10068.218213765533,
      "learning_rate": 4.0650742373176587e-07,
      "loss": 1.788,
      "step": 60576
    },
    {
      "epoch": 0.00021995846048472096,
      "grad_norm": 10025.68710862253,
      "learning_rate": 4.063999868911623e-07,
      "loss": 1.7876,
      "step": 60608
    },
    {
      "epoch": 0.0002200745948355577,
      "grad_norm": 8825.5915382483,
      "learning_rate": 4.062926351897919e-07,
      "loss": 1.7565,
      "step": 60640
    },
    {
      "epoch": 0.0002201907291863944,
      "grad_norm": 10000.429890759697,
      "learning_rate": 4.0618536851526543e-07,
      "loss": 1.7677,
      "step": 60672
    },
    {
      "epoch": 0.0002203068635372311,
      "grad_norm": 8471.349479274244,
      "learning_rate": 4.0607818675540085e-07,
      "loss": 1.7784,
      "step": 60704
    },
    {
      "epoch": 0.0002204229978880678,
      "grad_norm": 11235.051223737253,
      "learning_rate": 4.0597108979822337e-07,
      "loss": 1.7645,
      "step": 60736
    },
    {
      "epoch": 0.0002205391322389045,
      "grad_norm": 8485.01019445469,
      "learning_rate": 4.05864077531965e-07,
      "loss": 1.783,
      "step": 60768
    },
    {
      "epoch": 0.00022065526658974122,
      "grad_norm": 10003.233977069616,
      "learning_rate": 4.057571498450636e-07,
      "loss": 1.7922,
      "step": 60800
    },
    {
      "epoch": 0.00022077140094057792,
      "grad_norm": 9529.686248770207,
      "learning_rate": 4.056503066261628e-07,
      "loss": 1.7822,
      "step": 60832
    },
    {
      "epoch": 0.00022088753529141462,
      "grad_norm": 8586.909455677287,
      "learning_rate": 4.0554354776411125e-07,
      "loss": 1.7783,
      "step": 60864
    },
    {
      "epoch": 0.00022100366964225132,
      "grad_norm": 9561.723066477087,
      "learning_rate": 4.054368731479625e-07,
      "loss": 1.7829,
      "step": 60896
    },
    {
      "epoch": 0.00022111980399308804,
      "grad_norm": 11118.076002618438,
      "learning_rate": 4.053336123470733e-07,
      "loss": 1.7773,
      "step": 60928
    },
    {
      "epoch": 0.00022123593834392474,
      "grad_norm": 9409.856747049873,
      "learning_rate": 4.0522710326660824e-07,
      "loss": 1.7645,
      "step": 60960
    },
    {
      "epoch": 0.00022135207269476144,
      "grad_norm": 9330.464511480659,
      "learning_rate": 4.0512067810387423e-07,
      "loss": 1.787,
      "step": 60992
    },
    {
      "epoch": 0.00022146820704559814,
      "grad_norm": 12271.849575349268,
      "learning_rate": 4.0501433674873213e-07,
      "loss": 1.7983,
      "step": 61024
    },
    {
      "epoch": 0.00022158434139643484,
      "grad_norm": 7914.588555319853,
      "learning_rate": 4.049080790912451e-07,
      "loss": 1.7866,
      "step": 61056
    },
    {
      "epoch": 0.00022170047574727157,
      "grad_norm": 10753.715078985495,
      "learning_rate": 4.048019050216781e-07,
      "loss": 1.7912,
      "step": 61088
    },
    {
      "epoch": 0.00022181661009810827,
      "grad_norm": 9851.926613612182,
      "learning_rate": 4.046958144304975e-07,
      "loss": 1.786,
      "step": 61120
    },
    {
      "epoch": 0.00022193274444894497,
      "grad_norm": 9837.783998441924,
      "learning_rate": 4.0458980720837025e-07,
      "loss": 1.8002,
      "step": 61152
    },
    {
      "epoch": 0.00022204887879978167,
      "grad_norm": 8680.971950190831,
      "learning_rate": 4.0448388324616396e-07,
      "loss": 1.8114,
      "step": 61184
    },
    {
      "epoch": 0.0002221650131506184,
      "grad_norm": 9891.857257360723,
      "learning_rate": 4.0437804243494603e-07,
      "loss": 1.8085,
      "step": 61216
    },
    {
      "epoch": 0.0002222811475014551,
      "grad_norm": 9868.956581118391,
      "learning_rate": 4.0427228466598313e-07,
      "loss": 1.7817,
      "step": 61248
    },
    {
      "epoch": 0.0002223972818522918,
      "grad_norm": 9354.119627201697,
      "learning_rate": 4.041666098307412e-07,
      "loss": 1.794,
      "step": 61280
    },
    {
      "epoch": 0.0002225134162031285,
      "grad_norm": 9771.887944506936,
      "learning_rate": 4.040610178208843e-07,
      "loss": 1.7722,
      "step": 61312
    },
    {
      "epoch": 0.0002226295505539652,
      "grad_norm": 9837.26771009105,
      "learning_rate": 4.0395550852827484e-07,
      "loss": 1.7566,
      "step": 61344
    },
    {
      "epoch": 0.00022274568490480192,
      "grad_norm": 9487.112837950228,
      "learning_rate": 4.0385008184497263e-07,
      "loss": 1.7626,
      "step": 61376
    },
    {
      "epoch": 0.00022286181925563862,
      "grad_norm": 9529.268807206563,
      "learning_rate": 4.037447376632346e-07,
      "loss": 1.7608,
      "step": 61408
    },
    {
      "epoch": 0.00022297795360647532,
      "grad_norm": 9942.245621588716,
      "learning_rate": 4.036394758755142e-07,
      "loss": 1.7634,
      "step": 61440
    },
    {
      "epoch": 0.00022309408795731202,
      "grad_norm": 9357.247565390157,
      "learning_rate": 4.0353429637446123e-07,
      "loss": 1.7556,
      "step": 61472
    },
    {
      "epoch": 0.00022321022230814875,
      "grad_norm": 8678.906843606515,
      "learning_rate": 4.034291990529212e-07,
      "loss": 1.7751,
      "step": 61504
    },
    {
      "epoch": 0.00022332635665898545,
      "grad_norm": 9482.495241232658,
      "learning_rate": 4.0332418380393466e-07,
      "loss": 1.7902,
      "step": 61536
    },
    {
      "epoch": 0.00022344249100982215,
      "grad_norm": 9221.540977515635,
      "learning_rate": 4.032192505207372e-07,
      "loss": 1.776,
      "step": 61568
    },
    {
      "epoch": 0.00022355862536065885,
      "grad_norm": 9988.18131593535,
      "learning_rate": 4.031143990967585e-07,
      "loss": 1.7813,
      "step": 61600
    },
    {
      "epoch": 0.00022367475971149555,
      "grad_norm": 9661.916787056283,
      "learning_rate": 4.030096294256224e-07,
      "loss": 1.7726,
      "step": 61632
    },
    {
      "epoch": 0.00022379089406233228,
      "grad_norm": 9468.045310411226,
      "learning_rate": 4.02904941401146e-07,
      "loss": 1.7638,
      "step": 61664
    },
    {
      "epoch": 0.00022390702841316898,
      "grad_norm": 10121.144006484641,
      "learning_rate": 4.028003349173393e-07,
      "loss": 1.7713,
      "step": 61696
    },
    {
      "epoch": 0.00022402316276400568,
      "grad_norm": 8679.36345592233,
      "learning_rate": 4.0269580986840514e-07,
      "loss": 1.782,
      "step": 61728
    },
    {
      "epoch": 0.00022413929711484238,
      "grad_norm": 9972.807227656614,
      "learning_rate": 4.0259136614873797e-07,
      "loss": 1.8038,
      "step": 61760
    },
    {
      "epoch": 0.0002242554314656791,
      "grad_norm": 10688.428790051417,
      "learning_rate": 4.0248700365292437e-07,
      "loss": 1.8214,
      "step": 61792
    },
    {
      "epoch": 0.0002243715658165158,
      "grad_norm": 9201.142211703936,
      "learning_rate": 4.023827222757418e-07,
      "loss": 1.8136,
      "step": 61824
    },
    {
      "epoch": 0.0002244877001673525,
      "grad_norm": 9533.704526573078,
      "learning_rate": 4.0227852191215846e-07,
      "loss": 1.8057,
      "step": 61856
    },
    {
      "epoch": 0.0002246038345181892,
      "grad_norm": 10719.159388683425,
      "learning_rate": 4.0217440245733293e-07,
      "loss": 1.7895,
      "step": 61888
    },
    {
      "epoch": 0.0002247199688690259,
      "grad_norm": 9625.145193710066,
      "learning_rate": 4.020703638066137e-07,
      "loss": 1.7769,
      "step": 61920
    },
    {
      "epoch": 0.00022483610321986263,
      "grad_norm": 9830.383919257682,
      "learning_rate": 4.0196965332101803e-07,
      "loss": 1.7953,
      "step": 61952
    },
    {
      "epoch": 0.00022495223757069933,
      "grad_norm": 10129.81026475817,
      "learning_rate": 4.018657734482848e-07,
      "loss": 1.7879,
      "step": 61984
    },
    {
      "epoch": 0.00022506837192153603,
      "grad_norm": 9907.047895311702,
      "learning_rate": 4.0176197407008756e-07,
      "loss": 1.7825,
      "step": 62016
    },
    {
      "epoch": 0.00022518450627237273,
      "grad_norm": 9797.062621010442,
      "learning_rate": 4.0165825508252375e-07,
      "loss": 1.7752,
      "step": 62048
    },
    {
      "epoch": 0.00022530064062320946,
      "grad_norm": 9141.656305068573,
      "learning_rate": 4.015546163818787e-07,
      "loss": 1.7576,
      "step": 62080
    },
    {
      "epoch": 0.00022541677497404616,
      "grad_norm": 8248.68231901314,
      "learning_rate": 4.0145105786462477e-07,
      "loss": 1.7735,
      "step": 62112
    },
    {
      "epoch": 0.00022553290932488286,
      "grad_norm": 9628.356142145969,
      "learning_rate": 4.0134757942742117e-07,
      "loss": 1.7937,
      "step": 62144
    },
    {
      "epoch": 0.00022564904367571956,
      "grad_norm": 9312.730426679385,
      "learning_rate": 4.012441809671135e-07,
      "loss": 1.7557,
      "step": 62176
    },
    {
      "epoch": 0.00022576517802655626,
      "grad_norm": 8879.2340885912,
      "learning_rate": 4.011408623807334e-07,
      "loss": 1.7596,
      "step": 62208
    },
    {
      "epoch": 0.00022588131237739298,
      "grad_norm": 9808.55891555941,
      "learning_rate": 4.0103762356549773e-07,
      "loss": 1.7619,
      "step": 62240
    },
    {
      "epoch": 0.00022599744672822968,
      "grad_norm": 10613.52985580198,
      "learning_rate": 4.009344644188087e-07,
      "loss": 1.7665,
      "step": 62272
    },
    {
      "epoch": 0.00022611358107906638,
      "grad_norm": 9714.114782109587,
      "learning_rate": 4.0083138483825304e-07,
      "loss": 1.7753,
      "step": 62304
    },
    {
      "epoch": 0.00022622971542990308,
      "grad_norm": 9014.903770978368,
      "learning_rate": 4.007283847216017e-07,
      "loss": 1.7883,
      "step": 62336
    },
    {
      "epoch": 0.0002263458497807398,
      "grad_norm": 9922.855032701023,
      "learning_rate": 4.006254639668094e-07,
      "loss": 1.7907,
      "step": 62368
    },
    {
      "epoch": 0.0002264619841315765,
      "grad_norm": 9390.30489387858,
      "learning_rate": 4.005226224720144e-07,
      "loss": 1.774,
      "step": 62400
    },
    {
      "epoch": 0.0002265781184824132,
      "grad_norm": 11176.80741535793,
      "learning_rate": 4.004198601355376e-07,
      "loss": 1.78,
      "step": 62432
    },
    {
      "epoch": 0.0002266942528332499,
      "grad_norm": 10303.86655581292,
      "learning_rate": 4.0031717685588264e-07,
      "loss": 1.7756,
      "step": 62464
    },
    {
      "epoch": 0.0002268103871840866,
      "grad_norm": 9424.48460129253,
      "learning_rate": 4.002145725317353e-07,
      "loss": 1.7645,
      "step": 62496
    },
    {
      "epoch": 0.00022692652153492334,
      "grad_norm": 9228.667292735176,
      "learning_rate": 4.0011204706196275e-07,
      "loss": 1.7812,
      "step": 62528
    },
    {
      "epoch": 0.00022704265588576004,
      "grad_norm": 8184.30778991113,
      "learning_rate": 4.0000960034561387e-07,
      "loss": 1.8096,
      "step": 62560
    },
    {
      "epoch": 0.00022715879023659674,
      "grad_norm": 9269.56600925847,
      "learning_rate": 3.999072322819179e-07,
      "loss": 1.7923,
      "step": 62592
    },
    {
      "epoch": 0.00022727492458743344,
      "grad_norm": 9219.582420044848,
      "learning_rate": 3.9980494277028493e-07,
      "loss": 1.7865,
      "step": 62624
    },
    {
      "epoch": 0.00022739105893827016,
      "grad_norm": 8693.983781903438,
      "learning_rate": 3.9970273171030475e-07,
      "loss": 1.7919,
      "step": 62656
    },
    {
      "epoch": 0.00022750719328910686,
      "grad_norm": 10035.434420093632,
      "learning_rate": 3.9960059900174685e-07,
      "loss": 1.7956,
      "step": 62688
    },
    {
      "epoch": 0.00022762332763994356,
      "grad_norm": 9442.157910138974,
      "learning_rate": 3.994985445445601e-07,
      "loss": 1.8204,
      "step": 62720
    },
    {
      "epoch": 0.00022773946199078026,
      "grad_norm": 9749.252381593165,
      "learning_rate": 3.9939656823887174e-07,
      "loss": 1.8026,
      "step": 62752
    },
    {
      "epoch": 0.00022785559634161696,
      "grad_norm": 9840.26351273176,
      "learning_rate": 3.992946699849878e-07,
      "loss": 1.7797,
      "step": 62784
    },
    {
      "epoch": 0.0002279717306924537,
      "grad_norm": 9354.835861734828,
      "learning_rate": 3.99192849683392e-07,
      "loss": 1.7684,
      "step": 62816
    },
    {
      "epoch": 0.0002280878650432904,
      "grad_norm": 9909.055555399818,
      "learning_rate": 3.9909110723474565e-07,
      "loss": 1.7584,
      "step": 62848
    },
    {
      "epoch": 0.0002282039993941271,
      "grad_norm": 8974.23266914782,
      "learning_rate": 3.989894425398872e-07,
      "loss": 1.7683,
      "step": 62880
    },
    {
      "epoch": 0.0002283201337449638,
      "grad_norm": 9725.237272169763,
      "learning_rate": 3.9888785549983197e-07,
      "loss": 1.7658,
      "step": 62912
    },
    {
      "epoch": 0.00022843626809580052,
      "grad_norm": 8818.474017651806,
      "learning_rate": 3.9878951701418524e-07,
      "loss": 1.7914,
      "step": 62944
    },
    {
      "epoch": 0.00022855240244663722,
      "grad_norm": 9683.601602709605,
      "learning_rate": 3.986880825684355e-07,
      "loss": 1.7779,
      "step": 62976
    },
    {
      "epoch": 0.00022866853679747392,
      "grad_norm": 8489.8077716754,
      "learning_rate": 3.9858672548466506e-07,
      "loss": 1.773,
      "step": 63008
    },
    {
      "epoch": 0.00022878467114831062,
      "grad_norm": 10012.898980814696,
      "learning_rate": 3.9848544566458645e-07,
      "loss": 1.7815,
      "step": 63040
    },
    {
      "epoch": 0.00022890080549914732,
      "grad_norm": 8930.867147147583,
      "learning_rate": 3.9838424301008714e-07,
      "loss": 1.7871,
      "step": 63072
    },
    {
      "epoch": 0.00022901693984998404,
      "grad_norm": 11436.111926699565,
      "learning_rate": 3.982831174232289e-07,
      "loss": 1.7689,
      "step": 63104
    },
    {
      "epoch": 0.00022913307420082074,
      "grad_norm": 10857.062954593199,
      "learning_rate": 3.9818206880624717e-07,
      "loss": 1.7762,
      "step": 63136
    },
    {
      "epoch": 0.00022924920855165744,
      "grad_norm": 9073.180258321776,
      "learning_rate": 3.980810970615513e-07,
      "loss": 1.7516,
      "step": 63168
    },
    {
      "epoch": 0.00022936534290249414,
      "grad_norm": 10718.852643823404,
      "learning_rate": 3.979802020917235e-07,
      "loss": 1.755,
      "step": 63200
    },
    {
      "epoch": 0.00022948147725333087,
      "grad_norm": 10816.9124984905,
      "learning_rate": 3.9787938379951895e-07,
      "loss": 1.7486,
      "step": 63232
    },
    {
      "epoch": 0.00022959761160416757,
      "grad_norm": 10695.592924190787,
      "learning_rate": 3.9777864208786506e-07,
      "loss": 1.771,
      "step": 63264
    },
    {
      "epoch": 0.00022971374595500427,
      "grad_norm": 9636.644748043793,
      "learning_rate": 3.976779768598611e-07,
      "loss": 1.7863,
      "step": 63296
    },
    {
      "epoch": 0.00022982988030584097,
      "grad_norm": 9844.024786640877,
      "learning_rate": 3.975773880187782e-07,
      "loss": 1.8051,
      "step": 63328
    },
    {
      "epoch": 0.00022994601465667767,
      "grad_norm": 11812.930965683327,
      "learning_rate": 3.9747687546805837e-07,
      "loss": 1.8062,
      "step": 63360
    },
    {
      "epoch": 0.0002300621490075144,
      "grad_norm": 9525.365084866826,
      "learning_rate": 3.9737643911131456e-07,
      "loss": 1.7977,
      "step": 63392
    },
    {
      "epoch": 0.0002301782833583511,
      "grad_norm": 9952.075662895655,
      "learning_rate": 3.972760788523301e-07,
      "loss": 1.7922,
      "step": 63424
    },
    {
      "epoch": 0.0002302944177091878,
      "grad_norm": 9894.673213401238,
      "learning_rate": 3.971757945950585e-07,
      "loss": 1.7983,
      "step": 63456
    },
    {
      "epoch": 0.0002304105520600245,
      "grad_norm": 10008.92801452783,
      "learning_rate": 3.970755862436225e-07,
      "loss": 1.8054,
      "step": 63488
    },
    {
      "epoch": 0.0002305266864108612,
      "grad_norm": 8710.285873609431,
      "learning_rate": 3.9697545370231437e-07,
      "loss": 1.8065,
      "step": 63520
    },
    {
      "epoch": 0.00023064282076169792,
      "grad_norm": 10027.965895434627,
      "learning_rate": 3.9687539687559534e-07,
      "loss": 1.8106,
      "step": 63552
    },
    {
      "epoch": 0.00023075895511253462,
      "grad_norm": 10836.861999674997,
      "learning_rate": 3.9677541566809483e-07,
      "loss": 1.7872,
      "step": 63584
    },
    {
      "epoch": 0.00023087508946337132,
      "grad_norm": 10445.846255809052,
      "learning_rate": 3.9667550998461057e-07,
      "loss": 1.7548,
      "step": 63616
    },
    {
      "epoch": 0.00023099122381420802,
      "grad_norm": 10476.778226153307,
      "learning_rate": 3.965756797301079e-07,
      "loss": 1.7681,
      "step": 63648
    },
    {
      "epoch": 0.00023110735816504475,
      "grad_norm": 8022.793528441324,
      "learning_rate": 3.964759248097195e-07,
      "loss": 1.7672,
      "step": 63680
    },
    {
      "epoch": 0.00023122349251588145,
      "grad_norm": 11067.394092558556,
      "learning_rate": 3.963762451287451e-07,
      "loss": 1.7691,
      "step": 63712
    },
    {
      "epoch": 0.00023133962686671815,
      "grad_norm": 9205.284460569375,
      "learning_rate": 3.962766405926509e-07,
      "loss": 1.7644,
      "step": 63744
    },
    {
      "epoch": 0.00023145576121755485,
      "grad_norm": 9605.512688034929,
      "learning_rate": 3.9617711110706935e-07,
      "loss": 1.754,
      "step": 63776
    },
    {
      "epoch": 0.00023157189556839155,
      "grad_norm": 8672.046355964663,
      "learning_rate": 3.960776565777987e-07,
      "loss": 1.7671,
      "step": 63808
    },
    {
      "epoch": 0.00023168802991922828,
      "grad_norm": 12007.652726490718,
      "learning_rate": 3.9597827691080275e-07,
      "loss": 1.7776,
      "step": 63840
    },
    {
      "epoch": 0.00023180416427006498,
      "grad_norm": 11108.77040900567,
      "learning_rate": 3.9587897201221024e-07,
      "loss": 1.7915,
      "step": 63872
    },
    {
      "epoch": 0.00023192029862090168,
      "grad_norm": 9535.79624362853,
      "learning_rate": 3.9577974178831475e-07,
      "loss": 1.7581,
      "step": 63904
    },
    {
      "epoch": 0.00023203643297173838,
      "grad_norm": 8630.133486800769,
      "learning_rate": 3.956836836314237e-07,
      "loss": 1.7602,
      "step": 63936
    },
    {
      "epoch": 0.0002321525673225751,
      "grad_norm": 10383.768680012088,
      "learning_rate": 3.9558460015012845e-07,
      "loss": 1.7718,
      "step": 63968
    },
    {
      "epoch": 0.0002322687016734118,
      "grad_norm": 10372.021982236636,
      "learning_rate": 3.954855910663057e-07,
      "loss": 1.7656,
      "step": 64000
    },
    {
      "epoch": 0.0002323848360242485,
      "grad_norm": 9027.782895041284,
      "learning_rate": 3.95386656286899e-07,
      "loss": 1.7748,
      "step": 64032
    },
    {
      "epoch": 0.0002325009703750852,
      "grad_norm": 9393.168794395211,
      "learning_rate": 3.952877957190148e-07,
      "loss": 1.7845,
      "step": 64064
    },
    {
      "epoch": 0.0002326171047259219,
      "grad_norm": 9786.010014301028,
      "learning_rate": 3.95189009269922e-07,
      "loss": 1.8107,
      "step": 64096
    },
    {
      "epoch": 0.00023273323907675863,
      "grad_norm": 9558.5511454404,
      "learning_rate": 3.950902968470517e-07,
      "loss": 1.8061,
      "step": 64128
    },
    {
      "epoch": 0.00023284937342759533,
      "grad_norm": 9880.326614034579,
      "learning_rate": 3.9499165835799667e-07,
      "loss": 1.7963,
      "step": 64160
    },
    {
      "epoch": 0.00023296550777843203,
      "grad_norm": 9726.91009519467,
      "learning_rate": 3.948930937105112e-07,
      "loss": 1.7918,
      "step": 64192
    },
    {
      "epoch": 0.00023308164212926873,
      "grad_norm": 9166.317581231844,
      "learning_rate": 3.9479460281251055e-07,
      "loss": 1.7957,
      "step": 64224
    },
    {
      "epoch": 0.00023319777648010546,
      "grad_norm": 9086.029495879924,
      "learning_rate": 3.9469618557207074e-07,
      "loss": 1.8009,
      "step": 64256
    },
    {
      "epoch": 0.00023331391083094216,
      "grad_norm": 8367.524723596578,
      "learning_rate": 3.94597841897428e-07,
      "loss": 1.8105,
      "step": 64288
    },
    {
      "epoch": 0.00023343004518177886,
      "grad_norm": 8966.659021062416,
      "learning_rate": 3.9449957169697876e-07,
      "loss": 1.7915,
      "step": 64320
    },
    {
      "epoch": 0.00023354617953261556,
      "grad_norm": 9598.091893704706,
      "learning_rate": 3.9440137487927885e-07,
      "loss": 1.7604,
      "step": 64352
    },
    {
      "epoch": 0.00023366231388345226,
      "grad_norm": 9741.357810900901,
      "learning_rate": 3.9430325135304335e-07,
      "loss": 1.7695,
      "step": 64384
    },
    {
      "epoch": 0.00023377844823428898,
      "grad_norm": 9400.675933144383,
      "learning_rate": 3.942052010271464e-07,
      "loss": 1.7613,
      "step": 64416
    },
    {
      "epoch": 0.00023389458258512568,
      "grad_norm": 9329.30201033282,
      "learning_rate": 3.9410722381062055e-07,
      "loss": 1.7804,
      "step": 64448
    },
    {
      "epoch": 0.00023401071693596238,
      "grad_norm": 9893.289442849633,
      "learning_rate": 3.9400931961265653e-07,
      "loss": 1.7611,
      "step": 64480
    },
    {
      "epoch": 0.00023412685128679908,
      "grad_norm": 9478.523830217446,
      "learning_rate": 3.9391148834260297e-07,
      "loss": 1.7544,
      "step": 64512
    },
    {
      "epoch": 0.0002342429856376358,
      "grad_norm": 10269.991236607751,
      "learning_rate": 3.9381372990996604e-07,
      "loss": 1.7662,
      "step": 64544
    },
    {
      "epoch": 0.0002343591199884725,
      "grad_norm": 10271.433492945374,
      "learning_rate": 3.9371604422440884e-07,
      "loss": 1.785,
      "step": 64576
    },
    {
      "epoch": 0.0002344752543393092,
      "grad_norm": 9303.892088798108,
      "learning_rate": 3.936184311957514e-07,
      "loss": 1.7962,
      "step": 64608
    },
    {
      "epoch": 0.0002345913886901459,
      "grad_norm": 9929.320319135646,
      "learning_rate": 3.9352089073397024e-07,
      "loss": 1.7729,
      "step": 64640
    },
    {
      "epoch": 0.0002347075230409826,
      "grad_norm": 9470.25886657804,
      "learning_rate": 3.934234227491977e-07,
      "loss": 1.7679,
      "step": 64672
    },
    {
      "epoch": 0.00023482365739181934,
      "grad_norm": 10707.212989382439,
      "learning_rate": 3.933260271517221e-07,
      "loss": 1.7911,
      "step": 64704
    },
    {
      "epoch": 0.00023493979174265604,
      "grad_norm": 9548.651108926328,
      "learning_rate": 3.932287038519871e-07,
      "loss": 1.7818,
      "step": 64736
    },
    {
      "epoch": 0.00023505592609349274,
      "grad_norm": 8619.259364933858,
      "learning_rate": 3.9313145276059133e-07,
      "loss": 1.7492,
      "step": 64768
    },
    {
      "epoch": 0.00023517206044432944,
      "grad_norm": 9579.984133598553,
      "learning_rate": 3.9303427378828815e-07,
      "loss": 1.7782,
      "step": 64800
    },
    {
      "epoch": 0.00023528819479516616,
      "grad_norm": 10287.40598984992,
      "learning_rate": 3.9293716684598523e-07,
      "loss": 1.7823,
      "step": 64832
    },
    {
      "epoch": 0.00023540432914600286,
      "grad_norm": 11057.588344661779,
      "learning_rate": 3.928401318447444e-07,
      "loss": 1.7923,
      "step": 64864
    },
    {
      "epoch": 0.00023552046349683956,
      "grad_norm": 9925.564769825443,
      "learning_rate": 3.9274316869578083e-07,
      "loss": 1.8046,
      "step": 64896
    },
    {
      "epoch": 0.00023563659784767626,
      "grad_norm": 10263.536817296463,
      "learning_rate": 3.926462773104635e-07,
      "loss": 1.784,
      "step": 64928
    },
    {
      "epoch": 0.00023575273219851296,
      "grad_norm": 8349.373748970638,
      "learning_rate": 3.92552482132207e-07,
      "loss": 1.7803,
      "step": 64960
    },
    {
      "epoch": 0.0002358688665493497,
      "grad_norm": 9748.439157116385,
      "learning_rate": 3.924557317731462e-07,
      "loss": 1.788,
      "step": 64992
    },
    {
      "epoch": 0.0002359850009001864,
      "grad_norm": 10370.853581070363,
      "learning_rate": 3.923590529155073e-07,
      "loss": 1.7914,
      "step": 65024
    },
    {
      "epoch": 0.0002361011352510231,
      "grad_norm": 8607.622377869513,
      "learning_rate": 3.9226244547126417e-07,
      "loss": 1.7835,
      "step": 65056
    },
    {
      "epoch": 0.0002362172696018598,
      "grad_norm": 9339.32770599683,
      "learning_rate": 3.9216590935254235e-07,
      "loss": 1.7827,
      "step": 65088
    },
    {
      "epoch": 0.00023633340395269652,
      "grad_norm": 8637.6626468044,
      "learning_rate": 3.9206944447161856e-07,
      "loss": 1.7551,
      "step": 65120
    },
    {
      "epoch": 0.00023644953830353322,
      "grad_norm": 9849.290634355348,
      "learning_rate": 3.919730507409207e-07,
      "loss": 1.7663,
      "step": 65152
    },
    {
      "epoch": 0.00023656567265436992,
      "grad_norm": 8309.269161605009,
      "learning_rate": 3.9187672807302724e-07,
      "loss": 1.7796,
      "step": 65184
    },
    {
      "epoch": 0.00023668180700520662,
      "grad_norm": 11680.631746613708,
      "learning_rate": 3.9178047638066673e-07,
      "loss": 1.7928,
      "step": 65216
    },
    {
      "epoch": 0.00023679794135604332,
      "grad_norm": 10131.897749187956,
      "learning_rate": 3.916842955767181e-07,
      "loss": 1.772,
      "step": 65248
    },
    {
      "epoch": 0.00023691407570688004,
      "grad_norm": 11619.624950918167,
      "learning_rate": 3.9158818557420957e-07,
      "loss": 1.7905,
      "step": 65280
    },
    {
      "epoch": 0.00023703021005771674,
      "grad_norm": 10788.011586942239,
      "learning_rate": 3.914921462863188e-07,
      "loss": 1.8071,
      "step": 65312
    },
    {
      "epoch": 0.00023714634440855344,
      "grad_norm": 10268.10732316331,
      "learning_rate": 3.913961776263725e-07,
      "loss": 1.7733,
      "step": 65344
    },
    {
      "epoch": 0.00023726247875939014,
      "grad_norm": 10680.798191146576,
      "learning_rate": 3.9130027950784604e-07,
      "loss": 1.7656,
      "step": 65376
    },
    {
      "epoch": 0.00023737861311022687,
      "grad_norm": 9552.785352974282,
      "learning_rate": 3.9120445184436284e-07,
      "loss": 1.7733,
      "step": 65408
    },
    {
      "epoch": 0.00023749474746106357,
      "grad_norm": 9686.584950332082,
      "learning_rate": 3.911086945496947e-07,
      "loss": 1.7429,
      "step": 65440
    },
    {
      "epoch": 0.00023761088181190027,
      "grad_norm": 9267.794991258708,
      "learning_rate": 3.910130075377609e-07,
      "loss": 1.7753,
      "step": 65472
    },
    {
      "epoch": 0.00023772701616273697,
      "grad_norm": 9050.89332607561,
      "learning_rate": 3.90917390722628e-07,
      "loss": 1.7784,
      "step": 65504
    },
    {
      "epoch": 0.00023784315051357367,
      "grad_norm": 10423.335454642147,
      "learning_rate": 3.908218440185098e-07,
      "loss": 1.7701,
      "step": 65536
    },
    {
      "epoch": 0.0002379592848644104,
      "grad_norm": 9439.993644065657,
      "learning_rate": 3.9072636733976663e-07,
      "loss": 1.7847,
      "step": 65568
    },
    {
      "epoch": 0.0002380754192152471,
      "grad_norm": 10941.583432026646,
      "learning_rate": 3.906309606009052e-07,
      "loss": 1.7895,
      "step": 65600
    },
    {
      "epoch": 0.0002381915535660838,
      "grad_norm": 9652.24077610997,
      "learning_rate": 3.9053562371657844e-07,
      "loss": 1.7809,
      "step": 65632
    },
    {
      "epoch": 0.0002383076879169205,
      "grad_norm": 9535.24367806088,
      "learning_rate": 3.904403566015848e-07,
      "loss": 1.7857,
      "step": 65664
    },
    {
      "epoch": 0.00023842382226775722,
      "grad_norm": 9577.784503735716,
      "learning_rate": 3.903451591708682e-07,
      "loss": 1.7773,
      "step": 65696
    },
    {
      "epoch": 0.00023853995661859392,
      "grad_norm": 9442.44290424888,
      "learning_rate": 3.902500313395178e-07,
      "loss": 1.7776,
      "step": 65728
    },
    {
      "epoch": 0.00023865609096943062,
      "grad_norm": 9260.814327044896,
      "learning_rate": 3.9015497302276753e-07,
      "loss": 1.78,
      "step": 65760
    },
    {
      "epoch": 0.00023877222532026732,
      "grad_norm": 10032.745885349632,
      "learning_rate": 3.900599841359955e-07,
      "loss": 1.7963,
      "step": 65792
    },
    {
      "epoch": 0.00023888835967110402,
      "grad_norm": 11614.613984115012,
      "learning_rate": 3.8996506459472436e-07,
      "loss": 1.8042,
      "step": 65824
    },
    {
      "epoch": 0.00023900449402194075,
      "grad_norm": 11401.780913523991,
      "learning_rate": 3.898702143146203e-07,
      "loss": 1.7923,
      "step": 65856
    },
    {
      "epoch": 0.00023912062837277745,
      "grad_norm": 7539.373581405818,
      "learning_rate": 3.8977543321149305e-07,
      "loss": 1.781,
      "step": 65888
    },
    {
      "epoch": 0.00023923676272361415,
      "grad_norm": 10161.759394907951,
      "learning_rate": 3.8968072120129574e-07,
      "loss": 1.7821,
      "step": 65920
    },
    {
      "epoch": 0.00023935289707445085,
      "grad_norm": 8574.798423286695,
      "learning_rate": 3.8958903475017306e-07,
      "loss": 1.759,
      "step": 65952
    },
    {
      "epoch": 0.00023946903142528755,
      "grad_norm": 9552.050355813666,
      "learning_rate": 3.894944585216177e-07,
      "loss": 1.767,
      "step": 65984
    },
    {
      "epoch": 0.00023958516577612428,
      "grad_norm": 10105.015487370614,
      "learning_rate": 3.8939995113731845e-07,
      "loss": 1.7644,
      "step": 66016
    },
    {
      "epoch": 0.00023970130012696098,
      "grad_norm": 8725.069856453872,
      "learning_rate": 3.8930551251379325e-07,
      "loss": 1.7626,
      "step": 66048
    },
    {
      "epoch": 0.00023981743447779768,
      "grad_norm": 9523.158929682944,
      "learning_rate": 3.8921114256770233e-07,
      "loss": 1.797,
      "step": 66080
    },
    {
      "epoch": 0.00023993356882863438,
      "grad_norm": 9673.386583818514,
      "learning_rate": 3.891168412158467e-07,
      "loss": 1.7929,
      "step": 66112
    },
    {
      "epoch": 0.0002400497031794711,
      "grad_norm": 9498.394601194455,
      "learning_rate": 3.8902260837516885e-07,
      "loss": 1.8025,
      "step": 66144
    },
    {
      "epoch": 0.0002401658375303078,
      "grad_norm": 10586.831726253138,
      "learning_rate": 3.889284439627517e-07,
      "loss": 1.757,
      "step": 66176
    },
    {
      "epoch": 0.0002402819718811445,
      "grad_norm": 10360.863284495168,
      "learning_rate": 3.8883434789581883e-07,
      "loss": 1.7473,
      "step": 66208
    },
    {
      "epoch": 0.0002403981062319812,
      "grad_norm": 11126.537107294434,
      "learning_rate": 3.8874032009173364e-07,
      "loss": 1.7683,
      "step": 66240
    },
    {
      "epoch": 0.0002405142405828179,
      "grad_norm": 9974.03719664209,
      "learning_rate": 3.886463604679996e-07,
      "loss": 1.7444,
      "step": 66272
    },
    {
      "epoch": 0.00024063037493365463,
      "grad_norm": 9221.348057632355,
      "learning_rate": 3.885524689422597e-07,
      "loss": 1.7636,
      "step": 66304
    },
    {
      "epoch": 0.00024074650928449133,
      "grad_norm": 10397.341775665547,
      "learning_rate": 3.8845864543229595e-07,
      "loss": 1.7827,
      "step": 66336
    },
    {
      "epoch": 0.00024086264363532803,
      "grad_norm": 8964.386426298233,
      "learning_rate": 3.8836488985602936e-07,
      "loss": 1.7828,
      "step": 66368
    },
    {
      "epoch": 0.00024097877798616473,
      "grad_norm": 9778.328384749615,
      "learning_rate": 3.882712021315196e-07,
      "loss": 1.7987,
      "step": 66400
    },
    {
      "epoch": 0.00024109491233700146,
      "grad_norm": 9324.717261129155,
      "learning_rate": 3.881775821769645e-07,
      "loss": 1.8286,
      "step": 66432
    },
    {
      "epoch": 0.00024121104668783816,
      "grad_norm": 9162.585879542958,
      "learning_rate": 3.880840299107001e-07,
      "loss": 1.8076,
      "step": 66464
    },
    {
      "epoch": 0.00024132718103867486,
      "grad_norm": 9915.355565989552,
      "learning_rate": 3.8799054525119994e-07,
      "loss": 1.7967,
      "step": 66496
    },
    {
      "epoch": 0.00024144331538951156,
      "grad_norm": 9311.187464550372,
      "learning_rate": 3.87897128117075e-07,
      "loss": 1.7799,
      "step": 66528
    },
    {
      "epoch": 0.00024155944974034826,
      "grad_norm": 8588.58009219219,
      "learning_rate": 3.8780377842707365e-07,
      "loss": 1.7789,
      "step": 66560
    },
    {
      "epoch": 0.00024167558409118498,
      "grad_norm": 9197.50781462022,
      "learning_rate": 3.877104961000806e-07,
      "loss": 1.775,
      "step": 66592
    },
    {
      "epoch": 0.00024179171844202168,
      "grad_norm": 8841.882718064066,
      "learning_rate": 3.8761728105511745e-07,
      "loss": 1.7506,
      "step": 66624
    },
    {
      "epoch": 0.00024190785279285838,
      "grad_norm": 9779.32605039836,
      "learning_rate": 3.8752413321134174e-07,
      "loss": 1.7595,
      "step": 66656
    },
    {
      "epoch": 0.00024202398714369508,
      "grad_norm": 10838.4766457284,
      "learning_rate": 3.874310524880471e-07,
      "loss": 1.7643,
      "step": 66688
    },
    {
      "epoch": 0.0002421401214945318,
      "grad_norm": 11424.815797202158,
      "learning_rate": 3.873380388046629e-07,
      "loss": 1.7821,
      "step": 66720
    },
    {
      "epoch": 0.0002422562558453685,
      "grad_norm": 9728.367077778263,
      "learning_rate": 3.8724509208075365e-07,
      "loss": 1.7772,
      "step": 66752
    },
    {
      "epoch": 0.0002423723901962052,
      "grad_norm": 10112.010482589503,
      "learning_rate": 3.8715221223601885e-07,
      "loss": 1.7606,
      "step": 66784
    },
    {
      "epoch": 0.0002424885245470419,
      "grad_norm": 9047.180334225686,
      "learning_rate": 3.87059399190293e-07,
      "loss": 1.7625,
      "step": 66816
    },
    {
      "epoch": 0.0002426046588978786,
      "grad_norm": 10897.61928129259,
      "learning_rate": 3.869666528635449e-07,
      "loss": 1.7883,
      "step": 66848
    },
    {
      "epoch": 0.00024272079324871534,
      "grad_norm": 11251.043596040325,
      "learning_rate": 3.868739731758776e-07,
      "loss": 1.7747,
      "step": 66880
    },
    {
      "epoch": 0.00024283692759955204,
      "grad_norm": 9836.485957901836,
      "learning_rate": 3.867813600475279e-07,
      "loss": 1.7823,
      "step": 66912
    },
    {
      "epoch": 0.00024295306195038874,
      "grad_norm": 8164.527298013033,
      "learning_rate": 3.8669170447614275e-07,
      "loss": 1.7663,
      "step": 66944
    },
    {
      "epoch": 0.00024306919630122544,
      "grad_norm": 9317.375918143476,
      "learning_rate": 3.8659922215386916e-07,
      "loss": 1.7662,
      "step": 66976
    },
    {
      "epoch": 0.00024318533065206216,
      "grad_norm": 9087.961487594455,
      "learning_rate": 3.8650680615490247e-07,
      "loss": 1.7946,
      "step": 67008
    },
    {
      "epoch": 0.00024330146500289886,
      "grad_norm": 10422.270098207971,
      "learning_rate": 3.864144564000081e-07,
      "loss": 1.8004,
      "step": 67040
    },
    {
      "epoch": 0.00024341759935373556,
      "grad_norm": 8611.771826981949,
      "learning_rate": 3.863221728100837e-07,
      "loss": 1.798,
      "step": 67072
    },
    {
      "epoch": 0.00024353373370457226,
      "grad_norm": 9779.665331697193,
      "learning_rate": 3.8622995530615967e-07,
      "loss": 1.7833,
      "step": 67104
    },
    {
      "epoch": 0.00024364986805540896,
      "grad_norm": 9476.135288185791,
      "learning_rate": 3.861378038093977e-07,
      "loss": 1.7729,
      "step": 67136
    },
    {
      "epoch": 0.0002437660024062457,
      "grad_norm": 10042.023700430109,
      "learning_rate": 3.8604571824109154e-07,
      "loss": 1.7808,
      "step": 67168
    },
    {
      "epoch": 0.0002438821367570824,
      "grad_norm": 9491.166208638431,
      "learning_rate": 3.859536985226658e-07,
      "loss": 1.7747,
      "step": 67200
    },
    {
      "epoch": 0.0002439982711079191,
      "grad_norm": 9075.86326472584,
      "learning_rate": 3.858617445756766e-07,
      "loss": 1.7831,
      "step": 67232
    },
    {
      "epoch": 0.0002441144054587558,
      "grad_norm": 10004.3812402367,
      "learning_rate": 3.857698563218106e-07,
      "loss": 1.7906,
      "step": 67264
    },
    {
      "epoch": 0.0002442305398095925,
      "grad_norm": 11166.936374852326,
      "learning_rate": 3.8567803368288496e-07,
      "loss": 1.7956,
      "step": 67296
    },
    {
      "epoch": 0.0002443466741604292,
      "grad_norm": 9437.724619843493,
      "learning_rate": 3.8558627658084696e-07,
      "loss": 1.7909,
      "step": 67328
    },
    {
      "epoch": 0.0002444628085112659,
      "grad_norm": 9957.027568506577,
      "learning_rate": 3.85494584937774e-07,
      "loss": 1.7888,
      "step": 67360
    },
    {
      "epoch": 0.00024457894286210264,
      "grad_norm": 10299.736986933209,
      "learning_rate": 3.85402958675873e-07,
      "loss": 1.7405,
      "step": 67392
    },
    {
      "epoch": 0.00024469507721293934,
      "grad_norm": 9824.417132837958,
      "learning_rate": 3.853113977174803e-07,
      "loss": 1.756,
      "step": 67424
    },
    {
      "epoch": 0.00024481121156377604,
      "grad_norm": 10209.574623851868,
      "learning_rate": 3.8521990198506127e-07,
      "loss": 1.7537,
      "step": 67456
    },
    {
      "epoch": 0.00024492734591461274,
      "grad_norm": 11170.171260996853,
      "learning_rate": 3.851284714012101e-07,
      "loss": 1.7546,
      "step": 67488
    },
    {
      "epoch": 0.00024504348026544944,
      "grad_norm": 9652.460100927638,
      "learning_rate": 3.850371058886496e-07,
      "loss": 1.7543,
      "step": 67520
    },
    {
      "epoch": 0.00024515961461628614,
      "grad_norm": 9173.45333012601,
      "learning_rate": 3.849458053702308e-07,
      "loss": 1.7646,
      "step": 67552
    },
    {
      "epoch": 0.00024527574896712284,
      "grad_norm": 9737.19014911386,
      "learning_rate": 3.848545697689328e-07,
      "loss": 1.7784,
      "step": 67584
    },
    {
      "epoch": 0.00024539188331795954,
      "grad_norm": 9072.969965782979,
      "learning_rate": 3.847633990078622e-07,
      "loss": 1.807,
      "step": 67616
    },
    {
      "epoch": 0.00024550801766879624,
      "grad_norm": 8607.736636305737,
      "learning_rate": 3.8467229301025336e-07,
      "loss": 1.8167,
      "step": 67648
    },
    {
      "epoch": 0.000245624152019633,
      "grad_norm": 10140.105916606592,
      "learning_rate": 3.845812516994676e-07,
      "loss": 1.7968,
      "step": 67680
    },
    {
      "epoch": 0.0002457402863704697,
      "grad_norm": 10885.705673037464,
      "learning_rate": 3.844902749989931e-07,
      "loss": 1.7559,
      "step": 67712
    },
    {
      "epoch": 0.0002458564207213064,
      "grad_norm": 10521.584671521681,
      "learning_rate": 3.84399362832445e-07,
      "loss": 1.7608,
      "step": 67744
    },
    {
      "epoch": 0.0002459725550721431,
      "grad_norm": 10300.262715096154,
      "learning_rate": 3.843085151235645e-07,
      "loss": 1.7646,
      "step": 67776
    },
    {
      "epoch": 0.0002460886894229798,
      "grad_norm": 8877.173424012848,
      "learning_rate": 3.84217731796219e-07,
      "loss": 1.7668,
      "step": 67808
    },
    {
      "epoch": 0.0002462048237738165,
      "grad_norm": 10955.450150495872,
      "learning_rate": 3.841270127744016e-07,
      "loss": 1.7936,
      "step": 67840
    },
    {
      "epoch": 0.0002463209581246532,
      "grad_norm": 10006.059863902476,
      "learning_rate": 3.8403635798223133e-07,
      "loss": 1.8026,
      "step": 67872
    },
    {
      "epoch": 0.0002464370924754899,
      "grad_norm": 8581.752152095747,
      "learning_rate": 3.8394576734395207e-07,
      "loss": 1.7825,
      "step": 67904
    },
    {
      "epoch": 0.0002465532268263266,
      "grad_norm": 9295.50934591537,
      "learning_rate": 3.8385524078393306e-07,
      "loss": 1.7876,
      "step": 67936
    },
    {
      "epoch": 0.00024666936117716335,
      "grad_norm": 8044.547594489078,
      "learning_rate": 3.837676042135401e-07,
      "loss": 1.7944,
      "step": 67968
    },
    {
      "epoch": 0.00024678549552800005,
      "grad_norm": 9036.932001514673,
      "learning_rate": 3.836772035870571e-07,
      "loss": 1.7696,
      "step": 68000
    },
    {
      "epoch": 0.00024690162987883675,
      "grad_norm": 9277.817631318261,
      "learning_rate": 3.835868668150391e-07,
      "loss": 1.7812,
      "step": 68032
    },
    {
      "epoch": 0.00024701776422967345,
      "grad_norm": 9736.276701080347,
      "learning_rate": 3.8349659382234886e-07,
      "loss": 1.7799,
      "step": 68064
    },
    {
      "epoch": 0.00024713389858051015,
      "grad_norm": 10956.579210684327,
      "learning_rate": 3.834063845339729e-07,
      "loss": 1.7774,
      "step": 68096
    },
    {
      "epoch": 0.00024725003293134685,
      "grad_norm": 9114.325208154469,
      "learning_rate": 3.8331623887502107e-07,
      "loss": 1.7787,
      "step": 68128
    },
    {
      "epoch": 0.00024736616728218355,
      "grad_norm": 11158.875929053069,
      "learning_rate": 3.8322615677072645e-07,
      "loss": 1.7739,
      "step": 68160
    },
    {
      "epoch": 0.00024748230163302025,
      "grad_norm": 9038.538266777432,
      "learning_rate": 3.8313613814644506e-07,
      "loss": 1.7587,
      "step": 68192
    },
    {
      "epoch": 0.00024759843598385695,
      "grad_norm": 7780.126220055816,
      "learning_rate": 3.830461829276558e-07,
      "loss": 1.7759,
      "step": 68224
    },
    {
      "epoch": 0.0002477145703346937,
      "grad_norm": 9433.750579700525,
      "learning_rate": 3.829562910399597e-07,
      "loss": 1.7943,
      "step": 68256
    },
    {
      "epoch": 0.0002478307046855304,
      "grad_norm": 10572.886833783856,
      "learning_rate": 3.8286646240908003e-07,
      "loss": 1.7658,
      "step": 68288
    },
    {
      "epoch": 0.0002479468390363671,
      "grad_norm": 10509.595710587539,
      "learning_rate": 3.827766969608621e-07,
      "loss": 1.7442,
      "step": 68320
    },
    {
      "epoch": 0.0002480629733872038,
      "grad_norm": 10517.775430194351,
      "learning_rate": 3.8268699462127256e-07,
      "loss": 1.76,
      "step": 68352
    },
    {
      "epoch": 0.0002481791077380405,
      "grad_norm": 10357.423038574798,
      "learning_rate": 3.825973553164e-07,
      "loss": 1.7778,
      "step": 68384
    },
    {
      "epoch": 0.0002482952420888772,
      "grad_norm": 9750.25702225331,
      "learning_rate": 3.8250777897245337e-07,
      "loss": 1.7797,
      "step": 68416
    },
    {
      "epoch": 0.0002484113764397139,
      "grad_norm": 9570.778129284994,
      "learning_rate": 3.824182655157633e-07,
      "loss": 1.7786,
      "step": 68448
    },
    {
      "epoch": 0.0002485275107905506,
      "grad_norm": 8064.804399363942,
      "learning_rate": 3.8232881487278047e-07,
      "loss": 1.7541,
      "step": 68480
    },
    {
      "epoch": 0.0002486436451413873,
      "grad_norm": 8624.883883276343,
      "learning_rate": 3.822394269700762e-07,
      "loss": 1.7652,
      "step": 68512
    },
    {
      "epoch": 0.00024875977949222406,
      "grad_norm": 9267.510668998444,
      "learning_rate": 3.821501017343418e-07,
      "loss": 1.7724,
      "step": 68544
    },
    {
      "epoch": 0.00024887591384306076,
      "grad_norm": 10622.320085555697,
      "learning_rate": 3.820608390923884e-07,
      "loss": 1.7886,
      "step": 68576
    },
    {
      "epoch": 0.00024899204819389746,
      "grad_norm": 8508.364472682162,
      "learning_rate": 3.819716389711471e-07,
      "loss": 1.7875,
      "step": 68608
    },
    {
      "epoch": 0.00024910818254473416,
      "grad_norm": 8770.04663613598,
      "learning_rate": 3.818825012976679e-07,
      "loss": 1.7876,
      "step": 68640
    },
    {
      "epoch": 0.00024922431689557086,
      "grad_norm": 9150.34972009267,
      "learning_rate": 3.8179342599912016e-07,
      "loss": 1.7911,
      "step": 68672
    },
    {
      "epoch": 0.00024934045124640756,
      "grad_norm": 8838.026702833615,
      "learning_rate": 3.817044130027921e-07,
      "loss": 1.7937,
      "step": 68704
    },
    {
      "epoch": 0.00024945658559724426,
      "grad_norm": 10786.634414867318,
      "learning_rate": 3.816154622360904e-07,
      "loss": 1.8086,
      "step": 68736
    },
    {
      "epoch": 0.00024957271994808096,
      "grad_norm": 11211.854975872639,
      "learning_rate": 3.815265736265404e-07,
      "loss": 1.786,
      "step": 68768
    },
    {
      "epoch": 0.00024968885429891766,
      "grad_norm": 9908.042995465856,
      "learning_rate": 3.814377471017851e-07,
      "loss": 1.7961,
      "step": 68800
    },
    {
      "epoch": 0.0002498049886497544,
      "grad_norm": 9456.915987783756,
      "learning_rate": 3.813489825895857e-07,
      "loss": 1.8003,
      "step": 68832
    },
    {
      "epoch": 0.0002499211230005911,
      "grad_norm": 9834.102297617206,
      "learning_rate": 3.8126028001782117e-07,
      "loss": 1.793,
      "step": 68864
    },
    {
      "epoch": 0.0002500372573514278,
      "grad_norm": 8260.320453832284,
      "learning_rate": 3.811716393144874e-07,
      "loss": 1.7744,
      "step": 68896
    },
    {
      "epoch": 0.0002501533917022645,
      "grad_norm": 9423.615548185315,
      "learning_rate": 3.810830604076976e-07,
      "loss": 1.7421,
      "step": 68928
    },
    {
      "epoch": 0.0002502695260531012,
      "grad_norm": 9845.569358853758,
      "learning_rate": 3.809973084540216e-07,
      "loss": 1.75,
      "step": 68960
    },
    {
      "epoch": 0.0002503856604039379,
      "grad_norm": 9567.745711503834,
      "learning_rate": 3.8090885099955055e-07,
      "loss": 1.7718,
      "step": 68992
    },
    {
      "epoch": 0.0002505017947547746,
      "grad_norm": 9456.180624332426,
      "learning_rate": 3.808204551288973e-07,
      "loss": 1.7837,
      "step": 69024
    },
    {
      "epoch": 0.0002506179291056113,
      "grad_norm": 10502.48180193615,
      "learning_rate": 3.807321207706377e-07,
      "loss": 1.7429,
      "step": 69056
    },
    {
      "epoch": 0.000250734063456448,
      "grad_norm": 9528.961538383917,
      "learning_rate": 3.8064384785346323e-07,
      "loss": 1.753,
      "step": 69088
    },
    {
      "epoch": 0.00025085019780728477,
      "grad_norm": 9537.399226204176,
      "learning_rate": 3.805556363061814e-07,
      "loss": 1.7593,
      "step": 69120
    },
    {
      "epoch": 0.00025096633215812147,
      "grad_norm": 9982.874736267104,
      "learning_rate": 3.8046748605771476e-07,
      "loss": 1.7758,
      "step": 69152
    },
    {
      "epoch": 0.00025108246650895816,
      "grad_norm": 8998.102355496963,
      "learning_rate": 3.803793970371013e-07,
      "loss": 1.7862,
      "step": 69184
    },
    {
      "epoch": 0.00025119860085979486,
      "grad_norm": 9412.339241655074,
      "learning_rate": 3.8029136917349396e-07,
      "loss": 1.751,
      "step": 69216
    },
    {
      "epoch": 0.00025131473521063156,
      "grad_norm": 9165.393608569138,
      "learning_rate": 3.802034023961601e-07,
      "loss": 1.7517,
      "step": 69248
    },
    {
      "epoch": 0.00025143086956146826,
      "grad_norm": 9176.221989468215,
      "learning_rate": 3.8011549663448206e-07,
      "loss": 1.7582,
      "step": 69280
    },
    {
      "epoch": 0.00025154700391230496,
      "grad_norm": 9108.925403141691,
      "learning_rate": 3.8002765181795605e-07,
      "loss": 1.7673,
      "step": 69312
    },
    {
      "epoch": 0.00025166313826314166,
      "grad_norm": 10556.823385848606,
      "learning_rate": 3.7993986787619226e-07,
      "loss": 1.7864,
      "step": 69344
    },
    {
      "epoch": 0.00025177927261397836,
      "grad_norm": 11083.75279406754,
      "learning_rate": 3.7985214473891496e-07,
      "loss": 1.8043,
      "step": 69376
    },
    {
      "epoch": 0.0002518954069648151,
      "grad_norm": 8857.07118634597,
      "learning_rate": 3.797644823359616e-07,
      "loss": 1.8047,
      "step": 69408
    },
    {
      "epoch": 0.0002520115413156518,
      "grad_norm": 8201.088342409195,
      "learning_rate": 3.796768805972832e-07,
      "loss": 1.8047,
      "step": 69440
    },
    {
      "epoch": 0.0002521276756664885,
      "grad_norm": 8411.110033758921,
      "learning_rate": 3.795893394529435e-07,
      "loss": 1.7992,
      "step": 69472
    },
    {
      "epoch": 0.0002522438100173252,
      "grad_norm": 10277.271038558825,
      "learning_rate": 3.795018588331196e-07,
      "loss": 1.7958,
      "step": 69504
    },
    {
      "epoch": 0.0002523599443681619,
      "grad_norm": 9235.419102563781,
      "learning_rate": 3.794144386681006e-07,
      "loss": 1.785,
      "step": 69536
    },
    {
      "epoch": 0.0002524760787189986,
      "grad_norm": 11126.762691816519,
      "learning_rate": 3.793270788882883e-07,
      "loss": 1.7876,
      "step": 69568
    },
    {
      "epoch": 0.0002525922130698353,
      "grad_norm": 10831.67586294937,
      "learning_rate": 3.792397794241965e-07,
      "loss": 1.8156,
      "step": 69600
    },
    {
      "epoch": 0.000252708347420672,
      "grad_norm": 10059.387058861987,
      "learning_rate": 3.7915254020645105e-07,
      "loss": 1.7727,
      "step": 69632
    },
    {
      "epoch": 0.0002528244817715087,
      "grad_norm": 9539.75670549307,
      "learning_rate": 3.790653611657892e-07,
      "loss": 1.7439,
      "step": 69664
    },
    {
      "epoch": 0.00025294061612234547,
      "grad_norm": 8985.577443881946,
      "learning_rate": 3.7897824223305984e-07,
      "loss": 1.7519,
      "step": 69696
    },
    {
      "epoch": 0.00025305675047318217,
      "grad_norm": 9892.709740005517,
      "learning_rate": 3.7889118333922307e-07,
      "loss": 1.7399,
      "step": 69728
    },
    {
      "epoch": 0.00025317288482401887,
      "grad_norm": 10520.868500271257,
      "learning_rate": 3.788041844153497e-07,
      "loss": 1.7732,
      "step": 69760
    },
    {
      "epoch": 0.00025328901917485557,
      "grad_norm": 8506.589328279579,
      "learning_rate": 3.787172453926216e-07,
      "loss": 1.7733,
      "step": 69792
    },
    {
      "epoch": 0.00025340515352569227,
      "grad_norm": 9301.169818899125,
      "learning_rate": 3.7863036620233097e-07,
      "loss": 1.7561,
      "step": 69824
    },
    {
      "epoch": 0.00025352128787652897,
      "grad_norm": 9113.334515971637,
      "learning_rate": 3.7854354677588027e-07,
      "loss": 1.7717,
      "step": 69856
    },
    {
      "epoch": 0.00025363742222736567,
      "grad_norm": 9372.438743464798,
      "learning_rate": 3.784567870447821e-07,
      "loss": 1.7837,
      "step": 69888
    },
    {
      "epoch": 0.00025375355657820237,
      "grad_norm": 10059.798705739593,
      "learning_rate": 3.783700869406589e-07,
      "loss": 1.7762,
      "step": 69920
    },
    {
      "epoch": 0.00025386969092903907,
      "grad_norm": 8683.531079002367,
      "learning_rate": 3.7828615301144077e-07,
      "loss": 1.7771,
      "step": 69952
    },
    {
      "epoch": 0.0002539858252798758,
      "grad_norm": 12286.630376144632,
      "learning_rate": 3.7819957009852387e-07,
      "loss": 1.7676,
      "step": 69984
    },
    {
      "epoch": 0.0002541019596307125,
      "grad_norm": 9252.972711512772,
      "learning_rate": 3.7811304661023057e-07,
      "loss": 1.774,
      "step": 70016
    },
    {
      "epoch": 0.0002542180939815492,
      "grad_norm": 9275.141292724333,
      "learning_rate": 3.780265824786171e-07,
      "loss": 1.7513,
      "step": 70048
    },
    {
      "epoch": 0.0002543342283323859,
      "grad_norm": 9526.095107650353,
      "learning_rate": 3.7794017763584827e-07,
      "loss": 1.764,
      "step": 70080
    },
    {
      "epoch": 0.0002544503626832226,
      "grad_norm": 10012.921451804163,
      "learning_rate": 3.7785383201419716e-07,
      "loss": 1.7971,
      "step": 70112
    },
    {
      "epoch": 0.0002545664970340593,
      "grad_norm": 9374.934773106423,
      "learning_rate": 3.777675455460454e-07,
      "loss": 1.7834,
      "step": 70144
    },
    {
      "epoch": 0.000254682631384896,
      "grad_norm": 9191.761746259528,
      "learning_rate": 3.7768131816388255e-07,
      "loss": 1.7883,
      "step": 70176
    },
    {
      "epoch": 0.0002547987657357327,
      "grad_norm": 9509.237719186538,
      "learning_rate": 3.7759514980030587e-07,
      "loss": 1.8025,
      "step": 70208
    },
    {
      "epoch": 0.0002549149000865694,
      "grad_norm": 7458.503603270566,
      "learning_rate": 3.775090403880204e-07,
      "loss": 1.7782,
      "step": 70240
    },
    {
      "epoch": 0.0002550310344374062,
      "grad_norm": 9938.767126761748,
      "learning_rate": 3.774229898598384e-07,
      "loss": 1.7807,
      "step": 70272
    },
    {
      "epoch": 0.0002551471687882429,
      "grad_norm": 10403.81891422568,
      "learning_rate": 3.7733699814867946e-07,
      "loss": 1.7844,
      "step": 70304
    },
    {
      "epoch": 0.0002552633031390796,
      "grad_norm": 10630.878420902009,
      "learning_rate": 3.7725106518756996e-07,
      "loss": 1.7803,
      "step": 70336
    },
    {
      "epoch": 0.0002553794374899163,
      "grad_norm": 9170.07557220768,
      "learning_rate": 3.771651909096432e-07,
      "loss": 1.8039,
      "step": 70368
    },
    {
      "epoch": 0.000255495571840753,
      "grad_norm": 8776.040565084006,
      "learning_rate": 3.770793752481385e-07,
      "loss": 1.7946,
      "step": 70400
    },
    {
      "epoch": 0.0002556117061915897,
      "grad_norm": 10287.931959339543,
      "learning_rate": 3.7699361813640215e-07,
      "loss": 1.7715,
      "step": 70432
    },
    {
      "epoch": 0.0002557278405424264,
      "grad_norm": 9900.261713712422,
      "learning_rate": 3.769079195078859e-07,
      "loss": 1.7646,
      "step": 70464
    },
    {
      "epoch": 0.0002558439748932631,
      "grad_norm": 8903.908804564431,
      "learning_rate": 3.768222792961477e-07,
      "loss": 1.7551,
      "step": 70496
    },
    {
      "epoch": 0.0002559601092440998,
      "grad_norm": 11074.543602334139,
      "learning_rate": 3.767366974348509e-07,
      "loss": 1.7733,
      "step": 70528
    },
    {
      "epoch": 0.00025607624359493653,
      "grad_norm": 10796.584459911384,
      "learning_rate": 3.7665117385776453e-07,
      "loss": 1.7654,
      "step": 70560
    },
    {
      "epoch": 0.00025619237794577323,
      "grad_norm": 11258.099662021117,
      "learning_rate": 3.765657084987626e-07,
      "loss": 1.7656,
      "step": 70592
    },
    {
      "epoch": 0.00025630851229660993,
      "grad_norm": 8952.663402585848,
      "learning_rate": 3.764803012918241e-07,
      "loss": 1.7778,
      "step": 70624
    },
    {
      "epoch": 0.00025642464664744663,
      "grad_norm": 10022.522237441033,
      "learning_rate": 3.7639495217103307e-07,
      "loss": 1.7598,
      "step": 70656
    },
    {
      "epoch": 0.00025654078099828333,
      "grad_norm": 11611.927230223242,
      "learning_rate": 3.763096610705777e-07,
      "loss": 1.7824,
      "step": 70688
    },
    {
      "epoch": 0.00025665691534912003,
      "grad_norm": 8728.26546342399,
      "learning_rate": 3.7622442792475074e-07,
      "loss": 1.7905,
      "step": 70720
    },
    {
      "epoch": 0.00025677304969995673,
      "grad_norm": 8848.613450705145,
      "learning_rate": 3.761392526679492e-07,
      "loss": 1.7581,
      "step": 70752
    },
    {
      "epoch": 0.00025688918405079343,
      "grad_norm": 8978.715164209187,
      "learning_rate": 3.7605413523467376e-07,
      "loss": 1.7611,
      "step": 70784
    },
    {
      "epoch": 0.00025700531840163013,
      "grad_norm": 8464.65711059816,
      "learning_rate": 3.7596907555952887e-07,
      "loss": 1.749,
      "step": 70816
    },
    {
      "epoch": 0.0002571214527524669,
      "grad_norm": 9706.892293623125,
      "learning_rate": 3.758840735772226e-07,
      "loss": 1.763,
      "step": 70848
    },
    {
      "epoch": 0.0002572375871033036,
      "grad_norm": 8870.384320873589,
      "learning_rate": 3.757991292225662e-07,
      "loss": 1.768,
      "step": 70880
    },
    {
      "epoch": 0.0002573537214541403,
      "grad_norm": 10331.707022559243,
      "learning_rate": 3.75714242430474e-07,
      "loss": 1.7748,
      "step": 70912
    },
    {
      "epoch": 0.000257469855804977,
      "grad_norm": 9625.38871942323,
      "learning_rate": 3.7562941313596316e-07,
      "loss": 1.7776,
      "step": 70944
    },
    {
      "epoch": 0.0002575859901558137,
      "grad_norm": 10145.048644535915,
      "learning_rate": 3.7554728952613627e-07,
      "loss": 1.7823,
      "step": 70976
    },
    {
      "epoch": 0.0002577021245066504,
      "grad_norm": 9002.597069734933,
      "learning_rate": 3.754625732404823e-07,
      "loss": 1.8014,
      "step": 71008
    },
    {
      "epoch": 0.0002578182588574871,
      "grad_norm": 9346.647420332061,
      "learning_rate": 3.75377914260096e-07,
      "loss": 1.7988,
      "step": 71040
    },
    {
      "epoch": 0.0002579343932083238,
      "grad_norm": 9425.243020739572,
      "learning_rate": 3.7529331252040086e-07,
      "loss": 1.7919,
      "step": 71072
    },
    {
      "epoch": 0.0002580505275591605,
      "grad_norm": 10376.54296960216,
      "learning_rate": 3.75208767956922e-07,
      "loss": 1.7917,
      "step": 71104
    },
    {
      "epoch": 0.00025816666190999724,
      "grad_norm": 10391.355830689276,
      "learning_rate": 3.7512428050528644e-07,
      "loss": 1.8066,
      "step": 71136
    },
    {
      "epoch": 0.00025828279626083394,
      "grad_norm": 9536.284811183023,
      "learning_rate": 3.750398501012224e-07,
      "loss": 1.7916,
      "step": 71168
    },
    {
      "epoch": 0.00025839893061167064,
      "grad_norm": 8586.456545048137,
      "learning_rate": 3.7495547668055933e-07,
      "loss": 1.7769,
      "step": 71200
    },
    {
      "epoch": 0.00025851506496250734,
      "grad_norm": 10018.81889246432,
      "learning_rate": 3.748711601792278e-07,
      "loss": 1.7564,
      "step": 71232
    },
    {
      "epoch": 0.00025863119931334404,
      "grad_norm": 8749.298486164476,
      "learning_rate": 3.747869005332592e-07,
      "loss": 1.7597,
      "step": 71264
    },
    {
      "epoch": 0.00025874733366418074,
      "grad_norm": 10521.120282555465,
      "learning_rate": 3.747026976787853e-07,
      "loss": 1.7909,
      "step": 71296
    },
    {
      "epoch": 0.00025886346801501744,
      "grad_norm": 8457.721442563594,
      "learning_rate": 3.746185515520385e-07,
      "loss": 1.78,
      "step": 71328
    },
    {
      "epoch": 0.00025897960236585414,
      "grad_norm": 10023.694727993266,
      "learning_rate": 3.745344620893514e-07,
      "loss": 1.7594,
      "step": 71360
    },
    {
      "epoch": 0.00025909573671669084,
      "grad_norm": 10508.618938756892,
      "learning_rate": 3.7445042922715633e-07,
      "loss": 1.765,
      "step": 71392
    },
    {
      "epoch": 0.0002592118710675276,
      "grad_norm": 9394.018096640011,
      "learning_rate": 3.7436645290198567e-07,
      "loss": 1.7599,
      "step": 71424
    },
    {
      "epoch": 0.0002593280054183643,
      "grad_norm": 11239.643766596875,
      "learning_rate": 3.742825330504713e-07,
      "loss": 1.7668,
      "step": 71456
    },
    {
      "epoch": 0.000259444139769201,
      "grad_norm": 8442.958723101754,
      "learning_rate": 3.7419866960934454e-07,
      "loss": 1.752,
      "step": 71488
    },
    {
      "epoch": 0.0002595602741200377,
      "grad_norm": 9809.57843130886,
      "learning_rate": 3.7411486251543587e-07,
      "loss": 1.7492,
      "step": 71520
    },
    {
      "epoch": 0.0002596764084708744,
      "grad_norm": 9212.885975632174,
      "learning_rate": 3.7403111170567466e-07,
      "loss": 1.7561,
      "step": 71552
    },
    {
      "epoch": 0.0002597925428217111,
      "grad_norm": 9166.09600647953,
      "learning_rate": 3.7394741711708927e-07,
      "loss": 1.763,
      "step": 71584
    },
    {
      "epoch": 0.0002599086771725478,
      "grad_norm": 10244.126317065795,
      "learning_rate": 3.738637786868065e-07,
      "loss": 1.7879,
      "step": 71616
    },
    {
      "epoch": 0.0002600248115233845,
      "grad_norm": 10155.115065817816,
      "learning_rate": 3.737801963520516e-07,
      "loss": 1.7899,
      "step": 71648
    },
    {
      "epoch": 0.0002601409458742212,
      "grad_norm": 8574.436541254474,
      "learning_rate": 3.736966700501479e-07,
      "loss": 1.7809,
      "step": 71680
    },
    {
      "epoch": 0.00026025708022505795,
      "grad_norm": 8774.436734058774,
      "learning_rate": 3.73613199718517e-07,
      "loss": 1.7817,
      "step": 71712
    },
    {
      "epoch": 0.00026037321457589465,
      "grad_norm": 9403.207006122964,
      "learning_rate": 3.73529785294678e-07,
      "loss": 1.8003,
      "step": 71744
    },
    {
      "epoch": 0.00026048934892673135,
      "grad_norm": 8812.136630806403,
      "learning_rate": 3.734464267162477e-07,
      "loss": 1.7907,
      "step": 71776
    },
    {
      "epoch": 0.00026060548327756805,
      "grad_norm": 9880.12530284915,
      "learning_rate": 3.733631239209405e-07,
      "loss": 1.7766,
      "step": 71808
    },
    {
      "epoch": 0.00026072161762840475,
      "grad_norm": 10428.77135620491,
      "learning_rate": 3.7327987684656776e-07,
      "loss": 1.7825,
      "step": 71840
    },
    {
      "epoch": 0.00026083775197924145,
      "grad_norm": 9080.286228968776,
      "learning_rate": 3.73196685431038e-07,
      "loss": 1.7849,
      "step": 71872
    },
    {
      "epoch": 0.00026095388633007815,
      "grad_norm": 9540.02327041187,
      "learning_rate": 3.731135496123564e-07,
      "loss": 1.7876,
      "step": 71904
    },
    {
      "epoch": 0.00026107002068091485,
      "grad_norm": 9293.995158165299,
      "learning_rate": 3.7303046932862507e-07,
      "loss": 1.7915,
      "step": 71936
    },
    {
      "epoch": 0.00026118615503175155,
      "grad_norm": 8783.338772926842,
      "learning_rate": 3.729500382043046e-07,
      "loss": 1.7723,
      "step": 71968
    },
    {
      "epoch": 0.0002613022893825883,
      "grad_norm": 9421.7590714261,
      "learning_rate": 3.728670670744904e-07,
      "loss": 1.7493,
      "step": 72000
    },
    {
      "epoch": 0.000261418423733425,
      "grad_norm": 9602.212140960019,
      "learning_rate": 3.7278415129643374e-07,
      "loss": 1.7574,
      "step": 72032
    },
    {
      "epoch": 0.0002615345580842617,
      "grad_norm": 9660.343989734527,
      "learning_rate": 3.7270129080861817e-07,
      "loss": 1.7655,
      "step": 72064
    },
    {
      "epoch": 0.0002616506924350984,
      "grad_norm": 9802.87621058228,
      "learning_rate": 3.726184855496231e-07,
      "loss": 1.757,
      "step": 72096
    },
    {
      "epoch": 0.0002617668267859351,
      "grad_norm": 10049.770843158565,
      "learning_rate": 3.7253573545812297e-07,
      "loss": 1.7673,
      "step": 72128
    },
    {
      "epoch": 0.0002618829611367718,
      "grad_norm": 10415.763054140585,
      "learning_rate": 3.724530404728878e-07,
      "loss": 1.7825,
      "step": 72160
    },
    {
      "epoch": 0.0002619990954876085,
      "grad_norm": 10652.12805030056,
      "learning_rate": 3.723704005327827e-07,
      "loss": 1.7764,
      "step": 72192
    },
    {
      "epoch": 0.0002621152298384452,
      "grad_norm": 7916.612154198285,
      "learning_rate": 3.722878155767676e-07,
      "loss": 1.7885,
      "step": 72224
    },
    {
      "epoch": 0.0002622313641892819,
      "grad_norm": 10506.492468945095,
      "learning_rate": 3.7220528554389724e-07,
      "loss": 1.7734,
      "step": 72256
    },
    {
      "epoch": 0.00026234749854011865,
      "grad_norm": 10472.602923819848,
      "learning_rate": 3.721228103733208e-07,
      "loss": 1.7516,
      "step": 72288
    },
    {
      "epoch": 0.00026246363289095535,
      "grad_norm": 9847.810416534227,
      "learning_rate": 3.7204039000428194e-07,
      "loss": 1.7724,
      "step": 72320
    },
    {
      "epoch": 0.00026257976724179205,
      "grad_norm": 9568.167745185072,
      "learning_rate": 3.7195802437611824e-07,
      "loss": 1.7716,
      "step": 72352
    },
    {
      "epoch": 0.00026269590159262875,
      "grad_norm": 12299.241602635506,
      "learning_rate": 3.718757134282616e-07,
      "loss": 1.7768,
      "step": 72384
    },
    {
      "epoch": 0.00026281203594346545,
      "grad_norm": 10831.355778479441,
      "learning_rate": 3.7179345710023726e-07,
      "loss": 1.7776,
      "step": 72416
    },
    {
      "epoch": 0.00026292817029430215,
      "grad_norm": 9889.883214679534,
      "learning_rate": 3.717112553316644e-07,
      "loss": 1.797,
      "step": 72448
    },
    {
      "epoch": 0.00026304430464513885,
      "grad_norm": 9333.63476894184,
      "learning_rate": 3.716291080622555e-07,
      "loss": 1.7754,
      "step": 72480
    },
    {
      "epoch": 0.00026316043899597555,
      "grad_norm": 10080.960866901527,
      "learning_rate": 3.7154701523181633e-07,
      "loss": 1.7854,
      "step": 72512
    },
    {
      "epoch": 0.00026327657334681225,
      "grad_norm": 9417.88394492096,
      "learning_rate": 3.714649767802456e-07,
      "loss": 1.8062,
      "step": 72544
    },
    {
      "epoch": 0.000263392707697649,
      "grad_norm": 9793.177727377359,
      "learning_rate": 3.713829926475348e-07,
      "loss": 1.7779,
      "step": 72576
    },
    {
      "epoch": 0.0002635088420484857,
      "grad_norm": 9553.146811391522,
      "learning_rate": 3.713010627737682e-07,
      "loss": 1.7735,
      "step": 72608
    },
    {
      "epoch": 0.0002636249763993224,
      "grad_norm": 8794.117579382255,
      "learning_rate": 3.7121918709912273e-07,
      "loss": 1.7704,
      "step": 72640
    },
    {
      "epoch": 0.0002637411107501591,
      "grad_norm": 9021.229738788388,
      "learning_rate": 3.7113736556386724e-07,
      "loss": 1.7814,
      "step": 72672
    },
    {
      "epoch": 0.0002638572451009958,
      "grad_norm": 9334.113991161668,
      "learning_rate": 3.7105559810836284e-07,
      "loss": 1.7497,
      "step": 72704
    },
    {
      "epoch": 0.0002639733794518325,
      "grad_norm": 10866.643363983196,
      "learning_rate": 3.709738846730629e-07,
      "loss": 1.7623,
      "step": 72736
    },
    {
      "epoch": 0.0002640895138026692,
      "grad_norm": 10962.713715134589,
      "learning_rate": 3.70892225198512e-07,
      "loss": 1.7583,
      "step": 72768
    },
    {
      "epoch": 0.0002642056481535059,
      "grad_norm": 9029.947951123528,
      "learning_rate": 3.708106196253468e-07,
      "loss": 1.7697,
      "step": 72800
    },
    {
      "epoch": 0.0002643217825043426,
      "grad_norm": 9022.768200502549,
      "learning_rate": 3.707290678942949e-07,
      "loss": 1.785,
      "step": 72832
    },
    {
      "epoch": 0.0002644379168551793,
      "grad_norm": 10631.581349921564,
      "learning_rate": 3.706475699461755e-07,
      "loss": 1.7822,
      "step": 72864
    },
    {
      "epoch": 0.00026455405120601606,
      "grad_norm": 9564.885258067658,
      "learning_rate": 3.705661257218984e-07,
      "loss": 1.7811,
      "step": 72896
    },
    {
      "epoch": 0.00026467018555685276,
      "grad_norm": 8558.215818732313,
      "learning_rate": 3.704847351624647e-07,
      "loss": 1.7887,
      "step": 72928
    },
    {
      "epoch": 0.00026478631990768946,
      "grad_norm": 18136.32355247336,
      "learning_rate": 3.7040339820896597e-07,
      "loss": 1.7964,
      "step": 72960
    },
    {
      "epoch": 0.00026490245425852616,
      "grad_norm": 9095.1577226566,
      "learning_rate": 3.703246540990896e-07,
      "loss": 1.7735,
      "step": 72992
    },
    {
      "epoch": 0.00026501858860936286,
      "grad_norm": 9605.535487415576,
      "learning_rate": 3.7024342251047307e-07,
      "loss": 1.767,
      "step": 73024
    },
    {
      "epoch": 0.00026513472296019956,
      "grad_norm": 9754.842079705852,
      "learning_rate": 3.701622443534395e-07,
      "loss": 1.7559,
      "step": 73056
    },
    {
      "epoch": 0.00026525085731103626,
      "grad_norm": 10817.847290473275,
      "learning_rate": 3.700811195694384e-07,
      "loss": 1.7495,
      "step": 73088
    },
    {
      "epoch": 0.00026536699166187296,
      "grad_norm": 10076.818545552956,
      "learning_rate": 3.700000481000094e-07,
      "loss": 1.7666,
      "step": 73120
    },
    {
      "epoch": 0.00026548312601270966,
      "grad_norm": 9636.429629276603,
      "learning_rate": 3.6991902988678144e-07,
      "loss": 1.7915,
      "step": 73152
    },
    {
      "epoch": 0.0002655992603635464,
      "grad_norm": 10015.028507198569,
      "learning_rate": 3.698380648714729e-07,
      "loss": 1.7891,
      "step": 73184
    },
    {
      "epoch": 0.0002657153947143831,
      "grad_norm": 10092.676354664307,
      "learning_rate": 3.697571529958917e-07,
      "loss": 1.7671,
      "step": 73216
    },
    {
      "epoch": 0.0002658315290652198,
      "grad_norm": 9839.789123756667,
      "learning_rate": 3.696762942019345e-07,
      "loss": 1.7759,
      "step": 73248
    },
    {
      "epoch": 0.0002659476634160565,
      "grad_norm": 9751.513113358356,
      "learning_rate": 3.695954884315871e-07,
      "loss": 1.7892,
      "step": 73280
    },
    {
      "epoch": 0.0002660637977668932,
      "grad_norm": 9169.014014603752,
      "learning_rate": 3.6951473562692387e-07,
      "loss": 1.7958,
      "step": 73312
    },
    {
      "epoch": 0.0002661799321177299,
      "grad_norm": 9078.509789607544,
      "learning_rate": 3.6943403573010785e-07,
      "loss": 1.7666,
      "step": 73344
    },
    {
      "epoch": 0.0002662960664685666,
      "grad_norm": 8666.767217365423,
      "learning_rate": 3.693533886833904e-07,
      "loss": 1.7738,
      "step": 73376
    },
    {
      "epoch": 0.0002664122008194033,
      "grad_norm": 10040.04611543194,
      "learning_rate": 3.6927279442911107e-07,
      "loss": 1.7776,
      "step": 73408
    },
    {
      "epoch": 0.00026652833517024,
      "grad_norm": 8409.88668175737,
      "learning_rate": 3.6919225290969745e-07,
      "loss": 1.7846,
      "step": 73440
    },
    {
      "epoch": 0.00026664446952107677,
      "grad_norm": 8881.673941324349,
      "learning_rate": 3.691117640676651e-07,
      "loss": 1.7755,
      "step": 73472
    },
    {
      "epoch": 0.00026676060387191347,
      "grad_norm": 9271.890422130753,
      "learning_rate": 3.690313278456172e-07,
      "loss": 1.7661,
      "step": 73504
    },
    {
      "epoch": 0.00026687673822275017,
      "grad_norm": 10247.099296874214,
      "learning_rate": 3.689509441862443e-07,
      "loss": 1.7723,
      "step": 73536
    },
    {
      "epoch": 0.00026699287257358687,
      "grad_norm": 10230.189538811097,
      "learning_rate": 3.688706130323245e-07,
      "loss": 1.7651,
      "step": 73568
    },
    {
      "epoch": 0.00026710900692442357,
      "grad_norm": 9262.652967697753,
      "learning_rate": 3.68790334326723e-07,
      "loss": 1.765,
      "step": 73600
    },
    {
      "epoch": 0.00026722514127526027,
      "grad_norm": 7946.191792299001,
      "learning_rate": 3.6871010801239203e-07,
      "loss": 1.7555,
      "step": 73632
    },
    {
      "epoch": 0.00026734127562609697,
      "grad_norm": 9239.45117417696,
      "learning_rate": 3.686299340323706e-07,
      "loss": 1.7706,
      "step": 73664
    },
    {
      "epoch": 0.00026745740997693367,
      "grad_norm": 9470.73312896103,
      "learning_rate": 3.685498123297844e-07,
      "loss": 1.7772,
      "step": 73696
    },
    {
      "epoch": 0.00026757354432777037,
      "grad_norm": 10147.593902004553,
      "learning_rate": 3.684697428478455e-07,
      "loss": 1.7911,
      "step": 73728
    },
    {
      "epoch": 0.0002676896786786071,
      "grad_norm": 10264.012081052904,
      "learning_rate": 3.683897255298526e-07,
      "loss": 1.7767,
      "step": 73760
    },
    {
      "epoch": 0.0002678058130294438,
      "grad_norm": 9814.627043347087,
      "learning_rate": 3.683097603191904e-07,
      "loss": 1.7705,
      "step": 73792
    },
    {
      "epoch": 0.0002679219473802805,
      "grad_norm": 9361.061691923624,
      "learning_rate": 3.682298471593294e-07,
      "loss": 1.7551,
      "step": 73824
    },
    {
      "epoch": 0.0002680380817311172,
      "grad_norm": 10018.990767537416,
      "learning_rate": 3.681499859938261e-07,
      "loss": 1.7514,
      "step": 73856
    },
    {
      "epoch": 0.0002681542160819539,
      "grad_norm": 10388.253558707547,
      "learning_rate": 3.680701767663227e-07,
      "loss": 1.8008,
      "step": 73888
    },
    {
      "epoch": 0.0002682703504327906,
      "grad_norm": 8582.097762202433,
      "learning_rate": 3.679904194205468e-07,
      "loss": 1.7626,
      "step": 73920
    },
    {
      "epoch": 0.0002683864847836273,
      "grad_norm": 9763.19097426656,
      "learning_rate": 3.6791071390031143e-07,
      "loss": 1.7754,
      "step": 73952
    },
    {
      "epoch": 0.000268502619134464,
      "grad_norm": 10239.472056702924,
      "learning_rate": 3.678335485461642e-07,
      "loss": 1.7785,
      "step": 73984
    },
    {
      "epoch": 0.0002686187534853007,
      "grad_norm": 9229.556002322106,
      "learning_rate": 3.6775394489359144e-07,
      "loss": 1.7801,
      "step": 74016
    },
    {
      "epoch": 0.0002687348878361375,
      "grad_norm": 9830.602931661922,
      "learning_rate": 3.676743929002549e-07,
      "loss": 1.7973,
      "step": 74048
    },
    {
      "epoch": 0.0002688510221869742,
      "grad_norm": 10669.447970724634,
      "learning_rate": 3.675948925103043e-07,
      "loss": 1.8096,
      "step": 74080
    },
    {
      "epoch": 0.00026896715653781087,
      "grad_norm": 9383.164498185033,
      "learning_rate": 3.675154436679742e-07,
      "loss": 1.7996,
      "step": 74112
    },
    {
      "epoch": 0.00026908329088864757,
      "grad_norm": 9597.682011819312,
      "learning_rate": 3.674360463175832e-07,
      "loss": 1.7898,
      "step": 74144
    },
    {
      "epoch": 0.00026919942523948427,
      "grad_norm": 9119.548344079327,
      "learning_rate": 3.673567004035343e-07,
      "loss": 1.7787,
      "step": 74176
    },
    {
      "epoch": 0.00026931555959032097,
      "grad_norm": 10309.409197427367,
      "learning_rate": 3.6727740587031434e-07,
      "loss": 1.7635,
      "step": 74208
    },
    {
      "epoch": 0.00026943169394115767,
      "grad_norm": 9209.207674930563,
      "learning_rate": 3.671981626624942e-07,
      "loss": 1.7448,
      "step": 74240
    },
    {
      "epoch": 0.00026954782829199437,
      "grad_norm": 8842.008595336243,
      "learning_rate": 3.6711897072472816e-07,
      "loss": 1.7417,
      "step": 74272
    },
    {
      "epoch": 0.00026966396264283107,
      "grad_norm": 10301.755869753466,
      "learning_rate": 3.6703983000175426e-07,
      "loss": 1.7672,
      "step": 74304
    },
    {
      "epoch": 0.0002697800969936678,
      "grad_norm": 10008.863871589023,
      "learning_rate": 3.6696074043839385e-07,
      "loss": 1.7485,
      "step": 74336
    },
    {
      "epoch": 0.0002698962313445045,
      "grad_norm": 9800.48917146486,
      "learning_rate": 3.668817019795513e-07,
      "loss": 1.7576,
      "step": 74368
    },
    {
      "epoch": 0.0002700123656953412,
      "grad_norm": 11690.62461975407,
      "learning_rate": 3.668027145702142e-07,
      "loss": 1.7683,
      "step": 74400
    },
    {
      "epoch": 0.0002701285000461779,
      "grad_norm": 9522.34928995991,
      "learning_rate": 3.6672377815545303e-07,
      "loss": 1.7691,
      "step": 74432
    },
    {
      "epoch": 0.0002702446343970146,
      "grad_norm": 9913.7207949387,
      "learning_rate": 3.6664489268042084e-07,
      "loss": 1.7771,
      "step": 74464
    },
    {
      "epoch": 0.0002703607687478513,
      "grad_norm": 10489.059919744954,
      "learning_rate": 3.6656605809035324e-07,
      "loss": 1.7874,
      "step": 74496
    },
    {
      "epoch": 0.000270476903098688,
      "grad_norm": 9602.030826861575,
      "learning_rate": 3.664872743305685e-07,
      "loss": 1.7512,
      "step": 74528
    },
    {
      "epoch": 0.0002705930374495247,
      "grad_norm": 9742.631779965823,
      "learning_rate": 3.6640854134646656e-07,
      "loss": 1.7584,
      "step": 74560
    },
    {
      "epoch": 0.0002707091718003614,
      "grad_norm": 9673.924849821813,
      "learning_rate": 3.663298590835301e-07,
      "loss": 1.765,
      "step": 74592
    },
    {
      "epoch": 0.0002708253061511982,
      "grad_norm": 10413.446979746906,
      "learning_rate": 3.6625122748732325e-07,
      "loss": 1.7627,
      "step": 74624
    },
    {
      "epoch": 0.0002709414405020349,
      "grad_norm": 10270.179550523933,
      "learning_rate": 3.66172646503492e-07,
      "loss": 1.795,
      "step": 74656
    },
    {
      "epoch": 0.0002710575748528716,
      "grad_norm": 11269.843299709184,
      "learning_rate": 3.660941160777641e-07,
      "loss": 1.8204,
      "step": 74688
    },
    {
      "epoch": 0.0002711737092037083,
      "grad_norm": 9827.866401208352,
      "learning_rate": 3.660156361559486e-07,
      "loss": 1.8177,
      "step": 74720
    },
    {
      "epoch": 0.000271289843554545,
      "grad_norm": 9456.132190277376,
      "learning_rate": 3.659372066839358e-07,
      "loss": 1.7916,
      "step": 74752
    },
    {
      "epoch": 0.0002714059779053817,
      "grad_norm": 10007.073798069043,
      "learning_rate": 3.6585882760769707e-07,
      "loss": 1.7674,
      "step": 74784
    },
    {
      "epoch": 0.0002715221122562184,
      "grad_norm": 10377.625643662426,
      "learning_rate": 3.6578049887328514e-07,
      "loss": 1.7833,
      "step": 74816
    },
    {
      "epoch": 0.0002716382466070551,
      "grad_norm": 10459.220143012577,
      "learning_rate": 3.657022204268331e-07,
      "loss": 1.7684,
      "step": 74848
    },
    {
      "epoch": 0.0002717543809578918,
      "grad_norm": 9336.827405494867,
      "learning_rate": 3.6562399221455486e-07,
      "loss": 1.774,
      "step": 74880
    },
    {
      "epoch": 0.00027187051530872853,
      "grad_norm": 9429.290535347822,
      "learning_rate": 3.65545814182745e-07,
      "loss": 1.7806,
      "step": 74912
    },
    {
      "epoch": 0.00027198664965956523,
      "grad_norm": 9543.936399620441,
      "learning_rate": 3.654676862777782e-07,
      "loss": 1.7636,
      "step": 74944
    },
    {
      "epoch": 0.00027210278401040193,
      "grad_norm": 10258.732475310973,
      "learning_rate": 3.6539204762093485e-07,
      "loss": 1.7616,
      "step": 74976
    },
    {
      "epoch": 0.00027221891836123863,
      "grad_norm": 9661.863484856323,
      "learning_rate": 3.6531401824678776e-07,
      "loss": 1.7773,
      "step": 75008
    },
    {
      "epoch": 0.00027233505271207533,
      "grad_norm": 9824.04804548512,
      "learning_rate": 3.652360388407557e-07,
      "loss": 1.7556,
      "step": 75040
    },
    {
      "epoch": 0.00027245118706291203,
      "grad_norm": 10295.992812740304,
      "learning_rate": 3.651581093495309e-07,
      "loss": 1.7523,
      "step": 75072
    },
    {
      "epoch": 0.00027256732141374873,
      "grad_norm": 8403.087884819484,
      "learning_rate": 3.650802297198849e-07,
      "loss": 1.7594,
      "step": 75104
    },
    {
      "epoch": 0.00027268345576458543,
      "grad_norm": 9316.89969893419,
      "learning_rate": 3.6500239989866905e-07,
      "loss": 1.7468,
      "step": 75136
    },
    {
      "epoch": 0.00027279959011542213,
      "grad_norm": 9029.715610139669,
      "learning_rate": 3.649246198328137e-07,
      "loss": 1.7567,
      "step": 75168
    },
    {
      "epoch": 0.0002729157244662589,
      "grad_norm": 9092.416400495524,
      "learning_rate": 3.6484688946932844e-07,
      "loss": 1.767,
      "step": 75200
    },
    {
      "epoch": 0.0002730318588170956,
      "grad_norm": 9900.813400928228,
      "learning_rate": 3.647692087553018e-07,
      "loss": 1.7721,
      "step": 75232
    },
    {
      "epoch": 0.0002731479931679323,
      "grad_norm": 10548.017823268976,
      "learning_rate": 3.646915776379011e-07,
      "loss": 1.7912,
      "step": 75264
    },
    {
      "epoch": 0.000273264127518769,
      "grad_norm": 9171.040508033971,
      "learning_rate": 3.6461399606437257e-07,
      "loss": 1.7919,
      "step": 75296
    },
    {
      "epoch": 0.0002733802618696057,
      "grad_norm": 11219.526371465063,
      "learning_rate": 3.6453646398204067e-07,
      "loss": 1.7764,
      "step": 75328
    },
    {
      "epoch": 0.0002734963962204424,
      "grad_norm": 10703.803062463361,
      "learning_rate": 3.644589813383083e-07,
      "loss": 1.7518,
      "step": 75360
    },
    {
      "epoch": 0.0002736125305712791,
      "grad_norm": 8763.703440897576,
      "learning_rate": 3.6438154808065674e-07,
      "loss": 1.7604,
      "step": 75392
    },
    {
      "epoch": 0.0002737286649221158,
      "grad_norm": 8852.840674043558,
      "learning_rate": 3.6430416415664515e-07,
      "loss": 1.788,
      "step": 75424
    },
    {
      "epoch": 0.0002738447992729525,
      "grad_norm": 10731.8809162234,
      "learning_rate": 3.6422682951391073e-07,
      "loss": 1.7876,
      "step": 75456
    },
    {
      "epoch": 0.00027396093362378924,
      "grad_norm": 9409.243965377877,
      "learning_rate": 3.641495441001684e-07,
      "loss": 1.7931,
      "step": 75488
    },
    {
      "epoch": 0.00027407706797462594,
      "grad_norm": 10437.018635606626,
      "learning_rate": 3.6407230786321075e-07,
      "loss": 1.7788,
      "step": 75520
    },
    {
      "epoch": 0.00027419320232546264,
      "grad_norm": 9048.252317436776,
      "learning_rate": 3.6399512075090785e-07,
      "loss": 1.7805,
      "step": 75552
    },
    {
      "epoch": 0.00027430933667629934,
      "grad_norm": 9302.023435790732,
      "learning_rate": 3.6391798271120706e-07,
      "loss": 1.8007,
      "step": 75584
    },
    {
      "epoch": 0.00027442547102713604,
      "grad_norm": 9581.172057739075,
      "learning_rate": 3.638408936921329e-07,
      "loss": 1.8046,
      "step": 75616
    },
    {
      "epoch": 0.00027454160537797274,
      "grad_norm": 8441.603520658855,
      "learning_rate": 3.637638536417871e-07,
      "loss": 1.7744,
      "step": 75648
    },
    {
      "epoch": 0.00027465773972880944,
      "grad_norm": 9434.655478606517,
      "learning_rate": 3.636868625083481e-07,
      "loss": 1.7743,
      "step": 75680
    },
    {
      "epoch": 0.00027477387407964614,
      "grad_norm": 9834.040166686325,
      "learning_rate": 3.6360992024007104e-07,
      "loss": 1.77,
      "step": 75712
    },
    {
      "epoch": 0.00027489000843048284,
      "grad_norm": 9228.162005513339,
      "learning_rate": 3.63533026785288e-07,
      "loss": 1.7465,
      "step": 75744
    },
    {
      "epoch": 0.0002750061427813196,
      "grad_norm": 9608.504358119426,
      "learning_rate": 3.6345618209240703e-07,
      "loss": 1.7579,
      "step": 75776
    },
    {
      "epoch": 0.0002751222771321563,
      "grad_norm": 11758.774425934023,
      "learning_rate": 3.633793861099128e-07,
      "loss": 1.7541,
      "step": 75808
    },
    {
      "epoch": 0.000275238411482993,
      "grad_norm": 10294.827633331215,
      "learning_rate": 3.633026387863661e-07,
      "loss": 1.7743,
      "step": 75840
    },
    {
      "epoch": 0.0002753545458338297,
      "grad_norm": 10583.827474028476,
      "learning_rate": 3.6322594007040373e-07,
      "loss": 1.7917,
      "step": 75872
    },
    {
      "epoch": 0.0002754706801846664,
      "grad_norm": 9956.418834098935,
      "learning_rate": 3.631492899107383e-07,
      "loss": 1.7841,
      "step": 75904
    },
    {
      "epoch": 0.0002755868145355031,
      "grad_norm": 10254.613400806487,
      "learning_rate": 3.6307268825615816e-07,
      "loss": 1.7639,
      "step": 75936
    },
    {
      "epoch": 0.0002757029488863398,
      "grad_norm": 18446.072102211896,
      "learning_rate": 3.629961350555273e-07,
      "loss": 1.7607,
      "step": 75968
    },
    {
      "epoch": 0.0002758190832371765,
      "grad_norm": 9382.039117377415,
      "learning_rate": 3.629220203005604e-07,
      "loss": 1.7575,
      "step": 76000
    },
    {
      "epoch": 0.0002759352175880132,
      "grad_norm": 10085.119731564915,
      "learning_rate": 3.628455623444957e-07,
      "loss": 1.7718,
      "step": 76032
    },
    {
      "epoch": 0.00027605135193884995,
      "grad_norm": 9593.931936385623,
      "learning_rate": 3.6276915269101404e-07,
      "loss": 1.7444,
      "step": 76064
    },
    {
      "epoch": 0.00027616748628968665,
      "grad_norm": 9848.22095609151,
      "learning_rate": 3.6269279128927804e-07,
      "loss": 1.7512,
      "step": 76096
    },
    {
      "epoch": 0.00027628362064052335,
      "grad_norm": 9672.809416090033,
      "learning_rate": 3.62616478088525e-07,
      "loss": 1.753,
      "step": 76128
    },
    {
      "epoch": 0.00027639975499136005,
      "grad_norm": 9732.37237265406,
      "learning_rate": 3.625402130380671e-07,
      "loss": 1.7618,
      "step": 76160
    },
    {
      "epoch": 0.00027651588934219675,
      "grad_norm": 9301.615773616968,
      "learning_rate": 3.6246399608729105e-07,
      "loss": 1.7909,
      "step": 76192
    },
    {
      "epoch": 0.00027663202369303345,
      "grad_norm": 9109.785068814741,
      "learning_rate": 3.623878271856582e-07,
      "loss": 1.8004,
      "step": 76224
    },
    {
      "epoch": 0.00027674815804387015,
      "grad_norm": 9426.302350338652,
      "learning_rate": 3.623117062827038e-07,
      "loss": 1.7904,
      "step": 76256
    },
    {
      "epoch": 0.00027686429239470685,
      "grad_norm": 10128.616884846617,
      "learning_rate": 3.6223563332803774e-07,
      "loss": 1.7815,
      "step": 76288
    },
    {
      "epoch": 0.00027698042674554355,
      "grad_norm": 9335.703937036564,
      "learning_rate": 3.621596082713435e-07,
      "loss": 1.7858,
      "step": 76320
    },
    {
      "epoch": 0.0002770965610963803,
      "grad_norm": 10956.283402687244,
      "learning_rate": 3.6208363106237896e-07,
      "loss": 1.7895,
      "step": 76352
    },
    {
      "epoch": 0.000277212695447217,
      "grad_norm": 8835.56132908374,
      "learning_rate": 3.620077016509752e-07,
      "loss": 1.7892,
      "step": 76384
    },
    {
      "epoch": 0.0002773288297980537,
      "grad_norm": 8587.031151684498,
      "learning_rate": 3.6193181998703727e-07,
      "loss": 1.7937,
      "step": 76416
    },
    {
      "epoch": 0.0002774449641488904,
      "grad_norm": 9192.342574121136,
      "learning_rate": 3.618559860205436e-07,
      "loss": 1.8091,
      "step": 76448
    },
    {
      "epoch": 0.0002775610984997271,
      "grad_norm": 10133.055807603154,
      "learning_rate": 3.6178019970154597e-07,
      "loss": 1.7938,
      "step": 76480
    },
    {
      "epoch": 0.0002776772328505638,
      "grad_norm": 10565.57901868137,
      "learning_rate": 3.617044609801693e-07,
      "loss": 1.7626,
      "step": 76512
    },
    {
      "epoch": 0.0002777933672014005,
      "grad_norm": 8322.006729148925,
      "learning_rate": 3.616287698066116e-07,
      "loss": 1.7594,
      "step": 76544
    },
    {
      "epoch": 0.0002779095015522372,
      "grad_norm": 9649.117265325362,
      "learning_rate": 3.6155312613114385e-07,
      "loss": 1.7419,
      "step": 76576
    },
    {
      "epoch": 0.0002780256359030739,
      "grad_norm": 11703.13325567132,
      "learning_rate": 3.614775299041098e-07,
      "loss": 1.7515,
      "step": 76608
    },
    {
      "epoch": 0.00027814177025391065,
      "grad_norm": 8772.365701451348,
      "learning_rate": 3.614019810759257e-07,
      "loss": 1.7595,
      "step": 76640
    },
    {
      "epoch": 0.00027825790460474735,
      "grad_norm": 10431.486471256145,
      "learning_rate": 3.6132647959708057e-07,
      "loss": 1.7502,
      "step": 76672
    },
    {
      "epoch": 0.00027837403895558405,
      "grad_norm": 10546.927514684076,
      "learning_rate": 3.612510254181356e-07,
      "loss": 1.7596,
      "step": 76704
    },
    {
      "epoch": 0.00027849017330642075,
      "grad_norm": 8467.666856932907,
      "learning_rate": 3.611756184897242e-07,
      "loss": 1.7808,
      "step": 76736
    },
    {
      "epoch": 0.00027860630765725745,
      "grad_norm": 10205.60591047881,
      "learning_rate": 3.611002587625521e-07,
      "loss": 1.767,
      "step": 76768
    },
    {
      "epoch": 0.00027872244200809415,
      "grad_norm": 10497.915221604717,
      "learning_rate": 3.610249461873968e-07,
      "loss": 1.7552,
      "step": 76800
    },
    {
      "epoch": 0.00027883857635893085,
      "grad_norm": 8677.568438220467,
      "learning_rate": 3.609496807151078e-07,
      "loss": 1.7668,
      "step": 76832
    },
    {
      "epoch": 0.00027895471070976755,
      "grad_norm": 9249.959999913513,
      "learning_rate": 3.60874462296606e-07,
      "loss": 1.7574,
      "step": 76864
    },
    {
      "epoch": 0.00027907084506060425,
      "grad_norm": 10430.484648375646,
      "learning_rate": 3.6079929088288417e-07,
      "loss": 1.7403,
      "step": 76896
    },
    {
      "epoch": 0.000279186979411441,
      "grad_norm": 8842.354550683885,
      "learning_rate": 3.607241664250064e-07,
      "loss": 1.7617,
      "step": 76928
    },
    {
      "epoch": 0.0002793031137622777,
      "grad_norm": 8491.843616082435,
      "learning_rate": 3.6064908887410805e-07,
      "loss": 1.7844,
      "step": 76960
    },
    {
      "epoch": 0.0002794192481131144,
      "grad_norm": 10147.926684796259,
      "learning_rate": 3.6057640218174795e-07,
      "loss": 1.7879,
      "step": 76992
    },
    {
      "epoch": 0.0002795353824639511,
      "grad_norm": 8414.552156829262,
      "learning_rate": 3.6050141683644015e-07,
      "loss": 1.7988,
      "step": 77024
    },
    {
      "epoch": 0.0002796515168147878,
      "grad_norm": 9223.166918146933,
      "learning_rate": 3.604264782534637e-07,
      "loss": 1.8044,
      "step": 77056
    },
    {
      "epoch": 0.0002797676511656245,
      "grad_norm": 10877.903290616257,
      "learning_rate": 3.603515863842355e-07,
      "loss": 1.7919,
      "step": 77088
    },
    {
      "epoch": 0.0002798837855164612,
      "grad_norm": 10113.616563821273,
      "learning_rate": 3.6027674118024324e-07,
      "loss": 1.7908,
      "step": 77120
    },
    {
      "epoch": 0.0002799999198672979,
      "grad_norm": 10557.983424878068,
      "learning_rate": 3.602019425930452e-07,
      "loss": 1.8026,
      "step": 77152
    },
    {
      "epoch": 0.0002801160542181346,
      "grad_norm": 8467.364879347057,
      "learning_rate": 3.6012719057426976e-07,
      "loss": 1.7829,
      "step": 77184
    },
    {
      "epoch": 0.00028023218856897136,
      "grad_norm": 8534.296807587605,
      "learning_rate": 3.6005248507561576e-07,
      "loss": 1.779,
      "step": 77216
    },
    {
      "epoch": 0.00028034832291980806,
      "grad_norm": 9209.594127864702,
      "learning_rate": 3.59977826048852e-07,
      "loss": 1.7788,
      "step": 77248
    },
    {
      "epoch": 0.00028046445727064476,
      "grad_norm": 8891.087222606693,
      "learning_rate": 3.599032134458175e-07,
      "loss": 1.7564,
      "step": 77280
    },
    {
      "epoch": 0.00028058059162148146,
      "grad_norm": 9401.959902062974,
      "learning_rate": 3.598286472184208e-07,
      "loss": 1.7666,
      "step": 77312
    },
    {
      "epoch": 0.00028069672597231816,
      "grad_norm": 8510.70666866154,
      "learning_rate": 3.597541273186405e-07,
      "loss": 1.7508,
      "step": 77344
    },
    {
      "epoch": 0.00028081286032315486,
      "grad_norm": 9373.906122849748,
      "learning_rate": 3.5967965369852437e-07,
      "loss": 1.754,
      "step": 77376
    },
    {
      "epoch": 0.00028092899467399156,
      "grad_norm": 8943.878465185,
      "learning_rate": 3.596052263101901e-07,
      "loss": 1.7471,
      "step": 77408
    },
    {
      "epoch": 0.00028104512902482826,
      "grad_norm": 9206.606540957424,
      "learning_rate": 3.5953084510582426e-07,
      "loss": 1.7656,
      "step": 77440
    },
    {
      "epoch": 0.00028116126337566496,
      "grad_norm": 8598.992615417226,
      "learning_rate": 3.594565100376831e-07,
      "loss": 1.7728,
      "step": 77472
    },
    {
      "epoch": 0.0002812773977265017,
      "grad_norm": 10207.398591218038,
      "learning_rate": 3.593822210580915e-07,
      "loss": 1.7546,
      "step": 77504
    },
    {
      "epoch": 0.0002813935320773384,
      "grad_norm": 9845.038547410568,
      "learning_rate": 3.593079781194435e-07,
      "loss": 1.7733,
      "step": 77536
    },
    {
      "epoch": 0.0002815096664281751,
      "grad_norm": 9805.236968069667,
      "learning_rate": 3.59233781174202e-07,
      "loss": 1.7664,
      "step": 77568
    },
    {
      "epoch": 0.0002816258007790118,
      "grad_norm": 10135.367975559644,
      "learning_rate": 3.5915963017489846e-07,
      "loss": 1.7814,
      "step": 77600
    },
    {
      "epoch": 0.0002817419351298485,
      "grad_norm": 11228.920607075284,
      "learning_rate": 3.590855250741329e-07,
      "loss": 1.7646,
      "step": 77632
    },
    {
      "epoch": 0.0002818580694806852,
      "grad_norm": 9321.048653450962,
      "learning_rate": 3.590114658245738e-07,
      "loss": 1.7661,
      "step": 77664
    },
    {
      "epoch": 0.0002819742038315219,
      "grad_norm": 9717.186012421498,
      "learning_rate": 3.589374523789582e-07,
      "loss": 1.7604,
      "step": 77696
    },
    {
      "epoch": 0.0002820903381823586,
      "grad_norm": 10474.013270948248,
      "learning_rate": 3.588634846900908e-07,
      "loss": 1.7802,
      "step": 77728
    },
    {
      "epoch": 0.0002822064725331953,
      "grad_norm": 8277.770714389231,
      "learning_rate": 3.5878956271084476e-07,
      "loss": 1.7866,
      "step": 77760
    },
    {
      "epoch": 0.00028232260688403207,
      "grad_norm": 10569.537549013203,
      "learning_rate": 3.5871568639416106e-07,
      "loss": 1.7733,
      "step": 77792
    },
    {
      "epoch": 0.00028243874123486877,
      "grad_norm": 9341.305797371158,
      "learning_rate": 3.586418556930484e-07,
      "loss": 1.7793,
      "step": 77824
    },
    {
      "epoch": 0.00028255487558570547,
      "grad_norm": 10127.277719110896,
      "learning_rate": 3.5856807056058324e-07,
      "loss": 1.7804,
      "step": 77856
    },
    {
      "epoch": 0.00028267100993654217,
      "grad_norm": 8867.021596906145,
      "learning_rate": 3.5849433094990956e-07,
      "loss": 1.7837,
      "step": 77888
    },
    {
      "epoch": 0.00028278714428737887,
      "grad_norm": 9873.831475167075,
      "learning_rate": 3.584206368142387e-07,
      "loss": 1.7808,
      "step": 77920
    },
    {
      "epoch": 0.00028290327863821557,
      "grad_norm": 9807.419028470233,
      "learning_rate": 3.583469881068495e-07,
      "loss": 1.7829,
      "step": 77952
    },
    {
      "epoch": 0.00028301941298905227,
      "grad_norm": 10628.954605228117,
      "learning_rate": 3.5827568419855185e-07,
      "loss": 1.78,
      "step": 77984
    },
    {
      "epoch": 0.00028313554733988897,
      "grad_norm": 9329.88681603373,
      "learning_rate": 3.5820212479181497e-07,
      "loss": 1.7559,
      "step": 78016
    },
    {
      "epoch": 0.00028325168169072567,
      "grad_norm": 8761.70862332228,
      "learning_rate": 3.581286106750512e-07,
      "loss": 1.771,
      "step": 78048
    },
    {
      "epoch": 0.0002833678160415624,
      "grad_norm": 9351.362681449158,
      "learning_rate": 3.5805514180180496e-07,
      "loss": 1.7677,
      "step": 78080
    },
    {
      "epoch": 0.0002834839503923991,
      "grad_norm": 9399.970425485391,
      "learning_rate": 3.5798171812568753e-07,
      "loss": 1.7647,
      "step": 78112
    },
    {
      "epoch": 0.0002836000847432358,
      "grad_norm": 9074.877630028957,
      "learning_rate": 3.5790833960037676e-07,
      "loss": 1.7696,
      "step": 78144
    },
    {
      "epoch": 0.0002837162190940725,
      "grad_norm": 9514.936888913136,
      "learning_rate": 3.578350061796169e-07,
      "loss": 1.7847,
      "step": 78176
    },
    {
      "epoch": 0.0002838323534449092,
      "grad_norm": 7475.219194110631,
      "learning_rate": 3.5776171781721864e-07,
      "loss": 1.7703,
      "step": 78208
    },
    {
      "epoch": 0.0002839484877957459,
      "grad_norm": 10630.319092106314,
      "learning_rate": 3.576884744670585e-07,
      "loss": 1.7788,
      "step": 78240
    },
    {
      "epoch": 0.0002840646221465826,
      "grad_norm": 9560.69610436395,
      "learning_rate": 3.576152760830795e-07,
      "loss": 1.7643,
      "step": 78272
    },
    {
      "epoch": 0.0002841807564974193,
      "grad_norm": 9053.92202307928,
      "learning_rate": 3.5754212261929044e-07,
      "loss": 1.7546,
      "step": 78304
    },
    {
      "epoch": 0.000284296890848256,
      "grad_norm": 10681.939337030519,
      "learning_rate": 3.574690140297658e-07,
      "loss": 1.7566,
      "step": 78336
    },
    {
      "epoch": 0.0002844130251990927,
      "grad_norm": 8929.638514520058,
      "learning_rate": 3.5739595026864606e-07,
      "loss": 1.7553,
      "step": 78368
    },
    {
      "epoch": 0.0002845291595499295,
      "grad_norm": 8611.253451153321,
      "learning_rate": 3.57322931290137e-07,
      "loss": 1.7555,
      "step": 78400
    },
    {
      "epoch": 0.0002846452939007662,
      "grad_norm": 9193.164199556102,
      "learning_rate": 3.572499570485101e-07,
      "loss": 1.767,
      "step": 78432
    },
    {
      "epoch": 0.0002847614282516029,
      "grad_norm": 10400.148460478822,
      "learning_rate": 3.57177027498102e-07,
      "loss": 1.7794,
      "step": 78464
    },
    {
      "epoch": 0.0002848775626024396,
      "grad_norm": 10066.48478864395,
      "learning_rate": 3.571041425933147e-07,
      "loss": 1.7712,
      "step": 78496
    },
    {
      "epoch": 0.0002849936969532763,
      "grad_norm": 8404.871563563598,
      "learning_rate": 3.5703130228861533e-07,
      "loss": 1.7723,
      "step": 78528
    },
    {
      "epoch": 0.000285109831304113,
      "grad_norm": 9815.000356597038,
      "learning_rate": 3.5695850653853573e-07,
      "loss": 1.7718,
      "step": 78560
    },
    {
      "epoch": 0.0002852259656549497,
      "grad_norm": 9108.241872062907,
      "learning_rate": 3.5688575529767297e-07,
      "loss": 1.7721,
      "step": 78592
    },
    {
      "epoch": 0.0002853421000057864,
      "grad_norm": 9504.558958731332,
      "learning_rate": 3.5681304852068864e-07,
      "loss": 1.7695,
      "step": 78624
    },
    {
      "epoch": 0.0002854582343566231,
      "grad_norm": 9054.33321675318,
      "learning_rate": 3.5674038616230906e-07,
      "loss": 1.7861,
      "step": 78656
    },
    {
      "epoch": 0.00028557436870745983,
      "grad_norm": 8925.995070578965,
      "learning_rate": 3.56667768177325e-07,
      "loss": 1.7922,
      "step": 78688
    },
    {
      "epoch": 0.00028569050305829653,
      "grad_norm": 9602.91018389738,
      "learning_rate": 3.565951945205917e-07,
      "loss": 1.7834,
      "step": 78720
    },
    {
      "epoch": 0.00028580663740913323,
      "grad_norm": 9689.684411785556,
      "learning_rate": 3.5652266514702856e-07,
      "loss": 1.782,
      "step": 78752
    },
    {
      "epoch": 0.00028592277175996993,
      "grad_norm": 8681.614941933327,
      "learning_rate": 3.564501800116192e-07,
      "loss": 1.7743,
      "step": 78784
    },
    {
      "epoch": 0.00028603890611080663,
      "grad_norm": 8332.592993780507,
      "learning_rate": 3.563777390694114e-07,
      "loss": 1.7734,
      "step": 78816
    },
    {
      "epoch": 0.00028615504046164333,
      "grad_norm": 8995.86371617534,
      "learning_rate": 3.563053422755166e-07,
      "loss": 1.7668,
      "step": 78848
    },
    {
      "epoch": 0.00028627117481248,
      "grad_norm": 9129.677102723841,
      "learning_rate": 3.562329895851103e-07,
      "loss": 1.7602,
      "step": 78880
    },
    {
      "epoch": 0.0002863873091633167,
      "grad_norm": 10489.424388401874,
      "learning_rate": 3.561606809534316e-07,
      "loss": 1.7549,
      "step": 78912
    },
    {
      "epoch": 0.0002865034435141534,
      "grad_norm": 9079.07330072844,
      "learning_rate": 3.5608841633578306e-07,
      "loss": 1.761,
      "step": 78944
    },
    {
      "epoch": 0.0002866195778649902,
      "grad_norm": 9799.963061154875,
      "learning_rate": 3.5601619568753087e-07,
      "loss": 1.7872,
      "step": 78976
    },
    {
      "epoch": 0.0002867357122158269,
      "grad_norm": 9798.764309850503,
      "learning_rate": 3.5594627382227607e-07,
      "loss": 1.7982,
      "step": 79008
    },
    {
      "epoch": 0.0002868518465666636,
      "grad_norm": 11093.884531578648,
      "learning_rate": 3.5587413960858116e-07,
      "loss": 1.7806,
      "step": 79040
    },
    {
      "epoch": 0.0002869679809175003,
      "grad_norm": 10896.121328252544,
      "learning_rate": 3.558020492321485e-07,
      "loss": 1.7476,
      "step": 79072
    },
    {
      "epoch": 0.000287084115268337,
      "grad_norm": 9485.617112238928,
      "learning_rate": 3.5573000264859486e-07,
      "loss": 1.7472,
      "step": 79104
    },
    {
      "epoch": 0.0002872002496191737,
      "grad_norm": 9843.904103555662,
      "learning_rate": 3.556579998135999e-07,
      "loss": 1.7458,
      "step": 79136
    },
    {
      "epoch": 0.0002873163839700104,
      "grad_norm": 9351.588421225562,
      "learning_rate": 3.5558604068290624e-07,
      "loss": 1.7368,
      "step": 79168
    },
    {
      "epoch": 0.0002874325183208471,
      "grad_norm": 11245.795125290164,
      "learning_rate": 3.555141252123188e-07,
      "loss": 1.7708,
      "step": 79200
    },
    {
      "epoch": 0.0002875486526716838,
      "grad_norm": 9473.69473859064,
      "learning_rate": 3.5544225335770543e-07,
      "loss": 1.7652,
      "step": 79232
    },
    {
      "epoch": 0.00028766478702252053,
      "grad_norm": 9940.615876292575,
      "learning_rate": 3.5537042507499613e-07,
      "loss": 1.7914,
      "step": 79264
    },
    {
      "epoch": 0.00028778092137335723,
      "grad_norm": 8749.186019282022,
      "learning_rate": 3.552986403201834e-07,
      "loss": 1.8056,
      "step": 79296
    },
    {
      "epoch": 0.00028789705572419393,
      "grad_norm": 10059.88409475974,
      "learning_rate": 3.552268990493218e-07,
      "loss": 1.7959,
      "step": 79328
    },
    {
      "epoch": 0.00028801319007503063,
      "grad_norm": 10115.855870859372,
      "learning_rate": 3.5515520121852806e-07,
      "loss": 1.7788,
      "step": 79360
    },
    {
      "epoch": 0.00028812932442586733,
      "grad_norm": 8673.97625083214,
      "learning_rate": 3.550835467839809e-07,
      "loss": 1.7931,
      "step": 79392
    },
    {
      "epoch": 0.00028824545877670403,
      "grad_norm": 10225.235547409166,
      "learning_rate": 3.5501193570192083e-07,
      "loss": 1.7906,
      "step": 79424
    },
    {
      "epoch": 0.00028836159312754073,
      "grad_norm": 10331.817652281714,
      "learning_rate": 3.5494036792865017e-07,
      "loss": 1.7732,
      "step": 79456
    },
    {
      "epoch": 0.00028847772747837743,
      "grad_norm": 8311.810512758337,
      "learning_rate": 3.548688434205329e-07,
      "loss": 1.7629,
      "step": 79488
    },
    {
      "epoch": 0.00028859386182921413,
      "grad_norm": 10614.413313980194,
      "learning_rate": 3.5479736213399435e-07,
      "loss": 1.7536,
      "step": 79520
    },
    {
      "epoch": 0.0002887099961800509,
      "grad_norm": 9969.188532674061,
      "learning_rate": 3.5472592402552146e-07,
      "loss": 1.7472,
      "step": 79552
    },
    {
      "epoch": 0.0002888261305308876,
      "grad_norm": 9891.833803698888,
      "learning_rate": 3.5465452905166253e-07,
      "loss": 1.7774,
      "step": 79584
    },
    {
      "epoch": 0.0002889422648817243,
      "grad_norm": 10151.45733380188,
      "learning_rate": 3.545831771690268e-07,
      "loss": 1.7781,
      "step": 79616
    },
    {
      "epoch": 0.000289058399232561,
      "grad_norm": 9720.761389932375,
      "learning_rate": 3.545118683342848e-07,
      "loss": 1.7569,
      "step": 79648
    },
    {
      "epoch": 0.0002891745335833977,
      "grad_norm": 8854.023379232744,
      "learning_rate": 3.5444060250416794e-07,
      "loss": 1.7496,
      "step": 79680
    },
    {
      "epoch": 0.0002892906679342344,
      "grad_norm": 9858.680439085141,
      "learning_rate": 3.5436937963546856e-07,
      "loss": 1.7672,
      "step": 79712
    },
    {
      "epoch": 0.0002894068022850711,
      "grad_norm": 9171.561262947547,
      "learning_rate": 3.542981996850397e-07,
      "loss": 1.7616,
      "step": 79744
    },
    {
      "epoch": 0.0002895229366359078,
      "grad_norm": 8817.022173046862,
      "learning_rate": 3.54227062609795e-07,
      "loss": 1.7707,
      "step": 79776
    },
    {
      "epoch": 0.0002896390709867445,
      "grad_norm": 8439.53517677366,
      "learning_rate": 3.5415596836670875e-07,
      "loss": 1.7803,
      "step": 79808
    },
    {
      "epoch": 0.00028975520533758124,
      "grad_norm": 10028.719359918294,
      "learning_rate": 3.5408491691281553e-07,
      "loss": 1.757,
      "step": 79840
    },
    {
      "epoch": 0.00028987133968841794,
      "grad_norm": 9328.17817154025,
      "learning_rate": 3.540139082052104e-07,
      "loss": 1.7708,
      "step": 79872
    },
    {
      "epoch": 0.00028998747403925464,
      "grad_norm": 10243.238159878936,
      "learning_rate": 3.539429422010484e-07,
      "loss": 1.7868,
      "step": 79904
    },
    {
      "epoch": 0.00029010360839009134,
      "grad_norm": 10531.057116928005,
      "learning_rate": 3.538720188575449e-07,
      "loss": 1.7618,
      "step": 79936
    },
    {
      "epoch": 0.00029021974274092804,
      "grad_norm": 8470.270951982588,
      "learning_rate": 3.5380113813197504e-07,
      "loss": 1.7825,
      "step": 79968
    },
    {
      "epoch": 0.00029033587709176474,
      "grad_norm": 10927.044065070846,
      "learning_rate": 3.537325130298445e-07,
      "loss": 1.7836,
      "step": 80000
    },
    {
      "epoch": 0.00029045201144260144,
      "grad_norm": 8388.29410547818,
      "learning_rate": 3.5366171608370584e-07,
      "loss": 1.7803,
      "step": 80032
    },
    {
      "epoch": 0.00029056814579343814,
      "grad_norm": 8915.031127259175,
      "learning_rate": 3.5359096162901415e-07,
      "loss": 1.7686,
      "step": 80064
    },
    {
      "epoch": 0.00029068428014427484,
      "grad_norm": 9138.493420690305,
      "learning_rate": 3.535202496232818e-07,
      "loss": 1.7659,
      "step": 80096
    },
    {
      "epoch": 0.0002908004144951116,
      "grad_norm": 10307.516674737908,
      "learning_rate": 3.534495800240803e-07,
      "loss": 1.7682,
      "step": 80128
    },
    {
      "epoch": 0.0002909165488459483,
      "grad_norm": 10014.714673918572,
      "learning_rate": 3.5337895278904085e-07,
      "loss": 1.7852,
      "step": 80160
    },
    {
      "epoch": 0.000291032683196785,
      "grad_norm": 9715.924865909576,
      "learning_rate": 3.5330836787585376e-07,
      "loss": 1.8082,
      "step": 80192
    },
    {
      "epoch": 0.0002911488175476217,
      "grad_norm": 9573.080068609059,
      "learning_rate": 3.5323782524226847e-07,
      "loss": 1.7851,
      "step": 80224
    },
    {
      "epoch": 0.0002912649518984584,
      "grad_norm": 9868.43320897497,
      "learning_rate": 3.531673248460934e-07,
      "loss": 1.7665,
      "step": 80256
    },
    {
      "epoch": 0.0002913810862492951,
      "grad_norm": 8910.08035878465,
      "learning_rate": 3.53096866645196e-07,
      "loss": 1.7367,
      "step": 80288
    },
    {
      "epoch": 0.0002914972206001318,
      "grad_norm": 9178.11603761905,
      "learning_rate": 3.5302645059750246e-07,
      "loss": 1.749,
      "step": 80320
    },
    {
      "epoch": 0.0002916133549509685,
      "grad_norm": 9668.997569551871,
      "learning_rate": 3.5295607666099774e-07,
      "loss": 1.7377,
      "step": 80352
    },
    {
      "epoch": 0.0002917294893018052,
      "grad_norm": 10609.495746735563,
      "learning_rate": 3.528857447937253e-07,
      "loss": 1.7477,
      "step": 80384
    },
    {
      "epoch": 0.00029184562365264195,
      "grad_norm": 9024.225617746932,
      "learning_rate": 3.528154549537872e-07,
      "loss": 1.7624,
      "step": 80416
    },
    {
      "epoch": 0.00029196175800347865,
      "grad_norm": 10395.703535595849,
      "learning_rate": 3.5274520709934385e-07,
      "loss": 1.7534,
      "step": 80448
    },
    {
      "epoch": 0.00029207789235431535,
      "grad_norm": 9468.678999733807,
      "learning_rate": 3.52675001188614e-07,
      "loss": 1.7706,
      "step": 80480
    },
    {
      "epoch": 0.00029219402670515205,
      "grad_norm": 8925.82018640304,
      "learning_rate": 3.526048371798744e-07,
      "loss": 1.7919,
      "step": 80512
    },
    {
      "epoch": 0.00029231016105598875,
      "grad_norm": 8081.882082782451,
      "learning_rate": 3.5253471503146023e-07,
      "loss": 1.7992,
      "step": 80544
    },
    {
      "epoch": 0.00029242629540682545,
      "grad_norm": 9846.579406067876,
      "learning_rate": 3.524646347017643e-07,
      "loss": 1.7833,
      "step": 80576
    },
    {
      "epoch": 0.00029254242975766215,
      "grad_norm": 9639.891700636475,
      "learning_rate": 3.523945961492376e-07,
      "loss": 1.7632,
      "step": 80608
    },
    {
      "epoch": 0.00029265856410849885,
      "grad_norm": 9071.147005754014,
      "learning_rate": 3.5232459933238857e-07,
      "loss": 1.7626,
      "step": 80640
    },
    {
      "epoch": 0.00029277469845933555,
      "grad_norm": 9611.533072304335,
      "learning_rate": 3.5225464420978344e-07,
      "loss": 1.7517,
      "step": 80672
    },
    {
      "epoch": 0.0002928908328101723,
      "grad_norm": 9063.930273341692,
      "learning_rate": 3.521847307400462e-07,
      "loss": 1.7592,
      "step": 80704
    },
    {
      "epoch": 0.000293006967161009,
      "grad_norm": 10157.734983745146,
      "learning_rate": 3.521148588818581e-07,
      "loss": 1.7888,
      "step": 80736
    },
    {
      "epoch": 0.0002931231015118457,
      "grad_norm": 9236.826511307874,
      "learning_rate": 3.520450285939578e-07,
      "loss": 1.7895,
      "step": 80768
    },
    {
      "epoch": 0.0002932392358626824,
      "grad_norm": 10561.931736192959,
      "learning_rate": 3.5197523983514123e-07,
      "loss": 1.7822,
      "step": 80800
    },
    {
      "epoch": 0.0002933553702135191,
      "grad_norm": 10315.68805267007,
      "learning_rate": 3.519054925642614e-07,
      "loss": 1.7904,
      "step": 80832
    },
    {
      "epoch": 0.0002934715045643558,
      "grad_norm": 9200.110434119799,
      "learning_rate": 3.518357867402284e-07,
      "loss": 1.7681,
      "step": 80864
    },
    {
      "epoch": 0.0002935876389151925,
      "grad_norm": 10203.170389638703,
      "learning_rate": 3.5176612232200936e-07,
      "loss": 1.7663,
      "step": 80896
    },
    {
      "epoch": 0.0002937037732660292,
      "grad_norm": 9507.822463634879,
      "learning_rate": 3.5169649926862825e-07,
      "loss": 1.7656,
      "step": 80928
    },
    {
      "epoch": 0.0002938199076168659,
      "grad_norm": 9019.666956157527,
      "learning_rate": 3.5162691753916566e-07,
      "loss": 1.7725,
      "step": 80960
    },
    {
      "epoch": 0.00029393604196770265,
      "grad_norm": 9100.8204025791,
      "learning_rate": 3.5155954960722464e-07,
      "loss": 1.7695,
      "step": 80992
    },
    {
      "epoch": 0.00029405217631853935,
      "grad_norm": 9357.25237449541,
      "learning_rate": 3.514900491148644e-07,
      "loss": 1.7749,
      "step": 81024
    },
    {
      "epoch": 0.00029416831066937605,
      "grad_norm": 10615.880933770877,
      "learning_rate": 3.514205898252768e-07,
      "loss": 1.7499,
      "step": 81056
    },
    {
      "epoch": 0.00029428444502021275,
      "grad_norm": 8848.963329113756,
      "learning_rate": 3.513511716977668e-07,
      "loss": 1.7532,
      "step": 81088
    },
    {
      "epoch": 0.00029440057937104945,
      "grad_norm": 10333.692757189949,
      "learning_rate": 3.512817946916953e-07,
      "loss": 1.7642,
      "step": 81120
    },
    {
      "epoch": 0.00029451671372188615,
      "grad_norm": 10383.787940823908,
      "learning_rate": 3.512124587664799e-07,
      "loss": 1.7866,
      "step": 81152
    },
    {
      "epoch": 0.00029463284807272285,
      "grad_norm": 9673.910274547723,
      "learning_rate": 3.511431638815938e-07,
      "loss": 1.7653,
      "step": 81184
    },
    {
      "epoch": 0.00029474898242355955,
      "grad_norm": 10560.437396244532,
      "learning_rate": 3.5107390999656626e-07,
      "loss": 1.7427,
      "step": 81216
    },
    {
      "epoch": 0.00029486511677439625,
      "grad_norm": 9319.036216261851,
      "learning_rate": 3.5100469707098236e-07,
      "loss": 1.7627,
      "step": 81248
    },
    {
      "epoch": 0.000294981251125233,
      "grad_norm": 8646.727820395412,
      "learning_rate": 3.509355250644833e-07,
      "loss": 1.7671,
      "step": 81280
    },
    {
      "epoch": 0.0002950973854760697,
      "grad_norm": 9451.54452986389,
      "learning_rate": 3.508663939367653e-07,
      "loss": 1.7686,
      "step": 81312
    },
    {
      "epoch": 0.0002952135198269064,
      "grad_norm": 9650.688369230456,
      "learning_rate": 3.507973036475806e-07,
      "loss": 1.7735,
      "step": 81344
    },
    {
      "epoch": 0.0002953296541777431,
      "grad_norm": 9774.57835407748,
      "learning_rate": 3.507282541567367e-07,
      "loss": 1.7603,
      "step": 81376
    },
    {
      "epoch": 0.0002954457885285798,
      "grad_norm": 10176.747220993553,
      "learning_rate": 3.506592454240967e-07,
      "loss": 1.7643,
      "step": 81408
    },
    {
      "epoch": 0.0002955619228794165,
      "grad_norm": 9898.148715795292,
      "learning_rate": 3.5059027740957856e-07,
      "loss": 1.7697,
      "step": 81440
    },
    {
      "epoch": 0.0002956780572302532,
      "grad_norm": 9094.924298750375,
      "learning_rate": 3.505213500731557e-07,
      "loss": 1.7692,
      "step": 81472
    },
    {
      "epoch": 0.0002957941915810899,
      "grad_norm": 8789.342637535528,
      "learning_rate": 3.5045246337485655e-07,
      "loss": 1.777,
      "step": 81504
    },
    {
      "epoch": 0.0002959103259319266,
      "grad_norm": 9262.23806647184,
      "learning_rate": 3.5038361727476445e-07,
      "loss": 1.7832,
      "step": 81536
    },
    {
      "epoch": 0.00029602646028276336,
      "grad_norm": 10758.477587465617,
      "learning_rate": 3.503148117330176e-07,
      "loss": 1.786,
      "step": 81568
    },
    {
      "epoch": 0.00029614259463360006,
      "grad_norm": 10239.889550185588,
      "learning_rate": 3.5024604670980906e-07,
      "loss": 1.7963,
      "step": 81600
    },
    {
      "epoch": 0.00029625872898443676,
      "grad_norm": 10173.062075894357,
      "learning_rate": 3.501773221653866e-07,
      "loss": 1.7743,
      "step": 81632
    },
    {
      "epoch": 0.00029637486333527346,
      "grad_norm": 8340.734020456473,
      "learning_rate": 3.5010863806005224e-07,
      "loss": 1.7757,
      "step": 81664
    },
    {
      "epoch": 0.00029649099768611016,
      "grad_norm": 10065.77200218642,
      "learning_rate": 3.50039994354163e-07,
      "loss": 1.778,
      "step": 81696
    },
    {
      "epoch": 0.00029660713203694686,
      "grad_norm": 10056.171239592135,
      "learning_rate": 3.4997139100812984e-07,
      "loss": 1.8016,
      "step": 81728
    },
    {
      "epoch": 0.00029672326638778356,
      "grad_norm": 10144.61049030469,
      "learning_rate": 3.4990282798241826e-07,
      "loss": 1.8017,
      "step": 81760
    },
    {
      "epoch": 0.00029683940073862026,
      "grad_norm": 9844.04439242327,
      "learning_rate": 3.4983430523754785e-07,
      "loss": 1.7551,
      "step": 81792
    },
    {
      "epoch": 0.00029695553508945696,
      "grad_norm": 9967.564396581543,
      "learning_rate": 3.497658227340924e-07,
      "loss": 1.7412,
      "step": 81824
    },
    {
      "epoch": 0.0002970716694402937,
      "grad_norm": 9845.324778797294,
      "learning_rate": 3.496973804326796e-07,
      "loss": 1.7525,
      "step": 81856
    },
    {
      "epoch": 0.0002971878037911304,
      "grad_norm": 10036.66069965504,
      "learning_rate": 3.4962897829399104e-07,
      "loss": 1.7637,
      "step": 81888
    },
    {
      "epoch": 0.0002973039381419671,
      "grad_norm": 7875.989334680437,
      "learning_rate": 3.4956061627876217e-07,
      "loss": 1.75,
      "step": 81920
    },
    {
      "epoch": 0.0002974200724928038,
      "grad_norm": 9324.589535202073,
      "learning_rate": 3.494922943477822e-07,
      "loss": 1.7566,
      "step": 81952
    },
    {
      "epoch": 0.0002975362068436405,
      "grad_norm": 9320.419518455165,
      "learning_rate": 3.49424012461894e-07,
      "loss": 1.7383,
      "step": 81984
    },
    {
      "epoch": 0.0002976523411944772,
      "grad_norm": 9872.053687050127,
      "learning_rate": 3.4935790253556936e-07,
      "loss": 1.7608,
      "step": 82016
    },
    {
      "epoch": 0.0002977684755453139,
      "grad_norm": 10331.063352820947,
      "learning_rate": 3.492896993742306e-07,
      "loss": 1.7692,
      "step": 82048
    },
    {
      "epoch": 0.0002978846098961506,
      "grad_norm": 9506.746236226147,
      "learning_rate": 3.4922153614205067e-07,
      "loss": 1.7725,
      "step": 82080
    },
    {
      "epoch": 0.0002980007442469873,
      "grad_norm": 10917.344182538169,
      "learning_rate": 3.4915341280008426e-07,
      "loss": 1.7432,
      "step": 82112
    },
    {
      "epoch": 0.00029811687859782407,
      "grad_norm": 10032.113037640675,
      "learning_rate": 3.4908532930943904e-07,
      "loss": 1.7439,
      "step": 82144
    },
    {
      "epoch": 0.00029823301294866077,
      "grad_norm": 10727.560952984606,
      "learning_rate": 3.49017285631276e-07,
      "loss": 1.7584,
      "step": 82176
    },
    {
      "epoch": 0.00029834914729949747,
      "grad_norm": 10796.558710996758,
      "learning_rate": 3.4894928172680916e-07,
      "loss": 1.7496,
      "step": 82208
    },
    {
      "epoch": 0.00029846528165033417,
      "grad_norm": 9247.35691968251,
      "learning_rate": 3.4888131755730517e-07,
      "loss": 1.7744,
      "step": 82240
    },
    {
      "epoch": 0.00029858141600117087,
      "grad_norm": 10483.76707104846,
      "learning_rate": 3.488133930840836e-07,
      "loss": 1.7882,
      "step": 82272
    },
    {
      "epoch": 0.00029869755035200757,
      "grad_norm": 8635.80361055067,
      "learning_rate": 3.4874550826851676e-07,
      "loss": 1.7943,
      "step": 82304
    },
    {
      "epoch": 0.00029881368470284427,
      "grad_norm": 8253.985643311962,
      "learning_rate": 3.4867766307202953e-07,
      "loss": 1.8118,
      "step": 82336
    },
    {
      "epoch": 0.00029892981905368097,
      "grad_norm": 9351.118008024496,
      "learning_rate": 3.4860985745609914e-07,
      "loss": 1.8142,
      "step": 82368
    },
    {
      "epoch": 0.00029904595340451767,
      "grad_norm": 9387.916488763627,
      "learning_rate": 3.485420913822555e-07,
      "loss": 1.7763,
      "step": 82400
    },
    {
      "epoch": 0.0002991620877553544,
      "grad_norm": 8651.244303566973,
      "learning_rate": 3.4847436481208077e-07,
      "loss": 1.7762,
      "step": 82432
    },
    {
      "epoch": 0.0002992782221061911,
      "grad_norm": 9696.271242080638,
      "learning_rate": 3.4840667770720914e-07,
      "loss": 1.8001,
      "step": 82464
    },
    {
      "epoch": 0.0002993943564570278,
      "grad_norm": 10989.16393544113,
      "learning_rate": 3.4833903002932723e-07,
      "loss": 1.7709,
      "step": 82496
    },
    {
      "epoch": 0.0002995104908078645,
      "grad_norm": 10086.47569768549,
      "learning_rate": 3.4827142174017355e-07,
      "loss": 1.7777,
      "step": 82528
    },
    {
      "epoch": 0.0002996266251587012,
      "grad_norm": 8523.977475333919,
      "learning_rate": 3.482038528015386e-07,
      "loss": 1.7451,
      "step": 82560
    },
    {
      "epoch": 0.0002997427595095379,
      "grad_norm": 8444.835581584759,
      "learning_rate": 3.481363231752648e-07,
      "loss": 1.7406,
      "step": 82592
    },
    {
      "epoch": 0.0002998588938603746,
      "grad_norm": 9481.989453695885,
      "learning_rate": 3.4806883282324627e-07,
      "loss": 1.7467,
      "step": 82624
    },
    {
      "epoch": 0.0002999750282112113,
      "grad_norm": 11277.796238627474,
      "learning_rate": 3.480013817074289e-07,
      "loss": 1.7615,
      "step": 82656
    },
    {
      "epoch": 0.000300091162562048,
      "grad_norm": 10075.632188602362,
      "learning_rate": 3.479339697898101e-07,
      "loss": 1.7602,
      "step": 82688
    },
    {
      "epoch": 0.0003002072969128848,
      "grad_norm": 8721.163110503094,
      "learning_rate": 3.478665970324389e-07,
      "loss": 1.7486,
      "step": 82720
    },
    {
      "epoch": 0.0003003234312637215,
      "grad_norm": 9966.547847675241,
      "learning_rate": 3.477992633974157e-07,
      "loss": 1.7634,
      "step": 82752
    },
    {
      "epoch": 0.0003004395656145582,
      "grad_norm": 9538.765748250662,
      "learning_rate": 3.477319688468921e-07,
      "loss": 1.7664,
      "step": 82784
    },
    {
      "epoch": 0.0003005556999653949,
      "grad_norm": 10403.010718056576,
      "learning_rate": 3.476647133430712e-07,
      "loss": 1.7623,
      "step": 82816
    },
    {
      "epoch": 0.0003006718343162316,
      "grad_norm": 8861.303402998907,
      "learning_rate": 3.47597496848207e-07,
      "loss": 1.7653,
      "step": 82848
    },
    {
      "epoch": 0.0003007879686670683,
      "grad_norm": 9856.77421877969,
      "learning_rate": 3.4753031932460477e-07,
      "loss": 1.7552,
      "step": 82880
    },
    {
      "epoch": 0.000300904103017905,
      "grad_norm": 10171.489763058311,
      "learning_rate": 3.474631807346205e-07,
      "loss": 1.7602,
      "step": 82912
    },
    {
      "epoch": 0.0003010202373687417,
      "grad_norm": 10359.700092184135,
      "learning_rate": 3.4739608104066147e-07,
      "loss": 1.7697,
      "step": 82944
    },
    {
      "epoch": 0.0003011363717195784,
      "grad_norm": 9927.248460676301,
      "learning_rate": 3.473290202051854e-07,
      "loss": 1.7758,
      "step": 82976
    },
    {
      "epoch": 0.00030125250607041513,
      "grad_norm": 9113.410338616384,
      "learning_rate": 3.4726409204140304e-07,
      "loss": 1.7752,
      "step": 83008
    },
    {
      "epoch": 0.00030136864042125183,
      "grad_norm": 8643.430916019403,
      "learning_rate": 3.471971075990494e-07,
      "loss": 1.784,
      "step": 83040
    },
    {
      "epoch": 0.00030148477477208853,
      "grad_norm": 8396.418522203381,
      "learning_rate": 3.4713016190402377e-07,
      "loss": 1.776,
      "step": 83072
    },
    {
      "epoch": 0.00030160090912292523,
      "grad_norm": 8715.35369333913,
      "learning_rate": 3.4706325491898483e-07,
      "loss": 1.7891,
      "step": 83104
    },
    {
      "epoch": 0.00030171704347376193,
      "grad_norm": 8994.133532475487,
      "learning_rate": 3.469963866066415e-07,
      "loss": 1.7886,
      "step": 83136
    },
    {
      "epoch": 0.00030183317782459863,
      "grad_norm": 9798.220144495632,
      "learning_rate": 3.4692955692975294e-07,
      "loss": 1.7737,
      "step": 83168
    },
    {
      "epoch": 0.00030194931217543533,
      "grad_norm": 10678.794782183988,
      "learning_rate": 3.468627658511285e-07,
      "loss": 1.7754,
      "step": 83200
    },
    {
      "epoch": 0.00030206544652627203,
      "grad_norm": 10390.283345510843,
      "learning_rate": 3.4679601333362785e-07,
      "loss": 1.7801,
      "step": 83232
    },
    {
      "epoch": 0.00030218158087710873,
      "grad_norm": 10750.994837688277,
      "learning_rate": 3.467292993401603e-07,
      "loss": 1.7928,
      "step": 83264
    },
    {
      "epoch": 0.0003022977152279455,
      "grad_norm": 9140.57066052224,
      "learning_rate": 3.4666262383368545e-07,
      "loss": 1.7952,
      "step": 83296
    },
    {
      "epoch": 0.0003024138495787822,
      "grad_norm": 10345.328994285295,
      "learning_rate": 3.4659598677721254e-07,
      "loss": 1.7594,
      "step": 83328
    },
    {
      "epoch": 0.0003025299839296189,
      "grad_norm": 8623.98631724332,
      "learning_rate": 3.465293881338005e-07,
      "loss": 1.7375,
      "step": 83360
    },
    {
      "epoch": 0.0003026461182804556,
      "grad_norm": 10352.957645040378,
      "learning_rate": 3.4646282786655825e-07,
      "loss": 1.7515,
      "step": 83392
    },
    {
      "epoch": 0.0003027622526312923,
      "grad_norm": 9248.884257033385,
      "learning_rate": 3.463963059386439e-07,
      "loss": 1.7459,
      "step": 83424
    },
    {
      "epoch": 0.000302878386982129,
      "grad_norm": 10728.33854797657,
      "learning_rate": 3.463298223132653e-07,
      "loss": 1.7482,
      "step": 83456
    },
    {
      "epoch": 0.0003029945213329657,
      "grad_norm": 8533.21861902061,
      "learning_rate": 3.462633769536796e-07,
      "loss": 1.7642,
      "step": 83488
    },
    {
      "epoch": 0.0003031106556838024,
      "grad_norm": 11198.845119029016,
      "learning_rate": 3.4619696982319334e-07,
      "loss": 1.7635,
      "step": 83520
    },
    {
      "epoch": 0.0003032267900346391,
      "grad_norm": 9582.141723017876,
      "learning_rate": 3.4613060088516224e-07,
      "loss": 1.7864,
      "step": 83552
    },
    {
      "epoch": 0.0003033429243854758,
      "grad_norm": 9266.701678590933,
      "learning_rate": 3.460642701029914e-07,
      "loss": 1.7852,
      "step": 83584
    },
    {
      "epoch": 0.00030345905873631254,
      "grad_norm": 9739.672479092918,
      "learning_rate": 3.459979774401346e-07,
      "loss": 1.7744,
      "step": 83616
    },
    {
      "epoch": 0.00030357519308714924,
      "grad_norm": 8219.440248581408,
      "learning_rate": 3.4593172286009496e-07,
      "loss": 1.7372,
      "step": 83648
    },
    {
      "epoch": 0.00030369132743798594,
      "grad_norm": 10911.931634683202,
      "learning_rate": 3.458655063264243e-07,
      "loss": 1.7441,
      "step": 83680
    },
    {
      "epoch": 0.00030380746178882264,
      "grad_norm": 8776.858549617853,
      "learning_rate": 3.4579932780272326e-07,
      "loss": 1.7537,
      "step": 83712
    },
    {
      "epoch": 0.00030392359613965934,
      "grad_norm": 9150.644348896967,
      "learning_rate": 3.457331872526414e-07,
      "loss": 1.7571,
      "step": 83744
    },
    {
      "epoch": 0.00030403973049049604,
      "grad_norm": 9517.522366666653,
      "learning_rate": 3.4566708463987665e-07,
      "loss": 1.7528,
      "step": 83776
    },
    {
      "epoch": 0.00030415586484133273,
      "grad_norm": 8982.745237398198,
      "learning_rate": 3.456010199281758e-07,
      "loss": 1.7696,
      "step": 83808
    },
    {
      "epoch": 0.00030427199919216943,
      "grad_norm": 8654.401076908789,
      "learning_rate": 3.4553499308133396e-07,
      "loss": 1.7709,
      "step": 83840
    },
    {
      "epoch": 0.00030438813354300613,
      "grad_norm": 7756.462531334758,
      "learning_rate": 3.454690040631945e-07,
      "loss": 1.7882,
      "step": 83872
    },
    {
      "epoch": 0.0003045042678938429,
      "grad_norm": 8999.12951345851,
      "learning_rate": 3.454030528376496e-07,
      "loss": 1.7983,
      "step": 83904
    },
    {
      "epoch": 0.0003046204022446796,
      "grad_norm": 9400.804433664174,
      "learning_rate": 3.45337139368639e-07,
      "loss": 1.7757,
      "step": 83936
    },
    {
      "epoch": 0.0003047365365955163,
      "grad_norm": 9928.59003081505,
      "learning_rate": 3.4527126362015115e-07,
      "loss": 1.7787,
      "step": 83968
    },
    {
      "epoch": 0.000304852670946353,
      "grad_norm": 10680.258236578366,
      "learning_rate": 3.4520542555622227e-07,
      "loss": 1.7866,
      "step": 84000
    },
    {
      "epoch": 0.0003049688052971897,
      "grad_norm": 10201.883551580071,
      "learning_rate": 3.451416808343938e-07,
      "loss": 1.7827,
      "step": 84032
    },
    {
      "epoch": 0.0003050849396480264,
      "grad_norm": 9586.349774549226,
      "learning_rate": 3.450759168570269e-07,
      "loss": 1.7787,
      "step": 84064
    },
    {
      "epoch": 0.0003052010739988631,
      "grad_norm": 9175.285717622095,
      "learning_rate": 3.4501019045773406e-07,
      "loss": 1.7713,
      "step": 84096
    },
    {
      "epoch": 0.0003053172083496998,
      "grad_norm": 9596.158085400635,
      "learning_rate": 3.449445016007417e-07,
      "loss": 1.7692,
      "step": 84128
    },
    {
      "epoch": 0.0003054333427005365,
      "grad_norm": 9386.983541053005,
      "learning_rate": 3.448788502503236e-07,
      "loss": 1.7682,
      "step": 84160
    },
    {
      "epoch": 0.00030554947705137324,
      "grad_norm": 9620.766393588403,
      "learning_rate": 3.4481323637080134e-07,
      "loss": 1.7829,
      "step": 84192
    },
    {
      "epoch": 0.00030566561140220994,
      "grad_norm": 10339.418358882669,
      "learning_rate": 3.447476599265439e-07,
      "loss": 1.753,
      "step": 84224
    },
    {
      "epoch": 0.00030578174575304664,
      "grad_norm": 11313.908166500203,
      "learning_rate": 3.4468212088196766e-07,
      "loss": 1.7401,
      "step": 84256
    },
    {
      "epoch": 0.00030589788010388334,
      "grad_norm": 9715.571419118898,
      "learning_rate": 3.446166192015365e-07,
      "loss": 1.7561,
      "step": 84288
    },
    {
      "epoch": 0.00030601401445472004,
      "grad_norm": 10895.87187883558,
      "learning_rate": 3.445511548497613e-07,
      "loss": 1.7721,
      "step": 84320
    },
    {
      "epoch": 0.00030613014880555674,
      "grad_norm": 8685.210187439334,
      "learning_rate": 3.444857277912002e-07,
      "loss": 1.7671,
      "step": 84352
    },
    {
      "epoch": 0.00030624628315639344,
      "grad_norm": 8238.442814027418,
      "learning_rate": 3.4442033799045856e-07,
      "loss": 1.7391,
      "step": 84384
    },
    {
      "epoch": 0.00030636241750723014,
      "grad_norm": 10293.9409362984,
      "learning_rate": 3.443549854121887e-07,
      "loss": 1.7459,
      "step": 84416
    },
    {
      "epoch": 0.00030647855185806684,
      "grad_norm": 10171.463709810894,
      "learning_rate": 3.442896700210897e-07,
      "loss": 1.7537,
      "step": 84448
    },
    {
      "epoch": 0.0003065946862089036,
      "grad_norm": 10077.299340597163,
      "learning_rate": 3.442243917819077e-07,
      "loss": 1.7669,
      "step": 84480
    },
    {
      "epoch": 0.0003067108205597403,
      "grad_norm": 8920.979766819337,
      "learning_rate": 3.441591506594355e-07,
      "loss": 1.7717,
      "step": 84512
    },
    {
      "epoch": 0.000306826954910577,
      "grad_norm": 9583.15177799037,
      "learning_rate": 3.4409394661851275e-07,
      "loss": 1.7756,
      "step": 84544
    },
    {
      "epoch": 0.0003069430892614137,
      "grad_norm": 8975.657413248346,
      "learning_rate": 3.440287796240255e-07,
      "loss": 1.76,
      "step": 84576
    },
    {
      "epoch": 0.0003070592236122504,
      "grad_norm": 10303.516487102837,
      "learning_rate": 3.439636496409064e-07,
      "loss": 1.7725,
      "step": 84608
    },
    {
      "epoch": 0.0003071753579630871,
      "grad_norm": 9470.897528745625,
      "learning_rate": 3.438985566341346e-07,
      "loss": 1.7746,
      "step": 84640
    },
    {
      "epoch": 0.0003072914923139238,
      "grad_norm": 9906.67068191933,
      "learning_rate": 3.4383350056873574e-07,
      "loss": 1.79,
      "step": 84672
    },
    {
      "epoch": 0.0003074076266647605,
      "grad_norm": 10470.783924807158,
      "learning_rate": 3.437684814097813e-07,
      "loss": 1.7901,
      "step": 84704
    },
    {
      "epoch": 0.0003075237610155972,
      "grad_norm": 9233.505943031607,
      "learning_rate": 3.437034991223896e-07,
      "loss": 1.7894,
      "step": 84736
    },
    {
      "epoch": 0.00030763989536643395,
      "grad_norm": 9416.351628948443,
      "learning_rate": 3.436385536717247e-07,
      "loss": 1.7724,
      "step": 84768
    },
    {
      "epoch": 0.00030775602971727065,
      "grad_norm": 10995.68569940047,
      "learning_rate": 3.435736450229968e-07,
      "loss": 1.7906,
      "step": 84800
    },
    {
      "epoch": 0.00030787216406810735,
      "grad_norm": 8067.601502305379,
      "learning_rate": 3.435087731414622e-07,
      "loss": 1.7678,
      "step": 84832
    },
    {
      "epoch": 0.00030798829841894405,
      "grad_norm": 9689.507830638251,
      "learning_rate": 3.434439379924228e-07,
      "loss": 1.7532,
      "step": 84864
    },
    {
      "epoch": 0.00030810443276978075,
      "grad_norm": 9841.509233852297,
      "learning_rate": 3.433791395412266e-07,
      "loss": 1.7554,
      "step": 84896
    },
    {
      "epoch": 0.00030822056712061745,
      "grad_norm": 9711.62705214734,
      "learning_rate": 3.433143777532674e-07,
      "loss": 1.765,
      "step": 84928
    },
    {
      "epoch": 0.00030833670147145415,
      "grad_norm": 10954.964536683812,
      "learning_rate": 3.4324965259398446e-07,
      "loss": 1.7557,
      "step": 84960
    },
    {
      "epoch": 0.00030845283582229085,
      "grad_norm": 10822.956435281443,
      "learning_rate": 3.4318496402886274e-07,
      "loss": 1.7479,
      "step": 84992
    },
    {
      "epoch": 0.00030856897017312755,
      "grad_norm": 10335.226267479586,
      "learning_rate": 3.4312233184555054e-07,
      "loss": 1.7701,
      "step": 85024
    },
    {
      "epoch": 0.0003086851045239643,
      "grad_norm": 10210.55052384542,
      "learning_rate": 3.43057715224494e-07,
      "loss": 1.7705,
      "step": 85056
    },
    {
      "epoch": 0.000308801238874801,
      "grad_norm": 9769.221668075712,
      "learning_rate": 3.429931350953999e-07,
      "loss": 1.7677,
      "step": 85088
    },
    {
      "epoch": 0.0003089173732256377,
      "grad_norm": 10115.940687845101,
      "learning_rate": 3.4292859142393336e-07,
      "loss": 1.776,
      "step": 85120
    },
    {
      "epoch": 0.0003090335075764744,
      "grad_norm": 10495.918635355365,
      "learning_rate": 3.428640841758048e-07,
      "loss": 1.7479,
      "step": 85152
    },
    {
      "epoch": 0.0003091496419273111,
      "grad_norm": 9873.2306769365,
      "learning_rate": 3.427996133167695e-07,
      "loss": 1.7433,
      "step": 85184
    },
    {
      "epoch": 0.0003092657762781478,
      "grad_norm": 10268.138487574075,
      "learning_rate": 3.4273517881262815e-07,
      "loss": 1.7491,
      "step": 85216
    },
    {
      "epoch": 0.0003093819106289845,
      "grad_norm": 10666.271513513988,
      "learning_rate": 3.4267078062922627e-07,
      "loss": 1.7612,
      "step": 85248
    },
    {
      "epoch": 0.0003094980449798212,
      "grad_norm": 11997.262020977952,
      "learning_rate": 3.4260641873245416e-07,
      "loss": 1.783,
      "step": 85280
    },
    {
      "epoch": 0.0003096141793306579,
      "grad_norm": 8918.095200209515,
      "learning_rate": 3.4254209308824714e-07,
      "loss": 1.7982,
      "step": 85312
    },
    {
      "epoch": 0.00030973031368149466,
      "grad_norm": 9010.470686928624,
      "learning_rate": 3.424778036625852e-07,
      "loss": 1.7771,
      "step": 85344
    },
    {
      "epoch": 0.00030984644803233136,
      "grad_norm": 8495.339663603805,
      "learning_rate": 3.42413550421493e-07,
      "loss": 1.7693,
      "step": 85376
    },
    {
      "epoch": 0.00030996258238316806,
      "grad_norm": 8890.883758097392,
      "learning_rate": 3.4234933333103986e-07,
      "loss": 1.7847,
      "step": 85408
    },
    {
      "epoch": 0.00031007871673400476,
      "grad_norm": 10603.71991331344,
      "learning_rate": 3.4228515235733945e-07,
      "loss": 1.7801,
      "step": 85440
    },
    {
      "epoch": 0.00031019485108484146,
      "grad_norm": 8723.579425900816,
      "learning_rate": 3.422210074665502e-07,
      "loss": 1.7687,
      "step": 85472
    },
    {
      "epoch": 0.00031031098543567816,
      "grad_norm": 9329.105423351159,
      "learning_rate": 3.4215689862487464e-07,
      "loss": 1.766,
      "step": 85504
    },
    {
      "epoch": 0.00031042711978651486,
      "grad_norm": 9332.22577952334,
      "learning_rate": 3.420928257985598e-07,
      "loss": 1.7741,
      "step": 85536
    },
    {
      "epoch": 0.00031054325413735156,
      "grad_norm": 10018.463455041396,
      "learning_rate": 3.4202878895389674e-07,
      "loss": 1.7654,
      "step": 85568
    },
    {
      "epoch": 0.00031065938848818826,
      "grad_norm": 9349.145308529545,
      "learning_rate": 3.4196478805722086e-07,
      "loss": 1.7457,
      "step": 85600
    },
    {
      "epoch": 0.000310775522839025,
      "grad_norm": 10455.22194886364,
      "learning_rate": 3.4190082307491147e-07,
      "loss": 1.7534,
      "step": 85632
    },
    {
      "epoch": 0.0003108916571898617,
      "grad_norm": 9678.839909823904,
      "learning_rate": 3.41836893973392e-07,
      "loss": 1.7544,
      "step": 85664
    },
    {
      "epoch": 0.0003110077915406984,
      "grad_norm": 10825.431169242174,
      "learning_rate": 3.417730007191299e-07,
      "loss": 1.7626,
      "step": 85696
    },
    {
      "epoch": 0.0003111239258915351,
      "grad_norm": 9562.808164969116,
      "learning_rate": 3.417091432786362e-07,
      "loss": 1.7762,
      "step": 85728
    },
    {
      "epoch": 0.0003112400602423718,
      "grad_norm": 9060.393368943756,
      "learning_rate": 3.41645321618466e-07,
      "loss": 1.7603,
      "step": 85760
    },
    {
      "epoch": 0.0003113561945932085,
      "grad_norm": 9636.844296760222,
      "learning_rate": 3.4158153570521785e-07,
      "loss": 1.7689,
      "step": 85792
    },
    {
      "epoch": 0.0003114723289440452,
      "grad_norm": 8989.90233539831,
      "learning_rate": 3.415177855055342e-07,
      "loss": 1.7747,
      "step": 85824
    },
    {
      "epoch": 0.0003115884632948819,
      "grad_norm": 10484.683781593034,
      "learning_rate": 3.414540709861009e-07,
      "loss": 1.7905,
      "step": 85856
    },
    {
      "epoch": 0.0003117045976457186,
      "grad_norm": 10791.978502573103,
      "learning_rate": 3.413903921136473e-07,
      "loss": 1.7948,
      "step": 85888
    },
    {
      "epoch": 0.00031182073199655536,
      "grad_norm": 10777.723043389082,
      "learning_rate": 3.413267488549462e-07,
      "loss": 1.7532,
      "step": 85920
    },
    {
      "epoch": 0.00031193686634739206,
      "grad_norm": 9955.621125776132,
      "learning_rate": 3.412631411768137e-07,
      "loss": 1.745,
      "step": 85952
    },
    {
      "epoch": 0.00031205300069822876,
      "grad_norm": 9298.615810968855,
      "learning_rate": 3.4119956904610933e-07,
      "loss": 1.7442,
      "step": 85984
    },
    {
      "epoch": 0.00031216913504906546,
      "grad_norm": 8589.898835259935,
      "learning_rate": 3.411380174117552e-07,
      "loss": 1.7571,
      "step": 86016
    },
    {
      "epoch": 0.00031228526939990216,
      "grad_norm": 9031.669723810764,
      "learning_rate": 3.410745151683678e-07,
      "loss": 1.7879,
      "step": 86048
    },
    {
      "epoch": 0.00031240140375073886,
      "grad_norm": 9938.11098750663,
      "learning_rate": 3.4101104837427654e-07,
      "loss": 1.7794,
      "step": 86080
    },
    {
      "epoch": 0.00031251753810157556,
      "grad_norm": 8835.401066165587,
      "learning_rate": 3.409476169965118e-07,
      "loss": 1.7577,
      "step": 86112
    },
    {
      "epoch": 0.00031263367245241226,
      "grad_norm": 8285.643849454307,
      "learning_rate": 3.40884221002147e-07,
      "loss": 1.7712,
      "step": 86144
    },
    {
      "epoch": 0.00031274980680324896,
      "grad_norm": 11082.787375024389,
      "learning_rate": 3.4082086035829827e-07,
      "loss": 1.7835,
      "step": 86176
    },
    {
      "epoch": 0.0003128659411540857,
      "grad_norm": 8595.491957997518,
      "learning_rate": 3.4075753503212446e-07,
      "loss": 1.7675,
      "step": 86208
    },
    {
      "epoch": 0.0003129820755049224,
      "grad_norm": 8676.06362355648,
      "learning_rate": 3.406942449908274e-07,
      "loss": 1.7744,
      "step": 86240
    },
    {
      "epoch": 0.0003130982098557591,
      "grad_norm": 10441.065654424361,
      "learning_rate": 3.406309902016512e-07,
      "loss": 1.7645,
      "step": 86272
    },
    {
      "epoch": 0.0003132143442065958,
      "grad_norm": 10518.148030903538,
      "learning_rate": 3.4056777063188276e-07,
      "loss": 1.7693,
      "step": 86304
    },
    {
      "epoch": 0.0003133304785574325,
      "grad_norm": 9153.07128782465,
      "learning_rate": 3.405045862488514e-07,
      "loss": 1.7707,
      "step": 86336
    },
    {
      "epoch": 0.0003134466129082692,
      "grad_norm": 10483.328860624377,
      "learning_rate": 3.40441437019929e-07,
      "loss": 1.7544,
      "step": 86368
    },
    {
      "epoch": 0.0003135627472591059,
      "grad_norm": 9628.125258844528,
      "learning_rate": 3.4037832291252933e-07,
      "loss": 1.7459,
      "step": 86400
    },
    {
      "epoch": 0.0003136788816099426,
      "grad_norm": 9490.619632036676,
      "learning_rate": 3.403152438941089e-07,
      "loss": 1.7661,
      "step": 86432
    },
    {
      "epoch": 0.0003137950159607793,
      "grad_norm": 9451.637953286192,
      "learning_rate": 3.4025219993216625e-07,
      "loss": 1.7828,
      "step": 86464
    },
    {
      "epoch": 0.00031391115031161607,
      "grad_norm": 10097.406597735877,
      "learning_rate": 3.4018919099424204e-07,
      "loss": 1.7684,
      "step": 86496
    },
    {
      "epoch": 0.00031402728466245277,
      "grad_norm": 10630.8648754464,
      "learning_rate": 3.4012621704791893e-07,
      "loss": 1.7458,
      "step": 86528
    },
    {
      "epoch": 0.00031414341901328947,
      "grad_norm": 10592.365741419619,
      "learning_rate": 3.4006327806082175e-07,
      "loss": 1.7613,
      "step": 86560
    },
    {
      "epoch": 0.00031425955336412617,
      "grad_norm": 9392.90466256312,
      "learning_rate": 3.400003740006171e-07,
      "loss": 1.7706,
      "step": 86592
    },
    {
      "epoch": 0.00031437568771496287,
      "grad_norm": 11042.593898174468,
      "learning_rate": 3.3993750483501356e-07,
      "loss": 1.7824,
      "step": 86624
    },
    {
      "epoch": 0.00031449182206579957,
      "grad_norm": 9176.365075562328,
      "learning_rate": 3.398746705317612e-07,
      "loss": 1.7835,
      "step": 86656
    },
    {
      "epoch": 0.00031460795641663627,
      "grad_norm": 10239.596085783853,
      "learning_rate": 3.3981187105865225e-07,
      "loss": 1.7529,
      "step": 86688
    },
    {
      "epoch": 0.00031472409076747297,
      "grad_norm": 9930.794630843999,
      "learning_rate": 3.3974910638352015e-07,
      "loss": 1.7491,
      "step": 86720
    },
    {
      "epoch": 0.00031484022511830967,
      "grad_norm": 9917.881527826394,
      "learning_rate": 3.396863764742402e-07,
      "loss": 1.7609,
      "step": 86752
    },
    {
      "epoch": 0.0003149563594691464,
      "grad_norm": 9980.462113549653,
      "learning_rate": 3.396236812987292e-07,
      "loss": 1.7625,
      "step": 86784
    },
    {
      "epoch": 0.0003150724938199831,
      "grad_norm": 9888.409073253393,
      "learning_rate": 3.395610208249452e-07,
      "loss": 1.7637,
      "step": 86816
    },
    {
      "epoch": 0.0003151886281708198,
      "grad_norm": 8881.806122630689,
      "learning_rate": 3.3949839502088756e-07,
      "loss": 1.7729,
      "step": 86848
    },
    {
      "epoch": 0.0003153047625216565,
      "grad_norm": 8964.208609799307,
      "learning_rate": 3.3943580385459737e-07,
      "loss": 1.762,
      "step": 86880
    },
    {
      "epoch": 0.0003154208968724932,
      "grad_norm": 8698.26051575831,
      "learning_rate": 3.3937324729415657e-07,
      "loss": 1.7743,
      "step": 86912
    },
    {
      "epoch": 0.0003155370312233299,
      "grad_norm": 9512.528896145335,
      "learning_rate": 3.393107253076883e-07,
      "loss": 1.7809,
      "step": 86944
    },
    {
      "epoch": 0.0003156531655741666,
      "grad_norm": 9940.634185000472,
      "learning_rate": 3.392482378633568e-07,
      "loss": 1.7884,
      "step": 86976
    },
    {
      "epoch": 0.0003157692999250033,
      "grad_norm": 9929.472493541638,
      "learning_rate": 3.391857849293676e-07,
      "loss": 1.7791,
      "step": 87008
    },
    {
      "epoch": 0.00031588543427584,
      "grad_norm": 11185.49846899994,
      "learning_rate": 3.391253165291205e-07,
      "loss": 1.798,
      "step": 87040
    },
    {
      "epoch": 0.0003160015686266768,
      "grad_norm": 8830.646748681549,
      "learning_rate": 3.3906293144461015e-07,
      "loss": 1.7929,
      "step": 87072
    },
    {
      "epoch": 0.0003161177029775135,
      "grad_norm": 9426.333751782822,
      "learning_rate": 3.390005807762925e-07,
      "loss": 1.7487,
      "step": 87104
    },
    {
      "epoch": 0.0003162338373283502,
      "grad_norm": 10022.960640449508,
      "learning_rate": 3.389382644925351e-07,
      "loss": 1.7421,
      "step": 87136
    },
    {
      "epoch": 0.0003163499716791869,
      "grad_norm": 9956.426467362675,
      "learning_rate": 3.3887598256174607e-07,
      "loss": 1.7412,
      "step": 87168
    },
    {
      "epoch": 0.0003164661060300236,
      "grad_norm": 8941.201149733743,
      "learning_rate": 3.388137349523742e-07,
      "loss": 1.7393,
      "step": 87200
    },
    {
      "epoch": 0.0003165822403808603,
      "grad_norm": 8871.804551499092,
      "learning_rate": 3.387515216329088e-07,
      "loss": 1.7608,
      "step": 87232
    },
    {
      "epoch": 0.000316698374731697,
      "grad_norm": 10057.81765593312,
      "learning_rate": 3.3868934257187966e-07,
      "loss": 1.7665,
      "step": 87264
    },
    {
      "epoch": 0.0003168145090825337,
      "grad_norm": 10430.71943827462,
      "learning_rate": 3.3862719773785713e-07,
      "loss": 1.7529,
      "step": 87296
    },
    {
      "epoch": 0.0003169306434333704,
      "grad_norm": 8677.466450525751,
      "learning_rate": 3.3856508709945174e-07,
      "loss": 1.769,
      "step": 87328
    },
    {
      "epoch": 0.00031704677778420713,
      "grad_norm": 10222.163665291218,
      "learning_rate": 3.385030106253144e-07,
      "loss": 1.7615,
      "step": 87360
    },
    {
      "epoch": 0.00031716291213504383,
      "grad_norm": 8984.347722567287,
      "learning_rate": 3.3844096828413617e-07,
      "loss": 1.7785,
      "step": 87392
    },
    {
      "epoch": 0.00031727904648588053,
      "grad_norm": 9779.122660034487,
      "learning_rate": 3.383789600446483e-07,
      "loss": 1.7513,
      "step": 87424
    },
    {
      "epoch": 0.00031739518083671723,
      "grad_norm": 11263.562136375864,
      "learning_rate": 3.3831698587562216e-07,
      "loss": 1.7494,
      "step": 87456
    },
    {
      "epoch": 0.00031751131518755393,
      "grad_norm": 9511.29076413922,
      "learning_rate": 3.3825504574586914e-07,
      "loss": 1.753,
      "step": 87488
    },
    {
      "epoch": 0.00031762744953839063,
      "grad_norm": 8045.032007394377,
      "learning_rate": 3.381931396242406e-07,
      "loss": 1.751,
      "step": 87520
    },
    {
      "epoch": 0.00031774358388922733,
      "grad_norm": 8804.743039975669,
      "learning_rate": 3.3813126747962766e-07,
      "loss": 1.7919,
      "step": 87552
    },
    {
      "epoch": 0.00031785971824006403,
      "grad_norm": 11822.201994552453,
      "learning_rate": 3.3806942928096136e-07,
      "loss": 1.8037,
      "step": 87584
    },
    {
      "epoch": 0.00031797585259090073,
      "grad_norm": 10384.696914209871,
      "learning_rate": 3.380076249972126e-07,
      "loss": 1.8047,
      "step": 87616
    },
    {
      "epoch": 0.0003180919869417375,
      "grad_norm": 8103.441244310962,
      "learning_rate": 3.3794585459739177e-07,
      "loss": 1.7887,
      "step": 87648
    },
    {
      "epoch": 0.0003182081212925742,
      "grad_norm": 10445.753012588417,
      "learning_rate": 3.37884118050549e-07,
      "loss": 1.7879,
      "step": 87680
    },
    {
      "epoch": 0.0003183242556434109,
      "grad_norm": 8845.233292570638,
      "learning_rate": 3.37822415325774e-07,
      "loss": 1.7587,
      "step": 87712
    },
    {
      "epoch": 0.0003184403899942476,
      "grad_norm": 11821.064926646837,
      "learning_rate": 3.3776074639219604e-07,
      "loss": 1.7621,
      "step": 87744
    },
    {
      "epoch": 0.0003185565243450843,
      "grad_norm": 10035.81048047441,
      "learning_rate": 3.3769911121898355e-07,
      "loss": 1.7698,
      "step": 87776
    },
    {
      "epoch": 0.000318672658695921,
      "grad_norm": 8998.893820909323,
      "learning_rate": 3.3763750977534465e-07,
      "loss": 1.7626,
      "step": 87808
    },
    {
      "epoch": 0.0003187887930467577,
      "grad_norm": 10576.851516401279,
      "learning_rate": 3.3757594203052657e-07,
      "loss": 1.7844,
      "step": 87840
    },
    {
      "epoch": 0.0003189049273975944,
      "grad_norm": 10866.374004238949,
      "learning_rate": 3.3751440795381586e-07,
      "loss": 1.7612,
      "step": 87872
    },
    {
      "epoch": 0.0003190210617484311,
      "grad_norm": 10019.693408483115,
      "learning_rate": 3.3745290751453827e-07,
      "loss": 1.7684,
      "step": 87904
    },
    {
      "epoch": 0.00031913719609926784,
      "grad_norm": 9113.031987214794,
      "learning_rate": 3.3739144068205847e-07,
      "loss": 1.7263,
      "step": 87936
    },
    {
      "epoch": 0.00031925333045010454,
      "grad_norm": 9398.819925926871,
      "learning_rate": 3.373300074257805e-07,
      "loss": 1.746,
      "step": 87968
    },
    {
      "epoch": 0.00031936946480094124,
      "grad_norm": 10250.505353395998,
      "learning_rate": 3.372686077151471e-07,
      "loss": 1.7546,
      "step": 88000
    },
    {
      "epoch": 0.00031948559915177794,
      "grad_norm": 9556.208976367145,
      "learning_rate": 3.372091587062434e-07,
      "loss": 1.7392,
      "step": 88032
    },
    {
      "epoch": 0.00031960173350261464,
      "grad_norm": 10012.348276003986,
      "learning_rate": 3.3714782494944904e-07,
      "loss": 1.7397,
      "step": 88064
    },
    {
      "epoch": 0.00031971786785345134,
      "grad_norm": 10101.633333278336,
      "learning_rate": 3.370865246478118e-07,
      "loss": 1.7576,
      "step": 88096
    },
    {
      "epoch": 0.00031983400220428804,
      "grad_norm": 10563.069250932704,
      "learning_rate": 3.3702525777092857e-07,
      "loss": 1.7652,
      "step": 88128
    },
    {
      "epoch": 0.00031995013655512474,
      "grad_norm": 9730.776536330488,
      "learning_rate": 3.3696402428843484e-07,
      "loss": 1.7814,
      "step": 88160
    },
    {
      "epoch": 0.00032006627090596144,
      "grad_norm": 9198.815358512204,
      "learning_rate": 3.36902824170005e-07,
      "loss": 1.7763,
      "step": 88192
    },
    {
      "epoch": 0.0003201824052567982,
      "grad_norm": 9537.274872834483,
      "learning_rate": 3.368416573853517e-07,
      "loss": 1.772,
      "step": 88224
    },
    {
      "epoch": 0.0003202985396076349,
      "grad_norm": 8422.986999871246,
      "learning_rate": 3.367805239042261e-07,
      "loss": 1.7668,
      "step": 88256
    },
    {
      "epoch": 0.0003204146739584716,
      "grad_norm": 9681.583548159877,
      "learning_rate": 3.367194236964179e-07,
      "loss": 1.7683,
      "step": 88288
    },
    {
      "epoch": 0.0003205308083093083,
      "grad_norm": 9098.430414087916,
      "learning_rate": 3.366583567317551e-07,
      "loss": 1.7758,
      "step": 88320
    },
    {
      "epoch": 0.000320646942660145,
      "grad_norm": 10250.945907573603,
      "learning_rate": 3.3659732298010395e-07,
      "loss": 1.7768,
      "step": 88352
    },
    {
      "epoch": 0.0003207630770109817,
      "grad_norm": 7717.517735645315,
      "learning_rate": 3.3653632241136893e-07,
      "loss": 1.7745,
      "step": 88384
    },
    {
      "epoch": 0.0003208792113618184,
      "grad_norm": 9365.075546945684,
      "learning_rate": 3.3647535499549277e-07,
      "loss": 1.7798,
      "step": 88416
    },
    {
      "epoch": 0.0003209953457126551,
      "grad_norm": 9610.343178055611,
      "learning_rate": 3.3641442070245625e-07,
      "loss": 1.7902,
      "step": 88448
    },
    {
      "epoch": 0.0003211114800634918,
      "grad_norm": 9143.102974373634,
      "learning_rate": 3.363535195022781e-07,
      "loss": 1.802,
      "step": 88480
    },
    {
      "epoch": 0.00032122761441432854,
      "grad_norm": 10723.14757895274,
      "learning_rate": 3.3629265136501523e-07,
      "loss": 1.7757,
      "step": 88512
    },
    {
      "epoch": 0.00032134374876516524,
      "grad_norm": 10405.132195219818,
      "learning_rate": 3.3623181626076223e-07,
      "loss": 1.7671,
      "step": 88544
    },
    {
      "epoch": 0.00032145988311600194,
      "grad_norm": 8957.537049881514,
      "learning_rate": 3.3617101415965173e-07,
      "loss": 1.7615,
      "step": 88576
    },
    {
      "epoch": 0.00032157601746683864,
      "grad_norm": 9859.432843728893,
      "learning_rate": 3.3611024503185405e-07,
      "loss": 1.7666,
      "step": 88608
    },
    {
      "epoch": 0.00032169215181767534,
      "grad_norm": 9120.339138431202,
      "learning_rate": 3.3604950884757733e-07,
      "loss": 1.7401,
      "step": 88640
    },
    {
      "epoch": 0.00032180828616851204,
      "grad_norm": 8304.230849392374,
      "learning_rate": 3.359888055770673e-07,
      "loss": 1.7323,
      "step": 88672
    },
    {
      "epoch": 0.00032192442051934874,
      "grad_norm": 10902.76753856561,
      "learning_rate": 3.359281351906073e-07,
      "loss": 1.7467,
      "step": 88704
    },
    {
      "epoch": 0.00032204055487018544,
      "grad_norm": 8676.1793434668,
      "learning_rate": 3.358674976585184e-07,
      "loss": 1.7619,
      "step": 88736
    },
    {
      "epoch": 0.00032215668922102214,
      "grad_norm": 9116.727702416038,
      "learning_rate": 3.3580689295115875e-07,
      "loss": 1.7757,
      "step": 88768
    },
    {
      "epoch": 0.0003222728235718589,
      "grad_norm": 9249.616316366857,
      "learning_rate": 3.357463210389244e-07,
      "loss": 1.7754,
      "step": 88800
    },
    {
      "epoch": 0.0003223889579226956,
      "grad_norm": 9031.925043975953,
      "learning_rate": 3.356857818922484e-07,
      "loss": 1.7745,
      "step": 88832
    },
    {
      "epoch": 0.0003225050922735323,
      "grad_norm": 10344.8735130015,
      "learning_rate": 3.356252754816014e-07,
      "loss": 1.7659,
      "step": 88864
    },
    {
      "epoch": 0.000322621226624369,
      "grad_norm": 10517.788550831398,
      "learning_rate": 3.3556480177749103e-07,
      "loss": 1.7583,
      "step": 88896
    },
    {
      "epoch": 0.0003227373609752057,
      "grad_norm": 8965.03965412312,
      "learning_rate": 3.3550436075046234e-07,
      "loss": 1.7475,
      "step": 88928
    },
    {
      "epoch": 0.0003228534953260424,
      "grad_norm": 9122.23799294888,
      "learning_rate": 3.3544395237109724e-07,
      "loss": 1.735,
      "step": 88960
    },
    {
      "epoch": 0.0003229696296768791,
      "grad_norm": 9716.70355624787,
      "learning_rate": 3.35383576610015e-07,
      "loss": 1.7499,
      "step": 88992
    },
    {
      "epoch": 0.0003230857640277158,
      "grad_norm": 8975.914660913393,
      "learning_rate": 3.3532511866900266e-07,
      "loss": 1.7549,
      "step": 89024
    },
    {
      "epoch": 0.0003232018983785525,
      "grad_norm": 9791.168469595445,
      "learning_rate": 3.352648070394461e-07,
      "loss": 1.7663,
      "step": 89056
    },
    {
      "epoch": 0.0003233180327293892,
      "grad_norm": 9554.680319089697,
      "learning_rate": 3.352045279411655e-07,
      "loss": 1.7885,
      "step": 89088
    },
    {
      "epoch": 0.00032343416708022595,
      "grad_norm": 10163.76632946665,
      "learning_rate": 3.351442813449267e-07,
      "loss": 1.7853,
      "step": 89120
    },
    {
      "epoch": 0.00032355030143106265,
      "grad_norm": 7511.147981500564,
      "learning_rate": 3.350840672215319e-07,
      "loss": 1.7726,
      "step": 89152
    },
    {
      "epoch": 0.00032366643578189935,
      "grad_norm": 9414.486921760526,
      "learning_rate": 3.3502388554182035e-07,
      "loss": 1.7763,
      "step": 89184
    },
    {
      "epoch": 0.00032378257013273605,
      "grad_norm": 10061.543420370455,
      "learning_rate": 3.349637362766677e-07,
      "loss": 1.7814,
      "step": 89216
    },
    {
      "epoch": 0.00032389870448357275,
      "grad_norm": 9959.310919938187,
      "learning_rate": 3.3490361939698646e-07,
      "loss": 1.7739,
      "step": 89248
    },
    {
      "epoch": 0.00032401483883440945,
      "grad_norm": 9382.753007513307,
      "learning_rate": 3.348435348737253e-07,
      "loss": 1.7759,
      "step": 89280
    },
    {
      "epoch": 0.00032413097318524615,
      "grad_norm": 9354.67006366339,
      "learning_rate": 3.347834826778697e-07,
      "loss": 1.7874,
      "step": 89312
    },
    {
      "epoch": 0.00032424710753608285,
      "grad_norm": 9284.204327781676,
      "learning_rate": 3.3472346278044137e-07,
      "loss": 1.7908,
      "step": 89344
    },
    {
      "epoch": 0.00032436324188691955,
      "grad_norm": 10117.896224018114,
      "learning_rate": 3.3466347515249834e-07,
      "loss": 1.7636,
      "step": 89376
    },
    {
      "epoch": 0.0003244793762377563,
      "grad_norm": 8459.435796789287,
      "learning_rate": 3.3460351976513513e-07,
      "loss": 1.78,
      "step": 89408
    },
    {
      "epoch": 0.000324595510588593,
      "grad_norm": 10285.19012950174,
      "learning_rate": 3.3454359658948236e-07,
      "loss": 1.7627,
      "step": 89440
    },
    {
      "epoch": 0.0003247116449394297,
      "grad_norm": 9465.812590580907,
      "learning_rate": 3.3448370559670677e-07,
      "loss": 1.7395,
      "step": 89472
    },
    {
      "epoch": 0.0003248277792902664,
      "grad_norm": 9382.364840486645,
      "learning_rate": 3.344238467580114e-07,
      "loss": 1.746,
      "step": 89504
    },
    {
      "epoch": 0.0003249439136411031,
      "grad_norm": 9804.8292182985,
      "learning_rate": 3.343640200446352e-07,
      "loss": 1.74,
      "step": 89536
    },
    {
      "epoch": 0.0003250600479919398,
      "grad_norm": 9595.83597192032,
      "learning_rate": 3.343042254278531e-07,
      "loss": 1.7466,
      "step": 89568
    },
    {
      "epoch": 0.0003251761823427765,
      "grad_norm": 9832.631997588438,
      "learning_rate": 3.342444628789762e-07,
      "loss": 1.7727,
      "step": 89600
    },
    {
      "epoch": 0.0003252923166936132,
      "grad_norm": 9129.238522461772,
      "learning_rate": 3.341847323693512e-07,
      "loss": 1.771,
      "step": 89632
    },
    {
      "epoch": 0.0003254084510444499,
      "grad_norm": 8339.319996258688,
      "learning_rate": 3.3412503387036084e-07,
      "loss": 1.7571,
      "step": 89664
    },
    {
      "epoch": 0.00032552458539528666,
      "grad_norm": 8261.056106818305,
      "learning_rate": 3.3406536735342355e-07,
      "loss": 1.7474,
      "step": 89696
    },
    {
      "epoch": 0.00032564071974612336,
      "grad_norm": 9849.141637726609,
      "learning_rate": 3.340057327899934e-07,
      "loss": 1.7493,
      "step": 89728
    },
    {
      "epoch": 0.00032575685409696006,
      "grad_norm": 9983.617580817085,
      "learning_rate": 3.3394613015156033e-07,
      "loss": 1.7395,
      "step": 89760
    },
    {
      "epoch": 0.00032587298844779676,
      "grad_norm": 9445.04960283428,
      "learning_rate": 3.3388655940964964e-07,
      "loss": 1.7374,
      "step": 89792
    },
    {
      "epoch": 0.00032598912279863346,
      "grad_norm": 10660.27157252572,
      "learning_rate": 3.3382702053582236e-07,
      "loss": 1.7604,
      "step": 89824
    },
    {
      "epoch": 0.00032610525714947016,
      "grad_norm": 9017.372344535852,
      "learning_rate": 3.3376751350167494e-07,
      "loss": 1.7729,
      "step": 89856
    },
    {
      "epoch": 0.00032622139150030686,
      "grad_norm": 9526.778259201796,
      "learning_rate": 3.3370803827883914e-07,
      "loss": 1.7765,
      "step": 89888
    },
    {
      "epoch": 0.00032633752585114356,
      "grad_norm": 9631.264922116929,
      "learning_rate": 3.3364859483898233e-07,
      "loss": 1.7853,
      "step": 89920
    },
    {
      "epoch": 0.00032645366020198026,
      "grad_norm": 9289.850160255546,
      "learning_rate": 3.33589183153807e-07,
      "loss": 1.7848,
      "step": 89952
    },
    {
      "epoch": 0.000326569794552817,
      "grad_norm": 9327.536116252779,
      "learning_rate": 3.3352980319505096e-07,
      "loss": 1.8005,
      "step": 89984
    },
    {
      "epoch": 0.0003266859289036537,
      "grad_norm": 10152.566375060052,
      "learning_rate": 3.334704549344873e-07,
      "loss": 1.8126,
      "step": 90016
    },
    {
      "epoch": 0.0003268020632544904,
      "grad_norm": 8991.202144318633,
      "learning_rate": 3.334129915082791e-07,
      "loss": 1.7926,
      "step": 90048
    },
    {
      "epoch": 0.0003269181976053271,
      "grad_norm": 10430.506890846676,
      "learning_rate": 3.333537055711779e-07,
      "loss": 1.7682,
      "step": 90080
    },
    {
      "epoch": 0.0003270343319561638,
      "grad_norm": 8907.292967001815,
      "learning_rate": 3.33294451248677e-07,
      "loss": 1.7702,
      "step": 90112
    },
    {
      "epoch": 0.0003271504663070005,
      "grad_norm": 8685.262114639949,
      "learning_rate": 3.3323522851268854e-07,
      "loss": 1.7519,
      "step": 90144
    },
    {
      "epoch": 0.0003272666006578372,
      "grad_norm": 9254.316614423778,
      "learning_rate": 3.3317603733515976e-07,
      "loss": 1.7629,
      "step": 90176
    },
    {
      "epoch": 0.0003273827350086739,
      "grad_norm": 10338.384206441546,
      "learning_rate": 3.3311687768807256e-07,
      "loss": 1.7549,
      "step": 90208
    },
    {
      "epoch": 0.0003274988693595106,
      "grad_norm": 9480.518551218598,
      "learning_rate": 3.3305774954344367e-07,
      "loss": 1.7451,
      "step": 90240
    },
    {
      "epoch": 0.00032761500371034736,
      "grad_norm": 9036.755280519663,
      "learning_rate": 3.3299865287332465e-07,
      "loss": 1.7426,
      "step": 90272
    },
    {
      "epoch": 0.00032773113806118406,
      "grad_norm": 9364.761182219225,
      "learning_rate": 3.3293958764980175e-07,
      "loss": 1.7478,
      "step": 90304
    },
    {
      "epoch": 0.00032784727241202076,
      "grad_norm": 9097.728507710042,
      "learning_rate": 3.328805538449958e-07,
      "loss": 1.7625,
      "step": 90336
    },
    {
      "epoch": 0.00032796340676285746,
      "grad_norm": 9074.771622470727,
      "learning_rate": 3.3282155143106237e-07,
      "loss": 1.765,
      "step": 90368
    },
    {
      "epoch": 0.00032807954111369416,
      "grad_norm": 11038.771399028064,
      "learning_rate": 3.327625803801915e-07,
      "loss": 1.754,
      "step": 90400
    },
    {
      "epoch": 0.00032819567546453086,
      "grad_norm": 9753.48204489043,
      "learning_rate": 3.327036406646075e-07,
      "loss": 1.7575,
      "step": 90432
    },
    {
      "epoch": 0.00032831180981536756,
      "grad_norm": 9574.66490275247,
      "learning_rate": 3.3264473225656946e-07,
      "loss": 1.7519,
      "step": 90464
    },
    {
      "epoch": 0.00032842794416620426,
      "grad_norm": 8795.69462862371,
      "learning_rate": 3.325858551283706e-07,
      "loss": 1.754,
      "step": 90496
    },
    {
      "epoch": 0.00032854407851704096,
      "grad_norm": 8986.84494135734,
      "learning_rate": 3.325270092523385e-07,
      "loss": 1.745,
      "step": 90528
    },
    {
      "epoch": 0.0003286602128678777,
      "grad_norm": 8358.941440158556,
      "learning_rate": 3.3246819460083506e-07,
      "loss": 1.769,
      "step": 90560
    },
    {
      "epoch": 0.0003287763472187144,
      "grad_norm": 9943.719123145023,
      "learning_rate": 3.3240941114625636e-07,
      "loss": 1.7871,
      "step": 90592
    },
    {
      "epoch": 0.0003288924815695511,
      "grad_norm": 10478.637411419482,
      "learning_rate": 3.3235065886103264e-07,
      "loss": 1.7885,
      "step": 90624
    },
    {
      "epoch": 0.0003290086159203878,
      "grad_norm": 8993.815875366807,
      "learning_rate": 3.3229193771762813e-07,
      "loss": 1.7728,
      "step": 90656
    },
    {
      "epoch": 0.0003291247502712245,
      "grad_norm": 9846.474292862395,
      "learning_rate": 3.322332476885413e-07,
      "loss": 1.7624,
      "step": 90688
    },
    {
      "epoch": 0.0003292408846220612,
      "grad_norm": 10090.839410078826,
      "learning_rate": 3.321745887463044e-07,
      "loss": 1.7668,
      "step": 90720
    },
    {
      "epoch": 0.0003293570189728979,
      "grad_norm": 10014.250046808298,
      "learning_rate": 3.321159608634838e-07,
      "loss": 1.7867,
      "step": 90752
    },
    {
      "epoch": 0.0003294731533237346,
      "grad_norm": 8623.571649844396,
      "learning_rate": 3.3205736401267965e-07,
      "loss": 1.7837,
      "step": 90784
    },
    {
      "epoch": 0.0003295892876745713,
      "grad_norm": 8460.504476684591,
      "learning_rate": 3.31998798166526e-07,
      "loss": 1.7672,
      "step": 90816
    },
    {
      "epoch": 0.00032970542202540807,
      "grad_norm": 9614.626877835666,
      "learning_rate": 3.319402632976905e-07,
      "loss": 1.7738,
      "step": 90848
    },
    {
      "epoch": 0.00032982155637624477,
      "grad_norm": 8442.560275177193,
      "learning_rate": 3.3188175937887477e-07,
      "loss": 1.7632,
      "step": 90880
    },
    {
      "epoch": 0.00032993769072708147,
      "grad_norm": 9380.217481487303,
      "learning_rate": 3.3182328638281397e-07,
      "loss": 1.7538,
      "step": 90912
    },
    {
      "epoch": 0.00033005382507791817,
      "grad_norm": 8560.16845628636,
      "learning_rate": 3.3176484428227674e-07,
      "loss": 1.7621,
      "step": 90944
    },
    {
      "epoch": 0.00033016995942875487,
      "grad_norm": 9543.405052705244,
      "learning_rate": 3.3170643305006555e-07,
      "loss": 1.7531,
      "step": 90976
    },
    {
      "epoch": 0.00033028609377959157,
      "grad_norm": 11127.501786115336,
      "learning_rate": 3.316480526590162e-07,
      "loss": 1.7565,
      "step": 91008
    },
    {
      "epoch": 0.00033040222813042827,
      "grad_norm": 8514.497401491177,
      "learning_rate": 3.315915260401258e-07,
      "loss": 1.7599,
      "step": 91040
    },
    {
      "epoch": 0.00033051836248126497,
      "grad_norm": 10555.630724878547,
      "learning_rate": 3.3153320628835954e-07,
      "loss": 1.76,
      "step": 91072
    },
    {
      "epoch": 0.00033063449683210167,
      "grad_norm": 9349.180926690851,
      "learning_rate": 3.314749172973085e-07,
      "loss": 1.7625,
      "step": 91104
    },
    {
      "epoch": 0.0003307506311829384,
      "grad_norm": 10452.274202296838,
      "learning_rate": 3.3141665903994093e-07,
      "loss": 1.767,
      "step": 91136
    },
    {
      "epoch": 0.0003308667655337751,
      "grad_norm": 8949.487694834828,
      "learning_rate": 3.313584314892583e-07,
      "loss": 1.7744,
      "step": 91168
    },
    {
      "epoch": 0.0003309828998846118,
      "grad_norm": 9848.60629733974,
      "learning_rate": 3.3130023461829547e-07,
      "loss": 1.7654,
      "step": 91200
    },
    {
      "epoch": 0.0003310990342354485,
      "grad_norm": 10102.325276885515,
      "learning_rate": 3.312420684001202e-07,
      "loss": 1.7468,
      "step": 91232
    },
    {
      "epoch": 0.0003312151685862852,
      "grad_norm": 11711.759731142029,
      "learning_rate": 3.311839328078336e-07,
      "loss": 1.7591,
      "step": 91264
    },
    {
      "epoch": 0.0003313313029371219,
      "grad_norm": 8711.375436749355,
      "learning_rate": 3.3112582781456954e-07,
      "loss": 1.7408,
      "step": 91296
    },
    {
      "epoch": 0.0003314474372879586,
      "grad_norm": 8914.711885417273,
      "learning_rate": 3.3106775339349515e-07,
      "loss": 1.764,
      "step": 91328
    },
    {
      "epoch": 0.0003315635716387953,
      "grad_norm": 9536.540882311574,
      "learning_rate": 3.310097095178102e-07,
      "loss": 1.7788,
      "step": 91360
    },
    {
      "epoch": 0.000331679705989632,
      "grad_norm": 9801.528962360924,
      "learning_rate": 3.3095169616074763e-07,
      "loss": 1.7696,
      "step": 91392
    },
    {
      "epoch": 0.0003317958403404688,
      "grad_norm": 8602.331428165275,
      "learning_rate": 3.3089371329557293e-07,
      "loss": 1.7587,
      "step": 91424
    },
    {
      "epoch": 0.0003319119746913055,
      "grad_norm": 9486.193335579874,
      "learning_rate": 3.308357608955846e-07,
      "loss": 1.7584,
      "step": 91456
    },
    {
      "epoch": 0.0003320281090421422,
      "grad_norm": 10318.060379741923,
      "learning_rate": 3.307778389341138e-07,
      "loss": 1.7631,
      "step": 91488
    },
    {
      "epoch": 0.0003321442433929789,
      "grad_norm": 9182.490947449935,
      "learning_rate": 3.3071994738452427e-07,
      "loss": 1.7671,
      "step": 91520
    },
    {
      "epoch": 0.0003322603777438156,
      "grad_norm": 9234.465008867597,
      "learning_rate": 3.306620862202124e-07,
      "loss": 1.7841,
      "step": 91552
    },
    {
      "epoch": 0.0003323765120946523,
      "grad_norm": 9972.967061010479,
      "learning_rate": 3.306042554146072e-07,
      "loss": 1.7794,
      "step": 91584
    },
    {
      "epoch": 0.000332492646445489,
      "grad_norm": 9140.097045436662,
      "learning_rate": 3.3054645494117024e-07,
      "loss": 1.77,
      "step": 91616
    },
    {
      "epoch": 0.0003326087807963257,
      "grad_norm": 8083.304151150072,
      "learning_rate": 3.3048868477339543e-07,
      "loss": 1.7649,
      "step": 91648
    },
    {
      "epoch": 0.0003327249151471624,
      "grad_norm": 10379.47763618189,
      "learning_rate": 3.304309448848092e-07,
      "loss": 1.7646,
      "step": 91680
    },
    {
      "epoch": 0.00033284104949799913,
      "grad_norm": 8869.55444202244,
      "learning_rate": 3.303732352489703e-07,
      "loss": 1.7543,
      "step": 91712
    },
    {
      "epoch": 0.00033295718384883583,
      "grad_norm": 9421.309038557221,
      "learning_rate": 3.3031555583946966e-07,
      "loss": 1.7695,
      "step": 91744
    },
    {
      "epoch": 0.00033307331819967253,
      "grad_norm": 8041.896915529321,
      "learning_rate": 3.302579066299309e-07,
      "loss": 1.7775,
      "step": 91776
    },
    {
      "epoch": 0.00033318945255050923,
      "grad_norm": 11391.611299548453,
      "learning_rate": 3.3020028759400934e-07,
      "loss": 1.7548,
      "step": 91808
    },
    {
      "epoch": 0.00033330558690134593,
      "grad_norm": 10108.533622637855,
      "learning_rate": 3.301426987053928e-07,
      "loss": 1.7603,
      "step": 91840
    },
    {
      "epoch": 0.00033342172125218263,
      "grad_norm": 8388.615976429008,
      "learning_rate": 3.3008513993780106e-07,
      "loss": 1.7823,
      "step": 91872
    },
    {
      "epoch": 0.00033353785560301933,
      "grad_norm": 9875.914641186406,
      "learning_rate": 3.30027611264986e-07,
      "loss": 1.7759,
      "step": 91904
    },
    {
      "epoch": 0.00033365398995385603,
      "grad_norm": 9861.551399247484,
      "learning_rate": 3.299701126607315e-07,
      "loss": 1.769,
      "step": 91936
    },
    {
      "epoch": 0.00033377012430469273,
      "grad_norm": 8925.184255801109,
      "learning_rate": 3.299126440988535e-07,
      "loss": 1.7519,
      "step": 91968
    },
    {
      "epoch": 0.0003338862586555295,
      "grad_norm": 8402.865344630962,
      "learning_rate": 3.2985520555319964e-07,
      "loss": 1.7328,
      "step": 92000
    },
    {
      "epoch": 0.0003340023930063662,
      "grad_norm": 18205.03358964218,
      "learning_rate": 3.297977969976497e-07,
      "loss": 1.7321,
      "step": 92032
    },
    {
      "epoch": 0.0003341185273572029,
      "grad_norm": 8560.851359531947,
      "learning_rate": 3.297422110338025e-07,
      "loss": 1.7421,
      "step": 92064
    },
    {
      "epoch": 0.0003342346617080396,
      "grad_norm": 10025.412909202294,
      "learning_rate": 3.2968486144505914e-07,
      "loss": 1.7527,
      "step": 92096
    },
    {
      "epoch": 0.0003343507960588763,
      "grad_norm": 8949.9039100987,
      "learning_rate": 3.29627541769062e-07,
      "loss": 1.7736,
      "step": 92128
    },
    {
      "epoch": 0.000334466930409713,
      "grad_norm": 10162.94071615101,
      "learning_rate": 3.295702519798167e-07,
      "loss": 1.7841,
      "step": 92160
    },
    {
      "epoch": 0.0003345830647605497,
      "grad_norm": 9114.691656880117,
      "learning_rate": 3.2951299205136036e-07,
      "loss": 1.7965,
      "step": 92192
    },
    {
      "epoch": 0.0003346991991113864,
      "grad_norm": 10065.554728876099,
      "learning_rate": 3.294557619577617e-07,
      "loss": 1.7593,
      "step": 92224
    },
    {
      "epoch": 0.0003348153334622231,
      "grad_norm": 8032.064989776913,
      "learning_rate": 3.293985616731212e-07,
      "loss": 1.7675,
      "step": 92256
    },
    {
      "epoch": 0.00033493146781305984,
      "grad_norm": 8841.776631424253,
      "learning_rate": 3.2934139117157036e-07,
      "loss": 1.7812,
      "step": 92288
    },
    {
      "epoch": 0.00033504760216389654,
      "grad_norm": 9605.9288983419,
      "learning_rate": 3.2928425042727254e-07,
      "loss": 1.7795,
      "step": 92320
    },
    {
      "epoch": 0.00033516373651473324,
      "grad_norm": 9898.520444995807,
      "learning_rate": 3.2922713941442213e-07,
      "loss": 1.7765,
      "step": 92352
    },
    {
      "epoch": 0.00033527987086556994,
      "grad_norm": 9502.570599579883,
      "learning_rate": 3.2917005810724504e-07,
      "loss": 1.7763,
      "step": 92384
    },
    {
      "epoch": 0.00033539600521640664,
      "grad_norm": 9289.404501904306,
      "learning_rate": 3.2911300647999844e-07,
      "loss": 1.7312,
      "step": 92416
    },
    {
      "epoch": 0.00033551213956724334,
      "grad_norm": 8406.568741169016,
      "learning_rate": 3.290559845069706e-07,
      "loss": 1.7545,
      "step": 92448
    },
    {
      "epoch": 0.00033562827391808004,
      "grad_norm": 8960.72474747439,
      "learning_rate": 3.289989921624811e-07,
      "loss": 1.7626,
      "step": 92480
    },
    {
      "epoch": 0.00033574440826891674,
      "grad_norm": 10196.423294469487,
      "learning_rate": 3.289420294208805e-07,
      "loss": 1.7578,
      "step": 92512
    },
    {
      "epoch": 0.00033586054261975344,
      "grad_norm": 7751.59596470301,
      "learning_rate": 3.2888509625655053e-07,
      "loss": 1.7631,
      "step": 92544
    },
    {
      "epoch": 0.0003359766769705902,
      "grad_norm": 9618.455697252028,
      "learning_rate": 3.28828192643904e-07,
      "loss": 1.7485,
      "step": 92576
    },
    {
      "epoch": 0.0003360928113214269,
      "grad_norm": 8833.012623108834,
      "learning_rate": 3.2877131855738454e-07,
      "loss": 1.7573,
      "step": 92608
    },
    {
      "epoch": 0.0003362089456722636,
      "grad_norm": 8503.980244567834,
      "learning_rate": 3.287144739714668e-07,
      "loss": 1.7607,
      "step": 92640
    },
    {
      "epoch": 0.0003363250800231003,
      "grad_norm": 10196.689756975054,
      "learning_rate": 3.286576588606564e-07,
      "loss": 1.7651,
      "step": 92672
    },
    {
      "epoch": 0.000336441214373937,
      "grad_norm": 10255.96031583586,
      "learning_rate": 3.286008731994898e-07,
      "loss": 1.7632,
      "step": 92704
    },
    {
      "epoch": 0.0003365573487247737,
      "grad_norm": 9152.095934811872,
      "learning_rate": 3.2854411696253394e-07,
      "loss": 1.7541,
      "step": 92736
    },
    {
      "epoch": 0.0003366734830756104,
      "grad_norm": 9543.033322796267,
      "learning_rate": 3.2848739012438674e-07,
      "loss": 1.7751,
      "step": 92768
    },
    {
      "epoch": 0.0003367896174264471,
      "grad_norm": 10066.072322410564,
      "learning_rate": 3.2843069265967694e-07,
      "loss": 1.7538,
      "step": 92800
    },
    {
      "epoch": 0.0003369057517772838,
      "grad_norm": 9267.429201240224,
      "learning_rate": 3.283740245430636e-07,
      "loss": 1.742,
      "step": 92832
    },
    {
      "epoch": 0.00033702188612812054,
      "grad_norm": 8738.506165243576,
      "learning_rate": 3.2831738574923665e-07,
      "loss": 1.7627,
      "step": 92864
    },
    {
      "epoch": 0.00033713802047895724,
      "grad_norm": 9981.340791697276,
      "learning_rate": 3.282607762529165e-07,
      "loss": 1.7801,
      "step": 92896
    },
    {
      "epoch": 0.00033725415482979394,
      "grad_norm": 9159.461774580426,
      "learning_rate": 3.282041960288538e-07,
      "loss": 1.784,
      "step": 92928
    },
    {
      "epoch": 0.00033737028918063064,
      "grad_norm": 10441.19820710248,
      "learning_rate": 3.281476450518301e-07,
      "loss": 1.7783,
      "step": 92960
    },
    {
      "epoch": 0.00033748642353146734,
      "grad_norm": 9328.215263382379,
      "learning_rate": 3.2809112329665716e-07,
      "loss": 1.7684,
      "step": 92992
    },
    {
      "epoch": 0.00033760255788230404,
      "grad_norm": 9460.859897493461,
      "learning_rate": 3.2803463073817695e-07,
      "loss": 1.764,
      "step": 93024
    },
    {
      "epoch": 0.00033771869223314074,
      "grad_norm": 9416.009876800259,
      "learning_rate": 3.279799313907905e-07,
      "loss": 1.79,
      "step": 93056
    },
    {
      "epoch": 0.00033783482658397744,
      "grad_norm": 9954.191278049662,
      "learning_rate": 3.279234962398958e-07,
      "loss": 1.7719,
      "step": 93088
    },
    {
      "epoch": 0.00033795096093481414,
      "grad_norm": 9156.894779345233,
      "learning_rate": 3.278670902111847e-07,
      "loss": 1.7734,
      "step": 93120
    },
    {
      "epoch": 0.0003380670952856509,
      "grad_norm": 9604.641586233189,
      "learning_rate": 3.278107132796193e-07,
      "loss": 1.7581,
      "step": 93152
    },
    {
      "epoch": 0.0003381832296364876,
      "grad_norm": 9676.914590922046,
      "learning_rate": 3.2775436542019193e-07,
      "loss": 1.7276,
      "step": 93184
    },
    {
      "epoch": 0.0003382993639873243,
      "grad_norm": 9510.154783177823,
      "learning_rate": 3.2769804660792487e-07,
      "loss": 1.7388,
      "step": 93216
    },
    {
      "epoch": 0.000338415498338161,
      "grad_norm": 8830.725677994986,
      "learning_rate": 3.2764175681787045e-07,
      "loss": 1.7338,
      "step": 93248
    },
    {
      "epoch": 0.0003385316326889977,
      "grad_norm": 10549.349932578783,
      "learning_rate": 3.275854960251111e-07,
      "loss": 1.7461,
      "step": 93280
    },
    {
      "epoch": 0.0003386477670398344,
      "grad_norm": 9246.400813289461,
      "learning_rate": 3.2752926420475904e-07,
      "loss": 1.75,
      "step": 93312
    },
    {
      "epoch": 0.0003387639013906711,
      "grad_norm": 10119.417572172817,
      "learning_rate": 3.274730613319565e-07,
      "loss": 1.7439,
      "step": 93344
    },
    {
      "epoch": 0.0003388800357415078,
      "grad_norm": 9069.242085202048,
      "learning_rate": 3.274168873818755e-07,
      "loss": 1.7691,
      "step": 93376
    },
    {
      "epoch": 0.0003389961700923445,
      "grad_norm": 8372.696041299958,
      "learning_rate": 3.2736074232971777e-07,
      "loss": 1.7865,
      "step": 93408
    },
    {
      "epoch": 0.00033911230444318125,
      "grad_norm": 9617.480751215466,
      "learning_rate": 3.2730462615071503e-07,
      "loss": 1.7671,
      "step": 93440
    },
    {
      "epoch": 0.00033922843879401795,
      "grad_norm": 10030.34206794564,
      "learning_rate": 3.2724853882012855e-07,
      "loss": 1.7648,
      "step": 93472
    },
    {
      "epoch": 0.00033934457314485465,
      "grad_norm": 8166.968103280434,
      "learning_rate": 3.271924803132493e-07,
      "loss": 1.7688,
      "step": 93504
    },
    {
      "epoch": 0.00033946070749569135,
      "grad_norm": 9004.154541099348,
      "learning_rate": 3.2713645060539784e-07,
      "loss": 1.7723,
      "step": 93536
    },
    {
      "epoch": 0.00033957684184652805,
      "grad_norm": 8317.08723051526,
      "learning_rate": 3.2708044967192427e-07,
      "loss": 1.7539,
      "step": 93568
    },
    {
      "epoch": 0.00033969297619736475,
      "grad_norm": 9359.065551645634,
      "learning_rate": 3.2702447748820847e-07,
      "loss": 1.763,
      "step": 93600
    },
    {
      "epoch": 0.00033980911054820145,
      "grad_norm": 9740.842982001095,
      "learning_rate": 3.269685340296594e-07,
      "loss": 1.7837,
      "step": 93632
    },
    {
      "epoch": 0.00033992524489903815,
      "grad_norm": 8936.926652938359,
      "learning_rate": 3.269126192717157e-07,
      "loss": 1.7673,
      "step": 93664
    },
    {
      "epoch": 0.00034004137924987485,
      "grad_norm": 8870.619144118407,
      "learning_rate": 3.268567331898454e-07,
      "loss": 1.7792,
      "step": 93696
    },
    {
      "epoch": 0.0003401575136007116,
      "grad_norm": 9434.658340395797,
      "learning_rate": 3.268008757595459e-07,
      "loss": 1.7795,
      "step": 93728
    },
    {
      "epoch": 0.0003402736479515483,
      "grad_norm": 7794.945028670824,
      "learning_rate": 3.267450469563438e-07,
      "loss": 1.7526,
      "step": 93760
    },
    {
      "epoch": 0.000340389782302385,
      "grad_norm": 10206.678401909212,
      "learning_rate": 3.266892467557949e-07,
      "loss": 1.7655,
      "step": 93792
    },
    {
      "epoch": 0.0003405059166532217,
      "grad_norm": 10202.209760635193,
      "learning_rate": 3.266334751334844e-07,
      "loss": 1.7577,
      "step": 93824
    },
    {
      "epoch": 0.0003406220510040584,
      "grad_norm": 10502.928734405466,
      "learning_rate": 3.2657773206502665e-07,
      "loss": 1.7617,
      "step": 93856
    },
    {
      "epoch": 0.0003407381853548951,
      "grad_norm": 9181.380832968427,
      "learning_rate": 3.2652201752606495e-07,
      "loss": 1.7681,
      "step": 93888
    },
    {
      "epoch": 0.0003408543197057318,
      "grad_norm": 11233.748439412377,
      "learning_rate": 3.264663314922718e-07,
      "loss": 1.7668,
      "step": 93920
    },
    {
      "epoch": 0.0003409704540565685,
      "grad_norm": 8307.476632528074,
      "learning_rate": 3.2641067393934866e-07,
      "loss": 1.7343,
      "step": 93952
    },
    {
      "epoch": 0.0003410865884074052,
      "grad_norm": 9183.071708311985,
      "learning_rate": 3.263550448430261e-07,
      "loss": 1.7528,
      "step": 93984
    },
    {
      "epoch": 0.00034120272275824196,
      "grad_norm": 9528.038203114007,
      "learning_rate": 3.262994441790635e-07,
      "loss": 1.7545,
      "step": 94016
    },
    {
      "epoch": 0.00034131885710907866,
      "grad_norm": 9228.72753959071,
      "learning_rate": 3.262456081264776e-07,
      "loss": 1.7525,
      "step": 94048
    },
    {
      "epoch": 0.00034143499145991536,
      "grad_norm": 10926.195861323373,
      "learning_rate": 3.261900633679957e-07,
      "loss": 1.7464,
      "step": 94080
    },
    {
      "epoch": 0.00034155112581075206,
      "grad_norm": 8902.556711417232,
      "learning_rate": 3.2613454697007967e-07,
      "loss": 1.759,
      "step": 94112
    },
    {
      "epoch": 0.00034166726016158876,
      "grad_norm": 9066.838368472221,
      "learning_rate": 3.260790589086034e-07,
      "loss": 1.7772,
      "step": 94144
    },
    {
      "epoch": 0.00034178339451242546,
      "grad_norm": 9989.798396364164,
      "learning_rate": 3.2602359915946955e-07,
      "loss": 1.7559,
      "step": 94176
    },
    {
      "epoch": 0.00034189952886326216,
      "grad_norm": 8802.228127014205,
      "learning_rate": 3.259681676986093e-07,
      "loss": 1.7695,
      "step": 94208
    },
    {
      "epoch": 0.00034201566321409886,
      "grad_norm": 9796.756299918867,
      "learning_rate": 3.2591276450198265e-07,
      "loss": 1.7539,
      "step": 94240
    },
    {
      "epoch": 0.00034213179756493556,
      "grad_norm": 9321.05863086377,
      "learning_rate": 3.25857389545578e-07,
      "loss": 1.7475,
      "step": 94272
    },
    {
      "epoch": 0.00034224793191577226,
      "grad_norm": 10904.062729093226,
      "learning_rate": 3.258020428054124e-07,
      "loss": 1.7712,
      "step": 94304
    },
    {
      "epoch": 0.000342364066266609,
      "grad_norm": 9661.941833813738,
      "learning_rate": 3.257467242575315e-07,
      "loss": 1.7624,
      "step": 94336
    },
    {
      "epoch": 0.0003424802006174457,
      "grad_norm": 8409.01991911067,
      "learning_rate": 3.256914338780093e-07,
      "loss": 1.758,
      "step": 94368
    },
    {
      "epoch": 0.0003425963349682824,
      "grad_norm": 9622.571277990099,
      "learning_rate": 3.2563617164294817e-07,
      "loss": 1.7703,
      "step": 94400
    },
    {
      "epoch": 0.0003427124693191191,
      "grad_norm": 10235.444006001888,
      "learning_rate": 3.25580937528479e-07,
      "loss": 1.7807,
      "step": 94432
    },
    {
      "epoch": 0.0003428286036699558,
      "grad_norm": 10048.315281677818,
      "learning_rate": 3.2552573151076087e-07,
      "loss": 1.7755,
      "step": 94464
    },
    {
      "epoch": 0.0003429447380207925,
      "grad_norm": 9798.937289318674,
      "learning_rate": 3.254705535659811e-07,
      "loss": 1.7806,
      "step": 94496
    },
    {
      "epoch": 0.0003430608723716292,
      "grad_norm": 9077.045003744335,
      "learning_rate": 3.2541540367035565e-07,
      "loss": 1.7568,
      "step": 94528
    },
    {
      "epoch": 0.0003431770067224659,
      "grad_norm": 8312.676945485131,
      "learning_rate": 3.2536028180012814e-07,
      "loss": 1.762,
      "step": 94560
    },
    {
      "epoch": 0.0003432931410733026,
      "grad_norm": 9160.451408091198,
      "learning_rate": 3.2530518793157076e-07,
      "loss": 1.7757,
      "step": 94592
    },
    {
      "epoch": 0.00034340927542413937,
      "grad_norm": 9840.598457411012,
      "learning_rate": 3.2525012204098353e-07,
      "loss": 1.7832,
      "step": 94624
    },
    {
      "epoch": 0.00034352540977497606,
      "grad_norm": 9839.44612262296,
      "learning_rate": 3.2519508410469474e-07,
      "loss": 1.7823,
      "step": 94656
    },
    {
      "epoch": 0.00034364154412581276,
      "grad_norm": 11245.28985842517,
      "learning_rate": 3.251400740990607e-07,
      "loss": 1.7511,
      "step": 94688
    },
    {
      "epoch": 0.00034375767847664946,
      "grad_norm": 9548.263925971045,
      "learning_rate": 3.2508509200046554e-07,
      "loss": 1.7633,
      "step": 94720
    },
    {
      "epoch": 0.00034387381282748616,
      "grad_norm": 11295.929178248241,
      "learning_rate": 3.250301377853215e-07,
      "loss": 1.75,
      "step": 94752
    },
    {
      "epoch": 0.00034398994717832286,
      "grad_norm": 10160.020177145318,
      "learning_rate": 3.249752114300686e-07,
      "loss": 1.752,
      "step": 94784
    },
    {
      "epoch": 0.00034410608152915956,
      "grad_norm": 10164.944859663528,
      "learning_rate": 3.2492031291117493e-07,
      "loss": 1.7449,
      "step": 94816
    },
    {
      "epoch": 0.00034422221587999626,
      "grad_norm": 9863.655103459367,
      "learning_rate": 3.248654422051361e-07,
      "loss": 1.739,
      "step": 94848
    },
    {
      "epoch": 0.00034433835023083296,
      "grad_norm": 10284.510002912146,
      "learning_rate": 3.2481059928847577e-07,
      "loss": 1.747,
      "step": 94880
    },
    {
      "epoch": 0.0003444544845816697,
      "grad_norm": 8637.096155537462,
      "learning_rate": 3.247557841377451e-07,
      "loss": 1.7736,
      "step": 94912
    },
    {
      "epoch": 0.0003445706189325064,
      "grad_norm": 11347.26451617305,
      "learning_rate": 3.2470099672952317e-07,
      "loss": 1.7617,
      "step": 94944
    },
    {
      "epoch": 0.0003446867532833431,
      "grad_norm": 9899.3328058006,
      "learning_rate": 3.246462370404165e-07,
      "loss": 1.755,
      "step": 94976
    },
    {
      "epoch": 0.0003448028876341798,
      "grad_norm": 9657.941602639767,
      "learning_rate": 3.2459150504705933e-07,
      "loss": 1.7366,
      "step": 95008
    },
    {
      "epoch": 0.0003449190219850165,
      "grad_norm": 9970.049247621599,
      "learning_rate": 3.2453680072611355e-07,
      "loss": 1.7539,
      "step": 95040
    },
    {
      "epoch": 0.0003450351563358532,
      "grad_norm": 9724.287531742364,
      "learning_rate": 3.2448383228197815e-07,
      "loss": 1.7492,
      "step": 95072
    },
    {
      "epoch": 0.0003451512906866899,
      "grad_norm": 9258.283858253644,
      "learning_rate": 3.244291823729958e-07,
      "loss": 1.7471,
      "step": 95104
    },
    {
      "epoch": 0.0003452674250375266,
      "grad_norm": 9067.846712422966,
      "learning_rate": 3.243745600673013e-07,
      "loss": 1.7697,
      "step": 95136
    },
    {
      "epoch": 0.0003453835593883633,
      "grad_norm": 10110.204547881314,
      "learning_rate": 3.243199653416655e-07,
      "loss": 1.7723,
      "step": 95168
    },
    {
      "epoch": 0.00034549969373920007,
      "grad_norm": 9586.218441074667,
      "learning_rate": 3.242653981728867e-07,
      "loss": 1.7828,
      "step": 95200
    },
    {
      "epoch": 0.00034561582809003677,
      "grad_norm": 9601.53675199965,
      "learning_rate": 3.2421085853779017e-07,
      "loss": 1.7999,
      "step": 95232
    },
    {
      "epoch": 0.00034573196244087347,
      "grad_norm": 9913.77476040282,
      "learning_rate": 3.2415634641322867e-07,
      "loss": 1.7956,
      "step": 95264
    },
    {
      "epoch": 0.00034584809679171017,
      "grad_norm": 9393.49892212694,
      "learning_rate": 3.241018617760822e-07,
      "loss": 1.789,
      "step": 95296
    },
    {
      "epoch": 0.00034596423114254687,
      "grad_norm": 10264.82323276928,
      "learning_rate": 3.240474046032579e-07,
      "loss": 1.7972,
      "step": 95328
    },
    {
      "epoch": 0.00034608036549338357,
      "grad_norm": 9273.422561276931,
      "learning_rate": 3.239929748716901e-07,
      "loss": 1.7777,
      "step": 95360
    },
    {
      "epoch": 0.00034619649984422027,
      "grad_norm": 9940.91625555713,
      "learning_rate": 3.239385725583401e-07,
      "loss": 1.7564,
      "step": 95392
    },
    {
      "epoch": 0.00034631263419505697,
      "grad_norm": 9906.090046027242,
      "learning_rate": 3.2388419764019654e-07,
      "loss": 1.7621,
      "step": 95424
    },
    {
      "epoch": 0.00034642876854589367,
      "grad_norm": 10679.703928480414,
      "learning_rate": 3.238298500942749e-07,
      "loss": 1.7304,
      "step": 95456
    },
    {
      "epoch": 0.0003465449028967304,
      "grad_norm": 8901.136444297435,
      "learning_rate": 3.2377552989761763e-07,
      "loss": 1.7452,
      "step": 95488
    },
    {
      "epoch": 0.0003466610372475671,
      "grad_norm": 9746.911408235945,
      "learning_rate": 3.237212370272942e-07,
      "loss": 1.7473,
      "step": 95520
    },
    {
      "epoch": 0.0003467771715984038,
      "grad_norm": 12382.715049616543,
      "learning_rate": 3.236669714604009e-07,
      "loss": 1.761,
      "step": 95552
    },
    {
      "epoch": 0.0003468933059492405,
      "grad_norm": 10454.571440283911,
      "learning_rate": 3.236127331740611e-07,
      "loss": 1.7404,
      "step": 95584
    },
    {
      "epoch": 0.0003470094403000772,
      "grad_norm": 9838.197497509389,
      "learning_rate": 3.2355852214542476e-07,
      "loss": 1.7447,
      "step": 95616
    },
    {
      "epoch": 0.0003471255746509139,
      "grad_norm": 9166.433766738295,
      "learning_rate": 3.235043383516688e-07,
      "loss": 1.7578,
      "step": 95648
    },
    {
      "epoch": 0.0003472417090017506,
      "grad_norm": 9089.591079911132,
      "learning_rate": 3.234501817699966e-07,
      "loss": 1.7523,
      "step": 95680
    },
    {
      "epoch": 0.0003473578433525873,
      "grad_norm": 8114.23822672221,
      "learning_rate": 3.233960523776387e-07,
      "loss": 1.7512,
      "step": 95712
    },
    {
      "epoch": 0.000347473977703424,
      "grad_norm": 9265.121801681831,
      "learning_rate": 3.233419501518519e-07,
      "loss": 1.7523,
      "step": 95744
    },
    {
      "epoch": 0.0003475901120542608,
      "grad_norm": 9388.268317426808,
      "learning_rate": 3.232878750699198e-07,
      "loss": 1.7378,
      "step": 95776
    },
    {
      "epoch": 0.0003477062464050975,
      "grad_norm": 8671.62868208735,
      "learning_rate": 3.232338271091526e-07,
      "loss": 1.748,
      "step": 95808
    },
    {
      "epoch": 0.0003478223807559342,
      "grad_norm": 8081.769236992603,
      "learning_rate": 3.23179806246887e-07,
      "loss": 1.7623,
      "step": 95840
    },
    {
      "epoch": 0.0003479385151067709,
      "grad_norm": 11093.693704082514,
      "learning_rate": 3.2312581246048614e-07,
      "loss": 1.78,
      "step": 95872
    },
    {
      "epoch": 0.0003480546494576076,
      "grad_norm": 10801.640616128645,
      "learning_rate": 3.230718457273398e-07,
      "loss": 1.7849,
      "step": 95904
    },
    {
      "epoch": 0.0003481707838084443,
      "grad_norm": 10005.05202385275,
      "learning_rate": 3.2301790602486393e-07,
      "loss": 1.7754,
      "step": 95936
    },
    {
      "epoch": 0.000348286918159281,
      "grad_norm": 9413.760247637498,
      "learning_rate": 3.2296399333050115e-07,
      "loss": 1.7803,
      "step": 95968
    },
    {
      "epoch": 0.0003484030525101177,
      "grad_norm": 9060.77248362412,
      "learning_rate": 3.229101076217202e-07,
      "loss": 1.7727,
      "step": 96000
    },
    {
      "epoch": 0.0003485191868609544,
      "grad_norm": 9965.889122401473,
      "learning_rate": 3.228562488760163e-07,
      "loss": 1.7673,
      "step": 96032
    },
    {
      "epoch": 0.00034863532121179113,
      "grad_norm": 9225.553425133909,
      "learning_rate": 3.228040989072512e-07,
      "loss": 1.7784,
      "step": 96064
    },
    {
      "epoch": 0.00034875145556262783,
      "grad_norm": 9532.532507156742,
      "learning_rate": 3.2275029317943933e-07,
      "loss": 1.7807,
      "step": 96096
    },
    {
      "epoch": 0.00034886758991346453,
      "grad_norm": 9914.168447227434,
      "learning_rate": 3.2269651434804775e-07,
      "loss": 1.7826,
      "step": 96128
    },
    {
      "epoch": 0.00034898372426430123,
      "grad_norm": 9941.965097504619,
      "learning_rate": 3.226427623906755e-07,
      "loss": 1.7854,
      "step": 96160
    },
    {
      "epoch": 0.00034909985861513793,
      "grad_norm": 8911.553512154882,
      "learning_rate": 3.2258903728494806e-07,
      "loss": 1.7691,
      "step": 96192
    },
    {
      "epoch": 0.00034921599296597463,
      "grad_norm": 11006.965521886585,
      "learning_rate": 3.2253533900851655e-07,
      "loss": 1.7406,
      "step": 96224
    },
    {
      "epoch": 0.00034933212731681133,
      "grad_norm": 8712.892516265767,
      "learning_rate": 3.2248166753905846e-07,
      "loss": 1.7459,
      "step": 96256
    },
    {
      "epoch": 0.00034944826166764803,
      "grad_norm": 7899.052474822534,
      "learning_rate": 3.224280228542772e-07,
      "loss": 1.7362,
      "step": 96288
    },
    {
      "epoch": 0.00034956439601848473,
      "grad_norm": 9135.981392275271,
      "learning_rate": 3.2237440493190187e-07,
      "loss": 1.7387,
      "step": 96320
    },
    {
      "epoch": 0.0003496805303693215,
      "grad_norm": 9310.419539419263,
      "learning_rate": 3.2232081374968787e-07,
      "loss": 1.7371,
      "step": 96352
    },
    {
      "epoch": 0.0003497966647201582,
      "grad_norm": 9746.992356619554,
      "learning_rate": 3.222672492854163e-07,
      "loss": 1.744,
      "step": 96384
    },
    {
      "epoch": 0.0003499127990709949,
      "grad_norm": 11268.79248189441,
      "learning_rate": 3.2221371151689406e-07,
      "loss": 1.7561,
      "step": 96416
    },
    {
      "epoch": 0.0003500289334218316,
      "grad_norm": 8227.69578922313,
      "learning_rate": 3.2216020042195386e-07,
      "loss": 1.7908,
      "step": 96448
    },
    {
      "epoch": 0.0003501450677726683,
      "grad_norm": 10576.50093367367,
      "learning_rate": 3.2210671597845436e-07,
      "loss": 1.8009,
      "step": 96480
    },
    {
      "epoch": 0.000350261202123505,
      "grad_norm": 10005.995602637451,
      "learning_rate": 3.220532581642795e-07,
      "loss": 1.7573,
      "step": 96512
    },
    {
      "epoch": 0.0003503773364743417,
      "grad_norm": 9334.840223592475,
      "learning_rate": 3.219998269573395e-07,
      "loss": 1.7288,
      "step": 96544
    },
    {
      "epoch": 0.0003504934708251784,
      "grad_norm": 11482.212678747943,
      "learning_rate": 3.2194642233556977e-07,
      "loss": 1.7501,
      "step": 96576
    },
    {
      "epoch": 0.0003506096051760151,
      "grad_norm": 8805.568465465474,
      "learning_rate": 3.2189304427693145e-07,
      "loss": 1.7272,
      "step": 96608
    },
    {
      "epoch": 0.00035072573952685184,
      "grad_norm": 8734.674235482396,
      "learning_rate": 3.2183969275941133e-07,
      "loss": 1.7517,
      "step": 96640
    },
    {
      "epoch": 0.00035084187387768854,
      "grad_norm": 9398.979093497335,
      "learning_rate": 3.2178636776102165e-07,
      "loss": 1.7645,
      "step": 96672
    },
    {
      "epoch": 0.00035095800822852524,
      "grad_norm": 10771.263249962838,
      "learning_rate": 3.2173306925980017e-07,
      "loss": 1.7575,
      "step": 96704
    },
    {
      "epoch": 0.00035107414257936194,
      "grad_norm": 9475.842970416932,
      "learning_rate": 3.216797972338102e-07,
      "loss": 1.7733,
      "step": 96736
    },
    {
      "epoch": 0.00035119027693019864,
      "grad_norm": 9448.227294048338,
      "learning_rate": 3.216265516611402e-07,
      "loss": 1.7859,
      "step": 96768
    },
    {
      "epoch": 0.00035130641128103534,
      "grad_norm": 11674.876787358398,
      "learning_rate": 3.2157333251990433e-07,
      "loss": 1.7779,
      "step": 96800
    },
    {
      "epoch": 0.00035142254563187204,
      "grad_norm": 9120.008662276588,
      "learning_rate": 3.2152013978824183e-07,
      "loss": 1.7676,
      "step": 96832
    },
    {
      "epoch": 0.00035153867998270874,
      "grad_norm": 8806.057119960102,
      "learning_rate": 3.2146697344431744e-07,
      "loss": 1.7693,
      "step": 96864
    },
    {
      "epoch": 0.00035165481433354544,
      "grad_norm": 10796.291215042322,
      "learning_rate": 3.2141383346632107e-07,
      "loss": 1.7735,
      "step": 96896
    },
    {
      "epoch": 0.0003517709486843822,
      "grad_norm": 8723.956785771006,
      "learning_rate": 3.213607198324678e-07,
      "loss": 1.7715,
      "step": 96928
    },
    {
      "epoch": 0.0003518870830352189,
      "grad_norm": 10104.703756172172,
      "learning_rate": 3.2130763252099805e-07,
      "loss": 1.7578,
      "step": 96960
    },
    {
      "epoch": 0.0003520032173860556,
      "grad_norm": 9538.717523860323,
      "learning_rate": 3.212545715101773e-07,
      "loss": 1.756,
      "step": 96992
    },
    {
      "epoch": 0.0003521193517368923,
      "grad_norm": 9880.937809742554,
      "learning_rate": 3.2120153677829623e-07,
      "loss": 1.7607,
      "step": 97024
    },
    {
      "epoch": 0.000352235486087729,
      "grad_norm": 11155.074002443911,
      "learning_rate": 3.21150184421269e-07,
      "loss": 1.7918,
      "step": 97056
    },
    {
      "epoch": 0.0003523516204385657,
      "grad_norm": 9221.367360646685,
      "learning_rate": 3.210972013627044e-07,
      "loss": 1.786,
      "step": 97088
    },
    {
      "epoch": 0.0003524677547894024,
      "grad_norm": 8316.10822440401,
      "learning_rate": 3.210442445187773e-07,
      "loss": 1.7328,
      "step": 97120
    },
    {
      "epoch": 0.0003525838891402391,
      "grad_norm": 9195.85830686837,
      "learning_rate": 3.2099131386787765e-07,
      "loss": 1.7349,
      "step": 97152
    },
    {
      "epoch": 0.0003527000234910758,
      "grad_norm": 8912.969202235583,
      "learning_rate": 3.209384093884202e-07,
      "loss": 1.7545,
      "step": 97184
    },
    {
      "epoch": 0.00035281615784191255,
      "grad_norm": 9643.654079237807,
      "learning_rate": 3.2088553105884486e-07,
      "loss": 1.7551,
      "step": 97216
    },
    {
      "epoch": 0.00035293229219274925,
      "grad_norm": 8440.274640081328,
      "learning_rate": 3.2083267885761606e-07,
      "loss": 1.7607,
      "step": 97248
    },
    {
      "epoch": 0.00035304842654358595,
      "grad_norm": 9767.964271023928,
      "learning_rate": 3.2077985276322336e-07,
      "loss": 1.7451,
      "step": 97280
    },
    {
      "epoch": 0.00035316456089442265,
      "grad_norm": 9360.944503627825,
      "learning_rate": 3.2072705275418097e-07,
      "loss": 1.7353,
      "step": 97312
    },
    {
      "epoch": 0.00035328069524525935,
      "grad_norm": 9640.156845197074,
      "learning_rate": 3.206742788090278e-07,
      "loss": 1.7572,
      "step": 97344
    },
    {
      "epoch": 0.00035339682959609605,
      "grad_norm": 9553.96462208229,
      "learning_rate": 3.2062153090632754e-07,
      "loss": 1.7534,
      "step": 97376
    },
    {
      "epoch": 0.00035351296394693275,
      "grad_norm": 8890.923574072605,
      "learning_rate": 3.2056880902466855e-07,
      "loss": 1.7615,
      "step": 97408
    },
    {
      "epoch": 0.00035362909829776945,
      "grad_norm": 11489.298847188194,
      "learning_rate": 3.205161131426638e-07,
      "loss": 1.7649,
      "step": 97440
    },
    {
      "epoch": 0.00035374523264860615,
      "grad_norm": 9330.737591423305,
      "learning_rate": 3.20463443238951e-07,
      "loss": 1.7585,
      "step": 97472
    },
    {
      "epoch": 0.0003538613669994429,
      "grad_norm": 9040.82573662384,
      "learning_rate": 3.204107992921921e-07,
      "loss": 1.7647,
      "step": 97504
    },
    {
      "epoch": 0.0003539775013502796,
      "grad_norm": 9592.423572799526,
      "learning_rate": 3.2035818128107397e-07,
      "loss": 1.7633,
      "step": 97536
    },
    {
      "epoch": 0.0003540936357011163,
      "grad_norm": 9027.381015554844,
      "learning_rate": 3.203055891843077e-07,
      "loss": 1.7708,
      "step": 97568
    },
    {
      "epoch": 0.000354209770051953,
      "grad_norm": 9564.495804798076,
      "learning_rate": 3.202530229806289e-07,
      "loss": 1.7754,
      "step": 97600
    },
    {
      "epoch": 0.0003543259044027897,
      "grad_norm": 9088.338241945004,
      "learning_rate": 3.202004826487977e-07,
      "loss": 1.7894,
      "step": 97632
    },
    {
      "epoch": 0.0003544420387536264,
      "grad_norm": 9047.542262957382,
      "learning_rate": 3.2014796816759844e-07,
      "loss": 1.8021,
      "step": 97664
    },
    {
      "epoch": 0.0003545581731044631,
      "grad_norm": 8766.60892249677,
      "learning_rate": 3.2009547951583997e-07,
      "loss": 1.7993,
      "step": 97696
    },
    {
      "epoch": 0.0003546743074552998,
      "grad_norm": 9335.99485861041,
      "learning_rate": 3.2004301667235546e-07,
      "loss": 1.7383,
      "step": 97728
    },
    {
      "epoch": 0.0003547904418061365,
      "grad_norm": 8654.0177952209,
      "learning_rate": 3.199905796160021e-07,
      "loss": 1.7497,
      "step": 97760
    },
    {
      "epoch": 0.00035490657615697325,
      "grad_norm": 7963.147367718369,
      "learning_rate": 3.199381683256617e-07,
      "loss": 1.7406,
      "step": 97792
    },
    {
      "epoch": 0.00035502271050780995,
      "grad_norm": 9925.018488647767,
      "learning_rate": 3.1988578278023994e-07,
      "loss": 1.7542,
      "step": 97824
    },
    {
      "epoch": 0.00035513884485864665,
      "grad_norm": 9535.067697714578,
      "learning_rate": 3.1983342295866686e-07,
      "loss": 1.7625,
      "step": 97856
    },
    {
      "epoch": 0.00035525497920948335,
      "grad_norm": 9757.604111665936,
      "learning_rate": 3.197810888398966e-07,
      "loss": 1.7453,
      "step": 97888
    },
    {
      "epoch": 0.00035537111356032005,
      "grad_norm": 10748.240041978965,
      "learning_rate": 3.197287804029073e-07,
      "loss": 1.764,
      "step": 97920
    },
    {
      "epoch": 0.00035548724791115675,
      "grad_norm": 8309.835497770096,
      "learning_rate": 3.1967649762670137e-07,
      "loss": 1.751,
      "step": 97952
    },
    {
      "epoch": 0.00035560338226199345,
      "grad_norm": 11946.105976425959,
      "learning_rate": 3.196242404903051e-07,
      "loss": 1.7695,
      "step": 97984
    },
    {
      "epoch": 0.00035571951661283015,
      "grad_norm": 9655.830363050089,
      "learning_rate": 3.195720089727686e-07,
      "loss": 1.7621,
      "step": 98016
    },
    {
      "epoch": 0.00035583565096366685,
      "grad_norm": 9088.215336357298,
      "learning_rate": 3.195198030531665e-07,
      "loss": 1.7282,
      "step": 98048
    },
    {
      "epoch": 0.0003559517853145036,
      "grad_norm": 10402.264945674091,
      "learning_rate": 3.19469252959357e-07,
      "loss": 1.7398,
      "step": 98080
    },
    {
      "epoch": 0.0003560679196653403,
      "grad_norm": 9619.26244573876,
      "learning_rate": 3.194170973746275e-07,
      "loss": 1.7402,
      "step": 98112
    },
    {
      "epoch": 0.000356184054016177,
      "grad_norm": 9599.460401501743,
      "learning_rate": 3.193649673258497e-07,
      "loss": 1.7432,
      "step": 98144
    },
    {
      "epoch": 0.0003563001883670137,
      "grad_norm": 11253.269569329617,
      "learning_rate": 3.1931286279219256e-07,
      "loss": 1.7715,
      "step": 98176
    },
    {
      "epoch": 0.0003564163227178504,
      "grad_norm": 10289.991010686064,
      "learning_rate": 3.192607837528489e-07,
      "loss": 1.7944,
      "step": 98208
    },
    {
      "epoch": 0.0003565324570686871,
      "grad_norm": 8638.567705354864,
      "learning_rate": 3.1920873018703526e-07,
      "loss": 1.7751,
      "step": 98240
    },
    {
      "epoch": 0.0003566485914195238,
      "grad_norm": 9452.919125857368,
      "learning_rate": 3.191567020739917e-07,
      "loss": 1.7883,
      "step": 98272
    },
    {
      "epoch": 0.0003567647257703605,
      "grad_norm": 10195.759510698554,
      "learning_rate": 3.191046993929823e-07,
      "loss": 1.7791,
      "step": 98304
    },
    {
      "epoch": 0.0003568808601211972,
      "grad_norm": 8501.913549313473,
      "learning_rate": 3.1905272212329455e-07,
      "loss": 1.7596,
      "step": 98336
    },
    {
      "epoch": 0.00035699699447203396,
      "grad_norm": 11176.900464797922,
      "learning_rate": 3.190007702442397e-07,
      "loss": 1.7589,
      "step": 98368
    },
    {
      "epoch": 0.00035711312882287066,
      "grad_norm": 9451.230501897624,
      "learning_rate": 3.1894884373515235e-07,
      "loss": 1.7531,
      "step": 98400
    },
    {
      "epoch": 0.00035722926317370736,
      "grad_norm": 9768.32145253216,
      "learning_rate": 3.1889694257539086e-07,
      "loss": 1.7754,
      "step": 98432
    },
    {
      "epoch": 0.00035734539752454406,
      "grad_norm": 10095.509298693158,
      "learning_rate": 3.1884506674433705e-07,
      "loss": 1.7641,
      "step": 98464
    },
    {
      "epoch": 0.00035746153187538076,
      "grad_norm": 8557.504309084514,
      "learning_rate": 3.1879321622139607e-07,
      "loss": 1.7405,
      "step": 98496
    },
    {
      "epoch": 0.00035757766622621746,
      "grad_norm": 8988.47528783386,
      "learning_rate": 3.1874139098599666e-07,
      "loss": 1.7401,
      "step": 98528
    },
    {
      "epoch": 0.00035769380057705416,
      "grad_norm": 10416.419922410962,
      "learning_rate": 3.186895910175909e-07,
      "loss": 1.748,
      "step": 98560
    },
    {
      "epoch": 0.00035780993492789086,
      "grad_norm": 9889.529816932652,
      "learning_rate": 3.186378162956542e-07,
      "loss": 1.7651,
      "step": 98592
    },
    {
      "epoch": 0.00035792606927872756,
      "grad_norm": 10294.479297176715,
      "learning_rate": 3.185860667996855e-07,
      "loss": 1.7695,
      "step": 98624
    },
    {
      "epoch": 0.0003580422036295643,
      "grad_norm": 9383.22204788952,
      "learning_rate": 3.1853434250920683e-07,
      "loss": 1.7389,
      "step": 98656
    },
    {
      "epoch": 0.000358158337980401,
      "grad_norm": 10404.58187530859,
      "learning_rate": 3.1848264340376344e-07,
      "loss": 1.7574,
      "step": 98688
    },
    {
      "epoch": 0.0003582744723312377,
      "grad_norm": 8260.14685099484,
      "learning_rate": 3.18430969462924e-07,
      "loss": 1.7688,
      "step": 98720
    },
    {
      "epoch": 0.0003583906066820744,
      "grad_norm": 10176.563172309205,
      "learning_rate": 3.183793206662803e-07,
      "loss": 1.777,
      "step": 98752
    },
    {
      "epoch": 0.0003585067410329111,
      "grad_norm": 10026.577282403003,
      "learning_rate": 3.183276969934472e-07,
      "loss": 1.7795,
      "step": 98784
    },
    {
      "epoch": 0.0003586228753837478,
      "grad_norm": 9678.22235743734,
      "learning_rate": 3.1827609842406296e-07,
      "loss": 1.7472,
      "step": 98816
    },
    {
      "epoch": 0.0003587390097345845,
      "grad_norm": 8863.826713107606,
      "learning_rate": 3.182245249377886e-07,
      "loss": 1.7594,
      "step": 98848
    },
    {
      "epoch": 0.0003588551440854212,
      "grad_norm": 8486.441185797496,
      "learning_rate": 3.181729765143084e-07,
      "loss": 1.7348,
      "step": 98880
    },
    {
      "epoch": 0.0003589712784362579,
      "grad_norm": 9103.78712404898,
      "learning_rate": 3.181214531333297e-07,
      "loss": 1.7636,
      "step": 98912
    },
    {
      "epoch": 0.00035908741278709467,
      "grad_norm": 10461.898298110147,
      "learning_rate": 3.180699547745826e-07,
      "loss": 1.772,
      "step": 98944
    },
    {
      "epoch": 0.00035920354713793137,
      "grad_norm": 9877.178443259998,
      "learning_rate": 3.180184814178205e-07,
      "loss": 1.7478,
      "step": 98976
    },
    {
      "epoch": 0.00035931968148876807,
      "grad_norm": 9379.36586342595,
      "learning_rate": 3.1796703304281944e-07,
      "loss": 1.7668,
      "step": 99008
    },
    {
      "epoch": 0.00035943581583960477,
      "grad_norm": 11276.755029706019,
      "learning_rate": 3.1791560962937845e-07,
      "loss": 1.7851,
      "step": 99040
    },
    {
      "epoch": 0.00035955195019044147,
      "grad_norm": 9155.05193868391,
      "learning_rate": 3.17865816982241e-07,
      "loss": 1.7647,
      "step": 99072
    },
    {
      "epoch": 0.00035966808454127817,
      "grad_norm": 8998.930158635525,
      "learning_rate": 3.178144426529253e-07,
      "loss": 1.7559,
      "step": 99104
    },
    {
      "epoch": 0.00035978421889211487,
      "grad_norm": 9069.55456458585,
      "learning_rate": 3.177630932253328e-07,
      "loss": 1.7601,
      "step": 99136
    },
    {
      "epoch": 0.00035990035324295157,
      "grad_norm": 9614.345323525675,
      "learning_rate": 3.1771176867935283e-07,
      "loss": 1.7531,
      "step": 99168
    },
    {
      "epoch": 0.00036001648759378827,
      "grad_norm": 8108.8402376665435,
      "learning_rate": 3.1766046899489796e-07,
      "loss": 1.7708,
      "step": 99200
    },
    {
      "epoch": 0.000360132621944625,
      "grad_norm": 9950.18964643388,
      "learning_rate": 3.1760919415190306e-07,
      "loss": 1.7535,
      "step": 99232
    },
    {
      "epoch": 0.0003602487562954617,
      "grad_norm": 9187.47429928378,
      "learning_rate": 3.175579441303258e-07,
      "loss": 1.7351,
      "step": 99264
    },
    {
      "epoch": 0.0003603648906462984,
      "grad_norm": 8763.467806753215,
      "learning_rate": 3.175067189101466e-07,
      "loss": 1.7379,
      "step": 99296
    },
    {
      "epoch": 0.0003604810249971351,
      "grad_norm": 9702.578007931706,
      "learning_rate": 3.174555184713682e-07,
      "loss": 1.7564,
      "step": 99328
    },
    {
      "epoch": 0.0003605971593479718,
      "grad_norm": 9189.765394176284,
      "learning_rate": 3.174043427940162e-07,
      "loss": 1.7632,
      "step": 99360
    },
    {
      "epoch": 0.0003607132936988085,
      "grad_norm": 9332.313646679477,
      "learning_rate": 3.173531918581385e-07,
      "loss": 1.7587,
      "step": 99392
    },
    {
      "epoch": 0.0003608294280496452,
      "grad_norm": 9678.937131730941,
      "learning_rate": 3.1730206564380564e-07,
      "loss": 1.7677,
      "step": 99424
    },
    {
      "epoch": 0.0003609455624004819,
      "grad_norm": 7504.004664177655,
      "learning_rate": 3.1725096413111066e-07,
      "loss": 1.7677,
      "step": 99456
    },
    {
      "epoch": 0.0003610616967513186,
      "grad_norm": 9166.364601083682,
      "learning_rate": 3.171998873001689e-07,
      "loss": 1.76,
      "step": 99488
    },
    {
      "epoch": 0.0003611778311021554,
      "grad_norm": 8036.549010613947,
      "learning_rate": 3.171488351311181e-07,
      "loss": 1.7808,
      "step": 99520
    },
    {
      "epoch": 0.0003612939654529921,
      "grad_norm": 11392.260881844306,
      "learning_rate": 3.1709780760411865e-07,
      "loss": 1.7557,
      "step": 99552
    },
    {
      "epoch": 0.0003614100998038288,
      "grad_norm": 9769.842168633022,
      "learning_rate": 3.170468046993528e-07,
      "loss": 1.7454,
      "step": 99584
    },
    {
      "epoch": 0.00036152623415466547,
      "grad_norm": 10352.864820908268,
      "learning_rate": 3.169958263970256e-07,
      "loss": 1.7647,
      "step": 99616
    },
    {
      "epoch": 0.00036164236850550217,
      "grad_norm": 9554.777025132507,
      "learning_rate": 3.1694487267736396e-07,
      "loss": 1.7559,
      "step": 99648
    },
    {
      "epoch": 0.00036175850285633887,
      "grad_norm": 9741.846642192639,
      "learning_rate": 3.1689394352061735e-07,
      "loss": 1.7489,
      "step": 99680
    },
    {
      "epoch": 0.00036187463720717557,
      "grad_norm": 10160.918954504066,
      "learning_rate": 3.1684303890705723e-07,
      "loss": 1.7572,
      "step": 99712
    },
    {
      "epoch": 0.00036199077155801227,
      "grad_norm": 9144.700213785032,
      "learning_rate": 3.1679215881697737e-07,
      "loss": 1.7584,
      "step": 99744
    },
    {
      "epoch": 0.00036210690590884897,
      "grad_norm": 9054.183342521843,
      "learning_rate": 3.1674130323069364e-07,
      "loss": 1.7589,
      "step": 99776
    },
    {
      "epoch": 0.00036222304025968567,
      "grad_norm": 9243.086497485567,
      "learning_rate": 3.166904721285439e-07,
      "loss": 1.7739,
      "step": 99808
    },
    {
      "epoch": 0.0003623391746105224,
      "grad_norm": 10637.518695635745,
      "learning_rate": 3.1663966549088837e-07,
      "loss": 1.7723,
      "step": 99840
    },
    {
      "epoch": 0.0003624553089613591,
      "grad_norm": 10887.338701445824,
      "learning_rate": 3.165888832981091e-07,
      "loss": 1.7752,
      "step": 99872
    },
    {
      "epoch": 0.0003625714433121958,
      "grad_norm": 9537.326984013916,
      "learning_rate": 3.1653812553061016e-07,
      "loss": 1.7735,
      "step": 99904
    },
    {
      "epoch": 0.0003626875776630325,
      "grad_norm": 9561.130895453738,
      "learning_rate": 3.164873921688177e-07,
      "loss": 1.7771,
      "step": 99936
    },
    {
      "epoch": 0.0003628037120138692,
      "grad_norm": 9406.539002204796,
      "learning_rate": 3.1643668319317976e-07,
      "loss": 1.7793,
      "step": 99968
    },
    {
      "epoch": 0.0003629198463647059,
      "grad_norm": 8426.897293784943,
      "learning_rate": 3.1638599858416636e-07,
      "loss": 1.7555,
      "step": 100000
    },
    {
      "epoch": 0.0003630359807155426,
      "grad_norm": 9552.949282813135,
      "learning_rate": 3.163353383222693e-07,
      "loss": 1.7484,
      "step": 100032
    },
    {
      "epoch": 0.0003631521150663793,
      "grad_norm": 10054.882595038094,
      "learning_rate": 3.162862843929012e-07,
      "loss": 1.7323,
      "step": 100064
    },
    {
      "epoch": 0.000363268249417216,
      "grad_norm": 8341.429733564864,
      "learning_rate": 3.1623567200746427e-07,
      "loss": 1.7308,
      "step": 100096
    },
    {
      "epoch": 0.0003633843837680528,
      "grad_norm": 10199.983529398467,
      "learning_rate": 3.1618508391135756e-07,
      "loss": 1.7536,
      "step": 100128
    },
    {
      "epoch": 0.0003635005181188895,
      "grad_norm": 9472.897339251596,
      "learning_rate": 3.161345200851597e-07,
      "loss": 1.758,
      "step": 100160
    },
    {
      "epoch": 0.0003636166524697262,
      "grad_norm": 9994.513394858202,
      "learning_rate": 3.1608398050947067e-07,
      "loss": 1.7556,
      "step": 100192
    },
    {
      "epoch": 0.0003637327868205629,
      "grad_norm": 10698.132734267228,
      "learning_rate": 3.160334651649124e-07,
      "loss": 1.7639,
      "step": 100224
    },
    {
      "epoch": 0.0003638489211713996,
      "grad_norm": 9692.591603900373,
      "learning_rate": 3.1598297403212853e-07,
      "loss": 1.7612,
      "step": 100256
    },
    {
      "epoch": 0.0003639650555222363,
      "grad_norm": 9300.498481264323,
      "learning_rate": 3.159325070917842e-07,
      "loss": 1.7593,
      "step": 100288
    },
    {
      "epoch": 0.000364081189873073,
      "grad_norm": 11205.638223680078,
      "learning_rate": 3.158820643245661e-07,
      "loss": 1.7405,
      "step": 100320
    },
    {
      "epoch": 0.0003641973242239097,
      "grad_norm": 9091.472378003466,
      "learning_rate": 3.158316457111828e-07,
      "loss": 1.7411,
      "step": 100352
    },
    {
      "epoch": 0.0003643134585747464,
      "grad_norm": 9082.119246079077,
      "learning_rate": 3.157812512323641e-07,
      "loss": 1.7433,
      "step": 100384
    },
    {
      "epoch": 0.00036442959292558313,
      "grad_norm": 8282.638227038533,
      "learning_rate": 3.1573088086886146e-07,
      "loss": 1.7502,
      "step": 100416
    },
    {
      "epoch": 0.00036454572727641983,
      "grad_norm": 9480.259912048825,
      "learning_rate": 3.156805346014478e-07,
      "loss": 1.7853,
      "step": 100448
    },
    {
      "epoch": 0.00036466186162725653,
      "grad_norm": 9778.206890836376,
      "learning_rate": 3.156302124109176e-07,
      "loss": 1.7934,
      "step": 100480
    },
    {
      "epoch": 0.00036477799597809323,
      "grad_norm": 9369.411080745684,
      "learning_rate": 3.155799142780865e-07,
      "loss": 1.7823,
      "step": 100512
    },
    {
      "epoch": 0.00036489413032892993,
      "grad_norm": 10149.372394389713,
      "learning_rate": 3.155296401837918e-07,
      "loss": 1.7672,
      "step": 100544
    },
    {
      "epoch": 0.00036501026467976663,
      "grad_norm": 10956.011317993423,
      "learning_rate": 3.1547939010889204e-07,
      "loss": 1.7909,
      "step": 100576
    },
    {
      "epoch": 0.00036512639903060333,
      "grad_norm": 9121.335757442546,
      "learning_rate": 3.1542916403426714e-07,
      "loss": 1.7743,
      "step": 100608
    },
    {
      "epoch": 0.00036524253338144003,
      "grad_norm": 8898.057540834405,
      "learning_rate": 3.1537896194081823e-07,
      "loss": 1.7593,
      "step": 100640
    },
    {
      "epoch": 0.00036535866773227673,
      "grad_norm": 10288.115182092393,
      "learning_rate": 3.153287838094678e-07,
      "loss": 1.7606,
      "step": 100672
    },
    {
      "epoch": 0.0003654748020831135,
      "grad_norm": 8291.557995937796,
      "learning_rate": 3.152786296211596e-07,
      "loss": 1.7628,
      "step": 100704
    },
    {
      "epoch": 0.0003655909364339502,
      "grad_norm": 9245.425355277062,
      "learning_rate": 3.152284993568586e-07,
      "loss": 1.77,
      "step": 100736
    },
    {
      "epoch": 0.0003657070707847869,
      "grad_norm": 8018.388117321336,
      "learning_rate": 3.151783929975507e-07,
      "loss": 1.7682,
      "step": 100768
    },
    {
      "epoch": 0.0003658232051356236,
      "grad_norm": 10235.833136584437,
      "learning_rate": 3.151283105242433e-07,
      "loss": 1.7513,
      "step": 100800
    },
    {
      "epoch": 0.0003659393394864603,
      "grad_norm": 9987.629148101165,
      "learning_rate": 3.1507825191796465e-07,
      "loss": 1.7276,
      "step": 100832
    },
    {
      "epoch": 0.000366055473837297,
      "grad_norm": 9312.541114003203,
      "learning_rate": 3.150282171597644e-07,
      "loss": 1.7389,
      "step": 100864
    },
    {
      "epoch": 0.0003661716081881337,
      "grad_norm": 8942.484106779279,
      "learning_rate": 3.1497820623071286e-07,
      "loss": 1.7371,
      "step": 100896
    },
    {
      "epoch": 0.0003662877425389704,
      "grad_norm": 9034.923242618057,
      "learning_rate": 3.149282191119017e-07,
      "loss": 1.7281,
      "step": 100928
    },
    {
      "epoch": 0.0003664038768898071,
      "grad_norm": 9748.094377877145,
      "learning_rate": 3.1487825578444335e-07,
      "loss": 1.7447,
      "step": 100960
    },
    {
      "epoch": 0.00036652001124064384,
      "grad_norm": 8546.498347276503,
      "learning_rate": 3.148283162294714e-07,
      "loss": 1.7502,
      "step": 100992
    },
    {
      "epoch": 0.00036663614559148054,
      "grad_norm": 9752.565816235234,
      "learning_rate": 3.1477840042814025e-07,
      "loss": 1.7611,
      "step": 101024
    },
    {
      "epoch": 0.00036675227994231724,
      "grad_norm": 9296.339494661326,
      "learning_rate": 3.147285083616253e-07,
      "loss": 1.7745,
      "step": 101056
    },
    {
      "epoch": 0.00036686841429315394,
      "grad_norm": 10690.8860250215,
      "learning_rate": 3.1468019803828005e-07,
      "loss": 1.7641,
      "step": 101088
    },
    {
      "epoch": 0.00036698454864399064,
      "grad_norm": 8755.321010676878,
      "learning_rate": 3.146303526447527e-07,
      "loss": 1.7508,
      "step": 101120
    },
    {
      "epoch": 0.00036710068299482734,
      "grad_norm": 10005.65620036987,
      "learning_rate": 3.1458053093027873e-07,
      "loss": 1.7628,
      "step": 101152
    },
    {
      "epoch": 0.00036721681734566404,
      "grad_norm": 8496.432898575731,
      "learning_rate": 3.1453073287611625e-07,
      "loss": 1.7815,
      "step": 101184
    },
    {
      "epoch": 0.00036733295169650074,
      "grad_norm": 9044.410760243036,
      "learning_rate": 3.1448095846354405e-07,
      "loss": 1.7799,
      "step": 101216
    },
    {
      "epoch": 0.00036744908604733744,
      "grad_norm": 8923.140590621668,
      "learning_rate": 3.1443120767386164e-07,
      "loss": 1.7635,
      "step": 101248
    },
    {
      "epoch": 0.0003675652203981742,
      "grad_norm": 10093.606788457731,
      "learning_rate": 3.143814804883892e-07,
      "loss": 1.7716,
      "step": 101280
    },
    {
      "epoch": 0.0003676813547490109,
      "grad_norm": 10503.396212654268,
      "learning_rate": 3.143317768884678e-07,
      "loss": 1.7685,
      "step": 101312
    },
    {
      "epoch": 0.0003677974890998476,
      "grad_norm": 9343.81271216413,
      "learning_rate": 3.142820968554588e-07,
      "loss": 1.785,
      "step": 101344
    },
    {
      "epoch": 0.0003679136234506843,
      "grad_norm": 10112.706264892697,
      "learning_rate": 3.142324403707446e-07,
      "loss": 1.797,
      "step": 101376
    },
    {
      "epoch": 0.000368029757801521,
      "grad_norm": 10291.335967696323,
      "learning_rate": 3.1418280741572784e-07,
      "loss": 1.7644,
      "step": 101408
    },
    {
      "epoch": 0.0003681458921523577,
      "grad_norm": 9297.903849793243,
      "learning_rate": 3.1413319797183174e-07,
      "loss": 1.7539,
      "step": 101440
    },
    {
      "epoch": 0.0003682620265031944,
      "grad_norm": 10005.924045284373,
      "learning_rate": 3.140836120205003e-07,
      "loss": 1.748,
      "step": 101472
    },
    {
      "epoch": 0.0003683781608540311,
      "grad_norm": 9567.719791047395,
      "learning_rate": 3.1403404954319777e-07,
      "loss": 1.7522,
      "step": 101504
    },
    {
      "epoch": 0.0003684942952048678,
      "grad_norm": 9603.856725295313,
      "learning_rate": 3.1398451052140893e-07,
      "loss": 1.7309,
      "step": 101536
    },
    {
      "epoch": 0.00036861042955570455,
      "grad_norm": 9064.1376865094,
      "learning_rate": 3.1393499493663904e-07,
      "loss": 1.7355,
      "step": 101568
    },
    {
      "epoch": 0.00036872656390654125,
      "grad_norm": 8845.818560201198,
      "learning_rate": 3.138855027704138e-07,
      "loss": 1.7362,
      "step": 101600
    },
    {
      "epoch": 0.00036884269825737795,
      "grad_norm": 9449.484007076788,
      "learning_rate": 3.1383603400427923e-07,
      "loss": 1.7639,
      "step": 101632
    },
    {
      "epoch": 0.00036895883260821465,
      "grad_norm": 8224.546552850194,
      "learning_rate": 3.137865886198017e-07,
      "loss": 1.7553,
      "step": 101664
    },
    {
      "epoch": 0.00036907496695905135,
      "grad_norm": 9508.613148088421,
      "learning_rate": 3.1373716659856795e-07,
      "loss": 1.7602,
      "step": 101696
    },
    {
      "epoch": 0.00036919110130988805,
      "grad_norm": 8255.729283352259,
      "learning_rate": 3.136877679221849e-07,
      "loss": 1.7576,
      "step": 101728
    },
    {
      "epoch": 0.00036930723566072475,
      "grad_norm": 9606.992037053014,
      "learning_rate": 3.136383925722799e-07,
      "loss": 1.7661,
      "step": 101760
    },
    {
      "epoch": 0.00036942337001156145,
      "grad_norm": 9287.282379684597,
      "learning_rate": 3.135890405305004e-07,
      "loss": 1.777,
      "step": 101792
    },
    {
      "epoch": 0.00036953950436239815,
      "grad_norm": 8690.407355239455,
      "learning_rate": 3.135397117785141e-07,
      "loss": 1.7341,
      "step": 101824
    },
    {
      "epoch": 0.0003696556387132349,
      "grad_norm": 9478.986443707998,
      "learning_rate": 3.1349040629800903e-07,
      "loss": 1.7292,
      "step": 101856
    },
    {
      "epoch": 0.0003697717730640716,
      "grad_norm": 8356.370503992746,
      "learning_rate": 3.1344112407069314e-07,
      "loss": 1.7474,
      "step": 101888
    },
    {
      "epoch": 0.0003698879074149083,
      "grad_norm": 10035.781783199553,
      "learning_rate": 3.1339186507829456e-07,
      "loss": 1.7384,
      "step": 101920
    },
    {
      "epoch": 0.000370004041765745,
      "grad_norm": 8790.801897438027,
      "learning_rate": 3.133426293025616e-07,
      "loss": 1.7786,
      "step": 101952
    },
    {
      "epoch": 0.0003701201761165817,
      "grad_norm": 9935.054906743093,
      "learning_rate": 3.1329341672526256e-07,
      "loss": 1.7933,
      "step": 101984
    },
    {
      "epoch": 0.0003702363104674184,
      "grad_norm": 10150.176254627306,
      "learning_rate": 3.132442273281859e-07,
      "loss": 1.777,
      "step": 102016
    },
    {
      "epoch": 0.0003703524448182551,
      "grad_norm": 8926.001120322582,
      "learning_rate": 3.131950610931398e-07,
      "loss": 1.7677,
      "step": 102048
    },
    {
      "epoch": 0.0003704685791690918,
      "grad_norm": 10803.025594711882,
      "learning_rate": 3.131474533734107e-07,
      "loss": 1.7655,
      "step": 102080
    },
    {
      "epoch": 0.0003705847135199285,
      "grad_norm": 9723.741152457731,
      "learning_rate": 3.1309833268552727e-07,
      "loss": 1.7709,
      "step": 102112
    },
    {
      "epoch": 0.00037070084787076525,
      "grad_norm": 9494.695361095057,
      "learning_rate": 3.130492351057858e-07,
      "loss": 1.7706,
      "step": 102144
    },
    {
      "epoch": 0.00037081698222160195,
      "grad_norm": 9524.188259374128,
      "learning_rate": 3.1300016061607365e-07,
      "loss": 1.7681,
      "step": 102176
    },
    {
      "epoch": 0.00037093311657243865,
      "grad_norm": 9378.645424580247,
      "learning_rate": 3.129511091982983e-07,
      "loss": 1.7837,
      "step": 102208
    },
    {
      "epoch": 0.00037104925092327535,
      "grad_norm": 10758.030302987623,
      "learning_rate": 3.1290208083438704e-07,
      "loss": 1.7628,
      "step": 102240
    },
    {
      "epoch": 0.00037116538527411205,
      "grad_norm": 9453.035808670144,
      "learning_rate": 3.128530755062867e-07,
      "loss": 1.7615,
      "step": 102272
    },
    {
      "epoch": 0.00037128151962494875,
      "grad_norm": 8385.829714464753,
      "learning_rate": 3.1280409319596427e-07,
      "loss": 1.7599,
      "step": 102304
    },
    {
      "epoch": 0.00037139765397578545,
      "grad_norm": 10350.117294021358,
      "learning_rate": 3.127551338854063e-07,
      "loss": 1.7462,
      "step": 102336
    },
    {
      "epoch": 0.00037151378832662215,
      "grad_norm": 9691.52557650239,
      "learning_rate": 3.127061975566191e-07,
      "loss": 1.7582,
      "step": 102368
    },
    {
      "epoch": 0.00037162992267745885,
      "grad_norm": 8938.69431181087,
      "learning_rate": 3.1265728419162854e-07,
      "loss": 1.7445,
      "step": 102400
    },
    {
      "epoch": 0.0003717460570282956,
      "grad_norm": 10307.458270592222,
      "learning_rate": 3.1260839377248053e-07,
      "loss": 1.7341,
      "step": 102432
    },
    {
      "epoch": 0.0003718621913791323,
      "grad_norm": 8638.035424794229,
      "learning_rate": 3.125595262812402e-07,
      "loss": 1.7464,
      "step": 102464
    },
    {
      "epoch": 0.000371978325729969,
      "grad_norm": 8305.631342649396,
      "learning_rate": 3.1251068169999265e-07,
      "loss": 1.7666,
      "step": 102496
    },
    {
      "epoch": 0.0003720944600808057,
      "grad_norm": 10064.891852374769,
      "learning_rate": 3.124618600108423e-07,
      "loss": 1.7489,
      "step": 102528
    },
    {
      "epoch": 0.0003722105944316424,
      "grad_norm": 8780.02300680357,
      "learning_rate": 3.124130611959133e-07,
      "loss": 1.7673,
      "step": 102560
    },
    {
      "epoch": 0.0003723267287824791,
      "grad_norm": 9646.651854400054,
      "learning_rate": 3.123642852373493e-07,
      "loss": 1.7435,
      "step": 102592
    },
    {
      "epoch": 0.0003724428631333158,
      "grad_norm": 10117.37979913772,
      "learning_rate": 3.1231553211731345e-07,
      "loss": 1.7413,
      "step": 102624
    },
    {
      "epoch": 0.0003725589974841525,
      "grad_norm": 9738.398020208457,
      "learning_rate": 3.122668018179883e-07,
      "loss": 1.7226,
      "step": 102656
    },
    {
      "epoch": 0.0003726751318349892,
      "grad_norm": 8736.279642960155,
      "learning_rate": 3.1221809432157605e-07,
      "loss": 1.7261,
      "step": 102688
    },
    {
      "epoch": 0.00037279126618582596,
      "grad_norm": 8299.177308625234,
      "learning_rate": 3.1216940961029814e-07,
      "loss": 1.7669,
      "step": 102720
    },
    {
      "epoch": 0.00037290740053666266,
      "grad_norm": 9842.339356067743,
      "learning_rate": 3.121207476663955e-07,
      "loss": 1.7638,
      "step": 102752
    },
    {
      "epoch": 0.00037302353488749936,
      "grad_norm": 8037.29942206958,
      "learning_rate": 3.120721084721284e-07,
      "loss": 1.7678,
      "step": 102784
    },
    {
      "epoch": 0.00037313966923833606,
      "grad_norm": 8667.194701862882,
      "learning_rate": 3.120234920097766e-07,
      "loss": 1.7695,
      "step": 102816
    },
    {
      "epoch": 0.00037325580358917276,
      "grad_norm": 7676.779923900385,
      "learning_rate": 3.119748982616388e-07,
      "loss": 1.7737,
      "step": 102848
    },
    {
      "epoch": 0.00037337193794000946,
      "grad_norm": 10961.057613205034,
      "learning_rate": 3.119263272100334e-07,
      "loss": 1.79,
      "step": 102880
    },
    {
      "epoch": 0.00037348807229084616,
      "grad_norm": 10087.939531936141,
      "learning_rate": 3.1187777883729784e-07,
      "loss": 1.8032,
      "step": 102912
    },
    {
      "epoch": 0.00037360420664168286,
      "grad_norm": 9397.136159490294,
      "learning_rate": 3.118292531257889e-07,
      "loss": 1.7849,
      "step": 102944
    },
    {
      "epoch": 0.00037372034099251956,
      "grad_norm": 10088.410974975197,
      "learning_rate": 3.117807500578825e-07,
      "loss": 1.7871,
      "step": 102976
    },
    {
      "epoch": 0.0003738364753433563,
      "grad_norm": 10162.250636547005,
      "learning_rate": 3.1173226961597377e-07,
      "loss": 1.7658,
      "step": 103008
    },
    {
      "epoch": 0.000373952609694193,
      "grad_norm": 11392.207512154962,
      "learning_rate": 3.1168381178247697e-07,
      "loss": 1.7444,
      "step": 103040
    },
    {
      "epoch": 0.0003740687440450297,
      "grad_norm": 9739.570832434047,
      "learning_rate": 3.1163688979938157e-07,
      "loss": 1.7574,
      "step": 103072
    },
    {
      "epoch": 0.0003741848783958664,
      "grad_norm": 9887.785899785655,
      "learning_rate": 3.1158847642487795e-07,
      "loss": 1.7334,
      "step": 103104
    },
    {
      "epoch": 0.0003743010127467031,
      "grad_norm": 10930.106312383243,
      "learning_rate": 3.1154008560669125e-07,
      "loss": 1.746,
      "step": 103136
    },
    {
      "epoch": 0.0003744171470975398,
      "grad_norm": 9251.974600051602,
      "learning_rate": 3.1149171732731166e-07,
      "loss": 1.7409,
      "step": 103168
    },
    {
      "epoch": 0.0003745332814483765,
      "grad_norm": 9315.367303547404,
      "learning_rate": 3.114433715692481e-07,
      "loss": 1.7449,
      "step": 103200
    },
    {
      "epoch": 0.0003746494157992132,
      "grad_norm": 9529.608176625103,
      "learning_rate": 3.1139504831502874e-07,
      "loss": 1.759,
      "step": 103232
    },
    {
      "epoch": 0.0003747655501500499,
      "grad_norm": 8883.079421011613,
      "learning_rate": 3.113467475472006e-07,
      "loss": 1.738,
      "step": 103264
    },
    {
      "epoch": 0.00037488168450088667,
      "grad_norm": 9915.67970438739,
      "learning_rate": 3.112984692483296e-07,
      "loss": 1.7479,
      "step": 103296
    },
    {
      "epoch": 0.00037499781885172337,
      "grad_norm": 11421.897215436671,
      "learning_rate": 3.112502134010007e-07,
      "loss": 1.7671,
      "step": 103328
    },
    {
      "epoch": 0.00037511395320256007,
      "grad_norm": 9459.084733736134,
      "learning_rate": 3.112019799878177e-07,
      "loss": 1.7401,
      "step": 103360
    },
    {
      "epoch": 0.00037523008755339677,
      "grad_norm": 9847.858447398601,
      "learning_rate": 3.1115376899140335e-07,
      "loss": 1.7328,
      "step": 103392
    },
    {
      "epoch": 0.00037534622190423347,
      "grad_norm": 9637.312177158112,
      "learning_rate": 3.111055803943989e-07,
      "loss": 1.7404,
      "step": 103424
    },
    {
      "epoch": 0.00037546235625507017,
      "grad_norm": 9140.10667333812,
      "learning_rate": 3.110574141794649e-07,
      "loss": 1.7461,
      "step": 103456
    },
    {
      "epoch": 0.00037557849060590687,
      "grad_norm": 9108.592426934032,
      "learning_rate": 3.110092703292804e-07,
      "loss": 1.7794,
      "step": 103488
    },
    {
      "epoch": 0.00037569462495674357,
      "grad_norm": 9100.772439743783,
      "learning_rate": 3.1096114882654307e-07,
      "loss": 1.7934,
      "step": 103520
    },
    {
      "epoch": 0.00037581075930758027,
      "grad_norm": 9971.044980341829,
      "learning_rate": 3.1091304965396975e-07,
      "loss": 1.7802,
      "step": 103552
    },
    {
      "epoch": 0.000375926893658417,
      "grad_norm": 10934.665518432652,
      "learning_rate": 3.1086497279429557e-07,
      "loss": 1.7636,
      "step": 103584
    },
    {
      "epoch": 0.0003760430280092537,
      "grad_norm": 9524.543453625482,
      "learning_rate": 3.1081691823027466e-07,
      "loss": 1.7573,
      "step": 103616
    },
    {
      "epoch": 0.0003761591623600904,
      "grad_norm": 9478.038193634799,
      "learning_rate": 3.1076888594467954e-07,
      "loss": 1.7746,
      "step": 103648
    },
    {
      "epoch": 0.0003762752967109271,
      "grad_norm": 9631.602255076774,
      "learning_rate": 3.1072087592030147e-07,
      "loss": 1.7659,
      "step": 103680
    },
    {
      "epoch": 0.0003763914310617638,
      "grad_norm": 10533.182140265115,
      "learning_rate": 3.1067288813995045e-07,
      "loss": 1.7781,
      "step": 103712
    },
    {
      "epoch": 0.0003765075654126005,
      "grad_norm": 9723.020621185578,
      "learning_rate": 3.106249225864547e-07,
      "loss": 1.771,
      "step": 103744
    },
    {
      "epoch": 0.0003766236997634372,
      "grad_norm": 8231.396114876261,
      "learning_rate": 3.1057697924266146e-07,
      "loss": 1.7568,
      "step": 103776
    },
    {
      "epoch": 0.0003767398341142739,
      "grad_norm": 10585.449163828618,
      "learning_rate": 3.1052905809143613e-07,
      "loss": 1.7553,
      "step": 103808
    },
    {
      "epoch": 0.0003768559684651106,
      "grad_norm": 10266.333230516142,
      "learning_rate": 3.104811591156628e-07,
      "loss": 1.7547,
      "step": 103840
    },
    {
      "epoch": 0.0003769721028159474,
      "grad_norm": 9559.591204648868,
      "learning_rate": 3.104332822982439e-07,
      "loss": 1.7371,
      "step": 103872
    },
    {
      "epoch": 0.0003770882371667841,
      "grad_norm": 9480.272147992377,
      "learning_rate": 3.1038542762210043e-07,
      "loss": 1.7577,
      "step": 103904
    },
    {
      "epoch": 0.0003772043715176208,
      "grad_norm": 10748.866917028976,
      "learning_rate": 3.103375950701718e-07,
      "loss": 1.7614,
      "step": 103936
    },
    {
      "epoch": 0.0003773205058684575,
      "grad_norm": 9237.340309851099,
      "learning_rate": 3.102897846254157e-07,
      "loss": 1.7292,
      "step": 103968
    },
    {
      "epoch": 0.0003774366402192942,
      "grad_norm": 9430.29310254989,
      "learning_rate": 3.102419962708084e-07,
      "loss": 1.7462,
      "step": 104000
    },
    {
      "epoch": 0.0003775527745701309,
      "grad_norm": 8669.276382720764,
      "learning_rate": 3.101942299893442e-07,
      "loss": 1.7561,
      "step": 104032
    },
    {
      "epoch": 0.0003776689089209676,
      "grad_norm": 10126.429281834739,
      "learning_rate": 3.101464857640361e-07,
      "loss": 1.7561,
      "step": 104064
    },
    {
      "epoch": 0.0003777850432718043,
      "grad_norm": 10341.619795757335,
      "learning_rate": 3.101002545627989e-07,
      "loss": 1.7728,
      "step": 104096
    },
    {
      "epoch": 0.000377901177622641,
      "grad_norm": 10387.804002771712,
      "learning_rate": 3.10052553710976e-07,
      "loss": 1.7608,
      "step": 104128
    },
    {
      "epoch": 0.00037801731197347773,
      "grad_norm": 9496.228409215944,
      "learning_rate": 3.1000487486498623e-07,
      "loss": 1.7573,
      "step": 104160
    },
    {
      "epoch": 0.00037813344632431443,
      "grad_norm": 9078.654525864502,
      "learning_rate": 3.0995721800791465e-07,
      "loss": 1.7358,
      "step": 104192
    },
    {
      "epoch": 0.00037824958067515113,
      "grad_norm": 9424.042550837723,
      "learning_rate": 3.0990958312286496e-07,
      "loss": 1.7609,
      "step": 104224
    },
    {
      "epoch": 0.00037836571502598783,
      "grad_norm": 9622.944040157357,
      "learning_rate": 3.0986197019295865e-07,
      "loss": 1.7554,
      "step": 104256
    },
    {
      "epoch": 0.00037848184937682453,
      "grad_norm": 8875.284446145937,
      "learning_rate": 3.098143792013357e-07,
      "loss": 1.7626,
      "step": 104288
    },
    {
      "epoch": 0.00037859798372766123,
      "grad_norm": 9239.5886272063,
      "learning_rate": 3.097668101311538e-07,
      "loss": 1.766,
      "step": 104320
    },
    {
      "epoch": 0.0003787141180784979,
      "grad_norm": 10026.496995461575,
      "learning_rate": 3.097192629655892e-07,
      "loss": 1.7527,
      "step": 104352
    },
    {
      "epoch": 0.0003788302524293346,
      "grad_norm": 9269.637533366664,
      "learning_rate": 3.096717376878358e-07,
      "loss": 1.7509,
      "step": 104384
    },
    {
      "epoch": 0.0003789463867801713,
      "grad_norm": 9578.375645170741,
      "learning_rate": 3.096242342811059e-07,
      "loss": 1.7649,
      "step": 104416
    },
    {
      "epoch": 0.0003790625211310081,
      "grad_norm": 9840.971090293884,
      "learning_rate": 3.0957675272862946e-07,
      "loss": 1.7788,
      "step": 104448
    },
    {
      "epoch": 0.0003791786554818448,
      "grad_norm": 8992.782328067326,
      "learning_rate": 3.095292930136548e-07,
      "loss": 1.7615,
      "step": 104480
    },
    {
      "epoch": 0.0003792947898326815,
      "grad_norm": 8325.557398757155,
      "learning_rate": 3.094818551194479e-07,
      "loss": 1.7602,
      "step": 104512
    },
    {
      "epoch": 0.0003794109241835182,
      "grad_norm": 10427.23721797869,
      "learning_rate": 3.09434439029293e-07,
      "loss": 1.7612,
      "step": 104544
    },
    {
      "epoch": 0.0003795270585343549,
      "grad_norm": 9107.600122974218,
      "learning_rate": 3.0938704472649186e-07,
      "loss": 1.7395,
      "step": 104576
    },
    {
      "epoch": 0.0003796431928851916,
      "grad_norm": 8507.18367028713,
      "learning_rate": 3.093396721943646e-07,
      "loss": 1.7445,
      "step": 104608
    },
    {
      "epoch": 0.0003797593272360283,
      "grad_norm": 8443.529119982946,
      "learning_rate": 3.0929232141624887e-07,
      "loss": 1.7547,
      "step": 104640
    },
    {
      "epoch": 0.000379875461586865,
      "grad_norm": 8586.57335611826,
      "learning_rate": 3.092449923755002e-07,
      "loss": 1.7665,
      "step": 104672
    },
    {
      "epoch": 0.0003799915959377017,
      "grad_norm": 8745.6460024403,
      "learning_rate": 3.091976850554922e-07,
      "loss": 1.7657,
      "step": 104704
    },
    {
      "epoch": 0.00038010773028853843,
      "grad_norm": 9887.10291238035,
      "learning_rate": 3.091503994396161e-07,
      "loss": 1.781,
      "step": 104736
    },
    {
      "epoch": 0.00038022386463937513,
      "grad_norm": 10123.092610462476,
      "learning_rate": 3.0910313551128065e-07,
      "loss": 1.7716,
      "step": 104768
    },
    {
      "epoch": 0.00038033999899021183,
      "grad_norm": 9890.955868873341,
      "learning_rate": 3.0905589325391286e-07,
      "loss": 1.7592,
      "step": 104800
    },
    {
      "epoch": 0.00038045613334104853,
      "grad_norm": 9284.69374831502,
      "learning_rate": 3.090086726509571e-07,
      "loss": 1.7514,
      "step": 104832
    },
    {
      "epoch": 0.00038057226769188523,
      "grad_norm": 12120.229700793629,
      "learning_rate": 3.0896147368587554e-07,
      "loss": 1.7437,
      "step": 104864
    },
    {
      "epoch": 0.00038068840204272193,
      "grad_norm": 10219.510849350863,
      "learning_rate": 3.089142963421481e-07,
      "loss": 1.7352,
      "step": 104896
    },
    {
      "epoch": 0.00038080453639355863,
      "grad_norm": 10385.975736540115,
      "learning_rate": 3.088671406032723e-07,
      "loss": 1.7323,
      "step": 104928
    },
    {
      "epoch": 0.00038092067074439533,
      "grad_norm": 9529.022510205335,
      "learning_rate": 3.088200064527632e-07,
      "loss": 1.7314,
      "step": 104960
    },
    {
      "epoch": 0.00038103680509523203,
      "grad_norm": 8173.158752893522,
      "learning_rate": 3.087728938741536e-07,
      "loss": 1.7456,
      "step": 104992
    },
    {
      "epoch": 0.00038115293944606873,
      "grad_norm": 10169.978662711148,
      "learning_rate": 3.087258028509939e-07,
      "loss": 1.7685,
      "step": 105024
    },
    {
      "epoch": 0.0003812690737969055,
      "grad_norm": 9831.148356117918,
      "learning_rate": 3.0867873336685176e-07,
      "loss": 1.784,
      "step": 105056
    },
    {
      "epoch": 0.0003813852081477422,
      "grad_norm": 9079.271887106366,
      "learning_rate": 3.0863315532849223e-07,
      "loss": 1.7651,
      "step": 105088
    },
    {
      "epoch": 0.0003815013424985789,
      "grad_norm": 8990.167851603217,
      "learning_rate": 3.085861282013383e-07,
      "loss": 1.7552,
      "step": 105120
    },
    {
      "epoch": 0.0003816174768494156,
      "grad_norm": 9327.798990115512,
      "learning_rate": 3.0853912256452245e-07,
      "loss": 1.7588,
      "step": 105152
    },
    {
      "epoch": 0.0003817336112002523,
      "grad_norm": 9842.341794512116,
      "learning_rate": 3.08492138401682e-07,
      "loss": 1.7622,
      "step": 105184
    },
    {
      "epoch": 0.000381849745551089,
      "grad_norm": 9215.328860111287,
      "learning_rate": 3.084451756964715e-07,
      "loss": 1.7631,
      "step": 105216
    },
    {
      "epoch": 0.0003819658799019257,
      "grad_norm": 9539.44673448099,
      "learning_rate": 3.0839823443256335e-07,
      "loss": 1.7646,
      "step": 105248
    },
    {
      "epoch": 0.0003820820142527624,
      "grad_norm": 8758.429996294997,
      "learning_rate": 3.08351314593647e-07,
      "loss": 1.7707,
      "step": 105280
    },
    {
      "epoch": 0.0003821981486035991,
      "grad_norm": 11252.483103742035,
      "learning_rate": 3.083044161634294e-07,
      "loss": 1.763,
      "step": 105312
    },
    {
      "epoch": 0.00038231428295443584,
      "grad_norm": 8015.991142709678,
      "learning_rate": 3.082575391256348e-07,
      "loss": 1.7638,
      "step": 105344
    },
    {
      "epoch": 0.00038243041730527254,
      "grad_norm": 9588.05047963349,
      "learning_rate": 3.082106834640047e-07,
      "loss": 1.7504,
      "step": 105376
    },
    {
      "epoch": 0.00038254655165610924,
      "grad_norm": 9699.910618144891,
      "learning_rate": 3.081638491622982e-07,
      "loss": 1.7458,
      "step": 105408
    },
    {
      "epoch": 0.00038266268600694594,
      "grad_norm": 10185.312366344,
      "learning_rate": 3.0811703620429114e-07,
      "loss": 1.7398,
      "step": 105440
    },
    {
      "epoch": 0.00038277882035778264,
      "grad_norm": 9390.398287612725,
      "learning_rate": 3.0807024457377717e-07,
      "loss": 1.756,
      "step": 105472
    },
    {
      "epoch": 0.00038289495470861934,
      "grad_norm": 9103.046523005361,
      "learning_rate": 3.080234742545668e-07,
      "loss": 1.7504,
      "step": 105504
    },
    {
      "epoch": 0.00038301108905945604,
      "grad_norm": 10199.933038995894,
      "learning_rate": 3.0797672523048774e-07,
      "loss": 1.7497,
      "step": 105536
    },
    {
      "epoch": 0.00038312722341029274,
      "grad_norm": 11408.577562518474,
      "learning_rate": 3.0792999748538516e-07,
      "loss": 1.7604,
      "step": 105568
    },
    {
      "epoch": 0.00038324335776112944,
      "grad_norm": 9723.689217575806,
      "learning_rate": 3.078832910031211e-07,
      "loss": 1.758,
      "step": 105600
    },
    {
      "epoch": 0.0003833594921119662,
      "grad_norm": 8809.236289259132,
      "learning_rate": 3.078366057675749e-07,
      "loss": 1.756,
      "step": 105632
    },
    {
      "epoch": 0.0003834756264628029,
      "grad_norm": 8911.981036784133,
      "learning_rate": 3.077899417626428e-07,
      "loss": 1.758,
      "step": 105664
    },
    {
      "epoch": 0.0003835917608136396,
      "grad_norm": 8878.929890476667,
      "learning_rate": 3.077432989722384e-07,
      "loss": 1.7357,
      "step": 105696
    },
    {
      "epoch": 0.0003837078951644763,
      "grad_norm": 9076.678467368996,
      "learning_rate": 3.0769667738029217e-07,
      "loss": 1.7335,
      "step": 105728
    },
    {
      "epoch": 0.000383824029515313,
      "grad_norm": 9102.648295963105,
      "learning_rate": 3.076500769707517e-07,
      "loss": 1.747,
      "step": 105760
    },
    {
      "epoch": 0.0003839401638661497,
      "grad_norm": 8750.37256349694,
      "learning_rate": 3.076034977275814e-07,
      "loss": 1.7696,
      "step": 105792
    },
    {
      "epoch": 0.0003840562982169864,
      "grad_norm": 10529.409575090145,
      "learning_rate": 3.0755693963476305e-07,
      "loss": 1.7601,
      "step": 105824
    },
    {
      "epoch": 0.0003841724325678231,
      "grad_norm": 9265.180192527288,
      "learning_rate": 3.0751040267629504e-07,
      "loss": 1.7699,
      "step": 105856
    },
    {
      "epoch": 0.0003842885669186598,
      "grad_norm": 9496.966568331174,
      "learning_rate": 3.074638868361929e-07,
      "loss": 1.7798,
      "step": 105888
    },
    {
      "epoch": 0.00038440470126949655,
      "grad_norm": 10449.738369930608,
      "learning_rate": 3.074173920984889e-07,
      "loss": 1.7897,
      "step": 105920
    },
    {
      "epoch": 0.00038452083562033325,
      "grad_norm": 9478.454409870841,
      "learning_rate": 3.0737091844723247e-07,
      "loss": 1.7709,
      "step": 105952
    },
    {
      "epoch": 0.00038463696997116995,
      "grad_norm": 10456.69718410168,
      "learning_rate": 3.0732446586648964e-07,
      "loss": 1.7686,
      "step": 105984
    },
    {
      "epoch": 0.00038475310432200665,
      "grad_norm": 9143.343152261103,
      "learning_rate": 3.0727803434034355e-07,
      "loss": 1.7508,
      "step": 106016
    },
    {
      "epoch": 0.00038486923867284335,
      "grad_norm": 9470.993189734643,
      "learning_rate": 3.0723162385289393e-07,
      "loss": 1.7534,
      "step": 106048
    },
    {
      "epoch": 0.00038498537302368005,
      "grad_norm": 19841.959983832243,
      "learning_rate": 3.071852343882575e-07,
      "loss": 1.7457,
      "step": 106080
    },
    {
      "epoch": 0.00038510150737451675,
      "grad_norm": 9402.918483109379,
      "learning_rate": 3.0714031462705167e-07,
      "loss": 1.7282,
      "step": 106112
    },
    {
      "epoch": 0.00038521764172535345,
      "grad_norm": 9284.975821185535,
      "learning_rate": 3.0709396650472657e-07,
      "loss": 1.7274,
      "step": 106144
    },
    {
      "epoch": 0.00038533377607619015,
      "grad_norm": 8615.610715439736,
      "learning_rate": 3.070476393581596e-07,
      "loss": 1.7425,
      "step": 106176
    },
    {
      "epoch": 0.0003854499104270269,
      "grad_norm": 10587.482136938885,
      "learning_rate": 3.0700133317153407e-07,
      "loss": 1.743,
      "step": 106208
    },
    {
      "epoch": 0.0003855660447778636,
      "grad_norm": 9090.237180624057,
      "learning_rate": 3.069550479290497e-07,
      "loss": 1.7489,
      "step": 106240
    },
    {
      "epoch": 0.0003856821791287003,
      "grad_norm": 9333.321166658736,
      "learning_rate": 3.0690878361492286e-07,
      "loss": 1.7718,
      "step": 106272
    },
    {
      "epoch": 0.000385798313479537,
      "grad_norm": 8852.12754087965,
      "learning_rate": 3.068625402133868e-07,
      "loss": 1.7602,
      "step": 106304
    },
    {
      "epoch": 0.0003859144478303737,
      "grad_norm": 8854.349100865631,
      "learning_rate": 3.0681631770869126e-07,
      "loss": 1.7569,
      "step": 106336
    },
    {
      "epoch": 0.0003860305821812104,
      "grad_norm": 8206.23104719822,
      "learning_rate": 3.0677011608510255e-07,
      "loss": 1.758,
      "step": 106368
    },
    {
      "epoch": 0.0003861467165320471,
      "grad_norm": 11384.881027046353,
      "learning_rate": 3.067239353269036e-07,
      "loss": 1.7559,
      "step": 106400
    },
    {
      "epoch": 0.0003862628508828838,
      "grad_norm": 8999.833998469083,
      "learning_rate": 3.0667777541839396e-07,
      "loss": 1.7548,
      "step": 106432
    },
    {
      "epoch": 0.0003863789852337205,
      "grad_norm": 9728.603496905402,
      "learning_rate": 3.066316363438896e-07,
      "loss": 1.7572,
      "step": 106464
    },
    {
      "epoch": 0.00038649511958455725,
      "grad_norm": 8665.703549048974,
      "learning_rate": 3.0658551808772317e-07,
      "loss": 1.7802,
      "step": 106496
    },
    {
      "epoch": 0.00038661125393539395,
      "grad_norm": 8521.0073348167,
      "learning_rate": 3.0653942063424345e-07,
      "loss": 1.761,
      "step": 106528
    },
    {
      "epoch": 0.00038672738828623065,
      "grad_norm": 9271.976272618476,
      "learning_rate": 3.064933439678162e-07,
      "loss": 1.7679,
      "step": 106560
    },
    {
      "epoch": 0.00038684352263706735,
      "grad_norm": 10216.968630665358,
      "learning_rate": 3.064472880728232e-07,
      "loss": 1.7733,
      "step": 106592
    },
    {
      "epoch": 0.00038695965698790405,
      "grad_norm": 9644.132827787058,
      "learning_rate": 3.0640125293366285e-07,
      "loss": 1.7439,
      "step": 106624
    },
    {
      "epoch": 0.00038707579133874075,
      "grad_norm": 9473.677955261093,
      "learning_rate": 3.063552385347499e-07,
      "loss": 1.7652,
      "step": 106656
    },
    {
      "epoch": 0.00038719192568957745,
      "grad_norm": 9792.235495534203,
      "learning_rate": 3.0630924486051555e-07,
      "loss": 1.7642,
      "step": 106688
    },
    {
      "epoch": 0.00038730806004041415,
      "grad_norm": 9389.01602938242,
      "learning_rate": 3.0626327189540726e-07,
      "loss": 1.758,
      "step": 106720
    },
    {
      "epoch": 0.00038742419439125085,
      "grad_norm": 9432.360256054684,
      "learning_rate": 3.0621731962388885e-07,
      "loss": 1.7588,
      "step": 106752
    },
    {
      "epoch": 0.0003875403287420876,
      "grad_norm": 9003.630823173504,
      "learning_rate": 3.0617138803044046e-07,
      "loss": 1.7697,
      "step": 106784
    },
    {
      "epoch": 0.0003876564630929243,
      "grad_norm": 8732.862531839144,
      "learning_rate": 3.061254770995586e-07,
      "loss": 1.7439,
      "step": 106816
    },
    {
      "epoch": 0.000387772597443761,
      "grad_norm": 9077.134129228234,
      "learning_rate": 3.0607958681575597e-07,
      "loss": 1.7396,
      "step": 106848
    },
    {
      "epoch": 0.0003878887317945977,
      "grad_norm": 7833.9989788102475,
      "learning_rate": 3.060337171635614e-07,
      "loss": 1.7447,
      "step": 106880
    },
    {
      "epoch": 0.0003880048661454344,
      "grad_norm": 9628.241376284665,
      "learning_rate": 3.0598786812752017e-07,
      "loss": 1.7424,
      "step": 106912
    },
    {
      "epoch": 0.0003881210004962711,
      "grad_norm": 10176.859830026155,
      "learning_rate": 3.059420396921936e-07,
      "loss": 1.7386,
      "step": 106944
    },
    {
      "epoch": 0.0003882371348471078,
      "grad_norm": 8219.27575884883,
      "learning_rate": 3.058962318421594e-07,
      "loss": 1.7404,
      "step": 106976
    },
    {
      "epoch": 0.0003883532691979445,
      "grad_norm": 9757.556456408542,
      "learning_rate": 3.058504445620111e-07,
      "loss": 1.7477,
      "step": 107008
    },
    {
      "epoch": 0.0003884694035487812,
      "grad_norm": 10168.32159208195,
      "learning_rate": 3.058046778363586e-07,
      "loss": 1.7682,
      "step": 107040
    },
    {
      "epoch": 0.00038858553789961796,
      "grad_norm": 9724.771976761202,
      "learning_rate": 3.05758931649828e-07,
      "loss": 1.7732,
      "step": 107072
    },
    {
      "epoch": 0.00038870167225045466,
      "grad_norm": 9557.367419954095,
      "learning_rate": 3.057146346035127e-07,
      "loss": 1.7721,
      "step": 107104
    },
    {
      "epoch": 0.00038881780660129136,
      "grad_norm": 8460.791806917365,
      "learning_rate": 3.056689288085119e-07,
      "loss": 1.7447,
      "step": 107136
    },
    {
      "epoch": 0.00038893394095212806,
      "grad_norm": 10286.626852374884,
      "learning_rate": 3.056232435070859e-07,
      "loss": 1.7528,
      "step": 107168
    },
    {
      "epoch": 0.00038905007530296476,
      "grad_norm": 9580.930017487864,
      "learning_rate": 3.055775786839246e-07,
      "loss": 1.757,
      "step": 107200
    },
    {
      "epoch": 0.00038916620965380146,
      "grad_norm": 9936.207123445041,
      "learning_rate": 3.055319343237336e-07,
      "loss": 1.7383,
      "step": 107232
    },
    {
      "epoch": 0.00038928234400463816,
      "grad_norm": 8674.472779368209,
      "learning_rate": 3.054863104112347e-07,
      "loss": 1.7647,
      "step": 107264
    },
    {
      "epoch": 0.00038939847835547486,
      "grad_norm": 8904.03144648535,
      "learning_rate": 3.0544070693116553e-07,
      "loss": 1.765,
      "step": 107296
    },
    {
      "epoch": 0.00038951461270631156,
      "grad_norm": 10619.925423466966,
      "learning_rate": 3.053951238682797e-07,
      "loss": 1.7754,
      "step": 107328
    },
    {
      "epoch": 0.0003896307470571483,
      "grad_norm": 8938.390235383551,
      "learning_rate": 3.0534956120734685e-07,
      "loss": 1.7778,
      "step": 107360
    },
    {
      "epoch": 0.000389746881407985,
      "grad_norm": 8768.141080069368,
      "learning_rate": 3.053040189331524e-07,
      "loss": 1.7536,
      "step": 107392
    },
    {
      "epoch": 0.0003898630157588217,
      "grad_norm": 9766.663606370397,
      "learning_rate": 3.0525849703049755e-07,
      "loss": 1.758,
      "step": 107424
    },
    {
      "epoch": 0.0003899791501096584,
      "grad_norm": 9165.145607135764,
      "learning_rate": 3.052129954841996e-07,
      "loss": 1.76,
      "step": 107456
    },
    {
      "epoch": 0.0003900952844604951,
      "grad_norm": 9951.813603559905,
      "learning_rate": 3.0516751427909165e-07,
      "loss": 1.7617,
      "step": 107488
    },
    {
      "epoch": 0.0003902114188113318,
      "grad_norm": 9863.511950618806,
      "learning_rate": 3.051220534000223e-07,
      "loss": 1.7745,
      "step": 107520
    },
    {
      "epoch": 0.0003903275531621685,
      "grad_norm": 11840.031756714168,
      "learning_rate": 3.050766128318564e-07,
      "loss": 1.7486,
      "step": 107552
    },
    {
      "epoch": 0.0003904436875130052,
      "grad_norm": 9303.418941442978,
      "learning_rate": 3.050311925594743e-07,
      "loss": 1.7484,
      "step": 107584
    },
    {
      "epoch": 0.0003905598218638419,
      "grad_norm": 8650.303000473452,
      "learning_rate": 3.0498579256777213e-07,
      "loss": 1.7604,
      "step": 107616
    },
    {
      "epoch": 0.00039067595621467867,
      "grad_norm": 10253.949483004097,
      "learning_rate": 3.0494041284166185e-07,
      "loss": 1.7605,
      "step": 107648
    },
    {
      "epoch": 0.00039079209056551537,
      "grad_norm": 8724.315560546856,
      "learning_rate": 3.0489505336607095e-07,
      "loss": 1.7474,
      "step": 107680
    },
    {
      "epoch": 0.00039090822491635207,
      "grad_norm": 8728.399395078115,
      "learning_rate": 3.048497141259428e-07,
      "loss": 1.7426,
      "step": 107712
    },
    {
      "epoch": 0.00039102435926718877,
      "grad_norm": 8981.081894738518,
      "learning_rate": 3.0480439510623634e-07,
      "loss": 1.7235,
      "step": 107744
    },
    {
      "epoch": 0.00039114049361802547,
      "grad_norm": 8540.828765406786,
      "learning_rate": 3.047590962919262e-07,
      "loss": 1.7455,
      "step": 107776
    },
    {
      "epoch": 0.00039125662796886217,
      "grad_norm": 9284.452164775259,
      "learning_rate": 3.047138176680026e-07,
      "loss": 1.7581,
      "step": 107808
    },
    {
      "epoch": 0.00039137276231969887,
      "grad_norm": 9267.370608754136,
      "learning_rate": 3.0466855921947137e-07,
      "loss": 1.7544,
      "step": 107840
    },
    {
      "epoch": 0.00039148889667053557,
      "grad_norm": 9270.605373976394,
      "learning_rate": 3.046233209313539e-07,
      "loss": 1.7602,
      "step": 107872
    },
    {
      "epoch": 0.00039160503102137227,
      "grad_norm": 10473.03575855635,
      "learning_rate": 3.045781027886873e-07,
      "loss": 1.7356,
      "step": 107904
    },
    {
      "epoch": 0.000391721165372209,
      "grad_norm": 9845.506487733375,
      "learning_rate": 3.0453290477652395e-07,
      "loss": 1.7362,
      "step": 107936
    },
    {
      "epoch": 0.0003918372997230457,
      "grad_norm": 7511.244237807741,
      "learning_rate": 3.0448772687993203e-07,
      "loss": 1.7396,
      "step": 107968
    },
    {
      "epoch": 0.0003919534340738824,
      "grad_norm": 10872.121044212117,
      "learning_rate": 3.04442569083995e-07,
      "loss": 1.745,
      "step": 108000
    },
    {
      "epoch": 0.0003920695684247191,
      "grad_norm": 9189.776167023874,
      "learning_rate": 3.0439743137381194e-07,
      "loss": 1.761,
      "step": 108032
    },
    {
      "epoch": 0.0003921857027755558,
      "grad_norm": 9338.191580814779,
      "learning_rate": 3.0435231373449734e-07,
      "loss": 1.7689,
      "step": 108064
    },
    {
      "epoch": 0.0003923018371263925,
      "grad_norm": 17060.49870314464,
      "learning_rate": 3.0430721615118116e-07,
      "loss": 1.7816,
      "step": 108096
    },
    {
      "epoch": 0.0003924179714772292,
      "grad_norm": 10343.285068100946,
      "learning_rate": 3.0426354697899184e-07,
      "loss": 1.7805,
      "step": 108128
    },
    {
      "epoch": 0.0003925341058280659,
      "grad_norm": 9776.52637699096,
      "learning_rate": 3.0421848883752635e-07,
      "loss": 1.7656,
      "step": 108160
    },
    {
      "epoch": 0.0003926502401789026,
      "grad_norm": 9319.524880593432,
      "learning_rate": 3.0417345070800453e-07,
      "loss": 1.7811,
      "step": 108192
    },
    {
      "epoch": 0.0003927663745297394,
      "grad_norm": 10223.87675982061,
      "learning_rate": 3.041284325756173e-07,
      "loss": 1.8054,
      "step": 108224
    },
    {
      "epoch": 0.0003928825088805761,
      "grad_norm": 7937.686186792723,
      "learning_rate": 3.0408343442557113e-07,
      "loss": 1.7713,
      "step": 108256
    },
    {
      "epoch": 0.0003929986432314128,
      "grad_norm": 9040.32632154393,
      "learning_rate": 3.040384562430877e-07,
      "loss": 1.7611,
      "step": 108288
    },
    {
      "epoch": 0.0003931147775822495,
      "grad_norm": 9928.473095093726,
      "learning_rate": 3.039934980134038e-07,
      "loss": 1.756,
      "step": 108320
    },
    {
      "epoch": 0.0003932309119330862,
      "grad_norm": 8771.845529875683,
      "learning_rate": 3.039485597217718e-07,
      "loss": 1.7262,
      "step": 108352
    },
    {
      "epoch": 0.0003933470462839229,
      "grad_norm": 9679.820142957202,
      "learning_rate": 3.039036413534592e-07,
      "loss": 1.7408,
      "step": 108384
    },
    {
      "epoch": 0.0003934631806347596,
      "grad_norm": 10261.566352170608,
      "learning_rate": 3.038587428937487e-07,
      "loss": 1.7462,
      "step": 108416
    },
    {
      "epoch": 0.0003935793149855963,
      "grad_norm": 9996.712659669676,
      "learning_rate": 3.038138643279382e-07,
      "loss": 1.752,
      "step": 108448
    },
    {
      "epoch": 0.000393695449336433,
      "grad_norm": 8867.935780101252,
      "learning_rate": 3.037690056413409e-07,
      "loss": 1.7435,
      "step": 108480
    },
    {
      "epoch": 0.00039381158368726973,
      "grad_norm": 9436.019499767897,
      "learning_rate": 3.037241668192851e-07,
      "loss": 1.7413,
      "step": 108512
    },
    {
      "epoch": 0.00039392771803810643,
      "grad_norm": 10285.63678145403,
      "learning_rate": 3.0367934784711425e-07,
      "loss": 1.7378,
      "step": 108544
    },
    {
      "epoch": 0.00039404385238894313,
      "grad_norm": 8884.266767719215,
      "learning_rate": 3.03634548710187e-07,
      "loss": 1.7423,
      "step": 108576
    },
    {
      "epoch": 0.00039415998673977983,
      "grad_norm": 11184.571694973394,
      "learning_rate": 3.035897693938771e-07,
      "loss": 1.7497,
      "step": 108608
    },
    {
      "epoch": 0.00039427612109061653,
      "grad_norm": 9467.148250661336,
      "learning_rate": 3.035450098835733e-07,
      "loss": 1.7404,
      "step": 108640
    },
    {
      "epoch": 0.00039439225544145323,
      "grad_norm": 11071.96920154676,
      "learning_rate": 3.035002701646795e-07,
      "loss": 1.7269,
      "step": 108672
    },
    {
      "epoch": 0.00039450838979228993,
      "grad_norm": 9984.683570349138,
      "learning_rate": 3.0345555022261465e-07,
      "loss": 1.7463,
      "step": 108704
    },
    {
      "epoch": 0.00039462452414312663,
      "grad_norm": 9838.643504060912,
      "learning_rate": 3.0341085004281275e-07,
      "loss": 1.7561,
      "step": 108736
    },
    {
      "epoch": 0.00039474065849396333,
      "grad_norm": 9998.11982324677,
      "learning_rate": 3.0336616961072285e-07,
      "loss": 1.7533,
      "step": 108768
    },
    {
      "epoch": 0.0003948567928448001,
      "grad_norm": 9987.423091068087,
      "learning_rate": 3.033215089118088e-07,
      "loss": 1.7779,
      "step": 108800
    },
    {
      "epoch": 0.0003949729271956368,
      "grad_norm": 9630.199686403184,
      "learning_rate": 3.0327686793154965e-07,
      "loss": 1.7922,
      "step": 108832
    },
    {
      "epoch": 0.0003950890615464735,
      "grad_norm": 9476.437938381701,
      "learning_rate": 3.032322466554394e-07,
      "loss": 1.7787,
      "step": 108864
    },
    {
      "epoch": 0.0003952051958973102,
      "grad_norm": 9249.769294420266,
      "learning_rate": 3.0318764506898674e-07,
      "loss": 1.7631,
      "step": 108896
    },
    {
      "epoch": 0.0003953213302481469,
      "grad_norm": 9217.861682624663,
      "learning_rate": 3.0314306315771557e-07,
      "loss": 1.7644,
      "step": 108928
    },
    {
      "epoch": 0.0003954374645989836,
      "grad_norm": 10369.212506261023,
      "learning_rate": 3.030985009071645e-07,
      "loss": 1.7639,
      "step": 108960
    },
    {
      "epoch": 0.0003955535989498203,
      "grad_norm": 9240.90309439505,
      "learning_rate": 3.0305395830288714e-07,
      "loss": 1.7721,
      "step": 108992
    },
    {
      "epoch": 0.000395669733300657,
      "grad_norm": 10911.49586445415,
      "learning_rate": 3.030094353304518e-07,
      "loss": 1.7941,
      "step": 109024
    },
    {
      "epoch": 0.0003957858676514937,
      "grad_norm": 9077.424304283677,
      "learning_rate": 3.029649319754418e-07,
      "loss": 1.7767,
      "step": 109056
    },
    {
      "epoch": 0.00039590200200233044,
      "grad_norm": 9273.08643332952,
      "learning_rate": 3.0292044822345523e-07,
      "loss": 1.7586,
      "step": 109088
    },
    {
      "epoch": 0.00039601813635316714,
      "grad_norm": 9142.175889797789,
      "learning_rate": 3.028773732688446e-07,
      "loss": 1.7223,
      "step": 109120
    },
    {
      "epoch": 0.00039613427070400384,
      "grad_norm": 8796.304792354571,
      "learning_rate": 3.0283292806827975e-07,
      "loss": 1.7404,
      "step": 109152
    },
    {
      "epoch": 0.00039625040505484054,
      "grad_norm": 10597.30088277199,
      "learning_rate": 3.0278850242806953e-07,
      "loss": 1.727,
      "step": 109184
    },
    {
      "epoch": 0.00039636653940567724,
      "grad_norm": 9772.623700931086,
      "learning_rate": 3.0274409633387075e-07,
      "loss": 1.7339,
      "step": 109216
    },
    {
      "epoch": 0.00039648267375651394,
      "grad_norm": 11069.157510849685,
      "learning_rate": 3.026997097713547e-07,
      "loss": 1.7313,
      "step": 109248
    },
    {
      "epoch": 0.00039659880810735064,
      "grad_norm": 7883.123746333048,
      "learning_rate": 3.0265534272620754e-07,
      "loss": 1.731,
      "step": 109280
    },
    {
      "epoch": 0.00039671494245818733,
      "grad_norm": 9295.278801628276,
      "learning_rate": 3.0261099518413e-07,
      "loss": 1.7582,
      "step": 109312
    },
    {
      "epoch": 0.00039683107680902403,
      "grad_norm": 10323.621845069685,
      "learning_rate": 3.0256666713083755e-07,
      "loss": 1.7842,
      "step": 109344
    },
    {
      "epoch": 0.0003969472111598608,
      "grad_norm": 9056.570211730266,
      "learning_rate": 3.025223585520603e-07,
      "loss": 1.7639,
      "step": 109376
    },
    {
      "epoch": 0.0003970633455106975,
      "grad_norm": 8889.423378375,
      "learning_rate": 3.0247806943354283e-07,
      "loss": 1.7578,
      "step": 109408
    },
    {
      "epoch": 0.0003971794798615342,
      "grad_norm": 9424.342523486717,
      "learning_rate": 3.024337997610446e-07,
      "loss": 1.7475,
      "step": 109440
    },
    {
      "epoch": 0.0003972956142123709,
      "grad_norm": 9801.672102248676,
      "learning_rate": 3.023895495203394e-07,
      "loss": 1.7337,
      "step": 109472
    },
    {
      "epoch": 0.0003974117485632076,
      "grad_norm": 9924.688811242397,
      "learning_rate": 3.023453186972157e-07,
      "loss": 1.7238,
      "step": 109504
    },
    {
      "epoch": 0.0003975278829140443,
      "grad_norm": 10897.374913253192,
      "learning_rate": 3.0230110727747655e-07,
      "loss": 1.7399,
      "step": 109536
    },
    {
      "epoch": 0.000397644017264881,
      "grad_norm": 10438.994587602774,
      "learning_rate": 3.0225691524693947e-07,
      "loss": 1.7495,
      "step": 109568
    },
    {
      "epoch": 0.0003977601516157177,
      "grad_norm": 10839.301268993311,
      "learning_rate": 3.022127425914365e-07,
      "loss": 1.7756,
      "step": 109600
    },
    {
      "epoch": 0.0003978762859665544,
      "grad_norm": 9690.367691682291,
      "learning_rate": 3.0216858929681424e-07,
      "loss": 1.7912,
      "step": 109632
    },
    {
      "epoch": 0.00039799242031739114,
      "grad_norm": 9434.83534567509,
      "learning_rate": 3.0212445534893355e-07,
      "loss": 1.7733,
      "step": 109664
    },
    {
      "epoch": 0.00039810855466822784,
      "grad_norm": 8780.389057439312,
      "learning_rate": 3.0208034073367e-07,
      "loss": 1.7636,
      "step": 109696
    },
    {
      "epoch": 0.00039822468901906454,
      "grad_norm": 9077.404254521221,
      "learning_rate": 3.020362454369134e-07,
      "loss": 1.7587,
      "step": 109728
    },
    {
      "epoch": 0.00039834082336990124,
      "grad_norm": 10626.218894790376,
      "learning_rate": 3.0199216944456815e-07,
      "loss": 1.766,
      "step": 109760
    },
    {
      "epoch": 0.00039845695772073794,
      "grad_norm": 9275.38160940023,
      "learning_rate": 3.0194811274255287e-07,
      "loss": 1.7697,
      "step": 109792
    },
    {
      "epoch": 0.00039857309207157464,
      "grad_norm": 9058.97941271532,
      "learning_rate": 3.0190407531680063e-07,
      "loss": 1.7566,
      "step": 109824
    },
    {
      "epoch": 0.00039868922642241134,
      "grad_norm": 8935.567357476524,
      "learning_rate": 3.0186005715325894e-07,
      "loss": 1.755,
      "step": 109856
    },
    {
      "epoch": 0.00039880536077324804,
      "grad_norm": 8656.909841277084,
      "learning_rate": 3.0181605823788945e-07,
      "loss": 1.743,
      "step": 109888
    },
    {
      "epoch": 0.00039892149512408474,
      "grad_norm": 9755.055714858834,
      "learning_rate": 3.017720785566683e-07,
      "loss": 1.7664,
      "step": 109920
    },
    {
      "epoch": 0.0003990376294749215,
      "grad_norm": 10163.980322688549,
      "learning_rate": 3.017281180955859e-07,
      "loss": 1.767,
      "step": 109952
    },
    {
      "epoch": 0.0003991537638257582,
      "grad_norm": 10030.11924156438,
      "learning_rate": 3.016841768406469e-07,
      "loss": 1.7602,
      "step": 109984
    },
    {
      "epoch": 0.0003992698981765949,
      "grad_norm": 8628.744056929721,
      "learning_rate": 3.0164025477787023e-07,
      "loss": 1.7498,
      "step": 110016
    },
    {
      "epoch": 0.0003993860325274316,
      "grad_norm": 8878.832580919632,
      "learning_rate": 3.0159635189328904e-07,
      "loss": 1.7435,
      "step": 110048
    },
    {
      "epoch": 0.0003995021668782683,
      "grad_norm": 8793.274702862409,
      "learning_rate": 3.0155246817295084e-07,
      "loss": 1.7396,
      "step": 110080
    },
    {
      "epoch": 0.000399618301229105,
      "grad_norm": 8117.675159797909,
      "learning_rate": 3.015099740809962e-07,
      "loss": 1.7456,
      "step": 110112
    },
    {
      "epoch": 0.0003997344355799417,
      "grad_norm": 11123.096691119788,
      "learning_rate": 3.014661280495415e-07,
      "loss": 1.7522,
      "step": 110144
    },
    {
      "epoch": 0.0003998505699307784,
      "grad_norm": 10340.85064199266,
      "learning_rate": 3.014223011409916e-07,
      "loss": 1.7352,
      "step": 110176
    },
    {
      "epoch": 0.0003999667042816151,
      "grad_norm": 9666.043451174839,
      "learning_rate": 3.0137849334145005e-07,
      "loss": 1.7569,
      "step": 110208
    },
    {
      "epoch": 0.00040008283863245185,
      "grad_norm": 8927.264306605915,
      "learning_rate": 3.0133470463703477e-07,
      "loss": 1.7501,
      "step": 110240
    },
    {
      "epoch": 0.00040019897298328855,
      "grad_norm": 9270.389635824376,
      "learning_rate": 3.012909350138777e-07,
      "loss": 1.7388,
      "step": 110272
    },
    {
      "epoch": 0.00040031510733412525,
      "grad_norm": 10414.134625594197,
      "learning_rate": 3.0124718445812473e-07,
      "loss": 1.7405,
      "step": 110304
    },
    {
      "epoch": 0.00040043124168496195,
      "grad_norm": 10149.550926026235,
      "learning_rate": 3.0120345295593613e-07,
      "loss": 1.7551,
      "step": 110336
    },
    {
      "epoch": 0.00040054737603579865,
      "grad_norm": 9148.08646657868,
      "learning_rate": 3.011597404934859e-07,
      "loss": 1.7652,
      "step": 110368
    },
    {
      "epoch": 0.00040066351038663535,
      "grad_norm": 9817.572612412907,
      "learning_rate": 3.011160470569624e-07,
      "loss": 1.7512,
      "step": 110400
    },
    {
      "epoch": 0.00040077964473747205,
      "grad_norm": 9783.436001732725,
      "learning_rate": 3.0107237263256766e-07,
      "loss": 1.7512,
      "step": 110432
    },
    {
      "epoch": 0.00040089577908830875,
      "grad_norm": 8407.882313638791,
      "learning_rate": 3.0102871720651806e-07,
      "loss": 1.761,
      "step": 110464
    },
    {
      "epoch": 0.00040101191343914545,
      "grad_norm": 8281.765874498024,
      "learning_rate": 3.009850807650438e-07,
      "loss": 1.7688,
      "step": 110496
    },
    {
      "epoch": 0.00040112804778998215,
      "grad_norm": 8648.519526485443,
      "learning_rate": 3.00941463294389e-07,
      "loss": 1.778,
      "step": 110528
    },
    {
      "epoch": 0.0004012441821408189,
      "grad_norm": 9037.213951213062,
      "learning_rate": 3.008978647808118e-07,
      "loss": 1.7956,
      "step": 110560
    },
    {
      "epoch": 0.0004013603164916556,
      "grad_norm": 8580.969176031342,
      "learning_rate": 3.0085428521058433e-07,
      "loss": 1.7871,
      "step": 110592
    },
    {
      "epoch": 0.0004014764508424923,
      "grad_norm": 8758.087348274164,
      "learning_rate": 3.008107245699925e-07,
      "loss": 1.7566,
      "step": 110624
    },
    {
      "epoch": 0.000401592585193329,
      "grad_norm": 9335.846185536691,
      "learning_rate": 3.007671828453362e-07,
      "loss": 1.739,
      "step": 110656
    },
    {
      "epoch": 0.0004017087195441657,
      "grad_norm": 8586.581275455324,
      "learning_rate": 3.007236600229292e-07,
      "loss": 1.7382,
      "step": 110688
    },
    {
      "epoch": 0.0004018248538950024,
      "grad_norm": 9659.25928837196,
      "learning_rate": 3.0068015608909913e-07,
      "loss": 1.741,
      "step": 110720
    },
    {
      "epoch": 0.0004019409882458391,
      "grad_norm": 10141.216790898417,
      "learning_rate": 3.0063667103018737e-07,
      "loss": 1.746,
      "step": 110752
    },
    {
      "epoch": 0.0004020571225966758,
      "grad_norm": 8878.95770910077,
      "learning_rate": 3.005932048325492e-07,
      "loss": 1.7619,
      "step": 110784
    },
    {
      "epoch": 0.0004021732569475125,
      "grad_norm": 10851.98599335624,
      "learning_rate": 3.0054975748255387e-07,
      "loss": 1.7524,
      "step": 110816
    },
    {
      "epoch": 0.00040228939129834926,
      "grad_norm": 9083.018441024988,
      "learning_rate": 3.00506328966584e-07,
      "loss": 1.7578,
      "step": 110848
    },
    {
      "epoch": 0.00040240552564918596,
      "grad_norm": 11813.377840397725,
      "learning_rate": 3.004629192710364e-07,
      "loss": 1.7631,
      "step": 110880
    },
    {
      "epoch": 0.00040252166000002266,
      "grad_norm": 9926.84501742623,
      "learning_rate": 3.0041952838232136e-07,
      "loss": 1.7415,
      "step": 110912
    },
    {
      "epoch": 0.00040263779435085936,
      "grad_norm": 9740.993481159916,
      "learning_rate": 3.0037615628686303e-07,
      "loss": 1.72,
      "step": 110944
    },
    {
      "epoch": 0.00040275392870169606,
      "grad_norm": 9238.884239993486,
      "learning_rate": 3.0033280297109934e-07,
      "loss": 1.7425,
      "step": 110976
    },
    {
      "epoch": 0.00040287006305253276,
      "grad_norm": 9864.950886851897,
      "learning_rate": 3.0028946842148166e-07,
      "loss": 1.7249,
      "step": 111008
    },
    {
      "epoch": 0.00040298619740336946,
      "grad_norm": 9447.14697673324,
      "learning_rate": 3.002461526244754e-07,
      "loss": 1.7373,
      "step": 111040
    },
    {
      "epoch": 0.00040310233175420616,
      "grad_norm": 9073.46835559589,
      "learning_rate": 3.002028555665592e-07,
      "loss": 1.7757,
      "step": 111072
    },
    {
      "epoch": 0.00040321846610504286,
      "grad_norm": 8872.049368663364,
      "learning_rate": 3.001595772342258e-07,
      "loss": 1.7614,
      "step": 111104
    },
    {
      "epoch": 0.0004033346004558796,
      "grad_norm": 9574.969660526345,
      "learning_rate": 3.001176691940082e-07,
      "loss": 1.7683,
      "step": 111136
    },
    {
      "epoch": 0.0004034507348067163,
      "grad_norm": 10083.11251548846,
      "learning_rate": 3.0007442768824474e-07,
      "loss": 1.7891,
      "step": 111168
    },
    {
      "epoch": 0.000403566869157553,
      "grad_norm": 9330.43482373678,
      "learning_rate": 3.000312048680438e-07,
      "loss": 1.7763,
      "step": 111200
    },
    {
      "epoch": 0.0004036830035083897,
      "grad_norm": 9858.226412494289,
      "learning_rate": 2.99988000719952e-07,
      "loss": 1.7539,
      "step": 111232
    },
    {
      "epoch": 0.0004037991378592264,
      "grad_norm": 9625.293553964992,
      "learning_rate": 2.9994481523052935e-07,
      "loss": 1.7466,
      "step": 111264
    },
    {
      "epoch": 0.0004039152722100631,
      "grad_norm": 10380.351053793895,
      "learning_rate": 2.9990164838634954e-07,
      "loss": 1.7577,
      "step": 111296
    },
    {
      "epoch": 0.0004040314065608998,
      "grad_norm": 8131.653583374048,
      "learning_rate": 2.9985850017399957e-07,
      "loss": 1.7595,
      "step": 111328
    },
    {
      "epoch": 0.0004041475409117365,
      "grad_norm": 10963.64784184534,
      "learning_rate": 2.998153705800801e-07,
      "loss": 1.7636,
      "step": 111360
    },
    {
      "epoch": 0.0004042636752625732,
      "grad_norm": 9087.51825307658,
      "learning_rate": 2.997722595912053e-07,
      "loss": 1.7462,
      "step": 111392
    },
    {
      "epoch": 0.00040437980961340996,
      "grad_norm": 9831.838993799685,
      "learning_rate": 2.9972916719400267e-07,
      "loss": 1.7358,
      "step": 111424
    },
    {
      "epoch": 0.00040449594396424666,
      "grad_norm": 9928.993101014825,
      "learning_rate": 2.996860933751133e-07,
      "loss": 1.7493,
      "step": 111456
    },
    {
      "epoch": 0.00040461207831508336,
      "grad_norm": 9712.353782683165,
      "learning_rate": 2.996430381211917e-07,
      "loss": 1.7596,
      "step": 111488
    },
    {
      "epoch": 0.00040472821266592006,
      "grad_norm": 9120.683307735228,
      "learning_rate": 2.996000014189056e-07,
      "loss": 1.7376,
      "step": 111520
    },
    {
      "epoch": 0.00040484434701675676,
      "grad_norm": 8941.461401806753,
      "learning_rate": 2.9955698325493646e-07,
      "loss": 1.7412,
      "step": 111552
    },
    {
      "epoch": 0.00040496048136759346,
      "grad_norm": 8904.443609793932,
      "learning_rate": 2.9951398361597883e-07,
      "loss": 1.7503,
      "step": 111584
    },
    {
      "epoch": 0.00040507661571843016,
      "grad_norm": 10351.769124164237,
      "learning_rate": 2.9947100248874087e-07,
      "loss": 1.7542,
      "step": 111616
    },
    {
      "epoch": 0.00040519275006926686,
      "grad_norm": 10558.418252749794,
      "learning_rate": 2.994280398599438e-07,
      "loss": 1.7646,
      "step": 111648
    },
    {
      "epoch": 0.00040530888442010356,
      "grad_norm": 8590.204304904511,
      "learning_rate": 2.9938509571632245e-07,
      "loss": 1.7539,
      "step": 111680
    },
    {
      "epoch": 0.0004054250187709403,
      "grad_norm": 10349.7201894544,
      "learning_rate": 2.9934217004462483e-07,
      "loss": 1.7309,
      "step": 111712
    },
    {
      "epoch": 0.000405541153121777,
      "grad_norm": 9260.997894395614,
      "learning_rate": 2.992992628316123e-07,
      "loss": 1.752,
      "step": 111744
    },
    {
      "epoch": 0.0004056572874726137,
      "grad_norm": 9717.70744568903,
      "learning_rate": 2.992563740640593e-07,
      "loss": 1.7593,
      "step": 111776
    },
    {
      "epoch": 0.0004057734218234504,
      "grad_norm": 9378.128811228815,
      "learning_rate": 2.992135037287539e-07,
      "loss": 1.7696,
      "step": 111808
    },
    {
      "epoch": 0.0004058895561742871,
      "grad_norm": 10558.404046066811,
      "learning_rate": 2.9917065181249706e-07,
      "loss": 1.744,
      "step": 111840
    },
    {
      "epoch": 0.0004060056905251238,
      "grad_norm": 10949.15941979109,
      "learning_rate": 2.991278183021033e-07,
      "loss": 1.7512,
      "step": 111872
    },
    {
      "epoch": 0.0004061218248759605,
      "grad_norm": 8993.385013441824,
      "learning_rate": 2.9908500318439996e-07,
      "loss": 1.7692,
      "step": 111904
    },
    {
      "epoch": 0.0004062379592267972,
      "grad_norm": 8723.67365276808,
      "learning_rate": 2.9904220644622783e-07,
      "loss": 1.7687,
      "step": 111936
    },
    {
      "epoch": 0.0004063540935776339,
      "grad_norm": 9266.536893575723,
      "learning_rate": 2.9899942807444097e-07,
      "loss": 1.7622,
      "step": 111968
    },
    {
      "epoch": 0.00040647022792847067,
      "grad_norm": 8802.054192062214,
      "learning_rate": 2.9895666805590635e-07,
      "loss": 1.7528,
      "step": 112000
    },
    {
      "epoch": 0.00040658636227930737,
      "grad_norm": 11375.24839289235,
      "learning_rate": 2.9891392637750424e-07,
      "loss": 1.7497,
      "step": 112032
    },
    {
      "epoch": 0.00040670249663014407,
      "grad_norm": 10512.796392967954,
      "learning_rate": 2.9887120302612795e-07,
      "loss": 1.759,
      "step": 112064
    },
    {
      "epoch": 0.00040681863098098077,
      "grad_norm": 9660.335604936301,
      "learning_rate": 2.98828497988684e-07,
      "loss": 1.7648,
      "step": 112096
    },
    {
      "epoch": 0.00040693476533181747,
      "grad_norm": 10614.10156348619,
      "learning_rate": 2.987871449357253e-07,
      "loss": 1.7403,
      "step": 112128
    },
    {
      "epoch": 0.00040705089968265417,
      "grad_norm": 9789.632985970415,
      "learning_rate": 2.9874447591562204e-07,
      "loss": 1.7283,
      "step": 112160
    },
    {
      "epoch": 0.00040716703403349087,
      "grad_norm": 11632.74963196578,
      "learning_rate": 2.9870182517065675e-07,
      "loss": 1.7355,
      "step": 112192
    },
    {
      "epoch": 0.00040728316838432757,
      "grad_norm": 8879.735469032847,
      "learning_rate": 2.9865919268778767e-07,
      "loss": 1.7397,
      "step": 112224
    },
    {
      "epoch": 0.00040739930273516427,
      "grad_norm": 9006.302570977725,
      "learning_rate": 2.986165784539862e-07,
      "loss": 1.7462,
      "step": 112256
    },
    {
      "epoch": 0.000407515437086001,
      "grad_norm": 8730.812505145212,
      "learning_rate": 2.985739824562367e-07,
      "loss": 1.7505,
      "step": 112288
    },
    {
      "epoch": 0.0004076315714368377,
      "grad_norm": 8551.842725401351,
      "learning_rate": 2.985314046815365e-07,
      "loss": 1.7494,
      "step": 112320
    },
    {
      "epoch": 0.0004077477057876744,
      "grad_norm": 10137.363562583716,
      "learning_rate": 2.9848884511689596e-07,
      "loss": 1.7762,
      "step": 112352
    },
    {
      "epoch": 0.0004078638401385111,
      "grad_norm": 11250.792683184594,
      "learning_rate": 2.984463037493383e-07,
      "loss": 1.7918,
      "step": 112384
    },
    {
      "epoch": 0.0004079799744893478,
      "grad_norm": 9081.114689287873,
      "learning_rate": 2.9840378056589977e-07,
      "loss": 1.7751,
      "step": 112416
    },
    {
      "epoch": 0.0004080961088401845,
      "grad_norm": 10758.343552796592,
      "learning_rate": 2.983612755536296e-07,
      "loss": 1.7365,
      "step": 112448
    },
    {
      "epoch": 0.0004082122431910212,
      "grad_norm": 8685.28519969264,
      "learning_rate": 2.983187886995898e-07,
      "loss": 1.7398,
      "step": 112480
    },
    {
      "epoch": 0.0004083283775418579,
      "grad_norm": 10175.620374208149,
      "learning_rate": 2.9827631999085526e-07,
      "loss": 1.7565,
      "step": 112512
    },
    {
      "epoch": 0.0004084445118926946,
      "grad_norm": 8490.861322622104,
      "learning_rate": 2.9823386941451394e-07,
      "loss": 1.7258,
      "step": 112544
    },
    {
      "epoch": 0.0004085606462435314,
      "grad_norm": 9569.478355688987,
      "learning_rate": 2.9819143695766643e-07,
      "loss": 1.7477,
      "step": 112576
    },
    {
      "epoch": 0.0004086767805943681,
      "grad_norm": 9963.356161454834,
      "learning_rate": 2.981490226074264e-07,
      "loss": 1.7502,
      "step": 112608
    },
    {
      "epoch": 0.0004087929149452048,
      "grad_norm": 7914.63366176856,
      "learning_rate": 2.9810662635092004e-07,
      "loss": 1.7458,
      "step": 112640
    },
    {
      "epoch": 0.0004089090492960415,
      "grad_norm": 8169.911627429026,
      "learning_rate": 2.9806424817528665e-07,
      "loss": 1.7658,
      "step": 112672
    },
    {
      "epoch": 0.0004090251836468782,
      "grad_norm": 8720.139792457458,
      "learning_rate": 2.980218880676782e-07,
      "loss": 1.7653,
      "step": 112704
    },
    {
      "epoch": 0.0004091413179977149,
      "grad_norm": 8109.958199645667,
      "learning_rate": 2.979795460152593e-07,
      "loss": 1.7659,
      "step": 112736
    },
    {
      "epoch": 0.0004092574523485516,
      "grad_norm": 8978.668720918486,
      "learning_rate": 2.979372220052076e-07,
      "loss": 1.7567,
      "step": 112768
    },
    {
      "epoch": 0.0004093735866993883,
      "grad_norm": 9688.026527626771,
      "learning_rate": 2.9789491602471337e-07,
      "loss": 1.7711,
      "step": 112800
    },
    {
      "epoch": 0.000409489721050225,
      "grad_norm": 9098.164650081906,
      "learning_rate": 2.978526280609795e-07,
      "loss": 1.7582,
      "step": 112832
    },
    {
      "epoch": 0.00040960585540106173,
      "grad_norm": 8656.810729131139,
      "learning_rate": 2.978103581012217e-07,
      "loss": 1.7561,
      "step": 112864
    },
    {
      "epoch": 0.00040972198975189843,
      "grad_norm": 9676.203387692924,
      "learning_rate": 2.977681061326684e-07,
      "loss": 1.7373,
      "step": 112896
    },
    {
      "epoch": 0.00040983812410273513,
      "grad_norm": 9275.447266843794,
      "learning_rate": 2.9772587214256064e-07,
      "loss": 1.7445,
      "step": 112928
    },
    {
      "epoch": 0.00040995425845357183,
      "grad_norm": 10770.94294850734,
      "learning_rate": 2.976836561181522e-07,
      "loss": 1.7431,
      "step": 112960
    },
    {
      "epoch": 0.00041007039280440853,
      "grad_norm": 10038.669134900303,
      "learning_rate": 2.9764145804670937e-07,
      "loss": 1.7552,
      "step": 112992
    },
    {
      "epoch": 0.00041018652715524523,
      "grad_norm": 8318.914111829741,
      "learning_rate": 2.9759927791551126e-07,
      "loss": 1.748,
      "step": 113024
    },
    {
      "epoch": 0.00041030266150608193,
      "grad_norm": 10102.639160140285,
      "learning_rate": 2.975571157118495e-07,
      "loss": 1.731,
      "step": 113056
    },
    {
      "epoch": 0.00041041879585691863,
      "grad_norm": 9347.52523398573,
      "learning_rate": 2.9751497142302827e-07,
      "loss": 1.7376,
      "step": 113088
    },
    {
      "epoch": 0.00041053493020775533,
      "grad_norm": 8813.69695417309,
      "learning_rate": 2.974741612150936e-07,
      "loss": 1.7621,
      "step": 113120
    },
    {
      "epoch": 0.0004106510645585921,
      "grad_norm": 11414.596620117592,
      "learning_rate": 2.9743205215906174e-07,
      "loss": 1.7643,
      "step": 113152
    },
    {
      "epoch": 0.0004107671989094288,
      "grad_norm": 9437.13250940136,
      "learning_rate": 2.973899609802539e-07,
      "loss": 1.7577,
      "step": 113184
    },
    {
      "epoch": 0.0004108833332602655,
      "grad_norm": 9793.319559781554,
      "learning_rate": 2.9734788766602413e-07,
      "loss": 1.7395,
      "step": 113216
    },
    {
      "epoch": 0.0004109994676111022,
      "grad_norm": 9553.7472229487,
      "learning_rate": 2.973058322037392e-07,
      "loss": 1.736,
      "step": 113248
    },
    {
      "epoch": 0.0004111156019619389,
      "grad_norm": 8912.412468013361,
      "learning_rate": 2.9726379458077804e-07,
      "loss": 1.7479,
      "step": 113280
    },
    {
      "epoch": 0.0004112317363127756,
      "grad_norm": 9671.275407101175,
      "learning_rate": 2.9722177478453233e-07,
      "loss": 1.7595,
      "step": 113312
    },
    {
      "epoch": 0.0004113478706636123,
      "grad_norm": 9805.556180044046,
      "learning_rate": 2.971797728024061e-07,
      "loss": 1.7728,
      "step": 113344
    },
    {
      "epoch": 0.000411464005014449,
      "grad_norm": 10453.072658314397,
      "learning_rate": 2.971377886218159e-07,
      "loss": 1.7752,
      "step": 113376
    },
    {
      "epoch": 0.0004115801393652857,
      "grad_norm": 9452.730187623045,
      "learning_rate": 2.970958222301906e-07,
      "loss": 1.7512,
      "step": 113408
    },
    {
      "epoch": 0.00041169627371612244,
      "grad_norm": 9119.691990412834,
      "learning_rate": 2.970538736149717e-07,
      "loss": 1.765,
      "step": 113440
    },
    {
      "epoch": 0.00041181240806695914,
      "grad_norm": 10753.962246539642,
      "learning_rate": 2.970119427636129e-07,
      "loss": 1.7575,
      "step": 113472
    },
    {
      "epoch": 0.00041192854241779584,
      "grad_norm": 8794.458937308196,
      "learning_rate": 2.969700296635805e-07,
      "loss": 1.7569,
      "step": 113504
    },
    {
      "epoch": 0.00041204467676863254,
      "grad_norm": 10321.344001630794,
      "learning_rate": 2.969281343023529e-07,
      "loss": 1.7809,
      "step": 113536
    },
    {
      "epoch": 0.00041216081111946924,
      "grad_norm": 10128.093403992678,
      "learning_rate": 2.9688625666742123e-07,
      "loss": 1.7651,
      "step": 113568
    },
    {
      "epoch": 0.00041227694547030594,
      "grad_norm": 9443.62568084949,
      "learning_rate": 2.968443967462886e-07,
      "loss": 1.7675,
      "step": 113600
    },
    {
      "epoch": 0.00041239307982114264,
      "grad_norm": 9941.808487393026,
      "learning_rate": 2.9680255452647064e-07,
      "loss": 1.7784,
      "step": 113632
    },
    {
      "epoch": 0.00041250921417197934,
      "grad_norm": 9624.234618919054,
      "learning_rate": 2.967607299954953e-07,
      "loss": 1.7348,
      "step": 113664
    },
    {
      "epoch": 0.00041262534852281604,
      "grad_norm": 8544.67190710094,
      "learning_rate": 2.967189231409028e-07,
      "loss": 1.7256,
      "step": 113696
    },
    {
      "epoch": 0.0004127414828736528,
      "grad_norm": 9428.471350118216,
      "learning_rate": 2.9667713395024567e-07,
      "loss": 1.7404,
      "step": 113728
    },
    {
      "epoch": 0.0004128576172244895,
      "grad_norm": 8720.019724748334,
      "learning_rate": 2.966353624110886e-07,
      "loss": 1.734,
      "step": 113760
    },
    {
      "epoch": 0.0004129737515753262,
      "grad_norm": 10195.212602001,
      "learning_rate": 2.965936085110086e-07,
      "loss": 1.7334,
      "step": 113792
    },
    {
      "epoch": 0.0004130898859261629,
      "grad_norm": 10534.149040145578,
      "learning_rate": 2.96551872237595e-07,
      "loss": 1.7225,
      "step": 113824
    },
    {
      "epoch": 0.0004132060202769996,
      "grad_norm": 10197.185101781766,
      "learning_rate": 2.9651015357844936e-07,
      "loss": 1.7338,
      "step": 113856
    },
    {
      "epoch": 0.0004133221546278363,
      "grad_norm": 8922.424334226658,
      "learning_rate": 2.964684525211852e-07,
      "loss": 1.7469,
      "step": 113888
    },
    {
      "epoch": 0.000413438288978673,
      "grad_norm": 9025.840348687761,
      "learning_rate": 2.9642676905342845e-07,
      "loss": 1.7716,
      "step": 113920
    },
    {
      "epoch": 0.0004135544233295097,
      "grad_norm": 7755.582247645884,
      "learning_rate": 2.9638510316281714e-07,
      "loss": 1.7612,
      "step": 113952
    },
    {
      "epoch": 0.0004136705576803464,
      "grad_norm": 9804.045083535671,
      "learning_rate": 2.9634345483700154e-07,
      "loss": 1.7356,
      "step": 113984
    },
    {
      "epoch": 0.00041378669203118314,
      "grad_norm": 9760.80068437011,
      "learning_rate": 2.9630182406364396e-07,
      "loss": 1.7403,
      "step": 114016
    },
    {
      "epoch": 0.00041390282638201984,
      "grad_norm": 8897.65339850907,
      "learning_rate": 2.9626021083041893e-07,
      "loss": 1.7494,
      "step": 114048
    },
    {
      "epoch": 0.00041401896073285654,
      "grad_norm": 8358.622613804262,
      "learning_rate": 2.96218615125013e-07,
      "loss": 1.752,
      "step": 114080
    },
    {
      "epoch": 0.00041413509508369324,
      "grad_norm": 8843.244088002999,
      "learning_rate": 2.9617703693512484e-07,
      "loss": 1.7797,
      "step": 114112
    },
    {
      "epoch": 0.00041425122943452994,
      "grad_norm": 10332.65590252574,
      "learning_rate": 2.96136774755104e-07,
      "loss": 1.7845,
      "step": 114144
    },
    {
      "epoch": 0.00041436736378536664,
      "grad_norm": 9262.978138806115,
      "learning_rate": 2.9609523101298943e-07,
      "loss": 1.7672,
      "step": 114176
    },
    {
      "epoch": 0.00041448349813620334,
      "grad_norm": 10002.383016061722,
      "learning_rate": 2.960537047499443e-07,
      "loss": 1.7782,
      "step": 114208
    },
    {
      "epoch": 0.00041459963248704004,
      "grad_norm": 10600.772990683274,
      "learning_rate": 2.9601219595371514e-07,
      "loss": 1.784,
      "step": 114240
    },
    {
      "epoch": 0.00041471576683787674,
      "grad_norm": 8557.31196112424,
      "learning_rate": 2.959707046120606e-07,
      "loss": 1.7478,
      "step": 114272
    },
    {
      "epoch": 0.0004148319011887135,
      "grad_norm": 8389.561967111274,
      "learning_rate": 2.959292307127512e-07,
      "loss": 1.7555,
      "step": 114304
    },
    {
      "epoch": 0.0004149480355395502,
      "grad_norm": 8349.110371770157,
      "learning_rate": 2.9588777424356963e-07,
      "loss": 1.7583,
      "step": 114336
    },
    {
      "epoch": 0.0004150641698903869,
      "grad_norm": 8966.028998391652,
      "learning_rate": 2.958463351923104e-07,
      "loss": 1.7449,
      "step": 114368
    },
    {
      "epoch": 0.0004151803042412236,
      "grad_norm": 9349.15172622629,
      "learning_rate": 2.9580491354678e-07,
      "loss": 1.7476,
      "step": 114400
    },
    {
      "epoch": 0.0004152964385920603,
      "grad_norm": 8869.947688684528,
      "learning_rate": 2.957635092947969e-07,
      "loss": 1.7206,
      "step": 114432
    },
    {
      "epoch": 0.000415412572942897,
      "grad_norm": 8930.874761186611,
      "learning_rate": 2.9572212242419164e-07,
      "loss": 1.729,
      "step": 114464
    },
    {
      "epoch": 0.0004155287072937337,
      "grad_norm": 10651.153364776981,
      "learning_rate": 2.9568075292280633e-07,
      "loss": 1.7491,
      "step": 114496
    },
    {
      "epoch": 0.0004156448416445704,
      "grad_norm": 9551.703722373302,
      "learning_rate": 2.9563940077849534e-07,
      "loss": 1.761,
      "step": 114528
    },
    {
      "epoch": 0.0004157609759954071,
      "grad_norm": 9118.89752108225,
      "learning_rate": 2.9559806597912477e-07,
      "loss": 1.7403,
      "step": 114560
    },
    {
      "epoch": 0.00041587711034624385,
      "grad_norm": 9006.379072635129,
      "learning_rate": 2.955567485125725e-07,
      "loss": 1.7275,
      "step": 114592
    },
    {
      "epoch": 0.00041599324469708055,
      "grad_norm": 9478.692736870416,
      "learning_rate": 2.9551544836672847e-07,
      "loss": 1.7427,
      "step": 114624
    },
    {
      "epoch": 0.00041610937904791725,
      "grad_norm": 9499.46219530348,
      "learning_rate": 2.9547416552949433e-07,
      "loss": 1.758,
      "step": 114656
    },
    {
      "epoch": 0.00041622551339875395,
      "grad_norm": 9803.966646210094,
      "learning_rate": 2.954328999887836e-07,
      "loss": 1.7571,
      "step": 114688
    },
    {
      "epoch": 0.00041634164774959065,
      "grad_norm": 9749.055133703983,
      "learning_rate": 2.9539165173252156e-07,
      "loss": 1.7449,
      "step": 114720
    },
    {
      "epoch": 0.00041645778210042735,
      "grad_norm": 11890.255506085645,
      "learning_rate": 2.953504207486454e-07,
      "loss": 1.7442,
      "step": 114752
    },
    {
      "epoch": 0.00041657391645126405,
      "grad_norm": 9035.23259246822,
      "learning_rate": 2.95309207025104e-07,
      "loss": 1.7356,
      "step": 114784
    },
    {
      "epoch": 0.00041669005080210075,
      "grad_norm": 9548.804741955928,
      "learning_rate": 2.9526801054985795e-07,
      "loss": 1.7422,
      "step": 114816
    },
    {
      "epoch": 0.00041680618515293745,
      "grad_norm": 8432.057518779151,
      "learning_rate": 2.9522683131087975e-07,
      "loss": 1.772,
      "step": 114848
    },
    {
      "epoch": 0.0004169223195037742,
      "grad_norm": 11411.226577366693,
      "learning_rate": 2.9518566929615363e-07,
      "loss": 1.7632,
      "step": 114880
    },
    {
      "epoch": 0.0004170384538546109,
      "grad_norm": 9476.933048196552,
      "learning_rate": 2.9514452449367535e-07,
      "loss": 1.7744,
      "step": 114912
    },
    {
      "epoch": 0.0004171545882054476,
      "grad_norm": 9571.482643770503,
      "learning_rate": 2.951033968914525e-07,
      "loss": 1.7603,
      "step": 114944
    },
    {
      "epoch": 0.0004172707225562843,
      "grad_norm": 9910.763038232728,
      "learning_rate": 2.950622864775045e-07,
      "loss": 1.767,
      "step": 114976
    },
    {
      "epoch": 0.000417386856907121,
      "grad_norm": 9457.110340902234,
      "learning_rate": 2.950211932398622e-07,
      "loss": 1.7572,
      "step": 115008
    },
    {
      "epoch": 0.0004175029912579577,
      "grad_norm": 9326.039888398505,
      "learning_rate": 2.949801171665683e-07,
      "loss": 1.7603,
      "step": 115040
    },
    {
      "epoch": 0.0004176191256087944,
      "grad_norm": 10200.706348091784,
      "learning_rate": 2.949390582456771e-07,
      "loss": 1.7689,
      "step": 115072
    },
    {
      "epoch": 0.0004177352599596311,
      "grad_norm": 9284.834301160145,
      "learning_rate": 2.9489801646525437e-07,
      "loss": 1.766,
      "step": 115104
    },
    {
      "epoch": 0.0004178513943104678,
      "grad_norm": 10871.324666295272,
      "learning_rate": 2.9485827357459736e-07,
      "loss": 1.7636,
      "step": 115136
    },
    {
      "epoch": 0.00041796752866130456,
      "grad_norm": 9443.501681050308,
      "learning_rate": 2.948172655046415e-07,
      "loss": 1.7467,
      "step": 115168
    },
    {
      "epoch": 0.00041808366301214126,
      "grad_norm": 10592.025490905882,
      "learning_rate": 2.9477627453979327e-07,
      "loss": 1.7297,
      "step": 115200
    },
    {
      "epoch": 0.00041819979736297796,
      "grad_norm": 8365.158336815868,
      "learning_rate": 2.947353006681647e-07,
      "loss": 1.7309,
      "step": 115232
    },
    {
      "epoch": 0.00041831593171381466,
      "grad_norm": 9434.658976348854,
      "learning_rate": 2.946943438778793e-07,
      "loss": 1.7515,
      "step": 115264
    },
    {
      "epoch": 0.00041843206606465136,
      "grad_norm": 10055.17657726606,
      "learning_rate": 2.946534041570722e-07,
      "loss": 1.7468,
      "step": 115296
    },
    {
      "epoch": 0.00041854820041548806,
      "grad_norm": 9582.510318282992,
      "learning_rate": 2.9461248149388995e-07,
      "loss": 1.746,
      "step": 115328
    },
    {
      "epoch": 0.00041866433476632476,
      "grad_norm": 9987.206316082591,
      "learning_rate": 2.945715758764907e-07,
      "loss": 1.7488,
      "step": 115360
    },
    {
      "epoch": 0.00041878046911716146,
      "grad_norm": 8397.29408797858,
      "learning_rate": 2.9453068729304405e-07,
      "loss": 1.7471,
      "step": 115392
    },
    {
      "epoch": 0.00041889660346799816,
      "grad_norm": 9379.08556310262,
      "learning_rate": 2.944898157317311e-07,
      "loss": 1.7518,
      "step": 115424
    },
    {
      "epoch": 0.0004190127378188349,
      "grad_norm": 11701.706713125226,
      "learning_rate": 2.944489611807446e-07,
      "loss": 1.7656,
      "step": 115456
    },
    {
      "epoch": 0.0004191288721696716,
      "grad_norm": 9615.613344971813,
      "learning_rate": 2.944081236282885e-07,
      "loss": 1.7355,
      "step": 115488
    },
    {
      "epoch": 0.0004192450065205083,
      "grad_norm": 9844.030678538136,
      "learning_rate": 2.943673030625783e-07,
      "loss": 1.7296,
      "step": 115520
    },
    {
      "epoch": 0.000419361140871345,
      "grad_norm": 8587.214216496523,
      "learning_rate": 2.94326499471841e-07,
      "loss": 1.7183,
      "step": 115552
    },
    {
      "epoch": 0.0004194772752221817,
      "grad_norm": 10670.458097007833,
      "learning_rate": 2.942857128443149e-07,
      "loss": 1.7322,
      "step": 115584
    },
    {
      "epoch": 0.0004195934095730184,
      "grad_norm": 8413.095268686788,
      "learning_rate": 2.9424494316824986e-07,
      "loss": 1.7507,
      "step": 115616
    },
    {
      "epoch": 0.0004197095439238551,
      "grad_norm": 10720.473683564545,
      "learning_rate": 2.9420419043190706e-07,
      "loss": 1.7591,
      "step": 115648
    },
    {
      "epoch": 0.0004198256782746918,
      "grad_norm": 8795.354000834759,
      "learning_rate": 2.9416345462355894e-07,
      "loss": 1.7672,
      "step": 115680
    },
    {
      "epoch": 0.0004199418126255285,
      "grad_norm": 10239.606730729458,
      "learning_rate": 2.9412273573148946e-07,
      "loss": 1.7571,
      "step": 115712
    },
    {
      "epoch": 0.00042005794697636526,
      "grad_norm": 9294.32213773549,
      "learning_rate": 2.940820337439939e-07,
      "loss": 1.7733,
      "step": 115744
    },
    {
      "epoch": 0.00042017408132720196,
      "grad_norm": 8892.393603524306,
      "learning_rate": 2.9404134864937884e-07,
      "loss": 1.7804,
      "step": 115776
    },
    {
      "epoch": 0.00042029021567803866,
      "grad_norm": 8062.536201469114,
      "learning_rate": 2.9400068043596224e-07,
      "loss": 1.7685,
      "step": 115808
    },
    {
      "epoch": 0.00042040635002887536,
      "grad_norm": 9171.868184835628,
      "learning_rate": 2.9396002909207324e-07,
      "loss": 1.7708,
      "step": 115840
    },
    {
      "epoch": 0.00042052248437971206,
      "grad_norm": 8939.489247155007,
      "learning_rate": 2.9391939460605234e-07,
      "loss": 1.7811,
      "step": 115872
    },
    {
      "epoch": 0.00042063861873054876,
      "grad_norm": 8137.758413715658,
      "learning_rate": 2.9387877696625145e-07,
      "loss": 1.7826,
      "step": 115904
    },
    {
      "epoch": 0.00042075475308138546,
      "grad_norm": 9824.25793635326,
      "learning_rate": 2.938381761610335e-07,
      "loss": 1.7496,
      "step": 115936
    },
    {
      "epoch": 0.00042087088743222216,
      "grad_norm": 9931.451152777221,
      "learning_rate": 2.9379759217877287e-07,
      "loss": 1.7285,
      "step": 115968
    },
    {
      "epoch": 0.00042098702178305886,
      "grad_norm": 9998.427076295551,
      "learning_rate": 2.9375702500785513e-07,
      "loss": 1.7255,
      "step": 116000
    },
    {
      "epoch": 0.00042110315613389556,
      "grad_norm": 9121.201455948663,
      "learning_rate": 2.9371647463667695e-07,
      "loss": 1.7356,
      "step": 116032
    },
    {
      "epoch": 0.0004212192904847323,
      "grad_norm": 10164.698126358697,
      "learning_rate": 2.9367594105364637e-07,
      "loss": 1.7498,
      "step": 116064
    },
    {
      "epoch": 0.000421335424835569,
      "grad_norm": 10318.26904088084,
      "learning_rate": 2.936354242471826e-07,
      "loss": 1.7517,
      "step": 116096
    },
    {
      "epoch": 0.0004214515591864057,
      "grad_norm": 11986.19822963061,
      "learning_rate": 2.935949242057159e-07,
      "loss": 1.7267,
      "step": 116128
    },
    {
      "epoch": 0.0004215676935372424,
      "grad_norm": 10560.118181156875,
      "learning_rate": 2.935557057669614e-07,
      "loss": 1.7397,
      "step": 116160
    },
    {
      "epoch": 0.0004216838278880791,
      "grad_norm": 9676.645389803225,
      "learning_rate": 2.9351523869781526e-07,
      "loss": 1.7538,
      "step": 116192
    },
    {
      "epoch": 0.0004217999622389158,
      "grad_norm": 9829.685752861074,
      "learning_rate": 2.9347478835938454e-07,
      "loss": 1.7583,
      "step": 116224
    },
    {
      "epoch": 0.0004219160965897525,
      "grad_norm": 9046.547186634247,
      "learning_rate": 2.9343435474014387e-07,
      "loss": 1.7254,
      "step": 116256
    },
    {
      "epoch": 0.0004220322309405892,
      "grad_norm": 8837.535176733387,
      "learning_rate": 2.93393937828579e-07,
      "loss": 1.7292,
      "step": 116288
    },
    {
      "epoch": 0.0004221483652914259,
      "grad_norm": 10340.111508102802,
      "learning_rate": 2.9335353761318667e-07,
      "loss": 1.72,
      "step": 116320
    },
    {
      "epoch": 0.00042226449964226267,
      "grad_norm": 9304.655501414332,
      "learning_rate": 2.9331315408247483e-07,
      "loss": 1.7428,
      "step": 116352
    },
    {
      "epoch": 0.00042238063399309937,
      "grad_norm": 9037.103739583828,
      "learning_rate": 2.9327278722496235e-07,
      "loss": 1.7708,
      "step": 116384
    },
    {
      "epoch": 0.00042249676834393607,
      "grad_norm": 9819.289689178133,
      "learning_rate": 2.932324370291793e-07,
      "loss": 1.7621,
      "step": 116416
    },
    {
      "epoch": 0.00042261290269477277,
      "grad_norm": 9544.9934520669,
      "learning_rate": 2.931921034836667e-07,
      "loss": 1.768,
      "step": 116448
    },
    {
      "epoch": 0.00042272903704560947,
      "grad_norm": 8464.446113007041,
      "learning_rate": 2.931517865769767e-07,
      "loss": 1.778,
      "step": 116480
    },
    {
      "epoch": 0.00042284517139644617,
      "grad_norm": 8679.148575753268,
      "learning_rate": 2.9311148629767236e-07,
      "loss": 1.7796,
      "step": 116512
    },
    {
      "epoch": 0.00042296130574728287,
      "grad_norm": 9730.4041026054,
      "learning_rate": 2.930712026343278e-07,
      "loss": 1.7587,
      "step": 116544
    },
    {
      "epoch": 0.00042307744009811957,
      "grad_norm": 9569.78432358849,
      "learning_rate": 2.9303093557552816e-07,
      "loss": 1.7559,
      "step": 116576
    },
    {
      "epoch": 0.00042319357444895627,
      "grad_norm": 8481.878683404992,
      "learning_rate": 2.929906851098694e-07,
      "loss": 1.7618,
      "step": 116608
    },
    {
      "epoch": 0.000423309708799793,
      "grad_norm": 9405.481274235784,
      "learning_rate": 2.9295045122595867e-07,
      "loss": 1.7595,
      "step": 116640
    },
    {
      "epoch": 0.0004234258431506297,
      "grad_norm": 9826.921898539746,
      "learning_rate": 2.9291023391241393e-07,
      "loss": 1.7786,
      "step": 116672
    },
    {
      "epoch": 0.0004235419775014664,
      "grad_norm": 10097.873637553601,
      "learning_rate": 2.928700331578641e-07,
      "loss": 1.7506,
      "step": 116704
    },
    {
      "epoch": 0.0004236581118523031,
      "grad_norm": 8192.61057294926,
      "learning_rate": 2.928298489509489e-07,
      "loss": 1.7329,
      "step": 116736
    },
    {
      "epoch": 0.0004237742462031398,
      "grad_norm": 8801.521686617605,
      "learning_rate": 2.927896812803192e-07,
      "loss": 1.7333,
      "step": 116768
    },
    {
      "epoch": 0.0004238903805539765,
      "grad_norm": 10211.421252695434,
      "learning_rate": 2.927495301346367e-07,
      "loss": 1.7641,
      "step": 116800
    },
    {
      "epoch": 0.0004240065149048132,
      "grad_norm": 9754.654478760383,
      "learning_rate": 2.9270939550257377e-07,
      "loss": 1.7263,
      "step": 116832
    },
    {
      "epoch": 0.0004241226492556499,
      "grad_norm": 8256.52323923333,
      "learning_rate": 2.926692773728139e-07,
      "loss": 1.7318,
      "step": 116864
    },
    {
      "epoch": 0.0004242387836064866,
      "grad_norm": 9921.677882293901,
      "learning_rate": 2.9262917573405137e-07,
      "loss": 1.7357,
      "step": 116896
    },
    {
      "epoch": 0.0004243549179573234,
      "grad_norm": 9762.04281900054,
      "learning_rate": 2.925890905749911e-07,
      "loss": 1.7419,
      "step": 116928
    },
    {
      "epoch": 0.0004244710523081601,
      "grad_norm": 10071.123373288603,
      "learning_rate": 2.925490218843492e-07,
      "loss": 1.7629,
      "step": 116960
    },
    {
      "epoch": 0.0004245871866589968,
      "grad_norm": 9713.39116889668,
      "learning_rate": 2.9250896965085226e-07,
      "loss": 1.7553,
      "step": 116992
    },
    {
      "epoch": 0.0004247033210098335,
      "grad_norm": 9770.17891340788,
      "learning_rate": 2.924689338632379e-07,
      "loss": 1.7407,
      "step": 117024
    },
    {
      "epoch": 0.0004248194553606702,
      "grad_norm": 8838.411395720386,
      "learning_rate": 2.9242891451025435e-07,
      "loss": 1.7534,
      "step": 117056
    },
    {
      "epoch": 0.0004249355897115069,
      "grad_norm": 9218.447374693853,
      "learning_rate": 2.923889115806608e-07,
      "loss": 1.7619,
      "step": 117088
    },
    {
      "epoch": 0.0004250517240623436,
      "grad_norm": 9079.779292471816,
      "learning_rate": 2.92348925063227e-07,
      "loss": 1.7437,
      "step": 117120
    },
    {
      "epoch": 0.0004251678584131803,
      "grad_norm": 9152.264091469387,
      "learning_rate": 2.92310203764729e-07,
      "loss": 1.7445,
      "step": 117152
    },
    {
      "epoch": 0.000425283992764017,
      "grad_norm": 9922.794364492293,
      "learning_rate": 2.9227024952595776e-07,
      "loss": 1.7524,
      "step": 117184
    },
    {
      "epoch": 0.00042540012711485373,
      "grad_norm": 10897.722147311335,
      "learning_rate": 2.922303116660701e-07,
      "loss": 1.75,
      "step": 117216
    },
    {
      "epoch": 0.00042551626146569043,
      "grad_norm": 9195.625699211556,
      "learning_rate": 2.921903901738784e-07,
      "loss": 1.7552,
      "step": 117248
    },
    {
      "epoch": 0.00042563239581652713,
      "grad_norm": 9436.21767447106,
      "learning_rate": 2.9215048503820584e-07,
      "loss": 1.7731,
      "step": 117280
    },
    {
      "epoch": 0.00042574853016736383,
      "grad_norm": 8565.951202289212,
      "learning_rate": 2.921105962478862e-07,
      "loss": 1.7662,
      "step": 117312
    },
    {
      "epoch": 0.00042586466451820053,
      "grad_norm": 10071.573660555732,
      "learning_rate": 2.9207072379176396e-07,
      "loss": 1.7506,
      "step": 117344
    },
    {
      "epoch": 0.00042598079886903723,
      "grad_norm": 9143.47417560743,
      "learning_rate": 2.9203086765869424e-07,
      "loss": 1.7566,
      "step": 117376
    },
    {
      "epoch": 0.00042609693321987393,
      "grad_norm": 9067.900969904777,
      "learning_rate": 2.919910278375428e-07,
      "loss": 1.7607,
      "step": 117408
    },
    {
      "epoch": 0.00042621306757071063,
      "grad_norm": 9114.586880380262,
      "learning_rate": 2.919512043171861e-07,
      "loss": 1.7533,
      "step": 117440
    },
    {
      "epoch": 0.00042632920192154733,
      "grad_norm": 9320.941261482125,
      "learning_rate": 2.9191139708651114e-07,
      "loss": 1.7287,
      "step": 117472
    },
    {
      "epoch": 0.0004264453362723841,
      "grad_norm": 9322.957685198406,
      "learning_rate": 2.9187160613441543e-07,
      "loss": 1.7392,
      "step": 117504
    },
    {
      "epoch": 0.0004265614706232208,
      "grad_norm": 8229.331443076042,
      "learning_rate": 2.918318314498074e-07,
      "loss": 1.7367,
      "step": 117536
    },
    {
      "epoch": 0.0004266776049740575,
      "grad_norm": 9342.184755184411,
      "learning_rate": 2.9179207302160574e-07,
      "loss": 1.7548,
      "step": 117568
    },
    {
      "epoch": 0.0004267937393248942,
      "grad_norm": 10656.157374964017,
      "learning_rate": 2.9175233083873974e-07,
      "loss": 1.761,
      "step": 117600
    },
    {
      "epoch": 0.0004269098736757309,
      "grad_norm": 10333.51072966008,
      "learning_rate": 2.9171260489014934e-07,
      "loss": 1.7654,
      "step": 117632
    },
    {
      "epoch": 0.0004270260080265676,
      "grad_norm": 10839.487995288338,
      "learning_rate": 2.9167289516478506e-07,
      "loss": 1.7829,
      "step": 117664
    },
    {
      "epoch": 0.0004271421423774043,
      "grad_norm": 10209.385290016242,
      "learning_rate": 2.916332016516079e-07,
      "loss": 1.7513,
      "step": 117696
    },
    {
      "epoch": 0.000427258276728241,
      "grad_norm": 9845.461187775816,
      "learning_rate": 2.9159352433958917e-07,
      "loss": 1.7495,
      "step": 117728
    },
    {
      "epoch": 0.0004273744110790777,
      "grad_norm": 8107.025101725047,
      "learning_rate": 2.9155386321771105e-07,
      "loss": 1.7221,
      "step": 117760
    },
    {
      "epoch": 0.00042749054542991444,
      "grad_norm": 9534.181034572397,
      "learning_rate": 2.915142182749659e-07,
      "loss": 1.714,
      "step": 117792
    },
    {
      "epoch": 0.00042760667978075114,
      "grad_norm": 10019.422737862695,
      "learning_rate": 2.914745895003568e-07,
      "loss": 1.7302,
      "step": 117824
    },
    {
      "epoch": 0.00042772281413158784,
      "grad_norm": 10519.457590579468,
      "learning_rate": 2.9143497688289697e-07,
      "loss": 1.7327,
      "step": 117856
    },
    {
      "epoch": 0.00042783894848242454,
      "grad_norm": 9519.364264487414,
      "learning_rate": 2.913953804116104e-07,
      "loss": 1.7567,
      "step": 117888
    },
    {
      "epoch": 0.00042795508283326124,
      "grad_norm": 10156.290267612481,
      "learning_rate": 2.913558000755314e-07,
      "loss": 1.7781,
      "step": 117920
    },
    {
      "epoch": 0.00042807121718409794,
      "grad_norm": 8463.45496827389,
      "learning_rate": 2.913162358637046e-07,
      "loss": 1.7655,
      "step": 117952
    },
    {
      "epoch": 0.00042818735153493464,
      "grad_norm": 8102.087138509435,
      "learning_rate": 2.9127668776518526e-07,
      "loss": 1.7447,
      "step": 117984
    },
    {
      "epoch": 0.00042830348588577134,
      "grad_norm": 10413.031547056793,
      "learning_rate": 2.912371557690388e-07,
      "loss": 1.7538,
      "step": 118016
    },
    {
      "epoch": 0.00042841962023660804,
      "grad_norm": 11053.085903945557,
      "learning_rate": 2.911976398643412e-07,
      "loss": 1.7521,
      "step": 118048
    },
    {
      "epoch": 0.0004285357545874448,
      "grad_norm": 9633.88561277328,
      "learning_rate": 2.911581400401787e-07,
      "loss": 1.7435,
      "step": 118080
    },
    {
      "epoch": 0.0004286518889382815,
      "grad_norm": 8006.279660366605,
      "learning_rate": 2.9111865628564796e-07,
      "loss": 1.7496,
      "step": 118112
    },
    {
      "epoch": 0.0004287680232891182,
      "grad_norm": 9387.87483938724,
      "learning_rate": 2.91079188589856e-07,
      "loss": 1.756,
      "step": 118144
    },
    {
      "epoch": 0.0004288841576399549,
      "grad_norm": 8640.76408658401,
      "learning_rate": 2.910409695631143e-07,
      "loss": 1.7654,
      "step": 118176
    },
    {
      "epoch": 0.0004290002919907916,
      "grad_norm": 10087.761892511144,
      "learning_rate": 2.9100153345117094e-07,
      "loss": 1.7634,
      "step": 118208
    },
    {
      "epoch": 0.0004291164263416283,
      "grad_norm": 9013.083323702273,
      "learning_rate": 2.909621133656885e-07,
      "loss": 1.7707,
      "step": 118240
    },
    {
      "epoch": 0.000429232560692465,
      "grad_norm": 9194.253313891237,
      "learning_rate": 2.9092270929581513e-07,
      "loss": 1.7488,
      "step": 118272
    },
    {
      "epoch": 0.0004293486950433017,
      "grad_norm": 9939.664984293988,
      "learning_rate": 2.9088332123070885e-07,
      "loss": 1.7379,
      "step": 118304
    },
    {
      "epoch": 0.0004294648293941384,
      "grad_norm": 9182.530588024196,
      "learning_rate": 2.908439491595382e-07,
      "loss": 1.7489,
      "step": 118336
    },
    {
      "epoch": 0.00042958096374497514,
      "grad_norm": 9912.860333929859,
      "learning_rate": 2.908045930714819e-07,
      "loss": 1.7345,
      "step": 118368
    },
    {
      "epoch": 0.00042969709809581184,
      "grad_norm": 9317.301648009472,
      "learning_rate": 2.907652529557291e-07,
      "loss": 1.735,
      "step": 118400
    },
    {
      "epoch": 0.00042981323244664854,
      "grad_norm": 9601.671208701118,
      "learning_rate": 2.90725928801479e-07,
      "loss": 1.7581,
      "step": 118432
    },
    {
      "epoch": 0.00042992936679748524,
      "grad_norm": 8395.363184520369,
      "learning_rate": 2.9068662059794087e-07,
      "loss": 1.7537,
      "step": 118464
    },
    {
      "epoch": 0.00043004550114832194,
      "grad_norm": 9268.45078748331,
      "learning_rate": 2.906473283343346e-07,
      "loss": 1.7646,
      "step": 118496
    },
    {
      "epoch": 0.00043016163549915864,
      "grad_norm": 9365.640181002045,
      "learning_rate": 2.9060805199988995e-07,
      "loss": 1.7604,
      "step": 118528
    },
    {
      "epoch": 0.00043027776984999534,
      "grad_norm": 10377.898438508637,
      "learning_rate": 2.9056879158384705e-07,
      "loss": 1.7243,
      "step": 118560
    },
    {
      "epoch": 0.00043039390420083204,
      "grad_norm": 9356.248179692542,
      "learning_rate": 2.9052954707545596e-07,
      "loss": 1.7235,
      "step": 118592
    },
    {
      "epoch": 0.00043051003855166874,
      "grad_norm": 9057.439483650995,
      "learning_rate": 2.904903184639772e-07,
      "loss": 1.7309,
      "step": 118624
    },
    {
      "epoch": 0.0004306261729025055,
      "grad_norm": 9488.48913157411,
      "learning_rate": 2.904511057386813e-07,
      "loss": 1.7421,
      "step": 118656
    },
    {
      "epoch": 0.0004307423072533422,
      "grad_norm": 8672.470697557876,
      "learning_rate": 2.904119088888489e-07,
      "loss": 1.7587,
      "step": 118688
    },
    {
      "epoch": 0.0004308584416041789,
      "grad_norm": 9911.472544480965,
      "learning_rate": 2.9037272790377083e-07,
      "loss": 1.7471,
      "step": 118720
    },
    {
      "epoch": 0.0004309745759550156,
      "grad_norm": 11338.529887070898,
      "learning_rate": 2.9033356277274793e-07,
      "loss": 1.7585,
      "step": 118752
    },
    {
      "epoch": 0.0004310907103058523,
      "grad_norm": 9752.785960944699,
      "learning_rate": 2.902944134850912e-07,
      "loss": 1.7725,
      "step": 118784
    },
    {
      "epoch": 0.000431206844656689,
      "grad_norm": 10297.574665910415,
      "learning_rate": 2.902552800301218e-07,
      "loss": 1.7978,
      "step": 118816
    },
    {
      "epoch": 0.0004313229790075257,
      "grad_norm": 10083.069175603228,
      "learning_rate": 2.902161623971708e-07,
      "loss": 1.781,
      "step": 118848
    },
    {
      "epoch": 0.0004314391133583624,
      "grad_norm": 9949.242584237254,
      "learning_rate": 2.901770605755796e-07,
      "loss": 1.7574,
      "step": 118880
    },
    {
      "epoch": 0.0004315552477091991,
      "grad_norm": 9599.293828193822,
      "learning_rate": 2.9013797455469926e-07,
      "loss": 1.7416,
      "step": 118912
    },
    {
      "epoch": 0.00043167138206003585,
      "grad_norm": 8767.400869128775,
      "learning_rate": 2.9009890432389114e-07,
      "loss": 1.7555,
      "step": 118944
    },
    {
      "epoch": 0.00043178751641087255,
      "grad_norm": 10013.36466928075,
      "learning_rate": 2.900598498725267e-07,
      "loss": 1.7081,
      "step": 118976
    },
    {
      "epoch": 0.00043190365076170925,
      "grad_norm": 8754.475769570672,
      "learning_rate": 2.900208111899872e-07,
      "loss": 1.7256,
      "step": 119008
    },
    {
      "epoch": 0.00043201978511254595,
      "grad_norm": 10433.385069094307,
      "learning_rate": 2.89981788265664e-07,
      "loss": 1.741,
      "step": 119040
    },
    {
      "epoch": 0.00043213591946338265,
      "grad_norm": 9172.52462520543,
      "learning_rate": 2.8994278108895846e-07,
      "loss": 1.7382,
      "step": 119072
    },
    {
      "epoch": 0.00043225205381421935,
      "grad_norm": 9694.94775643479,
      "learning_rate": 2.899037896492819e-07,
      "loss": 1.7473,
      "step": 119104
    },
    {
      "epoch": 0.00043236818816505605,
      "grad_norm": 10038.281028144213,
      "learning_rate": 2.898648139360556e-07,
      "loss": 1.7499,
      "step": 119136
    },
    {
      "epoch": 0.00043248432251589275,
      "grad_norm": 9929.325958996411,
      "learning_rate": 2.8982707120084587e-07,
      "loss": 1.7531,
      "step": 119168
    },
    {
      "epoch": 0.00043260045686672945,
      "grad_norm": 10488.929401993322,
      "learning_rate": 2.897881264181921e-07,
      "loss": 1.7497,
      "step": 119200
    },
    {
      "epoch": 0.0004327165912175662,
      "grad_norm": 8903.308149221839,
      "learning_rate": 2.897491973306416e-07,
      "loss": 1.7505,
      "step": 119232
    },
    {
      "epoch": 0.0004328327255684029,
      "grad_norm": 10064.18302695256,
      "learning_rate": 2.8971028392765517e-07,
      "loss": 1.7563,
      "step": 119264
    },
    {
      "epoch": 0.0004329488599192396,
      "grad_norm": 10425.480132828416,
      "learning_rate": 2.8967138619870326e-07,
      "loss": 1.7299,
      "step": 119296
    },
    {
      "epoch": 0.0004330649942700763,
      "grad_norm": 8654.89040947371,
      "learning_rate": 2.8963250413326656e-07,
      "loss": 1.7439,
      "step": 119328
    },
    {
      "epoch": 0.000433181128620913,
      "grad_norm": 10691.279624067458,
      "learning_rate": 2.895936377208354e-07,
      "loss": 1.7508,
      "step": 119360
    },
    {
      "epoch": 0.0004332972629717497,
      "grad_norm": 10913.656215952562,
      "learning_rate": 2.8955478695091e-07,
      "loss": 1.774,
      "step": 119392
    },
    {
      "epoch": 0.0004334133973225864,
      "grad_norm": 8295.532773728279,
      "learning_rate": 2.8951595181300053e-07,
      "loss": 1.78,
      "step": 119424
    },
    {
      "epoch": 0.0004335295316734231,
      "grad_norm": 11031.916605921204,
      "learning_rate": 2.894771322966269e-07,
      "loss": 1.7885,
      "step": 119456
    },
    {
      "epoch": 0.0004336456660242598,
      "grad_norm": 9332.86740503689,
      "learning_rate": 2.8943832839131895e-07,
      "loss": 1.7421,
      "step": 119488
    },
    {
      "epoch": 0.00043376180037509656,
      "grad_norm": 8842.256499333187,
      "learning_rate": 2.8939954008661633e-07,
      "loss": 1.7452,
      "step": 119520
    },
    {
      "epoch": 0.00043387793472593326,
      "grad_norm": 9087.031528502583,
      "learning_rate": 2.893607673720685e-07,
      "loss": 1.7614,
      "step": 119552
    },
    {
      "epoch": 0.00043399406907676996,
      "grad_norm": 9763.681785064484,
      "learning_rate": 2.893220102372346e-07,
      "loss": 1.7462,
      "step": 119584
    },
    {
      "epoch": 0.00043411020342760666,
      "grad_norm": 10562.000189358074,
      "learning_rate": 2.892832686716837e-07,
      "loss": 1.7618,
      "step": 119616
    },
    {
      "epoch": 0.00043422633777844336,
      "grad_norm": 9456.019247019329,
      "learning_rate": 2.8924454266499453e-07,
      "loss": 1.7591,
      "step": 119648
    },
    {
      "epoch": 0.00043434247212928006,
      "grad_norm": 8969.376789944774,
      "learning_rate": 2.892058322067557e-07,
      "loss": 1.7552,
      "step": 119680
    },
    {
      "epoch": 0.00043445860648011676,
      "grad_norm": 9236.957940794144,
      "learning_rate": 2.8916713728656557e-07,
      "loss": 1.7409,
      "step": 119712
    },
    {
      "epoch": 0.00043457474083095346,
      "grad_norm": 8674.016947181968,
      "learning_rate": 2.891284578940321e-07,
      "loss": 1.7435,
      "step": 119744
    },
    {
      "epoch": 0.00043469087518179016,
      "grad_norm": 11614.81398904003,
      "learning_rate": 2.890897940187731e-07,
      "loss": 1.7308,
      "step": 119776
    },
    {
      "epoch": 0.0004348070095326269,
      "grad_norm": 9881.492195007797,
      "learning_rate": 2.890511456504161e-07,
      "loss": 1.7264,
      "step": 119808
    },
    {
      "epoch": 0.0004349231438834636,
      "grad_norm": 10720.614348067933,
      "learning_rate": 2.890125127785982e-07,
      "loss": 1.7238,
      "step": 119840
    },
    {
      "epoch": 0.0004350392782343003,
      "grad_norm": 9671.666971106893,
      "learning_rate": 2.889738953929664e-07,
      "loss": 1.7296,
      "step": 119872
    },
    {
      "epoch": 0.000435155412585137,
      "grad_norm": 11027.010655658225,
      "learning_rate": 2.8893529348317715e-07,
      "loss": 1.7334,
      "step": 119904
    },
    {
      "epoch": 0.0004352715469359737,
      "grad_norm": 10007.733209873251,
      "learning_rate": 2.888967070388968e-07,
      "loss": 1.7558,
      "step": 119936
    },
    {
      "epoch": 0.0004353876812868104,
      "grad_norm": 10221.725294684846,
      "learning_rate": 2.888581360498011e-07,
      "loss": 1.7603,
      "step": 119968
    },
    {
      "epoch": 0.0004355038156376471,
      "grad_norm": 10536.822101563639,
      "learning_rate": 2.8881958050557573e-07,
      "loss": 1.7751,
      "step": 120000
    },
    {
      "epoch": 0.0004356199499884838,
      "grad_norm": 9371.304925142496,
      "learning_rate": 2.8878104039591584e-07,
      "loss": 1.7691,
      "step": 120032
    },
    {
      "epoch": 0.0004357360843393205,
      "grad_norm": 10732.011926940819,
      "learning_rate": 2.8874251571052617e-07,
      "loss": 1.7562,
      "step": 120064
    },
    {
      "epoch": 0.00043585221869015727,
      "grad_norm": 9568.430592317634,
      "learning_rate": 2.887040064391212e-07,
      "loss": 1.734,
      "step": 120096
    },
    {
      "epoch": 0.00043596835304099397,
      "grad_norm": 9833.534359527097,
      "learning_rate": 2.8866551257142487e-07,
      "loss": 1.7325,
      "step": 120128
    },
    {
      "epoch": 0.00043608448739183066,
      "grad_norm": 16796.03262678422,
      "learning_rate": 2.886270340971709e-07,
      "loss": 1.7496,
      "step": 120160
    },
    {
      "epoch": 0.00043620062174266736,
      "grad_norm": 10847.765299820974,
      "learning_rate": 2.8858977274494905e-07,
      "loss": 1.7695,
      "step": 120192
    },
    {
      "epoch": 0.00043631675609350406,
      "grad_norm": 9412.687182733738,
      "learning_rate": 2.8855132454656936e-07,
      "loss": 1.7738,
      "step": 120224
    },
    {
      "epoch": 0.00043643289044434076,
      "grad_norm": 7847.433465789945,
      "learning_rate": 2.8851289171120993e-07,
      "loss": 1.7612,
      "step": 120256
    },
    {
      "epoch": 0.00043654902479517746,
      "grad_norm": 8847.757795057458,
      "learning_rate": 2.884744742286423e-07,
      "loss": 1.7449,
      "step": 120288
    },
    {
      "epoch": 0.00043666515914601416,
      "grad_norm": 8662.040637170898,
      "learning_rate": 2.884360720886475e-07,
      "loss": 1.7511,
      "step": 120320
    },
    {
      "epoch": 0.00043678129349685086,
      "grad_norm": 9928.928743827302,
      "learning_rate": 2.883976852810163e-07,
      "loss": 1.7644,
      "step": 120352
    },
    {
      "epoch": 0.0004368974278476876,
      "grad_norm": 10107.477430100946,
      "learning_rate": 2.8835931379554856e-07,
      "loss": 1.7612,
      "step": 120384
    },
    {
      "epoch": 0.0004370135621985243,
      "grad_norm": 11921.86294167149,
      "learning_rate": 2.88320957622054e-07,
      "loss": 1.7503,
      "step": 120416
    },
    {
      "epoch": 0.000437129696549361,
      "grad_norm": 10413.47473228797,
      "learning_rate": 2.882826167503517e-07,
      "loss": 1.746,
      "step": 120448
    },
    {
      "epoch": 0.0004372458309001977,
      "grad_norm": 9554.654572510719,
      "learning_rate": 2.8824429117027024e-07,
      "loss": 1.741,
      "step": 120480
    },
    {
      "epoch": 0.0004373619652510344,
      "grad_norm": 9082.102179561734,
      "learning_rate": 2.8820598087164755e-07,
      "loss": 1.7396,
      "step": 120512
    },
    {
      "epoch": 0.0004374780996018711,
      "grad_norm": 9305.002847930784,
      "learning_rate": 2.8816768584433116e-07,
      "loss": 1.7387,
      "step": 120544
    },
    {
      "epoch": 0.0004375942339527078,
      "grad_norm": 10963.583082186226,
      "learning_rate": 2.88129406078178e-07,
      "loss": 1.7463,
      "step": 120576
    },
    {
      "epoch": 0.0004377103683035445,
      "grad_norm": 8450.803157096963,
      "learning_rate": 2.880911415630544e-07,
      "loss": 1.7551,
      "step": 120608
    },
    {
      "epoch": 0.0004378265026543812,
      "grad_norm": 9175.665970380569,
      "learning_rate": 2.8805289228883606e-07,
      "loss": 1.7453,
      "step": 120640
    },
    {
      "epoch": 0.00043794263700521797,
      "grad_norm": 8912.841410010615,
      "learning_rate": 2.8801465824540827e-07,
      "loss": 1.7409,
      "step": 120672
    },
    {
      "epoch": 0.00043805877135605467,
      "grad_norm": 8484.102545349155,
      "learning_rate": 2.8797643942266543e-07,
      "loss": 1.7397,
      "step": 120704
    },
    {
      "epoch": 0.00043817490570689137,
      "grad_norm": 11172.959858515558,
      "learning_rate": 2.879382358105116e-07,
      "loss": 1.739,
      "step": 120736
    },
    {
      "epoch": 0.00043829104005772807,
      "grad_norm": 10768.488844772974,
      "learning_rate": 2.879000473988601e-07,
      "loss": 1.7491,
      "step": 120768
    },
    {
      "epoch": 0.00043840717440856477,
      "grad_norm": 11254.55179027579,
      "learning_rate": 2.878618741776335e-07,
      "loss": 1.7475,
      "step": 120800
    },
    {
      "epoch": 0.00043852330875940147,
      "grad_norm": 9094.6439182631,
      "learning_rate": 2.87823716136764e-07,
      "loss": 1.729,
      "step": 120832
    },
    {
      "epoch": 0.00043863944311023817,
      "grad_norm": 11059.582451430977,
      "learning_rate": 2.877855732661928e-07,
      "loss": 1.737,
      "step": 120864
    },
    {
      "epoch": 0.00043875557746107487,
      "grad_norm": 9859.396228978729,
      "learning_rate": 2.8774744555587073e-07,
      "loss": 1.7312,
      "step": 120896
    },
    {
      "epoch": 0.00043887171181191157,
      "grad_norm": 9311.817438072978,
      "learning_rate": 2.877093329957578e-07,
      "loss": 1.7591,
      "step": 120928
    },
    {
      "epoch": 0.0004389878461627483,
      "grad_norm": 10711.165949606046,
      "learning_rate": 2.876712355758232e-07,
      "loss": 1.7767,
      "step": 120960
    },
    {
      "epoch": 0.000439103980513585,
      "grad_norm": 9355.489618400525,
      "learning_rate": 2.876331532860457e-07,
      "loss": 1.7739,
      "step": 120992
    },
    {
      "epoch": 0.0004392201148644217,
      "grad_norm": 8447.638841711925,
      "learning_rate": 2.875950861164131e-07,
      "loss": 1.7545,
      "step": 121024
    },
    {
      "epoch": 0.0004393362492152584,
      "grad_norm": 9480.435011116315,
      "learning_rate": 2.875570340569227e-07,
      "loss": 1.7597,
      "step": 121056
    },
    {
      "epoch": 0.0004394523835660951,
      "grad_norm": 8503.929797452469,
      "learning_rate": 2.875189970975808e-07,
      "loss": 1.7779,
      "step": 121088
    },
    {
      "epoch": 0.0004395685179169318,
      "grad_norm": 8560.94247148058,
      "learning_rate": 2.874809752284031e-07,
      "loss": 1.7632,
      "step": 121120
    },
    {
      "epoch": 0.0004396846522677685,
      "grad_norm": 8425.194834542404,
      "learning_rate": 2.874429684394146e-07,
      "loss": 1.7626,
      "step": 121152
    },
    {
      "epoch": 0.0004398007866186052,
      "grad_norm": 8717.147813361891,
      "learning_rate": 2.8740616373384606e-07,
      "loss": 1.7721,
      "step": 121184
    },
    {
      "epoch": 0.0004399169209694419,
      "grad_norm": 9477.320929461026,
      "learning_rate": 2.8736818660486485e-07,
      "loss": 1.7635,
      "step": 121216
    },
    {
      "epoch": 0.0004400330553202786,
      "grad_norm": 8848.259941932085,
      "learning_rate": 2.8733022452651365e-07,
      "loss": 1.7227,
      "step": 121248
    },
    {
      "epoch": 0.0004401491896711154,
      "grad_norm": 8956.414014548456,
      "learning_rate": 2.87292277488854e-07,
      "loss": 1.7417,
      "step": 121280
    },
    {
      "epoch": 0.0004402653240219521,
      "grad_norm": 9570.130406635011,
      "learning_rate": 2.8725434548195647e-07,
      "loss": 1.7327,
      "step": 121312
    },
    {
      "epoch": 0.0004403814583727888,
      "grad_norm": 8662.278222269244,
      "learning_rate": 2.8721642849590096e-07,
      "loss": 1.7282,
      "step": 121344
    },
    {
      "epoch": 0.0004404975927236255,
      "grad_norm": 10196.770861405095,
      "learning_rate": 2.8717852652077646e-07,
      "loss": 1.7509,
      "step": 121376
    },
    {
      "epoch": 0.0004406137270744622,
      "grad_norm": 8989.432907586552,
      "learning_rate": 2.871406395466812e-07,
      "loss": 1.742,
      "step": 121408
    },
    {
      "epoch": 0.0004407298614252989,
      "grad_norm": 10549.221772244624,
      "learning_rate": 2.8710276756372245e-07,
      "loss": 1.7293,
      "step": 121440
    },
    {
      "epoch": 0.0004408459957761356,
      "grad_norm": 9961.142103192786,
      "learning_rate": 2.870649105620166e-07,
      "loss": 1.736,
      "step": 121472
    },
    {
      "epoch": 0.0004409621301269723,
      "grad_norm": 9644.832606116086,
      "learning_rate": 2.8702706853168925e-07,
      "loss": 1.7398,
      "step": 121504
    },
    {
      "epoch": 0.000441078264477809,
      "grad_norm": 10064.91281631391,
      "learning_rate": 2.8698924146287513e-07,
      "loss": 1.736,
      "step": 121536
    },
    {
      "epoch": 0.00044119439882864573,
      "grad_norm": 8498.44432822855,
      "learning_rate": 2.8695142934571793e-07,
      "loss": 1.7356,
      "step": 121568
    },
    {
      "epoch": 0.00044131053317948243,
      "grad_norm": 10537.258466982767,
      "learning_rate": 2.869136321703705e-07,
      "loss": 1.745,
      "step": 121600
    },
    {
      "epoch": 0.00044142666753031913,
      "grad_norm": 9133.491665294276,
      "learning_rate": 2.8687584992699494e-07,
      "loss": 1.7255,
      "step": 121632
    },
    {
      "epoch": 0.00044154280188115583,
      "grad_norm": 9398.926960031129,
      "learning_rate": 2.86838082605762e-07,
      "loss": 1.7477,
      "step": 121664
    },
    {
      "epoch": 0.00044165893623199253,
      "grad_norm": 9493.747626727814,
      "learning_rate": 2.868003301968518e-07,
      "loss": 1.7675,
      "step": 121696
    },
    {
      "epoch": 0.00044177507058282923,
      "grad_norm": 9735.666284338222,
      "learning_rate": 2.867625926904536e-07,
      "loss": 1.7688,
      "step": 121728
    },
    {
      "epoch": 0.00044189120493366593,
      "grad_norm": 9966.144088864057,
      "learning_rate": 2.8672487007676543e-07,
      "loss": 1.7791,
      "step": 121760
    },
    {
      "epoch": 0.00044200733928450263,
      "grad_norm": 9968.747363636016,
      "learning_rate": 2.866871623459944e-07,
      "loss": 1.776,
      "step": 121792
    },
    {
      "epoch": 0.00044212347363533933,
      "grad_norm": 9642.235529170608,
      "learning_rate": 2.8664946948835666e-07,
      "loss": 1.7635,
      "step": 121824
    },
    {
      "epoch": 0.0004422396079861761,
      "grad_norm": 9882.699226425946,
      "learning_rate": 2.8661179149407753e-07,
      "loss": 1.7638,
      "step": 121856
    },
    {
      "epoch": 0.0004423557423370128,
      "grad_norm": 9463.123374446726,
      "learning_rate": 2.8657412835339104e-07,
      "loss": 1.7764,
      "step": 121888
    },
    {
      "epoch": 0.0004424718766878495,
      "grad_norm": 9250.27145547632,
      "learning_rate": 2.865364800565404e-07,
      "loss": 1.7651,
      "step": 121920
    },
    {
      "epoch": 0.0004425880110386862,
      "grad_norm": 10461.31817697942,
      "learning_rate": 2.8649884659377775e-07,
      "loss": 1.7684,
      "step": 121952
    },
    {
      "epoch": 0.0004427041453895229,
      "grad_norm": 10094.532183315876,
      "learning_rate": 2.86461227955364e-07,
      "loss": 1.7533,
      "step": 121984
    },
    {
      "epoch": 0.0004428202797403596,
      "grad_norm": 10025.357250492372,
      "learning_rate": 2.864236241315694e-07,
      "loss": 1.7267,
      "step": 122016
    },
    {
      "epoch": 0.0004429364140911963,
      "grad_norm": 9173.854805914469,
      "learning_rate": 2.863860351126728e-07,
      "loss": 1.7213,
      "step": 122048
    },
    {
      "epoch": 0.000443052548442033,
      "grad_norm": 8902.587264385562,
      "learning_rate": 2.86348460888962e-07,
      "loss": 1.7229,
      "step": 122080
    },
    {
      "epoch": 0.0004431686827928697,
      "grad_norm": 9200.403252031945,
      "learning_rate": 2.8631090145073404e-07,
      "loss": 1.7342,
      "step": 122112
    },
    {
      "epoch": 0.00044328481714370644,
      "grad_norm": 8655.206987703992,
      "learning_rate": 2.862733567882944e-07,
      "loss": 1.7294,
      "step": 122144
    },
    {
      "epoch": 0.00044340095149454314,
      "grad_norm": 8815.622836759749,
      "learning_rate": 2.8623699947780416e-07,
      "loss": 1.7344,
      "step": 122176
    },
    {
      "epoch": 0.00044351708584537984,
      "grad_norm": 8368.068116357563,
      "learning_rate": 2.861994838769022e-07,
      "loss": 1.7593,
      "step": 122208
    },
    {
      "epoch": 0.00044363322019621654,
      "grad_norm": 9545.801590228031,
      "learning_rate": 2.86161983023061e-07,
      "loss": 1.7579,
      "step": 122240
    },
    {
      "epoch": 0.00044374935454705324,
      "grad_norm": 8367.813812460217,
      "learning_rate": 2.861244969066217e-07,
      "loss": 1.7524,
      "step": 122272
    },
    {
      "epoch": 0.00044386548889788994,
      "grad_norm": 11367.316305971257,
      "learning_rate": 2.8608702551793407e-07,
      "loss": 1.7385,
      "step": 122304
    },
    {
      "epoch": 0.00044398162324872664,
      "grad_norm": 8346.32637751484,
      "learning_rate": 2.8604956884735674e-07,
      "loss": 1.7393,
      "step": 122336
    },
    {
      "epoch": 0.00044409775759956334,
      "grad_norm": 9145.436457600043,
      "learning_rate": 2.860121268852572e-07,
      "loss": 1.7427,
      "step": 122368
    },
    {
      "epoch": 0.00044421389195040004,
      "grad_norm": 9666.20835695155,
      "learning_rate": 2.859746996220117e-07,
      "loss": 1.7287,
      "step": 122400
    },
    {
      "epoch": 0.0004443300263012368,
      "grad_norm": 8796.42995765896,
      "learning_rate": 2.859372870480055e-07,
      "loss": 1.7324,
      "step": 122432
    },
    {
      "epoch": 0.0004444461606520735,
      "grad_norm": 10571.77203689145,
      "learning_rate": 2.858998891536325e-07,
      "loss": 1.7506,
      "step": 122464
    },
    {
      "epoch": 0.0004445622950029102,
      "grad_norm": 10048.432016986531,
      "learning_rate": 2.858625059292954e-07,
      "loss": 1.7723,
      "step": 122496
    },
    {
      "epoch": 0.0004446784293537469,
      "grad_norm": 9578.360402490605,
      "learning_rate": 2.8582513736540567e-07,
      "loss": 1.7736,
      "step": 122528
    },
    {
      "epoch": 0.0004447945637045836,
      "grad_norm": 9495.558330082546,
      "learning_rate": 2.8578778345238365e-07,
      "loss": 1.7628,
      "step": 122560
    },
    {
      "epoch": 0.0004449106980554203,
      "grad_norm": 10071.91223154769,
      "learning_rate": 2.8575044418065836e-07,
      "loss": 1.7531,
      "step": 122592
    },
    {
      "epoch": 0.000445026832406257,
      "grad_norm": 7736.661812435645,
      "learning_rate": 2.8571311954066756e-07,
      "loss": 1.7658,
      "step": 122624
    },
    {
      "epoch": 0.0004451429667570937,
      "grad_norm": 9967.98073834415,
      "learning_rate": 2.8567580952285776e-07,
      "loss": 1.7499,
      "step": 122656
    },
    {
      "epoch": 0.0004452591011079304,
      "grad_norm": 7907.828526213754,
      "learning_rate": 2.8563851411768434e-07,
      "loss": 1.7555,
      "step": 122688
    },
    {
      "epoch": 0.00044537523545876715,
      "grad_norm": 8517.25260867611,
      "learning_rate": 2.856012333156112e-07,
      "loss": 1.7548,
      "step": 122720
    },
    {
      "epoch": 0.00044549136980960385,
      "grad_norm": 9385.177249258533,
      "learning_rate": 2.85563967107111e-07,
      "loss": 1.7334,
      "step": 122752
    },
    {
      "epoch": 0.00044560750416044055,
      "grad_norm": 10446.713167307696,
      "learning_rate": 2.855267154826652e-07,
      "loss": 1.7458,
      "step": 122784
    },
    {
      "epoch": 0.00044572363851127725,
      "grad_norm": 9363.53800654432,
      "learning_rate": 2.854894784327638e-07,
      "loss": 1.7644,
      "step": 122816
    },
    {
      "epoch": 0.00044583977286211395,
      "grad_norm": 9896.319517881382,
      "learning_rate": 2.8545225594790566e-07,
      "loss": 1.7408,
      "step": 122848
    },
    {
      "epoch": 0.00044595590721295065,
      "grad_norm": 9533.51666490388,
      "learning_rate": 2.8541504801859815e-07,
      "loss": 1.7407,
      "step": 122880
    },
    {
      "epoch": 0.00044607204156378735,
      "grad_norm": 11442.38279380654,
      "learning_rate": 2.853778546353573e-07,
      "loss": 1.73,
      "step": 122912
    },
    {
      "epoch": 0.00044618817591462405,
      "grad_norm": 8692.790346028138,
      "learning_rate": 2.8534067578870806e-07,
      "loss": 1.7542,
      "step": 122944
    },
    {
      "epoch": 0.00044630431026546075,
      "grad_norm": 9309.884746869855,
      "learning_rate": 2.853035114691836e-07,
      "loss": 1.7606,
      "step": 122976
    },
    {
      "epoch": 0.0004464204446162975,
      "grad_norm": 11220.977319289083,
      "learning_rate": 2.8526636166732595e-07,
      "loss": 1.734,
      "step": 123008
    },
    {
      "epoch": 0.0004465365789671342,
      "grad_norm": 10100.596616042045,
      "learning_rate": 2.852292263736858e-07,
      "loss": 1.7446,
      "step": 123040
    },
    {
      "epoch": 0.0004466527133179709,
      "grad_norm": 9849.622530838427,
      "learning_rate": 2.851921055788224e-07,
      "loss": 1.7336,
      "step": 123072
    },
    {
      "epoch": 0.0004467688476688076,
      "grad_norm": 8678.748527293552,
      "learning_rate": 2.8515499927330356e-07,
      "loss": 1.7529,
      "step": 123104
    },
    {
      "epoch": 0.0004468849820196443,
      "grad_norm": 10294.946138761485,
      "learning_rate": 2.851179074477056e-07,
      "loss": 1.7452,
      "step": 123136
    },
    {
      "epoch": 0.000447001116370481,
      "grad_norm": 9230.132935120708,
      "learning_rate": 2.8508083009261363e-07,
      "loss": 1.7232,
      "step": 123168
    },
    {
      "epoch": 0.0004471172507213177,
      "grad_norm": 11158.928980865503,
      "learning_rate": 2.8504492519525824e-07,
      "loss": 1.7339,
      "step": 123200
    },
    {
      "epoch": 0.0004472333850721544,
      "grad_norm": 9382.271473369336,
      "learning_rate": 2.850078763014939e-07,
      "loss": 1.7532,
      "step": 123232
    },
    {
      "epoch": 0.0004473495194229911,
      "grad_norm": 9637.548235936358,
      "learning_rate": 2.849708418503352e-07,
      "loss": 1.7451,
      "step": 123264
    },
    {
      "epoch": 0.00044746565377382785,
      "grad_norm": 10208.090516840062,
      "learning_rate": 2.84933821832401e-07,
      "loss": 1.743,
      "step": 123296
    },
    {
      "epoch": 0.00044758178812466455,
      "grad_norm": 10619.112957304862,
      "learning_rate": 2.8489681623831887e-07,
      "loss": 1.7563,
      "step": 123328
    },
    {
      "epoch": 0.00044769792247550125,
      "grad_norm": 10197.847223801698,
      "learning_rate": 2.8485982505872477e-07,
      "loss": 1.752,
      "step": 123360
    },
    {
      "epoch": 0.00044781405682633795,
      "grad_norm": 9301.935390014274,
      "learning_rate": 2.8482284828426313e-07,
      "loss": 1.7656,
      "step": 123392
    },
    {
      "epoch": 0.00044793019117717465,
      "grad_norm": 10129.734053764689,
      "learning_rate": 2.84785885905587e-07,
      "loss": 1.7762,
      "step": 123424
    },
    {
      "epoch": 0.00044804632552801135,
      "grad_norm": 9818.4335817889,
      "learning_rate": 2.8474893791335786e-07,
      "loss": 1.7713,
      "step": 123456
    },
    {
      "epoch": 0.00044816245987884805,
      "grad_norm": 11143.692924699602,
      "learning_rate": 2.847120042982457e-07,
      "loss": 1.7657,
      "step": 123488
    },
    {
      "epoch": 0.00044827859422968475,
      "grad_norm": 8292.318614235708,
      "learning_rate": 2.8467508505092896e-07,
      "loss": 1.75,
      "step": 123520
    },
    {
      "epoch": 0.00044839472858052145,
      "grad_norm": 9604.524662886759,
      "learning_rate": 2.846381801620944e-07,
      "loss": 1.7586,
      "step": 123552
    },
    {
      "epoch": 0.0004485108629313582,
      "grad_norm": 9241.72061901895,
      "learning_rate": 2.8460128962243757e-07,
      "loss": 1.7356,
      "step": 123584
    },
    {
      "epoch": 0.0004486269972821949,
      "grad_norm": 9412.648298964537,
      "learning_rate": 2.845644134226621e-07,
      "loss": 1.7314,
      "step": 123616
    },
    {
      "epoch": 0.0004487431316330316,
      "grad_norm": 8797.089405024823,
      "learning_rate": 2.8452755155348026e-07,
      "loss": 1.745,
      "step": 123648
    },
    {
      "epoch": 0.0004488592659838683,
      "grad_norm": 9519.251441158596,
      "learning_rate": 2.8449070400561264e-07,
      "loss": 1.7448,
      "step": 123680
    },
    {
      "epoch": 0.000448975400334705,
      "grad_norm": 9350.496671300407,
      "learning_rate": 2.844538707697884e-07,
      "loss": 1.7461,
      "step": 123712
    },
    {
      "epoch": 0.0004490915346855417,
      "grad_norm": 8289.63678335788,
      "learning_rate": 2.8441705183674483e-07,
      "loss": 1.7687,
      "step": 123744
    },
    {
      "epoch": 0.0004492076690363784,
      "grad_norm": 9220.147721159352,
      "learning_rate": 2.8438024719722787e-07,
      "loss": 1.731,
      "step": 123776
    },
    {
      "epoch": 0.0004493238033872151,
      "grad_norm": 10134.314974382827,
      "learning_rate": 2.843434568419916e-07,
      "loss": 1.731,
      "step": 123808
    },
    {
      "epoch": 0.0004494399377380518,
      "grad_norm": 8048.200419969672,
      "learning_rate": 2.8430668076179873e-07,
      "loss": 1.724,
      "step": 123840
    },
    {
      "epoch": 0.00044955607208888856,
      "grad_norm": 8319.170271126803,
      "learning_rate": 2.842699189474202e-07,
      "loss": 1.7253,
      "step": 123872
    },
    {
      "epoch": 0.00044967220643972526,
      "grad_norm": 9753.963399562252,
      "learning_rate": 2.842331713896352e-07,
      "loss": 1.7216,
      "step": 123904
    },
    {
      "epoch": 0.00044978834079056196,
      "grad_norm": 8874.943605454628,
      "learning_rate": 2.841964380792314e-07,
      "loss": 1.7385,
      "step": 123936
    },
    {
      "epoch": 0.00044990447514139866,
      "grad_norm": 9632.507669345507,
      "learning_rate": 2.841597190070049e-07,
      "loss": 1.7579,
      "step": 123968
    },
    {
      "epoch": 0.00045002060949223536,
      "grad_norm": 9324.731202560211,
      "learning_rate": 2.8412301416375985e-07,
      "loss": 1.7496,
      "step": 124000
    },
    {
      "epoch": 0.00045013674384307206,
      "grad_norm": 8896.375666528476,
      "learning_rate": 2.840863235403089e-07,
      "loss": 1.7717,
      "step": 124032
    },
    {
      "epoch": 0.00045025287819390876,
      "grad_norm": 9909.766092093194,
      "learning_rate": 2.8404964712747295e-07,
      "loss": 1.7646,
      "step": 124064
    },
    {
      "epoch": 0.00045036901254474546,
      "grad_norm": 9796.441292632748,
      "learning_rate": 2.840129849160812e-07,
      "loss": 1.7603,
      "step": 124096
    },
    {
      "epoch": 0.00045048514689558216,
      "grad_norm": 8996.895464547757,
      "learning_rate": 2.8397633689697113e-07,
      "loss": 1.7551,
      "step": 124128
    },
    {
      "epoch": 0.0004506012812464189,
      "grad_norm": 9821.638458017074,
      "learning_rate": 2.8393970306098846e-07,
      "loss": 1.7739,
      "step": 124160
    },
    {
      "epoch": 0.0004507174155972556,
      "grad_norm": 9946.821803973367,
      "learning_rate": 2.83904227548968e-07,
      "loss": 1.742,
      "step": 124192
    },
    {
      "epoch": 0.0004508335499480923,
      "grad_norm": 9943.127475799552,
      "learning_rate": 2.838676216092974e-07,
      "loss": 1.7519,
      "step": 124224
    },
    {
      "epoch": 0.000450949684298929,
      "grad_norm": 8873.99166102831,
      "learning_rate": 2.8383102982562625e-07,
      "loss": 1.7526,
      "step": 124256
    },
    {
      "epoch": 0.0004510658186497657,
      "grad_norm": 7978.2340151188855,
      "learning_rate": 2.8379445218883295e-07,
      "loss": 1.7309,
      "step": 124288
    },
    {
      "epoch": 0.0004511819530006024,
      "grad_norm": 9979.995490980946,
      "learning_rate": 2.8375788868980437e-07,
      "loss": 1.7492,
      "step": 124320
    },
    {
      "epoch": 0.0004512980873514391,
      "grad_norm": 9298.350283786904,
      "learning_rate": 2.8372133931943533e-07,
      "loss": 1.7552,
      "step": 124352
    },
    {
      "epoch": 0.0004514142217022758,
      "grad_norm": 9597.359220118835,
      "learning_rate": 2.8368480406862906e-07,
      "loss": 1.7439,
      "step": 124384
    },
    {
      "epoch": 0.0004515303560531125,
      "grad_norm": 8797.457246272927,
      "learning_rate": 2.8364828292829686e-07,
      "loss": 1.7311,
      "step": 124416
    },
    {
      "epoch": 0.00045164649040394927,
      "grad_norm": 8790.705546200486,
      "learning_rate": 2.836117758893583e-07,
      "loss": 1.7299,
      "step": 124448
    },
    {
      "epoch": 0.00045176262475478597,
      "grad_norm": 8923.61429018534,
      "learning_rate": 2.835752829427411e-07,
      "loss": 1.7523,
      "step": 124480
    },
    {
      "epoch": 0.00045187875910562267,
      "grad_norm": 11518.378010813849,
      "learning_rate": 2.8353880407938103e-07,
      "loss": 1.7543,
      "step": 124512
    },
    {
      "epoch": 0.00045199489345645937,
      "grad_norm": 9540.310686764871,
      "learning_rate": 2.835023392902223e-07,
      "loss": 1.7608,
      "step": 124544
    },
    {
      "epoch": 0.00045211102780729607,
      "grad_norm": 9087.670328527549,
      "learning_rate": 2.83465888566217e-07,
      "loss": 1.7258,
      "step": 124576
    },
    {
      "epoch": 0.00045222716215813277,
      "grad_norm": 9728.598049051056,
      "learning_rate": 2.834294518983255e-07,
      "loss": 1.7279,
      "step": 124608
    },
    {
      "epoch": 0.00045234329650896947,
      "grad_norm": 8755.593869064509,
      "learning_rate": 2.8339302927751624e-07,
      "loss": 1.7453,
      "step": 124640
    },
    {
      "epoch": 0.00045245943085980617,
      "grad_norm": 10318.14169315386,
      "learning_rate": 2.8335662069476584e-07,
      "loss": 1.7445,
      "step": 124672
    },
    {
      "epoch": 0.00045257556521064287,
      "grad_norm": 9345.060299430925,
      "learning_rate": 2.83320226141059e-07,
      "loss": 1.7612,
      "step": 124704
    },
    {
      "epoch": 0.0004526916995614796,
      "grad_norm": 8761.712275577189,
      "learning_rate": 2.8328384560738854e-07,
      "loss": 1.7605,
      "step": 124736
    },
    {
      "epoch": 0.0004528078339123163,
      "grad_norm": 10104.09441761111,
      "learning_rate": 2.832474790847553e-07,
      "loss": 1.7651,
      "step": 124768
    },
    {
      "epoch": 0.000452923968263153,
      "grad_norm": 9394.506266962622,
      "learning_rate": 2.832111265641683e-07,
      "loss": 1.7507,
      "step": 124800
    },
    {
      "epoch": 0.0004530401026139897,
      "grad_norm": 11422.304671124826,
      "learning_rate": 2.8317478803664457e-07,
      "loss": 1.7637,
      "step": 124832
    },
    {
      "epoch": 0.0004531562369648264,
      "grad_norm": 9342.579729389523,
      "learning_rate": 2.831384634932093e-07,
      "loss": 1.7438,
      "step": 124864
    },
    {
      "epoch": 0.0004532723713156631,
      "grad_norm": 9574.328592648155,
      "learning_rate": 2.831021529248956e-07,
      "loss": 1.7496,
      "step": 124896
    },
    {
      "epoch": 0.0004533885056664998,
      "grad_norm": 9623.13015603551,
      "learning_rate": 2.830658563227448e-07,
      "loss": 1.7578,
      "step": 124928
    },
    {
      "epoch": 0.0004535046400173365,
      "grad_norm": 9054.125247642645,
      "learning_rate": 2.8302957367780603e-07,
      "loss": 1.7616,
      "step": 124960
    },
    {
      "epoch": 0.0004536207743681732,
      "grad_norm": 9877.498671222385,
      "learning_rate": 2.8299330498113673e-07,
      "loss": 1.7516,
      "step": 124992
    },
    {
      "epoch": 0.00045373690871901,
      "grad_norm": 10323.507252867119,
      "learning_rate": 2.8295705022380207e-07,
      "loss": 1.7225,
      "step": 125024
    },
    {
      "epoch": 0.0004538530430698467,
      "grad_norm": 8865.42926202674,
      "learning_rate": 2.8292080939687556e-07,
      "loss": 1.7238,
      "step": 125056
    },
    {
      "epoch": 0.00045396917742068337,
      "grad_norm": 9440.086440282208,
      "learning_rate": 2.828845824914383e-07,
      "loss": 1.7392,
      "step": 125088
    },
    {
      "epoch": 0.00045408531177152007,
      "grad_norm": 10336.733913572507,
      "learning_rate": 2.8284836949857983e-07,
      "loss": 1.7318,
      "step": 125120
    },
    {
      "epoch": 0.00045420144612235677,
      "grad_norm": 7836.176235894647,
      "learning_rate": 2.828121704093973e-07,
      "loss": 1.7387,
      "step": 125152
    },
    {
      "epoch": 0.00045431758047319347,
      "grad_norm": 8542.79123003717,
      "learning_rate": 2.82775985214996e-07,
      "loss": 1.7349,
      "step": 125184
    },
    {
      "epoch": 0.00045443371482403017,
      "grad_norm": 9462.987794560448,
      "learning_rate": 2.8274094404978144e-07,
      "loss": 1.7457,
      "step": 125216
    },
    {
      "epoch": 0.00045454984917486687,
      "grad_norm": 9099.418882544094,
      "learning_rate": 2.82704786184768e-07,
      "loss": 1.7677,
      "step": 125248
    },
    {
      "epoch": 0.00045466598352570357,
      "grad_norm": 9524.55069806445,
      "learning_rate": 2.826686421881765e-07,
      "loss": 1.7847,
      "step": 125280
    },
    {
      "epoch": 0.0004547821178765403,
      "grad_norm": 9097.897889073058,
      "learning_rate": 2.8263251205114373e-07,
      "loss": 1.7697,
      "step": 125312
    },
    {
      "epoch": 0.000454898252227377,
      "grad_norm": 9319.522841862668,
      "learning_rate": 2.825963957648146e-07,
      "loss": 1.7392,
      "step": 125344
    },
    {
      "epoch": 0.0004550143865782137,
      "grad_norm": 10407.548030155805,
      "learning_rate": 2.825602933203416e-07,
      "loss": 1.7407,
      "step": 125376
    },
    {
      "epoch": 0.0004551305209290504,
      "grad_norm": 9734.282510796571,
      "learning_rate": 2.825242047088854e-07,
      "loss": 1.7399,
      "step": 125408
    },
    {
      "epoch": 0.0004552466552798871,
      "grad_norm": 9849.931979460569,
      "learning_rate": 2.824881299216145e-07,
      "loss": 1.7155,
      "step": 125440
    },
    {
      "epoch": 0.0004553627896307238,
      "grad_norm": 9312.475503323485,
      "learning_rate": 2.8245206894970523e-07,
      "loss": 1.7492,
      "step": 125472
    },
    {
      "epoch": 0.0004554789239815605,
      "grad_norm": 9810.870297787042,
      "learning_rate": 2.824160217843419e-07,
      "loss": 1.7593,
      "step": 125504
    },
    {
      "epoch": 0.0004555950583323972,
      "grad_norm": 10250.611298844571,
      "learning_rate": 2.823799884167166e-07,
      "loss": 1.7454,
      "step": 125536
    },
    {
      "epoch": 0.0004557111926832339,
      "grad_norm": 9519.881721954323,
      "learning_rate": 2.823439688380295e-07,
      "loss": 1.7681,
      "step": 125568
    },
    {
      "epoch": 0.0004558273270340707,
      "grad_norm": 9027.053229044348,
      "learning_rate": 2.823079630394882e-07,
      "loss": 1.7484,
      "step": 125600
    },
    {
      "epoch": 0.0004559434613849074,
      "grad_norm": 10603.5296010338,
      "learning_rate": 2.822719710123086e-07,
      "loss": 1.7488,
      "step": 125632
    },
    {
      "epoch": 0.0004560595957357441,
      "grad_norm": 9952.516264744309,
      "learning_rate": 2.8223599274771414e-07,
      "loss": 1.7567,
      "step": 125664
    },
    {
      "epoch": 0.0004561757300865808,
      "grad_norm": 11094.035694912829,
      "learning_rate": 2.8220002823693626e-07,
      "loss": 1.7692,
      "step": 125696
    },
    {
      "epoch": 0.0004562918644374175,
      "grad_norm": 8531.061129777467,
      "learning_rate": 2.8216407747121406e-07,
      "loss": 1.7402,
      "step": 125728
    },
    {
      "epoch": 0.0004564079987882542,
      "grad_norm": 10049.567950912118,
      "learning_rate": 2.821281404417946e-07,
      "loss": 1.7477,
      "step": 125760
    },
    {
      "epoch": 0.0004565241331390909,
      "grad_norm": 9407.551222289465,
      "learning_rate": 2.820922171399327e-07,
      "loss": 1.7179,
      "step": 125792
    },
    {
      "epoch": 0.0004566402674899276,
      "grad_norm": 10212.349191052957,
      "learning_rate": 2.820563075568909e-07,
      "loss": 1.7338,
      "step": 125824
    },
    {
      "epoch": 0.0004567564018407643,
      "grad_norm": 10122.447727699067,
      "learning_rate": 2.8202041168393955e-07,
      "loss": 1.7405,
      "step": 125856
    },
    {
      "epoch": 0.00045687253619160103,
      "grad_norm": 10150.171230082771,
      "learning_rate": 2.819845295123569e-07,
      "loss": 1.7649,
      "step": 125888
    },
    {
      "epoch": 0.00045698867054243773,
      "grad_norm": 10989.859416753246,
      "learning_rate": 2.819486610334288e-07,
      "loss": 1.7504,
      "step": 125920
    },
    {
      "epoch": 0.00045710480489327443,
      "grad_norm": 10466.59046681392,
      "learning_rate": 2.8191280623844896e-07,
      "loss": 1.7291,
      "step": 125952
    },
    {
      "epoch": 0.00045722093924411113,
      "grad_norm": 9451.2956783713,
      "learning_rate": 2.818769651187188e-07,
      "loss": 1.739,
      "step": 125984
    },
    {
      "epoch": 0.00045733707359494783,
      "grad_norm": 8708.783267483466,
      "learning_rate": 2.8184113766554744e-07,
      "loss": 1.7484,
      "step": 126016
    },
    {
      "epoch": 0.00045745320794578453,
      "grad_norm": 7454.846879715236,
      "learning_rate": 2.818053238702518e-07,
      "loss": 1.7473,
      "step": 126048
    },
    {
      "epoch": 0.00045756934229662123,
      "grad_norm": 11026.318515261564,
      "learning_rate": 2.8176952372415656e-07,
      "loss": 1.757,
      "step": 126080
    },
    {
      "epoch": 0.00045768547664745793,
      "grad_norm": 9319.323795211754,
      "learning_rate": 2.8173373721859387e-07,
      "loss": 1.7344,
      "step": 126112
    },
    {
      "epoch": 0.00045780161099829463,
      "grad_norm": 9012.647113917197,
      "learning_rate": 2.81697964344904e-07,
      "loss": 1.7332,
      "step": 126144
    },
    {
      "epoch": 0.0004579177453491314,
      "grad_norm": 9383.406311143091,
      "learning_rate": 2.816622050944345e-07,
      "loss": 1.7442,
      "step": 126176
    },
    {
      "epoch": 0.0004580338796999681,
      "grad_norm": 10365.438919794955,
      "learning_rate": 2.8162757630366826e-07,
      "loss": 1.7494,
      "step": 126208
    },
    {
      "epoch": 0.0004581500140508048,
      "grad_norm": 9168.278682500875,
      "learning_rate": 2.815918438486586e-07,
      "loss": 1.7569,
      "step": 126240
    },
    {
      "epoch": 0.0004582661484016415,
      "grad_norm": 10401.56545910278,
      "learning_rate": 2.815561249912282e-07,
      "loss": 1.758,
      "step": 126272
    },
    {
      "epoch": 0.0004583822827524782,
      "grad_norm": 9629.020199376466,
      "learning_rate": 2.8152041972275514e-07,
      "loss": 1.7537,
      "step": 126304
    },
    {
      "epoch": 0.0004584984171033149,
      "grad_norm": 9247.243913729106,
      "learning_rate": 2.814847280346254e-07,
      "loss": 1.746,
      "step": 126336
    },
    {
      "epoch": 0.0004586145514541516,
      "grad_norm": 9918.634785090135,
      "learning_rate": 2.814490499182324e-07,
      "loss": 1.7415,
      "step": 126368
    },
    {
      "epoch": 0.0004587306858049883,
      "grad_norm": 11755.459497612163,
      "learning_rate": 2.8141338536497715e-07,
      "loss": 1.7534,
      "step": 126400
    },
    {
      "epoch": 0.000458846820155825,
      "grad_norm": 9648.298088264064,
      "learning_rate": 2.813777343662685e-07,
      "loss": 1.7553,
      "step": 126432
    },
    {
      "epoch": 0.00045896295450666174,
      "grad_norm": 9617.696813686736,
      "learning_rate": 2.8134209691352274e-07,
      "loss": 1.7753,
      "step": 126464
    },
    {
      "epoch": 0.00045907908885749844,
      "grad_norm": 9120.11776239759,
      "learning_rate": 2.813064729981639e-07,
      "loss": 1.789,
      "step": 126496
    },
    {
      "epoch": 0.00045919522320833514,
      "grad_norm": 9495.91280499142,
      "learning_rate": 2.8127086261162347e-07,
      "loss": 1.7558,
      "step": 126528
    },
    {
      "epoch": 0.00045931135755917184,
      "grad_norm": 9749.729739844075,
      "learning_rate": 2.8123526574534063e-07,
      "loss": 1.7331,
      "step": 126560
    },
    {
      "epoch": 0.00045942749191000854,
      "grad_norm": 8291.099203362604,
      "learning_rate": 2.8119968239076215e-07,
      "loss": 1.7267,
      "step": 126592
    },
    {
      "epoch": 0.00045954362626084524,
      "grad_norm": 8336.701745894476,
      "learning_rate": 2.8116411253934225e-07,
      "loss": 1.73,
      "step": 126624
    },
    {
      "epoch": 0.00045965976061168194,
      "grad_norm": 7995.072982781333,
      "learning_rate": 2.8112855618254283e-07,
      "loss": 1.7301,
      "step": 126656
    },
    {
      "epoch": 0.00045977589496251864,
      "grad_norm": 9879.46233354832,
      "learning_rate": 2.8109301331183334e-07,
      "loss": 1.7328,
      "step": 126688
    },
    {
      "epoch": 0.00045989202931335534,
      "grad_norm": 8975.247517478278,
      "learning_rate": 2.8105748391869075e-07,
      "loss": 1.7212,
      "step": 126720
    },
    {
      "epoch": 0.00046000816366419204,
      "grad_norm": 9048.272321277693,
      "learning_rate": 2.810219679945996e-07,
      "loss": 1.7272,
      "step": 126752
    },
    {
      "epoch": 0.0004601242980150288,
      "grad_norm": 8409.249907096351,
      "learning_rate": 2.809864655310519e-07,
      "loss": 1.7472,
      "step": 126784
    },
    {
      "epoch": 0.0004602404323658655,
      "grad_norm": 9436.580418774589,
      "learning_rate": 2.8095097651954717e-07,
      "loss": 1.7512,
      "step": 126816
    },
    {
      "epoch": 0.0004603565667167022,
      "grad_norm": 8590.15704163783,
      "learning_rate": 2.8091550095159253e-07,
      "loss": 1.7332,
      "step": 126848
    },
    {
      "epoch": 0.0004604727010675389,
      "grad_norm": 9168.697508370531,
      "learning_rate": 2.8088003881870265e-07,
      "loss": 1.7334,
      "step": 126880
    },
    {
      "epoch": 0.0004605888354183756,
      "grad_norm": 9576.649622910927,
      "learning_rate": 2.8084459011239947e-07,
      "loss": 1.743,
      "step": 126912
    },
    {
      "epoch": 0.0004607049697692123,
      "grad_norm": 10228.861324702764,
      "learning_rate": 2.8080915482421265e-07,
      "loss": 1.7318,
      "step": 126944
    },
    {
      "epoch": 0.000460821104120049,
      "grad_norm": 10497.087596090641,
      "learning_rate": 2.8077373294567923e-07,
      "loss": 1.7365,
      "step": 126976
    },
    {
      "epoch": 0.0004609372384708857,
      "grad_norm": 9283.504402971972,
      "learning_rate": 2.8073832446834365e-07,
      "loss": 1.7504,
      "step": 127008
    },
    {
      "epoch": 0.0004610533728217224,
      "grad_norm": 10240.945464164919,
      "learning_rate": 2.8070292938375797e-07,
      "loss": 1.7535,
      "step": 127040
    },
    {
      "epoch": 0.00046116950717255915,
      "grad_norm": 8463.587655362235,
      "learning_rate": 2.806675476834816e-07,
      "loss": 1.7606,
      "step": 127072
    },
    {
      "epoch": 0.00046128564152339585,
      "grad_norm": 9934.705632277184,
      "learning_rate": 2.8063217935908147e-07,
      "loss": 1.7772,
      "step": 127104
    },
    {
      "epoch": 0.00046140177587423255,
      "grad_norm": 10873.182606762382,
      "learning_rate": 2.805968244021318e-07,
      "loss": 1.7397,
      "step": 127136
    },
    {
      "epoch": 0.00046151791022506925,
      "grad_norm": 9667.86512111128,
      "learning_rate": 2.8056148280421437e-07,
      "loss": 1.7241,
      "step": 127168
    },
    {
      "epoch": 0.00046163404457590595,
      "grad_norm": 9202.01499672762,
      "learning_rate": 2.8052725836264534e-07,
      "loss": 1.7215,
      "step": 127200
    },
    {
      "epoch": 0.00046175017892674265,
      "grad_norm": 12004.380033970934,
      "learning_rate": 2.8049194304075017e-07,
      "loss": 1.7322,
      "step": 127232
    },
    {
      "epoch": 0.00046186631327757935,
      "grad_norm": 8402.245890236729,
      "learning_rate": 2.8045664105293924e-07,
      "loss": 1.7336,
      "step": 127264
    },
    {
      "epoch": 0.00046198244762841605,
      "grad_norm": 10187.81350437865,
      "learning_rate": 2.8042135239082375e-07,
      "loss": 1.7178,
      "step": 127296
    },
    {
      "epoch": 0.00046209858197925275,
      "grad_norm": 9455.643394291052,
      "learning_rate": 2.8038607704602215e-07,
      "loss": 1.7279,
      "step": 127328
    },
    {
      "epoch": 0.0004622147163300895,
      "grad_norm": 9202.540953454105,
      "learning_rate": 2.8035081501016036e-07,
      "loss": 1.7238,
      "step": 127360
    },
    {
      "epoch": 0.0004623308506809262,
      "grad_norm": 9169.993020717082,
      "learning_rate": 2.8031556627487174e-07,
      "loss": 1.7522,
      "step": 127392
    },
    {
      "epoch": 0.0004624469850317629,
      "grad_norm": 9459.667647438784,
      "learning_rate": 2.802803308317969e-07,
      "loss": 1.7442,
      "step": 127424
    },
    {
      "epoch": 0.0004625631193825996,
      "grad_norm": 10309.104325788929,
      "learning_rate": 2.802451086725838e-07,
      "loss": 1.7253,
      "step": 127456
    },
    {
      "epoch": 0.0004626792537334363,
      "grad_norm": 9244.926392351645,
      "learning_rate": 2.802098997888877e-07,
      "loss": 1.7185,
      "step": 127488
    },
    {
      "epoch": 0.000462795388084273,
      "grad_norm": 8152.171857854813,
      "learning_rate": 2.8017470417237143e-07,
      "loss": 1.7274,
      "step": 127520
    },
    {
      "epoch": 0.0004629115224351097,
      "grad_norm": 9451.665673308593,
      "learning_rate": 2.80139521814705e-07,
      "loss": 1.7307,
      "step": 127552
    },
    {
      "epoch": 0.0004630276567859464,
      "grad_norm": 9622.775898876582,
      "learning_rate": 2.801043527075656e-07,
      "loss": 1.7268,
      "step": 127584
    },
    {
      "epoch": 0.0004631437911367831,
      "grad_norm": 7877.617660181281,
      "learning_rate": 2.8006919684263805e-07,
      "loss": 1.7357,
      "step": 127616
    },
    {
      "epoch": 0.00046325992548761985,
      "grad_norm": 9185.407339906053,
      "learning_rate": 2.800340542116141e-07,
      "loss": 1.7505,
      "step": 127648
    },
    {
      "epoch": 0.00046337605983845655,
      "grad_norm": 8470.188545717268,
      "learning_rate": 2.7999892480619313e-07,
      "loss": 1.7591,
      "step": 127680
    },
    {
      "epoch": 0.00046349219418929325,
      "grad_norm": 9125.444865868185,
      "learning_rate": 2.799638086180816e-07,
      "loss": 1.7524,
      "step": 127712
    },
    {
      "epoch": 0.00046360832854012995,
      "grad_norm": 9144.381116292125,
      "learning_rate": 2.799287056389933e-07,
      "loss": 1.7421,
      "step": 127744
    },
    {
      "epoch": 0.00046372446289096665,
      "grad_norm": 9038.622793324213,
      "learning_rate": 2.798936158606494e-07,
      "loss": 1.734,
      "step": 127776
    },
    {
      "epoch": 0.00046384059724180335,
      "grad_norm": 9730.718061890397,
      "learning_rate": 2.7985853927477814e-07,
      "loss": 1.7287,
      "step": 127808
    },
    {
      "epoch": 0.00046395673159264005,
      "grad_norm": 8274.493579669997,
      "learning_rate": 2.798234758731152e-07,
      "loss": 1.75,
      "step": 127840
    },
    {
      "epoch": 0.00046407286594347675,
      "grad_norm": 9075.65766212014,
      "learning_rate": 2.7978842564740327e-07,
      "loss": 1.7407,
      "step": 127872
    },
    {
      "epoch": 0.00046418900029431345,
      "grad_norm": 10048.985819474521,
      "learning_rate": 2.797533885893926e-07,
      "loss": 1.727,
      "step": 127904
    },
    {
      "epoch": 0.0004643051346451502,
      "grad_norm": 8643.284676556708,
      "learning_rate": 2.7971836469084035e-07,
      "loss": 1.7347,
      "step": 127936
    },
    {
      "epoch": 0.0004644212689959869,
      "grad_norm": 10803.005692861594,
      "learning_rate": 2.7968335394351123e-07,
      "loss": 1.7394,
      "step": 127968
    },
    {
      "epoch": 0.0004645374033468236,
      "grad_norm": 10892.155525881917,
      "learning_rate": 2.7964835633917683e-07,
      "loss": 1.7395,
      "step": 128000
    },
    {
      "epoch": 0.0004646535376976603,
      "grad_norm": 9318.968827075236,
      "learning_rate": 2.796133718696161e-07,
      "loss": 1.7488,
      "step": 128032
    },
    {
      "epoch": 0.000464769672048497,
      "grad_norm": 9074.402239266232,
      "learning_rate": 2.795784005266153e-07,
      "loss": 1.724,
      "step": 128064
    },
    {
      "epoch": 0.0004648858063993337,
      "grad_norm": 10303.056051482978,
      "learning_rate": 2.7954344230196764e-07,
      "loss": 1.7225,
      "step": 128096
    },
    {
      "epoch": 0.0004650019407501704,
      "grad_norm": 9612.987464883121,
      "learning_rate": 2.795084971874737e-07,
      "loss": 1.7266,
      "step": 128128
    },
    {
      "epoch": 0.0004651180751010071,
      "grad_norm": 9551.720682683304,
      "learning_rate": 2.794735651749412e-07,
      "loss": 1.7348,
      "step": 128160
    },
    {
      "epoch": 0.0004652342094518438,
      "grad_norm": 10634.228321791854,
      "learning_rate": 2.7943864625618493e-07,
      "loss": 1.7293,
      "step": 128192
    },
    {
      "epoch": 0.00046535034380268056,
      "grad_norm": 10880.078216630614,
      "learning_rate": 2.794048310323213e-07,
      "loss": 1.7501,
      "step": 128224
    },
    {
      "epoch": 0.00046546647815351726,
      "grad_norm": 10408.515263955758,
      "learning_rate": 2.7936993786804474e-07,
      "loss": 1.7606,
      "step": 128256
    },
    {
      "epoch": 0.00046558261250435396,
      "grad_norm": 8973.239325906781,
      "learning_rate": 2.7933505777328695e-07,
      "loss": 1.736,
      "step": 128288
    },
    {
      "epoch": 0.00046569874685519066,
      "grad_norm": 8554.732725222922,
      "learning_rate": 2.793001907398911e-07,
      "loss": 1.7331,
      "step": 128320
    },
    {
      "epoch": 0.00046581488120602736,
      "grad_norm": 9704.205583147957,
      "learning_rate": 2.792653367597075e-07,
      "loss": 1.741,
      "step": 128352
    },
    {
      "epoch": 0.00046593101555686406,
      "grad_norm": 9219.878957990717,
      "learning_rate": 2.7923049582459366e-07,
      "loss": 1.7214,
      "step": 128384
    },
    {
      "epoch": 0.00046604714990770076,
      "grad_norm": 10559.566089570159,
      "learning_rate": 2.7919566792641414e-07,
      "loss": 1.7219,
      "step": 128416
    },
    {
      "epoch": 0.00046616328425853746,
      "grad_norm": 9872.58041243524,
      "learning_rate": 2.7916085305704057e-07,
      "loss": 1.7381,
      "step": 128448
    },
    {
      "epoch": 0.00046627941860937416,
      "grad_norm": 8230.592688257633,
      "learning_rate": 2.7912605120835174e-07,
      "loss": 1.7191,
      "step": 128480
    },
    {
      "epoch": 0.0004663955529602109,
      "grad_norm": 8414.45030884371,
      "learning_rate": 2.7909126237223357e-07,
      "loss": 1.725,
      "step": 128512
    },
    {
      "epoch": 0.0004665116873110476,
      "grad_norm": 9251.620506700434,
      "learning_rate": 2.7905648654057883e-07,
      "loss": 1.7324,
      "step": 128544
    },
    {
      "epoch": 0.0004666278216618843,
      "grad_norm": 8447.347867822184,
      "learning_rate": 2.7902172370528757e-07,
      "loss": 1.7296,
      "step": 128576
    },
    {
      "epoch": 0.000466743956012721,
      "grad_norm": 9638.683312569201,
      "learning_rate": 2.7898697385826684e-07,
      "loss": 1.7366,
      "step": 128608
    },
    {
      "epoch": 0.0004668600903635577,
      "grad_norm": 9585.31418368746,
      "learning_rate": 2.789522369914308e-07,
      "loss": 1.7515,
      "step": 128640
    },
    {
      "epoch": 0.0004669762247143944,
      "grad_norm": 9779.992638034038,
      "learning_rate": 2.7891751309670055e-07,
      "loss": 1.7444,
      "step": 128672
    },
    {
      "epoch": 0.0004670923590652311,
      "grad_norm": 8170.091798749877,
      "learning_rate": 2.788828021660043e-07,
      "loss": 1.7371,
      "step": 128704
    },
    {
      "epoch": 0.0004672084934160678,
      "grad_norm": 8698.448942196534,
      "learning_rate": 2.788481041912772e-07,
      "loss": 1.7311,
      "step": 128736
    },
    {
      "epoch": 0.0004673246277669045,
      "grad_norm": 10812.45337562202,
      "learning_rate": 2.788134191644616e-07,
      "loss": 1.7494,
      "step": 128768
    },
    {
      "epoch": 0.00046744076211774127,
      "grad_norm": 10011.51866601666,
      "learning_rate": 2.787787470775067e-07,
      "loss": 1.749,
      "step": 128800
    },
    {
      "epoch": 0.00046755689646857797,
      "grad_norm": 10487.017783907873,
      "learning_rate": 2.7874408792236877e-07,
      "loss": 1.7678,
      "step": 128832
    },
    {
      "epoch": 0.00046767303081941467,
      "grad_norm": 9250.793911875888,
      "learning_rate": 2.7870944169101104e-07,
      "loss": 1.7415,
      "step": 128864
    },
    {
      "epoch": 0.00046778916517025137,
      "grad_norm": 8982.990593338056,
      "learning_rate": 2.786748083754038e-07,
      "loss": 1.721,
      "step": 128896
    },
    {
      "epoch": 0.00046790529952108807,
      "grad_norm": 9838.965596037015,
      "learning_rate": 2.786401879675243e-07,
      "loss": 1.7284,
      "step": 128928
    },
    {
      "epoch": 0.00046802143387192477,
      "grad_norm": 9398.541801790318,
      "learning_rate": 2.786055804593567e-07,
      "loss": 1.741,
      "step": 128960
    },
    {
      "epoch": 0.00046813756822276147,
      "grad_norm": 9834.348580358539,
      "learning_rate": 2.785709858428922e-07,
      "loss": 1.726,
      "step": 128992
    },
    {
      "epoch": 0.00046825370257359817,
      "grad_norm": 9171.082815022444,
      "learning_rate": 2.7853640411012893e-07,
      "loss": 1.7263,
      "step": 129024
    },
    {
      "epoch": 0.00046836983692443487,
      "grad_norm": 9297.135472821723,
      "learning_rate": 2.7850183525307195e-07,
      "loss": 1.7339,
      "step": 129056
    },
    {
      "epoch": 0.0004684859712752716,
      "grad_norm": 9771.074045364716,
      "learning_rate": 2.7846727926373335e-07,
      "loss": 1.7306,
      "step": 129088
    },
    {
      "epoch": 0.0004686021056261083,
      "grad_norm": 8820.383211629753,
      "learning_rate": 2.7843273613413207e-07,
      "loss": 1.7375,
      "step": 129120
    },
    {
      "epoch": 0.000468718239976945,
      "grad_norm": 9009.885459871286,
      "learning_rate": 2.7839820585629404e-07,
      "loss": 1.7222,
      "step": 129152
    },
    {
      "epoch": 0.0004688343743277817,
      "grad_norm": 8391.594127458739,
      "learning_rate": 2.78363688422252e-07,
      "loss": 1.7232,
      "step": 129184
    },
    {
      "epoch": 0.0004689505086786184,
      "grad_norm": 9441.65949396609,
      "learning_rate": 2.783302618985263e-07,
      "loss": 1.726,
      "step": 129216
    },
    {
      "epoch": 0.0004690666430294551,
      "grad_norm": 11227.595379243054,
      "learning_rate": 2.7829576972745143e-07,
      "loss": 1.739,
      "step": 129248
    },
    {
      "epoch": 0.0004691827773802918,
      "grad_norm": 9622.180314253106,
      "learning_rate": 2.7826129037656075e-07,
      "loss": 1.7288,
      "step": 129280
    },
    {
      "epoch": 0.0004692989117311285,
      "grad_norm": 9758.267879085919,
      "learning_rate": 2.782268238379144e-07,
      "loss": 1.7218,
      "step": 129312
    },
    {
      "epoch": 0.0004694150460819652,
      "grad_norm": 9465.097146886554,
      "learning_rate": 2.7819237010357953e-07,
      "loss": 1.7296,
      "step": 129344
    },
    {
      "epoch": 0.000469531180432802,
      "grad_norm": 10898.686985137247,
      "learning_rate": 2.7815792916563004e-07,
      "loss": 1.7527,
      "step": 129376
    },
    {
      "epoch": 0.0004696473147836387,
      "grad_norm": 10048.094545733535,
      "learning_rate": 2.7812350101614683e-07,
      "loss": 1.7516,
      "step": 129408
    },
    {
      "epoch": 0.0004697634491344754,
      "grad_norm": 9612.465032446153,
      "learning_rate": 2.7808908564721754e-07,
      "loss": 1.7489,
      "step": 129440
    },
    {
      "epoch": 0.0004698795834853121,
      "grad_norm": 8693.422571116625,
      "learning_rate": 2.780546830509367e-07,
      "loss": 1.738,
      "step": 129472
    },
    {
      "epoch": 0.0004699957178361488,
      "grad_norm": 9471.051156022757,
      "learning_rate": 2.7802029321940583e-07,
      "loss": 1.7297,
      "step": 129504
    },
    {
      "epoch": 0.0004701118521869855,
      "grad_norm": 9785.694354515677,
      "learning_rate": 2.7798591614473297e-07,
      "loss": 1.7461,
      "step": 129536
    },
    {
      "epoch": 0.0004702279865378222,
      "grad_norm": 10239.731344132031,
      "learning_rate": 2.779515518190333e-07,
      "loss": 1.7451,
      "step": 129568
    },
    {
      "epoch": 0.0004703441208886589,
      "grad_norm": 9100.443176021705,
      "learning_rate": 2.7791720023442865e-07,
      "loss": 1.7377,
      "step": 129600
    },
    {
      "epoch": 0.0004704602552394956,
      "grad_norm": 10390.581215697224,
      "learning_rate": 2.7788286138304773e-07,
      "loss": 1.7419,
      "step": 129632
    },
    {
      "epoch": 0.00047057638959033233,
      "grad_norm": 8910.63443308051,
      "learning_rate": 2.77848535257026e-07,
      "loss": 1.735,
      "step": 129664
    },
    {
      "epoch": 0.00047069252394116903,
      "grad_norm": 9319.497947851054,
      "learning_rate": 2.7781422184850576e-07,
      "loss": 1.7478,
      "step": 129696
    },
    {
      "epoch": 0.00047080865829200573,
      "grad_norm": 8945.83478497116,
      "learning_rate": 2.777799211496361e-07,
      "loss": 1.7156,
      "step": 129728
    },
    {
      "epoch": 0.00047092479264284243,
      "grad_norm": 9534.804140620823,
      "learning_rate": 2.777456331525729e-07,
      "loss": 1.726,
      "step": 129760
    },
    {
      "epoch": 0.00047104092699367913,
      "grad_norm": 10300.179804255846,
      "learning_rate": 2.777113578494789e-07,
      "loss": 1.7242,
      "step": 129792
    },
    {
      "epoch": 0.00047115706134451583,
      "grad_norm": 10888.639584447637,
      "learning_rate": 2.7767709523252323e-07,
      "loss": 1.7254,
      "step": 129824
    },
    {
      "epoch": 0.0004712731956953525,
      "grad_norm": 10137.574857923368,
      "learning_rate": 2.7764284529388236e-07,
      "loss": 1.7524,
      "step": 129856
    },
    {
      "epoch": 0.0004713893300461892,
      "grad_norm": 11446.533449040367,
      "learning_rate": 2.776086080257391e-07,
      "loss": 1.7467,
      "step": 129888
    },
    {
      "epoch": 0.0004715054643970259,
      "grad_norm": 10531.114850764852,
      "learning_rate": 2.7757438342028307e-07,
      "loss": 1.7648,
      "step": 129920
    },
    {
      "epoch": 0.0004716215987478627,
      "grad_norm": 10618.324161561466,
      "learning_rate": 2.7754017146971076e-07,
      "loss": 1.7682,
      "step": 129952
    },
    {
      "epoch": 0.0004717377330986994,
      "grad_norm": 9097.231886678497,
      "learning_rate": 2.7750597216622535e-07,
      "loss": 1.7933,
      "step": 129984
    },
    {
      "epoch": 0.0004718538674495361,
      "grad_norm": 9502.640264684336,
      "learning_rate": 2.7747178550203656e-07,
      "loss": 1.7713,
      "step": 130016
    },
    {
      "epoch": 0.0004719700018003728,
      "grad_norm": 11098.88462864625,
      "learning_rate": 2.774376114693611e-07,
      "loss": 1.7697,
      "step": 130048
    },
    {
      "epoch": 0.0004720861361512095,
      "grad_norm": 8067.071091790377,
      "learning_rate": 2.774034500604222e-07,
      "loss": 1.7454,
      "step": 130080
    },
    {
      "epoch": 0.0004722022705020462,
      "grad_norm": 10338.490798951267,
      "learning_rate": 2.773693012674499e-07,
      "loss": 1.7574,
      "step": 130112
    },
    {
      "epoch": 0.0004723184048528829,
      "grad_norm": 10866.104177671039,
      "learning_rate": 2.7733516508268085e-07,
      "loss": 1.7599,
      "step": 130144
    },
    {
      "epoch": 0.0004724345392037196,
      "grad_norm": 10004.142541967303,
      "learning_rate": 2.773010414983585e-07,
      "loss": 1.7745,
      "step": 130176
    },
    {
      "epoch": 0.0004725506735545563,
      "grad_norm": 8942.78390659195,
      "learning_rate": 2.7726799628468584e-07,
      "loss": 1.7839,
      "step": 130208
    },
    {
      "epoch": 0.00047266680790539303,
      "grad_norm": 10142.964655365808,
      "learning_rate": 2.772338974848509e-07,
      "loss": 1.7688,
      "step": 130240
    },
    {
      "epoch": 0.00047278294225622973,
      "grad_norm": 10949.472316052495,
      "learning_rate": 2.7719981126247436e-07,
      "loss": 1.7638,
      "step": 130272
    },
    {
      "epoch": 0.00047289907660706643,
      "grad_norm": 10865.743784941738,
      "learning_rate": 2.7716573760982617e-07,
      "loss": 1.7377,
      "step": 130304
    },
    {
      "epoch": 0.00047301521095790313,
      "grad_norm": 8625.290024109334,
      "learning_rate": 2.7713167651918274e-07,
      "loss": 1.7378,
      "step": 130336
    },
    {
      "epoch": 0.00047313134530873983,
      "grad_norm": 9940.946635004135,
      "learning_rate": 2.7709762798282724e-07,
      "loss": 1.7468,
      "step": 130368
    },
    {
      "epoch": 0.00047324747965957653,
      "grad_norm": 9072.200394612104,
      "learning_rate": 2.7706359199304943e-07,
      "loss": 1.7559,
      "step": 130400
    },
    {
      "epoch": 0.00047336361401041323,
      "grad_norm": 10963.540486539921,
      "learning_rate": 2.7702956854214567e-07,
      "loss": 1.768,
      "step": 130432
    },
    {
      "epoch": 0.00047347974836124993,
      "grad_norm": 9433.280871467783,
      "learning_rate": 2.76995557622419e-07,
      "loss": 1.786,
      "step": 130464
    },
    {
      "epoch": 0.00047359588271208663,
      "grad_norm": 10493.829615540744,
      "learning_rate": 2.7696155922617903e-07,
      "loss": 1.8026,
      "step": 130496
    },
    {
      "epoch": 0.0004737120170629234,
      "grad_norm": 10375.420184262419,
      "learning_rate": 2.7692757334574203e-07,
      "loss": 1.7652,
      "step": 130528
    },
    {
      "epoch": 0.0004738281514137601,
      "grad_norm": 10763.636931818166,
      "learning_rate": 2.7689359997343077e-07,
      "loss": 1.7582,
      "step": 130560
    },
    {
      "epoch": 0.0004739442857645968,
      "grad_norm": 8931.563133069149,
      "learning_rate": 2.768596391015748e-07,
      "loss": 1.7702,
      "step": 130592
    },
    {
      "epoch": 0.0004740604201154335,
      "grad_norm": 10148.063460582023,
      "learning_rate": 2.768256907225099e-07,
      "loss": 1.7732,
      "step": 130624
    },
    {
      "epoch": 0.0004741765544662702,
      "grad_norm": 9572.05620543465,
      "learning_rate": 2.7679175482857884e-07,
      "loss": 1.747,
      "step": 130656
    },
    {
      "epoch": 0.0004742926888171069,
      "grad_norm": 9972.9857114106,
      "learning_rate": 2.7675783141213073e-07,
      "loss": 1.7456,
      "step": 130688
    },
    {
      "epoch": 0.0004744088231679436,
      "grad_norm": 9375.58552838168,
      "learning_rate": 2.767239204655212e-07,
      "loss": 1.7574,
      "step": 130720
    },
    {
      "epoch": 0.0004745249575187803,
      "grad_norm": 10865.022135274277,
      "learning_rate": 2.766900219811126e-07,
      "loss": 1.7527,
      "step": 130752
    },
    {
      "epoch": 0.000474641091869617,
      "grad_norm": 10666.340422094168,
      "learning_rate": 2.766561359512737e-07,
      "loss": 1.7747,
      "step": 130784
    },
    {
      "epoch": 0.00047475722622045374,
      "grad_norm": 10520.9660202854,
      "learning_rate": 2.766222623683799e-07,
      "loss": 1.7829,
      "step": 130816
    },
    {
      "epoch": 0.00047487336057129044,
      "grad_norm": 8748.145060525689,
      "learning_rate": 2.765884012248131e-07,
      "loss": 1.7669,
      "step": 130848
    },
    {
      "epoch": 0.00047498949492212714,
      "grad_norm": 9390.31330680718,
      "learning_rate": 2.765545525129617e-07,
      "loss": 1.7497,
      "step": 130880
    },
    {
      "epoch": 0.00047510562927296384,
      "grad_norm": 10718.649728393963,
      "learning_rate": 2.765207162252206e-07,
      "loss": 1.7655,
      "step": 130912
    },
    {
      "epoch": 0.00047522176362380054,
      "grad_norm": 9601.664230746668,
      "learning_rate": 2.7648689235399123e-07,
      "loss": 1.75,
      "step": 130944
    },
    {
      "epoch": 0.00047533789797463724,
      "grad_norm": 8858.633528936616,
      "learning_rate": 2.7645308089168166e-07,
      "loss": 1.7286,
      "step": 130976
    },
    {
      "epoch": 0.00047545403232547394,
      "grad_norm": 9200.778445327329,
      "learning_rate": 2.7641928183070614e-07,
      "loss": 1.7298,
      "step": 131008
    },
    {
      "epoch": 0.00047557016667631064,
      "grad_norm": 10420.03838764522,
      "learning_rate": 2.7638549516348585e-07,
      "loss": 1.7324,
      "step": 131040
    },
    {
      "epoch": 0.00047568630102714734,
      "grad_norm": 9952.423825380429,
      "learning_rate": 2.76351720882448e-07,
      "loss": 1.7657,
      "step": 131072
    },
    {
      "epoch": 0.0004758024353779841,
      "grad_norm": 9034.400256796242,
      "learning_rate": 2.763179589800266e-07,
      "loss": 1.7872,
      "step": 131104
    },
    {
      "epoch": 0.0004759185697288208,
      "grad_norm": 9950.714848693031,
      "learning_rate": 2.76284209448662e-07,
      "loss": 1.7744,
      "step": 131136
    },
    {
      "epoch": 0.0004760347040796575,
      "grad_norm": 9630.094599743037,
      "learning_rate": 2.7625047228080097e-07,
      "loss": 1.7995,
      "step": 131168
    },
    {
      "epoch": 0.0004761508384304942,
      "grad_norm": 10070.73324043488,
      "learning_rate": 2.7621674746889687e-07,
      "loss": 1.7862,
      "step": 131200
    },
    {
      "epoch": 0.0004762669727813309,
      "grad_norm": 7878.087204391685,
      "learning_rate": 2.7618408833305376e-07,
      "loss": 1.7411,
      "step": 131232
    },
    {
      "epoch": 0.0004763831071321676,
      "grad_norm": 8827.537482220056,
      "learning_rate": 2.761503878249105e-07,
      "loss": 1.7459,
      "step": 131264
    },
    {
      "epoch": 0.0004764992414830043,
      "grad_norm": 9307.423273924958,
      "learning_rate": 2.761166996503577e-07,
      "loss": 1.7614,
      "step": 131296
    },
    {
      "epoch": 0.000476615375833841,
      "grad_norm": 10108.991047577398,
      "learning_rate": 2.7608302380187437e-07,
      "loss": 1.7582,
      "step": 131328
    },
    {
      "epoch": 0.0004767315101846777,
      "grad_norm": 9274.59012571445,
      "learning_rate": 2.7604936027194564e-07,
      "loss": 1.7664,
      "step": 131360
    },
    {
      "epoch": 0.00047684764453551445,
      "grad_norm": 8633.417052361134,
      "learning_rate": 2.760157090530632e-07,
      "loss": 1.8058,
      "step": 131392
    },
    {
      "epoch": 0.00047696377888635115,
      "grad_norm": 10426.893305294727,
      "learning_rate": 2.759820701377251e-07,
      "loss": 1.7702,
      "step": 131424
    },
    {
      "epoch": 0.00047707991323718785,
      "grad_norm": 10795.911633576852,
      "learning_rate": 2.759484435184358e-07,
      "loss": 1.7411,
      "step": 131456
    },
    {
      "epoch": 0.00047719604758802455,
      "grad_norm": 8697.051914298316,
      "learning_rate": 2.7591482918770614e-07,
      "loss": 1.749,
      "step": 131488
    },
    {
      "epoch": 0.00047731218193886125,
      "grad_norm": 10470.62672431789,
      "learning_rate": 2.7588122713805346e-07,
      "loss": 1.7654,
      "step": 131520
    },
    {
      "epoch": 0.00047742831628969795,
      "grad_norm": 8505.696914421535,
      "learning_rate": 2.7584763736200126e-07,
      "loss": 1.7533,
      "step": 131552
    },
    {
      "epoch": 0.00047754445064053465,
      "grad_norm": 9718.429605651316,
      "learning_rate": 2.758140598520796e-07,
      "loss": 1.7566,
      "step": 131584
    },
    {
      "epoch": 0.00047766058499137135,
      "grad_norm": 9264.593677004945,
      "learning_rate": 2.757804946008248e-07,
      "loss": 1.755,
      "step": 131616
    },
    {
      "epoch": 0.00047777671934220805,
      "grad_norm": 10140.4098536499,
      "learning_rate": 2.7574694160077965e-07,
      "loss": 1.743,
      "step": 131648
    },
    {
      "epoch": 0.0004778928536930448,
      "grad_norm": 9550.134658736493,
      "learning_rate": 2.757134008444931e-07,
      "loss": 1.7621,
      "step": 131680
    },
    {
      "epoch": 0.0004780089880438815,
      "grad_norm": 11871.821090296131,
      "learning_rate": 2.756798723245206e-07,
      "loss": 1.757,
      "step": 131712
    },
    {
      "epoch": 0.0004781251223947182,
      "grad_norm": 8278.364331194902,
      "learning_rate": 2.7564635603342384e-07,
      "loss": 1.7657,
      "step": 131744
    },
    {
      "epoch": 0.0004782412567455549,
      "grad_norm": 9467.983840290392,
      "learning_rate": 2.7561285196377096e-07,
      "loss": 1.7604,
      "step": 131776
    },
    {
      "epoch": 0.0004783573910963916,
      "grad_norm": 12209.857329223794,
      "learning_rate": 2.7557936010813635e-07,
      "loss": 1.7646,
      "step": 131808
    },
    {
      "epoch": 0.0004784735254472283,
      "grad_norm": 10139.613602105359,
      "learning_rate": 2.755458804591007e-07,
      "loss": 1.7652,
      "step": 131840
    },
    {
      "epoch": 0.000478589659798065,
      "grad_norm": 10011.529153930484,
      "learning_rate": 2.755124130092509e-07,
      "loss": 1.7576,
      "step": 131872
    },
    {
      "epoch": 0.0004787057941489017,
      "grad_norm": 10092.61829259385,
      "learning_rate": 2.7547895775118043e-07,
      "loss": 1.7249,
      "step": 131904
    },
    {
      "epoch": 0.0004788219284997384,
      "grad_norm": 10694.239944942323,
      "learning_rate": 2.7544551467748885e-07,
      "loss": 1.7417,
      "step": 131936
    },
    {
      "epoch": 0.0004789380628505751,
      "grad_norm": 9052.537323866718,
      "learning_rate": 2.75412083780782e-07,
      "loss": 1.7703,
      "step": 131968
    },
    {
      "epoch": 0.00047905419720141185,
      "grad_norm": 10869.996872124664,
      "learning_rate": 2.753786650536721e-07,
      "loss": 1.7742,
      "step": 132000
    },
    {
      "epoch": 0.00047917033155224855,
      "grad_norm": 11320.917188991358,
      "learning_rate": 2.753452584887775e-07,
      "loss": 1.771,
      "step": 132032
    },
    {
      "epoch": 0.00047928646590308525,
      "grad_norm": 10366.16438225827,
      "learning_rate": 2.7531186407872306e-07,
      "loss": 1.7564,
      "step": 132064
    },
    {
      "epoch": 0.00047940260025392195,
      "grad_norm": 9370.64330769238,
      "learning_rate": 2.752784818161397e-07,
      "loss": 1.761,
      "step": 132096
    },
    {
      "epoch": 0.00047951873460475865,
      "grad_norm": 10176.123820001405,
      "learning_rate": 2.7524511169366464e-07,
      "loss": 1.7622,
      "step": 132128
    },
    {
      "epoch": 0.00047963486895559535,
      "grad_norm": 9575.133837184732,
      "learning_rate": 2.7521175370394134e-07,
      "loss": 1.7616,
      "step": 132160
    },
    {
      "epoch": 0.00047975100330643205,
      "grad_norm": 8857.932490146897,
      "learning_rate": 2.751784078396195e-07,
      "loss": 1.766,
      "step": 132192
    },
    {
      "epoch": 0.00047986713765726875,
      "grad_norm": 9367.726084808415,
      "learning_rate": 2.7514507409335515e-07,
      "loss": 1.7662,
      "step": 132224
    },
    {
      "epoch": 0.00047998327200810545,
      "grad_norm": 9863.351965736598,
      "learning_rate": 2.751127935756774e-07,
      "loss": 1.7852,
      "step": 132256
    },
    {
      "epoch": 0.0004800994063589422,
      "grad_norm": 9771.870854652143,
      "learning_rate": 2.7507948366540063e-07,
      "loss": 1.7765,
      "step": 132288
    },
    {
      "epoch": 0.0004802155407097789,
      "grad_norm": 9227.181259734742,
      "learning_rate": 2.7504618585141523e-07,
      "loss": 1.7827,
      "step": 132320
    },
    {
      "epoch": 0.0004803316750606156,
      "grad_norm": 10683.657987786768,
      "learning_rate": 2.7501290012640194e-07,
      "loss": 1.7473,
      "step": 132352
    },
    {
      "epoch": 0.0004804478094114523,
      "grad_norm": 10250.593934011824,
      "learning_rate": 2.7497962648304744e-07,
      "loss": 1.7446,
      "step": 132384
    },
    {
      "epoch": 0.000480563943762289,
      "grad_norm": 11124.236603021352,
      "learning_rate": 2.749463649140448e-07,
      "loss": 1.7438,
      "step": 132416
    },
    {
      "epoch": 0.0004806800781131257,
      "grad_norm": 10380.735041412048,
      "learning_rate": 2.749131154120932e-07,
      "loss": 1.7488,
      "step": 132448
    },
    {
      "epoch": 0.0004807962124639624,
      "grad_norm": 9843.028802152312,
      "learning_rate": 2.7487987796989805e-07,
      "loss": 1.7598,
      "step": 132480
    },
    {
      "epoch": 0.0004809123468147991,
      "grad_norm": 10977.393497547586,
      "learning_rate": 2.748466525801708e-07,
      "loss": 1.7646,
      "step": 132512
    },
    {
      "epoch": 0.0004810284811656358,
      "grad_norm": 8128.472427215337,
      "learning_rate": 2.7481343923562926e-07,
      "loss": 1.7905,
      "step": 132544
    },
    {
      "epoch": 0.00048114461551647256,
      "grad_norm": 8256.803376610103,
      "learning_rate": 2.747802379289972e-07,
      "loss": 1.7412,
      "step": 132576
    },
    {
      "epoch": 0.00048126074986730926,
      "grad_norm": 9973.933125903743,
      "learning_rate": 2.747470486530047e-07,
      "loss": 1.7401,
      "step": 132608
    },
    {
      "epoch": 0.00048137688421814596,
      "grad_norm": 9303.445598271643,
      "learning_rate": 2.7471387140038783e-07,
      "loss": 1.7617,
      "step": 132640
    },
    {
      "epoch": 0.00048149301856898266,
      "grad_norm": 10154.669861694176,
      "learning_rate": 2.7468070616388906e-07,
      "loss": 1.7455,
      "step": 132672
    },
    {
      "epoch": 0.00048160915291981936,
      "grad_norm": 10716.541233065826,
      "learning_rate": 2.746475529362567e-07,
      "loss": 1.742,
      "step": 132704
    },
    {
      "epoch": 0.00048172528727065606,
      "grad_norm": 8723.078355718239,
      "learning_rate": 2.7461441171024534e-07,
      "loss": 1.7601,
      "step": 132736
    },
    {
      "epoch": 0.00048184142162149276,
      "grad_norm": 9735.193783382025,
      "learning_rate": 2.745812824786156e-07,
      "loss": 1.7443,
      "step": 132768
    },
    {
      "epoch": 0.00048195755597232946,
      "grad_norm": 10137.040791079022,
      "learning_rate": 2.745481652341344e-07,
      "loss": 1.7515,
      "step": 132800
    },
    {
      "epoch": 0.00048207369032316616,
      "grad_norm": 10174.479544428796,
      "learning_rate": 2.7451505996957455e-07,
      "loss": 1.7635,
      "step": 132832
    },
    {
      "epoch": 0.0004821898246740029,
      "grad_norm": 8639.943171109402,
      "learning_rate": 2.7448196667771497e-07,
      "loss": 1.769,
      "step": 132864
    },
    {
      "epoch": 0.0004823059590248396,
      "grad_norm": 9750.956773568429,
      "learning_rate": 2.744488853513409e-07,
      "loss": 1.7788,
      "step": 132896
    },
    {
      "epoch": 0.0004824220933756763,
      "grad_norm": 11487.960045195143,
      "learning_rate": 2.744158159832434e-07,
      "loss": 1.8022,
      "step": 132928
    },
    {
      "epoch": 0.000482538227726513,
      "grad_norm": 9266.992931906228,
      "learning_rate": 2.7438275856621977e-07,
      "loss": 1.7963,
      "step": 132960
    },
    {
      "epoch": 0.0004826543620773497,
      "grad_norm": 10391.506243081414,
      "learning_rate": 2.7434971309307334e-07,
      "loss": 1.7696,
      "step": 132992
    },
    {
      "epoch": 0.0004827704964281864,
      "grad_norm": 10526.563731816761,
      "learning_rate": 2.7431667955661347e-07,
      "loss": 1.7264,
      "step": 133024
    },
    {
      "epoch": 0.0004828866307790231,
      "grad_norm": 11232.004540597372,
      "learning_rate": 2.7428365794965556e-07,
      "loss": 1.7471,
      "step": 133056
    },
    {
      "epoch": 0.0004830027651298598,
      "grad_norm": 9312.580308378554,
      "learning_rate": 2.742506482650212e-07,
      "loss": 1.7366,
      "step": 133088
    },
    {
      "epoch": 0.0004831188994806965,
      "grad_norm": 9738.381487701126,
      "learning_rate": 2.7421765049553786e-07,
      "loss": 1.7705,
      "step": 133120
    },
    {
      "epoch": 0.00048323503383153327,
      "grad_norm": 9897.678111557276,
      "learning_rate": 2.741846646340392e-07,
      "loss": 1.766,
      "step": 133152
    },
    {
      "epoch": 0.00048335116818236997,
      "grad_norm": 9113.325188974659,
      "learning_rate": 2.7415169067336477e-07,
      "loss": 1.7549,
      "step": 133184
    },
    {
      "epoch": 0.00048346730253320667,
      "grad_norm": 8702.75232325958,
      "learning_rate": 2.7411872860636025e-07,
      "loss": 1.7812,
      "step": 133216
    },
    {
      "epoch": 0.00048358343688404337,
      "grad_norm": 10606.876260237977,
      "learning_rate": 2.740868079391654e-07,
      "loss": 1.7616,
      "step": 133248
    },
    {
      "epoch": 0.00048369957123488007,
      "grad_norm": 9403.686511150827,
      "learning_rate": 2.7405386926693915e-07,
      "loss": 1.7306,
      "step": 133280
    },
    {
      "epoch": 0.00048381570558571677,
      "grad_norm": 10505.433260936932,
      "learning_rate": 2.740209424671787e-07,
      "loss": 1.7374,
      "step": 133312
    },
    {
      "epoch": 0.00048393183993655347,
      "grad_norm": 9337.188549022667,
      "learning_rate": 2.7398802753275357e-07,
      "loss": 1.7499,
      "step": 133344
    },
    {
      "epoch": 0.00048404797428739017,
      "grad_norm": 9615.520162736906,
      "learning_rate": 2.739551244565393e-07,
      "loss": 1.7604,
      "step": 133376
    },
    {
      "epoch": 0.00048416410863822687,
      "grad_norm": 9303.334455989423,
      "learning_rate": 2.739222332314174e-07,
      "loss": 1.7652,
      "step": 133408
    },
    {
      "epoch": 0.0004842802429890636,
      "grad_norm": 10882.86102088968,
      "learning_rate": 2.7388935385027524e-07,
      "loss": 1.753,
      "step": 133440
    },
    {
      "epoch": 0.0004843963773399003,
      "grad_norm": 10346.599248062139,
      "learning_rate": 2.7385648630600627e-07,
      "loss": 1.7531,
      "step": 133472
    },
    {
      "epoch": 0.000484512511690737,
      "grad_norm": 9366.985107279716,
      "learning_rate": 2.738236305915099e-07,
      "loss": 1.7569,
      "step": 133504
    },
    {
      "epoch": 0.0004846286460415737,
      "grad_norm": 10309.809697564742,
      "learning_rate": 2.737907866996915e-07,
      "loss": 1.7878,
      "step": 133536
    },
    {
      "epoch": 0.0004847447803924104,
      "grad_norm": 9211.80774875377,
      "learning_rate": 2.737579546234624e-07,
      "loss": 1.7844,
      "step": 133568
    },
    {
      "epoch": 0.0004848609147432471,
      "grad_norm": 9910.126538041783,
      "learning_rate": 2.7372513435573984e-07,
      "loss": 1.7461,
      "step": 133600
    },
    {
      "epoch": 0.0004849770490940838,
      "grad_norm": 11079.205386669208,
      "learning_rate": 2.736923258894471e-07,
      "loss": 1.7513,
      "step": 133632
    },
    {
      "epoch": 0.0004850931834449205,
      "grad_norm": 10681.518244144883,
      "learning_rate": 2.736595292175132e-07,
      "loss": 1.7686,
      "step": 133664
    },
    {
      "epoch": 0.0004852093177957572,
      "grad_norm": 9671.074914403258,
      "learning_rate": 2.7362674433287335e-07,
      "loss": 1.7406,
      "step": 133696
    },
    {
      "epoch": 0.000485325452146594,
      "grad_norm": 9379.644342937529,
      "learning_rate": 2.735939712284685e-07,
      "loss": 1.7468,
      "step": 133728
    },
    {
      "epoch": 0.0004854415864974307,
      "grad_norm": 9756.121155459276,
      "learning_rate": 2.735612098972455e-07,
      "loss": 1.7386,
      "step": 133760
    },
    {
      "epoch": 0.0004855577208482674,
      "grad_norm": 8809.325059276676,
      "learning_rate": 2.735284603321573e-07,
      "loss": 1.7499,
      "step": 133792
    },
    {
      "epoch": 0.0004856738551991041,
      "grad_norm": 10697.003505655219,
      "learning_rate": 2.7349572252616257e-07,
      "loss": 1.7595,
      "step": 133824
    },
    {
      "epoch": 0.0004857899895499408,
      "grad_norm": 9101.811907526984,
      "learning_rate": 2.7346299647222593e-07,
      "loss": 1.7799,
      "step": 133856
    },
    {
      "epoch": 0.0004859061239007775,
      "grad_norm": 9246.168990452206,
      "learning_rate": 2.7343028216331796e-07,
      "loss": 1.7803,
      "step": 133888
    },
    {
      "epoch": 0.0004860222582516142,
      "grad_norm": 9477.366089795201,
      "learning_rate": 2.7339757959241495e-07,
      "loss": 1.7723,
      "step": 133920
    },
    {
      "epoch": 0.0004861383926024509,
      "grad_norm": 9359.575204035704,
      "learning_rate": 2.733648887524993e-07,
      "loss": 1.7342,
      "step": 133952
    },
    {
      "epoch": 0.0004862545269532876,
      "grad_norm": 11532.525655726937,
      "learning_rate": 2.733322096365591e-07,
      "loss": 1.7627,
      "step": 133984
    },
    {
      "epoch": 0.00048637066130412433,
      "grad_norm": 9419.32874466116,
      "learning_rate": 2.732995422375884e-07,
      "loss": 1.7475,
      "step": 134016
    },
    {
      "epoch": 0.00048648679565496103,
      "grad_norm": 10237.6653588599,
      "learning_rate": 2.732668865485871e-07,
      "loss": 1.7588,
      "step": 134048
    },
    {
      "epoch": 0.00048660293000579773,
      "grad_norm": 10007.458818301477,
      "learning_rate": 2.732342425625609e-07,
      "loss": 1.7543,
      "step": 134080
    },
    {
      "epoch": 0.00048671906435663443,
      "grad_norm": 10497.169713784759,
      "learning_rate": 2.732016102725214e-07,
      "loss": 1.7725,
      "step": 134112
    },
    {
      "epoch": 0.00048683519870747113,
      "grad_norm": 10164.53540502467,
      "learning_rate": 2.7316898967148605e-07,
      "loss": 1.7914,
      "step": 134144
    },
    {
      "epoch": 0.00048695133305830783,
      "grad_norm": 10107.200304733256,
      "learning_rate": 2.7313638075247804e-07,
      "loss": 1.7579,
      "step": 134176
    },
    {
      "epoch": 0.00048706746740914453,
      "grad_norm": 9411.145413816535,
      "learning_rate": 2.7310378350852654e-07,
      "loss": 1.7498,
      "step": 134208
    },
    {
      "epoch": 0.00048718360175998123,
      "grad_norm": 9065.277160682954,
      "learning_rate": 2.730722160553646e-07,
      "loss": 1.7612,
      "step": 134240
    },
    {
      "epoch": 0.00048729973611081793,
      "grad_norm": 10900.5509952479,
      "learning_rate": 2.730396417763315e-07,
      "loss": 1.7774,
      "step": 134272
    },
    {
      "epoch": 0.0004874158704616547,
      "grad_norm": 9203.57962968757,
      "learning_rate": 2.730070791516943e-07,
      "loss": 1.7578,
      "step": 134304
    },
    {
      "epoch": 0.0004875320048124914,
      "grad_norm": 10050.259300137484,
      "learning_rate": 2.7297452817450504e-07,
      "loss": 1.7599,
      "step": 134336
    },
    {
      "epoch": 0.0004876481391633281,
      "grad_norm": 9611.435064546813,
      "learning_rate": 2.729419888378218e-07,
      "loss": 1.7434,
      "step": 134368
    },
    {
      "epoch": 0.0004877642735141648,
      "grad_norm": 9219.404319152078,
      "learning_rate": 2.729094611347083e-07,
      "loss": 1.7183,
      "step": 134400
    },
    {
      "epoch": 0.0004878804078650015,
      "grad_norm": 10415.40762524444,
      "learning_rate": 2.7287694505823407e-07,
      "loss": 1.7258,
      "step": 134432
    },
    {
      "epoch": 0.0004879965422158382,
      "grad_norm": 11630.950348101396,
      "learning_rate": 2.7284444060147435e-07,
      "loss": 1.7508,
      "step": 134464
    },
    {
      "epoch": 0.0004881126765666749,
      "grad_norm": 9764.879517945934,
      "learning_rate": 2.728119477575103e-07,
      "loss": 1.757,
      "step": 134496
    },
    {
      "epoch": 0.0004882288109175116,
      "grad_norm": 9856.48618930702,
      "learning_rate": 2.727794665194287e-07,
      "loss": 1.7578,
      "step": 134528
    },
    {
      "epoch": 0.0004883449452683483,
      "grad_norm": 9295.65586712417,
      "learning_rate": 2.7274699688032225e-07,
      "loss": 1.7697,
      "step": 134560
    },
    {
      "epoch": 0.000488461079619185,
      "grad_norm": 8287.17068727319,
      "learning_rate": 2.727145388332893e-07,
      "loss": 1.785,
      "step": 134592
    },
    {
      "epoch": 0.0004885772139700217,
      "grad_norm": 9097.98263352926,
      "learning_rate": 2.726820923714339e-07,
      "loss": 1.7434,
      "step": 134624
    },
    {
      "epoch": 0.0004886933483208584,
      "grad_norm": 9592.031693025207,
      "learning_rate": 2.726496574878659e-07,
      "loss": 1.7425,
      "step": 134656
    },
    {
      "epoch": 0.0004888094826716951,
      "grad_norm": 9547.509308715022,
      "learning_rate": 2.72617234175701e-07,
      "loss": 1.7749,
      "step": 134688
    },
    {
      "epoch": 0.0004889256170225318,
      "grad_norm": 9780.879306074685,
      "learning_rate": 2.725848224280604e-07,
      "loss": 1.7772,
      "step": 134720
    },
    {
      "epoch": 0.0004890417513733685,
      "grad_norm": 9160.667006282894,
      "learning_rate": 2.7255242223807125e-07,
      "loss": 1.7801,
      "step": 134752
    },
    {
      "epoch": 0.0004891578857242053,
      "grad_norm": 11049.431840597053,
      "learning_rate": 2.7252003359886624e-07,
      "loss": 1.7761,
      "step": 134784
    },
    {
      "epoch": 0.0004892740200750419,
      "grad_norm": 10199.830488787546,
      "learning_rate": 2.724876565035838e-07,
      "loss": 1.7619,
      "step": 134816
    },
    {
      "epoch": 0.0004893901544258787,
      "grad_norm": 10753.370634363906,
      "learning_rate": 2.724552909453682e-07,
      "loss": 1.7673,
      "step": 134848
    },
    {
      "epoch": 0.0004895062887767153,
      "grad_norm": 9368.767048016509,
      "learning_rate": 2.724229369173692e-07,
      "loss": 1.7509,
      "step": 134880
    },
    {
      "epoch": 0.0004896224231275521,
      "grad_norm": 9710.789051359317,
      "learning_rate": 2.723905944127425e-07,
      "loss": 1.7631,
      "step": 134912
    },
    {
      "epoch": 0.0004897385574783887,
      "grad_norm": 9713.101152567084,
      "learning_rate": 2.7235826342464926e-07,
      "loss": 1.7482,
      "step": 134944
    },
    {
      "epoch": 0.0004898546918292255,
      "grad_norm": 9473.073630031597,
      "learning_rate": 2.7232594394625646e-07,
      "loss": 1.7567,
      "step": 134976
    },
    {
      "epoch": 0.0004899708261800621,
      "grad_norm": 10382.05461361093,
      "learning_rate": 2.722936359707367e-07,
      "loss": 1.7556,
      "step": 135008
    },
    {
      "epoch": 0.0004900869605308989,
      "grad_norm": 8617.070964080543,
      "learning_rate": 2.722613394912683e-07,
      "loss": 1.751,
      "step": 135040
    },
    {
      "epoch": 0.0004902030948817356,
      "grad_norm": 9780.466246554915,
      "learning_rate": 2.722290545010351e-07,
      "loss": 1.7389,
      "step": 135072
    },
    {
      "epoch": 0.0004903192292325723,
      "grad_norm": 8577.527382643555,
      "learning_rate": 2.7219678099322674e-07,
      "loss": 1.7478,
      "step": 135104
    },
    {
      "epoch": 0.000490435363583409,
      "grad_norm": 10999.198879918484,
      "learning_rate": 2.7216451896103845e-07,
      "loss": 1.7448,
      "step": 135136
    },
    {
      "epoch": 0.0004905514979342457,
      "grad_norm": 10305.72578715347,
      "learning_rate": 2.7213226839767123e-07,
      "loss": 1.739,
      "step": 135168
    },
    {
      "epoch": 0.0004906676322850824,
      "grad_norm": 9087.171837265982,
      "learning_rate": 2.7210002929633147e-07,
      "loss": 1.7588,
      "step": 135200
    },
    {
      "epoch": 0.0004907837666359191,
      "grad_norm": 8777.159677253228,
      "learning_rate": 2.720678016502314e-07,
      "loss": 1.7622,
      "step": 135232
    },
    {
      "epoch": 0.0004908999009867558,
      "grad_norm": 8890.90883993307,
      "learning_rate": 2.720365920355404e-07,
      "loss": 1.7783,
      "step": 135264
    },
    {
      "epoch": 0.0004910160353375925,
      "grad_norm": 10006.803485629165,
      "learning_rate": 2.720043869221287e-07,
      "loss": 1.7575,
      "step": 135296
    },
    {
      "epoch": 0.0004911321696884292,
      "grad_norm": 8929.58498475713,
      "learning_rate": 2.719721932438384e-07,
      "loss": 1.7407,
      "step": 135328
    },
    {
      "epoch": 0.000491248304039266,
      "grad_norm": 7991.640257168738,
      "learning_rate": 2.71940010993904e-07,
      "loss": 1.7479,
      "step": 135360
    },
    {
      "epoch": 0.0004913644383901026,
      "grad_norm": 8772.01117190351,
      "learning_rate": 2.719078401655655e-07,
      "loss": 1.7658,
      "step": 135392
    },
    {
      "epoch": 0.0004914805727409394,
      "grad_norm": 9172.5854588551,
      "learning_rate": 2.7187568075206856e-07,
      "loss": 1.7642,
      "step": 135424
    },
    {
      "epoch": 0.000491596707091776,
      "grad_norm": 10021.861104605272,
      "learning_rate": 2.7184353274666445e-07,
      "loss": 1.7542,
      "step": 135456
    },
    {
      "epoch": 0.0004917128414426128,
      "grad_norm": 10310.5109475719,
      "learning_rate": 2.7181139614261e-07,
      "loss": 1.7693,
      "step": 135488
    },
    {
      "epoch": 0.0004918289757934494,
      "grad_norm": 10968.811239145289,
      "learning_rate": 2.7177927093316764e-07,
      "loss": 1.7649,
      "step": 135520
    },
    {
      "epoch": 0.0004919451101442862,
      "grad_norm": 9047.164859777897,
      "learning_rate": 2.717471571116053e-07,
      "loss": 1.7445,
      "step": 135552
    },
    {
      "epoch": 0.0004920612444951228,
      "grad_norm": 9350.265771623821,
      "learning_rate": 2.7171505467119664e-07,
      "loss": 1.7605,
      "step": 135584
    },
    {
      "epoch": 0.0004921773788459596,
      "grad_norm": 9048.848545533294,
      "learning_rate": 2.716829636052206e-07,
      "loss": 1.7638,
      "step": 135616
    },
    {
      "epoch": 0.0004922935131967963,
      "grad_norm": 10622.26670725227,
      "learning_rate": 2.71650883906962e-07,
      "loss": 1.7585,
      "step": 135648
    },
    {
      "epoch": 0.000492409647547633,
      "grad_norm": 10801.830585599831,
      "learning_rate": 2.7161881556971095e-07,
      "loss": 1.7906,
      "step": 135680
    },
    {
      "epoch": 0.0004925257818984697,
      "grad_norm": 9872.345719230056,
      "learning_rate": 2.715867585867633e-07,
      "loss": 1.7885,
      "step": 135712
    },
    {
      "epoch": 0.0004926419162493064,
      "grad_norm": 9813.743220606499,
      "learning_rate": 2.7155471295142024e-07,
      "loss": 1.7285,
      "step": 135744
    },
    {
      "epoch": 0.0004927580506001431,
      "grad_norm": 7756.78735044348,
      "learning_rate": 2.7152267865698873e-07,
      "loss": 1.7313,
      "step": 135776
    },
    {
      "epoch": 0.0004928741849509798,
      "grad_norm": 9145.004647347097,
      "learning_rate": 2.71490655696781e-07,
      "loss": 1.7409,
      "step": 135808
    },
    {
      "epoch": 0.0004929903193018165,
      "grad_norm": 8821.35114367408,
      "learning_rate": 2.7145864406411497e-07,
      "loss": 1.7447,
      "step": 135840
    },
    {
      "epoch": 0.0004931064536526532,
      "grad_norm": 10133.17827732247,
      "learning_rate": 2.7142664375231406e-07,
      "loss": 1.7665,
      "step": 135872
    },
    {
      "epoch": 0.00049322258800349,
      "grad_norm": 10468.54096806236,
      "learning_rate": 2.7139465475470714e-07,
      "loss": 1.769,
      "step": 135904
    },
    {
      "epoch": 0.0004933387223543267,
      "grad_norm": 11286.994994240053,
      "learning_rate": 2.7136267706462855e-07,
      "loss": 1.7778,
      "step": 135936
    },
    {
      "epoch": 0.0004934548567051633,
      "grad_norm": 9117.85259806277,
      "learning_rate": 2.713307106754183e-07,
      "loss": 1.7671,
      "step": 135968
    },
    {
      "epoch": 0.0004935709910560001,
      "grad_norm": 9752.414060118654,
      "learning_rate": 2.712987555804217e-07,
      "loss": 1.7539,
      "step": 136000
    },
    {
      "epoch": 0.0004936871254068367,
      "grad_norm": 9953.945951229593,
      "learning_rate": 2.712668117729896e-07,
      "loss": 1.7377,
      "step": 136032
    },
    {
      "epoch": 0.0004938032597576735,
      "grad_norm": 8458.209030285312,
      "learning_rate": 2.7123487924647837e-07,
      "loss": 1.7471,
      "step": 136064
    },
    {
      "epoch": 0.0004939193941085101,
      "grad_norm": 9333.562878129658,
      "learning_rate": 2.7120295799424984e-07,
      "loss": 1.7584,
      "step": 136096
    },
    {
      "epoch": 0.0004940355284593469,
      "grad_norm": 9394.081328155511,
      "learning_rate": 2.711710480096713e-07,
      "loss": 1.7411,
      "step": 136128
    },
    {
      "epoch": 0.0004941516628101835,
      "grad_norm": 10890.830087738952,
      "learning_rate": 2.711391492861155e-07,
      "loss": 1.7453,
      "step": 136160
    },
    {
      "epoch": 0.0004942677971610203,
      "grad_norm": 10206.423271646145,
      "learning_rate": 2.711072618169607e-07,
      "loss": 1.7448,
      "step": 136192
    },
    {
      "epoch": 0.0004943839315118571,
      "grad_norm": 11081.905070880186,
      "learning_rate": 2.710753855955904e-07,
      "loss": 1.7427,
      "step": 136224
    },
    {
      "epoch": 0.0004945000658626937,
      "grad_norm": 10030.897866093543,
      "learning_rate": 2.7104451622593613e-07,
      "loss": 1.7654,
      "step": 136256
    },
    {
      "epoch": 0.0004946162002135305,
      "grad_norm": 9708.251953879237,
      "learning_rate": 2.710126621293275e-07,
      "loss": 1.7785,
      "step": 136288
    },
    {
      "epoch": 0.0004947323345643671,
      "grad_norm": 10015.852534856931,
      "learning_rate": 2.709808192608932e-07,
      "loss": 1.7772,
      "step": 136320
    },
    {
      "epoch": 0.0004948484689152039,
      "grad_norm": 9659.197585721084,
      "learning_rate": 2.709489876140386e-07,
      "loss": 1.7616,
      "step": 136352
    },
    {
      "epoch": 0.0004949646032660405,
      "grad_norm": 10667.986032986732,
      "learning_rate": 2.7091716718217417e-07,
      "loss": 1.7676,
      "step": 136384
    },
    {
      "epoch": 0.0004950807376168773,
      "grad_norm": 10107.745742746005,
      "learning_rate": 2.708853579587161e-07,
      "loss": 1.7648,
      "step": 136416
    },
    {
      "epoch": 0.0004951968719677139,
      "grad_norm": 8550.1154378172,
      "learning_rate": 2.7085355993708585e-07,
      "loss": 1.7472,
      "step": 136448
    },
    {
      "epoch": 0.0004953130063185507,
      "grad_norm": 9369.971237949452,
      "learning_rate": 2.7082177311071024e-07,
      "loss": 1.7493,
      "step": 136480
    },
    {
      "epoch": 0.0004954291406693874,
      "grad_norm": 11311.646918110555,
      "learning_rate": 2.707899974730216e-07,
      "loss": 1.7644,
      "step": 136512
    },
    {
      "epoch": 0.0004955452750202241,
      "grad_norm": 9197.946727395196,
      "learning_rate": 2.7075823301745763e-07,
      "loss": 1.756,
      "step": 136544
    },
    {
      "epoch": 0.0004956614093710608,
      "grad_norm": 10885.789544171796,
      "learning_rate": 2.707264797374613e-07,
      "loss": 1.7708,
      "step": 136576
    },
    {
      "epoch": 0.0004957775437218975,
      "grad_norm": 9271.021195100355,
      "learning_rate": 2.706947376264811e-07,
      "loss": 1.7766,
      "step": 136608
    },
    {
      "epoch": 0.0004958936780727342,
      "grad_norm": 9286.195238094017,
      "learning_rate": 2.7066300667797096e-07,
      "loss": 1.7761,
      "step": 136640
    },
    {
      "epoch": 0.0004960098124235709,
      "grad_norm": 9000.990501050426,
      "learning_rate": 2.7063128688538987e-07,
      "loss": 1.7359,
      "step": 136672
    },
    {
      "epoch": 0.0004961259467744076,
      "grad_norm": 9711.218873035454,
      "learning_rate": 2.7059957824220246e-07,
      "loss": 1.7204,
      "step": 136704
    },
    {
      "epoch": 0.0004962420811252443,
      "grad_norm": 10207.891457103176,
      "learning_rate": 2.7056788074187863e-07,
      "loss": 1.7537,
      "step": 136736
    },
    {
      "epoch": 0.000496358215476081,
      "grad_norm": 10947.547761941942,
      "learning_rate": 2.705361943778937e-07,
      "loss": 1.745,
      "step": 136768
    },
    {
      "epoch": 0.0004964743498269178,
      "grad_norm": 10212.977528615247,
      "learning_rate": 2.7050451914372823e-07,
      "loss": 1.7482,
      "step": 136800
    },
    {
      "epoch": 0.0004965904841777544,
      "grad_norm": 9463.813713297615,
      "learning_rate": 2.704728550328682e-07,
      "loss": 1.7574,
      "step": 136832
    },
    {
      "epoch": 0.0004967066185285912,
      "grad_norm": 9735.861954649932,
      "learning_rate": 2.704412020388049e-07,
      "loss": 1.7656,
      "step": 136864
    },
    {
      "epoch": 0.0004968227528794278,
      "grad_norm": 10181.66528618968,
      "learning_rate": 2.704095601550349e-07,
      "loss": 1.7513,
      "step": 136896
    },
    {
      "epoch": 0.0004969388872302646,
      "grad_norm": 10193.85736608081,
      "learning_rate": 2.703779293750602e-07,
      "loss": 1.7687,
      "step": 136928
    },
    {
      "epoch": 0.0004970550215811012,
      "grad_norm": 9284.964189483986,
      "learning_rate": 2.70346309692388e-07,
      "loss": 1.7537,
      "step": 136960
    },
    {
      "epoch": 0.000497171155931938,
      "grad_norm": 9235.50832385527,
      "learning_rate": 2.7031470110053095e-07,
      "loss": 1.7564,
      "step": 136992
    },
    {
      "epoch": 0.0004972872902827746,
      "grad_norm": 10862.558630451667,
      "learning_rate": 2.702831035930069e-07,
      "loss": 1.7657,
      "step": 137024
    },
    {
      "epoch": 0.0004974034246336114,
      "grad_norm": 8960.900512783299,
      "learning_rate": 2.7025151716333905e-07,
      "loss": 1.7746,
      "step": 137056
    },
    {
      "epoch": 0.0004975195589844481,
      "grad_norm": 10849.339979925046,
      "learning_rate": 2.702199418050559e-07,
      "loss": 1.7744,
      "step": 137088
    },
    {
      "epoch": 0.0004976356933352848,
      "grad_norm": 10406.036997820063,
      "learning_rate": 2.7018837751169123e-07,
      "loss": 1.7282,
      "step": 137120
    },
    {
      "epoch": 0.0004977518276861215,
      "grad_norm": 8958.475428330425,
      "learning_rate": 2.70156824276784e-07,
      "loss": 1.7317,
      "step": 137152
    },
    {
      "epoch": 0.0004978679620369582,
      "grad_norm": 10066.79412722839,
      "learning_rate": 2.7012528209387876e-07,
      "loss": 1.7463,
      "step": 137184
    },
    {
      "epoch": 0.0004979840963877949,
      "grad_norm": 9244.710703964727,
      "learning_rate": 2.700937509565249e-07,
      "loss": 1.7633,
      "step": 137216
    },
    {
      "epoch": 0.0004981002307386316,
      "grad_norm": 18907.80283375094,
      "learning_rate": 2.7006223085827743e-07,
      "loss": 1.7766,
      "step": 137248
    },
    {
      "epoch": 0.0004982163650894683,
      "grad_norm": 9944.913272623346,
      "learning_rate": 2.700317062840615e-07,
      "loss": 1.7696,
      "step": 137280
    },
    {
      "epoch": 0.000498332499440305,
      "grad_norm": 8958.575109915639,
      "learning_rate": 2.7000020790024013e-07,
      "loss": 1.7701,
      "step": 137312
    },
    {
      "epoch": 0.0004984486337911417,
      "grad_norm": 9226.270319040083,
      "learning_rate": 2.699687205364222e-07,
      "loss": 1.7568,
      "step": 137344
    },
    {
      "epoch": 0.0004985647681419785,
      "grad_norm": 9092.221950656505,
      "learning_rate": 2.6993724418618346e-07,
      "loss": 1.7299,
      "step": 137376
    },
    {
      "epoch": 0.0004986809024928151,
      "grad_norm": 9520.260500637574,
      "learning_rate": 2.699057788431049e-07,
      "loss": 1.7549,
      "step": 137408
    },
    {
      "epoch": 0.0004987970368436519,
      "grad_norm": 9292.27539411096,
      "learning_rate": 2.698743245007727e-07,
      "loss": 1.765,
      "step": 137440
    },
    {
      "epoch": 0.0004989131711944885,
      "grad_norm": 9957.68527319477,
      "learning_rate": 2.698428811527784e-07,
      "loss": 1.7448,
      "step": 137472
    },
    {
      "epoch": 0.0004990293055453253,
      "grad_norm": 8967.836862922964,
      "learning_rate": 2.698114487927186e-07,
      "loss": 1.7554,
      "step": 137504
    },
    {
      "epoch": 0.0004991454398961619,
      "grad_norm": 9574.249108937995,
      "learning_rate": 2.6978002741419526e-07,
      "loss": 1.7701,
      "step": 137536
    },
    {
      "epoch": 0.0004992615742469987,
      "grad_norm": 11017.126304077665,
      "learning_rate": 2.697486170108155e-07,
      "loss": 1.7569,
      "step": 137568
    },
    {
      "epoch": 0.0004993777085978353,
      "grad_norm": 10702.581557736432,
      "learning_rate": 2.697172175761916e-07,
      "loss": 1.7509,
      "step": 137600
    },
    {
      "epoch": 0.0004994938429486721,
      "grad_norm": 9375.609633511838,
      "learning_rate": 2.696858291039411e-07,
      "loss": 1.7673,
      "step": 137632
    },
    {
      "epoch": 0.0004996099772995088,
      "grad_norm": 9381.832550200414,
      "learning_rate": 2.6965445158768674e-07,
      "loss": 1.7776,
      "step": 137664
    },
    {
      "epoch": 0.0004997261116503455,
      "grad_norm": 9417.680393812481,
      "learning_rate": 2.6962308502105647e-07,
      "loss": 1.7724,
      "step": 137696
    },
    {
      "epoch": 0.0004998422460011822,
      "grad_norm": 10740.141246743453,
      "learning_rate": 2.695917293976833e-07,
      "loss": 1.7586,
      "step": 137728
    },
    {
      "epoch": 0.0004999583803520189,
      "grad_norm": 9875.696532397094,
      "learning_rate": 2.695603847112056e-07,
      "loss": 1.7512,
      "step": 137760
    },
    {
      "epoch": 0.0005000745147028556,
      "grad_norm": 9152.348114008775,
      "learning_rate": 2.6952905095526687e-07,
      "loss": 1.7252,
      "step": 137792
    },
    {
      "epoch": 0.0005001906490536923,
      "grad_norm": 10009.0357177902,
      "learning_rate": 2.694977281235156e-07,
      "loss": 1.7449,
      "step": 137824
    },
    {
      "epoch": 0.000500306783404529,
      "grad_norm": 8939.38689172809,
      "learning_rate": 2.6946641620960573e-07,
      "loss": 1.7466,
      "step": 137856
    },
    {
      "epoch": 0.0005004229177553657,
      "grad_norm": 9009.030802478144,
      "learning_rate": 2.694351152071962e-07,
      "loss": 1.734,
      "step": 137888
    },
    {
      "epoch": 0.0005005390521062024,
      "grad_norm": 10833.74616649292,
      "learning_rate": 2.6940382510995104e-07,
      "loss": 1.7432,
      "step": 137920
    },
    {
      "epoch": 0.0005006551864570392,
      "grad_norm": 9157.996287398242,
      "learning_rate": 2.693725459115396e-07,
      "loss": 1.7589,
      "step": 137952
    },
    {
      "epoch": 0.0005007713208078758,
      "grad_norm": 10253.218031427987,
      "learning_rate": 2.6934127760563626e-07,
      "loss": 1.7738,
      "step": 137984
    },
    {
      "epoch": 0.0005008874551587126,
      "grad_norm": 9667.564946769171,
      "learning_rate": 2.693100201859206e-07,
      "loss": 1.7607,
      "step": 138016
    },
    {
      "epoch": 0.0005010035895095492,
      "grad_norm": 9330.060021243164,
      "learning_rate": 2.692787736460773e-07,
      "loss": 1.7349,
      "step": 138048
    },
    {
      "epoch": 0.000501119723860386,
      "grad_norm": 10105.155120036505,
      "learning_rate": 2.6924753797979606e-07,
      "loss": 1.7418,
      "step": 138080
    },
    {
      "epoch": 0.0005012358582112226,
      "grad_norm": 8957.562726545653,
      "learning_rate": 2.692163131807719e-07,
      "loss": 1.7632,
      "step": 138112
    },
    {
      "epoch": 0.0005013519925620594,
      "grad_norm": 9662.713076563952,
      "learning_rate": 2.6918509924270496e-07,
      "loss": 1.7773,
      "step": 138144
    },
    {
      "epoch": 0.000501468126912896,
      "grad_norm": 9691.018315945956,
      "learning_rate": 2.691538961593002e-07,
      "loss": 1.7686,
      "step": 138176
    },
    {
      "epoch": 0.0005015842612637328,
      "grad_norm": 10561.561816322432,
      "learning_rate": 2.691227039242681e-07,
      "loss": 1.773,
      "step": 138208
    },
    {
      "epoch": 0.0005017003956145695,
      "grad_norm": 10370.203276696171,
      "learning_rate": 2.690915225313238e-07,
      "loss": 1.7766,
      "step": 138240
    },
    {
      "epoch": 0.0005018165299654062,
      "grad_norm": 10717.213257185844,
      "learning_rate": 2.6906132589014226e-07,
      "loss": 1.7869,
      "step": 138272
    },
    {
      "epoch": 0.0005019326643162429,
      "grad_norm": 9885.369188856832,
      "learning_rate": 2.6903016582421233e-07,
      "loss": 1.7538,
      "step": 138304
    },
    {
      "epoch": 0.0005020487986670796,
      "grad_norm": 11582.319629504273,
      "learning_rate": 2.6899901658174283e-07,
      "loss": 1.7511,
      "step": 138336
    },
    {
      "epoch": 0.0005021649330179163,
      "grad_norm": 8779.11259752374,
      "learning_rate": 2.6896787815646936e-07,
      "loss": 1.7514,
      "step": 138368
    },
    {
      "epoch": 0.000502281067368753,
      "grad_norm": 8596.104001232186,
      "learning_rate": 2.689367505421325e-07,
      "loss": 1.7552,
      "step": 138400
    },
    {
      "epoch": 0.0005023972017195897,
      "grad_norm": 9739.708414526585,
      "learning_rate": 2.6890563373247797e-07,
      "loss": 1.7731,
      "step": 138432
    },
    {
      "epoch": 0.0005025133360704264,
      "grad_norm": 9027.57043727713,
      "learning_rate": 2.6887452772125656e-07,
      "loss": 1.757,
      "step": 138464
    },
    {
      "epoch": 0.0005026294704212631,
      "grad_norm": 8013.333139212422,
      "learning_rate": 2.6884343250222407e-07,
      "loss": 1.7197,
      "step": 138496
    },
    {
      "epoch": 0.0005027456047720999,
      "grad_norm": 8955.27872263058,
      "learning_rate": 2.6881234806914137e-07,
      "loss": 1.7251,
      "step": 138528
    },
    {
      "epoch": 0.0005028617391229365,
      "grad_norm": 11338.056623601771,
      "learning_rate": 2.6878127441577445e-07,
      "loss": 1.7628,
      "step": 138560
    },
    {
      "epoch": 0.0005029778734737733,
      "grad_norm": 9512.590919407814,
      "learning_rate": 2.687502115358943e-07,
      "loss": 1.7398,
      "step": 138592
    },
    {
      "epoch": 0.0005030940078246099,
      "grad_norm": 9144.63208663968,
      "learning_rate": 2.687191594232769e-07,
      "loss": 1.7471,
      "step": 138624
    },
    {
      "epoch": 0.0005032101421754467,
      "grad_norm": 9696.763480667145,
      "learning_rate": 2.686881180717032e-07,
      "loss": 1.7665,
      "step": 138656
    },
    {
      "epoch": 0.0005033262765262833,
      "grad_norm": 10690.033956915198,
      "learning_rate": 2.6865708747495937e-07,
      "loss": 1.7628,
      "step": 138688
    },
    {
      "epoch": 0.0005034424108771201,
      "grad_norm": 9688.801783502437,
      "learning_rate": 2.6862606762683653e-07,
      "loss": 1.7213,
      "step": 138720
    },
    {
      "epoch": 0.0005035585452279567,
      "grad_norm": 9573.903070326125,
      "learning_rate": 2.6859505852113075e-07,
      "loss": 1.7502,
      "step": 138752
    },
    {
      "epoch": 0.0005036746795787935,
      "grad_norm": 9596.258229122433,
      "learning_rate": 2.6856406015164317e-07,
      "loss": 1.7666,
      "step": 138784
    },
    {
      "epoch": 0.0005037908139296302,
      "grad_norm": 11125.553469378501,
      "learning_rate": 2.6853307251218e-07,
      "loss": 1.7717,
      "step": 138816
    },
    {
      "epoch": 0.0005039069482804669,
      "grad_norm": 9954.782468743353,
      "learning_rate": 2.6850209559655215e-07,
      "loss": 1.7727,
      "step": 138848
    },
    {
      "epoch": 0.0005040230826313036,
      "grad_norm": 9751.248535444063,
      "learning_rate": 2.68471129398576e-07,
      "loss": 1.7678,
      "step": 138880
    },
    {
      "epoch": 0.0005041392169821403,
      "grad_norm": 9567.03830869303,
      "learning_rate": 2.684401739120725e-07,
      "loss": 1.7567,
      "step": 138912
    },
    {
      "epoch": 0.000504255351332977,
      "grad_norm": 9300.968121652713,
      "learning_rate": 2.684092291308678e-07,
      "loss": 1.7461,
      "step": 138944
    },
    {
      "epoch": 0.0005043714856838137,
      "grad_norm": 9471.808697392487,
      "learning_rate": 2.683782950487931e-07,
      "loss": 1.7611,
      "step": 138976
    },
    {
      "epoch": 0.0005044876200346504,
      "grad_norm": 9138.592889498908,
      "learning_rate": 2.6834737165968433e-07,
      "loss": 1.768,
      "step": 139008
    },
    {
      "epoch": 0.0005046037543854871,
      "grad_norm": 10759.994423790378,
      "learning_rate": 2.6831645895738257e-07,
      "loss": 1.7789,
      "step": 139040
    },
    {
      "epoch": 0.0005047198887363238,
      "grad_norm": 8683.80515672709,
      "learning_rate": 2.6828555693573376e-07,
      "loss": 1.7814,
      "step": 139072
    },
    {
      "epoch": 0.0005048360230871606,
      "grad_norm": 8556.14539380906,
      "learning_rate": 2.6825466558858897e-07,
      "loss": 1.7693,
      "step": 139104
    },
    {
      "epoch": 0.0005049521574379972,
      "grad_norm": 10767.16155725361,
      "learning_rate": 2.6822378490980404e-07,
      "loss": 1.7709,
      "step": 139136
    },
    {
      "epoch": 0.000505068291788834,
      "grad_norm": 9035.778439072086,
      "learning_rate": 2.6819291489323987e-07,
      "loss": 1.7194,
      "step": 139168
    },
    {
      "epoch": 0.0005051844261396706,
      "grad_norm": 11070.077867838148,
      "learning_rate": 2.681620555327622e-07,
      "loss": 1.7266,
      "step": 139200
    },
    {
      "epoch": 0.0005053005604905074,
      "grad_norm": 10077.220350870572,
      "learning_rate": 2.6813120682224187e-07,
      "loss": 1.7314,
      "step": 139232
    },
    {
      "epoch": 0.000505416694841344,
      "grad_norm": 7858.02265204167,
      "learning_rate": 2.6810133228408663e-07,
      "loss": 1.7343,
      "step": 139264
    },
    {
      "epoch": 0.0005055328291921808,
      "grad_norm": 9641.522079008066,
      "learning_rate": 2.680705045227769e-07,
      "loss": 1.7552,
      "step": 139296
    },
    {
      "epoch": 0.0005056489635430174,
      "grad_norm": 9787.847771599229,
      "learning_rate": 2.680396873932573e-07,
      "loss": 1.7626,
      "step": 139328
    },
    {
      "epoch": 0.0005057650978938542,
      "grad_norm": 9088.714320518608,
      "learning_rate": 2.6800888088941815e-07,
      "loss": 1.7929,
      "step": 139360
    },
    {
      "epoch": 0.0005058812322446909,
      "grad_norm": 10727.488242827396,
      "learning_rate": 2.679780850051547e-07,
      "loss": 1.7686,
      "step": 139392
    },
    {
      "epoch": 0.0005059973665955276,
      "grad_norm": 10405.156606221744,
      "learning_rate": 2.6794729973436693e-07,
      "loss": 1.7603,
      "step": 139424
    },
    {
      "epoch": 0.0005061135009463643,
      "grad_norm": 8807.858990696888,
      "learning_rate": 2.6791652507096005e-07,
      "loss": 1.7467,
      "step": 139456
    },
    {
      "epoch": 0.000506229635297201,
      "grad_norm": 9586.9484195963,
      "learning_rate": 2.678857610088439e-07,
      "loss": 1.7524,
      "step": 139488
    },
    {
      "epoch": 0.0005063457696480377,
      "grad_norm": 10321.0587635184,
      "learning_rate": 2.678550075419333e-07,
      "loss": 1.7358,
      "step": 139520
    },
    {
      "epoch": 0.0005064619039988744,
      "grad_norm": 10355.7237313478,
      "learning_rate": 2.67824264664148e-07,
      "loss": 1.7417,
      "step": 139552
    },
    {
      "epoch": 0.0005065780383497111,
      "grad_norm": 9016.81373878822,
      "learning_rate": 2.677935323694125e-07,
      "loss": 1.7527,
      "step": 139584
    },
    {
      "epoch": 0.0005066941727005478,
      "grad_norm": 8755.03489427655,
      "learning_rate": 2.677628106516564e-07,
      "loss": 1.7452,
      "step": 139616
    },
    {
      "epoch": 0.0005068103070513845,
      "grad_norm": 10594.784377230148,
      "learning_rate": 2.67732099504814e-07,
      "loss": 1.7557,
      "step": 139648
    },
    {
      "epoch": 0.0005069264414022213,
      "grad_norm": 9762.670228989607,
      "learning_rate": 2.677013989228245e-07,
      "loss": 1.7743,
      "step": 139680
    },
    {
      "epoch": 0.0005070425757530579,
      "grad_norm": 10318.929401832344,
      "learning_rate": 2.67670708899632e-07,
      "loss": 1.7653,
      "step": 139712
    },
    {
      "epoch": 0.0005071587101038947,
      "grad_norm": 10246.031719646391,
      "learning_rate": 2.676400294291854e-07,
      "loss": 1.7639,
      "step": 139744
    },
    {
      "epoch": 0.0005072748444547313,
      "grad_norm": 10680.982539073828,
      "learning_rate": 2.6760936050543854e-07,
      "loss": 1.7605,
      "step": 139776
    },
    {
      "epoch": 0.0005073909788055681,
      "grad_norm": 9459.753802293166,
      "learning_rate": 2.6757870212235007e-07,
      "loss": 1.7679,
      "step": 139808
    },
    {
      "epoch": 0.0005075071131564047,
      "grad_norm": 10689.443203460132,
      "learning_rate": 2.6754805427388346e-07,
      "loss": 1.7247,
      "step": 139840
    },
    {
      "epoch": 0.0005076232475072415,
      "grad_norm": 9158.831038948147,
      "learning_rate": 2.67517416954007e-07,
      "loss": 1.7389,
      "step": 139872
    },
    {
      "epoch": 0.0005077393818580781,
      "grad_norm": 9599.708849751642,
      "learning_rate": 2.6748679015669387e-07,
      "loss": 1.7439,
      "step": 139904
    },
    {
      "epoch": 0.0005078555162089149,
      "grad_norm": 8865.878636660893,
      "learning_rate": 2.674561738759221e-07,
      "loss": 1.7615,
      "step": 139936
    },
    {
      "epoch": 0.0005079716505597517,
      "grad_norm": 10265.980518196984,
      "learning_rate": 2.674255681056745e-07,
      "loss": 1.7843,
      "step": 139968
    },
    {
      "epoch": 0.0005080877849105883,
      "grad_norm": 10134.173079240358,
      "learning_rate": 2.673949728399386e-07,
      "loss": 1.8125,
      "step": 140000
    },
    {
      "epoch": 0.000508203919261425,
      "grad_norm": 9903.913569897508,
      "learning_rate": 2.6736438807270694e-07,
      "loss": 1.7825,
      "step": 140032
    },
    {
      "epoch": 0.0005083200536122617,
      "grad_norm": 10382.731625155298,
      "learning_rate": 2.673338137979768e-07,
      "loss": 1.7429,
      "step": 140064
    },
    {
      "epoch": 0.0005084361879630985,
      "grad_norm": 8867.460177525467,
      "learning_rate": 2.673032500097501e-07,
      "loss": 1.7205,
      "step": 140096
    },
    {
      "epoch": 0.0005085523223139351,
      "grad_norm": 7993.368376347984,
      "learning_rate": 2.672726967020338e-07,
      "loss": 1.736,
      "step": 140128
    },
    {
      "epoch": 0.0005086684566647719,
      "grad_norm": 9292.34717388454,
      "learning_rate": 2.672421538688396e-07,
      "loss": 1.7401,
      "step": 140160
    },
    {
      "epoch": 0.0005087845910156085,
      "grad_norm": 9135.626196380848,
      "learning_rate": 2.6721162150418373e-07,
      "loss": 1.7507,
      "step": 140192
    },
    {
      "epoch": 0.0005089007253664452,
      "grad_norm": 10124.195869302412,
      "learning_rate": 2.6718109960208765e-07,
      "loss": 1.7504,
      "step": 140224
    },
    {
      "epoch": 0.000509016859717282,
      "grad_norm": 8961.580664146253,
      "learning_rate": 2.6715058815657724e-07,
      "loss": 1.7558,
      "step": 140256
    },
    {
      "epoch": 0.0005091329940681186,
      "grad_norm": 11105.385180172725,
      "learning_rate": 2.6712104015964494e-07,
      "loss": 1.7765,
      "step": 140288
    },
    {
      "epoch": 0.0005092491284189554,
      "grad_norm": 9502.038518128624,
      "learning_rate": 2.670905492830979e-07,
      "loss": 1.7449,
      "step": 140320
    },
    {
      "epoch": 0.000509365262769792,
      "grad_norm": 9961.80646268537,
      "learning_rate": 2.6706006884542935e-07,
      "loss": 1.7441,
      "step": 140352
    },
    {
      "epoch": 0.0005094813971206288,
      "grad_norm": 10018.678655391637,
      "learning_rate": 2.670295988406842e-07,
      "loss": 1.7574,
      "step": 140384
    },
    {
      "epoch": 0.0005095975314714654,
      "grad_norm": 12079.41786676825,
      "learning_rate": 2.669991392629122e-07,
      "loss": 1.7586,
      "step": 140416
    },
    {
      "epoch": 0.0005097136658223022,
      "grad_norm": 9450.130157833806,
      "learning_rate": 2.669686901061677e-07,
      "loss": 1.7523,
      "step": 140448
    },
    {
      "epoch": 0.0005098298001731388,
      "grad_norm": 9876.363095795943,
      "learning_rate": 2.6693825136450987e-07,
      "loss": 1.7489,
      "step": 140480
    },
    {
      "epoch": 0.0005099459345239756,
      "grad_norm": 8498.357135352691,
      "learning_rate": 2.669078230320027e-07,
      "loss": 1.7361,
      "step": 140512
    },
    {
      "epoch": 0.0005100620688748124,
      "grad_norm": 9273.444236096962,
      "learning_rate": 2.668774051027148e-07,
      "loss": 1.7409,
      "step": 140544
    },
    {
      "epoch": 0.000510178203225649,
      "grad_norm": 9354.112036960003,
      "learning_rate": 2.6684699757071947e-07,
      "loss": 1.7636,
      "step": 140576
    },
    {
      "epoch": 0.0005102943375764858,
      "grad_norm": 10358.376224099993,
      "learning_rate": 2.6681660043009496e-07,
      "loss": 1.7891,
      "step": 140608
    },
    {
      "epoch": 0.0005104104719273224,
      "grad_norm": 9918.419833824337,
      "learning_rate": 2.6678621367492395e-07,
      "loss": 1.7619,
      "step": 140640
    },
    {
      "epoch": 0.0005105266062781592,
      "grad_norm": 10635.652495263279,
      "learning_rate": 2.6675583729929414e-07,
      "loss": 1.7526,
      "step": 140672
    },
    {
      "epoch": 0.0005106427406289958,
      "grad_norm": 10708.564796460822,
      "learning_rate": 2.667254712972976e-07,
      "loss": 1.776,
      "step": 140704
    },
    {
      "epoch": 0.0005107588749798326,
      "grad_norm": 8291.986251797574,
      "learning_rate": 2.6669511566303144e-07,
      "loss": 1.7608,
      "step": 140736
    },
    {
      "epoch": 0.0005108750093306692,
      "grad_norm": 8212.180465625437,
      "learning_rate": 2.666647703905973e-07,
      "loss": 1.7286,
      "step": 140768
    },
    {
      "epoch": 0.000510991143681506,
      "grad_norm": 10015.429296839951,
      "learning_rate": 2.666344354741015e-07,
      "loss": 1.7608,
      "step": 140800
    },
    {
      "epoch": 0.0005111072780323427,
      "grad_norm": 9099.418662749835,
      "learning_rate": 2.666041109076551e-07,
      "loss": 1.7524,
      "step": 140832
    },
    {
      "epoch": 0.0005112234123831794,
      "grad_norm": 10098.823594854997,
      "learning_rate": 2.665737966853738e-07,
      "loss": 1.7685,
      "step": 140864
    },
    {
      "epoch": 0.0005113395467340161,
      "grad_norm": 10384.512699207411,
      "learning_rate": 2.6654349280137805e-07,
      "loss": 1.7488,
      "step": 140896
    },
    {
      "epoch": 0.0005114556810848528,
      "grad_norm": 9199.093324888057,
      "learning_rate": 2.6651319924979296e-07,
      "loss": 1.7602,
      "step": 140928
    },
    {
      "epoch": 0.0005115718154356895,
      "grad_norm": 11065.369763365343,
      "learning_rate": 2.664829160247483e-07,
      "loss": 1.7444,
      "step": 140960
    },
    {
      "epoch": 0.0005116879497865262,
      "grad_norm": 10585.42318473853,
      "learning_rate": 2.664526431203786e-07,
      "loss": 1.7303,
      "step": 140992
    },
    {
      "epoch": 0.0005118040841373629,
      "grad_norm": 10369.763353133958,
      "learning_rate": 2.6642238053082277e-07,
      "loss": 1.7643,
      "step": 141024
    },
    {
      "epoch": 0.0005119202184881996,
      "grad_norm": 9975.62850150305,
      "learning_rate": 2.6639212825022463e-07,
      "loss": 1.7475,
      "step": 141056
    },
    {
      "epoch": 0.0005120363528390363,
      "grad_norm": 10386.704193342564,
      "learning_rate": 2.663618862727327e-07,
      "loss": 1.7545,
      "step": 141088
    },
    {
      "epoch": 0.0005121524871898731,
      "grad_norm": 9659.343766529899,
      "learning_rate": 2.6633165459249996e-07,
      "loss": 1.7735,
      "step": 141120
    },
    {
      "epoch": 0.0005122686215407097,
      "grad_norm": 10013.724681655673,
      "learning_rate": 2.6630143320368417e-07,
      "loss": 1.7793,
      "step": 141152
    },
    {
      "epoch": 0.0005123847558915465,
      "grad_norm": 10009.7155803749,
      "learning_rate": 2.6627122210044757e-07,
      "loss": 1.7646,
      "step": 141184
    },
    {
      "epoch": 0.0005125008902423831,
      "grad_norm": 9373.044862796722,
      "learning_rate": 2.6624102127695725e-07,
      "loss": 1.7566,
      "step": 141216
    },
    {
      "epoch": 0.0005126170245932199,
      "grad_norm": 9784.453484993426,
      "learning_rate": 2.6621083072738477e-07,
      "loss": 1.7172,
      "step": 141248
    },
    {
      "epoch": 0.0005127331589440565,
      "grad_norm": 10873.579263517602,
      "learning_rate": 2.661815934243352e-07,
      "loss": 1.7347,
      "step": 141280
    },
    {
      "epoch": 0.0005128492932948933,
      "grad_norm": 9842.980239744466,
      "learning_rate": 2.661514230845237e-07,
      "loss": 1.7386,
      "step": 141312
    },
    {
      "epoch": 0.0005129654276457299,
      "grad_norm": 11327.392639085132,
      "learning_rate": 2.661212630013543e-07,
      "loss": 1.7516,
      "step": 141344
    },
    {
      "epoch": 0.0005130815619965667,
      "grad_norm": 8949.68289941046,
      "learning_rate": 2.6609111316901683e-07,
      "loss": 1.7732,
      "step": 141376
    },
    {
      "epoch": 0.0005131976963474034,
      "grad_norm": 11036.377032341728,
      "learning_rate": 2.6606097358170594e-07,
      "loss": 1.7795,
      "step": 141408
    },
    {
      "epoch": 0.0005133138306982401,
      "grad_norm": 9893.34119496543,
      "learning_rate": 2.660308442336208e-07,
      "loss": 1.7294,
      "step": 141440
    },
    {
      "epoch": 0.0005134299650490768,
      "grad_norm": 8115.685183642844,
      "learning_rate": 2.66000725118965e-07,
      "loss": 1.733,
      "step": 141472
    },
    {
      "epoch": 0.0005135460993999135,
      "grad_norm": 8314.968189957193,
      "learning_rate": 2.6597061623194706e-07,
      "loss": 1.765,
      "step": 141504
    },
    {
      "epoch": 0.0005136622337507502,
      "grad_norm": 10088.394322190226,
      "learning_rate": 2.659405175667798e-07,
      "loss": 1.7727,
      "step": 141536
    },
    {
      "epoch": 0.0005137783681015869,
      "grad_norm": 9932.074304997925,
      "learning_rate": 2.6591042911768077e-07,
      "loss": 1.7511,
      "step": 141568
    },
    {
      "epoch": 0.0005138945024524236,
      "grad_norm": 10197.058399362044,
      "learning_rate": 2.6588035087887206e-07,
      "loss": 1.7529,
      "step": 141600
    },
    {
      "epoch": 0.0005140106368032603,
      "grad_norm": 9635.004618576993,
      "learning_rate": 2.6585028284458034e-07,
      "loss": 1.7667,
      "step": 141632
    },
    {
      "epoch": 0.000514126771154097,
      "grad_norm": 10019.56925221838,
      "learning_rate": 2.658202250090368e-07,
      "loss": 1.7484,
      "step": 141664
    },
    {
      "epoch": 0.0005142429055049338,
      "grad_norm": 9916.679988786569,
      "learning_rate": 2.657901773664773e-07,
      "loss": 1.77,
      "step": 141696
    },
    {
      "epoch": 0.0005143590398557704,
      "grad_norm": 10970.388689558817,
      "learning_rate": 2.657601399111421e-07,
      "loss": 1.7813,
      "step": 141728
    },
    {
      "epoch": 0.0005144751742066072,
      "grad_norm": 11228.030103272791,
      "learning_rate": 2.657301126372762e-07,
      "loss": 1.7682,
      "step": 141760
    },
    {
      "epoch": 0.0005145913085574438,
      "grad_norm": 9670.415192741208,
      "learning_rate": 2.6570009553912904e-07,
      "loss": 1.7809,
      "step": 141792
    },
    {
      "epoch": 0.0005147074429082806,
      "grad_norm": 8498.026594451208,
      "learning_rate": 2.656700886109546e-07,
      "loss": 1.7722,
      "step": 141824
    },
    {
      "epoch": 0.0005148235772591172,
      "grad_norm": 9943.052046529778,
      "learning_rate": 2.656400918470115e-07,
      "loss": 1.7497,
      "step": 141856
    },
    {
      "epoch": 0.000514939711609954,
      "grad_norm": 10171.482881074913,
      "learning_rate": 2.6561010524156275e-07,
      "loss": 1.7213,
      "step": 141888
    },
    {
      "epoch": 0.0005150558459607906,
      "grad_norm": 9387.17657232461,
      "learning_rate": 2.6558012878887605e-07,
      "loss": 1.7108,
      "step": 141920
    },
    {
      "epoch": 0.0005151719803116274,
      "grad_norm": 9513.041784834124,
      "learning_rate": 2.6555016248322345e-07,
      "loss": 1.7451,
      "step": 141952
    },
    {
      "epoch": 0.0005152881146624641,
      "grad_norm": 11579.026556667015,
      "learning_rate": 2.6552020631888166e-07,
      "loss": 1.7539,
      "step": 141984
    },
    {
      "epoch": 0.0005154042490133008,
      "grad_norm": 9380.601260047248,
      "learning_rate": 2.6549026029013194e-07,
      "loss": 1.7531,
      "step": 142016
    },
    {
      "epoch": 0.0005155203833641375,
      "grad_norm": 9092.012648473385,
      "learning_rate": 2.6546032439125993e-07,
      "loss": 1.7475,
      "step": 142048
    },
    {
      "epoch": 0.0005156365177149742,
      "grad_norm": 9816.469833906689,
      "learning_rate": 2.654303986165558e-07,
      "loss": 1.7658,
      "step": 142080
    },
    {
      "epoch": 0.0005157526520658109,
      "grad_norm": 10016.669506377855,
      "learning_rate": 2.654004829603143e-07,
      "loss": 1.7491,
      "step": 142112
    },
    {
      "epoch": 0.0005158687864166476,
      "grad_norm": 9615.712142114073,
      "learning_rate": 2.653705774168347e-07,
      "loss": 1.74,
      "step": 142144
    },
    {
      "epoch": 0.0005159849207674843,
      "grad_norm": 7315.1148999861925,
      "learning_rate": 2.653406819804206e-07,
      "loss": 1.7275,
      "step": 142176
    },
    {
      "epoch": 0.000516101055118321,
      "grad_norm": 9629.519614186369,
      "learning_rate": 2.653107966453804e-07,
      "loss": 1.7443,
      "step": 142208
    },
    {
      "epoch": 0.0005162171894691577,
      "grad_norm": 9484.07960742633,
      "learning_rate": 2.652809214060266e-07,
      "loss": 1.7555,
      "step": 142240
    },
    {
      "epoch": 0.0005163333238199945,
      "grad_norm": 9532.645907616627,
      "learning_rate": 2.6525198938992043e-07,
      "loss": 1.7623,
      "step": 142272
    },
    {
      "epoch": 0.0005164494581708311,
      "grad_norm": 10186.165323614181,
      "learning_rate": 2.652221340098463e-07,
      "loss": 1.7604,
      "step": 142304
    },
    {
      "epoch": 0.0005165655925216679,
      "grad_norm": 10470.724807767607,
      "learning_rate": 2.6519228870860076e-07,
      "loss": 1.7713,
      "step": 142336
    },
    {
      "epoch": 0.0005166817268725045,
      "grad_norm": 9319.736798858647,
      "learning_rate": 2.6516245348051436e-07,
      "loss": 1.7655,
      "step": 142368
    },
    {
      "epoch": 0.0005167978612233413,
      "grad_norm": 10071.52471078734,
      "learning_rate": 2.6513262831992196e-07,
      "loss": 1.7741,
      "step": 142400
    },
    {
      "epoch": 0.0005169139955741779,
      "grad_norm": 10548.130071249596,
      "learning_rate": 2.65102813221163e-07,
      "loss": 1.7762,
      "step": 142432
    },
    {
      "epoch": 0.0005170301299250147,
      "grad_norm": 9276.843967643306,
      "learning_rate": 2.650730081785812e-07,
      "loss": 1.7692,
      "step": 142464
    },
    {
      "epoch": 0.0005171462642758513,
      "grad_norm": 9434.131862551,
      "learning_rate": 2.6504321318652487e-07,
      "loss": 1.7604,
      "step": 142496
    },
    {
      "epoch": 0.0005172623986266881,
      "grad_norm": 9359.119509868437,
      "learning_rate": 2.650134282393468e-07,
      "loss": 1.7605,
      "step": 142528
    },
    {
      "epoch": 0.0005173785329775248,
      "grad_norm": 10320.413751395823,
      "learning_rate": 2.6498365333140407e-07,
      "loss": 1.7636,
      "step": 142560
    },
    {
      "epoch": 0.0005174946673283615,
      "grad_norm": 8237.376281316763,
      "learning_rate": 2.6495388845705836e-07,
      "loss": 1.7341,
      "step": 142592
    },
    {
      "epoch": 0.0005176108016791982,
      "grad_norm": 9217.345170926388,
      "learning_rate": 2.649241336106757e-07,
      "loss": 1.7186,
      "step": 142624
    },
    {
      "epoch": 0.0005177269360300349,
      "grad_norm": 10176.151237083694,
      "learning_rate": 2.648943887866265e-07,
      "loss": 1.737,
      "step": 142656
    },
    {
      "epoch": 0.0005178430703808716,
      "grad_norm": 9708.348160217576,
      "learning_rate": 2.648646539792857e-07,
      "loss": 1.7444,
      "step": 142688
    },
    {
      "epoch": 0.0005179592047317083,
      "grad_norm": 9496.660886859128,
      "learning_rate": 2.6483492918303255e-07,
      "loss": 1.7545,
      "step": 142720
    },
    {
      "epoch": 0.000518075339082545,
      "grad_norm": 8856.47401622113,
      "learning_rate": 2.648052143922508e-07,
      "loss": 1.7855,
      "step": 142752
    },
    {
      "epoch": 0.0005181914734333817,
      "grad_norm": 9822.235997979278,
      "learning_rate": 2.6477550960132863e-07,
      "loss": 1.7468,
      "step": 142784
    },
    {
      "epoch": 0.0005183076077842184,
      "grad_norm": 10627.337578151924,
      "learning_rate": 2.647458148046585e-07,
      "loss": 1.7227,
      "step": 142816
    },
    {
      "epoch": 0.0005184237421350552,
      "grad_norm": 8566.269549809882,
      "learning_rate": 2.647161299966374e-07,
      "loss": 1.7559,
      "step": 142848
    },
    {
      "epoch": 0.0005185398764858918,
      "grad_norm": 9459.616376999651,
      "learning_rate": 2.646864551716666e-07,
      "loss": 1.7613,
      "step": 142880
    },
    {
      "epoch": 0.0005186560108367286,
      "grad_norm": 8950.92531529562,
      "learning_rate": 2.6465679032415193e-07,
      "loss": 1.7554,
      "step": 142912
    },
    {
      "epoch": 0.0005187721451875652,
      "grad_norm": 9356.421003781306,
      "learning_rate": 2.646271354485034e-07,
      "loss": 1.761,
      "step": 142944
    },
    {
      "epoch": 0.000518888279538402,
      "grad_norm": 9796.04757032141,
      "learning_rate": 2.645974905391356e-07,
      "loss": 1.767,
      "step": 142976
    },
    {
      "epoch": 0.0005190044138892386,
      "grad_norm": 10184.222994416412,
      "learning_rate": 2.645678555904673e-07,
      "loss": 1.7456,
      "step": 143008
    },
    {
      "epoch": 0.0005191205482400754,
      "grad_norm": 9574.314179093979,
      "learning_rate": 2.6453823059692185e-07,
      "loss": 1.7444,
      "step": 143040
    },
    {
      "epoch": 0.000519236682590912,
      "grad_norm": 11354.057336476684,
      "learning_rate": 2.6450861555292684e-07,
      "loss": 1.7598,
      "step": 143072
    },
    {
      "epoch": 0.0005193528169417488,
      "grad_norm": 9730.050770679461,
      "learning_rate": 2.6447901045291424e-07,
      "loss": 1.7546,
      "step": 143104
    },
    {
      "epoch": 0.0005194689512925855,
      "grad_norm": 10535.016279057189,
      "learning_rate": 2.644494152913204e-07,
      "loss": 1.7619,
      "step": 143136
    },
    {
      "epoch": 0.0005195850856434222,
      "grad_norm": 9039.043754734237,
      "learning_rate": 2.6441983006258605e-07,
      "loss": 1.7763,
      "step": 143168
    },
    {
      "epoch": 0.0005197012199942589,
      "grad_norm": 9343.848885764366,
      "learning_rate": 2.643902547611562e-07,
      "loss": 1.7537,
      "step": 143200
    },
    {
      "epoch": 0.0005198173543450956,
      "grad_norm": 10090.218233517053,
      "learning_rate": 2.643606893814803e-07,
      "loss": 1.7446,
      "step": 143232
    },
    {
      "epoch": 0.0005199334886959323,
      "grad_norm": 8585.253752801951,
      "learning_rate": 2.6433113391801216e-07,
      "loss": 1.7229,
      "step": 143264
    },
    {
      "epoch": 0.000520049623046769,
      "grad_norm": 8697.330739945446,
      "learning_rate": 2.643025115137749e-07,
      "loss": 1.728,
      "step": 143296
    },
    {
      "epoch": 0.0005201657573976057,
      "grad_norm": 9557.150202858591,
      "learning_rate": 2.642729755566492e-07,
      "loss": 1.7588,
      "step": 143328
    },
    {
      "epoch": 0.0005202818917484424,
      "grad_norm": 10235.054469811092,
      "learning_rate": 2.642434494992914e-07,
      "loss": 1.7676,
      "step": 143360
    },
    {
      "epoch": 0.0005203980260992791,
      "grad_norm": 12614.542401530069,
      "learning_rate": 2.642139333361723e-07,
      "loss": 1.7661,
      "step": 143392
    },
    {
      "epoch": 0.0005205141604501159,
      "grad_norm": 11965.25486565163,
      "learning_rate": 2.641844270617673e-07,
      "loss": 1.7818,
      "step": 143424
    },
    {
      "epoch": 0.0005206302948009525,
      "grad_norm": 11183.553639161391,
      "learning_rate": 2.641549306705558e-07,
      "loss": 1.7803,
      "step": 143456
    },
    {
      "epoch": 0.0005207464291517893,
      "grad_norm": 9601.920432913408,
      "learning_rate": 2.641254441570219e-07,
      "loss": 1.7376,
      "step": 143488
    },
    {
      "epoch": 0.0005208625635026259,
      "grad_norm": 9961.637014065509,
      "learning_rate": 2.640959675156538e-07,
      "loss": 1.7391,
      "step": 143520
    },
    {
      "epoch": 0.0005209786978534627,
      "grad_norm": 9596.827809229464,
      "learning_rate": 2.6406650074094384e-07,
      "loss": 1.7589,
      "step": 143552
    },
    {
      "epoch": 0.0005210948322042993,
      "grad_norm": 9404.499242383934,
      "learning_rate": 2.64037043827389e-07,
      "loss": 1.7525,
      "step": 143584
    },
    {
      "epoch": 0.0005212109665551361,
      "grad_norm": 10088.788430728438,
      "learning_rate": 2.640075967694903e-07,
      "loss": 1.7404,
      "step": 143616
    },
    {
      "epoch": 0.0005213271009059727,
      "grad_norm": 10623.158287439757,
      "learning_rate": 2.639781595617532e-07,
      "loss": 1.7608,
      "step": 143648
    },
    {
      "epoch": 0.0005214432352568095,
      "grad_norm": 9633.845960985675,
      "learning_rate": 2.6394873219868744e-07,
      "loss": 1.7627,
      "step": 143680
    },
    {
      "epoch": 0.0005215593696076462,
      "grad_norm": 9094.653594282741,
      "learning_rate": 2.6391931467480685e-07,
      "loss": 1.754,
      "step": 143712
    },
    {
      "epoch": 0.0005216755039584829,
      "grad_norm": 10868.092748960142,
      "learning_rate": 2.6388990698462974e-07,
      "loss": 1.7526,
      "step": 143744
    },
    {
      "epoch": 0.0005217916383093196,
      "grad_norm": 10329.412761623966,
      "learning_rate": 2.638605091226787e-07,
      "loss": 1.7507,
      "step": 143776
    },
    {
      "epoch": 0.0005219077726601563,
      "grad_norm": 10663.896942487769,
      "learning_rate": 2.6383112108348054e-07,
      "loss": 1.7449,
      "step": 143808
    },
    {
      "epoch": 0.000522023907010993,
      "grad_norm": 10086.541032484824,
      "learning_rate": 2.638017428615662e-07,
      "loss": 1.751,
      "step": 143840
    },
    {
      "epoch": 0.0005221400413618297,
      "grad_norm": 10071.210652151012,
      "learning_rate": 2.637723744514712e-07,
      "loss": 1.7488,
      "step": 143872
    },
    {
      "epoch": 0.0005222561757126664,
      "grad_norm": 8953.044398415546,
      "learning_rate": 2.637430158477349e-07,
      "loss": 1.7479,
      "step": 143904
    },
    {
      "epoch": 0.0005223723100635031,
      "grad_norm": 10266.88930494529,
      "learning_rate": 2.637136670449013e-07,
      "loss": 1.7169,
      "step": 143936
    },
    {
      "epoch": 0.0005224884444143398,
      "grad_norm": 9021.713141083572,
      "learning_rate": 2.636843280375184e-07,
      "loss": 1.7471,
      "step": 143968
    },
    {
      "epoch": 0.0005226045787651766,
      "grad_norm": 9148.767895186762,
      "learning_rate": 2.636549988201386e-07,
      "loss": 1.7438,
      "step": 144000
    },
    {
      "epoch": 0.0005227207131160132,
      "grad_norm": 8943.400471856328,
      "learning_rate": 2.6362567938731845e-07,
      "loss": 1.7474,
      "step": 144032
    },
    {
      "epoch": 0.00052283684746685,
      "grad_norm": 10323.384135059587,
      "learning_rate": 2.6359636973361874e-07,
      "loss": 1.7609,
      "step": 144064
    },
    {
      "epoch": 0.0005229529818176866,
      "grad_norm": 11417.823084984282,
      "learning_rate": 2.635670698536045e-07,
      "loss": 1.7871,
      "step": 144096
    },
    {
      "epoch": 0.0005230691161685234,
      "grad_norm": 9440.717451549961,
      "learning_rate": 2.635377797418451e-07,
      "loss": 1.7917,
      "step": 144128
    },
    {
      "epoch": 0.00052318525051936,
      "grad_norm": 8646.257109293016,
      "learning_rate": 2.6350849939291393e-07,
      "loss": 1.7599,
      "step": 144160
    },
    {
      "epoch": 0.0005233013848701968,
      "grad_norm": 10894.898806322157,
      "learning_rate": 2.634792288013887e-07,
      "loss": 1.7304,
      "step": 144192
    },
    {
      "epoch": 0.0005234175192210334,
      "grad_norm": 9672.309341620541,
      "learning_rate": 2.6344996796185137e-07,
      "loss": 1.7478,
      "step": 144224
    },
    {
      "epoch": 0.0005235336535718702,
      "grad_norm": 9521.273864352395,
      "learning_rate": 2.6342071686888805e-07,
      "loss": 1.7706,
      "step": 144256
    },
    {
      "epoch": 0.000523649787922707,
      "grad_norm": 9276.15469901187,
      "learning_rate": 2.633923891619373e-07,
      "loss": 1.7854,
      "step": 144288
    },
    {
      "epoch": 0.0005237659222735436,
      "grad_norm": 10576.010779117049,
      "learning_rate": 2.6336315724173665e-07,
      "loss": 1.7534,
      "step": 144320
    },
    {
      "epoch": 0.0005238820566243804,
      "grad_norm": 10554.01459161394,
      "learning_rate": 2.633339350520624e-07,
      "loss": 1.7474,
      "step": 144352
    },
    {
      "epoch": 0.000523998190975217,
      "grad_norm": 8770.80737446673,
      "learning_rate": 2.6330472258751736e-07,
      "loss": 1.7455,
      "step": 144384
    },
    {
      "epoch": 0.0005241143253260538,
      "grad_norm": 10822.69448889693,
      "learning_rate": 2.6327551984270854e-07,
      "loss": 1.7338,
      "step": 144416
    },
    {
      "epoch": 0.0005242304596768904,
      "grad_norm": 11066.213805995256,
      "learning_rate": 2.632463268122471e-07,
      "loss": 1.7479,
      "step": 144448
    },
    {
      "epoch": 0.0005243465940277272,
      "grad_norm": 9618.101163951229,
      "learning_rate": 2.6321714349074845e-07,
      "loss": 1.7482,
      "step": 144480
    },
    {
      "epoch": 0.0005244627283785638,
      "grad_norm": 10617.599257836018,
      "learning_rate": 2.6318796987283204e-07,
      "loss": 1.7533,
      "step": 144512
    },
    {
      "epoch": 0.0005245788627294006,
      "grad_norm": 9197.46986948041,
      "learning_rate": 2.6315880595312175e-07,
      "loss": 1.7588,
      "step": 144544
    },
    {
      "epoch": 0.0005246949970802373,
      "grad_norm": 8526.320542883665,
      "learning_rate": 2.6312965172624537e-07,
      "loss": 1.7872,
      "step": 144576
    },
    {
      "epoch": 0.000524811131431074,
      "grad_norm": 9009.980466127548,
      "learning_rate": 2.6310050718683493e-07,
      "loss": 1.7342,
      "step": 144608
    },
    {
      "epoch": 0.0005249272657819107,
      "grad_norm": 10344.67843869494,
      "learning_rate": 2.630713723295267e-07,
      "loss": 1.7308,
      "step": 144640
    },
    {
      "epoch": 0.0005250434001327474,
      "grad_norm": 9431.889100281025,
      "learning_rate": 2.63042247148961e-07,
      "loss": 1.732,
      "step": 144672
    },
    {
      "epoch": 0.0005251595344835841,
      "grad_norm": 9694.29770535236,
      "learning_rate": 2.6301313163978246e-07,
      "loss": 1.7631,
      "step": 144704
    },
    {
      "epoch": 0.0005252756688344208,
      "grad_norm": 9813.344180247628,
      "learning_rate": 2.629840257966396e-07,
      "loss": 1.7575,
      "step": 144736
    },
    {
      "epoch": 0.0005253918031852575,
      "grad_norm": 10341.893830435507,
      "learning_rate": 2.6295492961418533e-07,
      "loss": 1.7456,
      "step": 144768
    },
    {
      "epoch": 0.0005255079375360942,
      "grad_norm": 9597.07913898807,
      "learning_rate": 2.629258430870766e-07,
      "loss": 1.7628,
      "step": 144800
    },
    {
      "epoch": 0.0005256240718869309,
      "grad_norm": 9142.402200734772,
      "learning_rate": 2.6289676620997446e-07,
      "loss": 1.7557,
      "step": 144832
    },
    {
      "epoch": 0.0005257402062377677,
      "grad_norm": 9057.49137454737,
      "learning_rate": 2.628676989775442e-07,
      "loss": 1.7378,
      "step": 144864
    },
    {
      "epoch": 0.0005258563405886043,
      "grad_norm": 9969.817450685845,
      "learning_rate": 2.6283864138445516e-07,
      "loss": 1.7685,
      "step": 144896
    },
    {
      "epoch": 0.0005259724749394411,
      "grad_norm": 9604.013744263384,
      "learning_rate": 2.6280959342538075e-07,
      "loss": 1.7628,
      "step": 144928
    },
    {
      "epoch": 0.0005260886092902777,
      "grad_norm": 10028.035600255915,
      "learning_rate": 2.6278055509499864e-07,
      "loss": 1.7516,
      "step": 144960
    },
    {
      "epoch": 0.0005262047436411145,
      "grad_norm": 8943.070166335496,
      "learning_rate": 2.627515263879906e-07,
      "loss": 1.7514,
      "step": 144992
    },
    {
      "epoch": 0.0005263208779919511,
      "grad_norm": 10550.705474042958,
      "learning_rate": 2.627225072990423e-07,
      "loss": 1.7622,
      "step": 145024
    },
    {
      "epoch": 0.0005264370123427879,
      "grad_norm": 10728.794340465289,
      "learning_rate": 2.626934978228437e-07,
      "loss": 1.7509,
      "step": 145056
    },
    {
      "epoch": 0.0005265531466936245,
      "grad_norm": 9677.674204063702,
      "learning_rate": 2.6266449795408897e-07,
      "loss": 1.7493,
      "step": 145088
    },
    {
      "epoch": 0.0005266692810444613,
      "grad_norm": 11256.727588424621,
      "learning_rate": 2.626355076874761e-07,
      "loss": 1.7574,
      "step": 145120
    },
    {
      "epoch": 0.000526785415395298,
      "grad_norm": 10337.341050773162,
      "learning_rate": 2.6260652701770743e-07,
      "loss": 1.7827,
      "step": 145152
    },
    {
      "epoch": 0.0005269015497461347,
      "grad_norm": 9855.385634261096,
      "learning_rate": 2.6257755593948917e-07,
      "loss": 1.7635,
      "step": 145184
    },
    {
      "epoch": 0.0005270176840969714,
      "grad_norm": 9568.001881270719,
      "learning_rate": 2.6254859444753183e-07,
      "loss": 1.7715,
      "step": 145216
    },
    {
      "epoch": 0.0005271338184478081,
      "grad_norm": 10211.467083627113,
      "learning_rate": 2.625196425365498e-07,
      "loss": 1.7488,
      "step": 145248
    },
    {
      "epoch": 0.0005272499527986448,
      "grad_norm": 10564.017606952386,
      "learning_rate": 2.624916045043474e-07,
      "loss": 1.749,
      "step": 145280
    },
    {
      "epoch": 0.0005273660871494815,
      "grad_norm": 9232.494570808043,
      "learning_rate": 2.6246267144048025e-07,
      "loss": 1.7551,
      "step": 145312
    },
    {
      "epoch": 0.0005274822215003182,
      "grad_norm": 8108.664748280076,
      "learning_rate": 2.6243374794192117e-07,
      "loss": 1.7226,
      "step": 145344
    },
    {
      "epoch": 0.0005275983558511549,
      "grad_norm": 11183.849963228226,
      "learning_rate": 2.624048340034007e-07,
      "loss": 1.7386,
      "step": 145376
    },
    {
      "epoch": 0.0005277144902019916,
      "grad_norm": 10015.116973855074,
      "learning_rate": 2.623759296196537e-07,
      "loss": 1.748,
      "step": 145408
    },
    {
      "epoch": 0.0005278306245528283,
      "grad_norm": 9857.637749481364,
      "learning_rate": 2.623470347854188e-07,
      "loss": 1.7604,
      "step": 145440
    },
    {
      "epoch": 0.000527946758903665,
      "grad_norm": 8638.137183444125,
      "learning_rate": 2.62318149495439e-07,
      "loss": 1.7679,
      "step": 145472
    },
    {
      "epoch": 0.0005280628932545018,
      "grad_norm": 9524.997847768786,
      "learning_rate": 2.622892737444611e-07,
      "loss": 1.794,
      "step": 145504
    },
    {
      "epoch": 0.0005281790276053384,
      "grad_norm": 9944.593405464097,
      "learning_rate": 2.6226040752723594e-07,
      "loss": 1.7111,
      "step": 145536
    },
    {
      "epoch": 0.0005282951619561752,
      "grad_norm": 8868.383956505266,
      "learning_rate": 2.622315508385187e-07,
      "loss": 1.7242,
      "step": 145568
    },
    {
      "epoch": 0.0005284112963070118,
      "grad_norm": 9519.768064401569,
      "learning_rate": 2.622027036730682e-07,
      "loss": 1.7332,
      "step": 145600
    },
    {
      "epoch": 0.0005285274306578486,
      "grad_norm": 10356.276357842136,
      "learning_rate": 2.621738660256476e-07,
      "loss": 1.7459,
      "step": 145632
    },
    {
      "epoch": 0.0005286435650086852,
      "grad_norm": 9914.961522870373,
      "learning_rate": 2.621450378910239e-07,
      "loss": 1.7429,
      "step": 145664
    },
    {
      "epoch": 0.000528759699359522,
      "grad_norm": 9372.417510973355,
      "learning_rate": 2.6211621926396825e-07,
      "loss": 1.7542,
      "step": 145696
    },
    {
      "epoch": 0.0005288758337103586,
      "grad_norm": 8458.53462486263,
      "learning_rate": 2.6208741013925583e-07,
      "loss": 1.7529,
      "step": 145728
    },
    {
      "epoch": 0.0005289919680611954,
      "grad_norm": 8967.94959843107,
      "learning_rate": 2.620586105116657e-07,
      "loss": 1.7533,
      "step": 145760
    },
    {
      "epoch": 0.0005291081024120321,
      "grad_norm": 10673.073409285631,
      "learning_rate": 2.6202982037598104e-07,
      "loss": 1.7757,
      "step": 145792
    },
    {
      "epoch": 0.0005292242367628688,
      "grad_norm": 10727.577825399356,
      "learning_rate": 2.620010397269891e-07,
      "loss": 1.7784,
      "step": 145824
    },
    {
      "epoch": 0.0005293403711137055,
      "grad_norm": 9377.919598716977,
      "learning_rate": 2.6197226855948096e-07,
      "loss": 1.7653,
      "step": 145856
    },
    {
      "epoch": 0.0005294565054645422,
      "grad_norm": 8698.962007044289,
      "learning_rate": 2.6194350686825185e-07,
      "loss": 1.778,
      "step": 145888
    },
    {
      "epoch": 0.0005295726398153789,
      "grad_norm": 9715.923116204656,
      "learning_rate": 2.6191475464810094e-07,
      "loss": 1.7834,
      "step": 145920
    },
    {
      "epoch": 0.0005296887741662156,
      "grad_norm": 9634.670207121777,
      "learning_rate": 2.618860118938314e-07,
      "loss": 1.7475,
      "step": 145952
    },
    {
      "epoch": 0.0005298049085170523,
      "grad_norm": 9339.191185536358,
      "learning_rate": 2.6185727860025044e-07,
      "loss": 1.7278,
      "step": 145984
    },
    {
      "epoch": 0.000529921042867889,
      "grad_norm": 9784.643580631846,
      "learning_rate": 2.618285547621692e-07,
      "loss": 1.7449,
      "step": 146016
    },
    {
      "epoch": 0.0005300371772187257,
      "grad_norm": 9571.604149775521,
      "learning_rate": 2.617998403744028e-07,
      "loss": 1.7255,
      "step": 146048
    },
    {
      "epoch": 0.0005301533115695625,
      "grad_norm": 11661.315020185331,
      "learning_rate": 2.6177113543177037e-07,
      "loss": 1.7527,
      "step": 146080
    },
    {
      "epoch": 0.0005302694459203991,
      "grad_norm": 10903.73284705747,
      "learning_rate": 2.61742439929095e-07,
      "loss": 1.7671,
      "step": 146112
    },
    {
      "epoch": 0.0005303855802712359,
      "grad_norm": 10161.280824777947,
      "learning_rate": 2.6171375386120374e-07,
      "loss": 1.7551,
      "step": 146144
    },
    {
      "epoch": 0.0005305017146220725,
      "grad_norm": 8121.031215307573,
      "learning_rate": 2.616850772229277e-07,
      "loss": 1.7628,
      "step": 146176
    },
    {
      "epoch": 0.0005306178489729093,
      "grad_norm": 9648.778368270256,
      "learning_rate": 2.6165641000910177e-07,
      "loss": 1.7172,
      "step": 146208
    },
    {
      "epoch": 0.0005307339833237459,
      "grad_norm": 9562.665318832402,
      "learning_rate": 2.61627752214565e-07,
      "loss": 1.7278,
      "step": 146240
    },
    {
      "epoch": 0.0005308501176745827,
      "grad_norm": 10837.457635441995,
      "learning_rate": 2.615991038341602e-07,
      "loss": 1.7511,
      "step": 146272
    },
    {
      "epoch": 0.0005309662520254193,
      "grad_norm": 10088.607436113272,
      "learning_rate": 2.615713596882215e-07,
      "loss": 1.7558,
      "step": 146304
    },
    {
      "epoch": 0.0005310823863762561,
      "grad_norm": 10973.772368698013,
      "learning_rate": 2.6154272982683364e-07,
      "loss": 1.7416,
      "step": 146336
    },
    {
      "epoch": 0.0005311985207270928,
      "grad_norm": 9580.92918249582,
      "learning_rate": 2.615141093642909e-07,
      "loss": 1.7503,
      "step": 146368
    },
    {
      "epoch": 0.0005313146550779295,
      "grad_norm": 8194.445435781485,
      "learning_rate": 2.61485498295452e-07,
      "loss": 1.762,
      "step": 146400
    },
    {
      "epoch": 0.0005314307894287662,
      "grad_norm": 9097.338511894564,
      "learning_rate": 2.614568966151793e-07,
      "loss": 1.7663,
      "step": 146432
    },
    {
      "epoch": 0.0005315469237796029,
      "grad_norm": 9946.470630329131,
      "learning_rate": 2.614283043183394e-07,
      "loss": 1.7563,
      "step": 146464
    },
    {
      "epoch": 0.0005316630581304396,
      "grad_norm": 10165.075405524545,
      "learning_rate": 2.613997213998026e-07,
      "loss": 1.7641,
      "step": 146496
    },
    {
      "epoch": 0.0005317791924812763,
      "grad_norm": 9336.347572793122,
      "learning_rate": 2.613711478544432e-07,
      "loss": 1.7625,
      "step": 146528
    },
    {
      "epoch": 0.000531895326832113,
      "grad_norm": 10401.528926076204,
      "learning_rate": 2.6134258367713946e-07,
      "loss": 1.7559,
      "step": 146560
    },
    {
      "epoch": 0.0005320114611829497,
      "grad_norm": 10911.409258203086,
      "learning_rate": 2.6131402886277347e-07,
      "loss": 1.7643,
      "step": 146592
    },
    {
      "epoch": 0.0005321275955337864,
      "grad_norm": 8333.42330618096,
      "learning_rate": 2.612854834062313e-07,
      "loss": 1.7549,
      "step": 146624
    },
    {
      "epoch": 0.0005322437298846232,
      "grad_norm": 10311.29245051269,
      "learning_rate": 2.6125694730240287e-07,
      "loss": 1.7265,
      "step": 146656
    },
    {
      "epoch": 0.0005323598642354598,
      "grad_norm": 8605.059906822264,
      "learning_rate": 2.6122842054618204e-07,
      "loss": 1.7481,
      "step": 146688
    },
    {
      "epoch": 0.0005324759985862966,
      "grad_norm": 9938.222577503484,
      "learning_rate": 2.611999031324667e-07,
      "loss": 1.746,
      "step": 146720
    },
    {
      "epoch": 0.0005325921329371332,
      "grad_norm": 9722.852770663556,
      "learning_rate": 2.611713950561584e-07,
      "loss": 1.7546,
      "step": 146752
    },
    {
      "epoch": 0.00053270826728797,
      "grad_norm": 7625.40136648557,
      "learning_rate": 2.6114289631216276e-07,
      "loss": 1.7478,
      "step": 146784
    },
    {
      "epoch": 0.0005328244016388066,
      "grad_norm": 10139.880275427318,
      "learning_rate": 2.611144068953892e-07,
      "loss": 1.7549,
      "step": 146816
    },
    {
      "epoch": 0.0005329405359896434,
      "grad_norm": 10111.662276797026,
      "learning_rate": 2.610859268007511e-07,
      "loss": 1.7906,
      "step": 146848
    },
    {
      "epoch": 0.00053305667034048,
      "grad_norm": 9653.72156217487,
      "learning_rate": 2.610574560231656e-07,
      "loss": 1.7514,
      "step": 146880
    },
    {
      "epoch": 0.0005331728046913168,
      "grad_norm": 10310.189232017034,
      "learning_rate": 2.6102899455755383e-07,
      "loss": 1.7183,
      "step": 146912
    },
    {
      "epoch": 0.0005332889390421535,
      "grad_norm": 8996.654600461217,
      "learning_rate": 2.610005423988408e-07,
      "loss": 1.7406,
      "step": 146944
    },
    {
      "epoch": 0.0005334050733929902,
      "grad_norm": 9318.683598019627,
      "learning_rate": 2.609720995419553e-07,
      "loss": 1.7485,
      "step": 146976
    },
    {
      "epoch": 0.0005335212077438269,
      "grad_norm": 10218.59618538672,
      "learning_rate": 2.6094366598183015e-07,
      "loss": 1.7599,
      "step": 147008
    },
    {
      "epoch": 0.0005336373420946636,
      "grad_norm": 8853.02626224502,
      "learning_rate": 2.6091524171340183e-07,
      "loss": 1.7794,
      "step": 147040
    },
    {
      "epoch": 0.0005337534764455003,
      "grad_norm": 9626.554523815881,
      "learning_rate": 2.6088682673161076e-07,
      "loss": 1.7616,
      "step": 147072
    },
    {
      "epoch": 0.000533869610796337,
      "grad_norm": 10694.35252832073,
      "learning_rate": 2.608584210314013e-07,
      "loss": 1.7583,
      "step": 147104
    },
    {
      "epoch": 0.0005339857451471737,
      "grad_norm": 10179.602349797364,
      "learning_rate": 2.6083002460772165e-07,
      "loss": 1.7468,
      "step": 147136
    },
    {
      "epoch": 0.0005341018794980104,
      "grad_norm": 10146.785106623674,
      "learning_rate": 2.6080163745552364e-07,
      "loss": 1.7572,
      "step": 147168
    },
    {
      "epoch": 0.0005342180138488471,
      "grad_norm": 9963.808006982068,
      "learning_rate": 2.6077325956976326e-07,
      "loss": 1.7488,
      "step": 147200
    },
    {
      "epoch": 0.0005343341481996839,
      "grad_norm": 9034.387859727964,
      "learning_rate": 2.6074489094540004e-07,
      "loss": 1.7495,
      "step": 147232
    },
    {
      "epoch": 0.0005344502825505205,
      "grad_norm": 8818.617351943558,
      "learning_rate": 2.607165315773977e-07,
      "loss": 1.7582,
      "step": 147264
    },
    {
      "epoch": 0.0005345664169013573,
      "grad_norm": 9835.049262713432,
      "learning_rate": 2.606890672618847e-07,
      "loss": 1.752,
      "step": 147296
    },
    {
      "epoch": 0.0005346825512521939,
      "grad_norm": 9945.770759473597,
      "learning_rate": 2.6066072610263895e-07,
      "loss": 1.7357,
      "step": 147328
    },
    {
      "epoch": 0.0005347986856030307,
      "grad_norm": 9413.93148477298,
      "learning_rate": 2.606323941848244e-07,
      "loss": 1.7256,
      "step": 147360
    },
    {
      "epoch": 0.0005349148199538673,
      "grad_norm": 10493.5706982895,
      "learning_rate": 2.6060407150342e-07,
      "loss": 1.7199,
      "step": 147392
    },
    {
      "epoch": 0.0005350309543047041,
      "grad_norm": 8766.919071144663,
      "learning_rate": 2.60575758053408e-07,
      "loss": 1.7497,
      "step": 147424
    },
    {
      "epoch": 0.0005351470886555407,
      "grad_norm": 9861.89880296893,
      "learning_rate": 2.6054745382977493e-07,
      "loss": 1.7519,
      "step": 147456
    },
    {
      "epoch": 0.0005352632230063775,
      "grad_norm": 10697.856233844237,
      "learning_rate": 2.6051915882751087e-07,
      "loss": 1.7518,
      "step": 147488
    },
    {
      "epoch": 0.0005353793573572142,
      "grad_norm": 10373.767782247682,
      "learning_rate": 2.604908730416098e-07,
      "loss": 1.7703,
      "step": 147520
    },
    {
      "epoch": 0.0005354954917080509,
      "grad_norm": 9094.98026385984,
      "learning_rate": 2.604625964670695e-07,
      "loss": 1.7711,
      "step": 147552
    },
    {
      "epoch": 0.0005356116260588876,
      "grad_norm": 8929.904814722271,
      "learning_rate": 2.604343290988915e-07,
      "loss": 1.7302,
      "step": 147584
    },
    {
      "epoch": 0.0005357277604097243,
      "grad_norm": 9142.4795323807,
      "learning_rate": 2.604060709320812e-07,
      "loss": 1.7597,
      "step": 147616
    },
    {
      "epoch": 0.000535843894760561,
      "grad_norm": 9331.594933343387,
      "learning_rate": 2.6037782196164777e-07,
      "loss": 1.7878,
      "step": 147648
    },
    {
      "epoch": 0.0005359600291113977,
      "grad_norm": 9862.912957133913,
      "learning_rate": 2.6034958218260404e-07,
      "loss": 1.7786,
      "step": 147680
    },
    {
      "epoch": 0.0005360761634622344,
      "grad_norm": 9810.0540263548,
      "learning_rate": 2.6032135158996694e-07,
      "loss": 1.7608,
      "step": 147712
    },
    {
      "epoch": 0.0005361922978130711,
      "grad_norm": 9988.787514007894,
      "learning_rate": 2.6029313017875686e-07,
      "loss": 1.7551,
      "step": 147744
    },
    {
      "epoch": 0.0005363084321639078,
      "grad_norm": 9561.636993737004,
      "learning_rate": 2.60264917943998e-07,
      "loss": 1.749,
      "step": 147776
    },
    {
      "epoch": 0.0005364245665147446,
      "grad_norm": 9123.561037226638,
      "learning_rate": 2.6023671488071865e-07,
      "loss": 1.7371,
      "step": 147808
    },
    {
      "epoch": 0.0005365407008655812,
      "grad_norm": 10130.184401085698,
      "learning_rate": 2.602085209839505e-07,
      "loss": 1.7493,
      "step": 147840
    },
    {
      "epoch": 0.000536656835216418,
      "grad_norm": 9718.870304721635,
      "learning_rate": 2.601803362487291e-07,
      "loss": 1.7551,
      "step": 147872
    },
    {
      "epoch": 0.0005367729695672546,
      "grad_norm": 10677.680459725325,
      "learning_rate": 2.6015216067009397e-07,
      "loss": 1.7419,
      "step": 147904
    },
    {
      "epoch": 0.0005368891039180914,
      "grad_norm": 9766.325921245922,
      "learning_rate": 2.6012399424308814e-07,
      "loss": 1.765,
      "step": 147936
    },
    {
      "epoch": 0.000537005238268928,
      "grad_norm": 9735.201590105877,
      "learning_rate": 2.6009583696275855e-07,
      "loss": 1.7655,
      "step": 147968
    },
    {
      "epoch": 0.0005371213726197648,
      "grad_norm": 9710.183314438507,
      "learning_rate": 2.6006768882415575e-07,
      "loss": 1.7474,
      "step": 148000
    },
    {
      "epoch": 0.0005372375069706014,
      "grad_norm": 7641.604150962022,
      "learning_rate": 2.600395498223342e-07,
      "loss": 1.7228,
      "step": 148032
    },
    {
      "epoch": 0.0005373536413214382,
      "grad_norm": 9400.780818634163,
      "learning_rate": 2.60011419952352e-07,
      "loss": 1.7227,
      "step": 148064
    },
    {
      "epoch": 0.000537469775672275,
      "grad_norm": 9738.812350589777,
      "learning_rate": 2.5998329920927096e-07,
      "loss": 1.7326,
      "step": 148096
    },
    {
      "epoch": 0.0005375859100231116,
      "grad_norm": 8174.3175861964155,
      "learning_rate": 2.5995518758815677e-07,
      "loss": 1.7346,
      "step": 148128
    },
    {
      "epoch": 0.0005377020443739483,
      "grad_norm": 11425.36546461425,
      "learning_rate": 2.5992708508407875e-07,
      "loss": 1.7579,
      "step": 148160
    },
    {
      "epoch": 0.000537818178724785,
      "grad_norm": 9971.511018897787,
      "learning_rate": 2.5989899169210997e-07,
      "loss": 1.7602,
      "step": 148192
    },
    {
      "epoch": 0.0005379343130756217,
      "grad_norm": 9779.50162329349,
      "learning_rate": 2.598709074073272e-07,
      "loss": 1.7866,
      "step": 148224
    },
    {
      "epoch": 0.0005380504474264584,
      "grad_norm": 8789.45072231479,
      "learning_rate": 2.5984283222481105e-07,
      "loss": 1.7681,
      "step": 148256
    },
    {
      "epoch": 0.0005381665817772951,
      "grad_norm": 18500.76495715785,
      "learning_rate": 2.5981476613964564e-07,
      "loss": 1.7475,
      "step": 148288
    },
    {
      "epoch": 0.0005382827161281318,
      "grad_norm": 10221.009930530348,
      "learning_rate": 2.597875857903607e-07,
      "loss": 1.7427,
      "step": 148320
    },
    {
      "epoch": 0.0005383988504789685,
      "grad_norm": 10204.436780146174,
      "learning_rate": 2.597595376012534e-07,
      "loss": 1.7516,
      "step": 148352
    },
    {
      "epoch": 0.0005385149848298053,
      "grad_norm": 10392.013953031434,
      "learning_rate": 2.597314984949251e-07,
      "loss": 1.7565,
      "step": 148384
    },
    {
      "epoch": 0.0005386311191806419,
      "grad_norm": 10519.914828552557,
      "learning_rate": 2.5970346846647477e-07,
      "loss": 1.7507,
      "step": 148416
    },
    {
      "epoch": 0.0005387472535314787,
      "grad_norm": 10122.215864127775,
      "learning_rate": 2.5967544751100503e-07,
      "loss": 1.765,
      "step": 148448
    },
    {
      "epoch": 0.0005388633878823153,
      "grad_norm": 8703.413812981662,
      "learning_rate": 2.596474356236224e-07,
      "loss": 1.7427,
      "step": 148480
    },
    {
      "epoch": 0.0005389795222331521,
      "grad_norm": 9530.975186202091,
      "learning_rate": 2.5961943279943673e-07,
      "loss": 1.7525,
      "step": 148512
    },
    {
      "epoch": 0.0005390956565839887,
      "grad_norm": 9946.232553082598,
      "learning_rate": 2.595914390335619e-07,
      "loss": 1.7642,
      "step": 148544
    },
    {
      "epoch": 0.0005392117909348255,
      "grad_norm": 9536.772200278247,
      "learning_rate": 2.5956345432111534e-07,
      "loss": 1.7879,
      "step": 148576
    },
    {
      "epoch": 0.0005393279252856621,
      "grad_norm": 9334.019070046943,
      "learning_rate": 2.5953547865721817e-07,
      "loss": 1.7657,
      "step": 148608
    },
    {
      "epoch": 0.0005394440596364989,
      "grad_norm": 9123.994958350208,
      "learning_rate": 2.595075120369952e-07,
      "loss": 1.7485,
      "step": 148640
    },
    {
      "epoch": 0.0005395601939873357,
      "grad_norm": 8959.53960870758,
      "learning_rate": 2.594795544555749e-07,
      "loss": 1.7527,
      "step": 148672
    },
    {
      "epoch": 0.0005396763283381723,
      "grad_norm": 10666.681208323422,
      "learning_rate": 2.594516059080893e-07,
      "loss": 1.7137,
      "step": 148704
    },
    {
      "epoch": 0.000539792462689009,
      "grad_norm": 8814.070455810981,
      "learning_rate": 2.594236663896745e-07,
      "loss": 1.7248,
      "step": 148736
    },
    {
      "epoch": 0.0005399085970398457,
      "grad_norm": 9269.597833778982,
      "learning_rate": 2.593957358954697e-07,
      "loss": 1.7302,
      "step": 148768
    },
    {
      "epoch": 0.0005400247313906825,
      "grad_norm": 10415.029236636832,
      "learning_rate": 2.5936781442061823e-07,
      "loss": 1.7514,
      "step": 148800
    },
    {
      "epoch": 0.0005401408657415191,
      "grad_norm": 12488.516965596837,
      "learning_rate": 2.593399019602668e-07,
      "loss": 1.7687,
      "step": 148832
    },
    {
      "epoch": 0.0005402570000923559,
      "grad_norm": 10075.65948213813,
      "learning_rate": 2.593119985095659e-07,
      "loss": 1.8003,
      "step": 148864
    },
    {
      "epoch": 0.0005403731344431925,
      "grad_norm": 9407.779121556798,
      "learning_rate": 2.592841040636697e-07,
      "loss": 1.7747,
      "step": 148896
    },
    {
      "epoch": 0.0005404892687940293,
      "grad_norm": 9967.80677982875,
      "learning_rate": 2.5925621861773594e-07,
      "loss": 1.7311,
      "step": 148928
    },
    {
      "epoch": 0.000540605403144866,
      "grad_norm": 10099.818810255954,
      "learning_rate": 2.59228342166926e-07,
      "loss": 1.7194,
      "step": 148960
    },
    {
      "epoch": 0.0005407215374957027,
      "grad_norm": 9284.43557789056,
      "learning_rate": 2.592004747064049e-07,
      "loss": 1.7432,
      "step": 148992
    },
    {
      "epoch": 0.0005408376718465394,
      "grad_norm": 10081.175427498521,
      "learning_rate": 2.591726162313414e-07,
      "loss": 1.7458,
      "step": 149024
    },
    {
      "epoch": 0.000540953806197376,
      "grad_norm": 11057.302202617057,
      "learning_rate": 2.5914476673690777e-07,
      "loss": 1.74,
      "step": 149056
    },
    {
      "epoch": 0.0005410699405482128,
      "grad_norm": 8961.524870243902,
      "learning_rate": 2.5911692621828e-07,
      "loss": 1.7402,
      "step": 149088
    },
    {
      "epoch": 0.0005411860748990495,
      "grad_norm": 9485.029572963913,
      "learning_rate": 2.5908909467063756e-07,
      "loss": 1.7485,
      "step": 149120
    },
    {
      "epoch": 0.0005413022092498862,
      "grad_norm": 8758.11634999216,
      "learning_rate": 2.590612720891638e-07,
      "loss": 1.7566,
      "step": 149152
    },
    {
      "epoch": 0.0005414183436007229,
      "grad_norm": 9334.019284316912,
      "learning_rate": 2.5903345846904556e-07,
      "loss": 1.7644,
      "step": 149184
    },
    {
      "epoch": 0.0005415344779515596,
      "grad_norm": 9569.115319610271,
      "learning_rate": 2.590056538054731e-07,
      "loss": 1.76,
      "step": 149216
    },
    {
      "epoch": 0.0005416506123023964,
      "grad_norm": 9820.338588867493,
      "learning_rate": 2.5897785809364057e-07,
      "loss": 1.7573,
      "step": 149248
    },
    {
      "epoch": 0.000541766746653233,
      "grad_norm": 10640.23401998283,
      "learning_rate": 2.589500713287457e-07,
      "loss": 1.7593,
      "step": 149280
    },
    {
      "epoch": 0.0005418828810040698,
      "grad_norm": 9051.505620613623,
      "learning_rate": 2.589231614276437e-07,
      "loss": 1.7676,
      "step": 149312
    },
    {
      "epoch": 0.0005419990153549064,
      "grad_norm": 9941.303737438064,
      "learning_rate": 2.5889539226301197e-07,
      "loss": 1.7628,
      "step": 149344
    },
    {
      "epoch": 0.0005421151497057432,
      "grad_norm": 9658.933067373435,
      "learning_rate": 2.588676320310821e-07,
      "loss": 1.7448,
      "step": 149376
    },
    {
      "epoch": 0.0005422312840565798,
      "grad_norm": 9046.683591239389,
      "learning_rate": 2.58839880727066e-07,
      "loss": 1.7536,
      "step": 149408
    },
    {
      "epoch": 0.0005423474184074166,
      "grad_norm": 8999.286305035528,
      "learning_rate": 2.5881213834617935e-07,
      "loss": 1.763,
      "step": 149440
    },
    {
      "epoch": 0.0005424635527582532,
      "grad_norm": 12394.283521043079,
      "learning_rate": 2.587844048836411e-07,
      "loss": 1.7644,
      "step": 149472
    },
    {
      "epoch": 0.00054257968710909,
      "grad_norm": 10932.651645415215,
      "learning_rate": 2.5875668033467415e-07,
      "loss": 1.748,
      "step": 149504
    },
    {
      "epoch": 0.0005426958214599267,
      "grad_norm": 8083.922191609714,
      "learning_rate": 2.5872896469450475e-07,
      "loss": 1.736,
      "step": 149536
    },
    {
      "epoch": 0.0005428119558107634,
      "grad_norm": 9078.769520149743,
      "learning_rate": 2.587012579583627e-07,
      "loss": 1.7554,
      "step": 149568
    },
    {
      "epoch": 0.0005429280901616001,
      "grad_norm": 9060.356615498089,
      "learning_rate": 2.586735601214815e-07,
      "loss": 1.7654,
      "step": 149600
    },
    {
      "epoch": 0.0005430442245124368,
      "grad_norm": 8451.18630725888,
      "learning_rate": 2.5864587117909824e-07,
      "loss": 1.7229,
      "step": 149632
    },
    {
      "epoch": 0.0005431603588632735,
      "grad_norm": 10374.075380485723,
      "learning_rate": 2.586181911264534e-07,
      "loss": 1.7235,
      "step": 149664
    },
    {
      "epoch": 0.0005432764932141102,
      "grad_norm": 9402.940816574355,
      "learning_rate": 2.5859051995879123e-07,
      "loss": 1.7485,
      "step": 149696
    },
    {
      "epoch": 0.0005433926275649469,
      "grad_norm": 9716.385130283794,
      "learning_rate": 2.585628576713594e-07,
      "loss": 1.7562,
      "step": 149728
    },
    {
      "epoch": 0.0005435087619157836,
      "grad_norm": 9308.576260631913,
      "learning_rate": 2.5853520425940924e-07,
      "loss": 1.7419,
      "step": 149760
    },
    {
      "epoch": 0.0005436248962666203,
      "grad_norm": 10437.74726653218,
      "learning_rate": 2.585075597181955e-07,
      "loss": 1.7631,
      "step": 149792
    },
    {
      "epoch": 0.0005437410306174571,
      "grad_norm": 8621.502305282997,
      "learning_rate": 2.584799240429766e-07,
      "loss": 1.7476,
      "step": 149824
    },
    {
      "epoch": 0.0005438571649682937,
      "grad_norm": 8218.600975835243,
      "learning_rate": 2.5845229722901453e-07,
      "loss": 1.7355,
      "step": 149856
    },
    {
      "epoch": 0.0005439732993191305,
      "grad_norm": 9073.805596330572,
      "learning_rate": 2.5842467927157463e-07,
      "loss": 1.7398,
      "step": 149888
    },
    {
      "epoch": 0.0005440894336699671,
      "grad_norm": 9965.774430519687,
      "learning_rate": 2.583970701659261e-07,
      "loss": 1.7632,
      "step": 149920
    },
    {
      "epoch": 0.0005442055680208039,
      "grad_norm": 9516.823944993414,
      "learning_rate": 2.583694699073413e-07,
      "loss": 1.7463,
      "step": 149952
    },
    {
      "epoch": 0.0005443217023716405,
      "grad_norm": 10856.822002777793,
      "learning_rate": 2.583418784910964e-07,
      "loss": 1.7741,
      "step": 149984
    },
    {
      "epoch": 0.0005444378367224773,
      "grad_norm": 9277.75145172579,
      "learning_rate": 2.5831429591247106e-07,
      "loss": 1.7743,
      "step": 150016
    },
    {
      "epoch": 0.0005445539710733139,
      "grad_norm": 9642.557129724459,
      "learning_rate": 2.582867221667484e-07,
      "loss": 1.76,
      "step": 150048
    },
    {
      "epoch": 0.0005446701054241507,
      "grad_norm": 9115.000493691703,
      "learning_rate": 2.5825915724921504e-07,
      "loss": 1.7377,
      "step": 150080
    },
    {
      "epoch": 0.0005447862397749874,
      "grad_norm": 8888.425282354574,
      "learning_rate": 2.5823160115516123e-07,
      "loss": 1.7461,
      "step": 150112
    },
    {
      "epoch": 0.0005449023741258241,
      "grad_norm": 9940.17062227807,
      "learning_rate": 2.582040538798806e-07,
      "loss": 1.7361,
      "step": 150144
    },
    {
      "epoch": 0.0005450185084766608,
      "grad_norm": 9226.331448631141,
      "learning_rate": 2.581765154186704e-07,
      "loss": 1.7406,
      "step": 150176
    },
    {
      "epoch": 0.0005451346428274975,
      "grad_norm": 9730.60131749318,
      "learning_rate": 2.581489857668314e-07,
      "loss": 1.7691,
      "step": 150208
    },
    {
      "epoch": 0.0005452507771783342,
      "grad_norm": 8119.994458126188,
      "learning_rate": 2.581214649196678e-07,
      "loss": 1.7622,
      "step": 150240
    },
    {
      "epoch": 0.0005453669115291709,
      "grad_norm": 9561.754964440366,
      "learning_rate": 2.580939528724874e-07,
      "loss": 1.7711,
      "step": 150272
    },
    {
      "epoch": 0.0005454830458800076,
      "grad_norm": 9957.047353507967,
      "learning_rate": 2.5806730896413743e-07,
      "loss": 1.74,
      "step": 150304
    },
    {
      "epoch": 0.0005455991802308443,
      "grad_norm": 9015.936113349517,
      "learning_rate": 2.5803981422822484e-07,
      "loss": 1.7242,
      "step": 150336
    },
    {
      "epoch": 0.000545715314581681,
      "grad_norm": 11856.455456838692,
      "learning_rate": 2.5801232827838584e-07,
      "loss": 1.7479,
      "step": 150368
    },
    {
      "epoch": 0.0005458314489325178,
      "grad_norm": 9015.334048164827,
      "learning_rate": 2.579848511099421e-07,
      "loss": 1.758,
      "step": 150400
    },
    {
      "epoch": 0.0005459475832833544,
      "grad_norm": 9140.371436653983,
      "learning_rate": 2.5795738271821866e-07,
      "loss": 1.7422,
      "step": 150432
    },
    {
      "epoch": 0.0005460637176341912,
      "grad_norm": 11099.875584888328,
      "learning_rate": 2.579299230985441e-07,
      "loss": 1.7448,
      "step": 150464
    },
    {
      "epoch": 0.0005461798519850278,
      "grad_norm": 9605.752443197774,
      "learning_rate": 2.579024722462505e-07,
      "loss": 1.7379,
      "step": 150496
    },
    {
      "epoch": 0.0005462959863358646,
      "grad_norm": 10864.951357461294,
      "learning_rate": 2.5787503015667336e-07,
      "loss": 1.7537,
      "step": 150528
    },
    {
      "epoch": 0.0005464121206867012,
      "grad_norm": 8813.414434826038,
      "learning_rate": 2.5784759682515177e-07,
      "loss": 1.763,
      "step": 150560
    },
    {
      "epoch": 0.000546528255037538,
      "grad_norm": 9812.84331883476,
      "learning_rate": 2.578201722470281e-07,
      "loss": 1.7797,
      "step": 150592
    },
    {
      "epoch": 0.0005466443893883746,
      "grad_norm": 8938.639941288608,
      "learning_rate": 2.5779275641764847e-07,
      "loss": 1.7638,
      "step": 150624
    },
    {
      "epoch": 0.0005467605237392114,
      "grad_norm": 9363.868644956528,
      "learning_rate": 2.577653493323621e-07,
      "loss": 1.7505,
      "step": 150656
    },
    {
      "epoch": 0.0005468766580900481,
      "grad_norm": 9640.276759512664,
      "learning_rate": 2.5773795098652197e-07,
      "loss": 1.7568,
      "step": 150688
    },
    {
      "epoch": 0.0005469927924408848,
      "grad_norm": 11207.960831480452,
      "learning_rate": 2.5771056137548443e-07,
      "loss": 1.7693,
      "step": 150720
    },
    {
      "epoch": 0.0005471089267917215,
      "grad_norm": 10225.79053178775,
      "learning_rate": 2.576831804946092e-07,
      "loss": 1.7051,
      "step": 150752
    },
    {
      "epoch": 0.0005472250611425582,
      "grad_norm": 8842.185024076345,
      "learning_rate": 2.5765580833925954e-07,
      "loss": 1.7166,
      "step": 150784
    },
    {
      "epoch": 0.0005473411954933949,
      "grad_norm": 10358.357591819275,
      "learning_rate": 2.576284449048021e-07,
      "loss": 1.74,
      "step": 150816
    },
    {
      "epoch": 0.0005474573298442316,
      "grad_norm": 10846.566645717898,
      "learning_rate": 2.5760109018660704e-07,
      "loss": 1.7394,
      "step": 150848
    },
    {
      "epoch": 0.0005475734641950683,
      "grad_norm": 10675.751964147537,
      "learning_rate": 2.5757374418004794e-07,
      "loss": 1.752,
      "step": 150880
    },
    {
      "epoch": 0.000547689598545905,
      "grad_norm": 9190.103590275792,
      "learning_rate": 2.575464068805017e-07,
      "loss": 1.7601,
      "step": 150912
    },
    {
      "epoch": 0.0005478057328967417,
      "grad_norm": 10183.460315629458,
      "learning_rate": 2.5751907828334886e-07,
      "loss": 1.767,
      "step": 150944
    },
    {
      "epoch": 0.0005479218672475785,
      "grad_norm": 8980.220153203372,
      "learning_rate": 2.574917583839733e-07,
      "loss": 1.749,
      "step": 150976
    },
    {
      "epoch": 0.0005480380015984151,
      "grad_norm": 9605.208066460611,
      "learning_rate": 2.5746444717776227e-07,
      "loss": 1.7418,
      "step": 151008
    },
    {
      "epoch": 0.0005481541359492519,
      "grad_norm": 10252.620250453052,
      "learning_rate": 2.5743714466010646e-07,
      "loss": 1.7597,
      "step": 151040
    },
    {
      "epoch": 0.0005482702703000885,
      "grad_norm": 10071.948967305187,
      "learning_rate": 2.5740985082640003e-07,
      "loss": 1.7511,
      "step": 151072
    },
    {
      "epoch": 0.0005483864046509253,
      "grad_norm": 8269.769767049142,
      "learning_rate": 2.573825656720406e-07,
      "loss": 1.757,
      "step": 151104
    },
    {
      "epoch": 0.0005485025390017619,
      "grad_norm": 11299.347591785996,
      "learning_rate": 2.573552891924289e-07,
      "loss": 1.7728,
      "step": 151136
    },
    {
      "epoch": 0.0005486186733525987,
      "grad_norm": 9686.828583184488,
      "learning_rate": 2.573280213829697e-07,
      "loss": 1.7878,
      "step": 151168
    },
    {
      "epoch": 0.0005487348077034353,
      "grad_norm": 10727.253143279504,
      "learning_rate": 2.573007622390705e-07,
      "loss": 1.7565,
      "step": 151200
    },
    {
      "epoch": 0.0005488509420542721,
      "grad_norm": 10170.790726388976,
      "learning_rate": 2.5727351175614256e-07,
      "loss": 1.746,
      "step": 151232
    },
    {
      "epoch": 0.0005489670764051088,
      "grad_norm": 10623.171654454238,
      "learning_rate": 2.572462699296005e-07,
      "loss": 1.7379,
      "step": 151264
    },
    {
      "epoch": 0.0005490832107559455,
      "grad_norm": 9600.625604615565,
      "learning_rate": 2.5721903675486234e-07,
      "loss": 1.7552,
      "step": 151296
    },
    {
      "epoch": 0.0005491993451067822,
      "grad_norm": 9450.71320060026,
      "learning_rate": 2.57192662862989e-07,
      "loss": 1.7672,
      "step": 151328
    },
    {
      "epoch": 0.0005493154794576189,
      "grad_norm": 11632.632032347623,
      "learning_rate": 2.5716544670811247e-07,
      "loss": 1.7518,
      "step": 151360
    },
    {
      "epoch": 0.0005494316138084556,
      "grad_norm": 12153.322837808597,
      "learning_rate": 2.57138239191457e-07,
      "loss": 1.759,
      "step": 151392
    },
    {
      "epoch": 0.0005495477481592923,
      "grad_norm": 7858.580915152556,
      "learning_rate": 2.5711104030845395e-07,
      "loss": 1.7173,
      "step": 151424
    },
    {
      "epoch": 0.000549663882510129,
      "grad_norm": 10708.185653975186,
      "learning_rate": 2.570838500545382e-07,
      "loss": 1.7391,
      "step": 151456
    },
    {
      "epoch": 0.0005497800168609657,
      "grad_norm": 9146.373926316373,
      "learning_rate": 2.570566684251479e-07,
      "loss": 1.7153,
      "step": 151488
    },
    {
      "epoch": 0.0005498961512118024,
      "grad_norm": 9485.272900660264,
      "learning_rate": 2.5702949541572476e-07,
      "loss": 1.7357,
      "step": 151520
    },
    {
      "epoch": 0.0005500122855626392,
      "grad_norm": 9158.883556416687,
      "learning_rate": 2.570023310217136e-07,
      "loss": 1.7415,
      "step": 151552
    },
    {
      "epoch": 0.0005501284199134758,
      "grad_norm": 9245.702461143772,
      "learning_rate": 2.569751752385628e-07,
      "loss": 1.7439,
      "step": 151584
    },
    {
      "epoch": 0.0005502445542643126,
      "grad_norm": 9432.234941942444,
      "learning_rate": 2.569480280617241e-07,
      "loss": 1.7715,
      "step": 151616
    },
    {
      "epoch": 0.0005503606886151492,
      "grad_norm": 10081.307653275939,
      "learning_rate": 2.569208894866524e-07,
      "loss": 1.7722,
      "step": 151648
    },
    {
      "epoch": 0.000550476822965986,
      "grad_norm": 9766.950803602935,
      "learning_rate": 2.568937595088063e-07,
      "loss": 1.7631,
      "step": 151680
    },
    {
      "epoch": 0.0005505929573168226,
      "grad_norm": 10244.960126813574,
      "learning_rate": 2.568666381236474e-07,
      "loss": 1.7721,
      "step": 151712
    },
    {
      "epoch": 0.0005507090916676594,
      "grad_norm": 10561.824274243536,
      "learning_rate": 2.56839525326641e-07,
      "loss": 1.7892,
      "step": 151744
    },
    {
      "epoch": 0.000550825226018496,
      "grad_norm": 10728.245429705643,
      "learning_rate": 2.5681242111325543e-07,
      "loss": 1.7849,
      "step": 151776
    },
    {
      "epoch": 0.0005509413603693328,
      "grad_norm": 10351.664407234231,
      "learning_rate": 2.567853254789625e-07,
      "loss": 1.7862,
      "step": 151808
    },
    {
      "epoch": 0.0005510574947201695,
      "grad_norm": 9482.651106098969,
      "learning_rate": 2.5675823841923747e-07,
      "loss": 1.7654,
      "step": 151840
    },
    {
      "epoch": 0.0005511736290710062,
      "grad_norm": 9826.80599177576,
      "learning_rate": 2.567311599295588e-07,
      "loss": 1.7592,
      "step": 151872
    },
    {
      "epoch": 0.0005512897634218429,
      "grad_norm": 9614.908319895723,
      "learning_rate": 2.5670409000540833e-07,
      "loss": 1.7685,
      "step": 151904
    },
    {
      "epoch": 0.0005514058977726796,
      "grad_norm": 9816.441717852756,
      "learning_rate": 2.566770286422712e-07,
      "loss": 1.7842,
      "step": 151936
    },
    {
      "epoch": 0.0005515220321235163,
      "grad_norm": 10671.523602560226,
      "learning_rate": 2.56649975835636e-07,
      "loss": 1.7838,
      "step": 151968
    },
    {
      "epoch": 0.000551638166474353,
      "grad_norm": 9991.697253219796,
      "learning_rate": 2.566229315809944e-07,
      "loss": 1.7714,
      "step": 152000
    },
    {
      "epoch": 0.0005517543008251897,
      "grad_norm": 12902.39264632727,
      "learning_rate": 2.565958958738418e-07,
      "loss": 1.7711,
      "step": 152032
    },
    {
      "epoch": 0.0005518704351760264,
      "grad_norm": 8062.358587907139,
      "learning_rate": 2.565688687096765e-07,
      "loss": 1.7528,
      "step": 152064
    },
    {
      "epoch": 0.0005519865695268631,
      "grad_norm": 9434.738894108305,
      "learning_rate": 2.565418500840003e-07,
      "loss": 1.7556,
      "step": 152096
    },
    {
      "epoch": 0.0005521027038776999,
      "grad_norm": 11118.564835445266,
      "learning_rate": 2.5651483999231836e-07,
      "loss": 1.748,
      "step": 152128
    },
    {
      "epoch": 0.0005522188382285365,
      "grad_norm": 9985.243912894666,
      "learning_rate": 2.564878384301391e-07,
      "loss": 1.7501,
      "step": 152160
    },
    {
      "epoch": 0.0005523349725793733,
      "grad_norm": 9838.058751603387,
      "learning_rate": 2.564608453929742e-07,
      "loss": 1.753,
      "step": 152192
    },
    {
      "epoch": 0.0005524511069302099,
      "grad_norm": 10201.969025634218,
      "learning_rate": 2.564338608763388e-07,
      "loss": 1.7663,
      "step": 152224
    },
    {
      "epoch": 0.0005525672412810467,
      "grad_norm": 10122.404457439943,
      "learning_rate": 2.564068848757512e-07,
      "loss": 1.7842,
      "step": 152256
    },
    {
      "epoch": 0.0005526833756318833,
      "grad_norm": 10138.370480506224,
      "learning_rate": 2.5637991738673305e-07,
      "loss": 1.7668,
      "step": 152288
    },
    {
      "epoch": 0.0005527995099827201,
      "grad_norm": 9104.164761250755,
      "learning_rate": 2.563538007442693e-07,
      "loss": 1.7765,
      "step": 152320
    },
    {
      "epoch": 0.0005529156443335567,
      "grad_norm": 9166.127644758173,
      "learning_rate": 2.563268499993288e-07,
      "loss": 1.773,
      "step": 152352
    },
    {
      "epoch": 0.0005530317786843935,
      "grad_norm": 10321.458230308352,
      "learning_rate": 2.5629990775268197e-07,
      "loss": 1.7746,
      "step": 152384
    },
    {
      "epoch": 0.0005531479130352302,
      "grad_norm": 9269.285733000142,
      "learning_rate": 2.5627297399986357e-07,
      "loss": 1.7514,
      "step": 152416
    },
    {
      "epoch": 0.0005532640473860669,
      "grad_norm": 8601.60357142783,
      "learning_rate": 2.5624604873641155e-07,
      "loss": 1.7495,
      "step": 152448
    },
    {
      "epoch": 0.0005533801817369036,
      "grad_norm": 10351.111534516474,
      "learning_rate": 2.5621913195786723e-07,
      "loss": 1.7427,
      "step": 152480
    },
    {
      "epoch": 0.0005534963160877403,
      "grad_norm": 11730.448755269339,
      "learning_rate": 2.561922236597751e-07,
      "loss": 1.7559,
      "step": 152512
    },
    {
      "epoch": 0.000553612450438577,
      "grad_norm": 9937.978466468923,
      "learning_rate": 2.561653238376831e-07,
      "loss": 1.7835,
      "step": 152544
    },
    {
      "epoch": 0.0005537285847894137,
      "grad_norm": 10617.490287257155,
      "learning_rate": 2.5613843248714214e-07,
      "loss": 1.7792,
      "step": 152576
    },
    {
      "epoch": 0.0005538447191402504,
      "grad_norm": 9759.827764873722,
      "learning_rate": 2.5611154960370673e-07,
      "loss": 1.7491,
      "step": 152608
    },
    {
      "epoch": 0.0005539608534910871,
      "grad_norm": 8932.756461473693,
      "learning_rate": 2.560846751829344e-07,
      "loss": 1.7548,
      "step": 152640
    },
    {
      "epoch": 0.0005540769878419238,
      "grad_norm": 10264.278834871935,
      "learning_rate": 2.5605780922038614e-07,
      "loss": 1.7664,
      "step": 152672
    },
    {
      "epoch": 0.0005541931221927606,
      "grad_norm": 10234.532622450328,
      "learning_rate": 2.5603095171162605e-07,
      "loss": 1.7604,
      "step": 152704
    },
    {
      "epoch": 0.0005543092565435972,
      "grad_norm": 8643.86348804746,
      "learning_rate": 2.560041026522215e-07,
      "loss": 1.7575,
      "step": 152736
    },
    {
      "epoch": 0.000554425390894434,
      "grad_norm": 10483.994467758937,
      "learning_rate": 2.5597726203774317e-07,
      "loss": 1.7547,
      "step": 152768
    },
    {
      "epoch": 0.0005545415252452706,
      "grad_norm": 9185.380340519385,
      "learning_rate": 2.55950429863765e-07,
      "loss": 1.7575,
      "step": 152800
    },
    {
      "epoch": 0.0005546576595961074,
      "grad_norm": 11045.793045318203,
      "learning_rate": 2.559236061258641e-07,
      "loss": 1.7789,
      "step": 152832
    },
    {
      "epoch": 0.000554773793946944,
      "grad_norm": 11451.834787491478,
      "learning_rate": 2.558967908196209e-07,
      "loss": 1.7985,
      "step": 152864
    },
    {
      "epoch": 0.0005548899282977808,
      "grad_norm": 9987.898277415525,
      "learning_rate": 2.5586998394061904e-07,
      "loss": 1.7769,
      "step": 152896
    },
    {
      "epoch": 0.0005550060626486174,
      "grad_norm": 10520.479836965613,
      "learning_rate": 2.558431854844454e-07,
      "loss": 1.7694,
      "step": 152928
    },
    {
      "epoch": 0.0005551221969994542,
      "grad_norm": 9355.93918321405,
      "learning_rate": 2.5581639544669015e-07,
      "loss": 1.7707,
      "step": 152960
    },
    {
      "epoch": 0.000555238331350291,
      "grad_norm": 9467.16050355121,
      "learning_rate": 2.557896138229465e-07,
      "loss": 1.7643,
      "step": 152992
    },
    {
      "epoch": 0.0005553544657011276,
      "grad_norm": 11240.113878426677,
      "learning_rate": 2.557628406088111e-07,
      "loss": 1.7427,
      "step": 153024
    },
    {
      "epoch": 0.0005554706000519644,
      "grad_norm": 9296.465027094977,
      "learning_rate": 2.5573607579988375e-07,
      "loss": 1.7506,
      "step": 153056
    },
    {
      "epoch": 0.000555586734402801,
      "grad_norm": 9940.67070171827,
      "learning_rate": 2.557093193917675e-07,
      "loss": 1.7447,
      "step": 153088
    },
    {
      "epoch": 0.0005557028687536378,
      "grad_norm": 8288.118845673003,
      "learning_rate": 2.5568257138006853e-07,
      "loss": 1.7635,
      "step": 153120
    },
    {
      "epoch": 0.0005558190031044744,
      "grad_norm": 8831.605176863377,
      "learning_rate": 2.5565583176039637e-07,
      "loss": 1.7945,
      "step": 153152
    },
    {
      "epoch": 0.0005559351374553112,
      "grad_norm": 8813.98831403809,
      "learning_rate": 2.556291005283636e-07,
      "loss": 1.7704,
      "step": 153184
    },
    {
      "epoch": 0.0005560512718061478,
      "grad_norm": 9836.394766376552,
      "learning_rate": 2.556023776795862e-07,
      "loss": 1.7405,
      "step": 153216
    },
    {
      "epoch": 0.0005561674061569846,
      "grad_norm": 10975.239405133721,
      "learning_rate": 2.5557566320968317e-07,
      "loss": 1.7453,
      "step": 153248
    },
    {
      "epoch": 0.0005562835405078213,
      "grad_norm": 8562.227280328407,
      "learning_rate": 2.555489571142769e-07,
      "loss": 1.7634,
      "step": 153280
    },
    {
      "epoch": 0.000556399674858658,
      "grad_norm": 11376.29570642395,
      "learning_rate": 2.5552309356625513e-07,
      "loss": 1.7548,
      "step": 153312
    },
    {
      "epoch": 0.0005565158092094947,
      "grad_norm": 10297.691197545206,
      "learning_rate": 2.5549640394535834e-07,
      "loss": 1.7415,
      "step": 153344
    },
    {
      "epoch": 0.0005566319435603314,
      "grad_norm": 9381.415458234434,
      "learning_rate": 2.554697226859808e-07,
      "loss": 1.7468,
      "step": 153376
    },
    {
      "epoch": 0.0005567480779111681,
      "grad_norm": 8283.08227654416,
      "learning_rate": 2.554430497837575e-07,
      "loss": 1.7561,
      "step": 153408
    },
    {
      "epoch": 0.0005568642122620048,
      "grad_norm": 9329.61767705408,
      "learning_rate": 2.554163852343266e-07,
      "loss": 1.7775,
      "step": 153440
    },
    {
      "epoch": 0.0005569803466128415,
      "grad_norm": 8824.221212095717,
      "learning_rate": 2.553897290333294e-07,
      "loss": 1.7857,
      "step": 153472
    },
    {
      "epoch": 0.0005570964809636782,
      "grad_norm": 9036.980690474003,
      "learning_rate": 2.553630811764105e-07,
      "loss": 1.7772,
      "step": 153504
    },
    {
      "epoch": 0.0005572126153145149,
      "grad_norm": 9666.684333317191,
      "learning_rate": 2.553364416592176e-07,
      "loss": 1.7774,
      "step": 153536
    },
    {
      "epoch": 0.0005573287496653517,
      "grad_norm": 9977.896171037259,
      "learning_rate": 2.5530981047740167e-07,
      "loss": 1.7771,
      "step": 153568
    },
    {
      "epoch": 0.0005574448840161883,
      "grad_norm": 10554.81994161909,
      "learning_rate": 2.552831876266167e-07,
      "loss": 1.7653,
      "step": 153600
    },
    {
      "epoch": 0.0005575610183670251,
      "grad_norm": 9598.952755379098,
      "learning_rate": 2.5525657310252e-07,
      "loss": 1.7556,
      "step": 153632
    },
    {
      "epoch": 0.0005576771527178617,
      "grad_norm": 9349.142420564573,
      "learning_rate": 2.5522996690077187e-07,
      "loss": 1.7542,
      "step": 153664
    },
    {
      "epoch": 0.0005577932870686985,
      "grad_norm": 10026.343102048722,
      "learning_rate": 2.5520336901703606e-07,
      "loss": 1.756,
      "step": 153696
    },
    {
      "epoch": 0.0005579094214195351,
      "grad_norm": 9531.117563014319,
      "learning_rate": 2.551767794469792e-07,
      "loss": 1.7916,
      "step": 153728
    },
    {
      "epoch": 0.0005580255557703719,
      "grad_norm": 9859.723119844693,
      "learning_rate": 2.5515019818627116e-07,
      "loss": 1.7853,
      "step": 153760
    },
    {
      "epoch": 0.0005581416901212085,
      "grad_norm": 9092.911140003514,
      "learning_rate": 2.551236252305851e-07,
      "loss": 1.7634,
      "step": 153792
    },
    {
      "epoch": 0.0005582578244720453,
      "grad_norm": 9489.72307288258,
      "learning_rate": 2.5509706057559714e-07,
      "loss": 1.7419,
      "step": 153824
    },
    {
      "epoch": 0.000558373958822882,
      "grad_norm": 10387.222150315261,
      "learning_rate": 2.5507050421698676e-07,
      "loss": 1.7472,
      "step": 153856
    },
    {
      "epoch": 0.0005584900931737187,
      "grad_norm": 9826.88801198019,
      "learning_rate": 2.550439561504364e-07,
      "loss": 1.76,
      "step": 153888
    },
    {
      "epoch": 0.0005586062275245554,
      "grad_norm": 9316.328032009178,
      "learning_rate": 2.5501741637163164e-07,
      "loss": 1.7546,
      "step": 153920
    },
    {
      "epoch": 0.0005587223618753921,
      "grad_norm": 10845.976396802642,
      "learning_rate": 2.549908848762614e-07,
      "loss": 1.7413,
      "step": 153952
    },
    {
      "epoch": 0.0005588384962262288,
      "grad_norm": 9992.071056592822,
      "learning_rate": 2.549643616600176e-07,
      "loss": 1.7559,
      "step": 153984
    },
    {
      "epoch": 0.0005589546305770655,
      "grad_norm": 9726.776033198255,
      "learning_rate": 2.549378467185953e-07,
      "loss": 1.7704,
      "step": 154016
    },
    {
      "epoch": 0.0005590707649279022,
      "grad_norm": 10764.170567210462,
      "learning_rate": 2.5491134004769265e-07,
      "loss": 1.7748,
      "step": 154048
    },
    {
      "epoch": 0.0005591868992787389,
      "grad_norm": 9758.462481354323,
      "learning_rate": 2.548848416430111e-07,
      "loss": 1.7792,
      "step": 154080
    },
    {
      "epoch": 0.0005593030336295756,
      "grad_norm": 10770.932828682946,
      "learning_rate": 2.5485835150025495e-07,
      "loss": 1.773,
      "step": 154112
    },
    {
      "epoch": 0.0005594191679804124,
      "grad_norm": 9808.013356434625,
      "learning_rate": 2.5483186961513197e-07,
      "loss": 1.7667,
      "step": 154144
    },
    {
      "epoch": 0.000559535302331249,
      "grad_norm": 11233.638235229047,
      "learning_rate": 2.5480539598335275e-07,
      "loss": 1.7519,
      "step": 154176
    },
    {
      "epoch": 0.0005596514366820858,
      "grad_norm": 10904.887894884569,
      "learning_rate": 2.5477893060063116e-07,
      "loss": 1.7542,
      "step": 154208
    },
    {
      "epoch": 0.0005597675710329224,
      "grad_norm": 9608.018422130548,
      "learning_rate": 2.547524734626841e-07,
      "loss": 1.7489,
      "step": 154240
    },
    {
      "epoch": 0.0005598837053837592,
      "grad_norm": 10720.60091599347,
      "learning_rate": 2.5472602456523174e-07,
      "loss": 1.7438,
      "step": 154272
    },
    {
      "epoch": 0.0005599998397345958,
      "grad_norm": 8587.501965065278,
      "learning_rate": 2.546995839039972e-07,
      "loss": 1.7687,
      "step": 154304
    },
    {
      "epoch": 0.0005601159740854326,
      "grad_norm": 10158.511702016196,
      "learning_rate": 2.546739773635598e-07,
      "loss": 1.7694,
      "step": 154336
    },
    {
      "epoch": 0.0005602321084362692,
      "grad_norm": 9701.534105490739,
      "learning_rate": 2.546475529048925e-07,
      "loss": 1.7698,
      "step": 154368
    },
    {
      "epoch": 0.000560348242787106,
      "grad_norm": 10260.656801589263,
      "learning_rate": 2.546211366697645e-07,
      "loss": 1.7705,
      "step": 154400
    },
    {
      "epoch": 0.0005604643771379427,
      "grad_norm": 8839.991628955313,
      "learning_rate": 2.545947286539112e-07,
      "loss": 1.7524,
      "step": 154432
    },
    {
      "epoch": 0.0005605805114887794,
      "grad_norm": 8625.443061083877,
      "learning_rate": 2.545683288530712e-07,
      "loss": 1.7588,
      "step": 154464
    },
    {
      "epoch": 0.0005606966458396161,
      "grad_norm": 9040.160175572111,
      "learning_rate": 2.545419372629862e-07,
      "loss": 1.7698,
      "step": 154496
    },
    {
      "epoch": 0.0005608127801904528,
      "grad_norm": 9129.363395111402,
      "learning_rate": 2.545155538794009e-07,
      "loss": 1.7577,
      "step": 154528
    },
    {
      "epoch": 0.0005609289145412895,
      "grad_norm": 8845.628637920541,
      "learning_rate": 2.544891786980631e-07,
      "loss": 1.7579,
      "step": 154560
    },
    {
      "epoch": 0.0005610450488921262,
      "grad_norm": 10055.387809527785,
      "learning_rate": 2.5446281171472386e-07,
      "loss": 1.7789,
      "step": 154592
    },
    {
      "epoch": 0.0005611611832429629,
      "grad_norm": 10345.700169635693,
      "learning_rate": 2.544364529251371e-07,
      "loss": 1.7665,
      "step": 154624
    },
    {
      "epoch": 0.0005612773175937996,
      "grad_norm": 10126.860717912536,
      "learning_rate": 2.5441010232505997e-07,
      "loss": 1.7626,
      "step": 154656
    },
    {
      "epoch": 0.0005613934519446363,
      "grad_norm": 10290.162292209001,
      "learning_rate": 2.543837599102526e-07,
      "loss": 1.7837,
      "step": 154688
    },
    {
      "epoch": 0.0005615095862954731,
      "grad_norm": 10619.178781807941,
      "learning_rate": 2.543574256764782e-07,
      "loss": 1.7786,
      "step": 154720
    },
    {
      "epoch": 0.0005616257206463097,
      "grad_norm": 10043.363579996494,
      "learning_rate": 2.5433109961950305e-07,
      "loss": 1.7607,
      "step": 154752
    },
    {
      "epoch": 0.0005617418549971465,
      "grad_norm": 9945.13448878395,
      "learning_rate": 2.5430478173509667e-07,
      "loss": 1.7492,
      "step": 154784
    },
    {
      "epoch": 0.0005618579893479831,
      "grad_norm": 9433.820647012535,
      "learning_rate": 2.5427847201903134e-07,
      "loss": 1.7591,
      "step": 154816
    },
    {
      "epoch": 0.0005619741236988199,
      "grad_norm": 10939.14749877704,
      "learning_rate": 2.5425217046708263e-07,
      "loss": 1.7453,
      "step": 154848
    },
    {
      "epoch": 0.0005620902580496565,
      "grad_norm": 10721.565184244322,
      "learning_rate": 2.5422587707502906e-07,
      "loss": 1.7715,
      "step": 154880
    },
    {
      "epoch": 0.0005622063924004933,
      "grad_norm": 9486.524758835556,
      "learning_rate": 2.541995918386523e-07,
      "loss": 1.7701,
      "step": 154912
    },
    {
      "epoch": 0.0005623225267513299,
      "grad_norm": 11473.752481206835,
      "learning_rate": 2.5417331475373694e-07,
      "loss": 1.7483,
      "step": 154944
    },
    {
      "epoch": 0.0005624386611021667,
      "grad_norm": 10398.84935942434,
      "learning_rate": 2.541470458160707e-07,
      "loss": 1.7562,
      "step": 154976
    },
    {
      "epoch": 0.0005625547954530034,
      "grad_norm": 9597.88080776168,
      "learning_rate": 2.541207850214444e-07,
      "loss": 1.759,
      "step": 155008
    },
    {
      "epoch": 0.0005626709298038401,
      "grad_norm": 9887.797934828563,
      "learning_rate": 2.540945323656518e-07,
      "loss": 1.7528,
      "step": 155040
    },
    {
      "epoch": 0.0005627870641546768,
      "grad_norm": 9279.440069314527,
      "learning_rate": 2.540682878444897e-07,
      "loss": 1.7466,
      "step": 155072
    },
    {
      "epoch": 0.0005629031985055135,
      "grad_norm": 10958.883154774487,
      "learning_rate": 2.540420514537581e-07,
      "loss": 1.7495,
      "step": 155104
    },
    {
      "epoch": 0.0005630193328563502,
      "grad_norm": 10489.025788890025,
      "learning_rate": 2.540158231892598e-07,
      "loss": 1.7541,
      "step": 155136
    },
    {
      "epoch": 0.0005631354672071869,
      "grad_norm": 8960.358140163818,
      "learning_rate": 2.5398960304680085e-07,
      "loss": 1.7547,
      "step": 155168
    },
    {
      "epoch": 0.0005632516015580236,
      "grad_norm": 9641.976560850997,
      "learning_rate": 2.539633910221901e-07,
      "loss": 1.7675,
      "step": 155200
    },
    {
      "epoch": 0.0005633677359088603,
      "grad_norm": 10997.543543901065,
      "learning_rate": 2.539371871112397e-07,
      "loss": 1.7707,
      "step": 155232
    },
    {
      "epoch": 0.000563483870259697,
      "grad_norm": 10762.741100667618,
      "learning_rate": 2.539109913097646e-07,
      "loss": 1.7779,
      "step": 155264
    },
    {
      "epoch": 0.0005636000046105338,
      "grad_norm": 9808.320549411097,
      "learning_rate": 2.538848036135829e-07,
      "loss": 1.7872,
      "step": 155296
    },
    {
      "epoch": 0.0005637161389613704,
      "grad_norm": 10034.093481725193,
      "learning_rate": 2.5385944200827865e-07,
      "loss": 1.7898,
      "step": 155328
    },
    {
      "epoch": 0.0005638322733122072,
      "grad_norm": 9953.256552505818,
      "learning_rate": 2.538332702571837e-07,
      "loss": 1.7601,
      "step": 155360
    },
    {
      "epoch": 0.0005639484076630438,
      "grad_norm": 9944.571785652714,
      "learning_rate": 2.538071065989848e-07,
      "loss": 1.7568,
      "step": 155392
    },
    {
      "epoch": 0.0005640645420138806,
      "grad_norm": 9493.738357464883,
      "learning_rate": 2.5378095102951184e-07,
      "loss": 1.7677,
      "step": 155424
    },
    {
      "epoch": 0.0005641806763647172,
      "grad_norm": 10034.81041176165,
      "learning_rate": 2.53754803544598e-07,
      "loss": 1.761,
      "step": 155456
    },
    {
      "epoch": 0.000564296810715554,
      "grad_norm": 10735.613443115395,
      "learning_rate": 2.5372866414007914e-07,
      "loss": 1.7646,
      "step": 155488
    },
    {
      "epoch": 0.0005644129450663906,
      "grad_norm": 8803.173064299031,
      "learning_rate": 2.5370253281179434e-07,
      "loss": 1.7594,
      "step": 155520
    },
    {
      "epoch": 0.0005645290794172274,
      "grad_norm": 10223.614624974867,
      "learning_rate": 2.536764095555857e-07,
      "loss": 1.7454,
      "step": 155552
    },
    {
      "epoch": 0.0005646452137680641,
      "grad_norm": 10759.49757191292,
      "learning_rate": 2.536502943672982e-07,
      "loss": 1.7489,
      "step": 155584
    },
    {
      "epoch": 0.0005647613481189008,
      "grad_norm": 9928.999546782143,
      "learning_rate": 2.536241872427799e-07,
      "loss": 1.7666,
      "step": 155616
    },
    {
      "epoch": 0.0005648774824697375,
      "grad_norm": 8860.812942388526,
      "learning_rate": 2.535980881778817e-07,
      "loss": 1.7514,
      "step": 155648
    },
    {
      "epoch": 0.0005649936168205742,
      "grad_norm": 10446.1682927282,
      "learning_rate": 2.535719971684577e-07,
      "loss": 1.7544,
      "step": 155680
    },
    {
      "epoch": 0.0005651097511714109,
      "grad_norm": 10273.307354498842,
      "learning_rate": 2.5354591421036486e-07,
      "loss": 1.7507,
      "step": 155712
    },
    {
      "epoch": 0.0005652258855222476,
      "grad_norm": 9294.986820862094,
      "learning_rate": 2.535198392994631e-07,
      "loss": 1.7706,
      "step": 155744
    },
    {
      "epoch": 0.0005653420198730843,
      "grad_norm": 8883.33450906809,
      "learning_rate": 2.5349377243161543e-07,
      "loss": 1.7438,
      "step": 155776
    },
    {
      "epoch": 0.000565458154223921,
      "grad_norm": 10061.950009814202,
      "learning_rate": 2.5346771360268773e-07,
      "loss": 1.7597,
      "step": 155808
    },
    {
      "epoch": 0.0005655742885747577,
      "grad_norm": 10840.315678060302,
      "learning_rate": 2.534416628085489e-07,
      "loss": 1.7627,
      "step": 155840
    },
    {
      "epoch": 0.0005656904229255945,
      "grad_norm": 10030.244064827137,
      "learning_rate": 2.5341562004507083e-07,
      "loss": 1.7693,
      "step": 155872
    },
    {
      "epoch": 0.0005658065572764311,
      "grad_norm": 10427.078977355068,
      "learning_rate": 2.533895853081284e-07,
      "loss": 1.7697,
      "step": 155904
    },
    {
      "epoch": 0.0005659226916272679,
      "grad_norm": 10350.168501043836,
      "learning_rate": 2.533635585935993e-07,
      "loss": 1.7734,
      "step": 155936
    },
    {
      "epoch": 0.0005660388259781045,
      "grad_norm": 10134.57705086897,
      "learning_rate": 2.5333753989736437e-07,
      "loss": 1.7524,
      "step": 155968
    },
    {
      "epoch": 0.0005661549603289413,
      "grad_norm": 10190.69722835489,
      "learning_rate": 2.5331152921530733e-07,
      "loss": 1.7602,
      "step": 156000
    },
    {
      "epoch": 0.0005662710946797779,
      "grad_norm": 9839.501511763692,
      "learning_rate": 2.5328552654331487e-07,
      "loss": 1.7555,
      "step": 156032
    },
    {
      "epoch": 0.0005663872290306147,
      "grad_norm": 11994.789702199869,
      "learning_rate": 2.532595318772766e-07,
      "loss": 1.7595,
      "step": 156064
    },
    {
      "epoch": 0.0005665033633814513,
      "grad_norm": 9770.056294617754,
      "learning_rate": 2.5323354521308516e-07,
      "loss": 1.7717,
      "step": 156096
    },
    {
      "epoch": 0.0005666194977322881,
      "grad_norm": 9598.408618099149,
      "learning_rate": 2.5320756654663604e-07,
      "loss": 1.76,
      "step": 156128
    },
    {
      "epoch": 0.0005667356320831248,
      "grad_norm": 9341.898201115231,
      "learning_rate": 2.5318159587382773e-07,
      "loss": 1.7537,
      "step": 156160
    },
    {
      "epoch": 0.0005668517664339615,
      "grad_norm": 9016.13442668198,
      "learning_rate": 2.531556331905617e-07,
      "loss": 1.7549,
      "step": 156192
    },
    {
      "epoch": 0.0005669679007847982,
      "grad_norm": 8659.307362601237,
      "learning_rate": 2.5312967849274235e-07,
      "loss": 1.7776,
      "step": 156224
    },
    {
      "epoch": 0.0005670840351356349,
      "grad_norm": 10100.898177884974,
      "learning_rate": 2.531037317762769e-07,
      "loss": 1.7733,
      "step": 156256
    },
    {
      "epoch": 0.0005672001694864716,
      "grad_norm": 8941.070964934794,
      "learning_rate": 2.5307779303707567e-07,
      "loss": 1.7666,
      "step": 156288
    },
    {
      "epoch": 0.0005673163038373083,
      "grad_norm": 11445.31834419646,
      "learning_rate": 2.530526724868429e-07,
      "loss": 1.7732,
      "step": 156320
    },
    {
      "epoch": 0.000567432438188145,
      "grad_norm": 10627.904779400313,
      "learning_rate": 2.5302674944094015e-07,
      "loss": 1.7538,
      "step": 156352
    },
    {
      "epoch": 0.0005675485725389817,
      "grad_norm": 10274.41034804431,
      "learning_rate": 2.530008343601774e-07,
      "loss": 1.7522,
      "step": 156384
    },
    {
      "epoch": 0.0005676647068898184,
      "grad_norm": 9528.173172229815,
      "learning_rate": 2.5297492724047657e-07,
      "loss": 1.756,
      "step": 156416
    },
    {
      "epoch": 0.0005677808412406551,
      "grad_norm": 9665.28230317149,
      "learning_rate": 2.5294902807776246e-07,
      "loss": 1.7638,
      "step": 156448
    },
    {
      "epoch": 0.0005678969755914918,
      "grad_norm": 9742.681355766492,
      "learning_rate": 2.5292313686796267e-07,
      "loss": 1.7703,
      "step": 156480
    },
    {
      "epoch": 0.0005680131099423286,
      "grad_norm": 9564.659638481653,
      "learning_rate": 2.5289725360700804e-07,
      "loss": 1.7693,
      "step": 156512
    },
    {
      "epoch": 0.0005681292442931652,
      "grad_norm": 10567.019447318151,
      "learning_rate": 2.528713782908319e-07,
      "loss": 1.7715,
      "step": 156544
    },
    {
      "epoch": 0.000568245378644002,
      "grad_norm": 10294.241885636842,
      "learning_rate": 2.5284551091537083e-07,
      "loss": 1.7565,
      "step": 156576
    },
    {
      "epoch": 0.0005683615129948386,
      "grad_norm": 11051.907165733886,
      "learning_rate": 2.528196514765642e-07,
      "loss": 1.7678,
      "step": 156608
    },
    {
      "epoch": 0.0005684776473456754,
      "grad_norm": 9455.536684926985,
      "learning_rate": 2.5279379997035427e-07,
      "loss": 1.7438,
      "step": 156640
    },
    {
      "epoch": 0.000568593781696512,
      "grad_norm": 9653.507963429667,
      "learning_rate": 2.527679563926862e-07,
      "loss": 1.764,
      "step": 156672
    },
    {
      "epoch": 0.0005687099160473488,
      "grad_norm": 10962.703316244584,
      "learning_rate": 2.5274212073950814e-07,
      "loss": 1.7491,
      "step": 156704
    },
    {
      "epoch": 0.0005688260503981854,
      "grad_norm": 11042.891288064011,
      "learning_rate": 2.5271629300677097e-07,
      "loss": 1.7526,
      "step": 156736
    },
    {
      "epoch": 0.0005689421847490222,
      "grad_norm": 10323.061367637025,
      "learning_rate": 2.526904731904286e-07,
      "loss": 1.7317,
      "step": 156768
    },
    {
      "epoch": 0.000569058319099859,
      "grad_norm": 11586.14621002169,
      "learning_rate": 2.5266466128643786e-07,
      "loss": 1.7478,
      "step": 156800
    },
    {
      "epoch": 0.0005691744534506956,
      "grad_norm": 10156.927094352897,
      "learning_rate": 2.5263885729075835e-07,
      "loss": 1.7667,
      "step": 156832
    },
    {
      "epoch": 0.0005692905878015323,
      "grad_norm": 10070.111915962007,
      "learning_rate": 2.526130611993526e-07,
      "loss": 1.7735,
      "step": 156864
    },
    {
      "epoch": 0.000569406722152369,
      "grad_norm": 9536.681603157358,
      "learning_rate": 2.5258727300818607e-07,
      "loss": 1.7356,
      "step": 156896
    },
    {
      "epoch": 0.0005695228565032057,
      "grad_norm": 8897.209225369492,
      "learning_rate": 2.525614927132271e-07,
      "loss": 1.7376,
      "step": 156928
    },
    {
      "epoch": 0.0005696389908540424,
      "grad_norm": 7439.037975437415,
      "learning_rate": 2.5253572031044684e-07,
      "loss": 1.7496,
      "step": 156960
    },
    {
      "epoch": 0.0005697551252048791,
      "grad_norm": 9930.054078402594,
      "learning_rate": 2.5250995579581935e-07,
      "loss": 1.7565,
      "step": 156992
    },
    {
      "epoch": 0.0005698712595557158,
      "grad_norm": 9879.727627824564,
      "learning_rate": 2.5248419916532165e-07,
      "loss": 1.7582,
      "step": 157024
    },
    {
      "epoch": 0.0005699873939065525,
      "grad_norm": 9120.974399700945,
      "learning_rate": 2.5245845041493345e-07,
      "loss": 1.7516,
      "step": 157056
    },
    {
      "epoch": 0.0005701035282573893,
      "grad_norm": 9441.054602108812,
      "learning_rate": 2.5243270954063754e-07,
      "loss": 1.766,
      "step": 157088
    },
    {
      "epoch": 0.000570219662608226,
      "grad_norm": 10882.580208755642,
      "learning_rate": 2.524069765384194e-07,
      "loss": 1.777,
      "step": 157120
    },
    {
      "epoch": 0.0005703357969590627,
      "grad_norm": 9459.691538311385,
      "learning_rate": 2.523812514042675e-07,
      "loss": 1.7876,
      "step": 157152
    },
    {
      "epoch": 0.0005704519313098993,
      "grad_norm": 11154.206919364551,
      "learning_rate": 2.523555341341731e-07,
      "loss": 1.7817,
      "step": 157184
    },
    {
      "epoch": 0.0005705680656607361,
      "grad_norm": 8425.28907515938,
      "learning_rate": 2.523298247241304e-07,
      "loss": 1.7411,
      "step": 157216
    },
    {
      "epoch": 0.0005706842000115727,
      "grad_norm": 10538.973764081586,
      "learning_rate": 2.523041231701363e-07,
      "loss": 1.7244,
      "step": 157248
    },
    {
      "epoch": 0.0005708003343624095,
      "grad_norm": 10757.647605308513,
      "learning_rate": 2.5227842946819063e-07,
      "loss": 1.7321,
      "step": 157280
    },
    {
      "epoch": 0.0005709164687132461,
      "grad_norm": 8690.849210520224,
      "learning_rate": 2.522527436142962e-07,
      "loss": 1.7262,
      "step": 157312
    },
    {
      "epoch": 0.0005710326030640829,
      "grad_norm": 10047.246587996136,
      "learning_rate": 2.522278679235723e-07,
      "loss": 1.7489,
      "step": 157344
    },
    {
      "epoch": 0.0005711487374149197,
      "grad_norm": 10284.523518374588,
      "learning_rate": 2.52202197508858e-07,
      "loss": 1.7193,
      "step": 157376
    },
    {
      "epoch": 0.0005712648717657563,
      "grad_norm": 10384.687188355747,
      "learning_rate": 2.521765349303447e-07,
      "loss": 1.7274,
      "step": 157408
    },
    {
      "epoch": 0.0005713810061165931,
      "grad_norm": 9603.146880059681,
      "learning_rate": 2.5215088018404634e-07,
      "loss": 1.7767,
      "step": 157440
    },
    {
      "epoch": 0.0005714971404674297,
      "grad_norm": 7419.068809493548,
      "learning_rate": 2.5212523326597977e-07,
      "loss": 1.776,
      "step": 157472
    },
    {
      "epoch": 0.0005716132748182665,
      "grad_norm": 10671.39934591523,
      "learning_rate": 2.520995941721647e-07,
      "loss": 1.7449,
      "step": 157504
    },
    {
      "epoch": 0.0005717294091691031,
      "grad_norm": 8744.424280648784,
      "learning_rate": 2.5207396289862343e-07,
      "loss": 1.7469,
      "step": 157536
    },
    {
      "epoch": 0.0005718455435199399,
      "grad_norm": 9485.864430825479,
      "learning_rate": 2.520483394413815e-07,
      "loss": 1.7509,
      "step": 157568
    },
    {
      "epoch": 0.0005719616778707765,
      "grad_norm": 10419.694621244906,
      "learning_rate": 2.5202272379646697e-07,
      "loss": 1.7264,
      "step": 157600
    },
    {
      "epoch": 0.0005720778122216133,
      "grad_norm": 8897.237548812553,
      "learning_rate": 2.519971159599108e-07,
      "loss": 1.7394,
      "step": 157632
    },
    {
      "epoch": 0.00057219394657245,
      "grad_norm": 9860.135090352464,
      "learning_rate": 2.519715159277468e-07,
      "loss": 1.7601,
      "step": 157664
    },
    {
      "epoch": 0.0005723100809232867,
      "grad_norm": 9775.710306673373,
      "learning_rate": 2.5194592369601164e-07,
      "loss": 1.7587,
      "step": 157696
    },
    {
      "epoch": 0.0005724262152741234,
      "grad_norm": 9317.962867494161,
      "learning_rate": 2.5192033926074475e-07,
      "loss": 1.7335,
      "step": 157728
    },
    {
      "epoch": 0.00057254234962496,
      "grad_norm": 9345.965332698383,
      "learning_rate": 2.518947626179883e-07,
      "loss": 1.7544,
      "step": 157760
    },
    {
      "epoch": 0.0005726584839757968,
      "grad_norm": 9551.47046270887,
      "learning_rate": 2.5186919376378737e-07,
      "loss": 1.7611,
      "step": 157792
    },
    {
      "epoch": 0.0005727746183266335,
      "grad_norm": 9639.327673650274,
      "learning_rate": 2.5184363269418993e-07,
      "loss": 1.7581,
      "step": 157824
    },
    {
      "epoch": 0.0005728907526774702,
      "grad_norm": 9110.343791537178,
      "learning_rate": 2.5181807940524656e-07,
      "loss": 1.7648,
      "step": 157856
    },
    {
      "epoch": 0.0005730068870283069,
      "grad_norm": 8915.59240880829,
      "learning_rate": 2.517925338930108e-07,
      "loss": 1.7629,
      "step": 157888
    },
    {
      "epoch": 0.0005731230213791436,
      "grad_norm": 10204.33398120622,
      "learning_rate": 2.517669961535389e-07,
      "loss": 1.7329,
      "step": 157920
    },
    {
      "epoch": 0.0005732391557299804,
      "grad_norm": 9702.180991921352,
      "learning_rate": 2.5174146618288995e-07,
      "loss": 1.7249,
      "step": 157952
    },
    {
      "epoch": 0.000573355290080817,
      "grad_norm": 8202.02706652447,
      "learning_rate": 2.517159439771259e-07,
      "loss": 1.7382,
      "step": 157984
    },
    {
      "epoch": 0.0005734714244316538,
      "grad_norm": 9021.621472883908,
      "learning_rate": 2.516904295323113e-07,
      "loss": 1.7631,
      "step": 158016
    },
    {
      "epoch": 0.0005735875587824904,
      "grad_norm": 8068.7406700178435,
      "learning_rate": 2.5166492284451374e-07,
      "loss": 1.7612,
      "step": 158048
    },
    {
      "epoch": 0.0005737036931333272,
      "grad_norm": 8376.01910217497,
      "learning_rate": 2.516394239098034e-07,
      "loss": 1.7321,
      "step": 158080
    },
    {
      "epoch": 0.0005738198274841638,
      "grad_norm": 9856.306610490565,
      "learning_rate": 2.516139327242534e-07,
      "loss": 1.7351,
      "step": 158112
    },
    {
      "epoch": 0.0005739359618350006,
      "grad_norm": 9226.439616666876,
      "learning_rate": 2.5158844928393945e-07,
      "loss": 1.7473,
      "step": 158144
    },
    {
      "epoch": 0.0005740520961858372,
      "grad_norm": 9751.698313627221,
      "learning_rate": 2.5156297358494025e-07,
      "loss": 1.7559,
      "step": 158176
    },
    {
      "epoch": 0.000574168230536674,
      "grad_norm": 10229.823458887255,
      "learning_rate": 2.5153750562333715e-07,
      "loss": 1.7589,
      "step": 158208
    },
    {
      "epoch": 0.0005742843648875107,
      "grad_norm": 9259.433675986886,
      "learning_rate": 2.515120453952144e-07,
      "loss": 1.7614,
      "step": 158240
    },
    {
      "epoch": 0.0005744004992383474,
      "grad_norm": 8155.7864121125685,
      "learning_rate": 2.514865928966588e-07,
      "loss": 1.7382,
      "step": 158272
    },
    {
      "epoch": 0.0005745166335891841,
      "grad_norm": 10582.334997532445,
      "learning_rate": 2.514611481237602e-07,
      "loss": 1.7343,
      "step": 158304
    },
    {
      "epoch": 0.0005746327679400208,
      "grad_norm": 20955.428127337316,
      "learning_rate": 2.51435711072611e-07,
      "loss": 1.7474,
      "step": 158336
    },
    {
      "epoch": 0.0005747489022908575,
      "grad_norm": 11054.795430038495,
      "learning_rate": 2.5141107628918804e-07,
      "loss": 1.762,
      "step": 158368
    },
    {
      "epoch": 0.0005748650366416942,
      "grad_norm": 8924.975966354195,
      "learning_rate": 2.513856544288245e-07,
      "loss": 1.7341,
      "step": 158400
    },
    {
      "epoch": 0.0005749811709925309,
      "grad_norm": 9860.925717193086,
      "learning_rate": 2.5136024027862616e-07,
      "loss": 1.7198,
      "step": 158432
    },
    {
      "epoch": 0.0005750973053433676,
      "grad_norm": 8458.775916171322,
      "learning_rate": 2.513348338346965e-07,
      "loss": 1.7456,
      "step": 158464
    },
    {
      "epoch": 0.0005752134396942043,
      "grad_norm": 8616.05025519234,
      "learning_rate": 2.5130943509314176e-07,
      "loss": 1.7437,
      "step": 158496
    },
    {
      "epoch": 0.0005753295740450411,
      "grad_norm": 9046.70138779876,
      "learning_rate": 2.5128404405007083e-07,
      "loss": 1.7389,
      "step": 158528
    },
    {
      "epoch": 0.0005754457083958777,
      "grad_norm": 9777.212077069822,
      "learning_rate": 2.512586607015955e-07,
      "loss": 1.747,
      "step": 158560
    },
    {
      "epoch": 0.0005755618427467145,
      "grad_norm": 10406.427629114614,
      "learning_rate": 2.5123328504383016e-07,
      "loss": 1.7467,
      "step": 158592
    },
    {
      "epoch": 0.0005756779770975511,
      "grad_norm": 9094.35396276173,
      "learning_rate": 2.51207917072892e-07,
      "loss": 1.7326,
      "step": 158624
    },
    {
      "epoch": 0.0005757941114483879,
      "grad_norm": 8491.159520348207,
      "learning_rate": 2.51182556784901e-07,
      "loss": 1.7472,
      "step": 158656
    },
    {
      "epoch": 0.0005759102457992245,
      "grad_norm": 8833.79193778074,
      "learning_rate": 2.511572041759798e-07,
      "loss": 1.7666,
      "step": 158688
    },
    {
      "epoch": 0.0005760263801500613,
      "grad_norm": 11784.00746775052,
      "learning_rate": 2.511318592422538e-07,
      "loss": 1.7537,
      "step": 158720
    },
    {
      "epoch": 0.0005761425145008979,
      "grad_norm": 9109.30249799621,
      "learning_rate": 2.511065219798514e-07,
      "loss": 1.7318,
      "step": 158752
    },
    {
      "epoch": 0.0005762586488517347,
      "grad_norm": 9254.571302875136,
      "learning_rate": 2.5108119238490313e-07,
      "loss": 1.7504,
      "step": 158784
    },
    {
      "epoch": 0.0005763747832025714,
      "grad_norm": 8534.560328452779,
      "learning_rate": 2.5105587045354287e-07,
      "loss": 1.7779,
      "step": 158816
    },
    {
      "epoch": 0.0005764909175534081,
      "grad_norm": 9349.629404420262,
      "learning_rate": 2.5103055618190694e-07,
      "loss": 1.7722,
      "step": 158848
    },
    {
      "epoch": 0.0005766070519042448,
      "grad_norm": 9183.458063278778,
      "learning_rate": 2.5100524956613437e-07,
      "loss": 1.7714,
      "step": 158880
    },
    {
      "epoch": 0.0005767231862550815,
      "grad_norm": 9750.799556959419,
      "learning_rate": 2.509799506023669e-07,
      "loss": 1.7535,
      "step": 158912
    },
    {
      "epoch": 0.0005768393206059182,
      "grad_norm": 9284.65745194727,
      "learning_rate": 2.509546592867492e-07,
      "loss": 1.7297,
      "step": 158944
    },
    {
      "epoch": 0.0005769554549567549,
      "grad_norm": 9589.75755689371,
      "learning_rate": 2.5092937561542846e-07,
      "loss": 1.7386,
      "step": 158976
    },
    {
      "epoch": 0.0005770715893075916,
      "grad_norm": 10146.571243528526,
      "learning_rate": 2.5090409958455464e-07,
      "loss": 1.742,
      "step": 159008
    },
    {
      "epoch": 0.0005771877236584283,
      "grad_norm": 8398.333406099093,
      "learning_rate": 2.5087883119028037e-07,
      "loss": 1.7461,
      "step": 159040
    },
    {
      "epoch": 0.000577303858009265,
      "grad_norm": 10608.080222170267,
      "learning_rate": 2.508535704287611e-07,
      "loss": 1.7292,
      "step": 159072
    },
    {
      "epoch": 0.0005774199923601018,
      "grad_norm": 11528.29250149388,
      "learning_rate": 2.508283172961549e-07,
      "loss": 1.7255,
      "step": 159104
    },
    {
      "epoch": 0.0005775361267109384,
      "grad_norm": 9530.368513336722,
      "learning_rate": 2.508030717886226e-07,
      "loss": 1.7387,
      "step": 159136
    },
    {
      "epoch": 0.0005776522610617752,
      "grad_norm": 7705.919283252323,
      "learning_rate": 2.507778339023277e-07,
      "loss": 1.7571,
      "step": 159168
    },
    {
      "epoch": 0.0005777683954126118,
      "grad_norm": 9870.808274908393,
      "learning_rate": 2.507526036334364e-07,
      "loss": 1.7492,
      "step": 159200
    },
    {
      "epoch": 0.0005778845297634486,
      "grad_norm": 10010.40998161414,
      "learning_rate": 2.507273809781176e-07,
      "loss": 1.7404,
      "step": 159232
    },
    {
      "epoch": 0.0005780006641142852,
      "grad_norm": 10707.922300801403,
      "learning_rate": 2.5070216593254294e-07,
      "loss": 1.7547,
      "step": 159264
    },
    {
      "epoch": 0.000578116798465122,
      "grad_norm": 9682.480260759636,
      "learning_rate": 2.5067695849288667e-07,
      "loss": 1.732,
      "step": 159296
    },
    {
      "epoch": 0.0005782329328159586,
      "grad_norm": 10895.657667162639,
      "learning_rate": 2.5065175865532584e-07,
      "loss": 1.7289,
      "step": 159328
    },
    {
      "epoch": 0.0005783490671667954,
      "grad_norm": 9662.722804675708,
      "learning_rate": 2.506273535585428e-07,
      "loss": 1.7389,
      "step": 159360
    },
    {
      "epoch": 0.0005784652015176321,
      "grad_norm": 10719.645143380447,
      "learning_rate": 2.5060216867644545e-07,
      "loss": 1.7572,
      "step": 159392
    },
    {
      "epoch": 0.0005785813358684688,
      "grad_norm": 10278.879316345729,
      "learning_rate": 2.505769913851099e-07,
      "loss": 1.75,
      "step": 159424
    },
    {
      "epoch": 0.0005786974702193055,
      "grad_norm": 9733.342694059427,
      "learning_rate": 2.5055182168072366e-07,
      "loss": 1.7451,
      "step": 159456
    },
    {
      "epoch": 0.0005788136045701422,
      "grad_norm": 9362.437609938985,
      "learning_rate": 2.5052665955947713e-07,
      "loss": 1.7506,
      "step": 159488
    },
    {
      "epoch": 0.0005789297389209789,
      "grad_norm": 9660.873666496214,
      "learning_rate": 2.505015050175632e-07,
      "loss": 1.7536,
      "step": 159520
    },
    {
      "epoch": 0.0005790458732718156,
      "grad_norm": 9861.976069733691,
      "learning_rate": 2.504763580511778e-07,
      "loss": 1.7562,
      "step": 159552
    },
    {
      "epoch": 0.0005791620076226523,
      "grad_norm": 10018.230682111487,
      "learning_rate": 2.504512186565191e-07,
      "loss": 1.768,
      "step": 159584
    },
    {
      "epoch": 0.000579278141973489,
      "grad_norm": 10990.828540196595,
      "learning_rate": 2.5042608682978814e-07,
      "loss": 1.7648,
      "step": 159616
    },
    {
      "epoch": 0.0005793942763243257,
      "grad_norm": 9303.485583371428,
      "learning_rate": 2.5040096256718873e-07,
      "loss": 1.7301,
      "step": 159648
    },
    {
      "epoch": 0.0005795104106751625,
      "grad_norm": 10136.546749263282,
      "learning_rate": 2.503758458649271e-07,
      "loss": 1.7358,
      "step": 159680
    },
    {
      "epoch": 0.0005796265450259991,
      "grad_norm": 10271.569695036878,
      "learning_rate": 2.503507367192124e-07,
      "loss": 1.7563,
      "step": 159712
    },
    {
      "epoch": 0.0005797426793768359,
      "grad_norm": 9812.459019022703,
      "learning_rate": 2.503256351262562e-07,
      "loss": 1.7555,
      "step": 159744
    },
    {
      "epoch": 0.0005798588137276725,
      "grad_norm": 9857.40361352826,
      "learning_rate": 2.503005410822729e-07,
      "loss": 1.7242,
      "step": 159776
    },
    {
      "epoch": 0.0005799749480785093,
      "grad_norm": 10328.851242998904,
      "learning_rate": 2.5027545458347956e-07,
      "loss": 1.7306,
      "step": 159808
    },
    {
      "epoch": 0.0005800910824293459,
      "grad_norm": 8717.288913417979,
      "learning_rate": 2.502503756260957e-07,
      "loss": 1.7406,
      "step": 159840
    },
    {
      "epoch": 0.0005802072167801827,
      "grad_norm": 9611.391262455192,
      "learning_rate": 2.502253042063438e-07,
      "loss": 1.7495,
      "step": 159872
    },
    {
      "epoch": 0.0005803233511310193,
      "grad_norm": 9957.516959563765,
      "learning_rate": 2.5020024032044863e-07,
      "loss": 1.7696,
      "step": 159904
    },
    {
      "epoch": 0.0005804394854818561,
      "grad_norm": 9157.106748313028,
      "learning_rate": 2.5017518396463794e-07,
      "loss": 1.7604,
      "step": 159936
    },
    {
      "epoch": 0.0005805556198326928,
      "grad_norm": 9638.245690995846,
      "learning_rate": 2.501501351351419e-07,
      "loss": 1.7232,
      "step": 159968
    },
    {
      "epoch": 0.0005806717541835295,
      "grad_norm": 10145.840921283952,
      "learning_rate": 2.501250938281934e-07,
      "loss": 1.7425,
      "step": 160000
    },
    {
      "epoch": 0.0005807878885343662,
      "grad_norm": 9897.459472005936,
      "learning_rate": 2.50100060040028e-07,
      "loss": 1.7722,
      "step": 160032
    },
    {
      "epoch": 0.0005809040228852029,
      "grad_norm": 9166.148809614646,
      "learning_rate": 2.500750337668839e-07,
      "loss": 1.7453,
      "step": 160064
    },
    {
      "epoch": 0.0005810201572360396,
      "grad_norm": 10226.204770099219,
      "learning_rate": 2.500500150050018e-07,
      "loss": 1.728,
      "step": 160096
    },
    {
      "epoch": 0.0005811362915868763,
      "grad_norm": 8659.31810248359,
      "learning_rate": 2.5002500375062514e-07,
      "loss": 1.718,
      "step": 160128
    },
    {
      "epoch": 0.000581252425937713,
      "grad_norm": 9903.516547166466,
      "learning_rate": 2.5e-07,
      "loss": 1.7386,
      "step": 160160
    },
    {
      "epoch": 0.0005813685602885497,
      "grad_norm": 9511.613217535709,
      "learning_rate": 2.4997500374937513e-07,
      "loss": 1.7555,
      "step": 160192
    },
    {
      "epoch": 0.0005814846946393864,
      "grad_norm": 9291.132008533728,
      "learning_rate": 2.4995001499500175e-07,
      "loss": 1.7658,
      "step": 160224
    },
    {
      "epoch": 0.0005816008289902232,
      "grad_norm": 9356.964571911129,
      "learning_rate": 2.4992503373313385e-07,
      "loss": 1.7427,
      "step": 160256
    },
    {
      "epoch": 0.0005817169633410598,
      "grad_norm": 9376.149636177955,
      "learning_rate": 2.4990005996002797e-07,
      "loss": 1.7425,
      "step": 160288
    },
    {
      "epoch": 0.0005818330976918966,
      "grad_norm": 9043.075472426402,
      "learning_rate": 2.498750936719433e-07,
      "loss": 1.7278,
      "step": 160320
    },
    {
      "epoch": 0.0005819492320427332,
      "grad_norm": 9942.066284228848,
      "learning_rate": 2.498501348651416e-07,
      "loss": 1.7318,
      "step": 160352
    },
    {
      "epoch": 0.00058206536639357,
      "grad_norm": 8882.28292726594,
      "learning_rate": 2.49825963151778e-07,
      "loss": 1.7501,
      "step": 160384
    },
    {
      "epoch": 0.0005821815007444066,
      "grad_norm": 8943.004528680503,
      "learning_rate": 2.498010190628379e-07,
      "loss": 1.7544,
      "step": 160416
    },
    {
      "epoch": 0.0005822976350952434,
      "grad_norm": 10867.642798693745,
      "learning_rate": 2.497760824440983e-07,
      "loss": 1.7285,
      "step": 160448
    },
    {
      "epoch": 0.00058241376944608,
      "grad_norm": 9794.821897308802,
      "learning_rate": 2.497511532918314e-07,
      "loss": 1.7366,
      "step": 160480
    },
    {
      "epoch": 0.0005825299037969168,
      "grad_norm": 8319.184094609278,
      "learning_rate": 2.49726231602312e-07,
      "loss": 1.7771,
      "step": 160512
    },
    {
      "epoch": 0.0005826460381477535,
      "grad_norm": 8854.077591708805,
      "learning_rate": 2.497013173718173e-07,
      "loss": 1.7688,
      "step": 160544
    },
    {
      "epoch": 0.0005827621724985902,
      "grad_norm": 9617.161535505162,
      "learning_rate": 2.496764105966274e-07,
      "loss": 1.77,
      "step": 160576
    },
    {
      "epoch": 0.0005828783068494269,
      "grad_norm": 9631.84509842221,
      "learning_rate": 2.496515112730248e-07,
      "loss": 1.7838,
      "step": 160608
    },
    {
      "epoch": 0.0005829944412002636,
      "grad_norm": 8829.51833340868,
      "learning_rate": 2.496266193972946e-07,
      "loss": 1.743,
      "step": 160640
    },
    {
      "epoch": 0.0005831105755511003,
      "grad_norm": 9821.067355435456,
      "learning_rate": 2.496017349657246e-07,
      "loss": 1.7175,
      "step": 160672
    },
    {
      "epoch": 0.000583226709901937,
      "grad_norm": 8402.449999851235,
      "learning_rate": 2.495768579746051e-07,
      "loss": 1.7213,
      "step": 160704
    },
    {
      "epoch": 0.0005833428442527737,
      "grad_norm": 10326.12202135923,
      "learning_rate": 2.49551988420229e-07,
      "loss": 1.7396,
      "step": 160736
    },
    {
      "epoch": 0.0005834589786036104,
      "grad_norm": 9560.923386368077,
      "learning_rate": 2.495271262988918e-07,
      "loss": 1.7405,
      "step": 160768
    },
    {
      "epoch": 0.0005835751129544471,
      "grad_norm": 9207.942006767853,
      "learning_rate": 2.495022716068916e-07,
      "loss": 1.7333,
      "step": 160800
    },
    {
      "epoch": 0.0005836912473052839,
      "grad_norm": 9741.325782458976,
      "learning_rate": 2.4947742434052906e-07,
      "loss": 1.7547,
      "step": 160832
    },
    {
      "epoch": 0.0005838073816561205,
      "grad_norm": 9947.865097597574,
      "learning_rate": 2.4945258449610736e-07,
      "loss": 1.751,
      "step": 160864
    },
    {
      "epoch": 0.0005839235160069573,
      "grad_norm": 9243.84671010938,
      "learning_rate": 2.494277520699324e-07,
      "loss": 1.755,
      "step": 160896
    },
    {
      "epoch": 0.0005840396503577939,
      "grad_norm": 7971.118491152919,
      "learning_rate": 2.494029270583125e-07,
      "loss": 1.7391,
      "step": 160928
    },
    {
      "epoch": 0.0005841557847086307,
      "grad_norm": 9671.764575298552,
      "learning_rate": 2.4937810945755867e-07,
      "loss": 1.7475,
      "step": 160960
    },
    {
      "epoch": 0.0005842719190594673,
      "grad_norm": 10650.368068757061,
      "learning_rate": 2.493532992639844e-07,
      "loss": 1.7129,
      "step": 160992
    },
    {
      "epoch": 0.0005843880534103041,
      "grad_norm": 10259.210690886506,
      "learning_rate": 2.493284964739058e-07,
      "loss": 1.7219,
      "step": 161024
    },
    {
      "epoch": 0.0005845041877611407,
      "grad_norm": 10064.626769036196,
      "learning_rate": 2.493037010836415e-07,
      "loss": 1.7275,
      "step": 161056
    },
    {
      "epoch": 0.0005846203221119775,
      "grad_norm": 8991.415350210444,
      "learning_rate": 2.492789130895128e-07,
      "loss": 1.7494,
      "step": 161088
    },
    {
      "epoch": 0.0005847364564628142,
      "grad_norm": 11776.593565203819,
      "learning_rate": 2.492541324878435e-07,
      "loss": 1.7514,
      "step": 161120
    },
    {
      "epoch": 0.0005848525908136509,
      "grad_norm": 9940.990695096742,
      "learning_rate": 2.492293592749597e-07,
      "loss": 1.7491,
      "step": 161152
    },
    {
      "epoch": 0.0005849687251644876,
      "grad_norm": 9695.905940137827,
      "learning_rate": 2.4920459344719064e-07,
      "loss": 1.7704,
      "step": 161184
    },
    {
      "epoch": 0.0005850848595153243,
      "grad_norm": 8089.4821836752935,
      "learning_rate": 2.4917983500086757e-07,
      "loss": 1.7712,
      "step": 161216
    },
    {
      "epoch": 0.000585200993866161,
      "grad_norm": 8631.334890965592,
      "learning_rate": 2.491550839323245e-07,
      "loss": 1.7636,
      "step": 161248
    },
    {
      "epoch": 0.0005853171282169977,
      "grad_norm": 10119.065371861177,
      "learning_rate": 2.4913034023789796e-07,
      "loss": 1.7457,
      "step": 161280
    },
    {
      "epoch": 0.0005854332625678344,
      "grad_norm": 10400.99091433119,
      "learning_rate": 2.491056039139272e-07,
      "loss": 1.7378,
      "step": 161312
    },
    {
      "epoch": 0.0005855493969186711,
      "grad_norm": 10956.73363735744,
      "learning_rate": 2.490808749567536e-07,
      "loss": 1.7254,
      "step": 161344
    },
    {
      "epoch": 0.0005856655312695078,
      "grad_norm": 8768.905518934504,
      "learning_rate": 2.490569258011176e-07,
      "loss": 1.738,
      "step": 161376
    },
    {
      "epoch": 0.0005857816656203446,
      "grad_norm": 10386.191987441787,
      "learning_rate": 2.49032211336645e-07,
      "loss": 1.7599,
      "step": 161408
    },
    {
      "epoch": 0.0005858977999711812,
      "grad_norm": 8860.688009404235,
      "learning_rate": 2.4900750422812395e-07,
      "loss": 1.7715,
      "step": 161440
    },
    {
      "epoch": 0.000586013934322018,
      "grad_norm": 10909.891841810348,
      "learning_rate": 2.4898280447190615e-07,
      "loss": 1.7477,
      "step": 161472
    },
    {
      "epoch": 0.0005861300686728546,
      "grad_norm": 9239.395759463927,
      "learning_rate": 2.4895811206434577e-07,
      "loss": 1.7201,
      "step": 161504
    },
    {
      "epoch": 0.0005862462030236914,
      "grad_norm": 11304.355797656053,
      "learning_rate": 2.489334270017996e-07,
      "loss": 1.7316,
      "step": 161536
    },
    {
      "epoch": 0.000586362337374528,
      "grad_norm": 8704.668862168164,
      "learning_rate": 2.4890874928062703e-07,
      "loss": 1.738,
      "step": 161568
    },
    {
      "epoch": 0.0005864784717253648,
      "grad_norm": 10042.428391579399,
      "learning_rate": 2.4888407889718975e-07,
      "loss": 1.7353,
      "step": 161600
    },
    {
      "epoch": 0.0005865946060762014,
      "grad_norm": 9758.453975912373,
      "learning_rate": 2.488594158478523e-07,
      "loss": 1.7422,
      "step": 161632
    },
    {
      "epoch": 0.0005867107404270382,
      "grad_norm": 9384.13522920466,
      "learning_rate": 2.4883476012898135e-07,
      "loss": 1.7116,
      "step": 161664
    },
    {
      "epoch": 0.000586826874777875,
      "grad_norm": 9446.749070447462,
      "learning_rate": 2.4881011173694645e-07,
      "loss": 1.7345,
      "step": 161696
    },
    {
      "epoch": 0.0005869430091287116,
      "grad_norm": 9160.532080616278,
      "learning_rate": 2.487854706681195e-07,
      "loss": 1.7549,
      "step": 161728
    },
    {
      "epoch": 0.0005870591434795484,
      "grad_norm": 8771.290783003376,
      "learning_rate": 2.487608369188748e-07,
      "loss": 1.77,
      "step": 161760
    },
    {
      "epoch": 0.000587175277830385,
      "grad_norm": 10354.274093339427,
      "learning_rate": 2.487362104855894e-07,
      "loss": 1.7405,
      "step": 161792
    },
    {
      "epoch": 0.0005872914121812218,
      "grad_norm": 8917.455915226046,
      "learning_rate": 2.4871159136464266e-07,
      "loss": 1.7194,
      "step": 161824
    },
    {
      "epoch": 0.0005874075465320584,
      "grad_norm": 8719.223474599099,
      "learning_rate": 2.4868697955241666e-07,
      "loss": 1.7216,
      "step": 161856
    },
    {
      "epoch": 0.0005875236808828952,
      "grad_norm": 9271.012673920794,
      "learning_rate": 2.4866237504529577e-07,
      "loss": 1.74,
      "step": 161888
    },
    {
      "epoch": 0.0005876398152337318,
      "grad_norm": 8711.445689436398,
      "learning_rate": 2.48637777839667e-07,
      "loss": 1.726,
      "step": 161920
    },
    {
      "epoch": 0.0005877559495845686,
      "grad_norm": 8783.281049812764,
      "learning_rate": 2.4861318793191975e-07,
      "loss": 1.7398,
      "step": 161952
    },
    {
      "epoch": 0.0005878720839354053,
      "grad_norm": 10321.480320186634,
      "learning_rate": 2.4858860531844606e-07,
      "loss": 1.7553,
      "step": 161984
    },
    {
      "epoch": 0.000587988218286242,
      "grad_norm": 9570.709064640927,
      "learning_rate": 2.485640299956403e-07,
      "loss": 1.7149,
      "step": 162016
    },
    {
      "epoch": 0.0005881043526370787,
      "grad_norm": 9868.388115594156,
      "learning_rate": 2.4853946195989954e-07,
      "loss": 1.7263,
      "step": 162048
    },
    {
      "epoch": 0.0005882204869879154,
      "grad_norm": 9145.359697682754,
      "learning_rate": 2.485149012076232e-07,
      "loss": 1.735,
      "step": 162080
    },
    {
      "epoch": 0.0005883366213387521,
      "grad_norm": 9338.720362019627,
      "learning_rate": 2.484903477352132e-07,
      "loss": 1.7512,
      "step": 162112
    },
    {
      "epoch": 0.0005884527556895888,
      "grad_norm": 9571.296568386124,
      "learning_rate": 2.484658015390739e-07,
      "loss": 1.7367,
      "step": 162144
    },
    {
      "epoch": 0.0005885688900404255,
      "grad_norm": 10606.55797136847,
      "learning_rate": 2.484412626156123e-07,
      "loss": 1.7323,
      "step": 162176
    },
    {
      "epoch": 0.0005886850243912622,
      "grad_norm": 10144.238759019821,
      "learning_rate": 2.4841673096123776e-07,
      "loss": 1.7688,
      "step": 162208
    },
    {
      "epoch": 0.0005888011587420989,
      "grad_norm": 10150.754356204272,
      "learning_rate": 2.4839220657236216e-07,
      "loss": 1.771,
      "step": 162240
    },
    {
      "epoch": 0.0005889172930929357,
      "grad_norm": 9919.197346559851,
      "learning_rate": 2.483676894453999e-07,
      "loss": 1.7778,
      "step": 162272
    },
    {
      "epoch": 0.0005890334274437723,
      "grad_norm": 10462.807271473559,
      "learning_rate": 2.4834317957676777e-07,
      "loss": 1.7872,
      "step": 162304
    },
    {
      "epoch": 0.0005891495617946091,
      "grad_norm": 10128.729930252854,
      "learning_rate": 2.483186769628851e-07,
      "loss": 1.7803,
      "step": 162336
    },
    {
      "epoch": 0.0005892656961454457,
      "grad_norm": 19056.634750133613,
      "learning_rate": 2.4829418160017365e-07,
      "loss": 1.7305,
      "step": 162368
    },
    {
      "epoch": 0.0005893818304962825,
      "grad_norm": 10345.788708455242,
      "learning_rate": 2.4827045862898576e-07,
      "loss": 1.7364,
      "step": 162400
    },
    {
      "epoch": 0.0005894979648471191,
      "grad_norm": 9891.913667233453,
      "learning_rate": 2.4824597753157033e-07,
      "loss": 1.7308,
      "step": 162432
    },
    {
      "epoch": 0.0005896140991979559,
      "grad_norm": 9210.326595729384,
      "learning_rate": 2.482215036747179e-07,
      "loss": 1.7261,
      "step": 162464
    },
    {
      "epoch": 0.0005897302335487925,
      "grad_norm": 10968.945984004115,
      "learning_rate": 2.481970370548599e-07,
      "loss": 1.714,
      "step": 162496
    },
    {
      "epoch": 0.0005898463678996293,
      "grad_norm": 9674.250978757993,
      "learning_rate": 2.481725776684305e-07,
      "loss": 1.7108,
      "step": 162528
    },
    {
      "epoch": 0.000589962502250466,
      "grad_norm": 9486.547527947141,
      "learning_rate": 2.481481255118661e-07,
      "loss": 1.755,
      "step": 162560
    },
    {
      "epoch": 0.0005900786366013027,
      "grad_norm": 9981.806249371904,
      "learning_rate": 2.4812368058160573e-07,
      "loss": 1.7616,
      "step": 162592
    },
    {
      "epoch": 0.0005901947709521394,
      "grad_norm": 9462.408361511354,
      "learning_rate": 2.4809924287409073e-07,
      "loss": 1.7567,
      "step": 162624
    },
    {
      "epoch": 0.0005903109053029761,
      "grad_norm": 10136.673024222493,
      "learning_rate": 2.48074812385765e-07,
      "loss": 1.7302,
      "step": 162656
    },
    {
      "epoch": 0.0005904270396538128,
      "grad_norm": 8475.327486298096,
      "learning_rate": 2.480503891130748e-07,
      "loss": 1.7169,
      "step": 162688
    },
    {
      "epoch": 0.0005905431740046495,
      "grad_norm": 8774.799143000368,
      "learning_rate": 2.4802597305246896e-07,
      "loss": 1.71,
      "step": 162720
    },
    {
      "epoch": 0.0005906593083554862,
      "grad_norm": 9930.51579727861,
      "learning_rate": 2.4800156420039863e-07,
      "loss": 1.7202,
      "step": 162752
    },
    {
      "epoch": 0.0005907754427063229,
      "grad_norm": 10117.42477115595,
      "learning_rate": 2.4797716255331745e-07,
      "loss": 1.7271,
      "step": 162784
    },
    {
      "epoch": 0.0005908915770571596,
      "grad_norm": 10828.059844681318,
      "learning_rate": 2.4795276810768153e-07,
      "loss": 1.7221,
      "step": 162816
    },
    {
      "epoch": 0.0005910077114079964,
      "grad_norm": 9564.421362528943,
      "learning_rate": 2.479283808599494e-07,
      "loss": 1.7227,
      "step": 162848
    },
    {
      "epoch": 0.000591123845758833,
      "grad_norm": 9322.169275442278,
      "learning_rate": 2.479040008065821e-07,
      "loss": 1.7521,
      "step": 162880
    },
    {
      "epoch": 0.0005912399801096698,
      "grad_norm": 8643.773481529928,
      "learning_rate": 2.4787962794404283e-07,
      "loss": 1.7548,
      "step": 162912
    },
    {
      "epoch": 0.0005913561144605064,
      "grad_norm": 8425.943982723835,
      "learning_rate": 2.4785526226879766e-07,
      "loss": 1.7643,
      "step": 162944
    },
    {
      "epoch": 0.0005914722488113432,
      "grad_norm": 10271.010661079074,
      "learning_rate": 2.478309037773148e-07,
      "loss": 1.7689,
      "step": 162976
    },
    {
      "epoch": 0.0005915883831621798,
      "grad_norm": 9437.91809669908,
      "learning_rate": 2.4780655246606477e-07,
      "loss": 1.7524,
      "step": 163008
    },
    {
      "epoch": 0.0005917045175130166,
      "grad_norm": 8564.58662166482,
      "learning_rate": 2.4778220833152094e-07,
      "loss": 1.7267,
      "step": 163040
    },
    {
      "epoch": 0.0005918206518638532,
      "grad_norm": 9285.879818304778,
      "learning_rate": 2.4775787137015874e-07,
      "loss": 1.7398,
      "step": 163072
    },
    {
      "epoch": 0.00059193678621469,
      "grad_norm": 8832.53530986432,
      "learning_rate": 2.4773354157845615e-07,
      "loss": 1.7595,
      "step": 163104
    },
    {
      "epoch": 0.0005920529205655267,
      "grad_norm": 10236.909299197683,
      "learning_rate": 2.4770921895289354e-07,
      "loss": 1.7778,
      "step": 163136
    },
    {
      "epoch": 0.0005921690549163634,
      "grad_norm": 10337.944089614724,
      "learning_rate": 2.4768490348995377e-07,
      "loss": 1.7568,
      "step": 163168
    },
    {
      "epoch": 0.0005922851892672001,
      "grad_norm": 10143.892743912465,
      "learning_rate": 2.4766059518612207e-07,
      "loss": 1.7533,
      "step": 163200
    },
    {
      "epoch": 0.0005924013236180368,
      "grad_norm": 9388.095547021238,
      "learning_rate": 2.47636294037886e-07,
      "loss": 1.7438,
      "step": 163232
    },
    {
      "epoch": 0.0005925174579688735,
      "grad_norm": 8850.21389572026,
      "learning_rate": 2.4761200004173573e-07,
      "loss": 1.7291,
      "step": 163264
    },
    {
      "epoch": 0.0005926335923197102,
      "grad_norm": 9560.00606694368,
      "learning_rate": 2.4758771319416364e-07,
      "loss": 1.7275,
      "step": 163296
    },
    {
      "epoch": 0.0005927497266705469,
      "grad_norm": 11262.290530793458,
      "learning_rate": 2.475634334916646e-07,
      "loss": 1.7348,
      "step": 163328
    },
    {
      "epoch": 0.0005928658610213836,
      "grad_norm": 10175.322501031602,
      "learning_rate": 2.47539160930736e-07,
      "loss": 1.7257,
      "step": 163360
    },
    {
      "epoch": 0.0005929819953722203,
      "grad_norm": 9085.585726853278,
      "learning_rate": 2.4751565369432947e-07,
      "loss": 1.7063,
      "step": 163392
    },
    {
      "epoch": 0.0005930981297230571,
      "grad_norm": 8716.853560775242,
      "learning_rate": 2.4749139518314047e-07,
      "loss": 1.7265,
      "step": 163424
    },
    {
      "epoch": 0.0005932142640738937,
      "grad_norm": 9827.442190112339,
      "learning_rate": 2.4746714380313736e-07,
      "loss": 1.7623,
      "step": 163456
    },
    {
      "epoch": 0.0005933303984247305,
      "grad_norm": 9815.974123845273,
      "learning_rate": 2.474428995508268e-07,
      "loss": 1.7656,
      "step": 163488
    },
    {
      "epoch": 0.0005934465327755671,
      "grad_norm": 10435.126448682833,
      "learning_rate": 2.4741866242271803e-07,
      "loss": 1.7211,
      "step": 163520
    },
    {
      "epoch": 0.0005935626671264039,
      "grad_norm": 8235.578425344513,
      "learning_rate": 2.473944324153227e-07,
      "loss": 1.7385,
      "step": 163552
    },
    {
      "epoch": 0.0005936788014772405,
      "grad_norm": 9088.783416937604,
      "learning_rate": 2.4737020952515464e-07,
      "loss": 1.744,
      "step": 163584
    },
    {
      "epoch": 0.0005937949358280773,
      "grad_norm": 9998.054410734121,
      "learning_rate": 2.473459937487304e-07,
      "loss": 1.7263,
      "step": 163616
    },
    {
      "epoch": 0.0005939110701789139,
      "grad_norm": 9564.459524719627,
      "learning_rate": 2.4732178508256865e-07,
      "loss": 1.7304,
      "step": 163648
    },
    {
      "epoch": 0.0005940272045297507,
      "grad_norm": 8799.39020614497,
      "learning_rate": 2.4729758352319056e-07,
      "loss": 1.7351,
      "step": 163680
    },
    {
      "epoch": 0.0005941433388805874,
      "grad_norm": 9915.790235780505,
      "learning_rate": 2.4727338906711966e-07,
      "loss": 1.7126,
      "step": 163712
    },
    {
      "epoch": 0.0005942594732314241,
      "grad_norm": 9461.027639744005,
      "learning_rate": 2.472492017108818e-07,
      "loss": 1.7418,
      "step": 163744
    },
    {
      "epoch": 0.0005943756075822608,
      "grad_norm": 9341.185577858947,
      "learning_rate": 2.472250214510053e-07,
      "loss": 1.7275,
      "step": 163776
    },
    {
      "epoch": 0.0005944917419330975,
      "grad_norm": 9504.89210880376,
      "learning_rate": 2.4720084828402084e-07,
      "loss": 1.7402,
      "step": 163808
    },
    {
      "epoch": 0.0005946078762839342,
      "grad_norm": 10349.261712798647,
      "learning_rate": 2.471766822064614e-07,
      "loss": 1.7282,
      "step": 163840
    },
    {
      "epoch": 0.0005947240106347709,
      "grad_norm": 8163.7253750968375,
      "learning_rate": 2.4715252321486243e-07,
      "loss": 1.7233,
      "step": 163872
    },
    {
      "epoch": 0.0005948401449856076,
      "grad_norm": 9414.794315331588,
      "learning_rate": 2.471283713057617e-07,
      "loss": 1.7533,
      "step": 163904
    },
    {
      "epoch": 0.0005949562793364443,
      "grad_norm": 9613.486360316949,
      "learning_rate": 2.471042264756994e-07,
      "loss": 1.7587,
      "step": 163936
    },
    {
      "epoch": 0.000595072413687281,
      "grad_norm": 11579.548523150634,
      "learning_rate": 2.470800887212179e-07,
      "loss": 1.7637,
      "step": 163968
    },
    {
      "epoch": 0.0005951885480381178,
      "grad_norm": 8485.471937376258,
      "learning_rate": 2.4705595803886223e-07,
      "loss": 1.771,
      "step": 164000
    },
    {
      "epoch": 0.0005953046823889544,
      "grad_norm": 8847.772827101744,
      "learning_rate": 2.4703183442517956e-07,
      "loss": 1.77,
      "step": 164032
    },
    {
      "epoch": 0.0005954208167397912,
      "grad_norm": 8967.194879113535,
      "learning_rate": 2.4700771787671946e-07,
      "loss": 1.7537,
      "step": 164064
    },
    {
      "epoch": 0.0005955369510906278,
      "grad_norm": 9271.856556267467,
      "learning_rate": 2.469836083900339e-07,
      "loss": 1.7451,
      "step": 164096
    },
    {
      "epoch": 0.0005956530854414646,
      "grad_norm": 8307.851587504438,
      "learning_rate": 2.4695950596167726e-07,
      "loss": 1.7497,
      "step": 164128
    },
    {
      "epoch": 0.0005957692197923012,
      "grad_norm": 9561.504693300109,
      "learning_rate": 2.469354105882061e-07,
      "loss": 1.7464,
      "step": 164160
    },
    {
      "epoch": 0.000595885354143138,
      "grad_norm": 10250.538522438712,
      "learning_rate": 2.469113222661794e-07,
      "loss": 1.7029,
      "step": 164192
    },
    {
      "epoch": 0.0005960014884939746,
      "grad_norm": 9631.826410395903,
      "learning_rate": 2.4688724099215865e-07,
      "loss": 1.7228,
      "step": 164224
    },
    {
      "epoch": 0.0005961176228448114,
      "grad_norm": 8464.790842070464,
      "learning_rate": 2.468631667627075e-07,
      "loss": 1.7354,
      "step": 164256
    },
    {
      "epoch": 0.0005962337571956481,
      "grad_norm": 10518.292827260515,
      "learning_rate": 2.4683909957439196e-07,
      "loss": 1.734,
      "step": 164288
    },
    {
      "epoch": 0.0005963498915464848,
      "grad_norm": 10252.941626674756,
      "learning_rate": 2.468150394237805e-07,
      "loss": 1.7661,
      "step": 164320
    },
    {
      "epoch": 0.0005964660258973215,
      "grad_norm": 10020.58920423345,
      "learning_rate": 2.467909863074438e-07,
      "loss": 1.7626,
      "step": 164352
    },
    {
      "epoch": 0.0005965821602481582,
      "grad_norm": 8580.080885399624,
      "learning_rate": 2.467676915557366e-07,
      "loss": 1.7162,
      "step": 164384
    },
    {
      "epoch": 0.0005966982945989949,
      "grad_norm": 9716.115890622137,
      "learning_rate": 2.4674365227811587e-07,
      "loss": 1.7091,
      "step": 164416
    },
    {
      "epoch": 0.0005968144289498316,
      "grad_norm": 8361.579994235539,
      "learning_rate": 2.4671962002460305e-07,
      "loss": 1.7168,
      "step": 164448
    },
    {
      "epoch": 0.0005969305633006683,
      "grad_norm": 9473.108465546036,
      "learning_rate": 2.4669559479177825e-07,
      "loss": 1.7247,
      "step": 164480
    },
    {
      "epoch": 0.000597046697651505,
      "grad_norm": 8681.663435079709,
      "learning_rate": 2.466715765762236e-07,
      "loss": 1.7216,
      "step": 164512
    },
    {
      "epoch": 0.0005971628320023417,
      "grad_norm": 10495.894625995443,
      "learning_rate": 2.4664756537452404e-07,
      "loss": 1.7066,
      "step": 164544
    },
    {
      "epoch": 0.0005972789663531785,
      "grad_norm": 9628.2320287787,
      "learning_rate": 2.4662356118326635e-07,
      "loss": 1.7376,
      "step": 164576
    },
    {
      "epoch": 0.0005973951007040151,
      "grad_norm": 9976.867444243208,
      "learning_rate": 2.465995639990399e-07,
      "loss": 1.7533,
      "step": 164608
    },
    {
      "epoch": 0.0005975112350548519,
      "grad_norm": 8780.429488356478,
      "learning_rate": 2.465755738184364e-07,
      "loss": 1.7498,
      "step": 164640
    },
    {
      "epoch": 0.0005976273694056885,
      "grad_norm": 8786.641906894807,
      "learning_rate": 2.4655159063804974e-07,
      "loss": 1.7529,
      "step": 164672
    },
    {
      "epoch": 0.0005977435037565253,
      "grad_norm": 10099.959801900204,
      "learning_rate": 2.465276144544762e-07,
      "loss": 1.761,
      "step": 164704
    },
    {
      "epoch": 0.0005978596381073619,
      "grad_norm": 8060.5635038749,
      "learning_rate": 2.465036452643144e-07,
      "loss": 1.7389,
      "step": 164736
    },
    {
      "epoch": 0.0005979757724581987,
      "grad_norm": 9247.581089128118,
      "learning_rate": 2.4647968306416527e-07,
      "loss": 1.7364,
      "step": 164768
    },
    {
      "epoch": 0.0005980919068090353,
      "grad_norm": 9858.985647621159,
      "learning_rate": 2.46455727850632e-07,
      "loss": 1.7454,
      "step": 164800
    },
    {
      "epoch": 0.0005982080411598721,
      "grad_norm": 10218.819109858046,
      "learning_rate": 2.4643177962032014e-07,
      "loss": 1.7651,
      "step": 164832
    },
    {
      "epoch": 0.0005983241755107088,
      "grad_norm": 10322.226600884134,
      "learning_rate": 2.464078383698376e-07,
      "loss": 1.7446,
      "step": 164864
    },
    {
      "epoch": 0.0005984403098615455,
      "grad_norm": 9053.620601726141,
      "learning_rate": 2.463839040957944e-07,
      "loss": 1.7639,
      "step": 164896
    },
    {
      "epoch": 0.0005985564442123822,
      "grad_norm": 10910.696586377975,
      "learning_rate": 2.4635997679480305e-07,
      "loss": 1.7737,
      "step": 164928
    },
    {
      "epoch": 0.0005986725785632189,
      "grad_norm": 9453.383838605096,
      "learning_rate": 2.4633605646347833e-07,
      "loss": 1.7534,
      "step": 164960
    },
    {
      "epoch": 0.0005987887129140556,
      "grad_norm": 7901.978992632163,
      "learning_rate": 2.463121430984373e-07,
      "loss": 1.7318,
      "step": 164992
    },
    {
      "epoch": 0.0005989048472648923,
      "grad_norm": 9942.863269702546,
      "learning_rate": 2.462882366962992e-07,
      "loss": 1.7247,
      "step": 165024
    },
    {
      "epoch": 0.000599020981615729,
      "grad_norm": 9406.944562396442,
      "learning_rate": 2.462643372536859e-07,
      "loss": 1.7192,
      "step": 165056
    },
    {
      "epoch": 0.0005991371159665657,
      "grad_norm": 9374.966986608539,
      "learning_rate": 2.4624044476722114e-07,
      "loss": 1.7087,
      "step": 165088
    },
    {
      "epoch": 0.0005992532503174024,
      "grad_norm": 9528.900251340656,
      "learning_rate": 2.462165592335313e-07,
      "loss": 1.7162,
      "step": 165120
    },
    {
      "epoch": 0.0005993693846682392,
      "grad_norm": 8014.839237314744,
      "learning_rate": 2.461926806492448e-07,
      "loss": 1.7327,
      "step": 165152
    },
    {
      "epoch": 0.0005994855190190758,
      "grad_norm": 10191.174024615613,
      "learning_rate": 2.4616880901099254e-07,
      "loss": 1.7527,
      "step": 165184
    },
    {
      "epoch": 0.0005996016533699126,
      "grad_norm": 7541.623963046686,
      "learning_rate": 2.4614494431540753e-07,
      "loss": 1.7305,
      "step": 165216
    },
    {
      "epoch": 0.0005997177877207492,
      "grad_norm": 8717.607928784135,
      "learning_rate": 2.4612108655912524e-07,
      "loss": 1.74,
      "step": 165248
    },
    {
      "epoch": 0.000599833922071586,
      "grad_norm": 8509.245324939222,
      "learning_rate": 2.4609723573878334e-07,
      "loss": 1.7469,
      "step": 165280
    },
    {
      "epoch": 0.0005999500564224226,
      "grad_norm": 7840.3376202814125,
      "learning_rate": 2.4607339185102173e-07,
      "loss": 1.7534,
      "step": 165312
    },
    {
      "epoch": 0.0006000661907732594,
      "grad_norm": 10105.317906924056,
      "learning_rate": 2.460495548924827e-07,
      "loss": 1.7429,
      "step": 165344
    },
    {
      "epoch": 0.000600182325124096,
      "grad_norm": 9316.591436786312,
      "learning_rate": 2.4602572485981074e-07,
      "loss": 1.725,
      "step": 165376
    },
    {
      "epoch": 0.0006002984594749328,
      "grad_norm": 9727.968544357038,
      "learning_rate": 2.460026461170941e-07,
      "loss": 1.7074,
      "step": 165408
    },
    {
      "epoch": 0.0006004145938257696,
      "grad_norm": 9331.094683904992,
      "learning_rate": 2.459788297099257e-07,
      "loss": 1.7236,
      "step": 165440
    },
    {
      "epoch": 0.0006005307281766062,
      "grad_norm": 8995.27831698386,
      "learning_rate": 2.459550202186761e-07,
      "loss": 1.7307,
      "step": 165472
    },
    {
      "epoch": 0.000600646862527443,
      "grad_norm": 10183.70266651575,
      "learning_rate": 2.459312176399988e-07,
      "loss": 1.7607,
      "step": 165504
    },
    {
      "epoch": 0.0006007629968782796,
      "grad_norm": 10169.234976142503,
      "learning_rate": 2.459074219705497e-07,
      "loss": 1.7291,
      "step": 165536
    },
    {
      "epoch": 0.0006008791312291164,
      "grad_norm": 8977.793381449586,
      "learning_rate": 2.458836332069867e-07,
      "loss": 1.7157,
      "step": 165568
    },
    {
      "epoch": 0.000600995265579953,
      "grad_norm": 8848.398499163563,
      "learning_rate": 2.4585985134597023e-07,
      "loss": 1.7434,
      "step": 165600
    },
    {
      "epoch": 0.0006011113999307898,
      "grad_norm": 9444.74954670583,
      "learning_rate": 2.458360763841628e-07,
      "loss": 1.7512,
      "step": 165632
    },
    {
      "epoch": 0.0006012275342816264,
      "grad_norm": 10557.774765546004,
      "learning_rate": 2.458123083182293e-07,
      "loss": 1.7534,
      "step": 165664
    },
    {
      "epoch": 0.0006013436686324632,
      "grad_norm": 9781.741971653106,
      "learning_rate": 2.4578854714483676e-07,
      "loss": 1.7579,
      "step": 165696
    },
    {
      "epoch": 0.0006014598029832999,
      "grad_norm": 10478.898987966246,
      "learning_rate": 2.457647928606545e-07,
      "loss": 1.7608,
      "step": 165728
    },
    {
      "epoch": 0.0006015759373341365,
      "grad_norm": 11883.794175262377,
      "learning_rate": 2.4574104546235427e-07,
      "loss": 1.7616,
      "step": 165760
    },
    {
      "epoch": 0.0006016920716849733,
      "grad_norm": 9333.91225585499,
      "learning_rate": 2.4571730494660974e-07,
      "loss": 1.7421,
      "step": 165792
    },
    {
      "epoch": 0.00060180820603581,
      "grad_norm": 8851.221836560193,
      "learning_rate": 2.4569357131009713e-07,
      "loss": 1.7604,
      "step": 165824
    },
    {
      "epoch": 0.0006019243403866467,
      "grad_norm": 9930.24209171156,
      "learning_rate": 2.456698445494947e-07,
      "loss": 1.7551,
      "step": 165856
    },
    {
      "epoch": 0.0006020404747374833,
      "grad_norm": 9677.812976080908,
      "learning_rate": 2.4564612466148313e-07,
      "loss": 1.7213,
      "step": 165888
    },
    {
      "epoch": 0.0006021566090883201,
      "grad_norm": 8697.215301462877,
      "learning_rate": 2.456224116427452e-07,
      "loss": 1.7369,
      "step": 165920
    },
    {
      "epoch": 0.0006022727434391567,
      "grad_norm": 9905.920048132833,
      "learning_rate": 2.45598705489966e-07,
      "loss": 1.7324,
      "step": 165952
    },
    {
      "epoch": 0.0006023888777899935,
      "grad_norm": 9436.528493042344,
      "learning_rate": 2.4557500619983275e-07,
      "loss": 1.7413,
      "step": 165984
    },
    {
      "epoch": 0.0006025050121408303,
      "grad_norm": 8727.4454452606,
      "learning_rate": 2.455513137690351e-07,
      "loss": 1.7352,
      "step": 166016
    },
    {
      "epoch": 0.0006026211464916669,
      "grad_norm": 9878.05426184732,
      "learning_rate": 2.455276281942648e-07,
      "loss": 1.7506,
      "step": 166048
    },
    {
      "epoch": 0.0006027372808425037,
      "grad_norm": 9167.552999574096,
      "learning_rate": 2.4550394947221594e-07,
      "loss": 1.7312,
      "step": 166080
    },
    {
      "epoch": 0.0006028534151933403,
      "grad_norm": 9000.154665337703,
      "learning_rate": 2.454802775995847e-07,
      "loss": 1.7333,
      "step": 166112
    },
    {
      "epoch": 0.0006029695495441771,
      "grad_norm": 9597.798080809995,
      "learning_rate": 2.4545661257306953e-07,
      "loss": 1.7111,
      "step": 166144
    },
    {
      "epoch": 0.0006030856838950137,
      "grad_norm": 10240.214743842045,
      "learning_rate": 2.454329543893712e-07,
      "loss": 1.7304,
      "step": 166176
    },
    {
      "epoch": 0.0006032018182458505,
      "grad_norm": 9451.150406167495,
      "learning_rate": 2.4540930304519263e-07,
      "loss": 1.7207,
      "step": 166208
    },
    {
      "epoch": 0.0006033179525966871,
      "grad_norm": 10393.402618969401,
      "learning_rate": 2.4538565853723893e-07,
      "loss": 1.6999,
      "step": 166240
    },
    {
      "epoch": 0.0006034340869475239,
      "grad_norm": 10514.760292084646,
      "learning_rate": 2.4536202086221754e-07,
      "loss": 1.7318,
      "step": 166272
    },
    {
      "epoch": 0.0006035502212983606,
      "grad_norm": 9938.920665746356,
      "learning_rate": 2.4533839001683804e-07,
      "loss": 1.738,
      "step": 166304
    },
    {
      "epoch": 0.0006036663556491973,
      "grad_norm": 10606.269560971945,
      "learning_rate": 2.453147659978122e-07,
      "loss": 1.7473,
      "step": 166336
    },
    {
      "epoch": 0.000603782490000034,
      "grad_norm": 10524.285628963136,
      "learning_rate": 2.452911488018541e-07,
      "loss": 1.7484,
      "step": 166368
    },
    {
      "epoch": 0.0006038986243508707,
      "grad_norm": 8444.146493281603,
      "learning_rate": 2.452682761467389e-07,
      "loss": 1.7544,
      "step": 166400
    },
    {
      "epoch": 0.0006040147587017074,
      "grad_norm": 10757.994980478472,
      "learning_rate": 2.452446723741011e-07,
      "loss": 1.7221,
      "step": 166432
    },
    {
      "epoch": 0.0006041308930525441,
      "grad_norm": 9817.34404001408,
      "learning_rate": 2.452210754147889e-07,
      "loss": 1.7472,
      "step": 166464
    },
    {
      "epoch": 0.0006042470274033808,
      "grad_norm": 8670.562034839495,
      "learning_rate": 2.4519748526552506e-07,
      "loss": 1.7579,
      "step": 166496
    },
    {
      "epoch": 0.0006043631617542175,
      "grad_norm": 10228.301129708687,
      "learning_rate": 2.4517390192303455e-07,
      "loss": 1.7546,
      "step": 166528
    },
    {
      "epoch": 0.0006044792961050542,
      "grad_norm": 10784.500359311969,
      "learning_rate": 2.4515032538404464e-07,
      "loss": 1.7363,
      "step": 166560
    },
    {
      "epoch": 0.000604595430455891,
      "grad_norm": 10096.382520487226,
      "learning_rate": 2.4512675564528474e-07,
      "loss": 1.7402,
      "step": 166592
    },
    {
      "epoch": 0.0006047115648067276,
      "grad_norm": 10578.873474997232,
      "learning_rate": 2.451031927034864e-07,
      "loss": 1.7794,
      "step": 166624
    },
    {
      "epoch": 0.0006048276991575644,
      "grad_norm": 9752.966112932003,
      "learning_rate": 2.4507963655538354e-07,
      "loss": 1.7594,
      "step": 166656
    },
    {
      "epoch": 0.000604943833508401,
      "grad_norm": 10196.295797984678,
      "learning_rate": 2.4505608719771215e-07,
      "loss": 1.7683,
      "step": 166688
    },
    {
      "epoch": 0.0006050599678592378,
      "grad_norm": 8257.431683035591,
      "learning_rate": 2.450325446272104e-07,
      "loss": 1.7461,
      "step": 166720
    },
    {
      "epoch": 0.0006051761022100744,
      "grad_norm": 9074.517948629558,
      "learning_rate": 2.4500900884061873e-07,
      "loss": 1.7171,
      "step": 166752
    },
    {
      "epoch": 0.0006052922365609112,
      "grad_norm": 8564.90630421606,
      "learning_rate": 2.4498547983467975e-07,
      "loss": 1.7072,
      "step": 166784
    },
    {
      "epoch": 0.0006054083709117478,
      "grad_norm": 9670.493886043256,
      "learning_rate": 2.4496195760613826e-07,
      "loss": 1.7151,
      "step": 166816
    },
    {
      "epoch": 0.0006055245052625846,
      "grad_norm": 8896.805494108547,
      "learning_rate": 2.4493844215174117e-07,
      "loss": 1.7366,
      "step": 166848
    },
    {
      "epoch": 0.0006056406396134213,
      "grad_norm": 11317.449712722384,
      "learning_rate": 2.4491493346823763e-07,
      "loss": 1.7359,
      "step": 166880
    },
    {
      "epoch": 0.000605756773964258,
      "grad_norm": 8084.206578261096,
      "learning_rate": 2.4489143155237906e-07,
      "loss": 1.725,
      "step": 166912
    },
    {
      "epoch": 0.0006058729083150947,
      "grad_norm": 9954.794221881233,
      "learning_rate": 2.4486793640091893e-07,
      "loss": 1.7231,
      "step": 166944
    },
    {
      "epoch": 0.0006059890426659314,
      "grad_norm": 8416.446280943044,
      "learning_rate": 2.448444480106129e-07,
      "loss": 1.7573,
      "step": 166976
    },
    {
      "epoch": 0.0006061051770167681,
      "grad_norm": 8918.035097486441,
      "learning_rate": 2.44820966378219e-07,
      "loss": 1.7484,
      "step": 167008
    },
    {
      "epoch": 0.0006062213113676048,
      "grad_norm": 9189.055446562503,
      "learning_rate": 2.4479749150049713e-07,
      "loss": 1.7462,
      "step": 167040
    },
    {
      "epoch": 0.0006063374457184415,
      "grad_norm": 9243.203016270929,
      "learning_rate": 2.447740233742096e-07,
      "loss": 1.7505,
      "step": 167072
    },
    {
      "epoch": 0.0006064535800692782,
      "grad_norm": 8089.767116549153,
      "learning_rate": 2.4475056199612085e-07,
      "loss": 1.7241,
      "step": 167104
    },
    {
      "epoch": 0.0006065697144201149,
      "grad_norm": 10045.676482945288,
      "learning_rate": 2.447271073629974e-07,
      "loss": 1.7076,
      "step": 167136
    },
    {
      "epoch": 0.0006066858487709516,
      "grad_norm": 9770.12998889984,
      "learning_rate": 2.4470365947160793e-07,
      "loss": 1.7275,
      "step": 167168
    },
    {
      "epoch": 0.0006068019831217883,
      "grad_norm": 10334.328715499618,
      "learning_rate": 2.4468021831872345e-07,
      "loss": 1.7415,
      "step": 167200
    },
    {
      "epoch": 0.0006069181174726251,
      "grad_norm": 10057.487857313077,
      "learning_rate": 2.44656783901117e-07,
      "loss": 1.7383,
      "step": 167232
    },
    {
      "epoch": 0.0006070342518234617,
      "grad_norm": 9144.887533480114,
      "learning_rate": 2.446333562155639e-07,
      "loss": 1.7272,
      "step": 167264
    },
    {
      "epoch": 0.0006071503861742985,
      "grad_norm": 9674.354758845677,
      "learning_rate": 2.446099352588414e-07,
      "loss": 1.7401,
      "step": 167296
    },
    {
      "epoch": 0.0006072665205251351,
      "grad_norm": 9096.167874440313,
      "learning_rate": 2.4458652102772913e-07,
      "loss": 1.744,
      "step": 167328
    },
    {
      "epoch": 0.0006073826548759719,
      "grad_norm": 8436.492991759076,
      "learning_rate": 2.4456311351900883e-07,
      "loss": 1.7466,
      "step": 167360
    },
    {
      "epoch": 0.0006074987892268085,
      "grad_norm": 9167.428428954327,
      "learning_rate": 2.445397127294643e-07,
      "loss": 1.7479,
      "step": 167392
    },
    {
      "epoch": 0.0006076149235776453,
      "grad_norm": 9172.662972114478,
      "learning_rate": 2.445170496190554e-07,
      "loss": 1.7455,
      "step": 167424
    },
    {
      "epoch": 0.0006077310579284819,
      "grad_norm": 9295.77043606392,
      "learning_rate": 2.4449366204849797e-07,
      "loss": 1.7391,
      "step": 167456
    },
    {
      "epoch": 0.0006078471922793187,
      "grad_norm": 10015.949480703266,
      "learning_rate": 2.4447028118758114e-07,
      "loss": 1.7555,
      "step": 167488
    },
    {
      "epoch": 0.0006079633266301554,
      "grad_norm": 9032.883260620609,
      "learning_rate": 2.444469070330973e-07,
      "loss": 1.7439,
      "step": 167520
    },
    {
      "epoch": 0.0006080794609809921,
      "grad_norm": 8302.03023362358,
      "learning_rate": 2.4442353958184105e-07,
      "loss": 1.761,
      "step": 167552
    },
    {
      "epoch": 0.0006081955953318288,
      "grad_norm": 10150.759577489756,
      "learning_rate": 2.444001788306091e-07,
      "loss": 1.7407,
      "step": 167584
    },
    {
      "epoch": 0.0006083117296826655,
      "grad_norm": 9394.485297236884,
      "learning_rate": 2.4437682477620023e-07,
      "loss": 1.7271,
      "step": 167616
    },
    {
      "epoch": 0.0006084278640335022,
      "grad_norm": 8420.726215713226,
      "learning_rate": 2.443534774154155e-07,
      "loss": 1.7589,
      "step": 167648
    },
    {
      "epoch": 0.0006085439983843389,
      "grad_norm": 9944.29766247974,
      "learning_rate": 2.44330136745058e-07,
      "loss": 1.7457,
      "step": 167680
    },
    {
      "epoch": 0.0006086601327351756,
      "grad_norm": 9992.46155859506,
      "learning_rate": 2.4430680276193296e-07,
      "loss": 1.7407,
      "step": 167712
    },
    {
      "epoch": 0.0006087762670860123,
      "grad_norm": 9625.086389222697,
      "learning_rate": 2.442834754628478e-07,
      "loss": 1.7416,
      "step": 167744
    },
    {
      "epoch": 0.000608892401436849,
      "grad_norm": 9499.52546183229,
      "learning_rate": 2.4426015484461204e-07,
      "loss": 1.7368,
      "step": 167776
    },
    {
      "epoch": 0.0006090085357876858,
      "grad_norm": 9586.501551661064,
      "learning_rate": 2.4423684090403736e-07,
      "loss": 1.7088,
      "step": 167808
    },
    {
      "epoch": 0.0006091246701385224,
      "grad_norm": 9682.771090963579,
      "learning_rate": 2.4421353363793756e-07,
      "loss": 1.7344,
      "step": 167840
    },
    {
      "epoch": 0.0006092408044893592,
      "grad_norm": 10127.696085487558,
      "learning_rate": 2.441902330431285e-07,
      "loss": 1.7529,
      "step": 167872
    },
    {
      "epoch": 0.0006093569388401958,
      "grad_norm": 8615.926647784323,
      "learning_rate": 2.4416693911642823e-07,
      "loss": 1.7206,
      "step": 167904
    },
    {
      "epoch": 0.0006094730731910326,
      "grad_norm": 8863.602089444223,
      "learning_rate": 2.4414365185465696e-07,
      "loss": 1.7026,
      "step": 167936
    },
    {
      "epoch": 0.0006095892075418692,
      "grad_norm": 8685.254285281462,
      "learning_rate": 2.44120371254637e-07,
      "loss": 1.7151,
      "step": 167968
    },
    {
      "epoch": 0.000609705341892706,
      "grad_norm": 8400.128570444622,
      "learning_rate": 2.440970973131927e-07,
      "loss": 1.7318,
      "step": 168000
    },
    {
      "epoch": 0.0006098214762435426,
      "grad_norm": 9296.504611949591,
      "learning_rate": 2.440738300271506e-07,
      "loss": 1.7464,
      "step": 168032
    },
    {
      "epoch": 0.0006099376105943794,
      "grad_norm": 9504.607619465414,
      "learning_rate": 2.4405056939333935e-07,
      "loss": 1.7442,
      "step": 168064
    },
    {
      "epoch": 0.0006100537449452161,
      "grad_norm": 9781.016920545633,
      "learning_rate": 2.440273154085897e-07,
      "loss": 1.7491,
      "step": 168096
    },
    {
      "epoch": 0.0006101698792960528,
      "grad_norm": 10537.294244729052,
      "learning_rate": 2.440040680697346e-07,
      "loss": 1.7274,
      "step": 168128
    },
    {
      "epoch": 0.0006102860136468895,
      "grad_norm": 9177.246645917281,
      "learning_rate": 2.4398082737360895e-07,
      "loss": 1.7348,
      "step": 168160
    },
    {
      "epoch": 0.0006104021479977262,
      "grad_norm": 8917.905583711907,
      "learning_rate": 2.4395759331704986e-07,
      "loss": 1.7358,
      "step": 168192
    },
    {
      "epoch": 0.0006105182823485629,
      "grad_norm": 9401.201838063047,
      "learning_rate": 2.4393436589689655e-07,
      "loss": 1.76,
      "step": 168224
    },
    {
      "epoch": 0.0006106344166993996,
      "grad_norm": 9306.143024905645,
      "learning_rate": 2.439111451099904e-07,
      "loss": 1.7561,
      "step": 168256
    },
    {
      "epoch": 0.0006107505510502363,
      "grad_norm": 9092.866654691468,
      "learning_rate": 2.438879309531747e-07,
      "loss": 1.732,
      "step": 168288
    },
    {
      "epoch": 0.000610866685401073,
      "grad_norm": 9494.140508755914,
      "learning_rate": 2.438647234232951e-07,
      "loss": 1.7595,
      "step": 168320
    },
    {
      "epoch": 0.0006109828197519097,
      "grad_norm": 10294.429950220652,
      "learning_rate": 2.43841522517199e-07,
      "loss": 1.7689,
      "step": 168352
    },
    {
      "epoch": 0.0006110989541027465,
      "grad_norm": 9146.73887240693,
      "learning_rate": 2.438183282317364e-07,
      "loss": 1.7533,
      "step": 168384
    },
    {
      "epoch": 0.0006112150884535831,
      "grad_norm": 10013.972837989926,
      "learning_rate": 2.4379586507824746e-07,
      "loss": 1.7563,
      "step": 168416
    },
    {
      "epoch": 0.0006113312228044199,
      "grad_norm": 9052.359581899074,
      "learning_rate": 2.437726838179586e-07,
      "loss": 1.7623,
      "step": 168448
    },
    {
      "epoch": 0.0006114473571552565,
      "grad_norm": 10413.826386108038,
      "learning_rate": 2.43749509168963e-07,
      "loss": 1.7157,
      "step": 168480
    },
    {
      "epoch": 0.0006115634915060933,
      "grad_norm": 10650.316427224123,
      "learning_rate": 2.437263411281188e-07,
      "loss": 1.7163,
      "step": 168512
    },
    {
      "epoch": 0.0006116796258569299,
      "grad_norm": 8666.721525467401,
      "learning_rate": 2.4370317969228605e-07,
      "loss": 1.7166,
      "step": 168544
    },
    {
      "epoch": 0.0006117957602077667,
      "grad_norm": 10067.111800312938,
      "learning_rate": 2.4368002485832697e-07,
      "loss": 1.7355,
      "step": 168576
    },
    {
      "epoch": 0.0006119118945586033,
      "grad_norm": 9641.736150714767,
      "learning_rate": 2.4365687662310586e-07,
      "loss": 1.7233,
      "step": 168608
    },
    {
      "epoch": 0.0006120280289094401,
      "grad_norm": 9681.181539460977,
      "learning_rate": 2.4363373498348907e-07,
      "loss": 1.7201,
      "step": 168640
    },
    {
      "epoch": 0.0006121441632602768,
      "grad_norm": 8333.296826586702,
      "learning_rate": 2.436105999363451e-07,
      "loss": 1.7308,
      "step": 168672
    },
    {
      "epoch": 0.0006122602976111135,
      "grad_norm": 9111.131214069963,
      "learning_rate": 2.4358747147854446e-07,
      "loss": 1.7411,
      "step": 168704
    },
    {
      "epoch": 0.0006123764319619502,
      "grad_norm": 9908.242427393468,
      "learning_rate": 2.435643496069598e-07,
      "loss": 1.7485,
      "step": 168736
    },
    {
      "epoch": 0.0006124925663127869,
      "grad_norm": 10354.29746530396,
      "learning_rate": 2.4354123431846586e-07,
      "loss": 1.7395,
      "step": 168768
    },
    {
      "epoch": 0.0006126087006636236,
      "grad_norm": 8551.985617387345,
      "learning_rate": 2.4351812560993935e-07,
      "loss": 1.7253,
      "step": 168800
    },
    {
      "epoch": 0.0006127248350144603,
      "grad_norm": 8866.411111605417,
      "learning_rate": 2.4349502347825913e-07,
      "loss": 1.727,
      "step": 168832
    },
    {
      "epoch": 0.000612840969365297,
      "grad_norm": 9783.176375799427,
      "learning_rate": 2.4347192792030617e-07,
      "loss": 1.7385,
      "step": 168864
    },
    {
      "epoch": 0.0006129571037161337,
      "grad_norm": 10180.67777704412,
      "learning_rate": 2.434488389329634e-07,
      "loss": 1.7391,
      "step": 168896
    },
    {
      "epoch": 0.0006130732380669704,
      "grad_norm": 10648.855713174069,
      "learning_rate": 2.434257565131159e-07,
      "loss": 1.7377,
      "step": 168928
    },
    {
      "epoch": 0.0006131893724178072,
      "grad_norm": 9190.382037760999,
      "learning_rate": 2.4340268065765084e-07,
      "loss": 1.7159,
      "step": 168960
    },
    {
      "epoch": 0.0006133055067686438,
      "grad_norm": 9440.942325848622,
      "learning_rate": 2.433796113634574e-07,
      "loss": 1.7351,
      "step": 168992
    },
    {
      "epoch": 0.0006134216411194806,
      "grad_norm": 8443.594495237203,
      "learning_rate": 2.433565486274268e-07,
      "loss": 1.7523,
      "step": 169024
    },
    {
      "epoch": 0.0006135377754703172,
      "grad_norm": 8495.787309013804,
      "learning_rate": 2.4333349244645234e-07,
      "loss": 1.7441,
      "step": 169056
    },
    {
      "epoch": 0.000613653909821154,
      "grad_norm": 9667.736860299829,
      "learning_rate": 2.433104428174295e-07,
      "loss": 1.7419,
      "step": 169088
    },
    {
      "epoch": 0.0006137700441719906,
      "grad_norm": 10527.527345013168,
      "learning_rate": 2.4328739973725567e-07,
      "loss": 1.746,
      "step": 169120
    },
    {
      "epoch": 0.0006138861785228274,
      "grad_norm": 9331.868087366001,
      "learning_rate": 2.4326436320283037e-07,
      "loss": 1.7214,
      "step": 169152
    },
    {
      "epoch": 0.000614002312873664,
      "grad_norm": 9739.43098953938,
      "learning_rate": 2.4324133321105506e-07,
      "loss": 1.7416,
      "step": 169184
    },
    {
      "epoch": 0.0006141184472245008,
      "grad_norm": 9448.71769077688,
      "learning_rate": 2.432183097588334e-07,
      "loss": 1.7481,
      "step": 169216
    },
    {
      "epoch": 0.0006142345815753375,
      "grad_norm": 10050.43302549696,
      "learning_rate": 2.431952928430711e-07,
      "loss": 1.7522,
      "step": 169248
    },
    {
      "epoch": 0.0006143507159261742,
      "grad_norm": 9910.307563340302,
      "learning_rate": 2.4317228246067575e-07,
      "loss": 1.7418,
      "step": 169280
    },
    {
      "epoch": 0.0006144668502770109,
      "grad_norm": 9238.654447483139,
      "learning_rate": 2.4314927860855715e-07,
      "loss": 1.7328,
      "step": 169312
    },
    {
      "epoch": 0.0006145829846278476,
      "grad_norm": 9760.443739912647,
      "learning_rate": 2.4312628128362717e-07,
      "loss": 1.7566,
      "step": 169344
    },
    {
      "epoch": 0.0006146991189786843,
      "grad_norm": 9930.735420904133,
      "learning_rate": 2.4310329048279954e-07,
      "loss": 1.7446,
      "step": 169376
    },
    {
      "epoch": 0.000614815253329521,
      "grad_norm": 10401.462397182428,
      "learning_rate": 2.43081024363058e-07,
      "loss": 1.7454,
      "step": 169408
    },
    {
      "epoch": 0.0006149313876803577,
      "grad_norm": 8846.262939795539,
      "learning_rate": 2.4305804639754593e-07,
      "loss": 1.756,
      "step": 169440
    },
    {
      "epoch": 0.0006150475220311944,
      "grad_norm": 8696.468018684367,
      "learning_rate": 2.4303507494698634e-07,
      "loss": 1.7398,
      "step": 169472
    },
    {
      "epoch": 0.0006151636563820311,
      "grad_norm": 9250.519336772395,
      "learning_rate": 2.4301211000830096e-07,
      "loss": 1.7116,
      "step": 169504
    },
    {
      "epoch": 0.0006152797907328679,
      "grad_norm": 9510.3295421347,
      "learning_rate": 2.4298915157841394e-07,
      "loss": 1.7104,
      "step": 169536
    },
    {
      "epoch": 0.0006153959250837045,
      "grad_norm": 8738.223503664804,
      "learning_rate": 2.429661996542512e-07,
      "loss": 1.7261,
      "step": 169568
    },
    {
      "epoch": 0.0006155120594345413,
      "grad_norm": 8753.96927113638,
      "learning_rate": 2.429432542327408e-07,
      "loss": 1.7544,
      "step": 169600
    },
    {
      "epoch": 0.0006156281937853779,
      "grad_norm": 8765.704649370751,
      "learning_rate": 2.429203153108128e-07,
      "loss": 1.7202,
      "step": 169632
    },
    {
      "epoch": 0.0006157443281362147,
      "grad_norm": 10539.730357082197,
      "learning_rate": 2.4289738288539927e-07,
      "loss": 1.7037,
      "step": 169664
    },
    {
      "epoch": 0.0006158604624870513,
      "grad_norm": 8743.916170686909,
      "learning_rate": 2.428744569534344e-07,
      "loss": 1.731,
      "step": 169696
    },
    {
      "epoch": 0.0006159765968378881,
      "grad_norm": 8162.088458231753,
      "learning_rate": 2.428515375118543e-07,
      "loss": 1.7335,
      "step": 169728
    },
    {
      "epoch": 0.0006160927311887247,
      "grad_norm": 9565.429838747446,
      "learning_rate": 2.428286245575972e-07,
      "loss": 1.7347,
      "step": 169760
    },
    {
      "epoch": 0.0006162088655395615,
      "grad_norm": 10066.39369387071,
      "learning_rate": 2.4280571808760313e-07,
      "loss": 1.7452,
      "step": 169792
    },
    {
      "epoch": 0.0006163249998903983,
      "grad_norm": 9164.143822529195,
      "learning_rate": 2.4278281809881445e-07,
      "loss": 1.7191,
      "step": 169824
    },
    {
      "epoch": 0.0006164411342412349,
      "grad_norm": 10523.114938077983,
      "learning_rate": 2.4275992458817536e-07,
      "loss": 1.7261,
      "step": 169856
    },
    {
      "epoch": 0.0006165572685920717,
      "grad_norm": 8799.361567750242,
      "learning_rate": 2.4273703755263215e-07,
      "loss": 1.7305,
      "step": 169888
    },
    {
      "epoch": 0.0006166734029429083,
      "grad_norm": 9865.729775338466,
      "learning_rate": 2.42714156989133e-07,
      "loss": 1.7455,
      "step": 169920
    },
    {
      "epoch": 0.000616789537293745,
      "grad_norm": 9065.435786546612,
      "learning_rate": 2.4269128289462826e-07,
      "loss": 1.7378,
      "step": 169952
    },
    {
      "epoch": 0.0006169056716445817,
      "grad_norm": 8482.778613166796,
      "learning_rate": 2.426684152660702e-07,
      "loss": 1.7379,
      "step": 169984
    },
    {
      "epoch": 0.0006170218059954185,
      "grad_norm": 9722.722663945528,
      "learning_rate": 2.426455541004131e-07,
      "loss": 1.7667,
      "step": 170016
    },
    {
      "epoch": 0.0006171379403462551,
      "grad_norm": 9560.855610247443,
      "learning_rate": 2.4262269939461326e-07,
      "loss": 1.7743,
      "step": 170048
    },
    {
      "epoch": 0.0006172540746970919,
      "grad_norm": 10387.782824067895,
      "learning_rate": 2.42599851145629e-07,
      "loss": 1.7467,
      "step": 170080
    },
    {
      "epoch": 0.0006173702090479286,
      "grad_norm": 8226.797311226283,
      "learning_rate": 2.425770093504207e-07,
      "loss": 1.755,
      "step": 170112
    },
    {
      "epoch": 0.0006174863433987653,
      "grad_norm": 11335.915137297032,
      "learning_rate": 2.425541740059506e-07,
      "loss": 1.7526,
      "step": 170144
    },
    {
      "epoch": 0.000617602477749602,
      "grad_norm": 10099.9463364911,
      "learning_rate": 2.42531345109183e-07,
      "loss": 1.7306,
      "step": 170176
    },
    {
      "epoch": 0.0006177186121004387,
      "grad_norm": 9679.324149960057,
      "learning_rate": 2.4250852265708433e-07,
      "loss": 1.7465,
      "step": 170208
    },
    {
      "epoch": 0.0006178347464512754,
      "grad_norm": 9960.627490273893,
      "learning_rate": 2.4248570664662287e-07,
      "loss": 1.732,
      "step": 170240
    },
    {
      "epoch": 0.000617950880802112,
      "grad_norm": 9424.919309999423,
      "learning_rate": 2.424628970747689e-07,
      "loss": 1.7307,
      "step": 170272
    },
    {
      "epoch": 0.0006180670151529488,
      "grad_norm": 11151.842897028277,
      "learning_rate": 2.424400939384948e-07,
      "loss": 1.7115,
      "step": 170304
    },
    {
      "epoch": 0.0006181831495037855,
      "grad_norm": 8781.704731998225,
      "learning_rate": 2.4241729723477473e-07,
      "loss": 1.7241,
      "step": 170336
    },
    {
      "epoch": 0.0006182992838546222,
      "grad_norm": 9699.450706096712,
      "learning_rate": 2.4239450696058515e-07,
      "loss": 1.7385,
      "step": 170368
    },
    {
      "epoch": 0.000618415418205459,
      "grad_norm": 9301.036716409628,
      "learning_rate": 2.4237172311290427e-07,
      "loss": 1.7294,
      "step": 170400
    },
    {
      "epoch": 0.0006185315525562956,
      "grad_norm": 10140.555606079975,
      "learning_rate": 2.4234965738601777e-07,
      "loss": 1.73,
      "step": 170432
    },
    {
      "epoch": 0.0006186476869071324,
      "grad_norm": 8095.2912239152965,
      "learning_rate": 2.423268861817031e-07,
      "loss": 1.741,
      "step": 170464
    },
    {
      "epoch": 0.000618763821257969,
      "grad_norm": 9360.509174184917,
      "learning_rate": 2.42304121394938e-07,
      "loss": 1.7358,
      "step": 170496
    },
    {
      "epoch": 0.0006188799556088058,
      "grad_norm": 8932.173643632326,
      "learning_rate": 2.422813630227088e-07,
      "loss": 1.7176,
      "step": 170528
    },
    {
      "epoch": 0.0006189960899596424,
      "grad_norm": 10535.418928547644,
      "learning_rate": 2.4225861106200365e-07,
      "loss": 1.7244,
      "step": 170560
    },
    {
      "epoch": 0.0006191122243104792,
      "grad_norm": 9811.281669588332,
      "learning_rate": 2.4223586550981257e-07,
      "loss": 1.7534,
      "step": 170592
    },
    {
      "epoch": 0.0006192283586613158,
      "grad_norm": 9466.477803280373,
      "learning_rate": 2.4221312636312774e-07,
      "loss": 1.7536,
      "step": 170624
    },
    {
      "epoch": 0.0006193444930121526,
      "grad_norm": 10025.37819735495,
      "learning_rate": 2.421903936189432e-07,
      "loss": 1.7122,
      "step": 170656
    },
    {
      "epoch": 0.0006194606273629893,
      "grad_norm": 9160.026200835891,
      "learning_rate": 2.4216766727425504e-07,
      "loss": 1.725,
      "step": 170688
    },
    {
      "epoch": 0.000619576761713826,
      "grad_norm": 9279.69848648112,
      "learning_rate": 2.421449473260613e-07,
      "loss": 1.7468,
      "step": 170720
    },
    {
      "epoch": 0.0006196928960646627,
      "grad_norm": 9808.217065297851,
      "learning_rate": 2.421222337713619e-07,
      "loss": 1.7446,
      "step": 170752
    },
    {
      "epoch": 0.0006198090304154994,
      "grad_norm": 9474.230311745647,
      "learning_rate": 2.4209952660715883e-07,
      "loss": 1.7522,
      "step": 170784
    },
    {
      "epoch": 0.0006199251647663361,
      "grad_norm": 8908.99096418893,
      "learning_rate": 2.4207682583045607e-07,
      "loss": 1.7462,
      "step": 170816
    },
    {
      "epoch": 0.0006200412991171728,
      "grad_norm": 8289.18126234431,
      "learning_rate": 2.4205413143825954e-07,
      "loss": 1.722,
      "step": 170848
    },
    {
      "epoch": 0.0006201574334680095,
      "grad_norm": 10360.760203768832,
      "learning_rate": 2.42031443427577e-07,
      "loss": 1.7266,
      "step": 170880
    },
    {
      "epoch": 0.0006202735678188462,
      "grad_norm": 9359.34271196434,
      "learning_rate": 2.4200876179541835e-07,
      "loss": 1.745,
      "step": 170912
    },
    {
      "epoch": 0.0006203897021696829,
      "grad_norm": 11265.591684416757,
      "learning_rate": 2.4198608653879533e-07,
      "loss": 1.7437,
      "step": 170944
    },
    {
      "epoch": 0.0006205058365205197,
      "grad_norm": 10547.910693592357,
      "learning_rate": 2.419634176547217e-07,
      "loss": 1.7418,
      "step": 170976
    },
    {
      "epoch": 0.0006206219708713563,
      "grad_norm": 11219.13739999649,
      "learning_rate": 2.419407551402133e-07,
      "loss": 1.7254,
      "step": 171008
    },
    {
      "epoch": 0.0006207381052221931,
      "grad_norm": 10324.595585300181,
      "learning_rate": 2.419180989922876e-07,
      "loss": 1.7494,
      "step": 171040
    },
    {
      "epoch": 0.0006208542395730297,
      "grad_norm": 9288.310072343624,
      "learning_rate": 2.418954492079643e-07,
      "loss": 1.7603,
      "step": 171072
    },
    {
      "epoch": 0.0006209703739238665,
      "grad_norm": 10253.731808468563,
      "learning_rate": 2.41872805784265e-07,
      "loss": 1.7557,
      "step": 171104
    },
    {
      "epoch": 0.0006210865082747031,
      "grad_norm": 10922.473346270981,
      "learning_rate": 2.4185016871821314e-07,
      "loss": 1.739,
      "step": 171136
    },
    {
      "epoch": 0.0006212026426255399,
      "grad_norm": 8998.81770012039,
      "learning_rate": 2.418275380068343e-07,
      "loss": 1.7477,
      "step": 171168
    },
    {
      "epoch": 0.0006213187769763765,
      "grad_norm": 9750.902932549376,
      "learning_rate": 2.4180491364715577e-07,
      "loss": 1.7453,
      "step": 171200
    },
    {
      "epoch": 0.0006214349113272133,
      "grad_norm": 8554.424586142542,
      "learning_rate": 2.4178229563620703e-07,
      "loss": 1.7195,
      "step": 171232
    },
    {
      "epoch": 0.00062155104567805,
      "grad_norm": 9591.527094263978,
      "learning_rate": 2.4175968397101933e-07,
      "loss": 1.7179,
      "step": 171264
    },
    {
      "epoch": 0.0006216671800288867,
      "grad_norm": 9171.206463710214,
      "learning_rate": 2.4173707864862594e-07,
      "loss": 1.7314,
      "step": 171296
    },
    {
      "epoch": 0.0006217833143797234,
      "grad_norm": 8622.775191317469,
      "learning_rate": 2.41714479666062e-07,
      "loss": 1.7154,
      "step": 171328
    },
    {
      "epoch": 0.0006218994487305601,
      "grad_norm": 9470.69617293259,
      "learning_rate": 2.4169188702036474e-07,
      "loss": 1.725,
      "step": 171360
    },
    {
      "epoch": 0.0006220155830813968,
      "grad_norm": 9724.454534831248,
      "learning_rate": 2.4166930070857315e-07,
      "loss": 1.7489,
      "step": 171392
    },
    {
      "epoch": 0.0006221317174322335,
      "grad_norm": 9838.093819434738,
      "learning_rate": 2.4164742625632935e-07,
      "loss": 1.7293,
      "step": 171424
    },
    {
      "epoch": 0.0006222478517830702,
      "grad_norm": 9298.335980163332,
      "learning_rate": 2.416248524057692e-07,
      "loss": 1.7371,
      "step": 171456
    },
    {
      "epoch": 0.0006223639861339069,
      "grad_norm": 10202.993482307043,
      "learning_rate": 2.416022848803359e-07,
      "loss": 1.7358,
      "step": 171488
    },
    {
      "epoch": 0.0006224801204847436,
      "grad_norm": 9395.468801502137,
      "learning_rate": 2.415797236770763e-07,
      "loss": 1.7272,
      "step": 171520
    },
    {
      "epoch": 0.0006225962548355804,
      "grad_norm": 8646.255837066123,
      "learning_rate": 2.415571687930389e-07,
      "loss": 1.7196,
      "step": 171552
    },
    {
      "epoch": 0.000622712389186417,
      "grad_norm": 9296.196103783526,
      "learning_rate": 2.415346202252744e-07,
      "loss": 1.722,
      "step": 171584
    },
    {
      "epoch": 0.0006228285235372538,
      "grad_norm": 11095.858146173283,
      "learning_rate": 2.415120779708353e-07,
      "loss": 1.7474,
      "step": 171616
    },
    {
      "epoch": 0.0006229446578880904,
      "grad_norm": 9801.941134285596,
      "learning_rate": 2.414895420267761e-07,
      "loss": 1.7366,
      "step": 171648
    },
    {
      "epoch": 0.0006230607922389272,
      "grad_norm": 8722.197773497228,
      "learning_rate": 2.4146701239015313e-07,
      "loss": 1.7175,
      "step": 171680
    },
    {
      "epoch": 0.0006231769265897638,
      "grad_norm": 9982.038368990574,
      "learning_rate": 2.414444890580247e-07,
      "loss": 1.738,
      "step": 171712
    },
    {
      "epoch": 0.0006232930609406006,
      "grad_norm": 7916.715985811288,
      "learning_rate": 2.4142197202745113e-07,
      "loss": 1.7675,
      "step": 171744
    },
    {
      "epoch": 0.0006234091952914372,
      "grad_norm": 9154.282713571829,
      "learning_rate": 2.4139946129549447e-07,
      "loss": 1.7856,
      "step": 171776
    },
    {
      "epoch": 0.000623525329642274,
      "grad_norm": 8967.996208741393,
      "learning_rate": 2.413769568592188e-07,
      "loss": 1.7548,
      "step": 171808
    },
    {
      "epoch": 0.0006236414639931107,
      "grad_norm": 10554.47080625078,
      "learning_rate": 2.413544587156901e-07,
      "loss": 1.7433,
      "step": 171840
    },
    {
      "epoch": 0.0006237575983439474,
      "grad_norm": 10342.350409843983,
      "learning_rate": 2.4133196686197624e-07,
      "loss": 1.7226,
      "step": 171872
    },
    {
      "epoch": 0.0006238737326947841,
      "grad_norm": 9762.114934787442,
      "learning_rate": 2.413094812951471e-07,
      "loss": 1.7319,
      "step": 171904
    },
    {
      "epoch": 0.0006239898670456208,
      "grad_norm": 9699.117485627236,
      "learning_rate": 2.412870020122743e-07,
      "loss": 1.7493,
      "step": 171936
    },
    {
      "epoch": 0.0006241060013964575,
      "grad_norm": 10081.023360750634,
      "learning_rate": 2.412645290104315e-07,
      "loss": 1.7569,
      "step": 171968
    },
    {
      "epoch": 0.0006242221357472942,
      "grad_norm": 11595.308534058075,
      "learning_rate": 2.4124206228669426e-07,
      "loss": 1.7311,
      "step": 172000
    },
    {
      "epoch": 0.0006243382700981309,
      "grad_norm": 10198.154048650176,
      "learning_rate": 2.4121960183813997e-07,
      "loss": 1.7104,
      "step": 172032
    },
    {
      "epoch": 0.0006244544044489676,
      "grad_norm": 9003.141229593148,
      "learning_rate": 2.41197147661848e-07,
      "loss": 1.7386,
      "step": 172064
    },
    {
      "epoch": 0.0006245705387998043,
      "grad_norm": 8940.560496971093,
      "learning_rate": 2.4117469975489957e-07,
      "loss": 1.7317,
      "step": 172096
    },
    {
      "epoch": 0.0006246866731506411,
      "grad_norm": 10993.292318500404,
      "learning_rate": 2.411522581143778e-07,
      "loss": 1.7375,
      "step": 172128
    },
    {
      "epoch": 0.0006248028075014777,
      "grad_norm": 9561.496535584793,
      "learning_rate": 2.4112982273736784e-07,
      "loss": 1.7274,
      "step": 172160
    },
    {
      "epoch": 0.0006249189418523145,
      "grad_norm": 8650.125085800782,
      "learning_rate": 2.411073936209565e-07,
      "loss": 1.7238,
      "step": 172192
    },
    {
      "epoch": 0.0006250350762031511,
      "grad_norm": 9932.45760121834,
      "learning_rate": 2.410849707622327e-07,
      "loss": 1.7137,
      "step": 172224
    },
    {
      "epoch": 0.0006251512105539879,
      "grad_norm": 8340.95114480357,
      "learning_rate": 2.4106255415828713e-07,
      "loss": 1.7339,
      "step": 172256
    },
    {
      "epoch": 0.0006252673449048245,
      "grad_norm": 8748.26154158642,
      "learning_rate": 2.410401438062125e-07,
      "loss": 1.731,
      "step": 172288
    },
    {
      "epoch": 0.0006253834792556613,
      "grad_norm": 10833.529803346644,
      "learning_rate": 2.4101773970310323e-07,
      "loss": 1.7479,
      "step": 172320
    },
    {
      "epoch": 0.0006254996136064979,
      "grad_norm": 9846.807604498019,
      "learning_rate": 2.409953418460558e-07,
      "loss": 1.7345,
      "step": 172352
    },
    {
      "epoch": 0.0006256157479573347,
      "grad_norm": 9128.36699525167,
      "learning_rate": 2.409729502321684e-07,
      "loss": 1.7373,
      "step": 172384
    },
    {
      "epoch": 0.0006257318823081714,
      "grad_norm": 9994.398231009209,
      "learning_rate": 2.409505648585413e-07,
      "loss": 1.7477,
      "step": 172416
    },
    {
      "epoch": 0.0006258480166590081,
      "grad_norm": 8351.919659575276,
      "learning_rate": 2.4092888497590044e-07,
      "loss": 1.7394,
      "step": 172448
    },
    {
      "epoch": 0.0006259641510098448,
      "grad_norm": 9834.164428155551,
      "learning_rate": 2.4090651187931877e-07,
      "loss": 1.7461,
      "step": 172480
    },
    {
      "epoch": 0.0006260802853606815,
      "grad_norm": 10531.266970312736,
      "learning_rate": 2.4088414501439964e-07,
      "loss": 1.7447,
      "step": 172512
    },
    {
      "epoch": 0.0006261964197115182,
      "grad_norm": 9038.356155850466,
      "learning_rate": 2.408617843782508e-07,
      "loss": 1.7325,
      "step": 172544
    },
    {
      "epoch": 0.0006263125540623549,
      "grad_norm": 8718.923672105406,
      "learning_rate": 2.408394299679816e-07,
      "loss": 1.7259,
      "step": 172576
    },
    {
      "epoch": 0.0006264286884131916,
      "grad_norm": 9093.658559677728,
      "learning_rate": 2.408170817807036e-07,
      "loss": 1.7327,
      "step": 172608
    },
    {
      "epoch": 0.0006265448227640283,
      "grad_norm": 8611.602638301421,
      "learning_rate": 2.4079473981353005e-07,
      "loss": 1.753,
      "step": 172640
    },
    {
      "epoch": 0.000626660957114865,
      "grad_norm": 10294.785281879365,
      "learning_rate": 2.4077240406357617e-07,
      "loss": 1.7264,
      "step": 172672
    },
    {
      "epoch": 0.0006267770914657018,
      "grad_norm": 10407.691963158786,
      "learning_rate": 2.4075007452795885e-07,
      "loss": 1.719,
      "step": 172704
    },
    {
      "epoch": 0.0006268932258165384,
      "grad_norm": 9289.84197928038,
      "learning_rate": 2.4072775120379714e-07,
      "loss": 1.7414,
      "step": 172736
    },
    {
      "epoch": 0.0006270093601673752,
      "grad_norm": 10559.95681809353,
      "learning_rate": 2.407054340882117e-07,
      "loss": 1.7547,
      "step": 172768
    },
    {
      "epoch": 0.0006271254945182118,
      "grad_norm": 9945.539904902096,
      "learning_rate": 2.4068312317832527e-07,
      "loss": 1.76,
      "step": 172800
    },
    {
      "epoch": 0.0006272416288690486,
      "grad_norm": 7842.3521981609565,
      "learning_rate": 2.406608184712624e-07,
      "loss": 1.7489,
      "step": 172832
    },
    {
      "epoch": 0.0006273577632198852,
      "grad_norm": 10400.205767195186,
      "learning_rate": 2.406385199641493e-07,
      "loss": 1.761,
      "step": 172864
    },
    {
      "epoch": 0.000627473897570722,
      "grad_norm": 10158.771579280638,
      "learning_rate": 2.4061622765411433e-07,
      "loss": 1.7185,
      "step": 172896
    },
    {
      "epoch": 0.0006275900319215586,
      "grad_norm": 9560.833541067432,
      "learning_rate": 2.405939415382876e-07,
      "loss": 1.7402,
      "step": 172928
    },
    {
      "epoch": 0.0006277061662723954,
      "grad_norm": 9579.70531905862,
      "learning_rate": 2.4057166161380103e-07,
      "loss": 1.7342,
      "step": 172960
    },
    {
      "epoch": 0.0006278223006232321,
      "grad_norm": 9751.750612069609,
      "learning_rate": 2.4054938787778847e-07,
      "loss": 1.7443,
      "step": 172992
    },
    {
      "epoch": 0.0006279384349740688,
      "grad_norm": 8537.571551676741,
      "learning_rate": 2.405271203273855e-07,
      "loss": 1.7071,
      "step": 173024
    },
    {
      "epoch": 0.0006280545693249055,
      "grad_norm": 9815.614295600659,
      "learning_rate": 2.4050485895972984e-07,
      "loss": 1.7033,
      "step": 173056
    },
    {
      "epoch": 0.0006281707036757422,
      "grad_norm": 10341.124697053025,
      "learning_rate": 2.4048260377196075e-07,
      "loss": 1.7342,
      "step": 173088
    },
    {
      "epoch": 0.0006282868380265789,
      "grad_norm": 9261.341047602124,
      "learning_rate": 2.4046035476121944e-07,
      "loss": 1.7435,
      "step": 173120
    },
    {
      "epoch": 0.0006284029723774156,
      "grad_norm": 8868.121672597867,
      "learning_rate": 2.404381119246491e-07,
      "loss": 1.7454,
      "step": 173152
    },
    {
      "epoch": 0.0006285191067282523,
      "grad_norm": 8856.55644141672,
      "learning_rate": 2.404158752593946e-07,
      "loss": 1.7363,
      "step": 173184
    },
    {
      "epoch": 0.000628635241079089,
      "grad_norm": 9945.117596087037,
      "learning_rate": 2.403936447626028e-07,
      "loss": 1.7312,
      "step": 173216
    },
    {
      "epoch": 0.0006287513754299257,
      "grad_norm": 8341.38957248731,
      "learning_rate": 2.4037142043142225e-07,
      "loss": 1.7069,
      "step": 173248
    },
    {
      "epoch": 0.0006288675097807625,
      "grad_norm": 8406.160598037608,
      "learning_rate": 2.403492022630035e-07,
      "loss": 1.7228,
      "step": 173280
    },
    {
      "epoch": 0.0006289836441315991,
      "grad_norm": 9730.802741809126,
      "learning_rate": 2.4032699025449885e-07,
      "loss": 1.7395,
      "step": 173312
    },
    {
      "epoch": 0.0006290997784824359,
      "grad_norm": 9752.666097021882,
      "learning_rate": 2.4030478440306247e-07,
      "loss": 1.7435,
      "step": 173344
    },
    {
      "epoch": 0.0006292159128332725,
      "grad_norm": 8651.074384144435,
      "learning_rate": 2.402825847058503e-07,
      "loss": 1.7148,
      "step": 173376
    },
    {
      "epoch": 0.0006293320471841093,
      "grad_norm": 10346.524150650786,
      "learning_rate": 2.4026039116002035e-07,
      "loss": 1.7252,
      "step": 173408
    },
    {
      "epoch": 0.0006294481815349459,
      "grad_norm": 9241.556146017834,
      "learning_rate": 2.4023889702585685e-07,
      "loss": 1.7468,
      "step": 173440
    },
    {
      "epoch": 0.0006295643158857827,
      "grad_norm": 9875.712227480102,
      "learning_rate": 2.402167155822617e-07,
      "loss": 1.7567,
      "step": 173472
    },
    {
      "epoch": 0.0006296804502366193,
      "grad_norm": 9378.167198338917,
      "learning_rate": 2.401945402816218e-07,
      "loss": 1.7643,
      "step": 173504
    },
    {
      "epoch": 0.0006297965845874561,
      "grad_norm": 10027.711503628332,
      "learning_rate": 2.4017237112110235e-07,
      "loss": 1.7561,
      "step": 173536
    },
    {
      "epoch": 0.0006299127189382928,
      "grad_norm": 8737.546108605093,
      "learning_rate": 2.4015020809787027e-07,
      "loss": 1.7428,
      "step": 173568
    },
    {
      "epoch": 0.0006300288532891295,
      "grad_norm": 9586.89397041607,
      "learning_rate": 2.4012805120909436e-07,
      "loss": 1.725,
      "step": 173600
    },
    {
      "epoch": 0.0006301449876399662,
      "grad_norm": 9598.95879770301,
      "learning_rate": 2.401059004519452e-07,
      "loss": 1.7374,
      "step": 173632
    },
    {
      "epoch": 0.0006302611219908029,
      "grad_norm": 9936.724611258984,
      "learning_rate": 2.4008375582359526e-07,
      "loss": 1.7538,
      "step": 173664
    },
    {
      "epoch": 0.0006303772563416396,
      "grad_norm": 8671.99331180554,
      "learning_rate": 2.400616173212188e-07,
      "loss": 1.7388,
      "step": 173696
    },
    {
      "epoch": 0.0006304933906924763,
      "grad_norm": 10545.235891150087,
      "learning_rate": 2.40039484941992e-07,
      "loss": 1.7355,
      "step": 173728
    },
    {
      "epoch": 0.000630609525043313,
      "grad_norm": 10846.736836486814,
      "learning_rate": 2.4001735868309263e-07,
      "loss": 1.7407,
      "step": 173760
    },
    {
      "epoch": 0.0006307256593941497,
      "grad_norm": 8061.3264417216105,
      "learning_rate": 2.3999523854170056e-07,
      "loss": 1.7419,
      "step": 173792
    },
    {
      "epoch": 0.0006308417937449864,
      "grad_norm": 10073.848916873829,
      "learning_rate": 2.399731245149972e-07,
      "loss": 1.7283,
      "step": 173824
    },
    {
      "epoch": 0.0006309579280958232,
      "grad_norm": 8921.69983803535,
      "learning_rate": 2.3995101660016605e-07,
      "loss": 1.7308,
      "step": 173856
    },
    {
      "epoch": 0.0006310740624466598,
      "grad_norm": 8078.472627916741,
      "learning_rate": 2.3992891479439227e-07,
      "loss": 1.7282,
      "step": 173888
    },
    {
      "epoch": 0.0006311901967974966,
      "grad_norm": 9377.623153016973,
      "learning_rate": 2.399068190948628e-07,
      "loss": 1.7057,
      "step": 173920
    },
    {
      "epoch": 0.0006313063311483332,
      "grad_norm": 9000.332882732728,
      "learning_rate": 2.3988472949876646e-07,
      "loss": 1.7106,
      "step": 173952
    },
    {
      "epoch": 0.00063142246549917,
      "grad_norm": 9806.961608979613,
      "learning_rate": 2.39862646003294e-07,
      "loss": 1.7346,
      "step": 173984
    },
    {
      "epoch": 0.0006315385998500066,
      "grad_norm": 10067.372249003212,
      "learning_rate": 2.3984056860563767e-07,
      "loss": 1.7474,
      "step": 174016
    },
    {
      "epoch": 0.0006316547342008434,
      "grad_norm": 9548.100229888667,
      "learning_rate": 2.398184973029919e-07,
      "loss": 1.7276,
      "step": 174048
    },
    {
      "epoch": 0.00063177086855168,
      "grad_norm": 8474.99132742919,
      "learning_rate": 2.397964320925526e-07,
      "loss": 1.7225,
      "step": 174080
    },
    {
      "epoch": 0.0006318870029025168,
      "grad_norm": 9730.407596807032,
      "learning_rate": 2.3977437297151774e-07,
      "loss": 1.7596,
      "step": 174112
    },
    {
      "epoch": 0.0006320031372533536,
      "grad_norm": 10147.431793315982,
      "learning_rate": 2.397523199370869e-07,
      "loss": 1.7575,
      "step": 174144
    },
    {
      "epoch": 0.0006321192716041902,
      "grad_norm": 10682.018910299681,
      "learning_rate": 2.3973027298646153e-07,
      "loss": 1.7486,
      "step": 174176
    },
    {
      "epoch": 0.000632235405955027,
      "grad_norm": 8826.142758872644,
      "learning_rate": 2.3970823211684503e-07,
      "loss": 1.7371,
      "step": 174208
    },
    {
      "epoch": 0.0006323515403058636,
      "grad_norm": 8169.159809919255,
      "learning_rate": 2.396861973254423e-07,
      "loss": 1.7341,
      "step": 174240
    },
    {
      "epoch": 0.0006324676746567004,
      "grad_norm": 8249.27899879741,
      "learning_rate": 2.396641686094603e-07,
      "loss": 1.7188,
      "step": 174272
    },
    {
      "epoch": 0.000632583809007537,
      "grad_norm": 9810.549831686296,
      "learning_rate": 2.396421459661077e-07,
      "loss": 1.7298,
      "step": 174304
    },
    {
      "epoch": 0.0006326999433583738,
      "grad_norm": 10952.07596759628,
      "learning_rate": 2.396201293925949e-07,
      "loss": 1.7566,
      "step": 174336
    },
    {
      "epoch": 0.0006328160777092104,
      "grad_norm": 9348.21137972393,
      "learning_rate": 2.395981188861341e-07,
      "loss": 1.7449,
      "step": 174368
    },
    {
      "epoch": 0.0006329322120600472,
      "grad_norm": 9547.236877756832,
      "learning_rate": 2.395761144439395e-07,
      "loss": 1.7063,
      "step": 174400
    },
    {
      "epoch": 0.0006330483464108839,
      "grad_norm": 17633.002466965176,
      "learning_rate": 2.395541160632269e-07,
      "loss": 1.72,
      "step": 174432
    },
    {
      "epoch": 0.0006331644807617206,
      "grad_norm": 9407.47011688052,
      "learning_rate": 2.3953281090959544e-07,
      "loss": 1.7417,
      "step": 174464
    },
    {
      "epoch": 0.0006332806151125573,
      "grad_norm": 9188.302237083844,
      "learning_rate": 2.3951082445429593e-07,
      "loss": 1.7553,
      "step": 174496
    },
    {
      "epoch": 0.000633396749463394,
      "grad_norm": 10258.316236108145,
      "learning_rate": 2.3948884405222336e-07,
      "loss": 1.7577,
      "step": 174528
    },
    {
      "epoch": 0.0006335128838142307,
      "grad_norm": 10980.619290367917,
      "learning_rate": 2.3946686970060073e-07,
      "loss": 1.7531,
      "step": 174560
    },
    {
      "epoch": 0.0006336290181650674,
      "grad_norm": 9231.923743185924,
      "learning_rate": 2.3944490139665274e-07,
      "loss": 1.7335,
      "step": 174592
    },
    {
      "epoch": 0.0006337451525159041,
      "grad_norm": 9711.986408557212,
      "learning_rate": 2.3942293913760577e-07,
      "loss": 1.7362,
      "step": 174624
    },
    {
      "epoch": 0.0006338612868667408,
      "grad_norm": 9210.087947462825,
      "learning_rate": 2.3940098292068823e-07,
      "loss": 1.7308,
      "step": 174656
    },
    {
      "epoch": 0.0006339774212175775,
      "grad_norm": 9325.938451437474,
      "learning_rate": 2.393790327431301e-07,
      "loss": 1.7423,
      "step": 174688
    },
    {
      "epoch": 0.0006340935555684143,
      "grad_norm": 9191.38139780958,
      "learning_rate": 2.393570886021632e-07,
      "loss": 1.7393,
      "step": 174720
    },
    {
      "epoch": 0.0006342096899192509,
      "grad_norm": 9061.224751654712,
      "learning_rate": 2.393351504950212e-07,
      "loss": 1.7165,
      "step": 174752
    },
    {
      "epoch": 0.0006343258242700877,
      "grad_norm": 9752.756123271001,
      "learning_rate": 2.393132184189394e-07,
      "loss": 1.7189,
      "step": 174784
    },
    {
      "epoch": 0.0006344419586209243,
      "grad_norm": 8954.989335560373,
      "learning_rate": 2.3929129237115496e-07,
      "loss": 1.7259,
      "step": 174816
    },
    {
      "epoch": 0.0006345580929717611,
      "grad_norm": 8091.656196354365,
      "learning_rate": 2.3926937234890684e-07,
      "loss": 1.7401,
      "step": 174848
    },
    {
      "epoch": 0.0006346742273225977,
      "grad_norm": 8221.101750982043,
      "learning_rate": 2.392474583494357e-07,
      "loss": 1.7542,
      "step": 174880
    },
    {
      "epoch": 0.0006347903616734345,
      "grad_norm": 8626.762776383735,
      "learning_rate": 2.39225550369984e-07,
      "loss": 1.7454,
      "step": 174912
    },
    {
      "epoch": 0.0006349064960242711,
      "grad_norm": 10195.565408548953,
      "learning_rate": 2.3920364840779594e-07,
      "loss": 1.7153,
      "step": 174944
    },
    {
      "epoch": 0.0006350226303751079,
      "grad_norm": 8778.260875594891,
      "learning_rate": 2.391817524601176e-07,
      "loss": 1.7182,
      "step": 174976
    },
    {
      "epoch": 0.0006351387647259446,
      "grad_norm": 8716.642013986808,
      "learning_rate": 2.3915986252419667e-07,
      "loss": 1.7215,
      "step": 175008
    },
    {
      "epoch": 0.0006352548990767813,
      "grad_norm": 8468.755752765574,
      "learning_rate": 2.3913797859728267e-07,
      "loss": 1.7421,
      "step": 175040
    },
    {
      "epoch": 0.000635371033427618,
      "grad_norm": 9674.756017595482,
      "learning_rate": 2.3911610067662694e-07,
      "loss": 1.7186,
      "step": 175072
    },
    {
      "epoch": 0.0006354871677784547,
      "grad_norm": 10135.655479543491,
      "learning_rate": 2.390942287594825e-07,
      "loss": 1.7197,
      "step": 175104
    },
    {
      "epoch": 0.0006356033021292914,
      "grad_norm": 9539.92285084109,
      "learning_rate": 2.3907236284310407e-07,
      "loss": 1.746,
      "step": 175136
    },
    {
      "epoch": 0.0006357194364801281,
      "grad_norm": 9041.660798769217,
      "learning_rate": 2.3905050292474825e-07,
      "loss": 1.7399,
      "step": 175168
    },
    {
      "epoch": 0.0006358355708309648,
      "grad_norm": 9809.130644455705,
      "learning_rate": 2.390286490016735e-07,
      "loss": 1.7558,
      "step": 175200
    },
    {
      "epoch": 0.0006359517051818015,
      "grad_norm": 11236.881061931732,
      "learning_rate": 2.390068010711396e-07,
      "loss": 1.7475,
      "step": 175232
    },
    {
      "epoch": 0.0006360678395326382,
      "grad_norm": 10178.7661334761,
      "learning_rate": 2.3898495913040864e-07,
      "loss": 1.731,
      "step": 175264
    },
    {
      "epoch": 0.000636183973883475,
      "grad_norm": 11043.569893834148,
      "learning_rate": 2.3896312317674403e-07,
      "loss": 1.7319,
      "step": 175296
    },
    {
      "epoch": 0.0006363001082343116,
      "grad_norm": 11009.400710302083,
      "learning_rate": 2.389412932074112e-07,
      "loss": 1.7406,
      "step": 175328
    },
    {
      "epoch": 0.0006364162425851484,
      "grad_norm": 10380.563953851448,
      "learning_rate": 2.389194692196771e-07,
      "loss": 1.7506,
      "step": 175360
    },
    {
      "epoch": 0.000636532376935985,
      "grad_norm": 9811.439140105798,
      "learning_rate": 2.388976512108106e-07,
      "loss": 1.7475,
      "step": 175392
    },
    {
      "epoch": 0.0006366485112868218,
      "grad_norm": 9901.756813818445,
      "learning_rate": 2.3887583917808235e-07,
      "loss": 1.7273,
      "step": 175424
    },
    {
      "epoch": 0.0006367646456376584,
      "grad_norm": 9823.313901123185,
      "learning_rate": 2.3885471446772746e-07,
      "loss": 1.7346,
      "step": 175456
    },
    {
      "epoch": 0.0006368807799884952,
      "grad_norm": 9869.106342521596,
      "learning_rate": 2.388329141925515e-07,
      "loss": 1.7695,
      "step": 175488
    },
    {
      "epoch": 0.0006369969143393318,
      "grad_norm": 9662.011798792217,
      "learning_rate": 2.38811119885421e-07,
      "loss": 1.741,
      "step": 175520
    },
    {
      "epoch": 0.0006371130486901686,
      "grad_norm": 9696.899091977806,
      "learning_rate": 2.387893315436134e-07,
      "loss": 1.7232,
      "step": 175552
    },
    {
      "epoch": 0.0006372291830410053,
      "grad_norm": 9321.023978083094,
      "learning_rate": 2.387675491644079e-07,
      "loss": 1.7335,
      "step": 175584
    },
    {
      "epoch": 0.000637345317391842,
      "grad_norm": 9155.614670790816,
      "learning_rate": 2.3874577274508544e-07,
      "loss": 1.7071,
      "step": 175616
    },
    {
      "epoch": 0.0006374614517426787,
      "grad_norm": 11698.855670534618,
      "learning_rate": 2.387240022829287e-07,
      "loss": 1.7069,
      "step": 175648
    },
    {
      "epoch": 0.0006375775860935154,
      "grad_norm": 8366.327629252874,
      "learning_rate": 2.387022377752222e-07,
      "loss": 1.709,
      "step": 175680
    },
    {
      "epoch": 0.0006376937204443521,
      "grad_norm": 9569.92309268993,
      "learning_rate": 2.38680479219252e-07,
      "loss": 1.7349,
      "step": 175712
    },
    {
      "epoch": 0.0006378098547951888,
      "grad_norm": 9366.438917753107,
      "learning_rate": 2.38658726612306e-07,
      "loss": 1.7417,
      "step": 175744
    },
    {
      "epoch": 0.0006379259891460255,
      "grad_norm": 9406.810298927048,
      "learning_rate": 2.3863697995167383e-07,
      "loss": 1.7176,
      "step": 175776
    },
    {
      "epoch": 0.0006380421234968622,
      "grad_norm": 10357.190642254298,
      "learning_rate": 2.3861523923464686e-07,
      "loss": 1.7422,
      "step": 175808
    },
    {
      "epoch": 0.0006381582578476989,
      "grad_norm": 8586.344158022086,
      "learning_rate": 2.385935044585181e-07,
      "loss": 1.7454,
      "step": 175840
    },
    {
      "epoch": 0.0006382743921985357,
      "grad_norm": 10019.938223362458,
      "learning_rate": 2.3857177562058237e-07,
      "loss": 1.7551,
      "step": 175872
    },
    {
      "epoch": 0.0006383905265493723,
      "grad_norm": 8565.679541052186,
      "learning_rate": 2.385500527181362e-07,
      "loss": 1.7593,
      "step": 175904
    },
    {
      "epoch": 0.0006385066609002091,
      "grad_norm": 9199.793693338997,
      "learning_rate": 2.385283357484778e-07,
      "loss": 1.7471,
      "step": 175936
    },
    {
      "epoch": 0.0006386227952510457,
      "grad_norm": 9980.4711311641,
      "learning_rate": 2.385066247089072e-07,
      "loss": 1.7097,
      "step": 175968
    },
    {
      "epoch": 0.0006387389296018825,
      "grad_norm": 8108.043167127319,
      "learning_rate": 2.3848491959672603e-07,
      "loss": 1.727,
      "step": 176000
    },
    {
      "epoch": 0.0006388550639527191,
      "grad_norm": 10366.771725083947,
      "learning_rate": 2.3846322040923773e-07,
      "loss": 1.7303,
      "step": 176032
    },
    {
      "epoch": 0.0006389711983035559,
      "grad_norm": 9401.597311095598,
      "learning_rate": 2.384415271437474e-07,
      "loss": 1.7512,
      "step": 176064
    },
    {
      "epoch": 0.0006390873326543925,
      "grad_norm": 9294.051215697062,
      "learning_rate": 2.3841983979756187e-07,
      "loss": 1.7278,
      "step": 176096
    },
    {
      "epoch": 0.0006392034670052293,
      "grad_norm": 9280.625625462973,
      "learning_rate": 2.3839815836798966e-07,
      "loss": 1.7108,
      "step": 176128
    },
    {
      "epoch": 0.000639319601356066,
      "grad_norm": 9352.403113638762,
      "learning_rate": 2.3837648285234108e-07,
      "loss": 1.7386,
      "step": 176160
    },
    {
      "epoch": 0.0006394357357069027,
      "grad_norm": 10568.167674672843,
      "learning_rate": 2.3835481324792813e-07,
      "loss": 1.7342,
      "step": 176192
    },
    {
      "epoch": 0.0006395518700577394,
      "grad_norm": 10163.986324272579,
      "learning_rate": 2.3833314955206445e-07,
      "loss": 1.7437,
      "step": 176224
    },
    {
      "epoch": 0.0006396680044085761,
      "grad_norm": 9347.193375553969,
      "learning_rate": 2.3831149176206543e-07,
      "loss": 1.7513,
      "step": 176256
    },
    {
      "epoch": 0.0006397841387594128,
      "grad_norm": 9183.342202052583,
      "learning_rate": 2.382898398752482e-07,
      "loss": 1.7376,
      "step": 176288
    },
    {
      "epoch": 0.0006399002731102495,
      "grad_norm": 9651.424972510536,
      "learning_rate": 2.3826819388893152e-07,
      "loss": 1.7309,
      "step": 176320
    },
    {
      "epoch": 0.0006400164074610862,
      "grad_norm": 8673.190647045642,
      "learning_rate": 2.38246553800436e-07,
      "loss": 1.7556,
      "step": 176352
    },
    {
      "epoch": 0.0006401325418119229,
      "grad_norm": 10171.13680961966,
      "learning_rate": 2.3822491960708376e-07,
      "loss": 1.7402,
      "step": 176384
    },
    {
      "epoch": 0.0006402486761627596,
      "grad_norm": 9388.068171887122,
      "learning_rate": 2.3820329130619873e-07,
      "loss": 1.7286,
      "step": 176416
    },
    {
      "epoch": 0.0006403648105135964,
      "grad_norm": 9006.401057026053,
      "learning_rate": 2.3818234450632761e-07,
      "loss": 1.7171,
      "step": 176448
    },
    {
      "epoch": 0.000640480944864433,
      "grad_norm": 9330.54189208751,
      "learning_rate": 2.3816072779842353e-07,
      "loss": 1.7356,
      "step": 176480
    },
    {
      "epoch": 0.0006405970792152698,
      "grad_norm": 9222.25221949606,
      "learning_rate": 2.381391169750521e-07,
      "loss": 1.7452,
      "step": 176512
    },
    {
      "epoch": 0.0006407132135661064,
      "grad_norm": 9126.841512812633,
      "learning_rate": 2.381175120335439e-07,
      "loss": 1.7218,
      "step": 176544
    },
    {
      "epoch": 0.0006408293479169432,
      "grad_norm": 9662.312973610407,
      "learning_rate": 2.3809591297123134e-07,
      "loss": 1.7329,
      "step": 176576
    },
    {
      "epoch": 0.0006409454822677798,
      "grad_norm": 9277.178450369487,
      "learning_rate": 2.3807431978544845e-07,
      "loss": 1.755,
      "step": 176608
    },
    {
      "epoch": 0.0006410616166186166,
      "grad_norm": 8685.418585192081,
      "learning_rate": 2.3805273247353096e-07,
      "loss": 1.7211,
      "step": 176640
    },
    {
      "epoch": 0.0006411777509694532,
      "grad_norm": 9758.395462369825,
      "learning_rate": 2.3803115103281635e-07,
      "loss": 1.7244,
      "step": 176672
    },
    {
      "epoch": 0.00064129388532029,
      "grad_norm": 9480.25685306047,
      "learning_rate": 2.380095754606437e-07,
      "loss": 1.7274,
      "step": 176704
    },
    {
      "epoch": 0.0006414100196711267,
      "grad_norm": 10288.843861192569,
      "learning_rate": 2.3798800575435377e-07,
      "loss": 1.7352,
      "step": 176736
    },
    {
      "epoch": 0.0006415261540219634,
      "grad_norm": 10224.361495956606,
      "learning_rate": 2.3796644191128912e-07,
      "loss": 1.7171,
      "step": 176768
    },
    {
      "epoch": 0.0006416422883728001,
      "grad_norm": 8711.758490683727,
      "learning_rate": 2.379448839287939e-07,
      "loss": 1.717,
      "step": 176800
    },
    {
      "epoch": 0.0006417584227236368,
      "grad_norm": 8954.849970825866,
      "learning_rate": 2.3792333180421396e-07,
      "loss": 1.7352,
      "step": 176832
    },
    {
      "epoch": 0.0006418745570744735,
      "grad_norm": 9368.423880247947,
      "learning_rate": 2.3790178553489688e-07,
      "loss": 1.7447,
      "step": 176864
    },
    {
      "epoch": 0.0006419906914253102,
      "grad_norm": 8749.62742063912,
      "learning_rate": 2.3788024511819188e-07,
      "loss": 1.7427,
      "step": 176896
    },
    {
      "epoch": 0.0006421068257761469,
      "grad_norm": 10053.005421265821,
      "learning_rate": 2.378587105514498e-07,
      "loss": 1.7499,
      "step": 176928
    },
    {
      "epoch": 0.0006422229601269836,
      "grad_norm": 9009.866258718828,
      "learning_rate": 2.3783718183202335e-07,
      "loss": 1.7254,
      "step": 176960
    },
    {
      "epoch": 0.0006423390944778203,
      "grad_norm": 8880.197745545986,
      "learning_rate": 2.3781565895726668e-07,
      "loss": 1.7182,
      "step": 176992
    },
    {
      "epoch": 0.0006424552288286571,
      "grad_norm": 9225.215878232877,
      "learning_rate": 2.3779414192453575e-07,
      "loss": 1.7245,
      "step": 177024
    },
    {
      "epoch": 0.0006425713631794937,
      "grad_norm": 10005.721163414459,
      "learning_rate": 2.3777263073118817e-07,
      "loss": 1.7472,
      "step": 177056
    },
    {
      "epoch": 0.0006426874975303305,
      "grad_norm": 10051.897333339612,
      "learning_rate": 2.3775112537458327e-07,
      "loss": 1.7654,
      "step": 177088
    },
    {
      "epoch": 0.0006428036318811671,
      "grad_norm": 9411.057963906078,
      "learning_rate": 2.3772962585208194e-07,
      "loss": 1.7267,
      "step": 177120
    },
    {
      "epoch": 0.0006429197662320039,
      "grad_norm": 9081.571890372283,
      "learning_rate": 2.3770813216104685e-07,
      "loss": 1.7389,
      "step": 177152
    },
    {
      "epoch": 0.0006430359005828405,
      "grad_norm": 9640.773205505873,
      "learning_rate": 2.3768664429884228e-07,
      "loss": 1.7599,
      "step": 177184
    },
    {
      "epoch": 0.0006431520349336773,
      "grad_norm": 9494.867455630963,
      "learning_rate": 2.376651622628342e-07,
      "loss": 1.7674,
      "step": 177216
    },
    {
      "epoch": 0.0006432681692845139,
      "grad_norm": 9372.85506129269,
      "learning_rate": 2.376436860503902e-07,
      "loss": 1.7403,
      "step": 177248
    },
    {
      "epoch": 0.0006433843036353507,
      "grad_norm": 9714.790064638557,
      "learning_rate": 2.3762221565887963e-07,
      "loss": 1.7365,
      "step": 177280
    },
    {
      "epoch": 0.0006435004379861874,
      "grad_norm": 9324.108965472251,
      "learning_rate": 2.3760075108567345e-07,
      "loss": 1.7042,
      "step": 177312
    },
    {
      "epoch": 0.0006436165723370241,
      "grad_norm": 10990.961377422815,
      "learning_rate": 2.3757929232814418e-07,
      "loss": 1.7066,
      "step": 177344
    },
    {
      "epoch": 0.0006437327066878608,
      "grad_norm": 9133.882526067433,
      "learning_rate": 2.375578393836662e-07,
      "loss": 1.7107,
      "step": 177376
    },
    {
      "epoch": 0.0006438488410386975,
      "grad_norm": 10613.35931738863,
      "learning_rate": 2.3753639224961538e-07,
      "loss": 1.72,
      "step": 177408
    },
    {
      "epoch": 0.0006439649753895342,
      "grad_norm": 9151.277724995565,
      "learning_rate": 2.3751495092336938e-07,
      "loss": 1.722,
      "step": 177440
    },
    {
      "epoch": 0.0006440811097403709,
      "grad_norm": 9196.212698714618,
      "learning_rate": 2.3749418517449517e-07,
      "loss": 1.7194,
      "step": 177472
    },
    {
      "epoch": 0.0006441972440912076,
      "grad_norm": 9357.786917856165,
      "learning_rate": 2.3747275527470767e-07,
      "loss": 1.7515,
      "step": 177504
    },
    {
      "epoch": 0.0006443133784420443,
      "grad_norm": 9453.821872660814,
      "learning_rate": 2.374513311749494e-07,
      "loss": 1.7375,
      "step": 177536
    },
    {
      "epoch": 0.000644429512792881,
      "grad_norm": 7643.672154141621,
      "learning_rate": 2.374299128726046e-07,
      "loss": 1.7421,
      "step": 177568
    },
    {
      "epoch": 0.0006445456471437178,
      "grad_norm": 10461.196489885848,
      "learning_rate": 2.3740850036505904e-07,
      "loss": 1.7471,
      "step": 177600
    },
    {
      "epoch": 0.0006446617814945544,
      "grad_norm": 10092.862626628781,
      "learning_rate": 2.373870936497002e-07,
      "loss": 1.7508,
      "step": 177632
    },
    {
      "epoch": 0.0006447779158453912,
      "grad_norm": 9202.268198656242,
      "learning_rate": 2.3736569272391722e-07,
      "loss": 1.7309,
      "step": 177664
    },
    {
      "epoch": 0.0006448940501962278,
      "grad_norm": 9598.357984572152,
      "learning_rate": 2.3734429758510085e-07,
      "loss": 1.7331,
      "step": 177696
    },
    {
      "epoch": 0.0006450101845470646,
      "grad_norm": 8569.398812052103,
      "learning_rate": 2.3732290823064347e-07,
      "loss": 1.7295,
      "step": 177728
    },
    {
      "epoch": 0.0006451263188979012,
      "grad_norm": 10668.999203299249,
      "learning_rate": 2.3730152465793913e-07,
      "loss": 1.7428,
      "step": 177760
    },
    {
      "epoch": 0.000645242453248738,
      "grad_norm": 8714.608998687205,
      "learning_rate": 2.3728014686438354e-07,
      "loss": 1.7271,
      "step": 177792
    },
    {
      "epoch": 0.0006453585875995746,
      "grad_norm": 8660.642123999813,
      "learning_rate": 2.3725877484737408e-07,
      "loss": 1.7064,
      "step": 177824
    },
    {
      "epoch": 0.0006454747219504114,
      "grad_norm": 9943.23991463547,
      "learning_rate": 2.3723740860430967e-07,
      "loss": 1.7454,
      "step": 177856
    },
    {
      "epoch": 0.000645590856301248,
      "grad_norm": 9948.972409249109,
      "learning_rate": 2.3721604813259092e-07,
      "loss": 1.7287,
      "step": 177888
    },
    {
      "epoch": 0.0006457069906520848,
      "grad_norm": 9867.285543653838,
      "learning_rate": 2.3719469342962014e-07,
      "loss": 1.7321,
      "step": 177920
    },
    {
      "epoch": 0.0006458231250029215,
      "grad_norm": 11207.017801360003,
      "learning_rate": 2.3717334449280118e-07,
      "loss": 1.7427,
      "step": 177952
    },
    {
      "epoch": 0.0006459392593537582,
      "grad_norm": 10674.683508188895,
      "learning_rate": 2.3715200131953954e-07,
      "loss": 1.7272,
      "step": 177984
    },
    {
      "epoch": 0.000646055393704595,
      "grad_norm": 10252.922900324571,
      "learning_rate": 2.3713066390724243e-07,
      "loss": 1.7328,
      "step": 178016
    },
    {
      "epoch": 0.0006461715280554316,
      "grad_norm": 10362.574969571993,
      "learning_rate": 2.3710933225331867e-07,
      "loss": 1.746,
      "step": 178048
    },
    {
      "epoch": 0.0006462876624062683,
      "grad_norm": 10220.910526954045,
      "learning_rate": 2.370880063551786e-07,
      "loss": 1.7676,
      "step": 178080
    },
    {
      "epoch": 0.000646403796757105,
      "grad_norm": 8308.711693156767,
      "learning_rate": 2.3706668621023436e-07,
      "loss": 1.7439,
      "step": 178112
    },
    {
      "epoch": 0.0006465199311079417,
      "grad_norm": 9976.032878855201,
      "learning_rate": 2.3704537181589957e-07,
      "loss": 1.7037,
      "step": 178144
    },
    {
      "epoch": 0.0006466360654587784,
      "grad_norm": 9389.527783653446,
      "learning_rate": 2.3702406316958958e-07,
      "loss": 1.7163,
      "step": 178176
    },
    {
      "epoch": 0.0006467521998096151,
      "grad_norm": 8623.075669388505,
      "learning_rate": 2.370027602687213e-07,
      "loss": 1.7462,
      "step": 178208
    },
    {
      "epoch": 0.0006468683341604519,
      "grad_norm": 8614.959431129088,
      "learning_rate": 2.3698146311071333e-07,
      "loss": 1.742,
      "step": 178240
    },
    {
      "epoch": 0.0006469844685112885,
      "grad_norm": 10052.522767942382,
      "learning_rate": 2.369601716929858e-07,
      "loss": 1.7455,
      "step": 178272
    },
    {
      "epoch": 0.0006471006028621253,
      "grad_norm": 9834.375425007935,
      "learning_rate": 2.3693888601296057e-07,
      "loss": 1.7293,
      "step": 178304
    },
    {
      "epoch": 0.000647216737212962,
      "grad_norm": 10092.64643193251,
      "learning_rate": 2.36917606068061e-07,
      "loss": 1.7172,
      "step": 178336
    },
    {
      "epoch": 0.0006473328715637987,
      "grad_norm": 8749.568789374709,
      "learning_rate": 2.3689633185571224e-07,
      "loss": 1.7337,
      "step": 178368
    },
    {
      "epoch": 0.0006474490059146353,
      "grad_norm": 8917.401639491181,
      "learning_rate": 2.3687506337334086e-07,
      "loss": 1.7311,
      "step": 178400
    },
    {
      "epoch": 0.0006475651402654721,
      "grad_norm": 9417.877043155746,
      "learning_rate": 2.368538006183752e-07,
      "loss": 1.7384,
      "step": 178432
    },
    {
      "epoch": 0.0006476812746163087,
      "grad_norm": 10038.964289208325,
      "learning_rate": 2.3683320778380702e-07,
      "loss": 1.7267,
      "step": 178464
    },
    {
      "epoch": 0.0006477974089671455,
      "grad_norm": 9788.318548147072,
      "learning_rate": 2.368119562971621e-07,
      "loss": 1.709,
      "step": 178496
    },
    {
      "epoch": 0.0006479135433179823,
      "grad_norm": 8079.845295548671,
      "learning_rate": 2.367907105302976e-07,
      "loss": 1.7335,
      "step": 178528
    },
    {
      "epoch": 0.0006480296776688189,
      "grad_norm": 8411.750471810252,
      "learning_rate": 2.3676947048064824e-07,
      "loss": 1.7394,
      "step": 178560
    },
    {
      "epoch": 0.0006481458120196557,
      "grad_norm": 9633.411649047288,
      "learning_rate": 2.367482361456503e-07,
      "loss": 1.7316,
      "step": 178592
    },
    {
      "epoch": 0.0006482619463704923,
      "grad_norm": 9023.57501215566,
      "learning_rate": 2.367270075227417e-07,
      "loss": 1.7507,
      "step": 178624
    },
    {
      "epoch": 0.000648378080721329,
      "grad_norm": 8977.860101382734,
      "learning_rate": 2.3670578460936192e-07,
      "loss": 1.7427,
      "step": 178656
    },
    {
      "epoch": 0.0006484942150721657,
      "grad_norm": 7781.4479372415,
      "learning_rate": 2.3668456740295212e-07,
      "loss": 1.6992,
      "step": 178688
    },
    {
      "epoch": 0.0006486103494230025,
      "grad_norm": 8654.806063685079,
      "learning_rate": 2.3666335590095498e-07,
      "loss": 1.7147,
      "step": 178720
    },
    {
      "epoch": 0.0006487264837738391,
      "grad_norm": 9595.03996865047,
      "learning_rate": 2.366421501008149e-07,
      "loss": 1.7314,
      "step": 178752
    },
    {
      "epoch": 0.0006488426181246759,
      "grad_norm": 10069.832371991104,
      "learning_rate": 2.3662094999997775e-07,
      "loss": 1.7484,
      "step": 178784
    },
    {
      "epoch": 0.0006489587524755126,
      "grad_norm": 9365.279280405897,
      "learning_rate": 2.365997555958911e-07,
      "loss": 1.7333,
      "step": 178816
    },
    {
      "epoch": 0.0006490748868263493,
      "grad_norm": 9516.535819298953,
      "learning_rate": 2.365785668860041e-07,
      "loss": 1.7361,
      "step": 178848
    },
    {
      "epoch": 0.000649191021177186,
      "grad_norm": 10091.001932414838,
      "learning_rate": 2.3655738386776742e-07,
      "loss": 1.761,
      "step": 178880
    },
    {
      "epoch": 0.0006493071555280227,
      "grad_norm": 9177.317690916012,
      "learning_rate": 2.3653620653863347e-07,
      "loss": 1.7607,
      "step": 178912
    },
    {
      "epoch": 0.0006494232898788594,
      "grad_norm": 8659.335309364109,
      "learning_rate": 2.365150348960561e-07,
      "loss": 1.768,
      "step": 178944
    },
    {
      "epoch": 0.000649539424229696,
      "grad_norm": 7814.612722329879,
      "learning_rate": 2.3649386893749093e-07,
      "loss": 1.7445,
      "step": 178976
    },
    {
      "epoch": 0.0006496555585805328,
      "grad_norm": 9095.365413220075,
      "learning_rate": 2.3647270866039498e-07,
      "loss": 1.7227,
      "step": 179008
    },
    {
      "epoch": 0.0006497716929313695,
      "grad_norm": 9700.02443295892,
      "learning_rate": 2.36451554062227e-07,
      "loss": 1.7089,
      "step": 179040
    },
    {
      "epoch": 0.0006498878272822062,
      "grad_norm": 9237.280768711104,
      "learning_rate": 2.3643040514044732e-07,
      "loss": 1.7118,
      "step": 179072
    },
    {
      "epoch": 0.000650003961633043,
      "grad_norm": 9548.605447917513,
      "learning_rate": 2.3640926189251775e-07,
      "loss": 1.7203,
      "step": 179104
    },
    {
      "epoch": 0.0006501200959838796,
      "grad_norm": 10304.54579299835,
      "learning_rate": 2.3638812431590185e-07,
      "loss": 1.7152,
      "step": 179136
    },
    {
      "epoch": 0.0006502362303347164,
      "grad_norm": 8624.658370045738,
      "learning_rate": 2.3636699240806465e-07,
      "loss": 1.7048,
      "step": 179168
    },
    {
      "epoch": 0.000650352364685553,
      "grad_norm": 9576.241956007587,
      "learning_rate": 2.363458661664728e-07,
      "loss": 1.7363,
      "step": 179200
    },
    {
      "epoch": 0.0006504684990363898,
      "grad_norm": 9441.55400344668,
      "learning_rate": 2.363247455885945e-07,
      "loss": 1.749,
      "step": 179232
    },
    {
      "epoch": 0.0006505846333872264,
      "grad_norm": 7994.836208453554,
      "learning_rate": 2.3630363067189965e-07,
      "loss": 1.7513,
      "step": 179264
    },
    {
      "epoch": 0.0006507007677380632,
      "grad_norm": 9704.206613628958,
      "learning_rate": 2.3628252141385964e-07,
      "loss": 1.7354,
      "step": 179296
    },
    {
      "epoch": 0.0006508169020888998,
      "grad_norm": 9042.18756717643,
      "learning_rate": 2.362614178119474e-07,
      "loss": 1.7395,
      "step": 179328
    },
    {
      "epoch": 0.0006509330364397366,
      "grad_norm": 10158.392786262992,
      "learning_rate": 2.362403198636375e-07,
      "loss": 1.7224,
      "step": 179360
    },
    {
      "epoch": 0.0006510491707905733,
      "grad_norm": 9270.574739464646,
      "learning_rate": 2.3621922756640612e-07,
      "loss": 1.7309,
      "step": 179392
    },
    {
      "epoch": 0.00065116530514141,
      "grad_norm": 8478.669353147343,
      "learning_rate": 2.361981409177309e-07,
      "loss": 1.7471,
      "step": 179424
    },
    {
      "epoch": 0.0006512814394922467,
      "grad_norm": 9833.823366320956,
      "learning_rate": 2.3617771861098633e-07,
      "loss": 1.7542,
      "step": 179456
    },
    {
      "epoch": 0.0006513975738430834,
      "grad_norm": 10107.788284288506,
      "learning_rate": 2.3615664307554127e-07,
      "loss": 1.7244,
      "step": 179488
    },
    {
      "epoch": 0.0006515137081939201,
      "grad_norm": 10323.340932082017,
      "learning_rate": 2.3613557318117368e-07,
      "loss": 1.7208,
      "step": 179520
    },
    {
      "epoch": 0.0006516298425447568,
      "grad_norm": 9072.773556085262,
      "learning_rate": 2.3611450892536754e-07,
      "loss": 1.728,
      "step": 179552
    },
    {
      "epoch": 0.0006517459768955935,
      "grad_norm": 8337.372367838681,
      "learning_rate": 2.3609345030560836e-07,
      "loss": 1.7283,
      "step": 179584
    },
    {
      "epoch": 0.0006518621112464302,
      "grad_norm": 10082.665421405196,
      "learning_rate": 2.3607239731938323e-07,
      "loss": 1.737,
      "step": 179616
    },
    {
      "epoch": 0.0006519782455972669,
      "grad_norm": 8831.977128593575,
      "learning_rate": 2.3605134996418086e-07,
      "loss": 1.7345,
      "step": 179648
    },
    {
      "epoch": 0.0006520943799481037,
      "grad_norm": 7916.921371341262,
      "learning_rate": 2.3603030823749148e-07,
      "loss": 1.7246,
      "step": 179680
    },
    {
      "epoch": 0.0006522105142989403,
      "grad_norm": 8618.894592695748,
      "learning_rate": 2.3600927213680692e-07,
      "loss": 1.709,
      "step": 179712
    },
    {
      "epoch": 0.0006523266486497771,
      "grad_norm": 9609.315688434843,
      "learning_rate": 2.3598824165962048e-07,
      "loss": 1.7363,
      "step": 179744
    },
    {
      "epoch": 0.0006524427830006137,
      "grad_norm": 9683.844381236204,
      "learning_rate": 2.3596721680342716e-07,
      "loss": 1.7602,
      "step": 179776
    },
    {
      "epoch": 0.0006525589173514505,
      "grad_norm": 8829.694785212001,
      "learning_rate": 2.3594619756572342e-07,
      "loss": 1.762,
      "step": 179808
    },
    {
      "epoch": 0.0006526750517022871,
      "grad_norm": 9661.218970709648,
      "learning_rate": 2.3592518394400733e-07,
      "loss": 1.7225,
      "step": 179840
    },
    {
      "epoch": 0.0006527911860531239,
      "grad_norm": 9244.062851365736,
      "learning_rate": 2.3590417593577851e-07,
      "loss": 1.7279,
      "step": 179872
    },
    {
      "epoch": 0.0006529073204039605,
      "grad_norm": 9311.326865705016,
      "learning_rate": 2.3588317353853814e-07,
      "loss": 1.7281,
      "step": 179904
    },
    {
      "epoch": 0.0006530234547547973,
      "grad_norm": 9205.145408954711,
      "learning_rate": 2.3586217674978893e-07,
      "loss": 1.7271,
      "step": 179936
    },
    {
      "epoch": 0.000653139589105634,
      "grad_norm": 9990.207905744504,
      "learning_rate": 2.3584118556703518e-07,
      "loss": 1.7303,
      "step": 179968
    },
    {
      "epoch": 0.0006532557234564707,
      "grad_norm": 8957.382430152238,
      "learning_rate": 2.358201999877827e-07,
      "loss": 1.7504,
      "step": 180000
    },
    {
      "epoch": 0.0006533718578073074,
      "grad_norm": 10612.701258397883,
      "learning_rate": 2.3579922000953898e-07,
      "loss": 1.722,
      "step": 180032
    },
    {
      "epoch": 0.0006534879921581441,
      "grad_norm": 10201.547921761678,
      "learning_rate": 2.3577824562981283e-07,
      "loss": 1.7213,
      "step": 180064
    },
    {
      "epoch": 0.0006536041265089808,
      "grad_norm": 9146.823055028452,
      "learning_rate": 2.3575727684611488e-07,
      "loss": 1.7304,
      "step": 180096
    },
    {
      "epoch": 0.0006537202608598175,
      "grad_norm": 10230.4766262379,
      "learning_rate": 2.3573631365595704e-07,
      "loss": 1.7532,
      "step": 180128
    },
    {
      "epoch": 0.0006538363952106542,
      "grad_norm": 9760.498552840423,
      "learning_rate": 2.3571535605685305e-07,
      "loss": 1.7231,
      "step": 180160
    },
    {
      "epoch": 0.0006539525295614909,
      "grad_norm": 10155.028311137297,
      "learning_rate": 2.3569440404631793e-07,
      "loss": 1.7146,
      "step": 180192
    },
    {
      "epoch": 0.0006540686639123276,
      "grad_norm": 9473.4293685022,
      "learning_rate": 2.3567345762186847e-07,
      "loss": 1.7452,
      "step": 180224
    },
    {
      "epoch": 0.0006541847982631644,
      "grad_norm": 9766.8764710116,
      "learning_rate": 2.3565251678102284e-07,
      "loss": 1.7308,
      "step": 180256
    },
    {
      "epoch": 0.000654300932614001,
      "grad_norm": 8023.7278119338025,
      "learning_rate": 2.356315815213008e-07,
      "loss": 1.7342,
      "step": 180288
    },
    {
      "epoch": 0.0006544170669648378,
      "grad_norm": 8974.186759812836,
      "learning_rate": 2.356106518402237e-07,
      "loss": 1.7392,
      "step": 180320
    },
    {
      "epoch": 0.0006545332013156744,
      "grad_norm": 11108.898325216593,
      "learning_rate": 2.3558972773531443e-07,
      "loss": 1.7431,
      "step": 180352
    },
    {
      "epoch": 0.0006546493356665112,
      "grad_norm": 10432.360423221582,
      "learning_rate": 2.3556880920409733e-07,
      "loss": 1.721,
      "step": 180384
    },
    {
      "epoch": 0.0006547654700173478,
      "grad_norm": 11005.687620498777,
      "learning_rate": 2.3554789624409838e-07,
      "loss": 1.7053,
      "step": 180416
    },
    {
      "epoch": 0.0006548816043681846,
      "grad_norm": 12044.53220345232,
      "learning_rate": 2.3552698885284505e-07,
      "loss": 1.7241,
      "step": 180448
    },
    {
      "epoch": 0.0006549977387190212,
      "grad_norm": 8183.716759517035,
      "learning_rate": 2.355067401256663e-07,
      "loss": 1.7342,
      "step": 180480
    },
    {
      "epoch": 0.000655113873069858,
      "grad_norm": 8554.884803432482,
      "learning_rate": 2.3548584369066117e-07,
      "loss": 1.7056,
      "step": 180512
    },
    {
      "epoch": 0.0006552300074206947,
      "grad_norm": 8827.497720192285,
      "learning_rate": 2.354649528170704e-07,
      "loss": 1.7243,
      "step": 180544
    },
    {
      "epoch": 0.0006553461417715314,
      "grad_norm": 10381.614806955611,
      "learning_rate": 2.3544406750242752e-07,
      "loss": 1.7595,
      "step": 180576
    },
    {
      "epoch": 0.0006554622761223681,
      "grad_norm": 10573.171898725566,
      "learning_rate": 2.3542318774426765e-07,
      "loss": 1.7654,
      "step": 180608
    },
    {
      "epoch": 0.0006555784104732048,
      "grad_norm": 10794.368994989933,
      "learning_rate": 2.3540231354012742e-07,
      "loss": 1.7743,
      "step": 180640
    },
    {
      "epoch": 0.0006556945448240415,
      "grad_norm": 9622.324251447775,
      "learning_rate": 2.3538144488754492e-07,
      "loss": 1.7546,
      "step": 180672
    },
    {
      "epoch": 0.0006558106791748782,
      "grad_norm": 10439.564262937414,
      "learning_rate": 2.3536058178405991e-07,
      "loss": 1.735,
      "step": 180704
    },
    {
      "epoch": 0.0006559268135257149,
      "grad_norm": 11556.610229647791,
      "learning_rate": 2.3533972422721356e-07,
      "loss": 1.7252,
      "step": 180736
    },
    {
      "epoch": 0.0006560429478765516,
      "grad_norm": 8155.516537902427,
      "learning_rate": 2.3531887221454858e-07,
      "loss": 1.7168,
      "step": 180768
    },
    {
      "epoch": 0.0006561590822273883,
      "grad_norm": 9587.79995619433,
      "learning_rate": 2.3529802574360928e-07,
      "loss": 1.7388,
      "step": 180800
    },
    {
      "epoch": 0.0006562752165782251,
      "grad_norm": 10249.561161337591,
      "learning_rate": 2.3527718481194138e-07,
      "loss": 1.7166,
      "step": 180832
    },
    {
      "epoch": 0.0006563913509290617,
      "grad_norm": 10126.611871697265,
      "learning_rate": 2.3525634941709223e-07,
      "loss": 1.7023,
      "step": 180864
    },
    {
      "epoch": 0.0006565074852798985,
      "grad_norm": 9122.201269430532,
      "learning_rate": 2.352355195566106e-07,
      "loss": 1.7219,
      "step": 180896
    },
    {
      "epoch": 0.0006566236196307351,
      "grad_norm": 10123.34608713937,
      "learning_rate": 2.352146952280469e-07,
      "loss": 1.7331,
      "step": 180928
    },
    {
      "epoch": 0.0006567397539815719,
      "grad_norm": 9486.48153953825,
      "learning_rate": 2.351938764289529e-07,
      "loss": 1.7429,
      "step": 180960
    },
    {
      "epoch": 0.0006568558883324085,
      "grad_norm": 7984.555591891135,
      "learning_rate": 2.3517306315688202e-07,
      "loss": 1.7519,
      "step": 180992
    },
    {
      "epoch": 0.0006569720226832453,
      "grad_norm": 8816.13486738945,
      "learning_rate": 2.3515225540938917e-07,
      "loss": 1.7441,
      "step": 181024
    },
    {
      "epoch": 0.0006570881570340819,
      "grad_norm": 8922.79855202391,
      "learning_rate": 2.3513145318403068e-07,
      "loss": 1.7082,
      "step": 181056
    },
    {
      "epoch": 0.0006572042913849187,
      "grad_norm": 8826.28053032533,
      "learning_rate": 2.351106564783645e-07,
      "loss": 1.7238,
      "step": 181088
    },
    {
      "epoch": 0.0006573204257357554,
      "grad_norm": 10239.584171244456,
      "learning_rate": 2.350898652899501e-07,
      "loss": 1.7307,
      "step": 181120
    },
    {
      "epoch": 0.0006574365600865921,
      "grad_norm": 8920.577896078257,
      "learning_rate": 2.3506907961634836e-07,
      "loss": 1.7477,
      "step": 181152
    },
    {
      "epoch": 0.0006575526944374288,
      "grad_norm": 10738.750020370155,
      "learning_rate": 2.3504829945512176e-07,
      "loss": 1.7453,
      "step": 181184
    },
    {
      "epoch": 0.0006576688287882655,
      "grad_norm": 8880.298418409147,
      "learning_rate": 2.350275248038342e-07,
      "loss": 1.742,
      "step": 181216
    },
    {
      "epoch": 0.0006577849631391022,
      "grad_norm": 9222.651787853643,
      "learning_rate": 2.3500675566005126e-07,
      "loss": 1.7345,
      "step": 181248
    },
    {
      "epoch": 0.0006579010974899389,
      "grad_norm": 8656.99555273075,
      "learning_rate": 2.3498599202133979e-07,
      "loss": 1.7297,
      "step": 181280
    },
    {
      "epoch": 0.0006580172318407756,
      "grad_norm": 9500.805334286142,
      "learning_rate": 2.3496523388526826e-07,
      "loss": 1.7265,
      "step": 181312
    },
    {
      "epoch": 0.0006581333661916123,
      "grad_norm": 8922.688159966143,
      "learning_rate": 2.3494448124940675e-07,
      "loss": 1.7221,
      "step": 181344
    },
    {
      "epoch": 0.000658249500542449,
      "grad_norm": 9976.522039267993,
      "learning_rate": 2.3492373411132663e-07,
      "loss": 1.7324,
      "step": 181376
    },
    {
      "epoch": 0.0006583656348932858,
      "grad_norm": 8942.229140432491,
      "learning_rate": 2.3490299246860096e-07,
      "loss": 1.7126,
      "step": 181408
    },
    {
      "epoch": 0.0006584817692441224,
      "grad_norm": 9147.943266111788,
      "learning_rate": 2.3488225631880414e-07,
      "loss": 1.7148,
      "step": 181440
    },
    {
      "epoch": 0.0006585979035949592,
      "grad_norm": 8986.04495871237,
      "learning_rate": 2.3486217340953092e-07,
      "loss": 1.7425,
      "step": 181472
    },
    {
      "epoch": 0.0006587140379457958,
      "grad_norm": 10271.111332275588,
      "learning_rate": 2.348414480668554e-07,
      "loss": 1.7711,
      "step": 181504
    },
    {
      "epoch": 0.0006588301722966326,
      "grad_norm": 9515.508394195236,
      "learning_rate": 2.3482072820991684e-07,
      "loss": 1.7225,
      "step": 181536
    },
    {
      "epoch": 0.0006589463066474692,
      "grad_norm": 10540.601405991974,
      "learning_rate": 2.3480001383629562e-07,
      "loss": 1.7354,
      "step": 181568
    },
    {
      "epoch": 0.000659062440998306,
      "grad_norm": 9709.818329917403,
      "learning_rate": 2.347793049435737e-07,
      "loss": 1.7517,
      "step": 181600
    },
    {
      "epoch": 0.0006591785753491426,
      "grad_norm": 10563.160322554988,
      "learning_rate": 2.3475860152933442e-07,
      "loss": 1.7388,
      "step": 181632
    },
    {
      "epoch": 0.0006592947096999794,
      "grad_norm": 9870.81394820103,
      "learning_rate": 2.3473790359116276e-07,
      "loss": 1.7265,
      "step": 181664
    },
    {
      "epoch": 0.0006594108440508161,
      "grad_norm": 9067.947066453355,
      "learning_rate": 2.34717211126645e-07,
      "loss": 1.7301,
      "step": 181696
    },
    {
      "epoch": 0.0006595269784016528,
      "grad_norm": 9192.572871617609,
      "learning_rate": 2.3469652413336912e-07,
      "loss": 1.7197,
      "step": 181728
    },
    {
      "epoch": 0.0006596431127524895,
      "grad_norm": 8937.762359785585,
      "learning_rate": 2.3467584260892443e-07,
      "loss": 1.7341,
      "step": 181760
    },
    {
      "epoch": 0.0006597592471033262,
      "grad_norm": 8983.868431805979,
      "learning_rate": 2.3465516655090176e-07,
      "loss": 1.7384,
      "step": 181792
    },
    {
      "epoch": 0.0006598753814541629,
      "grad_norm": 8978.941362989292,
      "learning_rate": 2.346344959568935e-07,
      "loss": 1.7419,
      "step": 181824
    },
    {
      "epoch": 0.0006599915158049996,
      "grad_norm": 8445.698313342717,
      "learning_rate": 2.346138308244934e-07,
      "loss": 1.7386,
      "step": 181856
    },
    {
      "epoch": 0.0006601076501558363,
      "grad_norm": 9040.864560427835,
      "learning_rate": 2.3459317115129687e-07,
      "loss": 1.7313,
      "step": 181888
    },
    {
      "epoch": 0.000660223784506673,
      "grad_norm": 9042.35190644558,
      "learning_rate": 2.345725169349006e-07,
      "loss": 1.7263,
      "step": 181920
    },
    {
      "epoch": 0.0006603399188575097,
      "grad_norm": 8558.212196481225,
      "learning_rate": 2.3455186817290287e-07,
      "loss": 1.7425,
      "step": 181952
    },
    {
      "epoch": 0.0006604560532083465,
      "grad_norm": 8416.863786470587,
      "learning_rate": 2.3453122486290345e-07,
      "loss": 1.746,
      "step": 181984
    },
    {
      "epoch": 0.0006605721875591831,
      "grad_norm": 10080.812665653499,
      "learning_rate": 2.3451058700250355e-07,
      "loss": 1.7294,
      "step": 182016
    },
    {
      "epoch": 0.0006606883219100199,
      "grad_norm": 10225.90944610796,
      "learning_rate": 2.344899545893059e-07,
      "loss": 1.739,
      "step": 182048
    },
    {
      "epoch": 0.0006608044562608565,
      "grad_norm": 9958.822219519736,
      "learning_rate": 2.3446932762091464e-07,
      "loss": 1.7209,
      "step": 182080
    },
    {
      "epoch": 0.0006609205906116933,
      "grad_norm": 10149.347171123865,
      "learning_rate": 2.344487060949354e-07,
      "loss": 1.7107,
      "step": 182112
    },
    {
      "epoch": 0.0006610367249625299,
      "grad_norm": 10318.748179890814,
      "learning_rate": 2.3442809000897537e-07,
      "loss": 1.723,
      "step": 182144
    },
    {
      "epoch": 0.0006611528593133667,
      "grad_norm": 9297.764785151321,
      "learning_rate": 2.3440747936064313e-07,
      "loss": 1.7256,
      "step": 182176
    },
    {
      "epoch": 0.0006612689936642033,
      "grad_norm": 9175.376395549121,
      "learning_rate": 2.3438687414754876e-07,
      "loss": 1.7037,
      "step": 182208
    },
    {
      "epoch": 0.0006613851280150401,
      "grad_norm": 8921.057672720202,
      "learning_rate": 2.3436627436730375e-07,
      "loss": 1.7147,
      "step": 182240
    },
    {
      "epoch": 0.0006615012623658768,
      "grad_norm": 9713.977764026435,
      "learning_rate": 2.3434568001752114e-07,
      "loss": 1.7358,
      "step": 182272
    },
    {
      "epoch": 0.0006616173967167135,
      "grad_norm": 9608.663382593855,
      "learning_rate": 2.343250910958154e-07,
      "loss": 1.746,
      "step": 182304
    },
    {
      "epoch": 0.0006617335310675502,
      "grad_norm": 8911.907315496499,
      "learning_rate": 2.3430450759980252e-07,
      "loss": 1.7674,
      "step": 182336
    },
    {
      "epoch": 0.0006618496654183869,
      "grad_norm": 9275.225819353403,
      "learning_rate": 2.3428392952709986e-07,
      "loss": 1.778,
      "step": 182368
    },
    {
      "epoch": 0.0006619657997692236,
      "grad_norm": 9436.942725268602,
      "learning_rate": 2.342633568753263e-07,
      "loss": 1.7512,
      "step": 182400
    },
    {
      "epoch": 0.0006620819341200603,
      "grad_norm": 9775.66908196058,
      "learning_rate": 2.3424278964210218e-07,
      "loss": 1.7263,
      "step": 182432
    },
    {
      "epoch": 0.000662198068470897,
      "grad_norm": 10920.406494265679,
      "learning_rate": 2.3422287029987277e-07,
      "loss": 1.7333,
      "step": 182464
    },
    {
      "epoch": 0.0006623142028217337,
      "grad_norm": 9157.059134896968,
      "learning_rate": 2.3420231372746932e-07,
      "loss": 1.7387,
      "step": 182496
    },
    {
      "epoch": 0.0006624303371725704,
      "grad_norm": 9190.643394235247,
      "learning_rate": 2.3418176256655925e-07,
      "loss": 1.7196,
      "step": 182528
    },
    {
      "epoch": 0.0006625464715234072,
      "grad_norm": 9942.588998847332,
      "learning_rate": 2.3416121681476882e-07,
      "loss": 1.7074,
      "step": 182560
    },
    {
      "epoch": 0.0006626626058742438,
      "grad_norm": 9530.760410376497,
      "learning_rate": 2.341406764697255e-07,
      "loss": 1.7152,
      "step": 182592
    },
    {
      "epoch": 0.0006627787402250806,
      "grad_norm": 9298.41298286971,
      "learning_rate": 2.341201415290584e-07,
      "loss": 1.7357,
      "step": 182624
    },
    {
      "epoch": 0.0006628948745759172,
      "grad_norm": 10663.527934037591,
      "learning_rate": 2.3409961199039804e-07,
      "loss": 1.732,
      "step": 182656
    },
    {
      "epoch": 0.000663011008926754,
      "grad_norm": 9098.441404988,
      "learning_rate": 2.3407908785137633e-07,
      "loss": 1.7356,
      "step": 182688
    },
    {
      "epoch": 0.0006631271432775906,
      "grad_norm": 9333.40741637265,
      "learning_rate": 2.340585691096267e-07,
      "loss": 1.747,
      "step": 182720
    },
    {
      "epoch": 0.0006632432776284274,
      "grad_norm": 8815.340719450383,
      "learning_rate": 2.3403805576278401e-07,
      "loss": 1.7254,
      "step": 182752
    },
    {
      "epoch": 0.000663359411979264,
      "grad_norm": 8172.093489430967,
      "learning_rate": 2.340175478084846e-07,
      "loss": 1.7216,
      "step": 182784
    },
    {
      "epoch": 0.0006634755463301008,
      "grad_norm": 9515.339930869523,
      "learning_rate": 2.339970452443662e-07,
      "loss": 1.7213,
      "step": 182816
    },
    {
      "epoch": 0.0006635916806809376,
      "grad_norm": 10253.962551131148,
      "learning_rate": 2.3397654806806805e-07,
      "loss": 1.7411,
      "step": 182848
    },
    {
      "epoch": 0.0006637078150317742,
      "grad_norm": 10322.16237035632,
      "learning_rate": 2.3395605627723079e-07,
      "loss": 1.728,
      "step": 182880
    },
    {
      "epoch": 0.000663823949382611,
      "grad_norm": 9341.200244079986,
      "learning_rate": 2.3393556986949656e-07,
      "loss": 1.7276,
      "step": 182912
    },
    {
      "epoch": 0.0006639400837334476,
      "grad_norm": 9571.341494273413,
      "learning_rate": 2.3391508884250883e-07,
      "loss": 1.7675,
      "step": 182944
    },
    {
      "epoch": 0.0006640562180842844,
      "grad_norm": 9674.757774745578,
      "learning_rate": 2.3389461319391267e-07,
      "loss": 1.7421,
      "step": 182976
    },
    {
      "epoch": 0.000664172352435121,
      "grad_norm": 9614.681689998895,
      "learning_rate": 2.3387414292135448e-07,
      "loss": 1.7355,
      "step": 183008
    },
    {
      "epoch": 0.0006642884867859578,
      "grad_norm": 7904.2021735276985,
      "learning_rate": 2.3385367802248215e-07,
      "loss": 1.7233,
      "step": 183040
    },
    {
      "epoch": 0.0006644046211367944,
      "grad_norm": 9006.14368084365,
      "learning_rate": 2.33833218494945e-07,
      "loss": 1.716,
      "step": 183072
    },
    {
      "epoch": 0.0006645207554876312,
      "grad_norm": 11061.212953379028,
      "learning_rate": 2.3381276433639384e-07,
      "loss": 1.7081,
      "step": 183104
    },
    {
      "epoch": 0.0006646368898384679,
      "grad_norm": 11461.633914935515,
      "learning_rate": 2.3379231554448076e-07,
      "loss": 1.7154,
      "step": 183136
    },
    {
      "epoch": 0.0006647530241893046,
      "grad_norm": 9303.409697524881,
      "learning_rate": 2.3377187211685944e-07,
      "loss": 1.7358,
      "step": 183168
    },
    {
      "epoch": 0.0006648691585401413,
      "grad_norm": 9339.989079222738,
      "learning_rate": 2.3375143405118498e-07,
      "loss": 1.7462,
      "step": 183200
    },
    {
      "epoch": 0.000664985292890978,
      "grad_norm": 10527.352563679056,
      "learning_rate": 2.3373100134511388e-07,
      "loss": 1.7282,
      "step": 183232
    },
    {
      "epoch": 0.0006651014272418147,
      "grad_norm": 9308.54897392714,
      "learning_rate": 2.3371057399630404e-07,
      "loss": 1.7284,
      "step": 183264
    },
    {
      "epoch": 0.0006652175615926514,
      "grad_norm": 10234.352153409614,
      "learning_rate": 2.3369015200241486e-07,
      "loss": 1.7508,
      "step": 183296
    },
    {
      "epoch": 0.0006653336959434881,
      "grad_norm": 9694.528972570044,
      "learning_rate": 2.336697353611071e-07,
      "loss": 1.7515,
      "step": 183328
    },
    {
      "epoch": 0.0006654498302943248,
      "grad_norm": 9730.005960943703,
      "learning_rate": 2.3364932407004307e-07,
      "loss": 1.7484,
      "step": 183360
    },
    {
      "epoch": 0.0006655659646451615,
      "grad_norm": 10144.22554954295,
      "learning_rate": 2.3362891812688638e-07,
      "loss": 1.7276,
      "step": 183392
    },
    {
      "epoch": 0.0006656820989959983,
      "grad_norm": 9013.668620489661,
      "learning_rate": 2.336085175293021e-07,
      "loss": 1.7043,
      "step": 183424
    },
    {
      "epoch": 0.0006657982333468349,
      "grad_norm": 8686.308306754947,
      "learning_rate": 2.335881222749568e-07,
      "loss": 1.7151,
      "step": 183456
    },
    {
      "epoch": 0.0006659143676976717,
      "grad_norm": 10920.292303780152,
      "learning_rate": 2.335683694654927e-07,
      "loss": 1.7269,
      "step": 183488
    },
    {
      "epoch": 0.0006660305020485083,
      "grad_norm": 9354.963922966244,
      "learning_rate": 2.3354798472383533e-07,
      "loss": 1.7576,
      "step": 183520
    },
    {
      "epoch": 0.0006661466363993451,
      "grad_norm": 9940.023943633134,
      "learning_rate": 2.3352760531849778e-07,
      "loss": 1.7434,
      "step": 183552
    },
    {
      "epoch": 0.0006662627707501817,
      "grad_norm": 9655.489008848801,
      "learning_rate": 2.3350723124715222e-07,
      "loss": 1.7211,
      "step": 183584
    },
    {
      "epoch": 0.0006663789051010185,
      "grad_norm": 9007.51075492003,
      "learning_rate": 2.3348686250747222e-07,
      "loss": 1.7372,
      "step": 183616
    },
    {
      "epoch": 0.0006664950394518551,
      "grad_norm": 7825.079168928581,
      "learning_rate": 2.3346649909713283e-07,
      "loss": 1.7551,
      "step": 183648
    },
    {
      "epoch": 0.0006666111738026919,
      "grad_norm": 9443.488126746386,
      "learning_rate": 2.3344614101381047e-07,
      "loss": 1.7277,
      "step": 183680
    },
    {
      "epoch": 0.0006667273081535286,
      "grad_norm": 10169.8289071154,
      "learning_rate": 2.3342578825518296e-07,
      "loss": 1.7373,
      "step": 183712
    },
    {
      "epoch": 0.0006668434425043653,
      "grad_norm": 9664.472153201126,
      "learning_rate": 2.3340544081892958e-07,
      "loss": 1.7502,
      "step": 183744
    },
    {
      "epoch": 0.000666959576855202,
      "grad_norm": 9090.013421332225,
      "learning_rate": 2.3338509870273104e-07,
      "loss": 1.713,
      "step": 183776
    },
    {
      "epoch": 0.0006670757112060387,
      "grad_norm": 8411.235462166067,
      "learning_rate": 2.333647619042694e-07,
      "loss": 1.7188,
      "step": 183808
    },
    {
      "epoch": 0.0006671918455568754,
      "grad_norm": 8408.34169143952,
      "learning_rate": 2.333444304212282e-07,
      "loss": 1.7087,
      "step": 183840
    },
    {
      "epoch": 0.0006673079799077121,
      "grad_norm": 9973.276191904042,
      "learning_rate": 2.3332410425129232e-07,
      "loss": 1.7292,
      "step": 183872
    },
    {
      "epoch": 0.0006674241142585488,
      "grad_norm": 9352.822889374096,
      "learning_rate": 2.3330378339214808e-07,
      "loss": 1.7142,
      "step": 183904
    },
    {
      "epoch": 0.0006675402486093855,
      "grad_norm": 10587.422915894122,
      "learning_rate": 2.332834678414833e-07,
      "loss": 1.6935,
      "step": 183936
    },
    {
      "epoch": 0.0006676563829602222,
      "grad_norm": 9567.58611144943,
      "learning_rate": 2.332631575969871e-07,
      "loss": 1.7298,
      "step": 183968
    },
    {
      "epoch": 0.000667772517311059,
      "grad_norm": 9914.210003827839,
      "learning_rate": 2.3324285265635e-07,
      "loss": 1.7277,
      "step": 184000
    },
    {
      "epoch": 0.0006678886516618956,
      "grad_norm": 10208.705500698901,
      "learning_rate": 2.33222553017264e-07,
      "loss": 1.7409,
      "step": 184032
    },
    {
      "epoch": 0.0006680047860127324,
      "grad_norm": 9129.31454162907,
      "learning_rate": 2.332022586774225e-07,
      "loss": 1.7525,
      "step": 184064
    },
    {
      "epoch": 0.000668120920363569,
      "grad_norm": 8615.56684147944,
      "learning_rate": 2.331819696345202e-07,
      "loss": 1.7619,
      "step": 184096
    },
    {
      "epoch": 0.0006682370547144058,
      "grad_norm": 9531.327294768551,
      "learning_rate": 2.3316168588625335e-07,
      "loss": 1.7444,
      "step": 184128
    },
    {
      "epoch": 0.0006683531890652424,
      "grad_norm": 9849.00482282347,
      "learning_rate": 2.3314140743031955e-07,
      "loss": 1.7458,
      "step": 184160
    },
    {
      "epoch": 0.0006684693234160792,
      "grad_norm": 9396.298207272905,
      "learning_rate": 2.331211342644177e-07,
      "loss": 1.7408,
      "step": 184192
    },
    {
      "epoch": 0.0006685854577669158,
      "grad_norm": 11499.842607618592,
      "learning_rate": 2.3310086638624828e-07,
      "loss": 1.7495,
      "step": 184224
    },
    {
      "epoch": 0.0006687015921177526,
      "grad_norm": 9930.806613765066,
      "learning_rate": 2.33080603793513e-07,
      "loss": 1.7037,
      "step": 184256
    },
    {
      "epoch": 0.0006688177264685893,
      "grad_norm": 8644.70635707194,
      "learning_rate": 2.3306034648391507e-07,
      "loss": 1.7057,
      "step": 184288
    },
    {
      "epoch": 0.000668933860819426,
      "grad_norm": 9700.863466722949,
      "learning_rate": 2.3304009445515908e-07,
      "loss": 1.7386,
      "step": 184320
    },
    {
      "epoch": 0.0006690499951702627,
      "grad_norm": 10090.279084346479,
      "learning_rate": 2.33019847704951e-07,
      "loss": 1.7383,
      "step": 184352
    },
    {
      "epoch": 0.0006691661295210994,
      "grad_norm": 9611.543268383075,
      "learning_rate": 2.3299960623099823e-07,
      "loss": 1.7392,
      "step": 184384
    },
    {
      "epoch": 0.0006692822638719361,
      "grad_norm": 8749.473698457527,
      "learning_rate": 2.3297937003100948e-07,
      "loss": 1.7299,
      "step": 184416
    },
    {
      "epoch": 0.0006693983982227728,
      "grad_norm": 9244.462234224336,
      "learning_rate": 2.3295913910269493e-07,
      "loss": 1.7193,
      "step": 184448
    },
    {
      "epoch": 0.0006695145325736095,
      "grad_norm": 8851.06456873974,
      "learning_rate": 2.3293954541586918e-07,
      "loss": 1.728,
      "step": 184480
    },
    {
      "epoch": 0.0006696306669244462,
      "grad_norm": 8806.713916098332,
      "learning_rate": 2.3291932485947687e-07,
      "loss": 1.7367,
      "step": 184512
    },
    {
      "epoch": 0.0006697468012752829,
      "grad_norm": 8841.825603346857,
      "learning_rate": 2.32899109567969e-07,
      "loss": 1.7331,
      "step": 184544
    },
    {
      "epoch": 0.0006698629356261197,
      "grad_norm": 8550.744996782443,
      "learning_rate": 2.328788995390612e-07,
      "loss": 1.7284,
      "step": 184576
    },
    {
      "epoch": 0.0006699790699769563,
      "grad_norm": 10237.150287067197,
      "learning_rate": 2.328586947704706e-07,
      "loss": 1.7144,
      "step": 184608
    },
    {
      "epoch": 0.0006700952043277931,
      "grad_norm": 10122.586230800902,
      "learning_rate": 2.328384952599156e-07,
      "loss": 1.7421,
      "step": 184640
    },
    {
      "epoch": 0.0006702113386786297,
      "grad_norm": 10188.075186216483,
      "learning_rate": 2.3281830100511605e-07,
      "loss": 1.7509,
      "step": 184672
    },
    {
      "epoch": 0.0006703274730294665,
      "grad_norm": 9489.595354913718,
      "learning_rate": 2.3279811200379317e-07,
      "loss": 1.7439,
      "step": 184704
    },
    {
      "epoch": 0.0006704436073803031,
      "grad_norm": 10651.03440985898,
      "learning_rate": 2.3277792825366958e-07,
      "loss": 1.7439,
      "step": 184736
    },
    {
      "epoch": 0.0006705597417311399,
      "grad_norm": 7194.052404590892,
      "learning_rate": 2.3275774975246922e-07,
      "loss": 1.732,
      "step": 184768
    },
    {
      "epoch": 0.0006706758760819765,
      "grad_norm": 8380.658923974892,
      "learning_rate": 2.327375764979175e-07,
      "loss": 1.6948,
      "step": 184800
    },
    {
      "epoch": 0.0006707920104328133,
      "grad_norm": 10900.885835564008,
      "learning_rate": 2.327174084877411e-07,
      "loss": 1.7129,
      "step": 184832
    },
    {
      "epoch": 0.00067090814478365,
      "grad_norm": 11141.733078834728,
      "learning_rate": 2.326972457196682e-07,
      "loss": 1.715,
      "step": 184864
    },
    {
      "epoch": 0.0006710242791344867,
      "grad_norm": 7895.111905476704,
      "learning_rate": 2.3267708819142827e-07,
      "loss": 1.7389,
      "step": 184896
    },
    {
      "epoch": 0.0006711404134853234,
      "grad_norm": 8612.941077239528,
      "learning_rate": 2.3265693590075217e-07,
      "loss": 1.74,
      "step": 184928
    },
    {
      "epoch": 0.0006712565478361601,
      "grad_norm": 10222.056348895754,
      "learning_rate": 2.326367888453722e-07,
      "loss": 1.719,
      "step": 184960
    },
    {
      "epoch": 0.0006713726821869968,
      "grad_norm": 9029.184237792471,
      "learning_rate": 2.32616647023022e-07,
      "loss": 1.7397,
      "step": 184992
    },
    {
      "epoch": 0.0006714888165378335,
      "grad_norm": 8685.708376407763,
      "learning_rate": 2.3259651043143647e-07,
      "loss": 1.7488,
      "step": 185024
    },
    {
      "epoch": 0.0006716049508886702,
      "grad_norm": 10449.876267210057,
      "learning_rate": 2.3257637906835204e-07,
      "loss": 1.7458,
      "step": 185056
    },
    {
      "epoch": 0.0006717210852395069,
      "grad_norm": 9077.087858999714,
      "learning_rate": 2.3255625293150648e-07,
      "loss": 1.7504,
      "step": 185088
    },
    {
      "epoch": 0.0006718372195903436,
      "grad_norm": 9467.668984496659,
      "learning_rate": 2.325361320186389e-07,
      "loss": 1.7313,
      "step": 185120
    },
    {
      "epoch": 0.0006719533539411804,
      "grad_norm": 8373.55253163196,
      "learning_rate": 2.3251601632748975e-07,
      "loss": 1.6988,
      "step": 185152
    },
    {
      "epoch": 0.000672069488292017,
      "grad_norm": 9115.662455356714,
      "learning_rate": 2.3249590585580084e-07,
      "loss": 1.7166,
      "step": 185184
    },
    {
      "epoch": 0.0006721856226428538,
      "grad_norm": 8959.837944963067,
      "learning_rate": 2.3247580060131548e-07,
      "loss": 1.741,
      "step": 185216
    },
    {
      "epoch": 0.0006723017569936904,
      "grad_norm": 8541.66107967297,
      "learning_rate": 2.324557005617782e-07,
      "loss": 1.7453,
      "step": 185248
    },
    {
      "epoch": 0.0006724178913445272,
      "grad_norm": 10407.25112601786,
      "learning_rate": 2.3243560573493491e-07,
      "loss": 1.7275,
      "step": 185280
    },
    {
      "epoch": 0.0006725340256953638,
      "grad_norm": 8339.887049594856,
      "learning_rate": 2.3241551611853302e-07,
      "loss": 1.7358,
      "step": 185312
    },
    {
      "epoch": 0.0006726501600462006,
      "grad_norm": 10103.660326832054,
      "learning_rate": 2.3239543171032115e-07,
      "loss": 1.7555,
      "step": 185344
    },
    {
      "epoch": 0.0006727662943970372,
      "grad_norm": 9513.229945712445,
      "learning_rate": 2.323753525080493e-07,
      "loss": 1.7503,
      "step": 185376
    },
    {
      "epoch": 0.000672882428747874,
      "grad_norm": 9725.112338682777,
      "learning_rate": 2.323552785094689e-07,
      "loss": 1.7444,
      "step": 185408
    },
    {
      "epoch": 0.0006729985630987107,
      "grad_norm": 9281.442237066392,
      "learning_rate": 2.3233520971233275e-07,
      "loss": 1.7339,
      "step": 185440
    },
    {
      "epoch": 0.0006731146974495474,
      "grad_norm": 8434.781087852843,
      "learning_rate": 2.3231514611439487e-07,
      "loss": 1.7186,
      "step": 185472
    },
    {
      "epoch": 0.0006732308318003841,
      "grad_norm": 9049.307708327748,
      "learning_rate": 2.3229571445979902e-07,
      "loss": 1.7341,
      "step": 185504
    },
    {
      "epoch": 0.0006733469661512208,
      "grad_norm": 9109.31391488953,
      "learning_rate": 2.322756610912248e-07,
      "loss": 1.7128,
      "step": 185536
    },
    {
      "epoch": 0.0006734631005020575,
      "grad_norm": 10554.415947839085,
      "learning_rate": 2.3225561291518943e-07,
      "loss": 1.7255,
      "step": 185568
    },
    {
      "epoch": 0.0006735792348528942,
      "grad_norm": 9719.221162212536,
      "learning_rate": 2.3223556992945242e-07,
      "loss": 1.7095,
      "step": 185600
    },
    {
      "epoch": 0.0006736953692037309,
      "grad_norm": 9176.941538442969,
      "learning_rate": 2.3221553213177455e-07,
      "loss": 1.7043,
      "step": 185632
    },
    {
      "epoch": 0.0006738115035545676,
      "grad_norm": 11869.042926874938,
      "learning_rate": 2.3219549951991804e-07,
      "loss": 1.7233,
      "step": 185664
    },
    {
      "epoch": 0.0006739276379054043,
      "grad_norm": 10182.966365455599,
      "learning_rate": 2.321754720916465e-07,
      "loss": 1.7194,
      "step": 185696
    },
    {
      "epoch": 0.0006740437722562411,
      "grad_norm": 8828.523998947956,
      "learning_rate": 2.321554498447248e-07,
      "loss": 1.7282,
      "step": 185728
    },
    {
      "epoch": 0.0006741599066070777,
      "grad_norm": 9184.076436964144,
      "learning_rate": 2.3213543277691917e-07,
      "loss": 1.7329,
      "step": 185760
    },
    {
      "epoch": 0.0006742760409579145,
      "grad_norm": 10524.675766977338,
      "learning_rate": 2.3211542088599723e-07,
      "loss": 1.755,
      "step": 185792
    },
    {
      "epoch": 0.0006743921753087511,
      "grad_norm": 9395.360663646712,
      "learning_rate": 2.3209541416972791e-07,
      "loss": 1.7188,
      "step": 185824
    },
    {
      "epoch": 0.0006745083096595879,
      "grad_norm": 9387.464300864212,
      "learning_rate": 2.320754126258815e-07,
      "loss": 1.7363,
      "step": 185856
    },
    {
      "epoch": 0.0006746244440104245,
      "grad_norm": 7949.671125273045,
      "learning_rate": 2.3205541625222965e-07,
      "loss": 1.7593,
      "step": 185888
    },
    {
      "epoch": 0.0006747405783612613,
      "grad_norm": 8956.141356633447,
      "learning_rate": 2.3203542504654534e-07,
      "loss": 1.7695,
      "step": 185920
    },
    {
      "epoch": 0.0006748567127120979,
      "grad_norm": 10503.657839057782,
      "learning_rate": 2.320154390066029e-07,
      "loss": 1.7192,
      "step": 185952
    },
    {
      "epoch": 0.0006749728470629347,
      "grad_norm": 9174.736944457864,
      "learning_rate": 2.3199545813017793e-07,
      "loss": 1.7224,
      "step": 185984
    },
    {
      "epoch": 0.0006750889814137714,
      "grad_norm": 9003.946912326837,
      "learning_rate": 2.3197548241504753e-07,
      "loss": 1.7315,
      "step": 186016
    },
    {
      "epoch": 0.0006752051157646081,
      "grad_norm": 9479.749469263415,
      "learning_rate": 2.3195551185899e-07,
      "loss": 1.7278,
      "step": 186048
    },
    {
      "epoch": 0.0006753212501154448,
      "grad_norm": 10660.186302311982,
      "learning_rate": 2.31935546459785e-07,
      "loss": 1.7431,
      "step": 186080
    },
    {
      "epoch": 0.0006754373844662815,
      "grad_norm": 9936.750072332503,
      "learning_rate": 2.3191558621521358e-07,
      "loss": 1.733,
      "step": 186112
    },
    {
      "epoch": 0.0006755535188171182,
      "grad_norm": 10150.68549409349,
      "learning_rate": 2.3189563112305812e-07,
      "loss": 1.7256,
      "step": 186144
    },
    {
      "epoch": 0.0006756696531679549,
      "grad_norm": 10979.764114041795,
      "learning_rate": 2.3187568118110225e-07,
      "loss": 1.7068,
      "step": 186176
    },
    {
      "epoch": 0.0006757857875187916,
      "grad_norm": 9323.447002048117,
      "learning_rate": 2.3185573638713104e-07,
      "loss": 1.7229,
      "step": 186208
    },
    {
      "epoch": 0.0006759019218696283,
      "grad_norm": 9775.702532299149,
      "learning_rate": 2.3183579673893086e-07,
      "loss": 1.7459,
      "step": 186240
    },
    {
      "epoch": 0.000676018056220465,
      "grad_norm": 8691.996663598073,
      "learning_rate": 2.3181586223428936e-07,
      "loss": 1.746,
      "step": 186272
    },
    {
      "epoch": 0.0006761341905713018,
      "grad_norm": 8961.726396180593,
      "learning_rate": 2.317959328709956e-07,
      "loss": 1.7047,
      "step": 186304
    },
    {
      "epoch": 0.0006762503249221384,
      "grad_norm": 9903.754035717971,
      "learning_rate": 2.317760086468399e-07,
      "loss": 1.7254,
      "step": 186336
    },
    {
      "epoch": 0.0006763664592729752,
      "grad_norm": 9672.138129700175,
      "learning_rate": 2.31756089559614e-07,
      "loss": 1.7558,
      "step": 186368
    },
    {
      "epoch": 0.0006764825936238118,
      "grad_norm": 9522.03360632591,
      "learning_rate": 2.3173617560711087e-07,
      "loss": 1.7343,
      "step": 186400
    },
    {
      "epoch": 0.0006765987279746486,
      "grad_norm": 8998.094353806255,
      "learning_rate": 2.3171626678712484e-07,
      "loss": 1.7381,
      "step": 186432
    },
    {
      "epoch": 0.0006767148623254852,
      "grad_norm": 9450.058412517883,
      "learning_rate": 2.3169636309745162e-07,
      "loss": 1.7467,
      "step": 186464
    },
    {
      "epoch": 0.000676830996676322,
      "grad_norm": 9710.994387806018,
      "learning_rate": 2.3167708628833611e-07,
      "loss": 1.7204,
      "step": 186496
    },
    {
      "epoch": 0.0006769471310271586,
      "grad_norm": 9069.90319683733,
      "learning_rate": 2.3165719269252946e-07,
      "loss": 1.7144,
      "step": 186528
    },
    {
      "epoch": 0.0006770632653779954,
      "grad_norm": 8495.001118304812,
      "learning_rate": 2.3163730422049924e-07,
      "loss": 1.711,
      "step": 186560
    },
    {
      "epoch": 0.0006771793997288321,
      "grad_norm": 9464.847595180812,
      "learning_rate": 2.3161742087004643e-07,
      "loss": 1.7274,
      "step": 186592
    },
    {
      "epoch": 0.0006772955340796688,
      "grad_norm": 9107.469132530727,
      "learning_rate": 2.3159754263897324e-07,
      "loss": 1.7152,
      "step": 186624
    },
    {
      "epoch": 0.0006774116684305055,
      "grad_norm": 8943.683133921953,
      "learning_rate": 2.315776695250832e-07,
      "loss": 1.7304,
      "step": 186656
    },
    {
      "epoch": 0.0006775278027813422,
      "grad_norm": 10039.315713732685,
      "learning_rate": 2.3155780152618127e-07,
      "loss": 1.7367,
      "step": 186688
    },
    {
      "epoch": 0.000677643937132179,
      "grad_norm": 9833.896481049615,
      "learning_rate": 2.3153793864007358e-07,
      "loss": 1.7355,
      "step": 186720
    },
    {
      "epoch": 0.0006777600714830156,
      "grad_norm": 10170.688078984627,
      "learning_rate": 2.315180808645677e-07,
      "loss": 1.7404,
      "step": 186752
    },
    {
      "epoch": 0.0006778762058338523,
      "grad_norm": 8553.36191213724,
      "learning_rate": 2.314982281974724e-07,
      "loss": 1.7477,
      "step": 186784
    },
    {
      "epoch": 0.000677992340184689,
      "grad_norm": 10448.441414871406,
      "learning_rate": 2.3147838063659786e-07,
      "loss": 1.7321,
      "step": 186816
    },
    {
      "epoch": 0.0006781084745355257,
      "grad_norm": 10005.293998678899,
      "learning_rate": 2.3145853817975553e-07,
      "loss": 1.7277,
      "step": 186848
    },
    {
      "epoch": 0.0006782246088863625,
      "grad_norm": 9418.50221638239,
      "learning_rate": 2.314387008247582e-07,
      "loss": 1.7234,
      "step": 186880
    },
    {
      "epoch": 0.0006783407432371991,
      "grad_norm": 8298.541920120666,
      "learning_rate": 2.3141886856941995e-07,
      "loss": 1.7263,
      "step": 186912
    },
    {
      "epoch": 0.0006784568775880359,
      "grad_norm": 10740.476525741304,
      "learning_rate": 2.3139904141155614e-07,
      "loss": 1.7371,
      "step": 186944
    },
    {
      "epoch": 0.0006785730119388725,
      "grad_norm": 8881.93357327108,
      "learning_rate": 2.3137921934898353e-07,
      "loss": 1.7167,
      "step": 186976
    },
    {
      "epoch": 0.0006786891462897093,
      "grad_norm": 8396.697207831185,
      "learning_rate": 2.313594023795201e-07,
      "loss": 1.7289,
      "step": 187008
    },
    {
      "epoch": 0.000678805280640546,
      "grad_norm": 9641.479761945258,
      "learning_rate": 2.3133959050098515e-07,
      "loss": 1.7562,
      "step": 187040
    },
    {
      "epoch": 0.0006789214149913827,
      "grad_norm": 8292.540623958377,
      "learning_rate": 2.3131978371119937e-07,
      "loss": 1.7533,
      "step": 187072
    },
    {
      "epoch": 0.0006790375493422193,
      "grad_norm": 10621.75823486865,
      "learning_rate": 2.3129998200798466e-07,
      "loss": 1.7541,
      "step": 187104
    },
    {
      "epoch": 0.0006791536836930561,
      "grad_norm": 9428.915738302045,
      "learning_rate": 2.3128018538916426e-07,
      "loss": 1.7466,
      "step": 187136
    },
    {
      "epoch": 0.0006792698180438929,
      "grad_norm": 9845.399737948683,
      "learning_rate": 2.312603938525627e-07,
      "loss": 1.7193,
      "step": 187168
    },
    {
      "epoch": 0.0006793859523947295,
      "grad_norm": 8289.57562243086,
      "learning_rate": 2.3124060739600585e-07,
      "loss": 1.7145,
      "step": 187200
    },
    {
      "epoch": 0.0006795020867455663,
      "grad_norm": 9177.734143022448,
      "learning_rate": 2.3122082601732088e-07,
      "loss": 1.7343,
      "step": 187232
    },
    {
      "epoch": 0.0006796182210964029,
      "grad_norm": 9968.131921277927,
      "learning_rate": 2.3120104971433618e-07,
      "loss": 1.7321,
      "step": 187264
    },
    {
      "epoch": 0.0006797343554472397,
      "grad_norm": 8494.57332654207,
      "learning_rate": 2.3118127848488154e-07,
      "loss": 1.7116,
      "step": 187296
    },
    {
      "epoch": 0.0006798504897980763,
      "grad_norm": 9866.301029261169,
      "learning_rate": 2.3116151232678798e-07,
      "loss": 1.6979,
      "step": 187328
    },
    {
      "epoch": 0.0006799666241489131,
      "grad_norm": 10346.157547611578,
      "learning_rate": 2.311417512378879e-07,
      "loss": 1.708,
      "step": 187360
    },
    {
      "epoch": 0.0006800827584997497,
      "grad_norm": 8817.693235761833,
      "learning_rate": 2.311219952160149e-07,
      "loss": 1.7388,
      "step": 187392
    },
    {
      "epoch": 0.0006801988928505865,
      "grad_norm": 10324.741933820911,
      "learning_rate": 2.3110224425900392e-07,
      "loss": 1.7261,
      "step": 187424
    },
    {
      "epoch": 0.0006803150272014232,
      "grad_norm": 9663.445762252717,
      "learning_rate": 2.3108249836469118e-07,
      "loss": 1.7294,
      "step": 187456
    },
    {
      "epoch": 0.0006804311615522599,
      "grad_norm": 8652.531074778062,
      "learning_rate": 2.3106275753091427e-07,
      "loss": 1.7355,
      "step": 187488
    },
    {
      "epoch": 0.0006805472959030966,
      "grad_norm": 8772.195506257258,
      "learning_rate": 2.3104363842194755e-07,
      "loss": 1.7215,
      "step": 187520
    },
    {
      "epoch": 0.0006806634302539333,
      "grad_norm": 8484.515307311314,
      "learning_rate": 2.3102390754478594e-07,
      "loss": 1.7129,
      "step": 187552
    },
    {
      "epoch": 0.00068077956460477,
      "grad_norm": 9619.839291796927,
      "learning_rate": 2.310041817217479e-07,
      "loss": 1.7375,
      "step": 187584
    },
    {
      "epoch": 0.0006808956989556067,
      "grad_norm": 10168.634126567835,
      "learning_rate": 2.3098446095067606e-07,
      "loss": 1.7522,
      "step": 187616
    },
    {
      "epoch": 0.0006810118333064434,
      "grad_norm": 9777.713025038114,
      "learning_rate": 2.3096474522941438e-07,
      "loss": 1.7457,
      "step": 187648
    },
    {
      "epoch": 0.0006811279676572801,
      "grad_norm": 9728.888528501084,
      "learning_rate": 2.3094503455580812e-07,
      "loss": 1.741,
      "step": 187680
    },
    {
      "epoch": 0.0006812441020081168,
      "grad_norm": 9313.830683451359,
      "learning_rate": 2.3092532892770377e-07,
      "loss": 1.7437,
      "step": 187712
    },
    {
      "epoch": 0.0006813602363589536,
      "grad_norm": 8397.08080227885,
      "learning_rate": 2.3090562834294907e-07,
      "loss": 1.7438,
      "step": 187744
    },
    {
      "epoch": 0.0006814763707097902,
      "grad_norm": 7997.634025135184,
      "learning_rate": 2.3088593279939323e-07,
      "loss": 1.7392,
      "step": 187776
    },
    {
      "epoch": 0.000681592505060627,
      "grad_norm": 10419.479257621277,
      "learning_rate": 2.3086624229488656e-07,
      "loss": 1.7309,
      "step": 187808
    },
    {
      "epoch": 0.0006817086394114636,
      "grad_norm": 8544.343274939274,
      "learning_rate": 2.3084655682728073e-07,
      "loss": 1.7335,
      "step": 187840
    },
    {
      "epoch": 0.0006818247737623004,
      "grad_norm": 8722.491387212716,
      "learning_rate": 2.308268763944286e-07,
      "loss": 1.7149,
      "step": 187872
    },
    {
      "epoch": 0.000681940908113137,
      "grad_norm": 9011.717261432474,
      "learning_rate": 2.308072009941845e-07,
      "loss": 1.7189,
      "step": 187904
    },
    {
      "epoch": 0.0006820570424639738,
      "grad_norm": 10659.083075011658,
      "learning_rate": 2.3078753062440392e-07,
      "loss": 1.7294,
      "step": 187936
    },
    {
      "epoch": 0.0006821731768148104,
      "grad_norm": 9475.037308633671,
      "learning_rate": 2.3076786528294355e-07,
      "loss": 1.737,
      "step": 187968
    },
    {
      "epoch": 0.0006822893111656472,
      "grad_norm": 9499.025002598952,
      "learning_rate": 2.3074820496766152e-07,
      "loss": 1.7192,
      "step": 188000
    },
    {
      "epoch": 0.0006824054455164839,
      "grad_norm": 9040.899180944338,
      "learning_rate": 2.3072854967641714e-07,
      "loss": 1.7361,
      "step": 188032
    },
    {
      "epoch": 0.0006825215798673206,
      "grad_norm": 10528.983236761278,
      "learning_rate": 2.30708899407071e-07,
      "loss": 1.7401,
      "step": 188064
    },
    {
      "epoch": 0.0006826377142181573,
      "grad_norm": 8914.38993986689,
      "learning_rate": 2.3068925415748505e-07,
      "loss": 1.7474,
      "step": 188096
    },
    {
      "epoch": 0.000682753848568994,
      "grad_norm": 8878.430604560695,
      "learning_rate": 2.3066961392552237e-07,
      "loss": 1.7233,
      "step": 188128
    },
    {
      "epoch": 0.0006828699829198307,
      "grad_norm": 9203.642974387914,
      "learning_rate": 2.3064997870904743e-07,
      "loss": 1.7346,
      "step": 188160
    },
    {
      "epoch": 0.0006829861172706674,
      "grad_norm": 9166.914857246138,
      "learning_rate": 2.3063034850592594e-07,
      "loss": 1.7131,
      "step": 188192
    },
    {
      "epoch": 0.0006831022516215041,
      "grad_norm": 10587.45805186495,
      "learning_rate": 2.3061072331402487e-07,
      "loss": 1.7243,
      "step": 188224
    },
    {
      "epoch": 0.0006832183859723408,
      "grad_norm": 11077.138439145734,
      "learning_rate": 2.3059110313121248e-07,
      "loss": 1.7399,
      "step": 188256
    },
    {
      "epoch": 0.0006833345203231775,
      "grad_norm": 10185.448443735799,
      "learning_rate": 2.3057148795535822e-07,
      "loss": 1.7305,
      "step": 188288
    },
    {
      "epoch": 0.0006834506546740143,
      "grad_norm": 10631.110760405048,
      "learning_rate": 2.3055187778433294e-07,
      "loss": 1.7161,
      "step": 188320
    },
    {
      "epoch": 0.0006835667890248509,
      "grad_norm": 8950.959948519489,
      "learning_rate": 2.3053227261600873e-07,
      "loss": 1.7064,
      "step": 188352
    },
    {
      "epoch": 0.0006836829233756877,
      "grad_norm": 9808.60458984865,
      "learning_rate": 2.305126724482588e-07,
      "loss": 1.7394,
      "step": 188384
    },
    {
      "epoch": 0.0006837990577265243,
      "grad_norm": 9387.539826812987,
      "learning_rate": 2.3049307727895777e-07,
      "loss": 1.7298,
      "step": 188416
    },
    {
      "epoch": 0.0006839151920773611,
      "grad_norm": 9375.668936134638,
      "learning_rate": 2.3047348710598155e-07,
      "loss": 1.7461,
      "step": 188448
    },
    {
      "epoch": 0.0006840313264281977,
      "grad_norm": 9837.01255463263,
      "learning_rate": 2.3045390192720718e-07,
      "loss": 1.7352,
      "step": 188480
    },
    {
      "epoch": 0.0006841474607790345,
      "grad_norm": 10089.543498097424,
      "learning_rate": 2.3043493354580459e-07,
      "loss": 1.7421,
      "step": 188512
    },
    {
      "epoch": 0.0006842635951298711,
      "grad_norm": 9880.074797287722,
      "learning_rate": 2.3041535819316617e-07,
      "loss": 1.7164,
      "step": 188544
    },
    {
      "epoch": 0.0006843797294807079,
      "grad_norm": 9158.403900243753,
      "learning_rate": 2.3039578782843477e-07,
      "loss": 1.7297,
      "step": 188576
    },
    {
      "epoch": 0.0006844958638315445,
      "grad_norm": 8125.352423126026,
      "learning_rate": 2.3037622244949247e-07,
      "loss": 1.7482,
      "step": 188608
    },
    {
      "epoch": 0.0006846119981823813,
      "grad_norm": 10175.649168480604,
      "learning_rate": 2.3035666205422262e-07,
      "loss": 1.7477,
      "step": 188640
    },
    {
      "epoch": 0.000684728132533218,
      "grad_norm": 10014.927458549064,
      "learning_rate": 2.3033710664050987e-07,
      "loss": 1.7092,
      "step": 188672
    },
    {
      "epoch": 0.0006848442668840547,
      "grad_norm": 9920.53849344883,
      "learning_rate": 2.3031755620624012e-07,
      "loss": 1.7105,
      "step": 188704
    },
    {
      "epoch": 0.0006849604012348914,
      "grad_norm": 9713.223975591214,
      "learning_rate": 2.3029801074930054e-07,
      "loss": 1.7454,
      "step": 188736
    },
    {
      "epoch": 0.0006850765355857281,
      "grad_norm": 9076.787978134114,
      "learning_rate": 2.3027847026757948e-07,
      "loss": 1.7457,
      "step": 188768
    },
    {
      "epoch": 0.0006851926699365648,
      "grad_norm": 10547.58076527504,
      "learning_rate": 2.3025893475896664e-07,
      "loss": 1.7501,
      "step": 188800
    },
    {
      "epoch": 0.0006853088042874015,
      "grad_norm": 9639.584845832314,
      "learning_rate": 2.3023940422135288e-07,
      "loss": 1.7567,
      "step": 188832
    },
    {
      "epoch": 0.0006854249386382382,
      "grad_norm": 9092.092608415292,
      "learning_rate": 2.3021987865263038e-07,
      "loss": 1.748,
      "step": 188864
    },
    {
      "epoch": 0.0006855410729890749,
      "grad_norm": 10198.160226236887,
      "learning_rate": 2.3020035805069258e-07,
      "loss": 1.7249,
      "step": 188896
    },
    {
      "epoch": 0.0006856572073399116,
      "grad_norm": 10418.22643255559,
      "learning_rate": 2.301808424134341e-07,
      "loss": 1.7264,
      "step": 188928
    },
    {
      "epoch": 0.0006857733416907484,
      "grad_norm": 9064.333511075152,
      "learning_rate": 2.3016133173875088e-07,
      "loss": 1.7409,
      "step": 188960
    },
    {
      "epoch": 0.000685889476041585,
      "grad_norm": 8837.38479415715,
      "learning_rate": 2.3014182602454005e-07,
      "loss": 1.7136,
      "step": 188992
    },
    {
      "epoch": 0.0006860056103924218,
      "grad_norm": 9797.043227423263,
      "learning_rate": 2.3012232526870002e-07,
      "loss": 1.7048,
      "step": 189024
    },
    {
      "epoch": 0.0006861217447432584,
      "grad_norm": 11116.37926664973,
      "learning_rate": 2.3010282946913046e-07,
      "loss": 1.7126,
      "step": 189056
    },
    {
      "epoch": 0.0006862378790940952,
      "grad_norm": 8834.251750997364,
      "learning_rate": 2.300833386237323e-07,
      "loss": 1.7205,
      "step": 189088
    },
    {
      "epoch": 0.0006863540134449318,
      "grad_norm": 10768.120355939564,
      "learning_rate": 2.3006385273040758e-07,
      "loss": 1.7233,
      "step": 189120
    },
    {
      "epoch": 0.0006864701477957686,
      "grad_norm": 11226.745209543147,
      "learning_rate": 2.3004437178705977e-07,
      "loss": 1.7245,
      "step": 189152
    },
    {
      "epoch": 0.0006865862821466052,
      "grad_norm": 9279.994612067401,
      "learning_rate": 2.300248957915935e-07,
      "loss": 1.7319,
      "step": 189184
    },
    {
      "epoch": 0.000686702416497442,
      "grad_norm": 9181.61336585243,
      "learning_rate": 2.3000542474191457e-07,
      "loss": 1.7082,
      "step": 189216
    },
    {
      "epoch": 0.0006868185508482787,
      "grad_norm": 9136.690867048092,
      "learning_rate": 2.2998595863593014e-07,
      "loss": 1.7219,
      "step": 189248
    },
    {
      "epoch": 0.0006869346851991154,
      "grad_norm": 9010.883641463804,
      "learning_rate": 2.2996649747154857e-07,
      "loss": 1.7152,
      "step": 189280
    },
    {
      "epoch": 0.0006870508195499521,
      "grad_norm": 9816.228399950767,
      "learning_rate": 2.2994704124667942e-07,
      "loss": 1.7405,
      "step": 189312
    },
    {
      "epoch": 0.0006871669539007888,
      "grad_norm": 9044.037704476912,
      "learning_rate": 2.299275899592335e-07,
      "loss": 1.7277,
      "step": 189344
    },
    {
      "epoch": 0.0006872830882516255,
      "grad_norm": 9930.028197341637,
      "learning_rate": 2.2990814360712295e-07,
      "loss": 1.7283,
      "step": 189376
    },
    {
      "epoch": 0.0006873992226024622,
      "grad_norm": 9663.366494136502,
      "learning_rate": 2.2988870218826097e-07,
      "loss": 1.762,
      "step": 189408
    },
    {
      "epoch": 0.0006875153569532989,
      "grad_norm": 9600.093020382667,
      "learning_rate": 2.298692657005622e-07,
      "loss": 1.768,
      "step": 189440
    },
    {
      "epoch": 0.0006876314913041356,
      "grad_norm": 8885.607463758457,
      "learning_rate": 2.298498341419423e-07,
      "loss": 1.7532,
      "step": 189472
    },
    {
      "epoch": 0.0006877476256549723,
      "grad_norm": 18955.654776345764,
      "learning_rate": 2.2983040751031833e-07,
      "loss": 1.7543,
      "step": 189504
    },
    {
      "epoch": 0.0006878637600058091,
      "grad_norm": 10136.311360647916,
      "learning_rate": 2.2981159265741684e-07,
      "loss": 1.7286,
      "step": 189536
    },
    {
      "epoch": 0.0006879798943566457,
      "grad_norm": 9938.777188366787,
      "learning_rate": 2.2979217571973355e-07,
      "loss": 1.7042,
      "step": 189568
    },
    {
      "epoch": 0.0006880960287074825,
      "grad_norm": 9647.03021660034,
      "learning_rate": 2.2977276370286954e-07,
      "loss": 1.7241,
      "step": 189600
    },
    {
      "epoch": 0.0006882121630583191,
      "grad_norm": 9509.50692728072,
      "learning_rate": 2.297533566047467e-07,
      "loss": 1.7398,
      "step": 189632
    },
    {
      "epoch": 0.0006883282974091559,
      "grad_norm": 10514.692006901583,
      "learning_rate": 2.2973395442328812e-07,
      "loss": 1.7314,
      "step": 189664
    },
    {
      "epoch": 0.0006884444317599925,
      "grad_norm": 8886.840946028009,
      "learning_rate": 2.2971455715641816e-07,
      "loss": 1.7055,
      "step": 189696
    },
    {
      "epoch": 0.0006885605661108293,
      "grad_norm": 8775.646756792345,
      "learning_rate": 2.2969516480206239e-07,
      "loss": 1.723,
      "step": 189728
    },
    {
      "epoch": 0.0006886767004616659,
      "grad_norm": 9223.266558004272,
      "learning_rate": 2.2967577735814756e-07,
      "loss": 1.7482,
      "step": 189760
    },
    {
      "epoch": 0.0006887928348125027,
      "grad_norm": 10128.053317395204,
      "learning_rate": 2.2965639482260177e-07,
      "loss": 1.7553,
      "step": 189792
    },
    {
      "epoch": 0.0006889089691633394,
      "grad_norm": 10515.765497575534,
      "learning_rate": 2.2963701719335419e-07,
      "loss": 1.7336,
      "step": 189824
    },
    {
      "epoch": 0.0006890251035141761,
      "grad_norm": 10864.053755389836,
      "learning_rate": 2.296176444683353e-07,
      "loss": 1.7242,
      "step": 189856
    },
    {
      "epoch": 0.0006891412378650128,
      "grad_norm": 10136.028216219605,
      "learning_rate": 2.295982766454768e-07,
      "loss": 1.7057,
      "step": 189888
    },
    {
      "epoch": 0.0006892573722158495,
      "grad_norm": 7634.404102482395,
      "learning_rate": 2.295789137227116e-07,
      "loss": 1.7039,
      "step": 189920
    },
    {
      "epoch": 0.0006893735065666862,
      "grad_norm": 10375.799728213724,
      "learning_rate": 2.295595556979738e-07,
      "loss": 1.7178,
      "step": 189952
    },
    {
      "epoch": 0.0006894896409175229,
      "grad_norm": 10345.334600678703,
      "learning_rate": 2.295402025691988e-07,
      "loss": 1.745,
      "step": 189984
    },
    {
      "epoch": 0.0006896057752683596,
      "grad_norm": 9273.925598148822,
      "learning_rate": 2.2952085433432312e-07,
      "loss": 1.7311,
      "step": 190016
    },
    {
      "epoch": 0.0006897219096191963,
      "grad_norm": 9735.86113294556,
      "learning_rate": 2.2950151099128457e-07,
      "loss": 1.7139,
      "step": 190048
    },
    {
      "epoch": 0.000689838043970033,
      "grad_norm": 10245.652346239354,
      "learning_rate": 2.2948217253802212e-07,
      "loss": 1.726,
      "step": 190080
    },
    {
      "epoch": 0.0006899541783208698,
      "grad_norm": 8778.786020857326,
      "learning_rate": 2.2946283897247603e-07,
      "loss": 1.7379,
      "step": 190112
    },
    {
      "epoch": 0.0006900703126717064,
      "grad_norm": 8752.126827234624,
      "learning_rate": 2.2944351029258767e-07,
      "loss": 1.7247,
      "step": 190144
    },
    {
      "epoch": 0.0006901864470225432,
      "grad_norm": 8871.350291810148,
      "learning_rate": 2.294241864962997e-07,
      "loss": 1.7333,
      "step": 190176
    },
    {
      "epoch": 0.0006903025813733798,
      "grad_norm": 10910.847446463542,
      "learning_rate": 2.2940486758155602e-07,
      "loss": 1.7406,
      "step": 190208
    },
    {
      "epoch": 0.0006904187157242166,
      "grad_norm": 9943.746678189264,
      "learning_rate": 2.2938555354630166e-07,
      "loss": 1.7002,
      "step": 190240
    },
    {
      "epoch": 0.0006905348500750532,
      "grad_norm": 8429.792049629694,
      "learning_rate": 2.293662443884829e-07,
      "loss": 1.7267,
      "step": 190272
    },
    {
      "epoch": 0.00069065098442589,
      "grad_norm": 9362.399158335431,
      "learning_rate": 2.2934694010604724e-07,
      "loss": 1.7344,
      "step": 190304
    },
    {
      "epoch": 0.0006907671187767266,
      "grad_norm": 9864.683877347516,
      "learning_rate": 2.2932764069694336e-07,
      "loss": 1.7494,
      "step": 190336
    },
    {
      "epoch": 0.0006908832531275634,
      "grad_norm": 9318.641853832563,
      "learning_rate": 2.293083461591212e-07,
      "loss": 1.7329,
      "step": 190368
    },
    {
      "epoch": 0.0006909993874784001,
      "grad_norm": 11222.435564528761,
      "learning_rate": 2.2928905649053182e-07,
      "loss": 1.7258,
      "step": 190400
    },
    {
      "epoch": 0.0006911155218292368,
      "grad_norm": 10235.260817390048,
      "learning_rate": 2.2926977168912756e-07,
      "loss": 1.7342,
      "step": 190432
    },
    {
      "epoch": 0.0006912316561800735,
      "grad_norm": 9299.848600918189,
      "learning_rate": 2.2925049175286194e-07,
      "loss": 1.734,
      "step": 190464
    },
    {
      "epoch": 0.0006913477905309102,
      "grad_norm": 8646.493277624171,
      "learning_rate": 2.2923121667968972e-07,
      "loss": 1.7411,
      "step": 190496
    },
    {
      "epoch": 0.0006914639248817469,
      "grad_norm": 9397.126901345964,
      "learning_rate": 2.292125485881356e-07,
      "loss": 1.7469,
      "step": 190528
    },
    {
      "epoch": 0.0006915800592325836,
      "grad_norm": 9398.087465011165,
      "learning_rate": 2.2919328308320606e-07,
      "loss": 1.7299,
      "step": 190560
    },
    {
      "epoch": 0.0006916961935834203,
      "grad_norm": 8864.181575306318,
      "learning_rate": 2.2917402243530515e-07,
      "loss": 1.7332,
      "step": 190592
    },
    {
      "epoch": 0.000691812327934257,
      "grad_norm": 9014.459051989754,
      "learning_rate": 2.2915476664239224e-07,
      "loss": 1.7532,
      "step": 190624
    },
    {
      "epoch": 0.0006919284622850937,
      "grad_norm": 9447.883466681837,
      "learning_rate": 2.291355157024281e-07,
      "loss": 1.7625,
      "step": 190656
    },
    {
      "epoch": 0.0006920445966359305,
      "grad_norm": 10275.50388058902,
      "learning_rate": 2.2911626961337464e-07,
      "loss": 1.7244,
      "step": 190688
    },
    {
      "epoch": 0.0006921607309867671,
      "grad_norm": 10791.553363626574,
      "learning_rate": 2.2909702837319496e-07,
      "loss": 1.6983,
      "step": 190720
    },
    {
      "epoch": 0.0006922768653376039,
      "grad_norm": 10568.441890837079,
      "learning_rate": 2.290777919798533e-07,
      "loss": 1.713,
      "step": 190752
    },
    {
      "epoch": 0.0006923929996884405,
      "grad_norm": 8476.053916770468,
      "learning_rate": 2.290585604313152e-07,
      "loss": 1.7248,
      "step": 190784
    },
    {
      "epoch": 0.0006925091340392773,
      "grad_norm": 8346.839761250962,
      "learning_rate": 2.290393337255473e-07,
      "loss": 1.7248,
      "step": 190816
    },
    {
      "epoch": 0.0006926252683901139,
      "grad_norm": 10370.68792318041,
      "learning_rate": 2.2902011186051753e-07,
      "loss": 1.7228,
      "step": 190848
    },
    {
      "epoch": 0.0006927414027409507,
      "grad_norm": 10041.30609034502,
      "learning_rate": 2.2900089483419493e-07,
      "loss": 1.7265,
      "step": 190880
    },
    {
      "epoch": 0.0006928575370917873,
      "grad_norm": 8296.00361619979,
      "learning_rate": 2.2898168264454976e-07,
      "loss": 1.7056,
      "step": 190912
    },
    {
      "epoch": 0.0006929736714426241,
      "grad_norm": 9276.225094293475,
      "learning_rate": 2.2896247528955357e-07,
      "loss": 1.7171,
      "step": 190944
    },
    {
      "epoch": 0.0006930898057934609,
      "grad_norm": 8711.614775688833,
      "learning_rate": 2.2894327276717886e-07,
      "loss": 1.7177,
      "step": 190976
    },
    {
      "epoch": 0.0006932059401442975,
      "grad_norm": 9403.58548639826,
      "learning_rate": 2.2892407507539956e-07,
      "loss": 1.7268,
      "step": 191008
    },
    {
      "epoch": 0.0006933220744951343,
      "grad_norm": 9493.174179377518,
      "learning_rate": 2.2890488221219072e-07,
      "loss": 1.7139,
      "step": 191040
    },
    {
      "epoch": 0.0006934382088459709,
      "grad_norm": 9559.987238485206,
      "learning_rate": 2.2888569417552847e-07,
      "loss": 1.7059,
      "step": 191072
    },
    {
      "epoch": 0.0006935543431968077,
      "grad_norm": 9642.745459670705,
      "learning_rate": 2.288665109633903e-07,
      "loss": 1.7434,
      "step": 191104
    },
    {
      "epoch": 0.0006936704775476443,
      "grad_norm": 9087.180750925998,
      "learning_rate": 2.2884733257375476e-07,
      "loss": 1.7465,
      "step": 191136
    },
    {
      "epoch": 0.000693786611898481,
      "grad_norm": 9319.719738275395,
      "learning_rate": 2.288281590046016e-07,
      "loss": 1.7621,
      "step": 191168
    },
    {
      "epoch": 0.0006939027462493177,
      "grad_norm": 9552.342853980903,
      "learning_rate": 2.2880899025391185e-07,
      "loss": 1.7674,
      "step": 191200
    },
    {
      "epoch": 0.0006940188806001544,
      "grad_norm": 9395.727539685258,
      "learning_rate": 2.287898263196676e-07,
      "loss": 1.7532,
      "step": 191232
    },
    {
      "epoch": 0.0006941350149509912,
      "grad_norm": 8412.705391251971,
      "learning_rate": 2.2877066719985214e-07,
      "loss": 1.7276,
      "step": 191264
    },
    {
      "epoch": 0.0006942511493018278,
      "grad_norm": 7946.89952119693,
      "learning_rate": 2.287515128924501e-07,
      "loss": 1.7149,
      "step": 191296
    },
    {
      "epoch": 0.0006943672836526646,
      "grad_norm": 9527.92737167953,
      "learning_rate": 2.2873236339544708e-07,
      "loss": 1.7254,
      "step": 191328
    },
    {
      "epoch": 0.0006944834180035012,
      "grad_norm": 10339.56807608519,
      "learning_rate": 2.2871321870682995e-07,
      "loss": 1.744,
      "step": 191360
    },
    {
      "epoch": 0.000694599552354338,
      "grad_norm": 9254.369346422262,
      "learning_rate": 2.2869407882458677e-07,
      "loss": 1.7204,
      "step": 191392
    },
    {
      "epoch": 0.0006947156867051746,
      "grad_norm": 8853.956290834058,
      "learning_rate": 2.2867494374670682e-07,
      "loss": 1.7113,
      "step": 191424
    },
    {
      "epoch": 0.0006948318210560114,
      "grad_norm": 9523.237159705728,
      "learning_rate": 2.2865581347118044e-07,
      "loss": 1.7284,
      "step": 191456
    },
    {
      "epoch": 0.000694947955406848,
      "grad_norm": 9114.243468330214,
      "learning_rate": 2.2863668799599922e-07,
      "loss": 1.7399,
      "step": 191488
    },
    {
      "epoch": 0.0006950640897576848,
      "grad_norm": 9911.374072246492,
      "learning_rate": 2.2861756731915596e-07,
      "loss": 1.7558,
      "step": 191520
    },
    {
      "epoch": 0.0006951802241085216,
      "grad_norm": 10188.063407733582,
      "learning_rate": 2.2859904873732972e-07,
      "loss": 1.7402,
      "step": 191552
    },
    {
      "epoch": 0.0006952963584593582,
      "grad_norm": 8614.676662533539,
      "learning_rate": 2.285799375013529e-07,
      "loss": 1.6955,
      "step": 191584
    },
    {
      "epoch": 0.000695412492810195,
      "grad_norm": 8630.314478627068,
      "learning_rate": 2.285608310577619e-07,
      "loss": 1.7031,
      "step": 191616
    },
    {
      "epoch": 0.0006955286271610316,
      "grad_norm": 9374.108810975047,
      "learning_rate": 2.285417294045542e-07,
      "loss": 1.7033,
      "step": 191648
    },
    {
      "epoch": 0.0006956447615118684,
      "grad_norm": 9708.772321977687,
      "learning_rate": 2.2852263253972834e-07,
      "loss": 1.7262,
      "step": 191680
    },
    {
      "epoch": 0.000695760895862705,
      "grad_norm": 8345.947399786317,
      "learning_rate": 2.285035404612841e-07,
      "loss": 1.7196,
      "step": 191712
    },
    {
      "epoch": 0.0006958770302135418,
      "grad_norm": 9980.846857857303,
      "learning_rate": 2.2848445316722235e-07,
      "loss": 1.7119,
      "step": 191744
    },
    {
      "epoch": 0.0006959931645643784,
      "grad_norm": 8884.967416935191,
      "learning_rate": 2.284653706555453e-07,
      "loss": 1.7452,
      "step": 191776
    },
    {
      "epoch": 0.0006961092989152152,
      "grad_norm": 9790.462910404185,
      "learning_rate": 2.2844629292425607e-07,
      "loss": 1.7541,
      "step": 191808
    },
    {
      "epoch": 0.0006962254332660519,
      "grad_norm": 9607.533710583586,
      "learning_rate": 2.284272199713592e-07,
      "loss": 1.7232,
      "step": 191840
    },
    {
      "epoch": 0.0006963415676168886,
      "grad_norm": 8787.903618042246,
      "learning_rate": 2.2840815179486023e-07,
      "loss": 1.7241,
      "step": 191872
    },
    {
      "epoch": 0.0006964577019677253,
      "grad_norm": 8340.681866610188,
      "learning_rate": 2.2838908839276594e-07,
      "loss": 1.7244,
      "step": 191904
    },
    {
      "epoch": 0.000696573836318562,
      "grad_norm": 9440.894449150463,
      "learning_rate": 2.2837002976308425e-07,
      "loss": 1.7066,
      "step": 191936
    },
    {
      "epoch": 0.0006966899706693987,
      "grad_norm": 8200.157559461892,
      "learning_rate": 2.2835097590382428e-07,
      "loss": 1.7216,
      "step": 191968
    },
    {
      "epoch": 0.0006968061050202354,
      "grad_norm": 8395.452102180085,
      "learning_rate": 2.2833192681299623e-07,
      "loss": 1.7256,
      "step": 192000
    },
    {
      "epoch": 0.0006969222393710721,
      "grad_norm": 9867.00603019984,
      "learning_rate": 2.2831288248861155e-07,
      "loss": 1.7477,
      "step": 192032
    },
    {
      "epoch": 0.0006970383737219088,
      "grad_norm": 9566.582252821538,
      "learning_rate": 2.2829384292868282e-07,
      "loss": 1.733,
      "step": 192064
    },
    {
      "epoch": 0.0006971545080727455,
      "grad_norm": 8051.318401355147,
      "learning_rate": 2.2827480813122374e-07,
      "loss": 1.734,
      "step": 192096
    },
    {
      "epoch": 0.0006972706424235823,
      "grad_norm": 8921.54728732634,
      "learning_rate": 2.2825577809424922e-07,
      "loss": 1.7563,
      "step": 192128
    },
    {
      "epoch": 0.0006973867767744189,
      "grad_norm": 10027.129998159991,
      "learning_rate": 2.282367528157753e-07,
      "loss": 1.7394,
      "step": 192160
    },
    {
      "epoch": 0.0006975029111252557,
      "grad_norm": 8711.602148858728,
      "learning_rate": 2.282177322938192e-07,
      "loss": 1.7291,
      "step": 192192
    },
    {
      "epoch": 0.0006976190454760923,
      "grad_norm": 8064.272936849297,
      "learning_rate": 2.281987165263993e-07,
      "loss": 1.7327,
      "step": 192224
    },
    {
      "epoch": 0.0006977351798269291,
      "grad_norm": 8984.159949600185,
      "learning_rate": 2.2817970551153515e-07,
      "loss": 1.7287,
      "step": 192256
    },
    {
      "epoch": 0.0006978513141777657,
      "grad_norm": 9705.274030134336,
      "learning_rate": 2.2816069924724732e-07,
      "loss": 1.7201,
      "step": 192288
    },
    {
      "epoch": 0.0006979674485286025,
      "grad_norm": 10637.069991308697,
      "learning_rate": 2.281416977315577e-07,
      "loss": 1.7305,
      "step": 192320
    },
    {
      "epoch": 0.0006980835828794391,
      "grad_norm": 10494.444244456206,
      "learning_rate": 2.2812270096248932e-07,
      "loss": 1.7523,
      "step": 192352
    },
    {
      "epoch": 0.0006981997172302759,
      "grad_norm": 12103.877230044925,
      "learning_rate": 2.2810370893806624e-07,
      "loss": 1.7748,
      "step": 192384
    },
    {
      "epoch": 0.0006983158515811126,
      "grad_norm": 9573.708999128812,
      "learning_rate": 2.2808472165631378e-07,
      "loss": 1.7175,
      "step": 192416
    },
    {
      "epoch": 0.0006984319859319493,
      "grad_norm": 8820.263034626576,
      "learning_rate": 2.2806573911525837e-07,
      "loss": 1.7015,
      "step": 192448
    },
    {
      "epoch": 0.000698548120282786,
      "grad_norm": 11527.921928951462,
      "learning_rate": 2.280467613129276e-07,
      "loss": 1.7235,
      "step": 192480
    },
    {
      "epoch": 0.0006986642546336227,
      "grad_norm": 8675.177462161797,
      "learning_rate": 2.2802778824735017e-07,
      "loss": 1.7184,
      "step": 192512
    },
    {
      "epoch": 0.0006987803889844594,
      "grad_norm": 9413.976630521238,
      "learning_rate": 2.2800941260524382e-07,
      "loss": 1.7249,
      "step": 192544
    },
    {
      "epoch": 0.0006988965233352961,
      "grad_norm": 9078.383556558954,
      "learning_rate": 2.279904488593933e-07,
      "loss": 1.729,
      "step": 192576
    },
    {
      "epoch": 0.0006990126576861328,
      "grad_norm": 7809.525465737339,
      "learning_rate": 2.2797148984445074e-07,
      "loss": 1.7027,
      "step": 192608
    },
    {
      "epoch": 0.0006991287920369695,
      "grad_norm": 9706.973472715375,
      "learning_rate": 2.279525355584494e-07,
      "loss": 1.7062,
      "step": 192640
    },
    {
      "epoch": 0.0006992449263878062,
      "grad_norm": 8872.552056764727,
      "learning_rate": 2.279335859994237e-07,
      "loss": 1.731,
      "step": 192672
    },
    {
      "epoch": 0.000699361060738643,
      "grad_norm": 9148.8565405738,
      "learning_rate": 2.279146411654092e-07,
      "loss": 1.7152,
      "step": 192704
    },
    {
      "epoch": 0.0006994771950894796,
      "grad_norm": 8797.920208776617,
      "learning_rate": 2.2789570105444264e-07,
      "loss": 1.7125,
      "step": 192736
    },
    {
      "epoch": 0.0006995933294403164,
      "grad_norm": 9809.39447672485,
      "learning_rate": 2.2787676566456192e-07,
      "loss": 1.6926,
      "step": 192768
    },
    {
      "epoch": 0.000699709463791153,
      "grad_norm": 7898.9991771109835,
      "learning_rate": 2.2785783499380598e-07,
      "loss": 1.7231,
      "step": 192800
    },
    {
      "epoch": 0.0006998255981419898,
      "grad_norm": 9206.729386704053,
      "learning_rate": 2.2783890904021497e-07,
      "loss": 1.7274,
      "step": 192832
    },
    {
      "epoch": 0.0006999417324928264,
      "grad_norm": 9043.700127713213,
      "learning_rate": 2.2781998780183018e-07,
      "loss": 1.7415,
      "step": 192864
    },
    {
      "epoch": 0.0007000578668436632,
      "grad_norm": 9840.508116962254,
      "learning_rate": 2.2780107127669401e-07,
      "loss": 1.7532,
      "step": 192896
    },
    {
      "epoch": 0.0007001740011944998,
      "grad_norm": 9284.71259652123,
      "learning_rate": 2.2778215946285005e-07,
      "loss": 1.77,
      "step": 192928
    },
    {
      "epoch": 0.0007002901355453366,
      "grad_norm": 8800.334652727703,
      "learning_rate": 2.2776325235834294e-07,
      "loss": 1.7493,
      "step": 192960
    },
    {
      "epoch": 0.0007004062698961733,
      "grad_norm": 9144.527215772285,
      "learning_rate": 2.2774434996121857e-07,
      "loss": 1.7478,
      "step": 192992
    },
    {
      "epoch": 0.00070052240424701,
      "grad_norm": 8679.399287969185,
      "learning_rate": 2.2772545226952383e-07,
      "loss": 1.7387,
      "step": 193024
    },
    {
      "epoch": 0.0007006385385978467,
      "grad_norm": 9319.522305354498,
      "learning_rate": 2.2770655928130687e-07,
      "loss": 1.7349,
      "step": 193056
    },
    {
      "epoch": 0.0007007546729486834,
      "grad_norm": 10134.417990195589,
      "learning_rate": 2.2768767099461689e-07,
      "loss": 1.7039,
      "step": 193088
    },
    {
      "epoch": 0.0007008708072995201,
      "grad_norm": 9252.396770567073,
      "learning_rate": 2.2766878740750426e-07,
      "loss": 1.7143,
      "step": 193120
    },
    {
      "epoch": 0.0007009869416503568,
      "grad_norm": 9521.451149903569,
      "learning_rate": 2.276499085180205e-07,
      "loss": 1.7496,
      "step": 193152
    },
    {
      "epoch": 0.0007011030760011935,
      "grad_norm": 9221.374734821267,
      "learning_rate": 2.2763103432421822e-07,
      "loss": 1.7267,
      "step": 193184
    },
    {
      "epoch": 0.0007012192103520302,
      "grad_norm": 9495.371504053963,
      "learning_rate": 2.2761216482415117e-07,
      "loss": 1.7358,
      "step": 193216
    },
    {
      "epoch": 0.0007013353447028669,
      "grad_norm": 11190.574069278127,
      "learning_rate": 2.275933000158742e-07,
      "loss": 1.7483,
      "step": 193248
    },
    {
      "epoch": 0.0007014514790537037,
      "grad_norm": 11108.021786078743,
      "learning_rate": 2.2757443989744337e-07,
      "loss": 1.7226,
      "step": 193280
    },
    {
      "epoch": 0.0007015676134045403,
      "grad_norm": 11171.129217764872,
      "learning_rate": 2.2755558446691582e-07,
      "loss": 1.7033,
      "step": 193312
    },
    {
      "epoch": 0.0007016837477553771,
      "grad_norm": 9170.476105415684,
      "learning_rate": 2.2753673372234977e-07,
      "loss": 1.7029,
      "step": 193344
    },
    {
      "epoch": 0.0007017998821062137,
      "grad_norm": 8787.38800782121,
      "learning_rate": 2.2751788766180468e-07,
      "loss": 1.7159,
      "step": 193376
    },
    {
      "epoch": 0.0007019160164570505,
      "grad_norm": 8983.982190543345,
      "learning_rate": 2.27499046283341e-07,
      "loss": 1.7229,
      "step": 193408
    },
    {
      "epoch": 0.0007020321508078871,
      "grad_norm": 10085.406883214975,
      "learning_rate": 2.2748020958502042e-07,
      "loss": 1.6971,
      "step": 193440
    },
    {
      "epoch": 0.0007021482851587239,
      "grad_norm": 10981.767617282749,
      "learning_rate": 2.2746137756490567e-07,
      "loss": 1.7246,
      "step": 193472
    },
    {
      "epoch": 0.0007022644195095605,
      "grad_norm": 9204.161667419798,
      "learning_rate": 2.2744255022106066e-07,
      "loss": 1.7442,
      "step": 193504
    },
    {
      "epoch": 0.0007023805538603973,
      "grad_norm": 9729.897635638312,
      "learning_rate": 2.274243156892377e-07,
      "loss": 1.7687,
      "step": 193536
    },
    {
      "epoch": 0.000702496688211234,
      "grad_norm": 9141.871143261646,
      "learning_rate": 2.2740549754614503e-07,
      "loss": 1.7415,
      "step": 193568
    },
    {
      "epoch": 0.0007026128225620707,
      "grad_norm": 9121.763426004864,
      "learning_rate": 2.273866840735809e-07,
      "loss": 1.7197,
      "step": 193600
    },
    {
      "epoch": 0.0007027289569129074,
      "grad_norm": 9181.534947926735,
      "learning_rate": 2.2736787526961352e-07,
      "loss": 1.6998,
      "step": 193632
    },
    {
      "epoch": 0.0007028450912637441,
      "grad_norm": 9722.274219543491,
      "learning_rate": 2.273490711323125e-07,
      "loss": 1.7078,
      "step": 193664
    },
    {
      "epoch": 0.0007029612256145808,
      "grad_norm": 9037.209082454605,
      "learning_rate": 2.273302716597483e-07,
      "loss": 1.7117,
      "step": 193696
    },
    {
      "epoch": 0.0007030773599654175,
      "grad_norm": 10928.58947897669,
      "learning_rate": 2.2731147684999266e-07,
      "loss": 1.7385,
      "step": 193728
    },
    {
      "epoch": 0.0007031934943162542,
      "grad_norm": 10615.642797306247,
      "learning_rate": 2.2729268670111834e-07,
      "loss": 1.7347,
      "step": 193760
    },
    {
      "epoch": 0.0007033096286670909,
      "grad_norm": 8837.832426562522,
      "learning_rate": 2.2727390121119929e-07,
      "loss": 1.7275,
      "step": 193792
    },
    {
      "epoch": 0.0007034257630179276,
      "grad_norm": 8122.935676219527,
      "learning_rate": 2.2725512037831055e-07,
      "loss": 1.7512,
      "step": 193824
    },
    {
      "epoch": 0.0007035418973687644,
      "grad_norm": 8580.409896968793,
      "learning_rate": 2.2723634420052824e-07,
      "loss": 1.7555,
      "step": 193856
    },
    {
      "epoch": 0.000703658031719601,
      "grad_norm": 9290.134014103349,
      "learning_rate": 2.2721757267592956e-07,
      "loss": 1.7512,
      "step": 193888
    },
    {
      "epoch": 0.0007037741660704378,
      "grad_norm": 10816.37499349944,
      "learning_rate": 2.27198805802593e-07,
      "loss": 1.7381,
      "step": 193920
    },
    {
      "epoch": 0.0007038903004212744,
      "grad_norm": 7866.197810886782,
      "learning_rate": 2.2718004357859796e-07,
      "loss": 1.7254,
      "step": 193952
    },
    {
      "epoch": 0.0007040064347721112,
      "grad_norm": 9244.996917252054,
      "learning_rate": 2.2716128600202505e-07,
      "loss": 1.7097,
      "step": 193984
    },
    {
      "epoch": 0.0007041225691229478,
      "grad_norm": 7867.797023309638,
      "learning_rate": 2.2714253307095593e-07,
      "loss": 1.722,
      "step": 194016
    },
    {
      "epoch": 0.0007042387034737846,
      "grad_norm": 9289.676420629516,
      "learning_rate": 2.271237847834735e-07,
      "loss": 1.7333,
      "step": 194048
    },
    {
      "epoch": 0.0007043548378246212,
      "grad_norm": 9896.98984540249,
      "learning_rate": 2.2710504113766155e-07,
      "loss": 1.7465,
      "step": 194080
    },
    {
      "epoch": 0.000704470972175458,
      "grad_norm": 11459.13295149332,
      "learning_rate": 2.270863021316052e-07,
      "loss": 1.7464,
      "step": 194112
    },
    {
      "epoch": 0.0007045871065262947,
      "grad_norm": 9453.07759409601,
      "learning_rate": 2.2706756776339056e-07,
      "loss": 1.7336,
      "step": 194144
    },
    {
      "epoch": 0.0007047032408771314,
      "grad_norm": 9639.931120085868,
      "learning_rate": 2.270488380311048e-07,
      "loss": 1.7376,
      "step": 194176
    },
    {
      "epoch": 0.0007048193752279681,
      "grad_norm": 8324.893873197423,
      "learning_rate": 2.2703011293283632e-07,
      "loss": 1.7141,
      "step": 194208
    },
    {
      "epoch": 0.0007049355095788048,
      "grad_norm": 9639.385250108016,
      "learning_rate": 2.2701139246667454e-07,
      "loss": 1.717,
      "step": 194240
    },
    {
      "epoch": 0.0007050516439296415,
      "grad_norm": 9979.373327018086,
      "learning_rate": 2.2699267663071e-07,
      "loss": 1.7273,
      "step": 194272
    },
    {
      "epoch": 0.0007051677782804782,
      "grad_norm": 9898.23398389834,
      "learning_rate": 2.2697396542303434e-07,
      "loss": 1.7077,
      "step": 194304
    },
    {
      "epoch": 0.0007052839126313149,
      "grad_norm": 11689.422226953735,
      "learning_rate": 2.269552588417403e-07,
      "loss": 1.7122,
      "step": 194336
    },
    {
      "epoch": 0.0007054000469821516,
      "grad_norm": 9560.393506545637,
      "learning_rate": 2.2693655688492176e-07,
      "loss": 1.7107,
      "step": 194368
    },
    {
      "epoch": 0.0007055161813329883,
      "grad_norm": 9668.312055369333,
      "learning_rate": 2.269178595506736e-07,
      "loss": 1.7426,
      "step": 194400
    },
    {
      "epoch": 0.0007056323156838251,
      "grad_norm": 9960.241161739006,
      "learning_rate": 2.2689916683709192e-07,
      "loss": 1.7138,
      "step": 194432
    },
    {
      "epoch": 0.0007057484500346617,
      "grad_norm": 8599.918139145278,
      "learning_rate": 2.2688047874227384e-07,
      "loss": 1.6922,
      "step": 194464
    },
    {
      "epoch": 0.0007058645843854985,
      "grad_norm": 8804.493171103037,
      "learning_rate": 2.268617952643176e-07,
      "loss": 1.7129,
      "step": 194496
    },
    {
      "epoch": 0.0007059807187363351,
      "grad_norm": 9793.148012768928,
      "learning_rate": 2.268431164013225e-07,
      "loss": 1.7252,
      "step": 194528
    },
    {
      "epoch": 0.0007060968530871719,
      "grad_norm": 8233.448244812134,
      "learning_rate": 2.268250256518917e-07,
      "loss": 1.7232,
      "step": 194560
    },
    {
      "epoch": 0.0007062129874380085,
      "grad_norm": 10206.226334938883,
      "learning_rate": 2.2680635586905121e-07,
      "loss": 1.7288,
      "step": 194592
    },
    {
      "epoch": 0.0007063291217888453,
      "grad_norm": 9710.505136191423,
      "learning_rate": 2.2678769069553575e-07,
      "loss": 1.7467,
      "step": 194624
    },
    {
      "epoch": 0.0007064452561396819,
      "grad_norm": 8455.12802978169,
      "learning_rate": 2.2676903012944901e-07,
      "loss": 1.7372,
      "step": 194656
    },
    {
      "epoch": 0.0007065613904905187,
      "grad_norm": 9329.042180202638,
      "learning_rate": 2.2675037416889567e-07,
      "loss": 1.7553,
      "step": 194688
    },
    {
      "epoch": 0.0007066775248413554,
      "grad_norm": 9234.687650375621,
      "learning_rate": 2.2673172281198168e-07,
      "loss": 1.7657,
      "step": 194720
    },
    {
      "epoch": 0.0007067936591921921,
      "grad_norm": 11312.628518606984,
      "learning_rate": 2.2671307605681395e-07,
      "loss": 1.7707,
      "step": 194752
    },
    {
      "epoch": 0.0007069097935430288,
      "grad_norm": 9553.63386361441,
      "learning_rate": 2.2669443390150054e-07,
      "loss": 1.7278,
      "step": 194784
    },
    {
      "epoch": 0.0007070259278938655,
      "grad_norm": 10302.083672733394,
      "learning_rate": 2.2667579634415056e-07,
      "loss": 1.7044,
      "step": 194816
    },
    {
      "epoch": 0.0007071420622447022,
      "grad_norm": 8371.77125822248,
      "learning_rate": 2.2665716338287422e-07,
      "loss": 1.7259,
      "step": 194848
    },
    {
      "epoch": 0.0007072581965955389,
      "grad_norm": 8517.81004718936,
      "learning_rate": 2.2663853501578285e-07,
      "loss": 1.7343,
      "step": 194880
    },
    {
      "epoch": 0.0007073743309463756,
      "grad_norm": 9776.091448017454,
      "learning_rate": 2.2661991124098876e-07,
      "loss": 1.7467,
      "step": 194912
    },
    {
      "epoch": 0.0007074904652972123,
      "grad_norm": 9569.607724457675,
      "learning_rate": 2.2660129205660555e-07,
      "loss": 1.737,
      "step": 194944
    },
    {
      "epoch": 0.000707606599648049,
      "grad_norm": 9162.774688924748,
      "learning_rate": 2.265826774607477e-07,
      "loss": 1.7312,
      "step": 194976
    },
    {
      "epoch": 0.0007077227339988858,
      "grad_norm": 9008.886501671559,
      "learning_rate": 2.2656406745153087e-07,
      "loss": 1.7044,
      "step": 195008
    },
    {
      "epoch": 0.0007078388683497224,
      "grad_norm": 8650.512470368447,
      "learning_rate": 2.2654546202707176e-07,
      "loss": 1.7193,
      "step": 195040
    },
    {
      "epoch": 0.0007079550027005592,
      "grad_norm": 9362.343510040635,
      "learning_rate": 2.265268611854882e-07,
      "loss": 1.7144,
      "step": 195072
    },
    {
      "epoch": 0.0007080711370513958,
      "grad_norm": 9714.537868576148,
      "learning_rate": 2.265082649248991e-07,
      "loss": 1.719,
      "step": 195104
    },
    {
      "epoch": 0.0007081872714022326,
      "grad_norm": 8203.42233461133,
      "learning_rate": 2.2648967324342445e-07,
      "loss": 1.6984,
      "step": 195136
    },
    {
      "epoch": 0.0007083034057530692,
      "grad_norm": 9117.393267815094,
      "learning_rate": 2.2647108613918526e-07,
      "loss": 1.699,
      "step": 195168
    },
    {
      "epoch": 0.000708419540103906,
      "grad_norm": 9967.799957864323,
      "learning_rate": 2.2645250361030364e-07,
      "loss": 1.7309,
      "step": 195200
    },
    {
      "epoch": 0.0007085356744547426,
      "grad_norm": 9316.293039616132,
      "learning_rate": 2.2643392565490284e-07,
      "loss": 1.7371,
      "step": 195232
    },
    {
      "epoch": 0.0007086518088055794,
      "grad_norm": 9599.794164459987,
      "learning_rate": 2.2641535227110715e-07,
      "loss": 1.7478,
      "step": 195264
    },
    {
      "epoch": 0.0007087679431564162,
      "grad_norm": 9775.850448937934,
      "learning_rate": 2.2639678345704192e-07,
      "loss": 1.7462,
      "step": 195296
    },
    {
      "epoch": 0.0007088840775072528,
      "grad_norm": 7991.826199311394,
      "learning_rate": 2.2637821921083356e-07,
      "loss": 1.7221,
      "step": 195328
    },
    {
      "epoch": 0.0007090002118580896,
      "grad_norm": 10209.725657430761,
      "learning_rate": 2.2635965953060963e-07,
      "loss": 1.6999,
      "step": 195360
    },
    {
      "epoch": 0.0007091163462089262,
      "grad_norm": 8262.16206570653,
      "learning_rate": 2.2634110441449873e-07,
      "loss": 1.7073,
      "step": 195392
    },
    {
      "epoch": 0.000709232480559763,
      "grad_norm": 9351.461169250504,
      "learning_rate": 2.2632255386063047e-07,
      "loss": 1.7216,
      "step": 195424
    },
    {
      "epoch": 0.0007093486149105996,
      "grad_norm": 9412.7029061795,
      "learning_rate": 2.2630400786713564e-07,
      "loss": 1.7171,
      "step": 195456
    },
    {
      "epoch": 0.0007094647492614364,
      "grad_norm": 8598.771540167816,
      "learning_rate": 2.26285466432146e-07,
      "loss": 1.7183,
      "step": 195488
    },
    {
      "epoch": 0.000709580883612273,
      "grad_norm": 9572.10102328637,
      "learning_rate": 2.2626692955379446e-07,
      "loss": 1.7478,
      "step": 195520
    },
    {
      "epoch": 0.0007096970179631098,
      "grad_norm": 7500.607842035204,
      "learning_rate": 2.2624897629640102e-07,
      "loss": 1.752,
      "step": 195552
    },
    {
      "epoch": 0.0007098131523139465,
      "grad_norm": 8975.668888723558,
      "learning_rate": 2.2623044838347846e-07,
      "loss": 1.7543,
      "step": 195584
    },
    {
      "epoch": 0.0007099292866647832,
      "grad_norm": 8643.3618459486,
      "learning_rate": 2.262119250216573e-07,
      "loss": 1.757,
      "step": 195616
    },
    {
      "epoch": 0.0007100454210156199,
      "grad_norm": 9154.144198121418,
      "learning_rate": 2.261934062090746e-07,
      "loss": 1.7542,
      "step": 195648
    },
    {
      "epoch": 0.0007101615553664566,
      "grad_norm": 8934.525393102871,
      "learning_rate": 2.2617489194386859e-07,
      "loss": 1.7073,
      "step": 195680
    },
    {
      "epoch": 0.0007102776897172933,
      "grad_norm": 8299.726381032087,
      "learning_rate": 2.2615638222417853e-07,
      "loss": 1.7079,
      "step": 195712
    },
    {
      "epoch": 0.00071039382406813,
      "grad_norm": 8525.68120445516,
      "learning_rate": 2.2613787704814478e-07,
      "loss": 1.7245,
      "step": 195744
    },
    {
      "epoch": 0.0007105099584189667,
      "grad_norm": 10699.625787848845,
      "learning_rate": 2.2611937641390866e-07,
      "loss": 1.7378,
      "step": 195776
    },
    {
      "epoch": 0.0007106260927698034,
      "grad_norm": 9576.447775662957,
      "learning_rate": 2.2610088031961264e-07,
      "loss": 1.7299,
      "step": 195808
    },
    {
      "epoch": 0.0007107422271206401,
      "grad_norm": 9581.427242326688,
      "learning_rate": 2.260823887634003e-07,
      "loss": 1.7151,
      "step": 195840
    },
    {
      "epoch": 0.0007108583614714769,
      "grad_norm": 8988.796693662618,
      "learning_rate": 2.2606390174341614e-07,
      "loss": 1.7516,
      "step": 195872
    },
    {
      "epoch": 0.0007109744958223135,
      "grad_norm": 11564.650102791697,
      "learning_rate": 2.2604541925780583e-07,
      "loss": 1.757,
      "step": 195904
    },
    {
      "epoch": 0.0007110906301731503,
      "grad_norm": 8276.391484215812,
      "learning_rate": 2.2602694130471604e-07,
      "loss": 1.7418,
      "step": 195936
    },
    {
      "epoch": 0.0007112067645239869,
      "grad_norm": 8575.398649625567,
      "learning_rate": 2.2600846788229458e-07,
      "loss": 1.713,
      "step": 195968
    },
    {
      "epoch": 0.0007113228988748237,
      "grad_norm": 10512.023021283774,
      "learning_rate": 2.2598999898869025e-07,
      "loss": 1.7136,
      "step": 196000
    },
    {
      "epoch": 0.0007114390332256603,
      "grad_norm": 7764.435330402334,
      "learning_rate": 2.259715346220529e-07,
      "loss": 1.6943,
      "step": 196032
    },
    {
      "epoch": 0.0007115551675764971,
      "grad_norm": 9211.345830007687,
      "learning_rate": 2.2595307478053357e-07,
      "loss": 1.7049,
      "step": 196064
    },
    {
      "epoch": 0.0007116713019273337,
      "grad_norm": 10718.68947213231,
      "learning_rate": 2.259346194622841e-07,
      "loss": 1.7466,
      "step": 196096
    },
    {
      "epoch": 0.0007117874362781705,
      "grad_norm": 10854.537300133987,
      "learning_rate": 2.2591616866545767e-07,
      "loss": 1.725,
      "step": 196128
    },
    {
      "epoch": 0.0007119035706290072,
      "grad_norm": 11004.84202521781,
      "learning_rate": 2.2589772238820833e-07,
      "loss": 1.703,
      "step": 196160
    },
    {
      "epoch": 0.0007120197049798439,
      "grad_norm": 9539.5440142598,
      "learning_rate": 2.2587928062869124e-07,
      "loss": 1.706,
      "step": 196192
    },
    {
      "epoch": 0.0007121358393306806,
      "grad_norm": 8850.633762618358,
      "learning_rate": 2.2586084338506266e-07,
      "loss": 1.7182,
      "step": 196224
    },
    {
      "epoch": 0.0007122519736815173,
      "grad_norm": 8869.72412197809,
      "learning_rate": 2.2584241065547978e-07,
      "loss": 1.7199,
      "step": 196256
    },
    {
      "epoch": 0.000712368108032354,
      "grad_norm": 9808.62661130497,
      "learning_rate": 2.2582398243810102e-07,
      "loss": 1.7204,
      "step": 196288
    },
    {
      "epoch": 0.0007124842423831907,
      "grad_norm": 9494.196332497027,
      "learning_rate": 2.2580555873108564e-07,
      "loss": 1.7293,
      "step": 196320
    },
    {
      "epoch": 0.0007126003767340274,
      "grad_norm": 10152.267332965577,
      "learning_rate": 2.2578713953259416e-07,
      "loss": 1.718,
      "step": 196352
    },
    {
      "epoch": 0.0007127165110848641,
      "grad_norm": 11199.245956759769,
      "learning_rate": 2.2576872484078803e-07,
      "loss": 1.7294,
      "step": 196384
    },
    {
      "epoch": 0.0007128326454357008,
      "grad_norm": 10046.304196071309,
      "learning_rate": 2.2575031465382973e-07,
      "loss": 1.751,
      "step": 196416
    },
    {
      "epoch": 0.0007129487797865376,
      "grad_norm": 10117.945245947914,
      "learning_rate": 2.2573190896988288e-07,
      "loss": 1.7689,
      "step": 196448
    },
    {
      "epoch": 0.0007130649141373742,
      "grad_norm": 9477.602228411994,
      "learning_rate": 2.257135077871121e-07,
      "loss": 1.7671,
      "step": 196480
    },
    {
      "epoch": 0.000713181048488211,
      "grad_norm": 8744.905488340055,
      "learning_rate": 2.2569511110368307e-07,
      "loss": 1.7467,
      "step": 196512
    },
    {
      "epoch": 0.0007132971828390476,
      "grad_norm": 19378.6965505939,
      "learning_rate": 2.2567671891776245e-07,
      "loss": 1.7332,
      "step": 196544
    },
    {
      "epoch": 0.0007134133171898844,
      "grad_norm": 8560.355483272877,
      "learning_rate": 2.2565890577480655e-07,
      "loss": 1.7263,
      "step": 196576
    },
    {
      "epoch": 0.000713529451540721,
      "grad_norm": 8551.028826989183,
      "learning_rate": 2.256405224380022e-07,
      "loss": 1.7361,
      "step": 196608
    },
    {
      "epoch": 0.0007136455858915578,
      "grad_norm": 9590.143898816117,
      "learning_rate": 2.2562214359326984e-07,
      "loss": 1.7308,
      "step": 196640
    },
    {
      "epoch": 0.0007137617202423944,
      "grad_norm": 9281.717513477772,
      "learning_rate": 2.256037692387804e-07,
      "loss": 1.7536,
      "step": 196672
    },
    {
      "epoch": 0.0007138778545932312,
      "grad_norm": 11391.412203936789,
      "learning_rate": 2.2558539937270576e-07,
      "loss": 1.7017,
      "step": 196704
    },
    {
      "epoch": 0.0007139939889440679,
      "grad_norm": 8229.438741493856,
      "learning_rate": 2.2556703399321885e-07,
      "loss": 1.7104,
      "step": 196736
    },
    {
      "epoch": 0.0007141101232949046,
      "grad_norm": 8911.135168989413,
      "learning_rate": 2.2554867309849363e-07,
      "loss": 1.7249,
      "step": 196768
    },
    {
      "epoch": 0.0007142262576457413,
      "grad_norm": 9548.024402985155,
      "learning_rate": 2.2553031668670516e-07,
      "loss": 1.739,
      "step": 196800
    },
    {
      "epoch": 0.000714342391996578,
      "grad_norm": 8749.163045686142,
      "learning_rate": 2.2551196475602952e-07,
      "loss": 1.6895,
      "step": 196832
    },
    {
      "epoch": 0.0007144585263474147,
      "grad_norm": 8703.068998922161,
      "learning_rate": 2.254936173046438e-07,
      "loss": 1.6981,
      "step": 196864
    },
    {
      "epoch": 0.0007145746606982514,
      "grad_norm": 10910.9959215463,
      "learning_rate": 2.2547527433072612e-07,
      "loss": 1.7214,
      "step": 196896
    },
    {
      "epoch": 0.0007146907950490881,
      "grad_norm": 9310.663778700206,
      "learning_rate": 2.2545693583245565e-07,
      "loss": 1.7219,
      "step": 196928
    },
    {
      "epoch": 0.0007148069293999248,
      "grad_norm": 9681.071221719216,
      "learning_rate": 2.2543860180801265e-07,
      "loss": 1.7417,
      "step": 196960
    },
    {
      "epoch": 0.0007149230637507615,
      "grad_norm": 10663.21208642124,
      "learning_rate": 2.2542027225557835e-07,
      "loss": 1.7334,
      "step": 196992
    },
    {
      "epoch": 0.0007150391981015983,
      "grad_norm": 10054.431659721,
      "learning_rate": 2.2540194717333504e-07,
      "loss": 1.7186,
      "step": 197024
    },
    {
      "epoch": 0.0007151553324524349,
      "grad_norm": 9926.49787185793,
      "learning_rate": 2.25383626559466e-07,
      "loss": 1.7222,
      "step": 197056
    },
    {
      "epoch": 0.0007152714668032717,
      "grad_norm": 8733.201360326006,
      "learning_rate": 2.2536531041215564e-07,
      "loss": 1.7236,
      "step": 197088
    },
    {
      "epoch": 0.0007153876011541083,
      "grad_norm": 9818.827017520984,
      "learning_rate": 2.2534699872958932e-07,
      "loss": 1.7186,
      "step": 197120
    },
    {
      "epoch": 0.0007155037355049451,
      "grad_norm": 9077.290344590725,
      "learning_rate": 2.253286915099535e-07,
      "loss": 1.7132,
      "step": 197152
    },
    {
      "epoch": 0.0007156198698557817,
      "grad_norm": 10013.223257273354,
      "learning_rate": 2.2531038875143551e-07,
      "loss": 1.7032,
      "step": 197184
    },
    {
      "epoch": 0.0007157360042066185,
      "grad_norm": 8049.03957003567,
      "learning_rate": 2.2529209045222397e-07,
      "loss": 1.7171,
      "step": 197216
    },
    {
      "epoch": 0.0007158521385574551,
      "grad_norm": 8184.317931263423,
      "learning_rate": 2.252737966105083e-07,
      "loss": 1.7564,
      "step": 197248
    },
    {
      "epoch": 0.0007159682729082919,
      "grad_norm": 9775.482085298914,
      "learning_rate": 2.252555072244791e-07,
      "loss": 1.7623,
      "step": 197280
    },
    {
      "epoch": 0.0007160844072591286,
      "grad_norm": 9564.840824603409,
      "learning_rate": 2.2523722229232788e-07,
      "loss": 1.754,
      "step": 197312
    },
    {
      "epoch": 0.0007162005416099653,
      "grad_norm": 10090.995292834103,
      "learning_rate": 2.252189418122473e-07,
      "loss": 1.7584,
      "step": 197344
    },
    {
      "epoch": 0.000716316675960802,
      "grad_norm": 8168.682023435605,
      "learning_rate": 2.252006657824309e-07,
      "loss": 1.7217,
      "step": 197376
    },
    {
      "epoch": 0.0007164328103116387,
      "grad_norm": 9954.123165804209,
      "learning_rate": 2.2518239420107335e-07,
      "loss": 1.7305,
      "step": 197408
    },
    {
      "epoch": 0.0007165489446624754,
      "grad_norm": 10715.151702145891,
      "learning_rate": 2.2516412706637035e-07,
      "loss": 1.7174,
      "step": 197440
    },
    {
      "epoch": 0.0007166650790133121,
      "grad_norm": 9288.254626139402,
      "learning_rate": 2.251458643765186e-07,
      "loss": 1.7229,
      "step": 197472
    },
    {
      "epoch": 0.0007167812133641488,
      "grad_norm": 10095.404697187725,
      "learning_rate": 2.251276061297158e-07,
      "loss": 1.7203,
      "step": 197504
    },
    {
      "epoch": 0.0007168973477149855,
      "grad_norm": 9294.129760230378,
      "learning_rate": 2.251093523241607e-07,
      "loss": 1.717,
      "step": 197536
    },
    {
      "epoch": 0.0007170134820658222,
      "grad_norm": 18183.739989342128,
      "learning_rate": 2.25091102958053e-07,
      "loss": 1.729,
      "step": 197568
    },
    {
      "epoch": 0.000717129616416659,
      "grad_norm": 9502.809479306632,
      "learning_rate": 2.2507342811645441e-07,
      "loss": 1.7272,
      "step": 197600
    },
    {
      "epoch": 0.0007172457507674956,
      "grad_norm": 12140.03887967415,
      "learning_rate": 2.2505518748525196e-07,
      "loss": 1.7456,
      "step": 197632
    },
    {
      "epoch": 0.0007173618851183324,
      "grad_norm": 10115.092683707846,
      "learning_rate": 2.250369512881585e-07,
      "loss": 1.7559,
      "step": 197664
    },
    {
      "epoch": 0.000717478019469169,
      "grad_norm": 10314.929180561541,
      "learning_rate": 2.2501871952337792e-07,
      "loss": 1.7364,
      "step": 197696
    },
    {
      "epoch": 0.0007175941538200058,
      "grad_norm": 9271.192372073832,
      "learning_rate": 2.25000492189115e-07,
      "loss": 1.6879,
      "step": 197728
    },
    {
      "epoch": 0.0007177102881708424,
      "grad_norm": 8964.087683640762,
      "learning_rate": 2.249822692835756e-07,
      "loss": 1.7048,
      "step": 197760
    },
    {
      "epoch": 0.0007178264225216792,
      "grad_norm": 9139.033865786909,
      "learning_rate": 2.249640508049666e-07,
      "loss": 1.7185,
      "step": 197792
    },
    {
      "epoch": 0.0007179425568725158,
      "grad_norm": 10336.411176032037,
      "learning_rate": 2.2494583675149588e-07,
      "loss": 1.7438,
      "step": 197824
    },
    {
      "epoch": 0.0007180586912233526,
      "grad_norm": 8311.31902889066,
      "learning_rate": 2.2492762712137231e-07,
      "loss": 1.7077,
      "step": 197856
    },
    {
      "epoch": 0.0007181748255741893,
      "grad_norm": 8908.70967087827,
      "learning_rate": 2.2490942191280586e-07,
      "loss": 1.6994,
      "step": 197888
    },
    {
      "epoch": 0.000718290959925026,
      "grad_norm": 8894.84446182169,
      "learning_rate": 2.248912211240074e-07,
      "loss": 1.7274,
      "step": 197920
    },
    {
      "epoch": 0.0007184070942758627,
      "grad_norm": 11180.71303629603,
      "learning_rate": 2.2487302475318894e-07,
      "loss": 1.7193,
      "step": 197952
    },
    {
      "epoch": 0.0007185232286266994,
      "grad_norm": 12116.985103564335,
      "learning_rate": 2.2485483279856335e-07,
      "loss": 1.7137,
      "step": 197984
    },
    {
      "epoch": 0.0007186393629775361,
      "grad_norm": 8664.511988565773,
      "learning_rate": 2.2483664525834465e-07,
      "loss": 1.7246,
      "step": 198016
    },
    {
      "epoch": 0.0007187554973283728,
      "grad_norm": 10778.80382973918,
      "learning_rate": 2.248184621307478e-07,
      "loss": 1.7118,
      "step": 198048
    },
    {
      "epoch": 0.0007188716316792095,
      "grad_norm": 10801.4310163052,
      "learning_rate": 2.2480028341398878e-07,
      "loss": 1.7129,
      "step": 198080
    },
    {
      "epoch": 0.0007189877660300462,
      "grad_norm": 9640.742917431207,
      "learning_rate": 2.2478210910628461e-07,
      "loss": 1.7278,
      "step": 198112
    },
    {
      "epoch": 0.0007191039003808829,
      "grad_norm": 9351.511535575411,
      "learning_rate": 2.2476393920585328e-07,
      "loss": 1.7464,
      "step": 198144
    },
    {
      "epoch": 0.0007192200347317197,
      "grad_norm": 9902.208642520112,
      "learning_rate": 2.2474577371091382e-07,
      "loss": 1.7604,
      "step": 198176
    },
    {
      "epoch": 0.0007193361690825563,
      "grad_norm": 10795.04645659295,
      "learning_rate": 2.2472761261968623e-07,
      "loss": 1.7429,
      "step": 198208
    },
    {
      "epoch": 0.0007194523034333931,
      "grad_norm": 9694.884217978057,
      "learning_rate": 2.2470945593039156e-07,
      "loss": 1.7624,
      "step": 198240
    },
    {
      "epoch": 0.0007195684377842297,
      "grad_norm": 8210.241409361846,
      "learning_rate": 2.2469130364125182e-07,
      "loss": 1.7731,
      "step": 198272
    },
    {
      "epoch": 0.0007196845721350665,
      "grad_norm": 8709.709065175484,
      "learning_rate": 2.2467315575049006e-07,
      "loss": 1.7346,
      "step": 198304
    },
    {
      "epoch": 0.0007198007064859031,
      "grad_norm": 7864.987603296016,
      "learning_rate": 2.2465501225633031e-07,
      "loss": 1.7254,
      "step": 198336
    },
    {
      "epoch": 0.0007199168408367399,
      "grad_norm": 9640.149376436031,
      "learning_rate": 2.2463687315699765e-07,
      "loss": 1.7375,
      "step": 198368
    },
    {
      "epoch": 0.0007200329751875765,
      "grad_norm": 8444.784070655685,
      "learning_rate": 2.246187384507181e-07,
      "loss": 1.7131,
      "step": 198400
    },
    {
      "epoch": 0.0007201491095384133,
      "grad_norm": 9045.021503567585,
      "learning_rate": 2.2460060813571873e-07,
      "loss": 1.7137,
      "step": 198432
    },
    {
      "epoch": 0.00072026524388925,
      "grad_norm": 10428.387603076517,
      "learning_rate": 2.2458248221022756e-07,
      "loss": 1.7094,
      "step": 198464
    },
    {
      "epoch": 0.0007203813782400867,
      "grad_norm": 10263.416585133822,
      "learning_rate": 2.2456436067247373e-07,
      "loss": 1.7212,
      "step": 198496
    },
    {
      "epoch": 0.0007204975125909234,
      "grad_norm": 10908.980887323985,
      "learning_rate": 2.245462435206872e-07,
      "loss": 1.7141,
      "step": 198528
    },
    {
      "epoch": 0.0007206136469417601,
      "grad_norm": 10441.387743015772,
      "learning_rate": 2.2452813075309906e-07,
      "loss": 1.718,
      "step": 198560
    },
    {
      "epoch": 0.0007207297812925968,
      "grad_norm": 19294.25406694957,
      "learning_rate": 2.245105881886595e-07,
      "loss": 1.7073,
      "step": 198592
    },
    {
      "epoch": 0.0007208459156434335,
      "grad_norm": 8881.936050208873,
      "learning_rate": 2.244924840472963e-07,
      "loss": 1.7211,
      "step": 198624
    },
    {
      "epoch": 0.0007209620499942702,
      "grad_norm": 10168.710832745712,
      "learning_rate": 2.2447438428488587e-07,
      "loss": 1.7292,
      "step": 198656
    },
    {
      "epoch": 0.0007210781843451069,
      "grad_norm": 9614.783616909952,
      "learning_rate": 2.2445628889966312e-07,
      "loss": 1.7329,
      "step": 198688
    },
    {
      "epoch": 0.0007211943186959436,
      "grad_norm": 10161.76539780367,
      "learning_rate": 2.2443819788986414e-07,
      "loss": 1.7134,
      "step": 198720
    },
    {
      "epoch": 0.0007213104530467804,
      "grad_norm": 9580.588917180405,
      "learning_rate": 2.2442011125372589e-07,
      "loss": 1.7037,
      "step": 198752
    },
    {
      "epoch": 0.000721426587397617,
      "grad_norm": 9636.169259617642,
      "learning_rate": 2.244020289894864e-07,
      "loss": 1.7123,
      "step": 198784
    },
    {
      "epoch": 0.0007215427217484538,
      "grad_norm": 11198.136184204941,
      "learning_rate": 2.2438395109538463e-07,
      "loss": 1.7374,
      "step": 198816
    },
    {
      "epoch": 0.0007216588560992904,
      "grad_norm": 10659.390976974248,
      "learning_rate": 2.2436587756966056e-07,
      "loss": 1.7369,
      "step": 198848
    },
    {
      "epoch": 0.0007217749904501272,
      "grad_norm": 8151.381478007271,
      "learning_rate": 2.2434780841055524e-07,
      "loss": 1.6927,
      "step": 198880
    },
    {
      "epoch": 0.0007218911248009638,
      "grad_norm": 9002.016662948365,
      "learning_rate": 2.2432974361631056e-07,
      "loss": 1.7069,
      "step": 198912
    },
    {
      "epoch": 0.0007220072591518006,
      "grad_norm": 9423.447988926346,
      "learning_rate": 2.2431168318516952e-07,
      "loss": 1.7359,
      "step": 198944
    },
    {
      "epoch": 0.0007221233935026372,
      "grad_norm": 8789.873036625728,
      "learning_rate": 2.2429362711537606e-07,
      "loss": 1.7366,
      "step": 198976
    },
    {
      "epoch": 0.000722239527853474,
      "grad_norm": 9369.03516910893,
      "learning_rate": 2.2427557540517512e-07,
      "loss": 1.7487,
      "step": 199008
    },
    {
      "epoch": 0.0007223556622043107,
      "grad_norm": 9376.02741036949,
      "learning_rate": 2.2425752805281264e-07,
      "loss": 1.7613,
      "step": 199040
    },
    {
      "epoch": 0.0007224717965551474,
      "grad_norm": 10206.27297302987,
      "learning_rate": 2.242394850565355e-07,
      "loss": 1.7339,
      "step": 199072
    },
    {
      "epoch": 0.0007225879309059841,
      "grad_norm": 9499.12332797085,
      "learning_rate": 2.2422144641459161e-07,
      "loss": 1.7312,
      "step": 199104
    },
    {
      "epoch": 0.0007227040652568208,
      "grad_norm": 9880.66758878164,
      "learning_rate": 2.242034121252299e-07,
      "loss": 1.7373,
      "step": 199136
    },
    {
      "epoch": 0.0007228201996076575,
      "grad_norm": 9625.417601330344,
      "learning_rate": 2.2418538218670022e-07,
      "loss": 1.7474,
      "step": 199168
    },
    {
      "epoch": 0.0007229363339584942,
      "grad_norm": 9621.025309186127,
      "learning_rate": 2.2416735659725343e-07,
      "loss": 1.7283,
      "step": 199200
    },
    {
      "epoch": 0.0007230524683093309,
      "grad_norm": 11408.458791615984,
      "learning_rate": 2.2414933535514138e-07,
      "loss": 1.7009,
      "step": 199232
    },
    {
      "epoch": 0.0007231686026601676,
      "grad_norm": 10404.782169752523,
      "learning_rate": 2.241313184586169e-07,
      "loss": 1.7335,
      "step": 199264
    },
    {
      "epoch": 0.0007232847370110043,
      "grad_norm": 9908.94908655807,
      "learning_rate": 2.2411330590593378e-07,
      "loss": 1.7241,
      "step": 199296
    },
    {
      "epoch": 0.000723400871361841,
      "grad_norm": 9964.482124024309,
      "learning_rate": 2.2409529769534686e-07,
      "loss": 1.7239,
      "step": 199328
    },
    {
      "epoch": 0.0007235170057126777,
      "grad_norm": 8352.695732516539,
      "learning_rate": 2.2407729382511185e-07,
      "loss": 1.7262,
      "step": 199360
    },
    {
      "epoch": 0.0007236331400635145,
      "grad_norm": 10019.379621513499,
      "learning_rate": 2.2405929429348555e-07,
      "loss": 1.747,
      "step": 199392
    },
    {
      "epoch": 0.0007237492744143511,
      "grad_norm": 10727.300033093135,
      "learning_rate": 2.2404129909872565e-07,
      "loss": 1.7336,
      "step": 199424
    },
    {
      "epoch": 0.0007238654087651879,
      "grad_norm": 9676.028317445129,
      "learning_rate": 2.2402330823909096e-07,
      "loss": 1.7143,
      "step": 199456
    },
    {
      "epoch": 0.0007239815431160245,
      "grad_norm": 8459.953191359868,
      "learning_rate": 2.2400532171284106e-07,
      "loss": 1.7111,
      "step": 199488
    },
    {
      "epoch": 0.0007240976774668613,
      "grad_norm": 9431.485779027607,
      "learning_rate": 2.2398733951823672e-07,
      "loss": 1.7289,
      "step": 199520
    },
    {
      "epoch": 0.0007242138118176979,
      "grad_norm": 9619.857379400175,
      "learning_rate": 2.2396936165353952e-07,
      "loss": 1.7109,
      "step": 199552
    },
    {
      "epoch": 0.0007243299461685347,
      "grad_norm": 9679.890701862289,
      "learning_rate": 2.2395138811701207e-07,
      "loss": 1.6912,
      "step": 199584
    },
    {
      "epoch": 0.0007244460805193713,
      "grad_norm": 16573.515499132947,
      "learning_rate": 2.2393341890691805e-07,
      "loss": 1.731,
      "step": 199616
    },
    {
      "epoch": 0.0007245622148702081,
      "grad_norm": 9112.615102153717,
      "learning_rate": 2.2391601535874612e-07,
      "loss": 1.7123,
      "step": 199648
    },
    {
      "epoch": 0.0007246783492210449,
      "grad_norm": 10006.703353252758,
      "learning_rate": 2.238980546612472e-07,
      "loss": 1.7262,
      "step": 199680
    },
    {
      "epoch": 0.0007247944835718815,
      "grad_norm": 9342.306888558092,
      "learning_rate": 2.238800982850325e-07,
      "loss": 1.7124,
      "step": 199712
    },
    {
      "epoch": 0.0007249106179227183,
      "grad_norm": 8791.148502897673,
      "learning_rate": 2.2386214622836945e-07,
      "loss": 1.7029,
      "step": 199744
    },
    {
      "epoch": 0.0007250267522735549,
      "grad_norm": 9124.565085525994,
      "learning_rate": 2.238441984895265e-07,
      "loss": 1.6991,
      "step": 199776
    },
    {
      "epoch": 0.0007251428866243917,
      "grad_norm": 9429.15754455296,
      "learning_rate": 2.2382625506677304e-07,
      "loss": 1.7121,
      "step": 199808
    },
    {
      "epoch": 0.0007252590209752283,
      "grad_norm": 9694.475952830044,
      "learning_rate": 2.238083159583795e-07,
      "loss": 1.7327,
      "step": 199840
    },
    {
      "epoch": 0.000725375155326065,
      "grad_norm": 10172.14470994195,
      "learning_rate": 2.237903811626172e-07,
      "loss": 1.7431,
      "step": 199872
    },
    {
      "epoch": 0.0007254912896769017,
      "grad_norm": 11559.147200377716,
      "learning_rate": 2.2377245067775849e-07,
      "loss": 1.7233,
      "step": 199904
    },
    {
      "epoch": 0.0007256074240277385,
      "grad_norm": 8418.204084007468,
      "learning_rate": 2.237545245020766e-07,
      "loss": 1.7462,
      "step": 199936
    },
    {
      "epoch": 0.0007257235583785752,
      "grad_norm": 10565.635996001376,
      "learning_rate": 2.2373660263384587e-07,
      "loss": 1.7694,
      "step": 199968
    },
    {
      "epoch": 0.0007258396927294119,
      "grad_norm": 8689.166588344362,
      "learning_rate": 2.2371868507134144e-07,
      "loss": 1.7734,
      "step": 200000
    },
    {
      "epoch": 0.0007259558270802486,
      "grad_norm": 10656.963545025385,
      "learning_rate": 2.2370077181283957e-07,
      "loss": 1.7716,
      "step": 200032
    },
    {
      "epoch": 0.0007260719614310853,
      "grad_norm": 9495.799808336315,
      "learning_rate": 2.2368286285661737e-07,
      "loss": 1.7365,
      "step": 200064
    },
    {
      "epoch": 0.000726188095781922,
      "grad_norm": 9203.177712073151,
      "learning_rate": 2.2366495820095303e-07,
      "loss": 1.7092,
      "step": 200096
    },
    {
      "epoch": 0.0007263042301327587,
      "grad_norm": 8700.88443780286,
      "learning_rate": 2.2364705784412557e-07,
      "loss": 1.7182,
      "step": 200128
    },
    {
      "epoch": 0.0007264203644835954,
      "grad_norm": 8751.381605209546,
      "learning_rate": 2.2362916178441506e-07,
      "loss": 1.7053,
      "step": 200160
    },
    {
      "epoch": 0.000726536498834432,
      "grad_norm": 8637.572112578859,
      "learning_rate": 2.2361127002010253e-07,
      "loss": 1.7293,
      "step": 200192
    },
    {
      "epoch": 0.0007266526331852688,
      "grad_norm": 10498.456648479338,
      "learning_rate": 2.2359338254946995e-07,
      "loss": 1.71,
      "step": 200224
    },
    {
      "epoch": 0.0007267687675361056,
      "grad_norm": 8536.952149332923,
      "learning_rate": 2.235754993708003e-07,
      "loss": 1.6958,
      "step": 200256
    },
    {
      "epoch": 0.0007268849018869422,
      "grad_norm": 9573.93806121598,
      "learning_rate": 2.2355762048237737e-07,
      "loss": 1.7327,
      "step": 200288
    },
    {
      "epoch": 0.000727001036237779,
      "grad_norm": 8071.166458449485,
      "learning_rate": 2.2353974588248612e-07,
      "loss": 1.7287,
      "step": 200320
    },
    {
      "epoch": 0.0007271171705886156,
      "grad_norm": 9848.644170646028,
      "learning_rate": 2.235218755694124e-07,
      "loss": 1.7176,
      "step": 200352
    },
    {
      "epoch": 0.0007272333049394524,
      "grad_norm": 9319.738086448568,
      "learning_rate": 2.235040095414429e-07,
      "loss": 1.7285,
      "step": 200384
    },
    {
      "epoch": 0.000727349439290289,
      "grad_norm": 10681.104250029583,
      "learning_rate": 2.2348614779686538e-07,
      "loss": 1.7346,
      "step": 200416
    },
    {
      "epoch": 0.0007274655736411258,
      "grad_norm": 9802.61903778781,
      "learning_rate": 2.2346829033396857e-07,
      "loss": 1.7171,
      "step": 200448
    },
    {
      "epoch": 0.0007275817079919624,
      "grad_norm": 8282.45845145027,
      "learning_rate": 2.234504371510421e-07,
      "loss": 1.734,
      "step": 200480
    },
    {
      "epoch": 0.0007276978423427992,
      "grad_norm": 10204.53938205934,
      "learning_rate": 2.2343258824637658e-07,
      "loss": 1.7271,
      "step": 200512
    },
    {
      "epoch": 0.0007278139766936359,
      "grad_norm": 9894.364254463244,
      "learning_rate": 2.234147436182636e-07,
      "loss": 1.7407,
      "step": 200544
    },
    {
      "epoch": 0.0007279301110444726,
      "grad_norm": 9810.54259457651,
      "learning_rate": 2.2339690326499568e-07,
      "loss": 1.7397,
      "step": 200576
    },
    {
      "epoch": 0.0007280462453953093,
      "grad_norm": 9482.893545748577,
      "learning_rate": 2.2337906718486625e-07,
      "loss": 1.748,
      "step": 200608
    },
    {
      "epoch": 0.000728162379746146,
      "grad_norm": 10882.980290343265,
      "learning_rate": 2.2336179255555297e-07,
      "loss": 1.7357,
      "step": 200640
    },
    {
      "epoch": 0.0007282785140969827,
      "grad_norm": 10417.008399727822,
      "learning_rate": 2.2334396488318163e-07,
      "loss": 1.7343,
      "step": 200672
    },
    {
      "epoch": 0.0007283946484478194,
      "grad_norm": 10654.996762083038,
      "learning_rate": 2.2332614147888816e-07,
      "loss": 1.7448,
      "step": 200704
    },
    {
      "epoch": 0.0007285107827986561,
      "grad_norm": 8726.83344633092,
      "learning_rate": 2.233083223409698e-07,
      "loss": 1.7362,
      "step": 200736
    },
    {
      "epoch": 0.0007286269171494928,
      "grad_norm": 10547.06859748243,
      "learning_rate": 2.2329050746772484e-07,
      "loss": 1.7547,
      "step": 200768
    },
    {
      "epoch": 0.0007287430515003295,
      "grad_norm": 9635.353340692805,
      "learning_rate": 2.2327269685745235e-07,
      "loss": 1.7652,
      "step": 200800
    },
    {
      "epoch": 0.0007288591858511663,
      "grad_norm": 9740.617331565798,
      "learning_rate": 2.2325489050845249e-07,
      "loss": 1.7621,
      "step": 200832
    },
    {
      "epoch": 0.0007289753202020029,
      "grad_norm": 9583.444996450911,
      "learning_rate": 2.232370884190263e-07,
      "loss": 1.7515,
      "step": 200864
    },
    {
      "epoch": 0.0007290914545528397,
      "grad_norm": 9060.083884821377,
      "learning_rate": 2.2321929058747577e-07,
      "loss": 1.7592,
      "step": 200896
    },
    {
      "epoch": 0.0007292075889036763,
      "grad_norm": 9300.16311684908,
      "learning_rate": 2.2320149701210388e-07,
      "loss": 1.7429,
      "step": 200928
    },
    {
      "epoch": 0.0007293237232545131,
      "grad_norm": 10226.61058220171,
      "learning_rate": 2.2318370769121448e-07,
      "loss": 1.7478,
      "step": 200960
    },
    {
      "epoch": 0.0007294398576053497,
      "grad_norm": 9886.221017153117,
      "learning_rate": 2.2316592262311247e-07,
      "loss": 1.74,
      "step": 200992
    },
    {
      "epoch": 0.0007295559919561865,
      "grad_norm": 9812.607808324961,
      "learning_rate": 2.231481418061036e-07,
      "loss": 1.7216,
      "step": 201024
    },
    {
      "epoch": 0.0007296721263070231,
      "grad_norm": 10517.791973603586,
      "learning_rate": 2.231303652384946e-07,
      "loss": 1.7343,
      "step": 201056
    },
    {
      "epoch": 0.0007297882606578599,
      "grad_norm": 10345.823311849086,
      "learning_rate": 2.2311259291859317e-07,
      "loss": 1.7208,
      "step": 201088
    },
    {
      "epoch": 0.0007299043950086966,
      "grad_norm": 9969.120522894686,
      "learning_rate": 2.2309482484470794e-07,
      "loss": 1.7375,
      "step": 201120
    },
    {
      "epoch": 0.0007300205293595333,
      "grad_norm": 9032.416730864448,
      "learning_rate": 2.2307706101514843e-07,
      "loss": 1.7458,
      "step": 201152
    },
    {
      "epoch": 0.00073013666371037,
      "grad_norm": 10225.83923206306,
      "learning_rate": 2.230593014282252e-07,
      "loss": 1.7573,
      "step": 201184
    },
    {
      "epoch": 0.0007302527980612067,
      "grad_norm": 10211.222062025681,
      "learning_rate": 2.2304154608224964e-07,
      "loss": 1.7441,
      "step": 201216
    },
    {
      "epoch": 0.0007303689324120434,
      "grad_norm": 8969.037518039491,
      "learning_rate": 2.2302379497553422e-07,
      "loss": 1.7351,
      "step": 201248
    },
    {
      "epoch": 0.0007304850667628801,
      "grad_norm": 9139.127091796021,
      "learning_rate": 2.2300604810639218e-07,
      "loss": 1.7331,
      "step": 201280
    },
    {
      "epoch": 0.0007306012011137168,
      "grad_norm": 9764.128737373345,
      "learning_rate": 2.2298830547313781e-07,
      "loss": 1.7205,
      "step": 201312
    },
    {
      "epoch": 0.0007307173354645535,
      "grad_norm": 10496.358606678794,
      "learning_rate": 2.2297056707408635e-07,
      "loss": 1.7233,
      "step": 201344
    },
    {
      "epoch": 0.0007308334698153902,
      "grad_norm": 10756.375039947241,
      "learning_rate": 2.2295283290755395e-07,
      "loss": 1.7248,
      "step": 201376
    },
    {
      "epoch": 0.000730949604166227,
      "grad_norm": 10127.826025362008,
      "learning_rate": 2.2293510297185763e-07,
      "loss": 1.7288,
      "step": 201408
    },
    {
      "epoch": 0.0007310657385170636,
      "grad_norm": 10815.480202006753,
      "learning_rate": 2.2291737726531547e-07,
      "loss": 1.7225,
      "step": 201440
    },
    {
      "epoch": 0.0007311818728679004,
      "grad_norm": 8937.493720277515,
      "learning_rate": 2.228996557862464e-07,
      "loss": 1.7223,
      "step": 201472
    },
    {
      "epoch": 0.000731298007218737,
      "grad_norm": 9602.037804549616,
      "learning_rate": 2.2288193853297027e-07,
      "loss": 1.7311,
      "step": 201504
    },
    {
      "epoch": 0.0007314141415695738,
      "grad_norm": 10776.919225827016,
      "learning_rate": 2.2286422550380797e-07,
      "loss": 1.7318,
      "step": 201536
    },
    {
      "epoch": 0.0007315302759204104,
      "grad_norm": 10011.853374875203,
      "learning_rate": 2.2284651669708125e-07,
      "loss": 1.7407,
      "step": 201568
    },
    {
      "epoch": 0.0007316464102712472,
      "grad_norm": 9871.214818855884,
      "learning_rate": 2.2282881211111275e-07,
      "loss": 1.7349,
      "step": 201600
    },
    {
      "epoch": 0.0007317625446220838,
      "grad_norm": 9906.481716532868,
      "learning_rate": 2.2281111174422615e-07,
      "loss": 1.7548,
      "step": 201632
    },
    {
      "epoch": 0.0007318786789729206,
      "grad_norm": 10813.383929187015,
      "learning_rate": 2.2279396853559616e-07,
      "loss": 1.7516,
      "step": 201664
    },
    {
      "epoch": 0.0007319948133237573,
      "grad_norm": 10361.235061516556,
      "learning_rate": 2.227762764701316e-07,
      "loss": 1.7655,
      "step": 201696
    },
    {
      "epoch": 0.000732110947674594,
      "grad_norm": 9942.734633892227,
      "learning_rate": 2.227585886187777e-07,
      "loss": 1.7644,
      "step": 201728
    },
    {
      "epoch": 0.0007322270820254307,
      "grad_norm": 9586.873212888548,
      "learning_rate": 2.227409049798617e-07,
      "loss": 1.78,
      "step": 201760
    },
    {
      "epoch": 0.0007323432163762674,
      "grad_norm": 10155.935210506219,
      "learning_rate": 2.2272322555171192e-07,
      "loss": 1.7758,
      "step": 201792
    },
    {
      "epoch": 0.0007324593507271041,
      "grad_norm": 10778.751690246881,
      "learning_rate": 2.227055503326575e-07,
      "loss": 1.7487,
      "step": 201824
    },
    {
      "epoch": 0.0007325754850779408,
      "grad_norm": 9899.411901724265,
      "learning_rate": 2.2268787932102856e-07,
      "loss": 1.7347,
      "step": 201856
    },
    {
      "epoch": 0.0007326916194287775,
      "grad_norm": 9310.768281941077,
      "learning_rate": 2.2267021251515612e-07,
      "loss": 1.7246,
      "step": 201888
    },
    {
      "epoch": 0.0007328077537796142,
      "grad_norm": 8630.56371275944,
      "learning_rate": 2.2265254991337212e-07,
      "loss": 1.7272,
      "step": 201920
    },
    {
      "epoch": 0.0007329238881304509,
      "grad_norm": 9291.717602252018,
      "learning_rate": 2.2263489151400949e-07,
      "loss": 1.7233,
      "step": 201952
    },
    {
      "epoch": 0.0007330400224812877,
      "grad_norm": 12314.167775371587,
      "learning_rate": 2.22617237315402e-07,
      "loss": 1.7278,
      "step": 201984
    },
    {
      "epoch": 0.0007331561568321243,
      "grad_norm": 9507.277738658948,
      "learning_rate": 2.2259958731588443e-07,
      "loss": 1.727,
      "step": 202016
    },
    {
      "epoch": 0.0007332722911829611,
      "grad_norm": 9391.981473576276,
      "learning_rate": 2.2258194151379242e-07,
      "loss": 1.7377,
      "step": 202048
    },
    {
      "epoch": 0.0007333884255337977,
      "grad_norm": 10356.673597251194,
      "learning_rate": 2.2256429990746256e-07,
      "loss": 1.7375,
      "step": 202080
    },
    {
      "epoch": 0.0007335045598846345,
      "grad_norm": 9001.572529286203,
      "learning_rate": 2.2254666249523227e-07,
      "loss": 1.7347,
      "step": 202112
    },
    {
      "epoch": 0.0007336206942354711,
      "grad_norm": 9145.527868854811,
      "learning_rate": 2.225290292754401e-07,
      "loss": 1.7299,
      "step": 202144
    },
    {
      "epoch": 0.0007337368285863079,
      "grad_norm": 9431.704617936251,
      "learning_rate": 2.2251140024642536e-07,
      "loss": 1.713,
      "step": 202176
    },
    {
      "epoch": 0.0007338529629371445,
      "grad_norm": 8683.602132755737,
      "learning_rate": 2.224937754065283e-07,
      "loss": 1.7284,
      "step": 202208
    },
    {
      "epoch": 0.0007339690972879813,
      "grad_norm": 10721.871851500558,
      "learning_rate": 2.224761547540901e-07,
      "loss": 1.7207,
      "step": 202240
    },
    {
      "epoch": 0.000734085231638818,
      "grad_norm": 9573.305489745953,
      "learning_rate": 2.2245853828745292e-07,
      "loss": 1.7251,
      "step": 202272
    },
    {
      "epoch": 0.0007342013659896547,
      "grad_norm": 9229.499661411772,
      "learning_rate": 2.2244092600495972e-07,
      "loss": 1.7353,
      "step": 202304
    },
    {
      "epoch": 0.0007343175003404914,
      "grad_norm": 8843.856398653248,
      "learning_rate": 2.2242331790495453e-07,
      "loss": 1.7413,
      "step": 202336
    },
    {
      "epoch": 0.0007344336346913281,
      "grad_norm": 10304.703198054758,
      "learning_rate": 2.2240571398578214e-07,
      "loss": 1.7524,
      "step": 202368
    },
    {
      "epoch": 0.0007345497690421648,
      "grad_norm": 10559.466842601476,
      "learning_rate": 2.2238811424578832e-07,
      "loss": 1.744,
      "step": 202400
    },
    {
      "epoch": 0.0007346659033930015,
      "grad_norm": 9712.462200698648,
      "learning_rate": 2.2237051868331985e-07,
      "loss": 1.7374,
      "step": 202432
    },
    {
      "epoch": 0.0007347820377438382,
      "grad_norm": 8212.113856979822,
      "learning_rate": 2.223529272967243e-07,
      "loss": 1.7387,
      "step": 202464
    },
    {
      "epoch": 0.0007348981720946749,
      "grad_norm": 10258.453879605835,
      "learning_rate": 2.223353400843502e-07,
      "loss": 1.7433,
      "step": 202496
    },
    {
      "epoch": 0.0007350143064455116,
      "grad_norm": 9898.931659527709,
      "learning_rate": 2.2231775704454693e-07,
      "loss": 1.7422,
      "step": 202528
    },
    {
      "epoch": 0.0007351304407963484,
      "grad_norm": 9303.116789549618,
      "learning_rate": 2.2230017817566493e-07,
      "loss": 1.7705,
      "step": 202560
    },
    {
      "epoch": 0.000735246575147185,
      "grad_norm": 9075.27575338623,
      "learning_rate": 2.2228260347605543e-07,
      "loss": 1.7545,
      "step": 202592
    },
    {
      "epoch": 0.0007353627094980218,
      "grad_norm": 9164.664096408553,
      "learning_rate": 2.2226503294407063e-07,
      "loss": 1.7533,
      "step": 202624
    },
    {
      "epoch": 0.0007354788438488584,
      "grad_norm": 17858.853490635953,
      "learning_rate": 2.2224801546395843e-07,
      "loss": 1.7567,
      "step": 202656
    },
    {
      "epoch": 0.0007355949781996952,
      "grad_norm": 9715.180080677866,
      "learning_rate": 2.2223045313217272e-07,
      "loss": 1.755,
      "step": 202688
    },
    {
      "epoch": 0.0007357111125505318,
      "grad_norm": 10291.580247950264,
      "learning_rate": 2.222128949631251e-07,
      "loss": 1.7343,
      "step": 202720
    },
    {
      "epoch": 0.0007358272469013686,
      "grad_norm": 11965.309523785834,
      "learning_rate": 2.2219534095517137e-07,
      "loss": 1.7185,
      "step": 202752
    },
    {
      "epoch": 0.0007359433812522052,
      "grad_norm": 9422.343869759796,
      "learning_rate": 2.2217779110666824e-07,
      "loss": 1.7228,
      "step": 202784
    },
    {
      "epoch": 0.000736059515603042,
      "grad_norm": 9211.924771729304,
      "learning_rate": 2.221602454159733e-07,
      "loss": 1.7238,
      "step": 202816
    },
    {
      "epoch": 0.0007361756499538787,
      "grad_norm": 9536.64049862424,
      "learning_rate": 2.2214270388144508e-07,
      "loss": 1.7268,
      "step": 202848
    },
    {
      "epoch": 0.0007362917843047154,
      "grad_norm": 10501.656821663904,
      "learning_rate": 2.2212516650144298e-07,
      "loss": 1.7337,
      "step": 202880
    },
    {
      "epoch": 0.0007364079186555521,
      "grad_norm": 8676.641055154927,
      "learning_rate": 2.2210763327432738e-07,
      "loss": 1.7493,
      "step": 202912
    },
    {
      "epoch": 0.0007365240530063888,
      "grad_norm": 8787.763310422057,
      "learning_rate": 2.2209010419845948e-07,
      "loss": 1.7701,
      "step": 202944
    },
    {
      "epoch": 0.0007366401873572255,
      "grad_norm": 10163.340002184323,
      "learning_rate": 2.2207257927220146e-07,
      "loss": 1.7536,
      "step": 202976
    },
    {
      "epoch": 0.0007367563217080622,
      "grad_norm": 9855.18726356836,
      "learning_rate": 2.2205505849391638e-07,
      "loss": 1.7223,
      "step": 203008
    },
    {
      "epoch": 0.0007368724560588989,
      "grad_norm": 9858.490148090628,
      "learning_rate": 2.2203754186196815e-07,
      "loss": 1.7141,
      "step": 203040
    },
    {
      "epoch": 0.0007369885904097356,
      "grad_norm": 9973.151558058265,
      "learning_rate": 2.2202002937472163e-07,
      "loss": 1.7198,
      "step": 203072
    },
    {
      "epoch": 0.0007371047247605723,
      "grad_norm": 9019.641123681142,
      "learning_rate": 2.220025210305426e-07,
      "loss": 1.7203,
      "step": 203104
    },
    {
      "epoch": 0.0007372208591114091,
      "grad_norm": 9705.249713428295,
      "learning_rate": 2.2198501682779767e-07,
      "loss": 1.7232,
      "step": 203136
    },
    {
      "epoch": 0.0007373369934622457,
      "grad_norm": 9008.58246340677,
      "learning_rate": 2.2196751676485446e-07,
      "loss": 1.727,
      "step": 203168
    },
    {
      "epoch": 0.0007374531278130825,
      "grad_norm": 8979.099509416299,
      "learning_rate": 2.2195002084008142e-07,
      "loss": 1.7266,
      "step": 203200
    },
    {
      "epoch": 0.0007375692621639191,
      "grad_norm": 10198.529501844861,
      "learning_rate": 2.219325290518479e-07,
      "loss": 1.728,
      "step": 203232
    },
    {
      "epoch": 0.0007376853965147559,
      "grad_norm": 9080.002533039295,
      "learning_rate": 2.219150413985242e-07,
      "loss": 1.7294,
      "step": 203264
    },
    {
      "epoch": 0.0007378015308655925,
      "grad_norm": 9198.512923293634,
      "learning_rate": 2.2189755787848143e-07,
      "loss": 1.7392,
      "step": 203296
    },
    {
      "epoch": 0.0007379176652164293,
      "grad_norm": 8970.52127805291,
      "learning_rate": 2.2188007849009164e-07,
      "loss": 1.7343,
      "step": 203328
    },
    {
      "epoch": 0.0007380337995672659,
      "grad_norm": 12232.893034764917,
      "learning_rate": 2.2186260323172785e-07,
      "loss": 1.7423,
      "step": 203360
    },
    {
      "epoch": 0.0007381499339181027,
      "grad_norm": 9132.31208402341,
      "learning_rate": 2.218451321017639e-07,
      "loss": 1.7477,
      "step": 203392
    },
    {
      "epoch": 0.0007382660682689394,
      "grad_norm": 9159.582850763456,
      "learning_rate": 2.218276650985745e-07,
      "loss": 1.7497,
      "step": 203424
    },
    {
      "epoch": 0.0007383822026197761,
      "grad_norm": 10032.352665252552,
      "learning_rate": 2.218102022205353e-07,
      "loss": 1.7678,
      "step": 203456
    },
    {
      "epoch": 0.0007384983369706128,
      "grad_norm": 9624.65417560548,
      "learning_rate": 2.2179274346602284e-07,
      "loss": 1.762,
      "step": 203488
    },
    {
      "epoch": 0.0007386144713214495,
      "grad_norm": 10018.452275676118,
      "learning_rate": 2.217752888334146e-07,
      "loss": 1.781,
      "step": 203520
    },
    {
      "epoch": 0.0007387306056722862,
      "grad_norm": 10298.336758914034,
      "learning_rate": 2.2175783832108884e-07,
      "loss": 1.7841,
      "step": 203552
    },
    {
      "epoch": 0.0007388467400231229,
      "grad_norm": 9476.74226725619,
      "learning_rate": 2.2174039192742486e-07,
      "loss": 1.7292,
      "step": 203584
    },
    {
      "epoch": 0.0007389628743739596,
      "grad_norm": 10128.148695590917,
      "learning_rate": 2.217229496508027e-07,
      "loss": 1.7128,
      "step": 203616
    },
    {
      "epoch": 0.0007390790087247963,
      "grad_norm": 11290.337993169203,
      "learning_rate": 2.2170551148960343e-07,
      "loss": 1.7236,
      "step": 203648
    },
    {
      "epoch": 0.000739195143075633,
      "grad_norm": 11443.273133155566,
      "learning_rate": 2.2168862219393653e-07,
      "loss": 1.7216,
      "step": 203680
    },
    {
      "epoch": 0.0007393112774264698,
      "grad_norm": 8795.791038900366,
      "learning_rate": 2.2167119213024816e-07,
      "loss": 1.7282,
      "step": 203712
    },
    {
      "epoch": 0.0007394274117773064,
      "grad_norm": 10058.07536261287,
      "learning_rate": 2.2165376617718148e-07,
      "loss": 1.7341,
      "step": 203744
    },
    {
      "epoch": 0.0007395435461281432,
      "grad_norm": 9065.491933701116,
      "learning_rate": 2.2163634433312108e-07,
      "loss": 1.7256,
      "step": 203776
    },
    {
      "epoch": 0.0007396596804789798,
      "grad_norm": 8790.55959538413,
      "learning_rate": 2.2161892659645238e-07,
      "loss": 1.749,
      "step": 203808
    },
    {
      "epoch": 0.0007397758148298166,
      "grad_norm": 11897.890065049349,
      "learning_rate": 2.2160151296556166e-07,
      "loss": 1.7379,
      "step": 203840
    },
    {
      "epoch": 0.0007398919491806532,
      "grad_norm": 8017.493748048701,
      "learning_rate": 2.215841034388361e-07,
      "loss": 1.7158,
      "step": 203872
    },
    {
      "epoch": 0.00074000808353149,
      "grad_norm": 9411.367594563502,
      "learning_rate": 2.2156669801466387e-07,
      "loss": 1.7249,
      "step": 203904
    },
    {
      "epoch": 0.0007401242178823266,
      "grad_norm": 9359.611316716095,
      "learning_rate": 2.2154929669143393e-07,
      "loss": 1.7151,
      "step": 203936
    },
    {
      "epoch": 0.0007402403522331634,
      "grad_norm": 10849.245319375907,
      "learning_rate": 2.215318994675361e-07,
      "loss": 1.7234,
      "step": 203968
    },
    {
      "epoch": 0.0007403564865840002,
      "grad_norm": 9700.56225174603,
      "learning_rate": 2.2151450634136112e-07,
      "loss": 1.7164,
      "step": 204000
    },
    {
      "epoch": 0.0007404726209348368,
      "grad_norm": 11519.527247244134,
      "learning_rate": 2.2149711731130066e-07,
      "loss": 1.7245,
      "step": 204032
    },
    {
      "epoch": 0.0007405887552856736,
      "grad_norm": 10930.868035064734,
      "learning_rate": 2.2147973237574723e-07,
      "loss": 1.7343,
      "step": 204064
    },
    {
      "epoch": 0.0007407048896365102,
      "grad_norm": 8970.908315215354,
      "learning_rate": 2.2146235153309422e-07,
      "loss": 1.753,
      "step": 204096
    },
    {
      "epoch": 0.000740821023987347,
      "grad_norm": 9134.252897747028,
      "learning_rate": 2.2144497478173589e-07,
      "loss": 1.7468,
      "step": 204128
    },
    {
      "epoch": 0.0007409371583381836,
      "grad_norm": 11244.485403965804,
      "learning_rate": 2.2142760212006742e-07,
      "loss": 1.7466,
      "step": 204160
    },
    {
      "epoch": 0.0007410532926890204,
      "grad_norm": 9289.375759436152,
      "learning_rate": 2.2141023354648487e-07,
      "loss": 1.7353,
      "step": 204192
    },
    {
      "epoch": 0.000741169427039857,
      "grad_norm": 10205.43815815862,
      "learning_rate": 2.213928690593851e-07,
      "loss": 1.7356,
      "step": 204224
    },
    {
      "epoch": 0.0007412855613906938,
      "grad_norm": 8722.242372234332,
      "learning_rate": 2.21375508657166e-07,
      "loss": 1.7459,
      "step": 204256
    },
    {
      "epoch": 0.0007414016957415305,
      "grad_norm": 9970.189366305938,
      "learning_rate": 2.2135815233822622e-07,
      "loss": 1.7415,
      "step": 204288
    },
    {
      "epoch": 0.0007415178300923672,
      "grad_norm": 11082.354984388472,
      "learning_rate": 2.2134080010096528e-07,
      "loss": 1.7729,
      "step": 204320
    },
    {
      "epoch": 0.0007416339644432039,
      "grad_norm": 11103.568435417508,
      "learning_rate": 2.213234519437837e-07,
      "loss": 1.7526,
      "step": 204352
    },
    {
      "epoch": 0.0007417500987940406,
      "grad_norm": 10070.766008601331,
      "learning_rate": 2.2130610786508272e-07,
      "loss": 1.7621,
      "step": 204384
    },
    {
      "epoch": 0.0007418662331448773,
      "grad_norm": 10281.9784088472,
      "learning_rate": 2.2128876786326453e-07,
      "loss": 1.77,
      "step": 204416
    },
    {
      "epoch": 0.000741982367495714,
      "grad_norm": 8540.735214254099,
      "learning_rate": 2.2127143193673225e-07,
      "loss": 1.7236,
      "step": 204448
    },
    {
      "epoch": 0.0007420985018465507,
      "grad_norm": 9648.406396913431,
      "learning_rate": 2.2125410008388985e-07,
      "loss": 1.7222,
      "step": 204480
    },
    {
      "epoch": 0.0007422146361973874,
      "grad_norm": 8697.407889710588,
      "learning_rate": 2.2123677230314206e-07,
      "loss": 1.7188,
      "step": 204512
    },
    {
      "epoch": 0.0007423307705482241,
      "grad_norm": 9772.868770222998,
      "learning_rate": 2.2121944859289464e-07,
      "loss": 1.7252,
      "step": 204544
    },
    {
      "epoch": 0.0007424469048990609,
      "grad_norm": 11044.052698171989,
      "learning_rate": 2.2120212895155413e-07,
      "loss": 1.7233,
      "step": 204576
    },
    {
      "epoch": 0.0007425630392498975,
      "grad_norm": 8809.218580555258,
      "learning_rate": 2.2118481337752797e-07,
      "loss": 1.7224,
      "step": 204608
    },
    {
      "epoch": 0.0007426791736007343,
      "grad_norm": 9518.311404865886,
      "learning_rate": 2.2116750186922452e-07,
      "loss": 1.7307,
      "step": 204640
    },
    {
      "epoch": 0.0007427953079515709,
      "grad_norm": 9767.30740787859,
      "learning_rate": 2.2115073522118146e-07,
      "loss": 1.7561,
      "step": 204672
    },
    {
      "epoch": 0.0007429114423024077,
      "grad_norm": 10420.786726538452,
      "learning_rate": 2.2113343171262142e-07,
      "loss": 1.777,
      "step": 204704
    },
    {
      "epoch": 0.0007430275766532443,
      "grad_norm": 10597.83808142019,
      "learning_rate": 2.2111613226506386e-07,
      "loss": 1.7319,
      "step": 204736
    },
    {
      "epoch": 0.0007431437110040811,
      "grad_norm": 9514.507238948321,
      "learning_rate": 2.2109883687692056e-07,
      "loss": 1.7248,
      "step": 204768
    },
    {
      "epoch": 0.0007432598453549177,
      "grad_norm": 8689.3471561447,
      "learning_rate": 2.2108154554660416e-07,
      "loss": 1.7156,
      "step": 204800
    },
    {
      "epoch": 0.0007433759797057545,
      "grad_norm": 10005.521675554954,
      "learning_rate": 2.2106425827252818e-07,
      "loss": 1.724,
      "step": 204832
    },
    {
      "epoch": 0.0007434921140565912,
      "grad_norm": 9357.993588371388,
      "learning_rate": 2.2104697505310698e-07,
      "loss": 1.7114,
      "step": 204864
    },
    {
      "epoch": 0.0007436082484074279,
      "grad_norm": 11252.257551264991,
      "learning_rate": 2.2102969588675582e-07,
      "loss": 1.7259,
      "step": 204896
    },
    {
      "epoch": 0.0007437243827582646,
      "grad_norm": 10614.835844232355,
      "learning_rate": 2.2101242077189084e-07,
      "loss": 1.7321,
      "step": 204928
    },
    {
      "epoch": 0.0007438405171091013,
      "grad_norm": 9721.146845923067,
      "learning_rate": 2.2099514970692897e-07,
      "loss": 1.7288,
      "step": 204960
    },
    {
      "epoch": 0.000743956651459938,
      "grad_norm": 8453.58894198198,
      "learning_rate": 2.2097788269028805e-07,
      "loss": 1.7298,
      "step": 204992
    },
    {
      "epoch": 0.0007440727858107747,
      "grad_norm": 9983.685892494816,
      "learning_rate": 2.2096061972038681e-07,
      "loss": 1.7277,
      "step": 205024
    },
    {
      "epoch": 0.0007441889201616114,
      "grad_norm": 11081.601328327959,
      "learning_rate": 2.2094336079564487e-07,
      "loss": 1.7373,
      "step": 205056
    },
    {
      "epoch": 0.0007443050545124481,
      "grad_norm": 10248.880524232878,
      "learning_rate": 2.209261059144826e-07,
      "loss": 1.7335,
      "step": 205088
    },
    {
      "epoch": 0.0007444211888632848,
      "grad_norm": 10287.501737545419,
      "learning_rate": 2.209088550753213e-07,
      "loss": 1.7444,
      "step": 205120
    },
    {
      "epoch": 0.0007445373232141216,
      "grad_norm": 9122.418538962133,
      "learning_rate": 2.208916082765832e-07,
      "loss": 1.7382,
      "step": 205152
    },
    {
      "epoch": 0.0007446534575649582,
      "grad_norm": 12858.027220378715,
      "learning_rate": 2.2087436551669126e-07,
      "loss": 1.7515,
      "step": 205184
    },
    {
      "epoch": 0.000744769591915795,
      "grad_norm": 10215.125256207091,
      "learning_rate": 2.208571267940694e-07,
      "loss": 1.7634,
      "step": 205216
    },
    {
      "epoch": 0.0007448857262666316,
      "grad_norm": 10875.76213421386,
      "learning_rate": 2.2083989210714238e-07,
      "loss": 1.7697,
      "step": 205248
    },
    {
      "epoch": 0.0007450018606174684,
      "grad_norm": 10889.087565080923,
      "learning_rate": 2.2082266145433575e-07,
      "loss": 1.7862,
      "step": 205280
    },
    {
      "epoch": 0.000745117994968305,
      "grad_norm": 10450.50084924163,
      "learning_rate": 2.2080543483407605e-07,
      "loss": 1.754,
      "step": 205312
    },
    {
      "epoch": 0.0007452341293191418,
      "grad_norm": 10020.529327335957,
      "learning_rate": 2.2078821224479057e-07,
      "loss": 1.7398,
      "step": 205344
    },
    {
      "epoch": 0.0007453502636699784,
      "grad_norm": 9449.561471306486,
      "learning_rate": 2.2077099368490752e-07,
      "loss": 1.7093,
      "step": 205376
    },
    {
      "epoch": 0.0007454663980208152,
      "grad_norm": 10660.096059604717,
      "learning_rate": 2.2075377915285592e-07,
      "loss": 1.7241,
      "step": 205408
    },
    {
      "epoch": 0.0007455825323716519,
      "grad_norm": 9461.487832259787,
      "learning_rate": 2.2073656864706567e-07,
      "loss": 1.7293,
      "step": 205440
    },
    {
      "epoch": 0.0007456986667224886,
      "grad_norm": 9534.663496946287,
      "learning_rate": 2.2071936216596755e-07,
      "loss": 1.716,
      "step": 205472
    },
    {
      "epoch": 0.0007458148010733253,
      "grad_norm": 9817.358198619422,
      "learning_rate": 2.207021597079932e-07,
      "loss": 1.7398,
      "step": 205504
    },
    {
      "epoch": 0.000745930935424162,
      "grad_norm": 9980.068737238236,
      "learning_rate": 2.20684961271575e-07,
      "loss": 1.7394,
      "step": 205536
    },
    {
      "epoch": 0.0007460470697749987,
      "grad_norm": 9212.730539856248,
      "learning_rate": 2.2066776685514638e-07,
      "loss": 1.7511,
      "step": 205568
    },
    {
      "epoch": 0.0007461632041258354,
      "grad_norm": 9813.437114487462,
      "learning_rate": 2.2065057645714144e-07,
      "loss": 1.7128,
      "step": 205600
    },
    {
      "epoch": 0.0007462793384766721,
      "grad_norm": 9715.883799222796,
      "learning_rate": 2.2063339007599527e-07,
      "loss": 1.7247,
      "step": 205632
    },
    {
      "epoch": 0.0007463954728275088,
      "grad_norm": 9226.019943615991,
      "learning_rate": 2.206162077101437e-07,
      "loss": 1.7144,
      "step": 205664
    },
    {
      "epoch": 0.0007465116071783455,
      "grad_norm": 8854.652788223828,
      "learning_rate": 2.2059956612078805e-07,
      "loss": 1.7185,
      "step": 205696
    },
    {
      "epoch": 0.0007466277415291823,
      "grad_norm": 10858.58296464138,
      "learning_rate": 2.205823916554802e-07,
      "loss": 1.712,
      "step": 205728
    },
    {
      "epoch": 0.0007467438758800189,
      "grad_norm": 9625.97839183114,
      "learning_rate": 2.2056522120082852e-07,
      "loss": 1.721,
      "step": 205760
    },
    {
      "epoch": 0.0007468600102308557,
      "grad_norm": 10063.94773436349,
      "learning_rate": 2.205480547552723e-07,
      "loss": 1.7313,
      "step": 205792
    },
    {
      "epoch": 0.0007469761445816923,
      "grad_norm": 10545.025936430882,
      "learning_rate": 2.2053089231725162e-07,
      "loss": 1.7443,
      "step": 205824
    },
    {
      "epoch": 0.0007470922789325291,
      "grad_norm": 10157.665676719233,
      "learning_rate": 2.205137338852075e-07,
      "loss": 1.7468,
      "step": 205856
    },
    {
      "epoch": 0.0007472084132833657,
      "grad_norm": 9870.486816768462,
      "learning_rate": 2.2049657945758176e-07,
      "loss": 1.7426,
      "step": 205888
    },
    {
      "epoch": 0.0007473245476342025,
      "grad_norm": 8870.816986050382,
      "learning_rate": 2.2047942903281701e-07,
      "loss": 1.7562,
      "step": 205920
    },
    {
      "epoch": 0.0007474406819850391,
      "grad_norm": 9807.841964469044,
      "learning_rate": 2.2046228260935677e-07,
      "loss": 1.7308,
      "step": 205952
    },
    {
      "epoch": 0.0007475568163358759,
      "grad_norm": 9630.974197868043,
      "learning_rate": 2.2044514018564545e-07,
      "loss": 1.7354,
      "step": 205984
    },
    {
      "epoch": 0.0007476729506867126,
      "grad_norm": 9478.569406825061,
      "learning_rate": 2.2042800176012818e-07,
      "loss": 1.7468,
      "step": 206016
    },
    {
      "epoch": 0.0007477890850375493,
      "grad_norm": 11588.638315177499,
      "learning_rate": 2.204108673312511e-07,
      "loss": 1.7435,
      "step": 206048
    },
    {
      "epoch": 0.000747905219388386,
      "grad_norm": 8988.395407412827,
      "learning_rate": 2.2039373689746103e-07,
      "loss": 1.7644,
      "step": 206080
    },
    {
      "epoch": 0.0007480213537392227,
      "grad_norm": 9121.249585446065,
      "learning_rate": 2.2037661045720573e-07,
      "loss": 1.7661,
      "step": 206112
    },
    {
      "epoch": 0.0007481374880900594,
      "grad_norm": 9983.333711741785,
      "learning_rate": 2.203594880089338e-07,
      "loss": 1.7602,
      "step": 206144
    },
    {
      "epoch": 0.0007482536224408961,
      "grad_norm": 9031.558558742781,
      "learning_rate": 2.2034236955109466e-07,
      "loss": 1.7337,
      "step": 206176
    },
    {
      "epoch": 0.0007483697567917328,
      "grad_norm": 9346.417923461371,
      "learning_rate": 2.2032525508213862e-07,
      "loss": 1.7246,
      "step": 206208
    },
    {
      "epoch": 0.0007484858911425695,
      "grad_norm": 9884.264666630492,
      "learning_rate": 2.2030814460051675e-07,
      "loss": 1.7106,
      "step": 206240
    },
    {
      "epoch": 0.0007486020254934062,
      "grad_norm": 10145.049334527654,
      "learning_rate": 2.20291038104681e-07,
      "loss": 1.722,
      "step": 206272
    },
    {
      "epoch": 0.000748718159844243,
      "grad_norm": 11986.788060193607,
      "learning_rate": 2.202739355930842e-07,
      "loss": 1.728,
      "step": 206304
    },
    {
      "epoch": 0.0007488342941950796,
      "grad_norm": 9027.29117731338,
      "learning_rate": 2.2025683706417995e-07,
      "loss": 1.7195,
      "step": 206336
    },
    {
      "epoch": 0.0007489504285459164,
      "grad_norm": 10208.391450174704,
      "learning_rate": 2.202397425164228e-07,
      "loss": 1.7293,
      "step": 206368
    },
    {
      "epoch": 0.000749066562896753,
      "grad_norm": 9193.766910249575,
      "learning_rate": 2.2022265194826797e-07,
      "loss": 1.7401,
      "step": 206400
    },
    {
      "epoch": 0.0007491826972475898,
      "grad_norm": 9976.965670984338,
      "learning_rate": 2.2020556535817166e-07,
      "loss": 1.7597,
      "step": 206432
    },
    {
      "epoch": 0.0007492988315984264,
      "grad_norm": 8571.251833892176,
      "learning_rate": 2.2018848274459092e-07,
      "loss": 1.7418,
      "step": 206464
    },
    {
      "epoch": 0.0007494149659492632,
      "grad_norm": 9358.546575189974,
      "learning_rate": 2.2017140410598352e-07,
      "loss": 1.7338,
      "step": 206496
    },
    {
      "epoch": 0.0007495311003000998,
      "grad_norm": 9582.76139742611,
      "learning_rate": 2.2015432944080812e-07,
      "loss": 1.7278,
      "step": 206528
    },
    {
      "epoch": 0.0007496472346509366,
      "grad_norm": 10034.277054177845,
      "learning_rate": 2.201372587475243e-07,
      "loss": 1.7137,
      "step": 206560
    },
    {
      "epoch": 0.0007497633690017733,
      "grad_norm": 11041.325463910573,
      "learning_rate": 2.2012019202459233e-07,
      "loss": 1.716,
      "step": 206592
    },
    {
      "epoch": 0.00074987950335261,
      "grad_norm": 9684.295121484063,
      "learning_rate": 2.201031292704734e-07,
      "loss": 1.7149,
      "step": 206624
    },
    {
      "epoch": 0.0007499956377034467,
      "grad_norm": 8953.767698572485,
      "learning_rate": 2.200860704836296e-07,
      "loss": 1.73,
      "step": 206656
    },
    {
      "epoch": 0.0007501117720542834,
      "grad_norm": 11409.625410152605,
      "learning_rate": 2.2006954856567021e-07,
      "loss": 1.7403,
      "step": 206688
    },
    {
      "epoch": 0.0007502279064051201,
      "grad_norm": 9291.244373064352,
      "learning_rate": 2.2005249758490785e-07,
      "loss": 1.7323,
      "step": 206720
    },
    {
      "epoch": 0.0007503440407559568,
      "grad_norm": 9932.114981211202,
      "learning_rate": 2.2003545056685956e-07,
      "loss": 1.7254,
      "step": 206752
    },
    {
      "epoch": 0.0007504601751067935,
      "grad_norm": 7934.305640697237,
      "learning_rate": 2.2001840750999065e-07,
      "loss": 1.7268,
      "step": 206784
    },
    {
      "epoch": 0.0007505763094576302,
      "grad_norm": 9727.523014621966,
      "learning_rate": 2.2000136841276733e-07,
      "loss": 1.7351,
      "step": 206816
    },
    {
      "epoch": 0.0007506924438084669,
      "grad_norm": 9663.471425942129,
      "learning_rate": 2.1998433327365653e-07,
      "loss": 1.7358,
      "step": 206848
    },
    {
      "epoch": 0.0007508085781593037,
      "grad_norm": 10997.918166634992,
      "learning_rate": 2.199673020911261e-07,
      "loss": 1.7412,
      "step": 206880
    },
    {
      "epoch": 0.0007509247125101403,
      "grad_norm": 10712.342040842423,
      "learning_rate": 2.1995027486364466e-07,
      "loss": 1.7409,
      "step": 206912
    },
    {
      "epoch": 0.0007510408468609771,
      "grad_norm": 10821.731654407256,
      "learning_rate": 2.199332515896817e-07,
      "loss": 1.7611,
      "step": 206944
    },
    {
      "epoch": 0.0007511569812118137,
      "grad_norm": 8453.463432227054,
      "learning_rate": 2.199162322677075e-07,
      "loss": 1.7681,
      "step": 206976
    },
    {
      "epoch": 0.0007512731155626505,
      "grad_norm": 10573.48400481128,
      "learning_rate": 2.1989921689619322e-07,
      "loss": 1.764,
      "step": 207008
    },
    {
      "epoch": 0.0007513892499134871,
      "grad_norm": 9270.20409699808,
      "learning_rate": 2.1988220547361086e-07,
      "loss": 1.7606,
      "step": 207040
    },
    {
      "epoch": 0.0007515053842643239,
      "grad_norm": 11004.748793134717,
      "learning_rate": 2.1986519799843312e-07,
      "loss": 1.7509,
      "step": 207072
    },
    {
      "epoch": 0.0007516215186151605,
      "grad_norm": 10026.494901011021,
      "learning_rate": 2.1984819446913366e-07,
      "loss": 1.7402,
      "step": 207104
    },
    {
      "epoch": 0.0007517376529659973,
      "grad_norm": 9585.854369851442,
      "learning_rate": 2.1983119488418693e-07,
      "loss": 1.7092,
      "step": 207136
    },
    {
      "epoch": 0.000751853787316834,
      "grad_norm": 10060.569367585515,
      "learning_rate": 2.1981419924206817e-07,
      "loss": 1.7268,
      "step": 207168
    },
    {
      "epoch": 0.0007519699216676707,
      "grad_norm": 10260.326700451598,
      "learning_rate": 2.1979720754125347e-07,
      "loss": 1.7221,
      "step": 207200
    },
    {
      "epoch": 0.0007520860560185074,
      "grad_norm": 12403.75878514251,
      "learning_rate": 2.1978021978021978e-07,
      "loss": 1.7261,
      "step": 207232
    },
    {
      "epoch": 0.0007522021903693441,
      "grad_norm": 10273.294700338349,
      "learning_rate": 2.1976323595744482e-07,
      "loss": 1.7415,
      "step": 207264
    },
    {
      "epoch": 0.0007523183247201808,
      "grad_norm": 11089.010956798626,
      "learning_rate": 2.1974625607140717e-07,
      "loss": 1.7381,
      "step": 207296
    },
    {
      "epoch": 0.0007524344590710175,
      "grad_norm": 9256.374452235605,
      "learning_rate": 2.1972928012058616e-07,
      "loss": 1.7365,
      "step": 207328
    },
    {
      "epoch": 0.0007525505934218542,
      "grad_norm": 9532.298568551028,
      "learning_rate": 2.1971230810346204e-07,
      "loss": 1.7149,
      "step": 207360
    },
    {
      "epoch": 0.0007526667277726909,
      "grad_norm": 9431.242442011551,
      "learning_rate": 2.1969534001851585e-07,
      "loss": 1.7222,
      "step": 207392
    },
    {
      "epoch": 0.0007527828621235276,
      "grad_norm": 10014.399432816726,
      "learning_rate": 2.1967837586422944e-07,
      "loss": 1.7137,
      "step": 207424
    },
    {
      "epoch": 0.0007528989964743644,
      "grad_norm": 9973.84559736113,
      "learning_rate": 2.1966141563908543e-07,
      "loss": 1.7188,
      "step": 207456
    },
    {
      "epoch": 0.000753015130825201,
      "grad_norm": 10012.192966578301,
      "learning_rate": 2.196444593415674e-07,
      "loss": 1.7145,
      "step": 207488
    },
    {
      "epoch": 0.0007531312651760378,
      "grad_norm": 10385.845752754081,
      "learning_rate": 2.1962750697015956e-07,
      "loss": 1.7328,
      "step": 207520
    },
    {
      "epoch": 0.0007532473995268744,
      "grad_norm": 9556.678921047833,
      "learning_rate": 2.196105585233471e-07,
      "loss": 1.7301,
      "step": 207552
    },
    {
      "epoch": 0.0007533635338777112,
      "grad_norm": 10119.201944817585,
      "learning_rate": 2.1959361399961598e-07,
      "loss": 1.742,
      "step": 207584
    },
    {
      "epoch": 0.0007534796682285478,
      "grad_norm": 10061.465201450532,
      "learning_rate": 2.195766733974529e-07,
      "loss": 1.7433,
      "step": 207616
    },
    {
      "epoch": 0.0007535958025793846,
      "grad_norm": 9830.842385065484,
      "learning_rate": 2.1955973671534549e-07,
      "loss": 1.7491,
      "step": 207648
    },
    {
      "epoch": 0.0007537119369302212,
      "grad_norm": 9699.175738174868,
      "learning_rate": 2.1954333304134462e-07,
      "loss": 1.7554,
      "step": 207680
    },
    {
      "epoch": 0.000753828071281058,
      "grad_norm": 9274.891266209,
      "learning_rate": 2.1952640407243012e-07,
      "loss": 1.7355,
      "step": 207712
    },
    {
      "epoch": 0.0007539442056318947,
      "grad_norm": 9324.465239358233,
      "learning_rate": 2.195094790190861e-07,
      "loss": 1.7426,
      "step": 207744
    },
    {
      "epoch": 0.0007540603399827314,
      "grad_norm": 9876.320772433426,
      "learning_rate": 2.1949255787980338e-07,
      "loss": 1.7392,
      "step": 207776
    },
    {
      "epoch": 0.0007541764743335681,
      "grad_norm": 11405.288773196406,
      "learning_rate": 2.1947564065307356e-07,
      "loss": 1.7481,
      "step": 207808
    },
    {
      "epoch": 0.0007542926086844048,
      "grad_norm": 9451.784170197709,
      "learning_rate": 2.1945872733738913e-07,
      "loss": 1.7729,
      "step": 207840
    },
    {
      "epoch": 0.0007544087430352415,
      "grad_norm": 9562.137522541705,
      "learning_rate": 2.1944181793124333e-07,
      "loss": 1.7584,
      "step": 207872
    },
    {
      "epoch": 0.0007545248773860782,
      "grad_norm": 9365.238704912972,
      "learning_rate": 2.1942491243313026e-07,
      "loss": 1.7455,
      "step": 207904
    },
    {
      "epoch": 0.000754641011736915,
      "grad_norm": 10018.606489926631,
      "learning_rate": 2.1940801084154477e-07,
      "loss": 1.7342,
      "step": 207936
    },
    {
      "epoch": 0.0007547571460877516,
      "grad_norm": 9531.685685124117,
      "learning_rate": 2.193911131549826e-07,
      "loss": 1.7216,
      "step": 207968
    },
    {
      "epoch": 0.0007548732804385883,
      "grad_norm": 10434.99880210822,
      "learning_rate": 2.1937421937194018e-07,
      "loss": 1.7156,
      "step": 208000
    },
    {
      "epoch": 0.0007549894147894251,
      "grad_norm": 9505.18279676935,
      "learning_rate": 2.1935732949091488e-07,
      "loss": 1.7301,
      "step": 208032
    },
    {
      "epoch": 0.0007551055491402617,
      "grad_norm": 11039.872281869932,
      "learning_rate": 2.1934044351040482e-07,
      "loss": 1.7201,
      "step": 208064
    },
    {
      "epoch": 0.0007552216834910985,
      "grad_norm": 10056.81937791467,
      "learning_rate": 2.1932356142890893e-07,
      "loss": 1.7231,
      "step": 208096
    },
    {
      "epoch": 0.0007553378178419351,
      "grad_norm": 9262.885295630082,
      "learning_rate": 2.1930668324492697e-07,
      "loss": 1.739,
      "step": 208128
    },
    {
      "epoch": 0.0007554539521927719,
      "grad_norm": 8714.212873231867,
      "learning_rate": 2.1928980895695947e-07,
      "loss": 1.7309,
      "step": 208160
    },
    {
      "epoch": 0.0007555700865436085,
      "grad_norm": 12311.449955224609,
      "learning_rate": 2.192729385635078e-07,
      "loss": 1.7461,
      "step": 208192
    },
    {
      "epoch": 0.0007556862208944453,
      "grad_norm": 9685.991843894975,
      "learning_rate": 2.1925607206307416e-07,
      "loss": 1.7419,
      "step": 208224
    },
    {
      "epoch": 0.000755802355245282,
      "grad_norm": 9312.401838408821,
      "learning_rate": 2.1923920945416148e-07,
      "loss": 1.7371,
      "step": 208256
    },
    {
      "epoch": 0.0007559184895961187,
      "grad_norm": 9831.381083042199,
      "learning_rate": 2.1922235073527352e-07,
      "loss": 1.7246,
      "step": 208288
    },
    {
      "epoch": 0.0007560346239469555,
      "grad_norm": 9114.983817868246,
      "learning_rate": 2.192054959049149e-07,
      "loss": 1.716,
      "step": 208320
    },
    {
      "epoch": 0.0007561507582977921,
      "grad_norm": 11046.551860196014,
      "learning_rate": 2.1918864496159103e-07,
      "loss": 1.7176,
      "step": 208352
    },
    {
      "epoch": 0.0007562668926486289,
      "grad_norm": 9602.929448871317,
      "learning_rate": 2.1917179790380803e-07,
      "loss": 1.7272,
      "step": 208384
    },
    {
      "epoch": 0.0007563830269994655,
      "grad_norm": 10569.013009737475,
      "learning_rate": 2.19154954730073e-07,
      "loss": 1.7305,
      "step": 208416
    },
    {
      "epoch": 0.0007564991613503023,
      "grad_norm": 9831.444858208788,
      "learning_rate": 2.1913811543889365e-07,
      "loss": 1.7469,
      "step": 208448
    },
    {
      "epoch": 0.0007566152957011389,
      "grad_norm": 9077.4929358276,
      "learning_rate": 2.1912128002877864e-07,
      "loss": 1.7354,
      "step": 208480
    },
    {
      "epoch": 0.0007567314300519757,
      "grad_norm": 10615.92935168655,
      "learning_rate": 2.191044484982373e-07,
      "loss": 1.7239,
      "step": 208512
    },
    {
      "epoch": 0.0007568475644028123,
      "grad_norm": 9803.813135714083,
      "learning_rate": 2.1908762084577993e-07,
      "loss": 1.7316,
      "step": 208544
    },
    {
      "epoch": 0.0007569636987536491,
      "grad_norm": 9785.954015833102,
      "learning_rate": 2.1907079706991748e-07,
      "loss": 1.7271,
      "step": 208576
    },
    {
      "epoch": 0.0007570798331044858,
      "grad_norm": 8791.235180564789,
      "learning_rate": 2.190539771691618e-07,
      "loss": 1.7411,
      "step": 208608
    },
    {
      "epoch": 0.0007571959674553225,
      "grad_norm": 10021.34681567303,
      "learning_rate": 2.190371611420254e-07,
      "loss": 1.7377,
      "step": 208640
    },
    {
      "epoch": 0.0007573121018061592,
      "grad_norm": 9374.424142314023,
      "learning_rate": 2.1902034898702177e-07,
      "loss": 1.7457,
      "step": 208672
    },
    {
      "epoch": 0.0007574282361569959,
      "grad_norm": 10648.396311182261,
      "learning_rate": 2.190040659029771e-07,
      "loss": 1.7736,
      "step": 208704
    },
    {
      "epoch": 0.0007575443705078326,
      "grad_norm": 9875.514872653475,
      "learning_rate": 2.1898726136689359e-07,
      "loss": 1.7443,
      "step": 208736
    },
    {
      "epoch": 0.0007576605048586693,
      "grad_norm": 10353.751011107039,
      "learning_rate": 2.1897046069853413e-07,
      "loss": 1.7553,
      "step": 208768
    },
    {
      "epoch": 0.000757776639209506,
      "grad_norm": 10097.165542863997,
      "learning_rate": 2.189536638964154e-07,
      "loss": 1.7475,
      "step": 208800
    },
    {
      "epoch": 0.0007578927735603427,
      "grad_norm": 10636.149021144824,
      "learning_rate": 2.1893687095905475e-07,
      "loss": 1.7479,
      "step": 208832
    },
    {
      "epoch": 0.0007580089079111794,
      "grad_norm": 8658.740670559431,
      "learning_rate": 2.1892008188497032e-07,
      "loss": 1.7385,
      "step": 208864
    },
    {
      "epoch": 0.0007581250422620162,
      "grad_norm": 10990.828176256782,
      "learning_rate": 2.189032966726811e-07,
      "loss": 1.7231,
      "step": 208896
    },
    {
      "epoch": 0.0007582411766128528,
      "grad_norm": 11172.383273053247,
      "learning_rate": 2.1888651532070677e-07,
      "loss": 1.7191,
      "step": 208928
    },
    {
      "epoch": 0.0007583573109636896,
      "grad_norm": 8712.052915358125,
      "learning_rate": 2.1886973782756797e-07,
      "loss": 1.7316,
      "step": 208960
    },
    {
      "epoch": 0.0007584734453145262,
      "grad_norm": 11022.167300490408,
      "learning_rate": 2.1885296419178598e-07,
      "loss": 1.731,
      "step": 208992
    },
    {
      "epoch": 0.000758589579665363,
      "grad_norm": 9439.716309296587,
      "learning_rate": 2.1883619441188296e-07,
      "loss": 1.7257,
      "step": 209024
    },
    {
      "epoch": 0.0007587057140161996,
      "grad_norm": 11325.02362028442,
      "learning_rate": 2.1881942848638184e-07,
      "loss": 1.7441,
      "step": 209056
    },
    {
      "epoch": 0.0007588218483670364,
      "grad_norm": 11790.764691062239,
      "learning_rate": 2.1880266641380634e-07,
      "loss": 1.7248,
      "step": 209088
    },
    {
      "epoch": 0.000758937982717873,
      "grad_norm": 8960.060937292781,
      "learning_rate": 2.1878590819268096e-07,
      "loss": 1.7186,
      "step": 209120
    },
    {
      "epoch": 0.0007590541170687098,
      "grad_norm": 9914.213231517668,
      "learning_rate": 2.18769153821531e-07,
      "loss": 1.7055,
      "step": 209152
    },
    {
      "epoch": 0.0007591702514195465,
      "grad_norm": 10148.204767346784,
      "learning_rate": 2.1875240329888257e-07,
      "loss": 1.716,
      "step": 209184
    },
    {
      "epoch": 0.0007592863857703832,
      "grad_norm": 10286.407341730153,
      "learning_rate": 2.1873565662326254e-07,
      "loss": 1.7227,
      "step": 209216
    },
    {
      "epoch": 0.0007594025201212199,
      "grad_norm": 10189.2368703451,
      "learning_rate": 2.1871891379319857e-07,
      "loss": 1.7232,
      "step": 209248
    },
    {
      "epoch": 0.0007595186544720566,
      "grad_norm": 10817.177820485342,
      "learning_rate": 2.1870217480721916e-07,
      "loss": 1.7352,
      "step": 209280
    },
    {
      "epoch": 0.0007596347888228933,
      "grad_norm": 8749.252539503017,
      "learning_rate": 2.1868543966385353e-07,
      "loss": 1.7328,
      "step": 209312
    },
    {
      "epoch": 0.00075975092317373,
      "grad_norm": 8294.79535612543,
      "learning_rate": 2.186687083616317e-07,
      "loss": 1.7426,
      "step": 209344
    },
    {
      "epoch": 0.0007598670575245667,
      "grad_norm": 10063.892884962557,
      "learning_rate": 2.1865198089908452e-07,
      "loss": 1.7333,
      "step": 209376
    },
    {
      "epoch": 0.0007599831918754034,
      "grad_norm": 10823.0609348742,
      "learning_rate": 2.186352572747436e-07,
      "loss": 1.7502,
      "step": 209408
    },
    {
      "epoch": 0.0007600993262262401,
      "grad_norm": 9102.619073651274,
      "learning_rate": 2.1861853748714135e-07,
      "loss": 1.7522,
      "step": 209440
    },
    {
      "epoch": 0.0007602154605770769,
      "grad_norm": 9306.736270035806,
      "learning_rate": 2.1860182153481089e-07,
      "loss": 1.7467,
      "step": 209472
    },
    {
      "epoch": 0.0007603315949279135,
      "grad_norm": 9160.037882017738,
      "learning_rate": 2.1858510941628625e-07,
      "loss": 1.734,
      "step": 209504
    },
    {
      "epoch": 0.0007604477292787503,
      "grad_norm": 9957.290796195519,
      "learning_rate": 2.1856840113010217e-07,
      "loss": 1.747,
      "step": 209536
    },
    {
      "epoch": 0.0007605638636295869,
      "grad_norm": 10294.586732841683,
      "learning_rate": 2.1855169667479414e-07,
      "loss": 1.7667,
      "step": 209568
    },
    {
      "epoch": 0.0007606799979804237,
      "grad_norm": 9512.404427903599,
      "learning_rate": 2.1853499604889853e-07,
      "loss": 1.7376,
      "step": 209600
    },
    {
      "epoch": 0.0007607961323312603,
      "grad_norm": 10894.326596903546,
      "learning_rate": 2.185182992509524e-07,
      "loss": 1.7601,
      "step": 209632
    },
    {
      "epoch": 0.0007609122666820971,
      "grad_norm": 8448.3945220379,
      "learning_rate": 2.1850160627949367e-07,
      "loss": 1.7287,
      "step": 209664
    },
    {
      "epoch": 0.0007610284010329337,
      "grad_norm": 9470.043505707881,
      "learning_rate": 2.1848543861100318e-07,
      "loss": 1.7313,
      "step": 209696
    },
    {
      "epoch": 0.0007611445353837705,
      "grad_norm": 7894.24157218412,
      "learning_rate": 2.1846875316867168e-07,
      "loss": 1.7283,
      "step": 209728
    },
    {
      "epoch": 0.0007612606697346072,
      "grad_norm": 10382.646483435714,
      "learning_rate": 2.184520715484915e-07,
      "loss": 1.7178,
      "step": 209760
    },
    {
      "epoch": 0.0007613768040854439,
      "grad_norm": 11282.497418568286,
      "learning_rate": 2.1843539374900363e-07,
      "loss": 1.7291,
      "step": 209792
    },
    {
      "epoch": 0.0007614929384362806,
      "grad_norm": 8995.282430251982,
      "learning_rate": 2.1841871976874985e-07,
      "loss": 1.7356,
      "step": 209824
    },
    {
      "epoch": 0.0007616090727871173,
      "grad_norm": 10165.269991495554,
      "learning_rate": 2.1840204960627266e-07,
      "loss": 1.734,
      "step": 209856
    },
    {
      "epoch": 0.000761725207137954,
      "grad_norm": 9606.019050574489,
      "learning_rate": 2.1838538326011547e-07,
      "loss": 1.7238,
      "step": 209888
    },
    {
      "epoch": 0.0007618413414887907,
      "grad_norm": 9069.385646227644,
      "learning_rate": 2.1836872072882223e-07,
      "loss": 1.7387,
      "step": 209920
    },
    {
      "epoch": 0.0007619574758396274,
      "grad_norm": 9768.802690196993,
      "learning_rate": 2.1835206201093796e-07,
      "loss": 1.7411,
      "step": 209952
    },
    {
      "epoch": 0.0007620736101904641,
      "grad_norm": 10600.519987245909,
      "learning_rate": 2.1833540710500823e-07,
      "loss": 1.7393,
      "step": 209984
    },
    {
      "epoch": 0.0007621897445413008,
      "grad_norm": 11166.956434051312,
      "learning_rate": 2.1831875600957947e-07,
      "loss": 1.7316,
      "step": 210016
    },
    {
      "epoch": 0.0007623058788921375,
      "grad_norm": 9186.869651845507,
      "learning_rate": 2.1830210872319891e-07,
      "loss": 1.7428,
      "step": 210048
    },
    {
      "epoch": 0.0007624220132429742,
      "grad_norm": 10111.747128958477,
      "learning_rate": 2.1828546524441452e-07,
      "loss": 1.7202,
      "step": 210080
    },
    {
      "epoch": 0.000762538147593811,
      "grad_norm": 10946.160057298632,
      "learning_rate": 2.182688255717751e-07,
      "loss": 1.7294,
      "step": 210112
    },
    {
      "epoch": 0.0007626542819446476,
      "grad_norm": 9370.695385082156,
      "learning_rate": 2.182521897038301e-07,
      "loss": 1.7325,
      "step": 210144
    },
    {
      "epoch": 0.0007627704162954844,
      "grad_norm": 10000.491987897396,
      "learning_rate": 2.1823555763912988e-07,
      "loss": 1.724,
      "step": 210176
    },
    {
      "epoch": 0.000762886550646321,
      "grad_norm": 10166.988934783001,
      "learning_rate": 2.1821892937622547e-07,
      "loss": 1.7501,
      "step": 210208
    },
    {
      "epoch": 0.0007630026849971578,
      "grad_norm": 10520.09885885109,
      "learning_rate": 2.1820230491366877e-07,
      "loss": 1.7364,
      "step": 210240
    },
    {
      "epoch": 0.0007631188193479944,
      "grad_norm": 9943.849154125379,
      "learning_rate": 2.1818568425001235e-07,
      "loss": 1.7305,
      "step": 210272
    },
    {
      "epoch": 0.0007632349536988312,
      "grad_norm": 9293.834945812197,
      "learning_rate": 2.1816906738380964e-07,
      "loss": 1.7304,
      "step": 210304
    },
    {
      "epoch": 0.0007633510880496678,
      "grad_norm": 9550.53579648807,
      "learning_rate": 2.1815245431361478e-07,
      "loss": 1.731,
      "step": 210336
    },
    {
      "epoch": 0.0007634672224005046,
      "grad_norm": 10209.427897781541,
      "learning_rate": 2.1813584503798274e-07,
      "loss": 1.7393,
      "step": 210368
    },
    {
      "epoch": 0.0007635833567513413,
      "grad_norm": 9810.141894998258,
      "learning_rate": 2.181192395554692e-07,
      "loss": 1.7538,
      "step": 210400
    },
    {
      "epoch": 0.000763699491102178,
      "grad_norm": 9481.070403704425,
      "learning_rate": 2.1810263786463062e-07,
      "loss": 1.7569,
      "step": 210432
    },
    {
      "epoch": 0.0007638156254530147,
      "grad_norm": 10078.078586714832,
      "learning_rate": 2.1808603996402423e-07,
      "loss": 1.7427,
      "step": 210464
    },
    {
      "epoch": 0.0007639317598038514,
      "grad_norm": 9569.532590466475,
      "learning_rate": 2.180694458522081e-07,
      "loss": 1.7483,
      "step": 210496
    },
    {
      "epoch": 0.0007640478941546881,
      "grad_norm": 9148.230320668583,
      "learning_rate": 2.1805285552774097e-07,
      "loss": 1.747,
      "step": 210528
    },
    {
      "epoch": 0.0007641640285055248,
      "grad_norm": 9813.394112130623,
      "learning_rate": 2.1803626898918236e-07,
      "loss": 1.7495,
      "step": 210560
    },
    {
      "epoch": 0.0007642801628563615,
      "grad_norm": 8498.577528033735,
      "learning_rate": 2.1801968623509263e-07,
      "loss": 1.7518,
      "step": 210592
    },
    {
      "epoch": 0.0007643962972071982,
      "grad_norm": 10119.4926750307,
      "learning_rate": 2.1800310726403284e-07,
      "loss": 1.7382,
      "step": 210624
    },
    {
      "epoch": 0.0007645124315580349,
      "grad_norm": 9511.781746865305,
      "learning_rate": 2.1798653207456487e-07,
      "loss": 1.7274,
      "step": 210656
    },
    {
      "epoch": 0.0007646285659088717,
      "grad_norm": 9938.002817467903,
      "learning_rate": 2.1797047846458746e-07,
      "loss": 1.7271,
      "step": 210688
    },
    {
      "epoch": 0.0007647447002597083,
      "grad_norm": 11141.376126852552,
      "learning_rate": 2.1795391071592843e-07,
      "loss": 1.7372,
      "step": 210720
    },
    {
      "epoch": 0.0007648608346105451,
      "grad_norm": 12016.968003618882,
      "learning_rate": 2.179373467445962e-07,
      "loss": 1.7143,
      "step": 210752
    },
    {
      "epoch": 0.0007649769689613817,
      "grad_norm": 9434.309937668997,
      "learning_rate": 2.179207865491556e-07,
      "loss": 1.7341,
      "step": 210784
    },
    {
      "epoch": 0.0007650931033122185,
      "grad_norm": 8902.6024285037,
      "learning_rate": 2.1790423012817231e-07,
      "loss": 1.7436,
      "step": 210816
    },
    {
      "epoch": 0.0007652092376630551,
      "grad_norm": 9776.519216981062,
      "learning_rate": 2.1788767748021272e-07,
      "loss": 1.7287,
      "step": 210848
    },
    {
      "epoch": 0.0007653253720138919,
      "grad_norm": 10046.232527669266,
      "learning_rate": 2.17871128603844e-07,
      "loss": 1.7112,
      "step": 210880
    },
    {
      "epoch": 0.0007654415063647285,
      "grad_norm": 9092.970141818349,
      "learning_rate": 2.178545834976341e-07,
      "loss": 1.7159,
      "step": 210912
    },
    {
      "epoch": 0.0007655576407155653,
      "grad_norm": 9754.007176540315,
      "learning_rate": 2.1783804216015166e-07,
      "loss": 1.7251,
      "step": 210944
    },
    {
      "epoch": 0.000765673775066402,
      "grad_norm": 8558.5905381669,
      "learning_rate": 2.1782150458996615e-07,
      "loss": 1.7325,
      "step": 210976
    },
    {
      "epoch": 0.0007657899094172387,
      "grad_norm": 10629.582494152815,
      "learning_rate": 2.1780497078564782e-07,
      "loss": 1.7306,
      "step": 211008
    },
    {
      "epoch": 0.0007659060437680754,
      "grad_norm": 8721.64250585863,
      "learning_rate": 2.177884407457676e-07,
      "loss": 1.732,
      "step": 211040
    },
    {
      "epoch": 0.0007660221781189121,
      "grad_norm": 10394.013276882035,
      "learning_rate": 2.1777191446889722e-07,
      "loss": 1.735,
      "step": 211072
    },
    {
      "epoch": 0.0007661383124697488,
      "grad_norm": 9592.815228075646,
      "learning_rate": 2.1775539195360917e-07,
      "loss": 1.7447,
      "step": 211104
    },
    {
      "epoch": 0.0007662544468205855,
      "grad_norm": 10643.960822926774,
      "learning_rate": 2.1773887319847674e-07,
      "loss": 1.7395,
      "step": 211136
    },
    {
      "epoch": 0.0007663705811714222,
      "grad_norm": 10759.21930253306,
      "learning_rate": 2.177223582020739e-07,
      "loss": 1.7493,
      "step": 211168
    },
    {
      "epoch": 0.0007664867155222589,
      "grad_norm": 8864.671905942148,
      "learning_rate": 2.1770584696297537e-07,
      "loss": 1.7541,
      "step": 211200
    },
    {
      "epoch": 0.0007666028498730956,
      "grad_norm": 11940.67820519421,
      "learning_rate": 2.1768933947975673e-07,
      "loss": 1.7512,
      "step": 211232
    },
    {
      "epoch": 0.0007667189842239324,
      "grad_norm": 10075.20242972815,
      "learning_rate": 2.1767283575099424e-07,
      "loss": 1.7473,
      "step": 211264
    },
    {
      "epoch": 0.000766835118574769,
      "grad_norm": 10612.86144260821,
      "learning_rate": 2.176563357752649e-07,
      "loss": 1.7512,
      "step": 211296
    },
    {
      "epoch": 0.0007669512529256058,
      "grad_norm": 9553.632816892221,
      "learning_rate": 2.1763983955114656e-07,
      "loss": 1.7486,
      "step": 211328
    },
    {
      "epoch": 0.0007670673872764424,
      "grad_norm": 8789.394177075006,
      "learning_rate": 2.1762334707721767e-07,
      "loss": 1.7437,
      "step": 211360
    },
    {
      "epoch": 0.0007671835216272792,
      "grad_norm": 9002.117084330774,
      "learning_rate": 2.1760685835205756e-07,
      "loss": 1.7501,
      "step": 211392
    },
    {
      "epoch": 0.0007672996559781158,
      "grad_norm": 10710.82088357377,
      "learning_rate": 2.175903733742463e-07,
      "loss": 1.7405,
      "step": 211424
    },
    {
      "epoch": 0.0007674157903289526,
      "grad_norm": 9677.783423904464,
      "learning_rate": 2.1757389214236466e-07,
      "loss": 1.7383,
      "step": 211456
    },
    {
      "epoch": 0.0007675319246797892,
      "grad_norm": 11163.169084090772,
      "learning_rate": 2.175574146549942e-07,
      "loss": 1.7129,
      "step": 211488
    },
    {
      "epoch": 0.000767648059030626,
      "grad_norm": 9142.426483160802,
      "learning_rate": 2.1754094091071722e-07,
      "loss": 1.721,
      "step": 211520
    },
    {
      "epoch": 0.0007677641933814627,
      "grad_norm": 11158.279616500029,
      "learning_rate": 2.1752447090811678e-07,
      "loss": 1.7306,
      "step": 211552
    },
    {
      "epoch": 0.0007678803277322994,
      "grad_norm": 11634.828404407175,
      "learning_rate": 2.1750800464577668e-07,
      "loss": 1.7362,
      "step": 211584
    },
    {
      "epoch": 0.0007679964620831361,
      "grad_norm": 8525.087213630133,
      "learning_rate": 2.1749154212228147e-07,
      "loss": 1.7176,
      "step": 211616
    },
    {
      "epoch": 0.0007681125964339728,
      "grad_norm": 10112.076740215138,
      "learning_rate": 2.1747508333621646e-07,
      "loss": 1.7296,
      "step": 211648
    },
    {
      "epoch": 0.0007682287307848095,
      "grad_norm": 10107.357122413356,
      "learning_rate": 2.174586282861677e-07,
      "loss": 1.736,
      "step": 211680
    },
    {
      "epoch": 0.0007683448651356462,
      "grad_norm": 10849.271311936116,
      "learning_rate": 2.174426910178141e-07,
      "loss": 1.7432,
      "step": 211712
    },
    {
      "epoch": 0.0007684609994864829,
      "grad_norm": 10484.946161044414,
      "learning_rate": 2.1742624331891816e-07,
      "loss": 1.7263,
      "step": 211744
    },
    {
      "epoch": 0.0007685771338373196,
      "grad_norm": 9790.559126015225,
      "learning_rate": 2.1740979935184526e-07,
      "loss": 1.7376,
      "step": 211776
    },
    {
      "epoch": 0.0007686932681881563,
      "grad_norm": 9268.54789058135,
      "learning_rate": 2.1739335911518436e-07,
      "loss": 1.7459,
      "step": 211808
    },
    {
      "epoch": 0.0007688094025389931,
      "grad_norm": 10499.985523799545,
      "learning_rate": 2.1737692260752529e-07,
      "loss": 1.7287,
      "step": 211840
    },
    {
      "epoch": 0.0007689255368898297,
      "grad_norm": 9220.524822373181,
      "learning_rate": 2.1736048982745853e-07,
      "loss": 1.7281,
      "step": 211872
    },
    {
      "epoch": 0.0007690416712406665,
      "grad_norm": 9152.89932207276,
      "learning_rate": 2.1734406077357533e-07,
      "loss": 1.731,
      "step": 211904
    },
    {
      "epoch": 0.0007691578055915031,
      "grad_norm": 10118.123739112902,
      "learning_rate": 2.1732763544446775e-07,
      "loss": 1.7312,
      "step": 211936
    },
    {
      "epoch": 0.0007692739399423399,
      "grad_norm": 9434.865340851453,
      "learning_rate": 2.173112138387285e-07,
      "loss": 1.7423,
      "step": 211968
    },
    {
      "epoch": 0.0007693900742931765,
      "grad_norm": 9174.491702541345,
      "learning_rate": 2.1729479595495107e-07,
      "loss": 1.7363,
      "step": 212000
    },
    {
      "epoch": 0.0007695062086440133,
      "grad_norm": 10724.77086002307,
      "learning_rate": 2.1727838179172974e-07,
      "loss": 1.7249,
      "step": 212032
    },
    {
      "epoch": 0.0007696223429948499,
      "grad_norm": 9785.031732191777,
      "learning_rate": 2.1726197134765944e-07,
      "loss": 1.7292,
      "step": 212064
    },
    {
      "epoch": 0.0007697384773456867,
      "grad_norm": 8866.412916168523,
      "learning_rate": 2.1724556462133595e-07,
      "loss": 1.7334,
      "step": 212096
    },
    {
      "epoch": 0.0007698546116965234,
      "grad_norm": 11107.087106888106,
      "learning_rate": 2.172291616113557e-07,
      "loss": 1.7506,
      "step": 212128
    },
    {
      "epoch": 0.0007699707460473601,
      "grad_norm": 9124.4678748955,
      "learning_rate": 2.1721276231631592e-07,
      "loss": 1.7482,
      "step": 212160
    },
    {
      "epoch": 0.0007700868803981968,
      "grad_norm": 10265.391955497851,
      "learning_rate": 2.1719636673481451e-07,
      "loss": 1.7397,
      "step": 212192
    },
    {
      "epoch": 0.0007702030147490335,
      "grad_norm": 9295.97450512855,
      "learning_rate": 2.1717997486545022e-07,
      "loss": 1.7452,
      "step": 212224
    },
    {
      "epoch": 0.0007703191490998702,
      "grad_norm": 9390.640020786655,
      "learning_rate": 2.171635867068225e-07,
      "loss": 1.7466,
      "step": 212256
    },
    {
      "epoch": 0.0007704352834507069,
      "grad_norm": 8659.5570325508,
      "learning_rate": 2.1714720225753144e-07,
      "loss": 1.7524,
      "step": 212288
    },
    {
      "epoch": 0.0007705514178015436,
      "grad_norm": 9042.797244215973,
      "learning_rate": 2.1713082151617799e-07,
      "loss": 1.7497,
      "step": 212320
    },
    {
      "epoch": 0.0007706675521523803,
      "grad_norm": 12107.56490794082,
      "learning_rate": 2.1711444448136384e-07,
      "loss": 1.7377,
      "step": 212352
    },
    {
      "epoch": 0.000770783686503217,
      "grad_norm": 9609.532142617558,
      "learning_rate": 2.1709807115169131e-07,
      "loss": 1.7357,
      "step": 212384
    },
    {
      "epoch": 0.0007708998208540538,
      "grad_norm": 10165.808280702522,
      "learning_rate": 2.1708170152576358e-07,
      "loss": 1.7433,
      "step": 212416
    },
    {
      "epoch": 0.0007710159552048904,
      "grad_norm": 9867.407156897905,
      "learning_rate": 2.1706533560218446e-07,
      "loss": 1.7264,
      "step": 212448
    },
    {
      "epoch": 0.0007711320895557272,
      "grad_norm": 10566.258940609017,
      "learning_rate": 2.1704897337955857e-07,
      "loss": 1.7193,
      "step": 212480
    },
    {
      "epoch": 0.0007712482239065638,
      "grad_norm": 8935.863248729805,
      "learning_rate": 2.1703261485649125e-07,
      "loss": 1.7154,
      "step": 212512
    },
    {
      "epoch": 0.0007713643582574006,
      "grad_norm": 10217.115248444641,
      "learning_rate": 2.170162600315886e-07,
      "loss": 1.7336,
      "step": 212544
    },
    {
      "epoch": 0.0007714804926082372,
      "grad_norm": 10501.142414042388,
      "learning_rate": 2.1699990890345737e-07,
      "loss": 1.744,
      "step": 212576
    },
    {
      "epoch": 0.000771596626959074,
      "grad_norm": 10937.138748319872,
      "learning_rate": 2.1698356147070513e-07,
      "loss": 1.7186,
      "step": 212608
    },
    {
      "epoch": 0.0007717127613099106,
      "grad_norm": 10600.34169260595,
      "learning_rate": 2.1696721773194015e-07,
      "loss": 1.714,
      "step": 212640
    },
    {
      "epoch": 0.0007718288956607474,
      "grad_norm": 9436.566112734017,
      "learning_rate": 2.1695087768577145e-07,
      "loss": 1.7253,
      "step": 212672
    },
    {
      "epoch": 0.0007719450300115842,
      "grad_norm": 9128.079425596603,
      "learning_rate": 2.169350517860424e-07,
      "loss": 1.7238,
      "step": 212704
    },
    {
      "epoch": 0.0007720611643624208,
      "grad_norm": 8369.259226478769,
      "learning_rate": 2.1691871900561048e-07,
      "loss": 1.7327,
      "step": 212736
    },
    {
      "epoch": 0.0007721772987132576,
      "grad_norm": 10469.510781311608,
      "learning_rate": 2.1690238991364957e-07,
      "loss": 1.7306,
      "step": 212768
    },
    {
      "epoch": 0.0007722934330640942,
      "grad_norm": 8982.552643875793,
      "learning_rate": 2.1688606450877174e-07,
      "loss": 1.7271,
      "step": 212800
    },
    {
      "epoch": 0.000772409567414931,
      "grad_norm": 10360.52431105685,
      "learning_rate": 2.1686974278958948e-07,
      "loss": 1.7367,
      "step": 212832
    },
    {
      "epoch": 0.0007725257017657676,
      "grad_norm": 8711.238717886223,
      "learning_rate": 2.1685342475471627e-07,
      "loss": 1.7372,
      "step": 212864
    },
    {
      "epoch": 0.0007726418361166044,
      "grad_norm": 9467.034805048517,
      "learning_rate": 2.1683711040276618e-07,
      "loss": 1.7355,
      "step": 212896
    },
    {
      "epoch": 0.000772757970467441,
      "grad_norm": 9896.667722016335,
      "learning_rate": 2.1682079973235405e-07,
      "loss": 1.7496,
      "step": 212928
    },
    {
      "epoch": 0.0007728741048182778,
      "grad_norm": 9887.052037892792,
      "learning_rate": 2.1680449274209546e-07,
      "loss": 1.754,
      "step": 212960
    },
    {
      "epoch": 0.0007729902391691145,
      "grad_norm": 10191.195611899519,
      "learning_rate": 2.167881894306067e-07,
      "loss": 1.7691,
      "step": 212992
    },
    {
      "epoch": 0.0007731063735199512,
      "grad_norm": 11274.292350298532,
      "learning_rate": 2.1677188979650478e-07,
      "loss": 1.7404,
      "step": 213024
    },
    {
      "epoch": 0.0007732225078707879,
      "grad_norm": 9290.784143440209,
      "learning_rate": 2.1675559383840744e-07,
      "loss": 1.7367,
      "step": 213056
    },
    {
      "epoch": 0.0007733386422216246,
      "grad_norm": 10854.052515074725,
      "learning_rate": 2.1673930155493318e-07,
      "loss": 1.7418,
      "step": 213088
    },
    {
      "epoch": 0.0007734547765724613,
      "grad_norm": 10014.925062126025,
      "learning_rate": 2.167230129447012e-07,
      "loss": 1.7424,
      "step": 213120
    },
    {
      "epoch": 0.000773570910923298,
      "grad_norm": 10712.155151975721,
      "learning_rate": 2.1670672800633147e-07,
      "loss": 1.7569,
      "step": 213152
    },
    {
      "epoch": 0.0007736870452741347,
      "grad_norm": 10805.179498740406,
      "learning_rate": 2.166904467384446e-07,
      "loss": 1.7269,
      "step": 213184
    },
    {
      "epoch": 0.0007738031796249714,
      "grad_norm": 9819.970468387366,
      "learning_rate": 2.1667416913966194e-07,
      "loss": 1.6988,
      "step": 213216
    },
    {
      "epoch": 0.0007739193139758081,
      "grad_norm": 11147.136852124853,
      "learning_rate": 2.1665789520860568e-07,
      "loss": 1.7126,
      "step": 213248
    },
    {
      "epoch": 0.0007740354483266449,
      "grad_norm": 9528.978329285885,
      "learning_rate": 2.166416249438986e-07,
      "loss": 1.7165,
      "step": 213280
    },
    {
      "epoch": 0.0007741515826774815,
      "grad_norm": 8381.727029675925,
      "learning_rate": 2.1662535834416425e-07,
      "loss": 1.7046,
      "step": 213312
    },
    {
      "epoch": 0.0007742677170283183,
      "grad_norm": 9788.29464207121,
      "learning_rate": 2.1660909540802697e-07,
      "loss": 1.7197,
      "step": 213344
    },
    {
      "epoch": 0.0007743838513791549,
      "grad_norm": 12138.103476243725,
      "learning_rate": 2.1659283613411168e-07,
      "loss": 1.7091,
      "step": 213376
    },
    {
      "epoch": 0.0007744999857299917,
      "grad_norm": 9238.432442790281,
      "learning_rate": 2.1657658052104417e-07,
      "loss": 1.6978,
      "step": 213408
    },
    {
      "epoch": 0.0007746161200808283,
      "grad_norm": 9746.63654806108,
      "learning_rate": 2.1656032856745085e-07,
      "loss": 1.7153,
      "step": 213440
    },
    {
      "epoch": 0.0007747322544316651,
      "grad_norm": 9088.319316573334,
      "learning_rate": 2.165440802719589e-07,
      "loss": 1.7251,
      "step": 213472
    },
    {
      "epoch": 0.0007748483887825017,
      "grad_norm": 9656.30301927192,
      "learning_rate": 2.165278356331962e-07,
      "loss": 1.7005,
      "step": 213504
    },
    {
      "epoch": 0.0007749645231333385,
      "grad_norm": 9751.089580144364,
      "learning_rate": 2.1651159464979136e-07,
      "loss": 1.7383,
      "step": 213536
    },
    {
      "epoch": 0.0007750806574841752,
      "grad_norm": 9371.825862658781,
      "learning_rate": 2.164953573203737e-07,
      "loss": 1.7412,
      "step": 213568
    },
    {
      "epoch": 0.0007751967918350119,
      "grad_norm": 9229.073409611607,
      "learning_rate": 2.1647912364357326e-07,
      "loss": 1.713,
      "step": 213600
    },
    {
      "epoch": 0.0007753129261858486,
      "grad_norm": 8973.416517692689,
      "learning_rate": 2.164628936180209e-07,
      "loss": 1.7046,
      "step": 213632
    },
    {
      "epoch": 0.0007754290605366853,
      "grad_norm": 8077.901460156592,
      "learning_rate": 2.1644666724234797e-07,
      "loss": 1.7232,
      "step": 213664
    },
    {
      "epoch": 0.000775545194887522,
      "grad_norm": 9743.169504837735,
      "learning_rate": 2.1643044451518676e-07,
      "loss": 1.7071,
      "step": 213696
    },
    {
      "epoch": 0.0007756613292383587,
      "grad_norm": 7579.642207914567,
      "learning_rate": 2.1641473222622842e-07,
      "loss": 1.724,
      "step": 213728
    },
    {
      "epoch": 0.0007757774635891954,
      "grad_norm": 7940.228963953117,
      "learning_rate": 2.1639851667808018e-07,
      "loss": 1.7303,
      "step": 213760
    },
    {
      "epoch": 0.0007758935979400321,
      "grad_norm": 10139.64476695313,
      "learning_rate": 2.163823047743872e-07,
      "loss": 1.7184,
      "step": 213792
    },
    {
      "epoch": 0.0007760097322908688,
      "grad_norm": 10282.646935492825,
      "learning_rate": 2.1636609651378456e-07,
      "loss": 1.7083,
      "step": 213824
    },
    {
      "epoch": 0.0007761258666417056,
      "grad_norm": 9595.378470909836,
      "learning_rate": 2.16349891894908e-07,
      "loss": 1.7284,
      "step": 213856
    },
    {
      "epoch": 0.0007762420009925422,
      "grad_norm": 10366.27782764865,
      "learning_rate": 2.16333690916394e-07,
      "loss": 1.7186,
      "step": 213888
    },
    {
      "epoch": 0.000776358135343379,
      "grad_norm": 9664.15914604059,
      "learning_rate": 2.1631749357687976e-07,
      "loss": 1.7117,
      "step": 213920
    },
    {
      "epoch": 0.0007764742696942156,
      "grad_norm": 9847.349389556563,
      "learning_rate": 2.1630129987500322e-07,
      "loss": 1.7363,
      "step": 213952
    },
    {
      "epoch": 0.0007765904040450524,
      "grad_norm": 9853.677080156422,
      "learning_rate": 2.162851098094029e-07,
      "loss": 1.7371,
      "step": 213984
    },
    {
      "epoch": 0.000776706538395889,
      "grad_norm": 8994.183009034228,
      "learning_rate": 2.1626892337871825e-07,
      "loss": 1.7266,
      "step": 214016
    },
    {
      "epoch": 0.0007768226727467258,
      "grad_norm": 10763.732809764464,
      "learning_rate": 2.1625274058158927e-07,
      "loss": 1.7219,
      "step": 214048
    },
    {
      "epoch": 0.0007769388070975624,
      "grad_norm": 9152.625415693576,
      "learning_rate": 2.1623656141665673e-07,
      "loss": 1.7333,
      "step": 214080
    },
    {
      "epoch": 0.0007770549414483992,
      "grad_norm": 9801.705769915765,
      "learning_rate": 2.1622038588256204e-07,
      "loss": 1.7176,
      "step": 214112
    },
    {
      "epoch": 0.0007771710757992359,
      "grad_norm": 11057.574598437037,
      "learning_rate": 2.1620421397794747e-07,
      "loss": 1.7414,
      "step": 214144
    },
    {
      "epoch": 0.0007772872101500726,
      "grad_norm": 9885.209760040501,
      "learning_rate": 2.1618804570145586e-07,
      "loss": 1.725,
      "step": 214176
    },
    {
      "epoch": 0.0007774033445009093,
      "grad_norm": 9629.795428772099,
      "learning_rate": 2.1617188105173085e-07,
      "loss": 1.7154,
      "step": 214208
    },
    {
      "epoch": 0.000777519478851746,
      "grad_norm": 8222.486606860482,
      "learning_rate": 2.1615572002741672e-07,
      "loss": 1.6858,
      "step": 214240
    },
    {
      "epoch": 0.0007776356132025827,
      "grad_norm": 8820.724800151062,
      "learning_rate": 2.1613956262715852e-07,
      "loss": 1.7014,
      "step": 214272
    },
    {
      "epoch": 0.0007777517475534194,
      "grad_norm": 10418.337679303739,
      "learning_rate": 2.1612340884960195e-07,
      "loss": 1.7143,
      "step": 214304
    },
    {
      "epoch": 0.0007778678819042561,
      "grad_norm": 10787.005145080817,
      "learning_rate": 2.1610725869339348e-07,
      "loss": 1.718,
      "step": 214336
    },
    {
      "epoch": 0.0007779840162550928,
      "grad_norm": 9805.57004972174,
      "learning_rate": 2.1609111215718023e-07,
      "loss": 1.7139,
      "step": 214368
    },
    {
      "epoch": 0.0007781001506059295,
      "grad_norm": 8621.722217747449,
      "learning_rate": 2.1607496923961008e-07,
      "loss": 1.7184,
      "step": 214400
    },
    {
      "epoch": 0.0007782162849567663,
      "grad_norm": 9492.447313522473,
      "learning_rate": 2.160588299393316e-07,
      "loss": 1.7036,
      "step": 214432
    },
    {
      "epoch": 0.0007783324193076029,
      "grad_norm": 8411.359580947661,
      "learning_rate": 2.1604269425499403e-07,
      "loss": 1.7017,
      "step": 214464
    },
    {
      "epoch": 0.0007784485536584397,
      "grad_norm": 9629.72055669322,
      "learning_rate": 2.1602656218524736e-07,
      "loss": 1.7253,
      "step": 214496
    },
    {
      "epoch": 0.0007785646880092763,
      "grad_norm": 9099.925604091497,
      "learning_rate": 2.1601043372874225e-07,
      "loss": 1.7073,
      "step": 214528
    },
    {
      "epoch": 0.0007786808223601131,
      "grad_norm": 9638.095247506117,
      "learning_rate": 2.1599430888413013e-07,
      "loss": 1.7195,
      "step": 214560
    },
    {
      "epoch": 0.0007787969567109497,
      "grad_norm": 8858.621901853583,
      "learning_rate": 2.1597818765006308e-07,
      "loss": 1.7319,
      "step": 214592
    },
    {
      "epoch": 0.0007789130910617865,
      "grad_norm": 8884.628973682582,
      "learning_rate": 2.1596207002519383e-07,
      "loss": 1.7273,
      "step": 214624
    },
    {
      "epoch": 0.0007790292254126231,
      "grad_norm": 8101.310511269149,
      "learning_rate": 2.1594595600817595e-07,
      "loss": 1.7084,
      "step": 214656
    },
    {
      "epoch": 0.0007791453597634599,
      "grad_norm": 9024.095633358504,
      "learning_rate": 2.159298455976636e-07,
      "loss": 1.7339,
      "step": 214688
    },
    {
      "epoch": 0.0007792614941142966,
      "grad_norm": 9762.670741144557,
      "learning_rate": 2.1591424207542203e-07,
      "loss": 1.7561,
      "step": 214720
    },
    {
      "epoch": 0.0007793776284651333,
      "grad_norm": 11140.323155097432,
      "learning_rate": 2.158981387612873e-07,
      "loss": 1.7205,
      "step": 214752
    },
    {
      "epoch": 0.00077949376281597,
      "grad_norm": 10278.291686851468,
      "learning_rate": 2.158820390496669e-07,
      "loss": 1.7267,
      "step": 214784
    },
    {
      "epoch": 0.0007796098971668067,
      "grad_norm": 9821.405907506318,
      "learning_rate": 2.1586594293921786e-07,
      "loss": 1.7316,
      "step": 214816
    },
    {
      "epoch": 0.0007797260315176434,
      "grad_norm": 9943.943282219585,
      "learning_rate": 2.158498504285978e-07,
      "loss": 1.7229,
      "step": 214848
    },
    {
      "epoch": 0.0007798421658684801,
      "grad_norm": 9368.015584957147,
      "learning_rate": 2.1583376151646514e-07,
      "loss": 1.7191,
      "step": 214880
    },
    {
      "epoch": 0.0007799583002193168,
      "grad_norm": 10299.309491417374,
      "learning_rate": 2.15817676201479e-07,
      "loss": 1.7389,
      "step": 214912
    },
    {
      "epoch": 0.0007800744345701535,
      "grad_norm": 8312.141120072492,
      "learning_rate": 2.1580159448229918e-07,
      "loss": 1.7179,
      "step": 214944
    },
    {
      "epoch": 0.0007801905689209902,
      "grad_norm": 9677.673687410626,
      "learning_rate": 2.157855163575861e-07,
      "loss": 1.709,
      "step": 214976
    },
    {
      "epoch": 0.000780306703271827,
      "grad_norm": 8490.477018401261,
      "learning_rate": 2.1576944182600095e-07,
      "loss": 1.7229,
      "step": 215008
    },
    {
      "epoch": 0.0007804228376226636,
      "grad_norm": 9361.04203601287,
      "learning_rate": 2.1575337088620566e-07,
      "loss": 1.7071,
      "step": 215040
    },
    {
      "epoch": 0.0007805389719735004,
      "grad_norm": 8119.997413792693,
      "learning_rate": 2.157373035368628e-07,
      "loss": 1.6975,
      "step": 215072
    },
    {
      "epoch": 0.000780655106324337,
      "grad_norm": 9608.168399856448,
      "learning_rate": 2.1572123977663564e-07,
      "loss": 1.7042,
      "step": 215104
    },
    {
      "epoch": 0.0007807712406751738,
      "grad_norm": 10092.240583735605,
      "learning_rate": 2.1570517960418815e-07,
      "loss": 1.705,
      "step": 215136
    },
    {
      "epoch": 0.0007808873750260104,
      "grad_norm": 9243.589995234535,
      "learning_rate": 2.1568912301818497e-07,
      "loss": 1.7013,
      "step": 215168
    },
    {
      "epoch": 0.0007810035093768472,
      "grad_norm": 11045.502795255632,
      "learning_rate": 2.1567307001729156e-07,
      "loss": 1.7303,
      "step": 215200
    },
    {
      "epoch": 0.0007811196437276838,
      "grad_norm": 9454.233549050923,
      "learning_rate": 2.1565702060017385e-07,
      "loss": 1.7265,
      "step": 215232
    },
    {
      "epoch": 0.0007812357780785206,
      "grad_norm": 8366.799388057538,
      "learning_rate": 2.1564097476549872e-07,
      "loss": 1.7125,
      "step": 215264
    },
    {
      "epoch": 0.0007813519124293573,
      "grad_norm": 11539.692370249739,
      "learning_rate": 2.1562493251193355e-07,
      "loss": 1.7239,
      "step": 215296
    },
    {
      "epoch": 0.000781468046780194,
      "grad_norm": 8133.530106909299,
      "learning_rate": 2.156088938381465e-07,
      "loss": 1.7381,
      "step": 215328
    },
    {
      "epoch": 0.0007815841811310307,
      "grad_norm": 8955.051758644391,
      "learning_rate": 2.155928587428064e-07,
      "loss": 1.7084,
      "step": 215360
    },
    {
      "epoch": 0.0007817003154818674,
      "grad_norm": 9678.112212616674,
      "learning_rate": 2.1557682722458282e-07,
      "loss": 1.715,
      "step": 215392
    },
    {
      "epoch": 0.0007818164498327041,
      "grad_norm": 11550.710800639066,
      "learning_rate": 2.1556079928214593e-07,
      "loss": 1.7238,
      "step": 215424
    },
    {
      "epoch": 0.0007819325841835408,
      "grad_norm": 8660.276900884866,
      "learning_rate": 2.1554477491416668e-07,
      "loss": 1.7219,
      "step": 215456
    },
    {
      "epoch": 0.0007820487185343775,
      "grad_norm": 9045.646687771969,
      "learning_rate": 2.1552875411931664e-07,
      "loss": 1.7107,
      "step": 215488
    },
    {
      "epoch": 0.0007821648528852142,
      "grad_norm": 7612.18667663898,
      "learning_rate": 2.1551273689626814e-07,
      "loss": 1.7201,
      "step": 215520
    },
    {
      "epoch": 0.0007822809872360509,
      "grad_norm": 8230.274114511618,
      "learning_rate": 2.154967232436942e-07,
      "loss": 1.7189,
      "step": 215552
    },
    {
      "epoch": 0.0007823971215868877,
      "grad_norm": 11023.070715549275,
      "learning_rate": 2.1548071316026844e-07,
      "loss": 1.7165,
      "step": 215584
    },
    {
      "epoch": 0.0007825132559377243,
      "grad_norm": 9919.689108031562,
      "learning_rate": 2.1546470664466523e-07,
      "loss": 1.7134,
      "step": 215616
    },
    {
      "epoch": 0.0007826293902885611,
      "grad_norm": 8272.368463747247,
      "learning_rate": 2.1544870369555965e-07,
      "loss": 1.7238,
      "step": 215648
    },
    {
      "epoch": 0.0007827455246393977,
      "grad_norm": 8728.499641977423,
      "learning_rate": 2.1543270431162745e-07,
      "loss": 1.7133,
      "step": 215680
    },
    {
      "epoch": 0.0007828616589902345,
      "grad_norm": 10081.26460321323,
      "learning_rate": 2.1541720830699075e-07,
      "loss": 1.7189,
      "step": 215712
    },
    {
      "epoch": 0.0007829777933410711,
      "grad_norm": 10731.707599445672,
      "learning_rate": 2.1540121593812633e-07,
      "loss": 1.7402,
      "step": 215744
    },
    {
      "epoch": 0.0007830939276919079,
      "grad_norm": 8845.865700992752,
      "learning_rate": 2.1538522713050795e-07,
      "loss": 1.7097,
      "step": 215776
    },
    {
      "epoch": 0.0007832100620427445,
      "grad_norm": 9008.091029735435,
      "learning_rate": 2.1536924188281408e-07,
      "loss": 1.7399,
      "step": 215808
    },
    {
      "epoch": 0.0007833261963935813,
      "grad_norm": 9003.332271997962,
      "learning_rate": 2.153532601937239e-07,
      "loss": 1.7323,
      "step": 215840
    },
    {
      "epoch": 0.000783442330744418,
      "grad_norm": 9895.036129292303,
      "learning_rate": 2.1533728206191724e-07,
      "loss": 1.7375,
      "step": 215872
    },
    {
      "epoch": 0.0007835584650952547,
      "grad_norm": 10309.823664835398,
      "learning_rate": 2.1532130748607465e-07,
      "loss": 1.7118,
      "step": 215904
    },
    {
      "epoch": 0.0007836745994460914,
      "grad_norm": 10430.168742642662,
      "learning_rate": 2.1530533646487734e-07,
      "loss": 1.7189,
      "step": 215936
    },
    {
      "epoch": 0.0007837907337969281,
      "grad_norm": 9482.855266215973,
      "learning_rate": 2.152893689970072e-07,
      "loss": 1.6982,
      "step": 215968
    },
    {
      "epoch": 0.0007839068681477648,
      "grad_norm": 10468.532084299117,
      "learning_rate": 2.1527340508114683e-07,
      "loss": 1.6898,
      "step": 216000
    },
    {
      "epoch": 0.0007840230024986015,
      "grad_norm": 8972.511799936516,
      "learning_rate": 2.152574447159795e-07,
      "loss": 1.7118,
      "step": 216032
    },
    {
      "epoch": 0.0007841391368494382,
      "grad_norm": 9481.339462333368,
      "learning_rate": 2.1524148790018917e-07,
      "loss": 1.7263,
      "step": 216064
    },
    {
      "epoch": 0.0007842552712002749,
      "grad_norm": 10942.846064895548,
      "learning_rate": 2.1522553463246044e-07,
      "loss": 1.7232,
      "step": 216096
    },
    {
      "epoch": 0.0007843714055511116,
      "grad_norm": 10437.337399931077,
      "learning_rate": 2.152095849114787e-07,
      "loss": 1.7012,
      "step": 216128
    },
    {
      "epoch": 0.0007844875399019484,
      "grad_norm": 9113.900591952932,
      "learning_rate": 2.1519363873592983e-07,
      "loss": 1.7199,
      "step": 216160
    },
    {
      "epoch": 0.000784603674252785,
      "grad_norm": 9054.231607375636,
      "learning_rate": 2.1517769610450066e-07,
      "loss": 1.6988,
      "step": 216192
    },
    {
      "epoch": 0.0007847198086036218,
      "grad_norm": 10891.728788397184,
      "learning_rate": 2.1516175701587845e-07,
      "loss": 1.7169,
      "step": 216224
    },
    {
      "epoch": 0.0007848359429544584,
      "grad_norm": 8890.985997064667,
      "learning_rate": 2.1514582146875131e-07,
      "loss": 1.7258,
      "step": 216256
    },
    {
      "epoch": 0.0007849520773052952,
      "grad_norm": 9589.057096503284,
      "learning_rate": 2.1512988946180788e-07,
      "loss": 1.7182,
      "step": 216288
    },
    {
      "epoch": 0.0007850682116561318,
      "grad_norm": 10865.822012162724,
      "learning_rate": 2.1511396099373767e-07,
      "loss": 1.706,
      "step": 216320
    },
    {
      "epoch": 0.0007851843460069686,
      "grad_norm": 10626.087332598016,
      "learning_rate": 2.1509803606323065e-07,
      "loss": 1.717,
      "step": 216352
    },
    {
      "epoch": 0.0007853004803578052,
      "grad_norm": 7892.931394608723,
      "learning_rate": 2.1508211466897768e-07,
      "loss": 1.7121,
      "step": 216384
    },
    {
      "epoch": 0.000785416614708642,
      "grad_norm": 11176.123478201196,
      "learning_rate": 2.1506619680967014e-07,
      "loss": 1.7249,
      "step": 216416
    },
    {
      "epoch": 0.0007855327490594788,
      "grad_norm": 9550.464805442718,
      "learning_rate": 2.1505028248400016e-07,
      "loss": 1.7393,
      "step": 216448
    },
    {
      "epoch": 0.0007856488834103154,
      "grad_norm": 12054.12427345927,
      "learning_rate": 2.1503437169066054e-07,
      "loss": 1.7378,
      "step": 216480
    },
    {
      "epoch": 0.0007857650177611522,
      "grad_norm": 8448.966564024266,
      "learning_rate": 2.1501846442834474e-07,
      "loss": 1.7192,
      "step": 216512
    },
    {
      "epoch": 0.0007858811521119888,
      "grad_norm": 9796.350544973368,
      "learning_rate": 2.1500256069574694e-07,
      "loss": 1.7105,
      "step": 216544
    },
    {
      "epoch": 0.0007859972864628256,
      "grad_norm": 10012.15970707619,
      "learning_rate": 2.1498666049156191e-07,
      "loss": 1.7279,
      "step": 216576
    },
    {
      "epoch": 0.0007861134208136622,
      "grad_norm": 8377.224122583804,
      "learning_rate": 2.1497076381448522e-07,
      "loss": 1.7167,
      "step": 216608
    },
    {
      "epoch": 0.000786229555164499,
      "grad_norm": 11332.453573697092,
      "learning_rate": 2.1495487066321297e-07,
      "loss": 1.7271,
      "step": 216640
    },
    {
      "epoch": 0.0007863456895153356,
      "grad_norm": 9971.924989689804,
      "learning_rate": 2.1493898103644206e-07,
      "loss": 1.746,
      "step": 216672
    },
    {
      "epoch": 0.0007864618238661723,
      "grad_norm": 9879.535414178139,
      "learning_rate": 2.1492309493287e-07,
      "loss": 1.7389,
      "step": 216704
    },
    {
      "epoch": 0.0007865779582170091,
      "grad_norm": 12069.702564686504,
      "learning_rate": 2.149077086285753e-07,
      "loss": 1.698,
      "step": 216736
    },
    {
      "epoch": 0.0007866940925678457,
      "grad_norm": 10592.895921323876,
      "learning_rate": 2.1489182945749727e-07,
      "loss": 1.6942,
      "step": 216768
    },
    {
      "epoch": 0.0007868102269186825,
      "grad_norm": 9465.275062035967,
      "learning_rate": 2.1487595380575532e-07,
      "loss": 1.6992,
      "step": 216800
    },
    {
      "epoch": 0.0007869263612695191,
      "grad_norm": 10166.483757917484,
      "learning_rate": 2.1486008167204966e-07,
      "loss": 1.6967,
      "step": 216832
    },
    {
      "epoch": 0.0007870424956203559,
      "grad_norm": 9851.833331923557,
      "learning_rate": 2.1484421305508114e-07,
      "loss": 1.7102,
      "step": 216864
    },
    {
      "epoch": 0.0007871586299711925,
      "grad_norm": 10552.722302799406,
      "learning_rate": 2.1482834795355132e-07,
      "loss": 1.705,
      "step": 216896
    },
    {
      "epoch": 0.0007872747643220293,
      "grad_norm": 7823.106544078254,
      "learning_rate": 2.148124863661624e-07,
      "loss": 1.7039,
      "step": 216928
    },
    {
      "epoch": 0.000787390898672866,
      "grad_norm": 10248.137196583582,
      "learning_rate": 2.1479662829161726e-07,
      "loss": 1.7189,
      "step": 216960
    },
    {
      "epoch": 0.0007875070330237027,
      "grad_norm": 8150.800083427393,
      "learning_rate": 2.1478077372861945e-07,
      "loss": 1.7342,
      "step": 216992
    },
    {
      "epoch": 0.0007876231673745395,
      "grad_norm": 9101.92045669484,
      "learning_rate": 2.1476492267587317e-07,
      "loss": 1.7027,
      "step": 217024
    },
    {
      "epoch": 0.0007877393017253761,
      "grad_norm": 9375.325274357152,
      "learning_rate": 2.1474907513208337e-07,
      "loss": 1.7367,
      "step": 217056
    },
    {
      "epoch": 0.0007878554360762129,
      "grad_norm": 8354.815377972154,
      "learning_rate": 2.1473323109595553e-07,
      "loss": 1.7383,
      "step": 217088
    },
    {
      "epoch": 0.0007879715704270495,
      "grad_norm": 10158.503826843795,
      "learning_rate": 2.1471739056619593e-07,
      "loss": 1.7235,
      "step": 217120
    },
    {
      "epoch": 0.0007880877047778863,
      "grad_norm": 8698.578734483008,
      "learning_rate": 2.1470155354151145e-07,
      "loss": 1.7011,
      "step": 217152
    },
    {
      "epoch": 0.0007882038391287229,
      "grad_norm": 10051.765218109702,
      "learning_rate": 2.1468572002060965e-07,
      "loss": 1.714,
      "step": 217184
    },
    {
      "epoch": 0.0007883199734795597,
      "grad_norm": 9127.871164734963,
      "learning_rate": 2.146698900021987e-07,
      "loss": 1.7124,
      "step": 217216
    },
    {
      "epoch": 0.0007884361078303963,
      "grad_norm": 9247.038985534775,
      "learning_rate": 2.1465406348498764e-07,
      "loss": 1.7105,
      "step": 217248
    },
    {
      "epoch": 0.0007885522421812331,
      "grad_norm": 9314.745192435486,
      "learning_rate": 2.1463824046768592e-07,
      "loss": 1.7447,
      "step": 217280
    },
    {
      "epoch": 0.0007886683765320698,
      "grad_norm": 10632.835934030018,
      "learning_rate": 2.146224209490038e-07,
      "loss": 1.7265,
      "step": 217312
    },
    {
      "epoch": 0.0007887845108829065,
      "grad_norm": 9328.21419136589,
      "learning_rate": 2.1460660492765217e-07,
      "loss": 1.6971,
      "step": 217344
    },
    {
      "epoch": 0.0007889006452337432,
      "grad_norm": 10419.717270636473,
      "learning_rate": 2.1459079240234256e-07,
      "loss": 1.7046,
      "step": 217376
    },
    {
      "epoch": 0.0007890167795845799,
      "grad_norm": 9082.340116952239,
      "learning_rate": 2.1457498337178724e-07,
      "loss": 1.7197,
      "step": 217408
    },
    {
      "epoch": 0.0007891329139354166,
      "grad_norm": 9674.31093153409,
      "learning_rate": 2.1455917783469908e-07,
      "loss": 1.7065,
      "step": 217440
    },
    {
      "epoch": 0.0007892490482862533,
      "grad_norm": 9473.000791723814,
      "learning_rate": 2.1454337578979163e-07,
      "loss": 1.7367,
      "step": 217472
    },
    {
      "epoch": 0.00078936518263709,
      "grad_norm": 8318.342142518544,
      "learning_rate": 2.1452757723577909e-07,
      "loss": 1.7325,
      "step": 217504
    },
    {
      "epoch": 0.0007894813169879267,
      "grad_norm": 9073.32728385789,
      "learning_rate": 2.1451178217137632e-07,
      "loss": 1.7323,
      "step": 217536
    },
    {
      "epoch": 0.0007895974513387634,
      "grad_norm": 9516.927445347053,
      "learning_rate": 2.1449599059529892e-07,
      "loss": 1.7321,
      "step": 217568
    },
    {
      "epoch": 0.0007897135856896002,
      "grad_norm": 9380.73259399286,
      "learning_rate": 2.1448020250626306e-07,
      "loss": 1.7213,
      "step": 217600
    },
    {
      "epoch": 0.0007898297200404368,
      "grad_norm": 9307.946819787918,
      "learning_rate": 2.1446441790298557e-07,
      "loss": 1.7077,
      "step": 217632
    },
    {
      "epoch": 0.0007899458543912736,
      "grad_norm": 7790.023491620548,
      "learning_rate": 2.14448636784184e-07,
      "loss": 1.7173,
      "step": 217664
    },
    {
      "epoch": 0.0007900619887421102,
      "grad_norm": 9509.489996839999,
      "learning_rate": 2.1443285914857656e-07,
      "loss": 1.7304,
      "step": 217696
    },
    {
      "epoch": 0.000790178123092947,
      "grad_norm": 8588.911921774492,
      "learning_rate": 2.14417577884493e-07,
      "loss": 1.7087,
      "step": 217728
    },
    {
      "epoch": 0.0007902942574437836,
      "grad_norm": 10247.039182124756,
      "learning_rate": 2.1440180710268051e-07,
      "loss": 1.6868,
      "step": 217760
    },
    {
      "epoch": 0.0007904103917946204,
      "grad_norm": 9567.611823229452,
      "learning_rate": 2.1438603980026064e-07,
      "loss": 1.705,
      "step": 217792
    },
    {
      "epoch": 0.000790526526145457,
      "grad_norm": 10060.181708100505,
      "learning_rate": 2.1437027597595414e-07,
      "loss": 1.7285,
      "step": 217824
    },
    {
      "epoch": 0.0007906426604962938,
      "grad_norm": 12340.080226643584,
      "learning_rate": 2.1435451562848254e-07,
      "loss": 1.715,
      "step": 217856
    },
    {
      "epoch": 0.0007907587948471305,
      "grad_norm": 8198.489860943904,
      "learning_rate": 2.1433875875656786e-07,
      "loss": 1.721,
      "step": 217888
    },
    {
      "epoch": 0.0007908749291979672,
      "grad_norm": 10015.07563625957,
      "learning_rate": 2.1432300535893294e-07,
      "loss": 1.7195,
      "step": 217920
    },
    {
      "epoch": 0.0007909910635488039,
      "grad_norm": 8846.829601614354,
      "learning_rate": 2.143072554343012e-07,
      "loss": 1.7096,
      "step": 217952
    },
    {
      "epoch": 0.0007911071978996406,
      "grad_norm": 7739.941860246755,
      "learning_rate": 2.142915089813967e-07,
      "loss": 1.7032,
      "step": 217984
    },
    {
      "epoch": 0.0007912233322504773,
      "grad_norm": 9408.661222511946,
      "learning_rate": 2.142757659989442e-07,
      "loss": 1.716,
      "step": 218016
    },
    {
      "epoch": 0.000791339466601314,
      "grad_norm": 10027.85600215719,
      "learning_rate": 2.142600264856691e-07,
      "loss": 1.7108,
      "step": 218048
    },
    {
      "epoch": 0.0007914556009521507,
      "grad_norm": 9251.754644390436,
      "learning_rate": 2.1424429044029748e-07,
      "loss": 1.7133,
      "step": 218080
    },
    {
      "epoch": 0.0007915717353029874,
      "grad_norm": 9478.266297166376,
      "learning_rate": 2.14228557861556e-07,
      "loss": 1.7268,
      "step": 218112
    },
    {
      "epoch": 0.0007916878696538241,
      "grad_norm": 11259.08770727007,
      "learning_rate": 2.1421282874817202e-07,
      "loss": 1.7374,
      "step": 218144
    },
    {
      "epoch": 0.0007918040040046609,
      "grad_norm": 9471.739650138194,
      "learning_rate": 2.1419710309887365e-07,
      "loss": 1.7034,
      "step": 218176
    },
    {
      "epoch": 0.0007919201383554975,
      "grad_norm": 8829.8373710958,
      "learning_rate": 2.1418138091238943e-07,
      "loss": 1.7117,
      "step": 218208
    },
    {
      "epoch": 0.0007920362727063343,
      "grad_norm": 9011.461146784133,
      "learning_rate": 2.1416566218744877e-07,
      "loss": 1.7347,
      "step": 218240
    },
    {
      "epoch": 0.0007921524070571709,
      "grad_norm": 10125.785302878981,
      "learning_rate": 2.1414994692278163e-07,
      "loss": 1.7145,
      "step": 218272
    },
    {
      "epoch": 0.0007922685414080077,
      "grad_norm": 8439.575937213907,
      "learning_rate": 2.1413423511711862e-07,
      "loss": 1.7332,
      "step": 218304
    },
    {
      "epoch": 0.0007923846757588443,
      "grad_norm": 9305.5507091198,
      "learning_rate": 2.1411852676919106e-07,
      "loss": 1.7296,
      "step": 218336
    },
    {
      "epoch": 0.0007925008101096811,
      "grad_norm": 9920.525187710578,
      "learning_rate": 2.1410282187773083e-07,
      "loss": 1.7278,
      "step": 218368
    },
    {
      "epoch": 0.0007926169444605177,
      "grad_norm": 12990.42201008112,
      "learning_rate": 2.1408712044147057e-07,
      "loss": 1.7172,
      "step": 218400
    },
    {
      "epoch": 0.0007927330788113545,
      "grad_norm": 8960.356466123432,
      "learning_rate": 2.1407142245914345e-07,
      "loss": 1.748,
      "step": 218432
    },
    {
      "epoch": 0.0007928492131621912,
      "grad_norm": 10767.932763534513,
      "learning_rate": 2.140557279294834e-07,
      "loss": 1.7166,
      "step": 218464
    },
    {
      "epoch": 0.0007929653475130279,
      "grad_norm": 7511.748265217626,
      "learning_rate": 2.1404003685122497e-07,
      "loss": 1.6944,
      "step": 218496
    },
    {
      "epoch": 0.0007930814818638646,
      "grad_norm": 8855.55960964636,
      "learning_rate": 2.1402434922310327e-07,
      "loss": 1.7069,
      "step": 218528
    },
    {
      "epoch": 0.0007931976162147013,
      "grad_norm": 9618.23684466129,
      "learning_rate": 2.140086650438542e-07,
      "loss": 1.7111,
      "step": 218560
    },
    {
      "epoch": 0.000793313750565538,
      "grad_norm": 8595.184465734286,
      "learning_rate": 2.1399298431221422e-07,
      "loss": 1.6854,
      "step": 218592
    },
    {
      "epoch": 0.0007934298849163747,
      "grad_norm": 10502.918641977572,
      "learning_rate": 2.1397730702692044e-07,
      "loss": 1.6905,
      "step": 218624
    },
    {
      "epoch": 0.0007935460192672114,
      "grad_norm": 9601.474053498245,
      "learning_rate": 2.1396163318671066e-07,
      "loss": 1.7089,
      "step": 218656
    },
    {
      "epoch": 0.0007936621536180481,
      "grad_norm": 8717.161579321564,
      "learning_rate": 2.139459627903233e-07,
      "loss": 1.7084,
      "step": 218688
    },
    {
      "epoch": 0.0007937782879688848,
      "grad_norm": 21423.985436888255,
      "learning_rate": 2.139302958364974e-07,
      "loss": 1.7446,
      "step": 218720
    },
    {
      "epoch": 0.0007938944223197216,
      "grad_norm": 9539.835428349905,
      "learning_rate": 2.1391512175666158e-07,
      "loss": 1.7227,
      "step": 218752
    },
    {
      "epoch": 0.0007940105566705582,
      "grad_norm": 9212.168691464567,
      "learning_rate": 2.1389946157669622e-07,
      "loss": 1.7174,
      "step": 218784
    },
    {
      "epoch": 0.000794126691021395,
      "grad_norm": 9422.160261850782,
      "learning_rate": 2.1388380483555273e-07,
      "loss": 1.7184,
      "step": 218816
    },
    {
      "epoch": 0.0007942428253722316,
      "grad_norm": 9512.358908283475,
      "learning_rate": 2.138681515319728e-07,
      "loss": 1.7293,
      "step": 218848
    },
    {
      "epoch": 0.0007943589597230684,
      "grad_norm": 11090.805922023881,
      "learning_rate": 2.1385250166469867e-07,
      "loss": 1.7213,
      "step": 218880
    },
    {
      "epoch": 0.000794475094073905,
      "grad_norm": 8604.653857070603,
      "learning_rate": 2.1383685523247324e-07,
      "loss": 1.7089,
      "step": 218912
    },
    {
      "epoch": 0.0007945912284247418,
      "grad_norm": 10028.791153474082,
      "learning_rate": 2.1382121223404012e-07,
      "loss": 1.72,
      "step": 218944
    },
    {
      "epoch": 0.0007947073627755784,
      "grad_norm": 7919.312848978755,
      "learning_rate": 2.138055726681435e-07,
      "loss": 1.7282,
      "step": 218976
    },
    {
      "epoch": 0.0007948234971264152,
      "grad_norm": 9814.748697750747,
      "learning_rate": 2.1378993653352823e-07,
      "loss": 1.7113,
      "step": 219008
    },
    {
      "epoch": 0.0007949396314772519,
      "grad_norm": 9236.366385110543,
      "learning_rate": 2.1377430382893977e-07,
      "loss": 1.712,
      "step": 219040
    },
    {
      "epoch": 0.0007950557658280886,
      "grad_norm": 11091.190197629829,
      "learning_rate": 2.1375867455312433e-07,
      "loss": 1.7135,
      "step": 219072
    },
    {
      "epoch": 0.0007951719001789253,
      "grad_norm": 10969.562251977059,
      "learning_rate": 2.1374304870482863e-07,
      "loss": 1.6906,
      "step": 219104
    },
    {
      "epoch": 0.000795288034529762,
      "grad_norm": 8145.459594154279,
      "learning_rate": 2.1372742628280008e-07,
      "loss": 1.7145,
      "step": 219136
    },
    {
      "epoch": 0.0007954041688805987,
      "grad_norm": 9036.598807073378,
      "learning_rate": 2.1371180728578676e-07,
      "loss": 1.7267,
      "step": 219168
    },
    {
      "epoch": 0.0007955203032314354,
      "grad_norm": 9190.120347416567,
      "learning_rate": 2.136961917125374e-07,
      "loss": 1.7162,
      "step": 219200
    },
    {
      "epoch": 0.0007956364375822721,
      "grad_norm": 8451.154477348051,
      "learning_rate": 2.1368057956180127e-07,
      "loss": 1.72,
      "step": 219232
    },
    {
      "epoch": 0.0007957525719331088,
      "grad_norm": 10052.811149126397,
      "learning_rate": 2.1366497083232843e-07,
      "loss": 1.7389,
      "step": 219264
    },
    {
      "epoch": 0.0007958687062839455,
      "grad_norm": 9767.172774145034,
      "learning_rate": 2.136493655228694e-07,
      "loss": 1.7339,
      "step": 219296
    },
    {
      "epoch": 0.0007959848406347823,
      "grad_norm": 9182.997549819993,
      "learning_rate": 2.136337636321755e-07,
      "loss": 1.7197,
      "step": 219328
    },
    {
      "epoch": 0.0007961009749856189,
      "grad_norm": 9193.886882053748,
      "learning_rate": 2.136181651589986e-07,
      "loss": 1.7163,
      "step": 219360
    },
    {
      "epoch": 0.0007962171093364557,
      "grad_norm": 9601.782542840678,
      "learning_rate": 2.1360257010209128e-07,
      "loss": 1.7179,
      "step": 219392
    },
    {
      "epoch": 0.0007963332436872923,
      "grad_norm": 10341.0284788313,
      "learning_rate": 2.1358697846020665e-07,
      "loss": 1.7102,
      "step": 219424
    },
    {
      "epoch": 0.0007964493780381291,
      "grad_norm": 9239.985714274671,
      "learning_rate": 2.1357139023209853e-07,
      "loss": 1.7166,
      "step": 219456
    },
    {
      "epoch": 0.0007965655123889657,
      "grad_norm": 10195.632202075554,
      "learning_rate": 2.1355580541652138e-07,
      "loss": 1.7025,
      "step": 219488
    },
    {
      "epoch": 0.0007966816467398025,
      "grad_norm": 9957.1889607459,
      "learning_rate": 2.1354022401223027e-07,
      "loss": 1.6888,
      "step": 219520
    },
    {
      "epoch": 0.0007967977810906391,
      "grad_norm": 8592.158867246346,
      "learning_rate": 2.1352464601798088e-07,
      "loss": 1.7317,
      "step": 219552
    },
    {
      "epoch": 0.0007969139154414759,
      "grad_norm": 9277.689690865933,
      "learning_rate": 2.1350907143252962e-07,
      "loss": 1.7355,
      "step": 219584
    },
    {
      "epoch": 0.0007970300497923126,
      "grad_norm": 10688.574086378407,
      "learning_rate": 2.1349350025463346e-07,
      "loss": 1.7216,
      "step": 219616
    },
    {
      "epoch": 0.0007971461841431493,
      "grad_norm": 11362.592309856056,
      "learning_rate": 2.1347793248305e-07,
      "loss": 1.7077,
      "step": 219648
    },
    {
      "epoch": 0.000797262318493986,
      "grad_norm": 9406.907036853292,
      "learning_rate": 2.134623681165375e-07,
      "loss": 1.7178,
      "step": 219680
    },
    {
      "epoch": 0.0007973784528448227,
      "grad_norm": 8838.084294687396,
      "learning_rate": 2.1344680715385486e-07,
      "loss": 1.6961,
      "step": 219712
    },
    {
      "epoch": 0.0007974945871956594,
      "grad_norm": 16844.188315261737,
      "learning_rate": 2.1343124959376155e-07,
      "loss": 1.7108,
      "step": 219744
    },
    {
      "epoch": 0.0007976107215464961,
      "grad_norm": 10143.314645617575,
      "learning_rate": 2.1341618145100556e-07,
      "loss": 1.7213,
      "step": 219776
    },
    {
      "epoch": 0.0007977268558973328,
      "grad_norm": 8941.03651709353,
      "learning_rate": 2.134006305861374e-07,
      "loss": 1.7182,
      "step": 219808
    },
    {
      "epoch": 0.0007978429902481695,
      "grad_norm": 9864.158555092268,
      "learning_rate": 2.133850831201797e-07,
      "loss": 1.7076,
      "step": 219840
    },
    {
      "epoch": 0.0007979591245990062,
      "grad_norm": 10756.359235354683,
      "learning_rate": 2.133695390518944e-07,
      "loss": 1.7315,
      "step": 219872
    },
    {
      "epoch": 0.000798075258949843,
      "grad_norm": 10477.977476593467,
      "learning_rate": 2.1335399838004424e-07,
      "loss": 1.7272,
      "step": 219904
    },
    {
      "epoch": 0.0007981913933006796,
      "grad_norm": 9477.224488213837,
      "learning_rate": 2.1333846110339247e-07,
      "loss": 1.7108,
      "step": 219936
    },
    {
      "epoch": 0.0007983075276515164,
      "grad_norm": 9408.414531683859,
      "learning_rate": 2.1332292722070305e-07,
      "loss": 1.7364,
      "step": 219968
    },
    {
      "epoch": 0.000798423662002353,
      "grad_norm": 10533.6198906169,
      "learning_rate": 2.1330739673074046e-07,
      "loss": 1.7529,
      "step": 220000
    },
    {
      "epoch": 0.0007985397963531898,
      "grad_norm": 9337.23192386266,
      "learning_rate": 2.1329186963226994e-07,
      "loss": 1.7372,
      "step": 220032
    },
    {
      "epoch": 0.0007986559307040264,
      "grad_norm": 10393.456787806452,
      "learning_rate": 2.132763459240573e-07,
      "loss": 1.7266,
      "step": 220064
    },
    {
      "epoch": 0.0007987720650548632,
      "grad_norm": 11035.959043055569,
      "learning_rate": 2.1326082560486895e-07,
      "loss": 1.7421,
      "step": 220096
    },
    {
      "epoch": 0.0007988881994056998,
      "grad_norm": 10556.26581703966,
      "learning_rate": 2.1324530867347196e-07,
      "loss": 1.7266,
      "step": 220128
    },
    {
      "epoch": 0.0007990043337565366,
      "grad_norm": 8873.230527829197,
      "learning_rate": 2.1322979512863408e-07,
      "loss": 1.7375,
      "step": 220160
    },
    {
      "epoch": 0.0007991204681073733,
      "grad_norm": 9281.864036927067,
      "learning_rate": 2.1321428496912355e-07,
      "loss": 1.7553,
      "step": 220192
    },
    {
      "epoch": 0.00079923660245821,
      "grad_norm": 8676.995217239664,
      "learning_rate": 2.1319877819370936e-07,
      "loss": 1.7356,
      "step": 220224
    },
    {
      "epoch": 0.0007993527368090467,
      "grad_norm": 9746.77649276929,
      "learning_rate": 2.131832748011611e-07,
      "loss": 1.6962,
      "step": 220256
    },
    {
      "epoch": 0.0007994688711598834,
      "grad_norm": 9682.693530211518,
      "learning_rate": 2.1316777479024896e-07,
      "loss": 1.708,
      "step": 220288
    },
    {
      "epoch": 0.0007995850055107201,
      "grad_norm": 10312.545757474243,
      "learning_rate": 2.1315227815974377e-07,
      "loss": 1.7155,
      "step": 220320
    },
    {
      "epoch": 0.0007997011398615568,
      "grad_norm": 9191.664702326778,
      "learning_rate": 2.1313678490841697e-07,
      "loss": 1.712,
      "step": 220352
    },
    {
      "epoch": 0.0007998172742123935,
      "grad_norm": 10384.014252686675,
      "learning_rate": 2.1312129503504065e-07,
      "loss": 1.7271,
      "step": 220384
    },
    {
      "epoch": 0.0007999334085632302,
      "grad_norm": 9623.36770574626,
      "learning_rate": 2.131058085383875e-07,
      "loss": 1.73,
      "step": 220416
    },
    {
      "epoch": 0.0008000495429140669,
      "grad_norm": 10011.187142392255,
      "learning_rate": 2.1309032541723088e-07,
      "loss": 1.7251,
      "step": 220448
    },
    {
      "epoch": 0.0008001656772649037,
      "grad_norm": 8905.739609936954,
      "learning_rate": 2.130748456703447e-07,
      "loss": 1.7166,
      "step": 220480
    },
    {
      "epoch": 0.0008002818116157403,
      "grad_norm": 9536.83133960122,
      "learning_rate": 2.1305936929650356e-07,
      "loss": 1.718,
      "step": 220512
    },
    {
      "epoch": 0.0008003979459665771,
      "grad_norm": 8717.042503051136,
      "learning_rate": 2.1304389629448266e-07,
      "loss": 1.7078,
      "step": 220544
    },
    {
      "epoch": 0.0008005140803174137,
      "grad_norm": 8002.640689172544,
      "learning_rate": 2.130284266630578e-07,
      "loss": 1.7231,
      "step": 220576
    },
    {
      "epoch": 0.0008006302146682505,
      "grad_norm": 9414.345436619586,
      "learning_rate": 2.1301296040100543e-07,
      "loss": 1.7336,
      "step": 220608
    },
    {
      "epoch": 0.0008007463490190871,
      "grad_norm": 7785.501139939548,
      "learning_rate": 2.1299749750710263e-07,
      "loss": 1.7391,
      "step": 220640
    },
    {
      "epoch": 0.0008008624833699239,
      "grad_norm": 10571.161714778562,
      "learning_rate": 2.1298203798012705e-07,
      "loss": 1.7033,
      "step": 220672
    },
    {
      "epoch": 0.0008009786177207605,
      "grad_norm": 10048.563877490156,
      "learning_rate": 2.1296658181885702e-07,
      "loss": 1.7119,
      "step": 220704
    },
    {
      "epoch": 0.0008010947520715973,
      "grad_norm": 8541.276719554284,
      "learning_rate": 2.1295112902207149e-07,
      "loss": 1.7233,
      "step": 220736
    },
    {
      "epoch": 0.000801210886422434,
      "grad_norm": 10065.203823072834,
      "learning_rate": 2.1293616233245085e-07,
      "loss": 1.7035,
      "step": 220768
    },
    {
      "epoch": 0.0008013270207732707,
      "grad_norm": 8602.732124156837,
      "learning_rate": 2.1292071615592808e-07,
      "loss": 1.7298,
      "step": 220800
    },
    {
      "epoch": 0.0008014431551241075,
      "grad_norm": 9627.18941332308,
      "learning_rate": 2.1290527334026838e-07,
      "loss": 1.7337,
      "step": 220832
    },
    {
      "epoch": 0.0008015592894749441,
      "grad_norm": 11473.224481374014,
      "learning_rate": 2.1288983388425314e-07,
      "loss": 1.7091,
      "step": 220864
    },
    {
      "epoch": 0.0008016754238257809,
      "grad_norm": 8982.643931493667,
      "learning_rate": 2.1287439778666437e-07,
      "loss": 1.7057,
      "step": 220896
    },
    {
      "epoch": 0.0008017915581766175,
      "grad_norm": 11008.331572041241,
      "learning_rate": 2.1285896504628465e-07,
      "loss": 1.7304,
      "step": 220928
    },
    {
      "epoch": 0.0008019076925274543,
      "grad_norm": 9909.69787632297,
      "learning_rate": 2.1284353566189725e-07,
      "loss": 1.7126,
      "step": 220960
    },
    {
      "epoch": 0.0008020238268782909,
      "grad_norm": 9789.230408974956,
      "learning_rate": 2.12828109632286e-07,
      "loss": 1.7465,
      "step": 220992
    },
    {
      "epoch": 0.0008021399612291277,
      "grad_norm": 9337.463467130674,
      "learning_rate": 2.1281268695623544e-07,
      "loss": 1.7409,
      "step": 221024
    },
    {
      "epoch": 0.0008022560955799643,
      "grad_norm": 8888.922994379016,
      "learning_rate": 2.127972676325306e-07,
      "loss": 1.7367,
      "step": 221056
    },
    {
      "epoch": 0.000802372229930801,
      "grad_norm": 10194.611125491741,
      "learning_rate": 2.1278185165995722e-07,
      "loss": 1.7129,
      "step": 221088
    },
    {
      "epoch": 0.0008024883642816378,
      "grad_norm": 10212.69484514249,
      "learning_rate": 2.1276643903730158e-07,
      "loss": 1.7147,
      "step": 221120
    },
    {
      "epoch": 0.0008026044986324745,
      "grad_norm": 10758.240190663155,
      "learning_rate": 2.1275102976335064e-07,
      "loss": 1.7197,
      "step": 221152
    },
    {
      "epoch": 0.0008027206329833112,
      "grad_norm": 10233.702164905915,
      "learning_rate": 2.1273562383689197e-07,
      "loss": 1.7241,
      "step": 221184
    },
    {
      "epoch": 0.0008028367673341479,
      "grad_norm": 10286.313333745962,
      "learning_rate": 2.127202212567137e-07,
      "loss": 1.7419,
      "step": 221216
    },
    {
      "epoch": 0.0008029529016849846,
      "grad_norm": 12129.384155842374,
      "learning_rate": 2.1270482202160467e-07,
      "loss": 1.7236,
      "step": 221248
    },
    {
      "epoch": 0.0008030690360358213,
      "grad_norm": 9681.79074345237,
      "learning_rate": 2.126894261303542e-07,
      "loss": 1.6998,
      "step": 221280
    },
    {
      "epoch": 0.000803185170386658,
      "grad_norm": 9842.765058661107,
      "learning_rate": 2.1267403358175237e-07,
      "loss": 1.7157,
      "step": 221312
    },
    {
      "epoch": 0.0008033013047374947,
      "grad_norm": 9397.848477178168,
      "learning_rate": 2.1265864437458976e-07,
      "loss": 1.7352,
      "step": 221344
    },
    {
      "epoch": 0.0008034174390883314,
      "grad_norm": 10621.786478742642,
      "learning_rate": 2.126432585076576e-07,
      "loss": 1.7057,
      "step": 221376
    },
    {
      "epoch": 0.0008035335734391682,
      "grad_norm": 10040.999751020812,
      "learning_rate": 2.1262787597974775e-07,
      "loss": 1.7371,
      "step": 221408
    },
    {
      "epoch": 0.0008036497077900048,
      "grad_norm": 9579.370960558945,
      "learning_rate": 2.1261249678965266e-07,
      "loss": 1.7198,
      "step": 221440
    },
    {
      "epoch": 0.0008037658421408416,
      "grad_norm": 10556.258617521646,
      "learning_rate": 2.125971209361654e-07,
      "loss": 1.7039,
      "step": 221472
    },
    {
      "epoch": 0.0008038819764916782,
      "grad_norm": 9709.591134543205,
      "learning_rate": 2.1258174841807964e-07,
      "loss": 1.7026,
      "step": 221504
    },
    {
      "epoch": 0.000803998110842515,
      "grad_norm": 8931.06040736485,
      "learning_rate": 2.1256637923418968e-07,
      "loss": 1.7073,
      "step": 221536
    },
    {
      "epoch": 0.0008041142451933516,
      "grad_norm": 10952.873412945117,
      "learning_rate": 2.1255101338329043e-07,
      "loss": 1.7049,
      "step": 221568
    },
    {
      "epoch": 0.0008042303795441884,
      "grad_norm": 11441.885683749859,
      "learning_rate": 2.125356508641774e-07,
      "loss": 1.7094,
      "step": 221600
    },
    {
      "epoch": 0.000804346513895025,
      "grad_norm": 9861.130462578822,
      "learning_rate": 2.1252029167564665e-07,
      "loss": 1.7192,
      "step": 221632
    },
    {
      "epoch": 0.0008044626482458618,
      "grad_norm": 10215.978269358251,
      "learning_rate": 2.12504935816495e-07,
      "loss": 1.7205,
      "step": 221664
    },
    {
      "epoch": 0.0008045787825966985,
      "grad_norm": 8886.780856980777,
      "learning_rate": 2.1248958328551973e-07,
      "loss": 1.7099,
      "step": 221696
    },
    {
      "epoch": 0.0008046949169475352,
      "grad_norm": 10303.877910767382,
      "learning_rate": 2.1247423408151877e-07,
      "loss": 1.72,
      "step": 221728
    },
    {
      "epoch": 0.0008048110512983719,
      "grad_norm": 8220.192455167944,
      "learning_rate": 2.12459367711656e-07,
      "loss": 1.7383,
      "step": 221760
    },
    {
      "epoch": 0.0008049271856492086,
      "grad_norm": 9787.117655367181,
      "learning_rate": 2.124440250541253e-07,
      "loss": 1.7351,
      "step": 221792
    },
    {
      "epoch": 0.0008050433200000453,
      "grad_norm": 9411.673177496124,
      "learning_rate": 2.124286857200039e-07,
      "loss": 1.7556,
      "step": 221824
    },
    {
      "epoch": 0.000805159454350882,
      "grad_norm": 9167.85100227965,
      "learning_rate": 2.1241334970809218e-07,
      "loss": 1.7477,
      "step": 221856
    },
    {
      "epoch": 0.0008052755887017187,
      "grad_norm": 11458.970634398189,
      "learning_rate": 2.1239801701719107e-07,
      "loss": 1.7408,
      "step": 221888
    },
    {
      "epoch": 0.0008053917230525554,
      "grad_norm": 9139.710061046795,
      "learning_rate": 2.1238268764610218e-07,
      "loss": 1.7305,
      "step": 221920
    },
    {
      "epoch": 0.0008055078574033921,
      "grad_norm": 10586.9655709273,
      "learning_rate": 2.1236736159362763e-07,
      "loss": 1.7401,
      "step": 221952
    },
    {
      "epoch": 0.0008056239917542289,
      "grad_norm": 9727.408082320799,
      "learning_rate": 2.1235203885857022e-07,
      "loss": 1.7235,
      "step": 221984
    },
    {
      "epoch": 0.0008057401261050655,
      "grad_norm": 9532.164706927802,
      "learning_rate": 2.1233671943973333e-07,
      "loss": 1.7067,
      "step": 222016
    },
    {
      "epoch": 0.0008058562604559023,
      "grad_norm": 8272.515457827807,
      "learning_rate": 2.1232140333592095e-07,
      "loss": 1.7199,
      "step": 222048
    },
    {
      "epoch": 0.0008059723948067389,
      "grad_norm": 10036.906894058548,
      "learning_rate": 2.1230609054593767e-07,
      "loss": 1.7194,
      "step": 222080
    },
    {
      "epoch": 0.0008060885291575757,
      "grad_norm": 9935.649349690235,
      "learning_rate": 2.1229078106858866e-07,
      "loss": 1.6988,
      "step": 222112
    },
    {
      "epoch": 0.0008062046635084123,
      "grad_norm": 9653.490560413886,
      "learning_rate": 2.1227547490267972e-07,
      "loss": 1.7064,
      "step": 222144
    },
    {
      "epoch": 0.0008063207978592491,
      "grad_norm": 10967.959700874178,
      "learning_rate": 2.122601720470173e-07,
      "loss": 1.727,
      "step": 222176
    },
    {
      "epoch": 0.0008064369322100857,
      "grad_norm": 9601.751715181976,
      "learning_rate": 2.1224487250040834e-07,
      "loss": 1.7085,
      "step": 222208
    },
    {
      "epoch": 0.0008065530665609225,
      "grad_norm": 9997.327742952113,
      "learning_rate": 2.1222957626166045e-07,
      "loss": 1.7303,
      "step": 222240
    },
    {
      "epoch": 0.0008066692009117592,
      "grad_norm": 9425.83089175697,
      "learning_rate": 2.1221428332958186e-07,
      "loss": 1.7218,
      "step": 222272
    },
    {
      "epoch": 0.0008067853352625959,
      "grad_norm": 7819.677614838095,
      "learning_rate": 2.1219899370298137e-07,
      "loss": 1.7204,
      "step": 222304
    },
    {
      "epoch": 0.0008069014696134326,
      "grad_norm": 11455.12531577023,
      "learning_rate": 2.1218370738066834e-07,
      "loss": 1.7099,
      "step": 222336
    },
    {
      "epoch": 0.0008070176039642693,
      "grad_norm": 9165.277518984354,
      "learning_rate": 2.1216842436145286e-07,
      "loss": 1.7259,
      "step": 222368
    },
    {
      "epoch": 0.000807133738315106,
      "grad_norm": 8977.685670594621,
      "learning_rate": 2.1215314464414545e-07,
      "loss": 1.7263,
      "step": 222400
    },
    {
      "epoch": 0.0008072498726659427,
      "grad_norm": 11214.676277093335,
      "learning_rate": 2.1213786822755737e-07,
      "loss": 1.7085,
      "step": 222432
    },
    {
      "epoch": 0.0008073660070167794,
      "grad_norm": 10463.348030147903,
      "learning_rate": 2.1212259511050038e-07,
      "loss": 1.7187,
      "step": 222464
    },
    {
      "epoch": 0.0008074821413676161,
      "grad_norm": 8528.742814741221,
      "learning_rate": 2.1210732529178694e-07,
      "loss": 1.7249,
      "step": 222496
    },
    {
      "epoch": 0.0008075982757184528,
      "grad_norm": 10662.015756881998,
      "learning_rate": 2.1209205877023e-07,
      "loss": 1.6926,
      "step": 222528
    },
    {
      "epoch": 0.0008077144100692896,
      "grad_norm": 10751.792594725774,
      "learning_rate": 2.120767955446432e-07,
      "loss": 1.7107,
      "step": 222560
    },
    {
      "epoch": 0.0008078305444201262,
      "grad_norm": 9846.245985145812,
      "learning_rate": 2.1206153561384066e-07,
      "loss": 1.725,
      "step": 222592
    },
    {
      "epoch": 0.000807946678770963,
      "grad_norm": 9083.785774664659,
      "learning_rate": 2.1204627897663727e-07,
      "loss": 1.7042,
      "step": 222624
    },
    {
      "epoch": 0.0008080628131217996,
      "grad_norm": 10638.551217153585,
      "learning_rate": 2.1203102563184837e-07,
      "loss": 1.7353,
      "step": 222656
    },
    {
      "epoch": 0.0008081789474726364,
      "grad_norm": 9018.546667839559,
      "learning_rate": 2.1201577557828998e-07,
      "loss": 1.7319,
      "step": 222688
    },
    {
      "epoch": 0.000808295081823473,
      "grad_norm": 9842.438925388362,
      "learning_rate": 2.1200052881477862e-07,
      "loss": 1.736,
      "step": 222720
    },
    {
      "epoch": 0.0008084112161743098,
      "grad_norm": 10004.787553966351,
      "learning_rate": 2.119852853401315e-07,
      "loss": 1.7336,
      "step": 222752
    },
    {
      "epoch": 0.0008085273505251464,
      "grad_norm": 12047.453506861937,
      "learning_rate": 2.1197052135925613e-07,
      "loss": 1.747,
      "step": 222784
    },
    {
      "epoch": 0.0008086434848759832,
      "grad_norm": 10556.590358633794,
      "learning_rate": 2.119552843561062e-07,
      "loss": 1.7419,
      "step": 222816
    },
    {
      "epoch": 0.0008087596192268199,
      "grad_norm": 9046.93119239889,
      "learning_rate": 2.1194005063831251e-07,
      "loss": 1.7254,
      "step": 222848
    },
    {
      "epoch": 0.0008088757535776566,
      "grad_norm": 9257.890688488387,
      "learning_rate": 2.119248202046946e-07,
      "loss": 1.7368,
      "step": 222880
    },
    {
      "epoch": 0.0008089918879284933,
      "grad_norm": 8742.347853980646,
      "learning_rate": 2.1190959305407263e-07,
      "loss": 1.7357,
      "step": 222912
    },
    {
      "epoch": 0.00080910802227933,
      "grad_norm": 9848.255886196297,
      "learning_rate": 2.1189436918526733e-07,
      "loss": 1.7125,
      "step": 222944
    },
    {
      "epoch": 0.0008092241566301667,
      "grad_norm": 10041.448102738967,
      "learning_rate": 2.1187914859709996e-07,
      "loss": 1.7316,
      "step": 222976
    },
    {
      "epoch": 0.0008093402909810034,
      "grad_norm": 12761.34804791406,
      "learning_rate": 2.1186393128839247e-07,
      "loss": 1.719,
      "step": 223008
    },
    {
      "epoch": 0.0008094564253318401,
      "grad_norm": 9946.91228472434,
      "learning_rate": 2.118487172579674e-07,
      "loss": 1.7032,
      "step": 223040
    },
    {
      "epoch": 0.0008095725596826768,
      "grad_norm": 9179.009096847001,
      "learning_rate": 2.1183350650464778e-07,
      "loss": 1.7318,
      "step": 223072
    },
    {
      "epoch": 0.0008096886940335135,
      "grad_norm": 9021.244703476345,
      "learning_rate": 2.1181829902725733e-07,
      "loss": 1.7299,
      "step": 223104
    },
    {
      "epoch": 0.0008098048283843503,
      "grad_norm": 9516.435151883294,
      "learning_rate": 2.1180309482462033e-07,
      "loss": 1.7062,
      "step": 223136
    },
    {
      "epoch": 0.0008099209627351869,
      "grad_norm": 9925.426741455502,
      "learning_rate": 2.1178789389556163e-07,
      "loss": 1.7042,
      "step": 223168
    },
    {
      "epoch": 0.0008100370970860237,
      "grad_norm": 9034.862810247867,
      "learning_rate": 2.117726962389067e-07,
      "loss": 1.715,
      "step": 223200
    },
    {
      "epoch": 0.0008101532314368603,
      "grad_norm": 9649.61408554767,
      "learning_rate": 2.117575018534816e-07,
      "loss": 1.7023,
      "step": 223232
    },
    {
      "epoch": 0.0008102693657876971,
      "grad_norm": 10788.211158482207,
      "learning_rate": 2.1174231073811296e-07,
      "loss": 1.7113,
      "step": 223264
    },
    {
      "epoch": 0.0008103855001385337,
      "grad_norm": 9322.94352659073,
      "learning_rate": 2.1172712289162798e-07,
      "loss": 1.7183,
      "step": 223296
    },
    {
      "epoch": 0.0008105016344893705,
      "grad_norm": 10231.931684682027,
      "learning_rate": 2.1171193831285453e-07,
      "loss": 1.7192,
      "step": 223328
    },
    {
      "epoch": 0.0008106177688402071,
      "grad_norm": 9880.291696098855,
      "learning_rate": 2.11696757000621e-07,
      "loss": 1.7013,
      "step": 223360
    },
    {
      "epoch": 0.0008107339031910439,
      "grad_norm": 9765.825720337221,
      "learning_rate": 2.1168157895375633e-07,
      "loss": 1.7009,
      "step": 223392
    },
    {
      "epoch": 0.0008108500375418806,
      "grad_norm": 9515.187018656017,
      "learning_rate": 2.1166640417109017e-07,
      "loss": 1.7125,
      "step": 223424
    },
    {
      "epoch": 0.0008109661718927173,
      "grad_norm": 9299.133723094856,
      "learning_rate": 2.1165123265145263e-07,
      "loss": 1.7046,
      "step": 223456
    },
    {
      "epoch": 0.000811082306243554,
      "grad_norm": 10076.246622626899,
      "learning_rate": 2.116360643936745e-07,
      "loss": 1.7459,
      "step": 223488
    },
    {
      "epoch": 0.0008111984405943907,
      "grad_norm": 9122.918940777672,
      "learning_rate": 2.1162089939658712e-07,
      "loss": 1.7423,
      "step": 223520
    },
    {
      "epoch": 0.0008113145749452274,
      "grad_norm": 9294.42886895155,
      "learning_rate": 2.1160573765902239e-07,
      "loss": 1.7371,
      "step": 223552
    },
    {
      "epoch": 0.0008114307092960641,
      "grad_norm": 9719.421381954791,
      "learning_rate": 2.1159057917981285e-07,
      "loss": 1.7457,
      "step": 223584
    },
    {
      "epoch": 0.0008115468436469008,
      "grad_norm": 7571.719619742928,
      "learning_rate": 2.115754239577916e-07,
      "loss": 1.7433,
      "step": 223616
    },
    {
      "epoch": 0.0008116629779977375,
      "grad_norm": 8372.741844820011,
      "learning_rate": 2.115602719917923e-07,
      "loss": 1.7331,
      "step": 223648
    },
    {
      "epoch": 0.0008117791123485742,
      "grad_norm": 10600.829024184855,
      "learning_rate": 2.1154512328064923e-07,
      "loss": 1.7454,
      "step": 223680
    },
    {
      "epoch": 0.000811895246699411,
      "grad_norm": 10431.698231831671,
      "learning_rate": 2.1152997782319724e-07,
      "loss": 1.7429,
      "step": 223712
    },
    {
      "epoch": 0.0008120113810502476,
      "grad_norm": 8732.204647166716,
      "learning_rate": 2.1151483561827175e-07,
      "loss": 1.7194,
      "step": 223744
    },
    {
      "epoch": 0.0008121275154010844,
      "grad_norm": 10299.312501327455,
      "learning_rate": 2.1150016970780427e-07,
      "loss": 1.6953,
      "step": 223776
    },
    {
      "epoch": 0.000812243649751921,
      "grad_norm": 8400.308684804386,
      "learning_rate": 2.1148503390288938e-07,
      "loss": 1.7075,
      "step": 223808
    },
    {
      "epoch": 0.0008123597841027578,
      "grad_norm": 7668.224305535148,
      "learning_rate": 2.114699013470471e-07,
      "loss": 1.71,
      "step": 223840
    },
    {
      "epoch": 0.0008124759184535944,
      "grad_norm": 10286.59691054335,
      "learning_rate": 2.1145477203911522e-07,
      "loss": 1.6965,
      "step": 223872
    },
    {
      "epoch": 0.0008125920528044312,
      "grad_norm": 9368.94636552051,
      "learning_rate": 2.1143964597793207e-07,
      "loss": 1.715,
      "step": 223904
    },
    {
      "epoch": 0.0008127081871552678,
      "grad_norm": 9312.882045854549,
      "learning_rate": 2.114245231623366e-07,
      "loss": 1.7316,
      "step": 223936
    },
    {
      "epoch": 0.0008128243215061046,
      "grad_norm": 12621.083313250096,
      "learning_rate": 2.1140940359116824e-07,
      "loss": 1.7214,
      "step": 223968
    },
    {
      "epoch": 0.0008129404558569413,
      "grad_norm": 8367.30219365836,
      "learning_rate": 2.113942872632671e-07,
      "loss": 1.7094,
      "step": 224000
    },
    {
      "epoch": 0.000813056590207778,
      "grad_norm": 10024.828178078666,
      "learning_rate": 2.1137917417747384e-07,
      "loss": 1.7229,
      "step": 224032
    },
    {
      "epoch": 0.0008131727245586147,
      "grad_norm": 11292.574728555042,
      "learning_rate": 2.1136406433262966e-07,
      "loss": 1.7078,
      "step": 224064
    },
    {
      "epoch": 0.0008132888589094514,
      "grad_norm": 8597.214897860818,
      "learning_rate": 2.1134895772757642e-07,
      "loss": 1.7204,
      "step": 224096
    },
    {
      "epoch": 0.0008134049932602881,
      "grad_norm": 9774.570271884078,
      "learning_rate": 2.1133385436115646e-07,
      "loss": 1.7402,
      "step": 224128
    },
    {
      "epoch": 0.0008135211276111248,
      "grad_norm": 8280.241783909452,
      "learning_rate": 2.113187542322128e-07,
      "loss": 1.7382,
      "step": 224160
    },
    {
      "epoch": 0.0008136372619619615,
      "grad_norm": 10390.34494133857,
      "learning_rate": 2.11303657339589e-07,
      "loss": 1.704,
      "step": 224192
    },
    {
      "epoch": 0.0008137533963127982,
      "grad_norm": 8143.671039525111,
      "learning_rate": 2.1128856368212917e-07,
      "loss": 1.7247,
      "step": 224224
    },
    {
      "epoch": 0.0008138695306636349,
      "grad_norm": 11093.380729065419,
      "learning_rate": 2.11273473258678e-07,
      "loss": 1.7201,
      "step": 224256
    },
    {
      "epoch": 0.0008139856650144717,
      "grad_norm": 9421.930693865244,
      "learning_rate": 2.1125838606808077e-07,
      "loss": 1.7079,
      "step": 224288
    },
    {
      "epoch": 0.0008141017993653083,
      "grad_norm": 8658.311267216026,
      "learning_rate": 2.1124330210918339e-07,
      "loss": 1.744,
      "step": 224320
    },
    {
      "epoch": 0.0008142179337161451,
      "grad_norm": 9168.735572585785,
      "learning_rate": 2.1122822138083226e-07,
      "loss": 1.7474,
      "step": 224352
    },
    {
      "epoch": 0.0008143340680669817,
      "grad_norm": 9753.39243545547,
      "learning_rate": 2.1121314388187442e-07,
      "loss": 1.7216,
      "step": 224384
    },
    {
      "epoch": 0.0008144502024178185,
      "grad_norm": 8698.987527293048,
      "learning_rate": 2.1119806961115743e-07,
      "loss": 1.7307,
      "step": 224416
    },
    {
      "epoch": 0.0008145663367686551,
      "grad_norm": 9823.395441495777,
      "learning_rate": 2.111829985675295e-07,
      "loss": 1.7499,
      "step": 224448
    },
    {
      "epoch": 0.0008146824711194919,
      "grad_norm": 8929.79675020658,
      "learning_rate": 2.1116793074983937e-07,
      "loss": 1.73,
      "step": 224480
    },
    {
      "epoch": 0.0008147986054703285,
      "grad_norm": 10385.282663461789,
      "learning_rate": 2.111528661569363e-07,
      "loss": 1.7561,
      "step": 224512
    },
    {
      "epoch": 0.0008149147398211653,
      "grad_norm": 10451.202705909018,
      "learning_rate": 2.1113780478767026e-07,
      "loss": 1.7516,
      "step": 224544
    },
    {
      "epoch": 0.000815030874172002,
      "grad_norm": 10190.894563285403,
      "learning_rate": 2.1112274664089166e-07,
      "loss": 1.7518,
      "step": 224576
    },
    {
      "epoch": 0.0008151470085228387,
      "grad_norm": 9203.228129303327,
      "learning_rate": 2.1110769171545154e-07,
      "loss": 1.7178,
      "step": 224608
    },
    {
      "epoch": 0.0008152631428736754,
      "grad_norm": 10047.891520115054,
      "learning_rate": 2.1109264001020157e-07,
      "loss": 1.7238,
      "step": 224640
    },
    {
      "epoch": 0.0008153792772245121,
      "grad_norm": 10470.831294601208,
      "learning_rate": 2.1107759152399388e-07,
      "loss": 1.7295,
      "step": 224672
    },
    {
      "epoch": 0.0008154954115753488,
      "grad_norm": 9091.130512758025,
      "learning_rate": 2.1106254625568122e-07,
      "loss": 1.7373,
      "step": 224704
    },
    {
      "epoch": 0.0008156115459261855,
      "grad_norm": 8440.946392437285,
      "learning_rate": 2.11047504204117e-07,
      "loss": 1.7537,
      "step": 224736
    },
    {
      "epoch": 0.0008157276802770222,
      "grad_norm": 10633.337763844427,
      "learning_rate": 2.1103246536815503e-07,
      "loss": 1.7389,
      "step": 224768
    },
    {
      "epoch": 0.0008158438146278589,
      "grad_norm": 8987.425437799193,
      "learning_rate": 2.1101789956117695e-07,
      "loss": 1.7169,
      "step": 224800
    },
    {
      "epoch": 0.0008159599489786956,
      "grad_norm": 9023.708106981298,
      "learning_rate": 2.110028670525849e-07,
      "loss": 1.7337,
      "step": 224832
    },
    {
      "epoch": 0.0008160760833295324,
      "grad_norm": 13098.514419582092,
      "learning_rate": 2.1098783775619608e-07,
      "loss": 1.7209,
      "step": 224864
    },
    {
      "epoch": 0.000816192217680369,
      "grad_norm": 8828.491830431742,
      "learning_rate": 2.1097281167086655e-07,
      "loss": 1.6831,
      "step": 224896
    },
    {
      "epoch": 0.0008163083520312058,
      "grad_norm": 9505.509560249782,
      "learning_rate": 2.1095778879545317e-07,
      "loss": 1.7256,
      "step": 224928
    },
    {
      "epoch": 0.0008164244863820424,
      "grad_norm": 8611.236380450835,
      "learning_rate": 2.1094276912881316e-07,
      "loss": 1.7109,
      "step": 224960
    },
    {
      "epoch": 0.0008165406207328792,
      "grad_norm": 9535.466637768704,
      "learning_rate": 2.1092775266980445e-07,
      "loss": 1.7019,
      "step": 224992
    },
    {
      "epoch": 0.0008166567550837158,
      "grad_norm": 9812.742124401313,
      "learning_rate": 2.109127394172855e-07,
      "loss": 1.6888,
      "step": 225024
    },
    {
      "epoch": 0.0008167728894345526,
      "grad_norm": 7740.992572015555,
      "learning_rate": 2.108977293701153e-07,
      "loss": 1.7087,
      "step": 225056
    },
    {
      "epoch": 0.0008168890237853892,
      "grad_norm": 9386.412946381593,
      "learning_rate": 2.108827225271534e-07,
      "loss": 1.7071,
      "step": 225088
    },
    {
      "epoch": 0.000817005158136226,
      "grad_norm": 9496.013268735465,
      "learning_rate": 2.1086771888726e-07,
      "loss": 1.7135,
      "step": 225120
    },
    {
      "epoch": 0.0008171212924870628,
      "grad_norm": 9687.842071379982,
      "learning_rate": 2.1085271844929587e-07,
      "loss": 1.7161,
      "step": 225152
    },
    {
      "epoch": 0.0008172374268378994,
      "grad_norm": 11277.979783631465,
      "learning_rate": 2.1083772121212216e-07,
      "loss": 1.7192,
      "step": 225184
    },
    {
      "epoch": 0.0008173535611887362,
      "grad_norm": 11528.984170342155,
      "learning_rate": 2.1082272717460083e-07,
      "loss": 1.7023,
      "step": 225216
    },
    {
      "epoch": 0.0008174696955395728,
      "grad_norm": 8720.649975775888,
      "learning_rate": 2.108077363355943e-07,
      "loss": 1.7151,
      "step": 225248
    },
    {
      "epoch": 0.0008175858298904096,
      "grad_norm": 8638.311293302644,
      "learning_rate": 2.1079274869396553e-07,
      "loss": 1.7416,
      "step": 225280
    },
    {
      "epoch": 0.0008177019642412462,
      "grad_norm": 9154.502061827285,
      "learning_rate": 2.1077776424857805e-07,
      "loss": 1.7301,
      "step": 225312
    },
    {
      "epoch": 0.000817818098592083,
      "grad_norm": 9592.615284686444,
      "learning_rate": 2.1076278299829603e-07,
      "loss": 1.7599,
      "step": 225344
    },
    {
      "epoch": 0.0008179342329429196,
      "grad_norm": 8292.331155953674,
      "learning_rate": 2.1074780494198413e-07,
      "loss": 1.7428,
      "step": 225376
    },
    {
      "epoch": 0.0008180503672937564,
      "grad_norm": 9547.962924100617,
      "learning_rate": 2.107328300785076e-07,
      "loss": 1.7314,
      "step": 225408
    },
    {
      "epoch": 0.0008181665016445931,
      "grad_norm": 9366.340587443956,
      "learning_rate": 2.1071785840673226e-07,
      "loss": 1.7196,
      "step": 225440
    },
    {
      "epoch": 0.0008182826359954298,
      "grad_norm": 10209.019541562255,
      "learning_rate": 2.1070288992552447e-07,
      "loss": 1.7218,
      "step": 225472
    },
    {
      "epoch": 0.0008183987703462665,
      "grad_norm": 10124.152902835871,
      "learning_rate": 2.1068792463375117e-07,
      "loss": 1.7064,
      "step": 225504
    },
    {
      "epoch": 0.0008185149046971032,
      "grad_norm": 10543.254905388563,
      "learning_rate": 2.106729625302799e-07,
      "loss": 1.7007,
      "step": 225536
    },
    {
      "epoch": 0.0008186310390479399,
      "grad_norm": 10137.032701930088,
      "learning_rate": 2.106580036139787e-07,
      "loss": 1.7178,
      "step": 225568
    },
    {
      "epoch": 0.0008187471733987766,
      "grad_norm": 10077.390535252665,
      "learning_rate": 2.1064304788371617e-07,
      "loss": 1.7201,
      "step": 225600
    },
    {
      "epoch": 0.0008188633077496133,
      "grad_norm": 14318.722289366464,
      "learning_rate": 2.1062809533836158e-07,
      "loss": 1.6914,
      "step": 225632
    },
    {
      "epoch": 0.00081897944210045,
      "grad_norm": 9372.02134013789,
      "learning_rate": 2.106131459767846e-07,
      "loss": 1.7062,
      "step": 225664
    },
    {
      "epoch": 0.0008190955764512867,
      "grad_norm": 9747.660847608517,
      "learning_rate": 2.1059819979785556e-07,
      "loss": 1.7286,
      "step": 225696
    },
    {
      "epoch": 0.0008192117108021235,
      "grad_norm": 10250.389260901266,
      "learning_rate": 2.105832568004454e-07,
      "loss": 1.7071,
      "step": 225728
    },
    {
      "epoch": 0.0008193278451529601,
      "grad_norm": 9127.118712934547,
      "learning_rate": 2.1056831698342552e-07,
      "loss": 1.724,
      "step": 225760
    },
    {
      "epoch": 0.0008194439795037969,
      "grad_norm": 10469.684235926125,
      "learning_rate": 2.105538470674854e-07,
      "loss": 1.7202,
      "step": 225792
    },
    {
      "epoch": 0.0008195601138546335,
      "grad_norm": 10684.698217544565,
      "learning_rate": 2.1053891350856295e-07,
      "loss": 1.7089,
      "step": 225824
    },
    {
      "epoch": 0.0008196762482054703,
      "grad_norm": 11445.886422641106,
      "learning_rate": 2.1052398312668363e-07,
      "loss": 1.6989,
      "step": 225856
    },
    {
      "epoch": 0.0008197923825563069,
      "grad_norm": 11842.925652050679,
      "learning_rate": 2.1050905592072114e-07,
      "loss": 1.7284,
      "step": 225888
    },
    {
      "epoch": 0.0008199085169071437,
      "grad_norm": 10743.31475849051,
      "learning_rate": 2.1049413188954966e-07,
      "loss": 1.7242,
      "step": 225920
    },
    {
      "epoch": 0.0008200246512579803,
      "grad_norm": 9745.676785118621,
      "learning_rate": 2.10479211032044e-07,
      "loss": 1.715,
      "step": 225952
    },
    {
      "epoch": 0.0008201407856088171,
      "grad_norm": 10264.259544652989,
      "learning_rate": 2.1046429334707948e-07,
      "loss": 1.7182,
      "step": 225984
    },
    {
      "epoch": 0.0008202569199596538,
      "grad_norm": 11246.852715315516,
      "learning_rate": 2.1044937883353199e-07,
      "loss": 1.7135,
      "step": 226016
    },
    {
      "epoch": 0.0008203730543104905,
      "grad_norm": 9560.755200296679,
      "learning_rate": 2.1043446749027799e-07,
      "loss": 1.6866,
      "step": 226048
    },
    {
      "epoch": 0.0008204891886613272,
      "grad_norm": 9981.158850554379,
      "learning_rate": 2.1041955931619445e-07,
      "loss": 1.7103,
      "step": 226080
    },
    {
      "epoch": 0.0008206053230121639,
      "grad_norm": 10636.759468936016,
      "learning_rate": 2.1040465431015898e-07,
      "loss": 1.7241,
      "step": 226112
    },
    {
      "epoch": 0.0008207214573630006,
      "grad_norm": 9337.510589016754,
      "learning_rate": 2.1038975247104965e-07,
      "loss": 1.6976,
      "step": 226144
    },
    {
      "epoch": 0.0008208375917138373,
      "grad_norm": 8995.340126976856,
      "learning_rate": 2.1037485379774518e-07,
      "loss": 1.7297,
      "step": 226176
    },
    {
      "epoch": 0.000820953726064674,
      "grad_norm": 8279.934782351851,
      "learning_rate": 2.1035995828912482e-07,
      "loss": 1.7301,
      "step": 226208
    },
    {
      "epoch": 0.0008210698604155107,
      "grad_norm": 9253.540943876566,
      "learning_rate": 2.1034506594406827e-07,
      "loss": 1.722,
      "step": 226240
    },
    {
      "epoch": 0.0008211859947663474,
      "grad_norm": 7998.637133912252,
      "learning_rate": 2.10330176761456e-07,
      "loss": 1.7237,
      "step": 226272
    },
    {
      "epoch": 0.0008213021291171842,
      "grad_norm": 9641.552986941471,
      "learning_rate": 2.103152907401688e-07,
      "loss": 1.7401,
      "step": 226304
    },
    {
      "epoch": 0.0008214182634680208,
      "grad_norm": 9284.69891811253,
      "learning_rate": 2.1030040787908813e-07,
      "loss": 1.7293,
      "step": 226336
    },
    {
      "epoch": 0.0008215343978188576,
      "grad_norm": 9221.411714048994,
      "learning_rate": 2.1028552817709608e-07,
      "loss": 1.7141,
      "step": 226368
    },
    {
      "epoch": 0.0008216505321696942,
      "grad_norm": 8602.583797906302,
      "learning_rate": 2.102706516330751e-07,
      "loss": 1.718,
      "step": 226400
    },
    {
      "epoch": 0.000821766666520531,
      "grad_norm": 12515.139152242775,
      "learning_rate": 2.102557782459084e-07,
      "loss": 1.7236,
      "step": 226432
    },
    {
      "epoch": 0.0008218828008713676,
      "grad_norm": 10541.112654743805,
      "learning_rate": 2.1024090801447955e-07,
      "loss": 1.7087,
      "step": 226464
    },
    {
      "epoch": 0.0008219989352222044,
      "grad_norm": 10873.042444504666,
      "learning_rate": 2.1022604093767286e-07,
      "loss": 1.719,
      "step": 226496
    },
    {
      "epoch": 0.000822115069573041,
      "grad_norm": 9365.898675514272,
      "learning_rate": 2.1021117701437307e-07,
      "loss": 1.7165,
      "step": 226528
    },
    {
      "epoch": 0.0008222312039238778,
      "grad_norm": 9722.451954111164,
      "learning_rate": 2.1019631624346545e-07,
      "loss": 1.6981,
      "step": 226560
    },
    {
      "epoch": 0.0008223473382747145,
      "grad_norm": 8544.571844159309,
      "learning_rate": 2.1018145862383594e-07,
      "loss": 1.7317,
      "step": 226592
    },
    {
      "epoch": 0.0008224634726255512,
      "grad_norm": 10528.60351613641,
      "learning_rate": 2.1016660415437096e-07,
      "loss": 1.7266,
      "step": 226624
    },
    {
      "epoch": 0.0008225796069763879,
      "grad_norm": 10621.236086256627,
      "learning_rate": 2.1015175283395745e-07,
      "loss": 1.7018,
      "step": 226656
    },
    {
      "epoch": 0.0008226957413272246,
      "grad_norm": 9448.541051400476,
      "learning_rate": 2.1013690466148294e-07,
      "loss": 1.7074,
      "step": 226688
    },
    {
      "epoch": 0.0008228118756780613,
      "grad_norm": 10166.671825135303,
      "learning_rate": 2.1012205963583553e-07,
      "loss": 1.7209,
      "step": 226720
    },
    {
      "epoch": 0.000822928010028898,
      "grad_norm": 8942.290869793937,
      "learning_rate": 2.1010721775590386e-07,
      "loss": 1.697,
      "step": 226752
    },
    {
      "epoch": 0.0008230441443797347,
      "grad_norm": 9627.95845441805,
      "learning_rate": 2.1009284268346804e-07,
      "loss": 1.7127,
      "step": 226784
    },
    {
      "epoch": 0.0008231602787305714,
      "grad_norm": 8762.472824494236,
      "learning_rate": 2.1007800699341846e-07,
      "loss": 1.7184,
      "step": 226816
    },
    {
      "epoch": 0.0008232764130814081,
      "grad_norm": 10059.03335316073,
      "learning_rate": 2.1006317444578843e-07,
      "loss": 1.7245,
      "step": 226848
    },
    {
      "epoch": 0.0008233925474322449,
      "grad_norm": 9630.604134736304,
      "learning_rate": 2.1004834503946878e-07,
      "loss": 1.7018,
      "step": 226880
    },
    {
      "epoch": 0.0008235086817830815,
      "grad_norm": 8519.91197137623,
      "learning_rate": 2.1003351877335083e-07,
      "loss": 1.712,
      "step": 226912
    },
    {
      "epoch": 0.0008236248161339183,
      "grad_norm": 9365.60345092616,
      "learning_rate": 2.100186956463265e-07,
      "loss": 1.7192,
      "step": 226944
    },
    {
      "epoch": 0.0008237409504847549,
      "grad_norm": 9174.335289273005,
      "learning_rate": 2.1000387565728823e-07,
      "loss": 1.7057,
      "step": 226976
    },
    {
      "epoch": 0.0008238570848355917,
      "grad_norm": 9765.364509325804,
      "learning_rate": 2.0998905880512902e-07,
      "loss": 1.7346,
      "step": 227008
    },
    {
      "epoch": 0.0008239732191864283,
      "grad_norm": 9799.809181815735,
      "learning_rate": 2.0997424508874244e-07,
      "loss": 1.7362,
      "step": 227040
    },
    {
      "epoch": 0.0008240893535372651,
      "grad_norm": 10860.301469112172,
      "learning_rate": 2.099594345070225e-07,
      "loss": 1.7364,
      "step": 227072
    },
    {
      "epoch": 0.0008242054878881017,
      "grad_norm": 10809.863088864724,
      "learning_rate": 2.0994462705886393e-07,
      "loss": 1.742,
      "step": 227104
    },
    {
      "epoch": 0.0008243216222389385,
      "grad_norm": 10117.356374073219,
      "learning_rate": 2.0992982274316184e-07,
      "loss": 1.7401,
      "step": 227136
    },
    {
      "epoch": 0.0008244377565897752,
      "grad_norm": 8799.871476334185,
      "learning_rate": 2.09915021558812e-07,
      "loss": 1.7159,
      "step": 227168
    },
    {
      "epoch": 0.0008245538909406119,
      "grad_norm": 10245.785670215828,
      "learning_rate": 2.0990022350471065e-07,
      "loss": 1.7358,
      "step": 227200
    },
    {
      "epoch": 0.0008246700252914486,
      "grad_norm": 10508.828859582783,
      "learning_rate": 2.098854285797546e-07,
      "loss": 1.7244,
      "step": 227232
    },
    {
      "epoch": 0.0008247861596422853,
      "grad_norm": 11078.65298671278,
      "learning_rate": 2.0987063678284123e-07,
      "loss": 1.7194,
      "step": 227264
    },
    {
      "epoch": 0.000824902293993122,
      "grad_norm": 8924.53404946163,
      "learning_rate": 2.0985584811286844e-07,
      "loss": 1.6996,
      "step": 227296
    },
    {
      "epoch": 0.0008250184283439587,
      "grad_norm": 9771.04088621064,
      "learning_rate": 2.0984106256873465e-07,
      "loss": 1.7034,
      "step": 227328
    },
    {
      "epoch": 0.0008251345626947954,
      "grad_norm": 9148.102754123393,
      "learning_rate": 2.0982628014933887e-07,
      "loss": 1.7107,
      "step": 227360
    },
    {
      "epoch": 0.0008252506970456321,
      "grad_norm": 8608.084339735526,
      "learning_rate": 2.0981150085358066e-07,
      "loss": 1.6937,
      "step": 227392
    },
    {
      "epoch": 0.0008253668313964688,
      "grad_norm": 10475.96296289749,
      "learning_rate": 2.0979672468036e-07,
      "loss": 1.7207,
      "step": 227424
    },
    {
      "epoch": 0.0008254829657473056,
      "grad_norm": 8904.486509619743,
      "learning_rate": 2.0978195162857764e-07,
      "loss": 1.7287,
      "step": 227456
    },
    {
      "epoch": 0.0008255991000981422,
      "grad_norm": 9521.365868403545,
      "learning_rate": 2.097671816971346e-07,
      "loss": 1.7152,
      "step": 227488
    },
    {
      "epoch": 0.000825715234448979,
      "grad_norm": 11870.900555560223,
      "learning_rate": 2.097524148849327e-07,
      "loss": 1.6981,
      "step": 227520
    },
    {
      "epoch": 0.0008258313687998156,
      "grad_norm": 11065.550325221064,
      "learning_rate": 2.097376511908741e-07,
      "loss": 1.7165,
      "step": 227552
    },
    {
      "epoch": 0.0008259475031506524,
      "grad_norm": 8483.771095450418,
      "learning_rate": 2.0972289061386164e-07,
      "loss": 1.7081,
      "step": 227584
    },
    {
      "epoch": 0.000826063637501489,
      "grad_norm": 9280.666786389866,
      "learning_rate": 2.097081331527986e-07,
      "loss": 1.7162,
      "step": 227616
    },
    {
      "epoch": 0.0008261797718523258,
      "grad_norm": 8495.795783798007,
      "learning_rate": 2.0969337880658884e-07,
      "loss": 1.745,
      "step": 227648
    },
    {
      "epoch": 0.0008262959062031624,
      "grad_norm": 9825.621405285267,
      "learning_rate": 2.096786275741368e-07,
      "loss": 1.7354,
      "step": 227680
    },
    {
      "epoch": 0.0008264120405539992,
      "grad_norm": 9444.221301939086,
      "learning_rate": 2.096638794543474e-07,
      "loss": 1.7152,
      "step": 227712
    },
    {
      "epoch": 0.0008265281749048359,
      "grad_norm": 10850.551138075889,
      "learning_rate": 2.096491344461261e-07,
      "loss": 1.7207,
      "step": 227744
    },
    {
      "epoch": 0.0008266443092556726,
      "grad_norm": 10065.3915969524,
      "learning_rate": 2.0963439254837895e-07,
      "loss": 1.715,
      "step": 227776
    },
    {
      "epoch": 0.0008267604436065093,
      "grad_norm": 11095.100900848085,
      "learning_rate": 2.09620114300094e-07,
      "loss": 1.7113,
      "step": 227808
    },
    {
      "epoch": 0.000826876577957346,
      "grad_norm": 9553.17224800223,
      "learning_rate": 2.0960537852289787e-07,
      "loss": 1.7363,
      "step": 227840
    },
    {
      "epoch": 0.0008269927123081827,
      "grad_norm": 12205.152354641052,
      "learning_rate": 2.095906458529313e-07,
      "loss": 1.7294,
      "step": 227872
    },
    {
      "epoch": 0.0008271088466590194,
      "grad_norm": 8831.42027082847,
      "learning_rate": 2.0957591628910247e-07,
      "loss": 1.6982,
      "step": 227904
    },
    {
      "epoch": 0.0008272249810098561,
      "grad_norm": 9123.071631857332,
      "learning_rate": 2.0956118983031998e-07,
      "loss": 1.7146,
      "step": 227936
    },
    {
      "epoch": 0.0008273411153606928,
      "grad_norm": 8867.909336478357,
      "learning_rate": 2.0954646647549315e-07,
      "loss": 1.7352,
      "step": 227968
    },
    {
      "epoch": 0.0008274572497115295,
      "grad_norm": 11114.118408582843,
      "learning_rate": 2.0953174622353173e-07,
      "loss": 1.7147,
      "step": 228000
    },
    {
      "epoch": 0.0008275733840623663,
      "grad_norm": 8939.943288410726,
      "learning_rate": 2.09517029073346e-07,
      "loss": 1.7308,
      "step": 228032
    },
    {
      "epoch": 0.0008276895184132029,
      "grad_norm": 9033.430356182529,
      "learning_rate": 2.0950231502384678e-07,
      "loss": 1.74,
      "step": 228064
    },
    {
      "epoch": 0.0008278056527640397,
      "grad_norm": 8802.288793262807,
      "learning_rate": 2.0948760407394544e-07,
      "loss": 1.7288,
      "step": 228096
    },
    {
      "epoch": 0.0008279217871148763,
      "grad_norm": 9372.300678061923,
      "learning_rate": 2.0947289622255397e-07,
      "loss": 1.6964,
      "step": 228128
    },
    {
      "epoch": 0.0008280379214657131,
      "grad_norm": 11918.760170420412,
      "learning_rate": 2.094581914685847e-07,
      "loss": 1.711,
      "step": 228160
    },
    {
      "epoch": 0.0008281540558165497,
      "grad_norm": 9986.387535039885,
      "learning_rate": 2.0944348981095065e-07,
      "loss": 1.7154,
      "step": 228192
    },
    {
      "epoch": 0.0008282701901673865,
      "grad_norm": 9844.178584320785,
      "learning_rate": 2.0942879124856534e-07,
      "loss": 1.7215,
      "step": 228224
    },
    {
      "epoch": 0.0008283863245182231,
      "grad_norm": 8711.448674015133,
      "learning_rate": 2.0941409578034279e-07,
      "loss": 1.7325,
      "step": 228256
    },
    {
      "epoch": 0.0008285024588690599,
      "grad_norm": 8442.277536304999,
      "learning_rate": 2.0939940340519763e-07,
      "loss": 1.7395,
      "step": 228288
    },
    {
      "epoch": 0.0008286185932198966,
      "grad_norm": 10415.915322236448,
      "learning_rate": 2.093847141220449e-07,
      "loss": 1.6989,
      "step": 228320
    },
    {
      "epoch": 0.0008287347275707333,
      "grad_norm": 9060.533096898878,
      "learning_rate": 2.093700279298003e-07,
      "loss": 1.7133,
      "step": 228352
    },
    {
      "epoch": 0.00082885086192157,
      "grad_norm": 10002.23894935529,
      "learning_rate": 2.0935534482737994e-07,
      "loss": 1.7168,
      "step": 228384
    },
    {
      "epoch": 0.0008289669962724067,
      "grad_norm": 9421.713007728478,
      "learning_rate": 2.093406648137006e-07,
      "loss": 1.6978,
      "step": 228416
    },
    {
      "epoch": 0.0008290831306232434,
      "grad_norm": 9793.53705256686,
      "learning_rate": 2.0932598788767947e-07,
      "loss": 1.7258,
      "step": 228448
    },
    {
      "epoch": 0.0008291992649740801,
      "grad_norm": 9795.650871687905,
      "learning_rate": 2.093113140482343e-07,
      "loss": 1.7154,
      "step": 228480
    },
    {
      "epoch": 0.0008293153993249168,
      "grad_norm": 8824.218378984056,
      "learning_rate": 2.0929664329428347e-07,
      "loss": 1.7069,
      "step": 228512
    },
    {
      "epoch": 0.0008294315336757535,
      "grad_norm": 9846.47530845429,
      "learning_rate": 2.0928197562474572e-07,
      "loss": 1.7021,
      "step": 228544
    },
    {
      "epoch": 0.0008295476680265902,
      "grad_norm": 9496.90991849454,
      "learning_rate": 2.0926731103854048e-07,
      "loss": 1.719,
      "step": 228576
    },
    {
      "epoch": 0.000829663802377427,
      "grad_norm": 10504.124142449955,
      "learning_rate": 2.092526495345876e-07,
      "loss": 1.7163,
      "step": 228608
    },
    {
      "epoch": 0.0008297799367282636,
      "grad_norm": 10692.77419568935,
      "learning_rate": 2.0923799111180753e-07,
      "loss": 1.7216,
      "step": 228640
    },
    {
      "epoch": 0.0008298960710791004,
      "grad_norm": 9365.885863067091,
      "learning_rate": 2.092233357691212e-07,
      "loss": 1.7109,
      "step": 228672
    },
    {
      "epoch": 0.000830012205429937,
      "grad_norm": 8979.620259231457,
      "learning_rate": 2.092086835054501e-07,
      "loss": 1.7219,
      "step": 228704
    },
    {
      "epoch": 0.0008301283397807738,
      "grad_norm": 9358.331795784974,
      "learning_rate": 2.0919403431971622e-07,
      "loss": 1.7049,
      "step": 228736
    },
    {
      "epoch": 0.0008302444741316104,
      "grad_norm": 10115.41753957789,
      "learning_rate": 2.0917938821084212e-07,
      "loss": 1.7203,
      "step": 228768
    },
    {
      "epoch": 0.0008303606084824472,
      "grad_norm": 8444.54640581719,
      "learning_rate": 2.0916520272598836e-07,
      "loss": 1.7326,
      "step": 228800
    },
    {
      "epoch": 0.0008304767428332838,
      "grad_norm": 9665.058716841817,
      "learning_rate": 2.0915056267153523e-07,
      "loss": 1.7282,
      "step": 228832
    },
    {
      "epoch": 0.0008305928771841206,
      "grad_norm": 9879.020194331015,
      "learning_rate": 2.0913592569074626e-07,
      "loss": 1.7575,
      "step": 228864
    },
    {
      "epoch": 0.0008307090115349573,
      "grad_norm": 8388.157366191934,
      "learning_rate": 2.0912129178254606e-07,
      "loss": 1.7499,
      "step": 228896
    },
    {
      "epoch": 0.000830825145885794,
      "grad_norm": 11036.052917596942,
      "learning_rate": 2.091066609458598e-07,
      "loss": 1.7331,
      "step": 228928
    },
    {
      "epoch": 0.0008309412802366307,
      "grad_norm": 9740.417444853172,
      "learning_rate": 2.0909203317961317e-07,
      "loss": 1.7243,
      "step": 228960
    },
    {
      "epoch": 0.0008310574145874674,
      "grad_norm": 11218.535020224343,
      "learning_rate": 2.090774084827324e-07,
      "loss": 1.7166,
      "step": 228992
    },
    {
      "epoch": 0.0008311735489383041,
      "grad_norm": 8953.590564684093,
      "learning_rate": 2.0906278685414417e-07,
      "loss": 1.7008,
      "step": 229024
    },
    {
      "epoch": 0.0008312896832891408,
      "grad_norm": 8467.298034201938,
      "learning_rate": 2.0904816829277578e-07,
      "loss": 1.7155,
      "step": 229056
    },
    {
      "epoch": 0.0008314058176399775,
      "grad_norm": 11645.15830721077,
      "learning_rate": 2.09033552797555e-07,
      "loss": 1.7187,
      "step": 229088
    },
    {
      "epoch": 0.0008315219519908142,
      "grad_norm": 10683.995132907914,
      "learning_rate": 2.0901894036741012e-07,
      "loss": 1.7161,
      "step": 229120
    },
    {
      "epoch": 0.000831638086341651,
      "grad_norm": 10101.12171988834,
      "learning_rate": 2.0900433100127e-07,
      "loss": 1.6939,
      "step": 229152
    },
    {
      "epoch": 0.0008317542206924877,
      "grad_norm": 9341.751655872682,
      "learning_rate": 2.0898972469806402e-07,
      "loss": 1.7172,
      "step": 229184
    },
    {
      "epoch": 0.0008318703550433243,
      "grad_norm": 10979.399437127697,
      "learning_rate": 2.08975121456722e-07,
      "loss": 1.7336,
      "step": 229216
    },
    {
      "epoch": 0.0008319864893941611,
      "grad_norm": 8350.583213165413,
      "learning_rate": 2.089605212761744e-07,
      "loss": 1.7185,
      "step": 229248
    },
    {
      "epoch": 0.0008321026237449977,
      "grad_norm": 9535.835569052142,
      "learning_rate": 2.0894592415535213e-07,
      "loss": 1.7301,
      "step": 229280
    },
    {
      "epoch": 0.0008322187580958345,
      "grad_norm": 8195.869569484375,
      "learning_rate": 2.0893133009318663e-07,
      "loss": 1.7285,
      "step": 229312
    },
    {
      "epoch": 0.0008323348924466711,
      "grad_norm": 9245.321411395063,
      "learning_rate": 2.0891673908860986e-07,
      "loss": 1.7237,
      "step": 229344
    },
    {
      "epoch": 0.0008324510267975079,
      "grad_norm": 9336.755432161646,
      "learning_rate": 2.0890215114055434e-07,
      "loss": 1.7176,
      "step": 229376
    },
    {
      "epoch": 0.0008325671611483445,
      "grad_norm": 7664.77018572638,
      "learning_rate": 2.088875662479531e-07,
      "loss": 1.7462,
      "step": 229408
    },
    {
      "epoch": 0.0008326832954991813,
      "grad_norm": 10316.614464057478,
      "learning_rate": 2.0887298440973966e-07,
      "loss": 1.7315,
      "step": 229440
    },
    {
      "epoch": 0.000832799429850018,
      "grad_norm": 9566.449184519824,
      "learning_rate": 2.0885840562484804e-07,
      "loss": 1.7367,
      "step": 229472
    },
    {
      "epoch": 0.0008329155642008547,
      "grad_norm": 8965.160567441055,
      "learning_rate": 2.0884382989221287e-07,
      "loss": 1.7311,
      "step": 229504
    },
    {
      "epoch": 0.0008330316985516915,
      "grad_norm": 11395.36554920464,
      "learning_rate": 2.0882925721076925e-07,
      "loss": 1.7308,
      "step": 229536
    },
    {
      "epoch": 0.0008331478329025281,
      "grad_norm": 11586.101328747302,
      "learning_rate": 2.0881468757945276e-07,
      "loss": 1.7102,
      "step": 229568
    },
    {
      "epoch": 0.0008332639672533649,
      "grad_norm": 8832.719513264305,
      "learning_rate": 2.0880012099719958e-07,
      "loss": 1.723,
      "step": 229600
    },
    {
      "epoch": 0.0008333801016042015,
      "grad_norm": 10458.381136676939,
      "learning_rate": 2.0878555746294634e-07,
      "loss": 1.7405,
      "step": 229632
    },
    {
      "epoch": 0.0008334962359550383,
      "grad_norm": 8854.761205137042,
      "learning_rate": 2.0877099697563028e-07,
      "loss": 1.7188,
      "step": 229664
    },
    {
      "epoch": 0.0008336123703058749,
      "grad_norm": 10324.911040778996,
      "learning_rate": 2.08756439534189e-07,
      "loss": 1.7435,
      "step": 229696
    },
    {
      "epoch": 0.0008337285046567117,
      "grad_norm": 9193.813028335959,
      "learning_rate": 2.0874188513756078e-07,
      "loss": 1.7509,
      "step": 229728
    },
    {
      "epoch": 0.0008338446390075484,
      "grad_norm": 8524.783750922952,
      "learning_rate": 2.0872733378468434e-07,
      "loss": 1.7434,
      "step": 229760
    },
    {
      "epoch": 0.000833960773358385,
      "grad_norm": 8583.447675613805,
      "learning_rate": 2.0871324006314643e-07,
      "loss": 1.7167,
      "step": 229792
    },
    {
      "epoch": 0.0008340769077092218,
      "grad_norm": 10001.07094265409,
      "learning_rate": 2.086986946995569e-07,
      "loss": 1.7301,
      "step": 229824
    },
    {
      "epoch": 0.0008341930420600585,
      "grad_norm": 10751.9551710375,
      "learning_rate": 2.0868415237657157e-07,
      "loss": 1.71,
      "step": 229856
    },
    {
      "epoch": 0.0008343091764108952,
      "grad_norm": 8591.45785067936,
      "learning_rate": 2.086696130931312e-07,
      "loss": 1.7076,
      "step": 229888
    },
    {
      "epoch": 0.0008344253107617319,
      "grad_norm": 10464.921595501803,
      "learning_rate": 2.0865507684817713e-07,
      "loss": 1.7098,
      "step": 229920
    },
    {
      "epoch": 0.0008345414451125686,
      "grad_norm": 10504.972536851299,
      "learning_rate": 2.0864054364065115e-07,
      "loss": 1.7167,
      "step": 229952
    },
    {
      "epoch": 0.0008346575794634053,
      "grad_norm": 11098.397001369161,
      "learning_rate": 2.0862601346949564e-07,
      "loss": 1.7143,
      "step": 229984
    },
    {
      "epoch": 0.000834773713814242,
      "grad_norm": 8613.466201245583,
      "learning_rate": 2.0861148633365338e-07,
      "loss": 1.7134,
      "step": 230016
    },
    {
      "epoch": 0.0008348898481650788,
      "grad_norm": 8915.083286206585,
      "learning_rate": 2.085969622320678e-07,
      "loss": 1.7255,
      "step": 230048
    },
    {
      "epoch": 0.0008350059825159154,
      "grad_norm": 9432.626569519223,
      "learning_rate": 2.0858244116368276e-07,
      "loss": 1.7069,
      "step": 230080
    },
    {
      "epoch": 0.0008351221168667522,
      "grad_norm": 7002.221790260574,
      "learning_rate": 2.0856792312744262e-07,
      "loss": 1.7246,
      "step": 230112
    },
    {
      "epoch": 0.0008352382512175888,
      "grad_norm": 8202.313941809347,
      "learning_rate": 2.0855340812229237e-07,
      "loss": 1.7153,
      "step": 230144
    },
    {
      "epoch": 0.0008353543855684256,
      "grad_norm": 9285.658619613367,
      "learning_rate": 2.0853889614717733e-07,
      "loss": 1.6948,
      "step": 230176
    },
    {
      "epoch": 0.0008354705199192622,
      "grad_norm": 9907.646743803494,
      "learning_rate": 2.0852438720104353e-07,
      "loss": 1.7117,
      "step": 230208
    },
    {
      "epoch": 0.000835586654270099,
      "grad_norm": 9110.094291498854,
      "learning_rate": 2.0850988128283737e-07,
      "loss": 1.724,
      "step": 230240
    },
    {
      "epoch": 0.0008357027886209356,
      "grad_norm": 9979.182130816132,
      "learning_rate": 2.0849537839150584e-07,
      "loss": 1.6904,
      "step": 230272
    },
    {
      "epoch": 0.0008358189229717724,
      "grad_norm": 8702.66625810734,
      "learning_rate": 2.0848087852599638e-07,
      "loss": 1.7163,
      "step": 230304
    },
    {
      "epoch": 0.0008359350573226091,
      "grad_norm": 10847.348247382859,
      "learning_rate": 2.08466381685257e-07,
      "loss": 1.711,
      "step": 230336
    },
    {
      "epoch": 0.0008360511916734458,
      "grad_norm": 8840.677915182749,
      "learning_rate": 2.084518878682362e-07,
      "loss": 1.7113,
      "step": 230368
    },
    {
      "epoch": 0.0008361673260242825,
      "grad_norm": 10066.785385613422,
      "learning_rate": 2.0843739707388302e-07,
      "loss": 1.6997,
      "step": 230400
    },
    {
      "epoch": 0.0008362834603751192,
      "grad_norm": 9389.415104254365,
      "learning_rate": 2.0842290930114692e-07,
      "loss": 1.7002,
      "step": 230432
    },
    {
      "epoch": 0.0008363995947259559,
      "grad_norm": 9913.367339103297,
      "learning_rate": 2.0840842454897799e-07,
      "loss": 1.7054,
      "step": 230464
    },
    {
      "epoch": 0.0008365157290767926,
      "grad_norm": 10636.79707430766,
      "learning_rate": 2.0839394281632677e-07,
      "loss": 1.7015,
      "step": 230496
    },
    {
      "epoch": 0.0008366318634276293,
      "grad_norm": 9049.513357081694,
      "learning_rate": 2.0837946410214426e-07,
      "loss": 1.7292,
      "step": 230528
    },
    {
      "epoch": 0.000836747997778466,
      "grad_norm": 9545.265213706742,
      "learning_rate": 2.083649884053821e-07,
      "loss": 1.7386,
      "step": 230560
    },
    {
      "epoch": 0.0008368641321293027,
      "grad_norm": 8717.076918325316,
      "learning_rate": 2.083505157249923e-07,
      "loss": 1.7324,
      "step": 230592
    },
    {
      "epoch": 0.0008369802664801395,
      "grad_norm": 9964.513435185885,
      "learning_rate": 2.083360460599275e-07,
      "loss": 1.7447,
      "step": 230624
    },
    {
      "epoch": 0.0008370964008309761,
      "grad_norm": 10150.446098571234,
      "learning_rate": 2.083215794091408e-07,
      "loss": 1.7448,
      "step": 230656
    },
    {
      "epoch": 0.0008372125351818129,
      "grad_norm": 8754.01050947507,
      "learning_rate": 2.0830711577158572e-07,
      "loss": 1.715,
      "step": 230688
    },
    {
      "epoch": 0.0008373286695326495,
      "grad_norm": 9876.813656235498,
      "learning_rate": 2.0829265514621646e-07,
      "loss": 1.7338,
      "step": 230720
    },
    {
      "epoch": 0.0008374448038834863,
      "grad_norm": 7718.679161618263,
      "learning_rate": 2.0827819753198757e-07,
      "loss": 1.7135,
      "step": 230752
    },
    {
      "epoch": 0.0008375609382343229,
      "grad_norm": 9036.15891847858,
      "learning_rate": 2.0826374292785425e-07,
      "loss": 1.7105,
      "step": 230784
    },
    {
      "epoch": 0.0008376770725851597,
      "grad_norm": 10497.77576441791,
      "learning_rate": 2.082497428995816e-07,
      "loss": 1.699,
      "step": 230816
    },
    {
      "epoch": 0.0008377932069359963,
      "grad_norm": 9908.92769173335,
      "learning_rate": 2.082352942185223e-07,
      "loss": 1.7038,
      "step": 230848
    },
    {
      "epoch": 0.0008379093412868331,
      "grad_norm": 9347.701963584419,
      "learning_rate": 2.0822084854445954e-07,
      "loss": 1.7057,
      "step": 230880
    },
    {
      "epoch": 0.0008380254756376698,
      "grad_norm": 10553.03956213564,
      "learning_rate": 2.082064058763505e-07,
      "loss": 1.6936,
      "step": 230912
    },
    {
      "epoch": 0.0008381416099885065,
      "grad_norm": 9707.931602560866,
      "learning_rate": 2.081919662131528e-07,
      "loss": 1.7155,
      "step": 230944
    },
    {
      "epoch": 0.0008382577443393432,
      "grad_norm": 9265.25941352966,
      "learning_rate": 2.081775295538246e-07,
      "loss": 1.7303,
      "step": 230976
    },
    {
      "epoch": 0.0008383738786901799,
      "grad_norm": 9142.607068008556,
      "learning_rate": 2.081630958973245e-07,
      "loss": 1.7053,
      "step": 231008
    },
    {
      "epoch": 0.0008384900130410166,
      "grad_norm": 10273.349210457123,
      "learning_rate": 2.0814866524261175e-07,
      "loss": 1.7002,
      "step": 231040
    },
    {
      "epoch": 0.0008386061473918533,
      "grad_norm": 10076.20285623508,
      "learning_rate": 2.0813423758864596e-07,
      "loss": 1.7107,
      "step": 231072
    },
    {
      "epoch": 0.00083872228174269,
      "grad_norm": 8463.382657070399,
      "learning_rate": 2.0811981293438737e-07,
      "loss": 1.6973,
      "step": 231104
    },
    {
      "epoch": 0.0008388384160935267,
      "grad_norm": 9657.234179618925,
      "learning_rate": 2.0810539127879657e-07,
      "loss": 1.7147,
      "step": 231136
    },
    {
      "epoch": 0.0008389545504443634,
      "grad_norm": 10045.770652369085,
      "learning_rate": 2.0809097262083477e-07,
      "loss": 1.728,
      "step": 231168
    },
    {
      "epoch": 0.0008390706847952002,
      "grad_norm": 8724.638101377042,
      "learning_rate": 2.080765569594637e-07,
      "loss": 1.7274,
      "step": 231200
    },
    {
      "epoch": 0.0008391868191460368,
      "grad_norm": 10568.031226297546,
      "learning_rate": 2.0806214429364548e-07,
      "loss": 1.7135,
      "step": 231232
    },
    {
      "epoch": 0.0008393029534968736,
      "grad_norm": 10877.789849045623,
      "learning_rate": 2.0804773462234285e-07,
      "loss": 1.715,
      "step": 231264
    },
    {
      "epoch": 0.0008394190878477102,
      "grad_norm": 10006.693359946632,
      "learning_rate": 2.08033327944519e-07,
      "loss": 1.7062,
      "step": 231296
    },
    {
      "epoch": 0.000839535222198547,
      "grad_norm": 9028.989755227325,
      "learning_rate": 2.080189242591376e-07,
      "loss": 1.698,
      "step": 231328
    },
    {
      "epoch": 0.0008396513565493836,
      "grad_norm": 8414.03315895534,
      "learning_rate": 2.0800452356516286e-07,
      "loss": 1.719,
      "step": 231360
    },
    {
      "epoch": 0.0008397674909002204,
      "grad_norm": 9816.253256716638,
      "learning_rate": 2.079901258615595e-07,
      "loss": 1.7315,
      "step": 231392
    },
    {
      "epoch": 0.000839883625251057,
      "grad_norm": 9460.061416291122,
      "learning_rate": 2.079757311472927e-07,
      "loss": 1.6967,
      "step": 231424
    },
    {
      "epoch": 0.0008399997596018938,
      "grad_norm": 11224.902672183845,
      "learning_rate": 2.0796133942132814e-07,
      "loss": 1.7124,
      "step": 231456
    },
    {
      "epoch": 0.0008401158939527305,
      "grad_norm": 10777.437172166674,
      "learning_rate": 2.0794695068263212e-07,
      "loss": 1.731,
      "step": 231488
    },
    {
      "epoch": 0.0008402320283035672,
      "grad_norm": 9531.749262333751,
      "learning_rate": 2.0793256493017124e-07,
      "loss": 1.7151,
      "step": 231520
    },
    {
      "epoch": 0.0008403481626544039,
      "grad_norm": 9245.057057693046,
      "learning_rate": 2.0791818216291275e-07,
      "loss": 1.7384,
      "step": 231552
    },
    {
      "epoch": 0.0008404642970052406,
      "grad_norm": 8944.75198091037,
      "learning_rate": 2.0790380237982435e-07,
      "loss": 1.7424,
      "step": 231584
    },
    {
      "epoch": 0.0008405804313560773,
      "grad_norm": 8737.578611949652,
      "learning_rate": 2.0788942557987425e-07,
      "loss": 1.7183,
      "step": 231616
    },
    {
      "epoch": 0.000840696565706914,
      "grad_norm": 10196.872461691379,
      "learning_rate": 2.0787505176203118e-07,
      "loss": 1.6998,
      "step": 231648
    },
    {
      "epoch": 0.0008408127000577507,
      "grad_norm": 9325.988419465253,
      "learning_rate": 2.0786068092526426e-07,
      "loss": 1.7129,
      "step": 231680
    },
    {
      "epoch": 0.0008409288344085874,
      "grad_norm": 9351.161959884985,
      "learning_rate": 2.078463130685433e-07,
      "loss": 1.7158,
      "step": 231712
    },
    {
      "epoch": 0.0008410449687594241,
      "grad_norm": 9502.498197842502,
      "learning_rate": 2.078319481908384e-07,
      "loss": 1.7213,
      "step": 231744
    },
    {
      "epoch": 0.0008411611031102608,
      "grad_norm": 9239.510809561294,
      "learning_rate": 2.0781758629112035e-07,
      "loss": 1.7415,
      "step": 231776
    },
    {
      "epoch": 0.0008412772374610975,
      "grad_norm": 22304.423955798546,
      "learning_rate": 2.0780322736836026e-07,
      "loss": 1.737,
      "step": 231808
    },
    {
      "epoch": 0.0008413933718119343,
      "grad_norm": 11299.486891005272,
      "learning_rate": 2.0778931999983274e-07,
      "loss": 1.711,
      "step": 231840
    },
    {
      "epoch": 0.0008415095061627709,
      "grad_norm": 11077.899439875775,
      "learning_rate": 2.0777496693495412e-07,
      "loss": 1.7237,
      "step": 231872
    },
    {
      "epoch": 0.0008416256405136077,
      "grad_norm": 9278.911574101781,
      "learning_rate": 2.0776061684398219e-07,
      "loss": 1.7151,
      "step": 231904
    },
    {
      "epoch": 0.0008417417748644443,
      "grad_norm": 9034.17511453038,
      "learning_rate": 2.077462697258901e-07,
      "loss": 1.6874,
      "step": 231936
    },
    {
      "epoch": 0.0008418579092152811,
      "grad_norm": 9916.287208426347,
      "learning_rate": 2.0773192557965148e-07,
      "loss": 1.7223,
      "step": 231968
    },
    {
      "epoch": 0.0008419740435661177,
      "grad_norm": 9425.350073074209,
      "learning_rate": 2.0771758440424055e-07,
      "loss": 1.723,
      "step": 232000
    },
    {
      "epoch": 0.0008420901779169545,
      "grad_norm": 8031.241373536223,
      "learning_rate": 2.0770324619863196e-07,
      "loss": 1.7055,
      "step": 232032
    },
    {
      "epoch": 0.0008422063122677911,
      "grad_norm": 7763.060736590948,
      "learning_rate": 2.0768891096180087e-07,
      "loss": 1.6916,
      "step": 232064
    },
    {
      "epoch": 0.0008423224466186279,
      "grad_norm": 9159.085980598718,
      "learning_rate": 2.0767457869272288e-07,
      "loss": 1.7056,
      "step": 232096
    },
    {
      "epoch": 0.0008424385809694646,
      "grad_norm": 10348.366247867341,
      "learning_rate": 2.0766024939037416e-07,
      "loss": 1.7068,
      "step": 232128
    },
    {
      "epoch": 0.0008425547153203013,
      "grad_norm": 11376.353018432577,
      "learning_rate": 2.0764592305373137e-07,
      "loss": 1.7068,
      "step": 232160
    },
    {
      "epoch": 0.000842670849671138,
      "grad_norm": 11111.44653049278,
      "learning_rate": 2.076315996817716e-07,
      "loss": 1.7195,
      "step": 232192
    },
    {
      "epoch": 0.0008427869840219747,
      "grad_norm": 10353.765788349667,
      "learning_rate": 2.076172792734725e-07,
      "loss": 1.72,
      "step": 232224
    },
    {
      "epoch": 0.0008429031183728114,
      "grad_norm": 9712.110378285453,
      "learning_rate": 2.0760296182781218e-07,
      "loss": 1.7006,
      "step": 232256
    },
    {
      "epoch": 0.0008430192527236481,
      "grad_norm": 10185.30706459064,
      "learning_rate": 2.0758864734376923e-07,
      "loss": 1.7222,
      "step": 232288
    },
    {
      "epoch": 0.0008431353870744848,
      "grad_norm": 10853.366850890096,
      "learning_rate": 2.075743358203228e-07,
      "loss": 1.7345,
      "step": 232320
    },
    {
      "epoch": 0.0008432515214253215,
      "grad_norm": 10374.124734164323,
      "learning_rate": 2.0756002725645247e-07,
      "loss": 1.7263,
      "step": 232352
    },
    {
      "epoch": 0.0008433676557761582,
      "grad_norm": 9885.108800615197,
      "learning_rate": 2.0754572165113833e-07,
      "loss": 1.7581,
      "step": 232384
    },
    {
      "epoch": 0.000843483790126995,
      "grad_norm": 8949.373609365071,
      "learning_rate": 2.0753141900336094e-07,
      "loss": 1.7439,
      "step": 232416
    },
    {
      "epoch": 0.0008435999244778316,
      "grad_norm": 9768.223789410233,
      "learning_rate": 2.075171193121014e-07,
      "loss": 1.7232,
      "step": 232448
    },
    {
      "epoch": 0.0008437160588286684,
      "grad_norm": 9583.093863674716,
      "learning_rate": 2.075028225763413e-07,
      "loss": 1.7181,
      "step": 232480
    },
    {
      "epoch": 0.000843832193179505,
      "grad_norm": 9808.121328776475,
      "learning_rate": 2.0748852879506263e-07,
      "loss": 1.7184,
      "step": 232512
    },
    {
      "epoch": 0.0008439483275303418,
      "grad_norm": 7357.309834443565,
      "learning_rate": 2.0747423796724795e-07,
      "loss": 1.7031,
      "step": 232544
    },
    {
      "epoch": 0.0008440644618811784,
      "grad_norm": 9927.212096051942,
      "learning_rate": 2.0745995009188032e-07,
      "loss": 1.7214,
      "step": 232576
    },
    {
      "epoch": 0.0008441805962320152,
      "grad_norm": 9870.286318035562,
      "learning_rate": 2.0744566516794328e-07,
      "loss": 1.7181,
      "step": 232608
    },
    {
      "epoch": 0.0008442967305828518,
      "grad_norm": 10135.307494101991,
      "learning_rate": 2.074313831944208e-07,
      "loss": 1.7205,
      "step": 232640
    },
    {
      "epoch": 0.0008444128649336886,
      "grad_norm": 9411.350806340182,
      "learning_rate": 2.0741710417029742e-07,
      "loss": 1.7007,
      "step": 232672
    },
    {
      "epoch": 0.0008445289992845253,
      "grad_norm": 9947.381967130848,
      "learning_rate": 2.0740282809455816e-07,
      "loss": 1.7179,
      "step": 232704
    },
    {
      "epoch": 0.000844645133635362,
      "grad_norm": 9970.642506879885,
      "learning_rate": 2.0738855496618846e-07,
      "loss": 1.7325,
      "step": 232736
    },
    {
      "epoch": 0.0008447612679861987,
      "grad_norm": 8766.792686039746,
      "learning_rate": 2.073742847841743e-07,
      "loss": 1.7216,
      "step": 232768
    },
    {
      "epoch": 0.0008448774023370354,
      "grad_norm": 8125.179013412566,
      "learning_rate": 2.0736001754750217e-07,
      "loss": 1.7175,
      "step": 232800
    },
    {
      "epoch": 0.0008449935366878721,
      "grad_norm": 9694.64677025419,
      "learning_rate": 2.0734619896973728e-07,
      "loss": 1.7103,
      "step": 232832
    },
    {
      "epoch": 0.0008451096710387088,
      "grad_norm": 9874.948708727556,
      "learning_rate": 2.073319375287472e-07,
      "loss": 1.7059,
      "step": 232864
    },
    {
      "epoch": 0.0008452258053895455,
      "grad_norm": 9151.928758463977,
      "learning_rate": 2.0731767903009305e-07,
      "loss": 1.7026,
      "step": 232896
    },
    {
      "epoch": 0.0008453419397403822,
      "grad_norm": 9246.284442953289,
      "learning_rate": 2.0730342347276322e-07,
      "loss": 1.7351,
      "step": 232928
    },
    {
      "epoch": 0.0008454580740912189,
      "grad_norm": 8955.669377550737,
      "learning_rate": 2.0728917085574665e-07,
      "loss": 1.7129,
      "step": 232960
    },
    {
      "epoch": 0.0008455742084420557,
      "grad_norm": 9599.882499280915,
      "learning_rate": 2.0727492117803265e-07,
      "loss": 1.7214,
      "step": 232992
    },
    {
      "epoch": 0.0008456903427928923,
      "grad_norm": 8929.872339513035,
      "learning_rate": 2.0726067443861108e-07,
      "loss": 1.7158,
      "step": 233024
    },
    {
      "epoch": 0.0008458064771437291,
      "grad_norm": 8147.579640604932,
      "learning_rate": 2.0724643063647234e-07,
      "loss": 1.7058,
      "step": 233056
    },
    {
      "epoch": 0.0008459226114945657,
      "grad_norm": 7210.629792188752,
      "learning_rate": 2.072321897706072e-07,
      "loss": 1.6964,
      "step": 233088
    },
    {
      "epoch": 0.0008460387458454025,
      "grad_norm": 10491.933282288826,
      "learning_rate": 2.07217951840007e-07,
      "loss": 1.7117,
      "step": 233120
    },
    {
      "epoch": 0.0008461548801962391,
      "grad_norm": 9039.624328477374,
      "learning_rate": 2.0720371684366357e-07,
      "loss": 1.7249,
      "step": 233152
    },
    {
      "epoch": 0.0008462710145470759,
      "grad_norm": 13214.718612214185,
      "learning_rate": 2.0718948478056913e-07,
      "loss": 1.709,
      "step": 233184
    },
    {
      "epoch": 0.0008463871488979125,
      "grad_norm": 9308.047056176714,
      "learning_rate": 2.0717525564971646e-07,
      "loss": 1.7284,
      "step": 233216
    },
    {
      "epoch": 0.0008465032832487493,
      "grad_norm": 8578.548711757718,
      "learning_rate": 2.0716102945009884e-07,
      "loss": 1.7332,
      "step": 233248
    },
    {
      "epoch": 0.000846619417599586,
      "grad_norm": 8978.628514422457,
      "learning_rate": 2.0714680618070999e-07,
      "loss": 1.7187,
      "step": 233280
    },
    {
      "epoch": 0.0008467355519504227,
      "grad_norm": 9426.180668754447,
      "learning_rate": 2.0713258584054413e-07,
      "loss": 1.7164,
      "step": 233312
    },
    {
      "epoch": 0.0008468516863012594,
      "grad_norm": 9328.75704475146,
      "learning_rate": 2.0711836842859597e-07,
      "loss": 1.7421,
      "step": 233344
    },
    {
      "epoch": 0.0008469678206520961,
      "grad_norm": 10289.816324891324,
      "learning_rate": 2.0710415394386067e-07,
      "loss": 1.708,
      "step": 233376
    },
    {
      "epoch": 0.0008470839550029328,
      "grad_norm": 9763.500601730919,
      "learning_rate": 2.0708994238533396e-07,
      "loss": 1.7129,
      "step": 233408
    },
    {
      "epoch": 0.0008472000893537695,
      "grad_norm": 9195.127405316362,
      "learning_rate": 2.0707573375201192e-07,
      "loss": 1.7181,
      "step": 233440
    },
    {
      "epoch": 0.0008473162237046062,
      "grad_norm": 9786.190474336783,
      "learning_rate": 2.0706152804289122e-07,
      "loss": 1.7266,
      "step": 233472
    },
    {
      "epoch": 0.0008474323580554429,
      "grad_norm": 10818.30263950866,
      "learning_rate": 2.0704732525696894e-07,
      "loss": 1.7176,
      "step": 233504
    },
    {
      "epoch": 0.0008475484924062796,
      "grad_norm": 8454.272647602513,
      "learning_rate": 2.0703312539324275e-07,
      "loss": 1.7349,
      "step": 233536
    },
    {
      "epoch": 0.0008476646267571164,
      "grad_norm": 9412.980824372267,
      "learning_rate": 2.0701892845071066e-07,
      "loss": 1.7405,
      "step": 233568
    },
    {
      "epoch": 0.000847780761107953,
      "grad_norm": 9637.72130744607,
      "learning_rate": 2.0700473442837122e-07,
      "loss": 1.7142,
      "step": 233600
    },
    {
      "epoch": 0.0008478968954587898,
      "grad_norm": 9825.80826191922,
      "learning_rate": 2.0699054332522353e-07,
      "loss": 1.7303,
      "step": 233632
    },
    {
      "epoch": 0.0008480130298096264,
      "grad_norm": 10013.875373700235,
      "learning_rate": 2.0697635514026707e-07,
      "loss": 1.7149,
      "step": 233664
    },
    {
      "epoch": 0.0008481291641604632,
      "grad_norm": 10024.730819328766,
      "learning_rate": 2.0696216987250183e-07,
      "loss": 1.6986,
      "step": 233696
    },
    {
      "epoch": 0.0008482452985112998,
      "grad_norm": 10421.431571526055,
      "learning_rate": 2.069479875209283e-07,
      "loss": 1.7015,
      "step": 233728
    },
    {
      "epoch": 0.0008483614328621366,
      "grad_norm": 9439.657409037682,
      "learning_rate": 2.0693380808454742e-07,
      "loss": 1.7207,
      "step": 233760
    },
    {
      "epoch": 0.0008484775672129732,
      "grad_norm": 9126.329820908293,
      "learning_rate": 2.069196315623607e-07,
      "loss": 1.685,
      "step": 233792
    },
    {
      "epoch": 0.00084859370156381,
      "grad_norm": 10613.11641319363,
      "learning_rate": 2.0690545795336997e-07,
      "loss": 1.7196,
      "step": 233824
    },
    {
      "epoch": 0.0008487098359146468,
      "grad_norm": 10997.329857742741,
      "learning_rate": 2.068917300467812e-07,
      "loss": 1.7176,
      "step": 233856
    },
    {
      "epoch": 0.0008488259702654834,
      "grad_norm": 10529.00223193062,
      "learning_rate": 2.0687756217023023e-07,
      "loss": 1.7137,
      "step": 233888
    },
    {
      "epoch": 0.0008489421046163202,
      "grad_norm": 10627.270392720795,
      "learning_rate": 2.06863397203915e-07,
      "loss": 1.7039,
      "step": 233920
    },
    {
      "epoch": 0.0008490582389671568,
      "grad_norm": 11548.742096003356,
      "learning_rate": 2.0684923514683942e-07,
      "loss": 1.7005,
      "step": 233952
    },
    {
      "epoch": 0.0008491743733179936,
      "grad_norm": 10391.541752791065,
      "learning_rate": 2.0683507599800767e-07,
      "loss": 1.7043,
      "step": 233984
    },
    {
      "epoch": 0.0008492905076688302,
      "grad_norm": 10287.456828584993,
      "learning_rate": 2.0682091975642458e-07,
      "loss": 1.7131,
      "step": 234016
    },
    {
      "epoch": 0.000849406642019667,
      "grad_norm": 10245.1949713024,
      "learning_rate": 2.068067664210954e-07,
      "loss": 1.7309,
      "step": 234048
    },
    {
      "epoch": 0.0008495227763705036,
      "grad_norm": 9512.36752864396,
      "learning_rate": 2.0679261599102582e-07,
      "loss": 1.7379,
      "step": 234080
    },
    {
      "epoch": 0.0008496389107213404,
      "grad_norm": 9407.912095677766,
      "learning_rate": 2.0677846846522202e-07,
      "loss": 1.743,
      "step": 234112
    },
    {
      "epoch": 0.0008497550450721771,
      "grad_norm": 8670.242557160671,
      "learning_rate": 2.0676432384269076e-07,
      "loss": 1.7498,
      "step": 234144
    },
    {
      "epoch": 0.0008498711794230138,
      "grad_norm": 11448.612492350328,
      "learning_rate": 2.0675018212243908e-07,
      "loss": 1.7548,
      "step": 234176
    },
    {
      "epoch": 0.0008499873137738505,
      "grad_norm": 9406.004890494158,
      "learning_rate": 2.0673604330347468e-07,
      "loss": 1.7352,
      "step": 234208
    },
    {
      "epoch": 0.0008501034481246872,
      "grad_norm": 11615.530465717009,
      "learning_rate": 2.0672190738480564e-07,
      "loss": 1.7442,
      "step": 234240
    },
    {
      "epoch": 0.0008502195824755239,
      "grad_norm": 8597.777387208858,
      "learning_rate": 2.0670777436544048e-07,
      "loss": 1.7272,
      "step": 234272
    },
    {
      "epoch": 0.0008503357168263606,
      "grad_norm": 10120.073517519524,
      "learning_rate": 2.0669364424438834e-07,
      "loss": 1.7186,
      "step": 234304
    },
    {
      "epoch": 0.0008504518511771973,
      "grad_norm": 8939.51587056033,
      "learning_rate": 2.0667951702065867e-07,
      "loss": 1.7264,
      "step": 234336
    },
    {
      "epoch": 0.000850567985528034,
      "grad_norm": 10465.05594824987,
      "learning_rate": 2.0666539269326147e-07,
      "loss": 1.7193,
      "step": 234368
    },
    {
      "epoch": 0.0008506841198788707,
      "grad_norm": 12231.71991177038,
      "learning_rate": 2.0665127126120727e-07,
      "loss": 1.7108,
      "step": 234400
    },
    {
      "epoch": 0.0008508002542297075,
      "grad_norm": 10148.100906080901,
      "learning_rate": 2.0663715272350695e-07,
      "loss": 1.7117,
      "step": 234432
    },
    {
      "epoch": 0.0008509163885805441,
      "grad_norm": 10302.059017497424,
      "learning_rate": 2.0662303707917194e-07,
      "loss": 1.7389,
      "step": 234464
    },
    {
      "epoch": 0.0008510325229313809,
      "grad_norm": 12129.892662344544,
      "learning_rate": 2.0660892432721413e-07,
      "loss": 1.7477,
      "step": 234496
    },
    {
      "epoch": 0.0008511486572822175,
      "grad_norm": 9357.259000369713,
      "learning_rate": 2.065948144666459e-07,
      "loss": 1.7134,
      "step": 234528
    },
    {
      "epoch": 0.0008512647916330543,
      "grad_norm": 8221.172300833987,
      "learning_rate": 2.0658070749648006e-07,
      "loss": 1.7114,
      "step": 234560
    },
    {
      "epoch": 0.0008513809259838909,
      "grad_norm": 8731.497237014966,
      "learning_rate": 2.0656660341572993e-07,
      "loss": 1.7266,
      "step": 234592
    },
    {
      "epoch": 0.0008514970603347277,
      "grad_norm": 7516.506901480236,
      "learning_rate": 2.0655250222340926e-07,
      "loss": 1.704,
      "step": 234624
    },
    {
      "epoch": 0.0008516131946855643,
      "grad_norm": 8884.088923463114,
      "learning_rate": 2.0653840391853232e-07,
      "loss": 1.7348,
      "step": 234656
    },
    {
      "epoch": 0.0008517293290364011,
      "grad_norm": 8416.113711208993,
      "learning_rate": 2.065243085001138e-07,
      "loss": 1.7486,
      "step": 234688
    },
    {
      "epoch": 0.0008518454633872378,
      "grad_norm": 8364.865091560054,
      "learning_rate": 2.0651021596716893e-07,
      "loss": 1.7276,
      "step": 234720
    },
    {
      "epoch": 0.0008519615977380745,
      "grad_norm": 8379.629586085533,
      "learning_rate": 2.0649612631871333e-07,
      "loss": 1.7138,
      "step": 234752
    },
    {
      "epoch": 0.0008520777320889112,
      "grad_norm": 8346.977057593964,
      "learning_rate": 2.0648203955376319e-07,
      "loss": 1.7111,
      "step": 234784
    },
    {
      "epoch": 0.0008521938664397479,
      "grad_norm": 8394.779091792709,
      "learning_rate": 2.0646795567133504e-07,
      "loss": 1.6948,
      "step": 234816
    },
    {
      "epoch": 0.0008523100007905846,
      "grad_norm": 10850.022857118782,
      "learning_rate": 2.0645431465811644e-07,
      "loss": 1.7022,
      "step": 234848
    },
    {
      "epoch": 0.0008524261351414213,
      "grad_norm": 10473.888962558272,
      "learning_rate": 2.0644023644778145e-07,
      "loss": 1.713,
      "step": 234880
    },
    {
      "epoch": 0.000852542269492258,
      "grad_norm": 8534.86930186983,
      "learning_rate": 2.0642616111705177e-07,
      "loss": 1.7313,
      "step": 234912
    },
    {
      "epoch": 0.0008526584038430947,
      "grad_norm": 10385.273419607209,
      "learning_rate": 2.0641208866494586e-07,
      "loss": 1.7047,
      "step": 234944
    },
    {
      "epoch": 0.0008527745381939314,
      "grad_norm": 10398.169069600666,
      "learning_rate": 2.0639801909048264e-07,
      "loss": 1.7146,
      "step": 234976
    },
    {
      "epoch": 0.0008528906725447682,
      "grad_norm": 10485.049928350365,
      "learning_rate": 2.063839523926815e-07,
      "loss": 1.7379,
      "step": 235008
    },
    {
      "epoch": 0.0008530068068956048,
      "grad_norm": 10266.637424200779,
      "learning_rate": 2.0636988857056232e-07,
      "loss": 1.7096,
      "step": 235040
    },
    {
      "epoch": 0.0008531229412464416,
      "grad_norm": 9153.98601703105,
      "learning_rate": 2.0635582762314542e-07,
      "loss": 1.7326,
      "step": 235072
    },
    {
      "epoch": 0.0008532390755972782,
      "grad_norm": 8962.556666487526,
      "learning_rate": 2.0634176954945163e-07,
      "loss": 1.7388,
      "step": 235104
    },
    {
      "epoch": 0.000853355209948115,
      "grad_norm": 9686.557076691388,
      "learning_rate": 2.0632771434850216e-07,
      "loss": 1.7141,
      "step": 235136
    },
    {
      "epoch": 0.0008534713442989516,
      "grad_norm": 9195.484979053579,
      "learning_rate": 2.0631366201931878e-07,
      "loss": 1.6979,
      "step": 235168
    },
    {
      "epoch": 0.0008535874786497884,
      "grad_norm": 12189.979819507496,
      "learning_rate": 2.062996125609237e-07,
      "loss": 1.7127,
      "step": 235200
    },
    {
      "epoch": 0.000853703613000625,
      "grad_norm": 10329.357966495303,
      "learning_rate": 2.0628556597233957e-07,
      "loss": 1.7071,
      "step": 235232
    },
    {
      "epoch": 0.0008538197473514618,
      "grad_norm": 9358.002457789804,
      "learning_rate": 2.0627152225258948e-07,
      "loss": 1.7134,
      "step": 235264
    },
    {
      "epoch": 0.0008539358817022985,
      "grad_norm": 8894.336175342149,
      "learning_rate": 2.0625748140069708e-07,
      "loss": 1.7263,
      "step": 235296
    },
    {
      "epoch": 0.0008540520160531352,
      "grad_norm": 9758.19163574891,
      "learning_rate": 2.0624344341568642e-07,
      "loss": 1.7392,
      "step": 235328
    },
    {
      "epoch": 0.0008541681504039719,
      "grad_norm": 9525.029763733024,
      "learning_rate": 2.0622940829658198e-07,
      "loss": 1.7129,
      "step": 235360
    },
    {
      "epoch": 0.0008542842847548086,
      "grad_norm": 7188.790718890069,
      "learning_rate": 2.0621537604240883e-07,
      "loss": 1.7105,
      "step": 235392
    },
    {
      "epoch": 0.0008544004191056453,
      "grad_norm": 9950.843984306055,
      "learning_rate": 2.0620134665219239e-07,
      "loss": 1.7043,
      "step": 235424
    },
    {
      "epoch": 0.000854516553456482,
      "grad_norm": 10703.17186632075,
      "learning_rate": 2.0618732012495855e-07,
      "loss": 1.6858,
      "step": 235456
    },
    {
      "epoch": 0.0008546326878073187,
      "grad_norm": 9233.029838574117,
      "learning_rate": 2.0617329645973374e-07,
      "loss": 1.7145,
      "step": 235488
    },
    {
      "epoch": 0.0008547488221581554,
      "grad_norm": 9526.172998639066,
      "learning_rate": 2.0615927565554474e-07,
      "loss": 1.7234,
      "step": 235520
    },
    {
      "epoch": 0.0008548649565089921,
      "grad_norm": 9160.37957728827,
      "learning_rate": 2.0614525771141893e-07,
      "loss": 1.7037,
      "step": 235552
    },
    {
      "epoch": 0.0008549810908598289,
      "grad_norm": 8335.232450267958,
      "learning_rate": 2.0613124262638403e-07,
      "loss": 1.6948,
      "step": 235584
    },
    {
      "epoch": 0.0008550972252106655,
      "grad_norm": 9429.071322245898,
      "learning_rate": 2.0611723039946834e-07,
      "loss": 1.7084,
      "step": 235616
    },
    {
      "epoch": 0.0008552133595615023,
      "grad_norm": 10089.557968513784,
      "learning_rate": 2.061032210297005e-07,
      "loss": 1.7058,
      "step": 235648
    },
    {
      "epoch": 0.0008553294939123389,
      "grad_norm": 10306.310494061394,
      "learning_rate": 2.0608921451610966e-07,
      "loss": 1.7012,
      "step": 235680
    },
    {
      "epoch": 0.0008554456282631757,
      "grad_norm": 8985.056037666098,
      "learning_rate": 2.0607521085772549e-07,
      "loss": 1.7087,
      "step": 235712
    },
    {
      "epoch": 0.0008555617626140123,
      "grad_norm": 9973.249019251449,
      "learning_rate": 2.0606121005357804e-07,
      "loss": 1.7153,
      "step": 235744
    },
    {
      "epoch": 0.0008556778969648491,
      "grad_norm": 9922.610342042057,
      "learning_rate": 2.0604721210269787e-07,
      "loss": 1.7003,
      "step": 235776
    },
    {
      "epoch": 0.0008557940313156857,
      "grad_norm": 8507.731307463817,
      "learning_rate": 2.0603321700411594e-07,
      "loss": 1.7118,
      "step": 235808
    },
    {
      "epoch": 0.0008559101656665225,
      "grad_norm": 19642.336724534583,
      "learning_rate": 2.0601922475686378e-07,
      "loss": 1.7282,
      "step": 235840
    },
    {
      "epoch": 0.0008560263000173592,
      "grad_norm": 9389.186333223981,
      "learning_rate": 2.0600567248549062e-07,
      "loss": 1.726,
      "step": 235872
    },
    {
      "epoch": 0.0008561424343681959,
      "grad_norm": 10711.190596754406,
      "learning_rate": 2.0599168584896527e-07,
      "loss": 1.7521,
      "step": 235904
    },
    {
      "epoch": 0.0008562585687190326,
      "grad_norm": 10586.47967928905,
      "learning_rate": 2.0597770206089703e-07,
      "loss": 1.7553,
      "step": 235936
    },
    {
      "epoch": 0.0008563747030698693,
      "grad_norm": 9524.69611063786,
      "learning_rate": 2.0596372112031917e-07,
      "loss": 1.7241,
      "step": 235968
    },
    {
      "epoch": 0.000856490837420706,
      "grad_norm": 10273.240189930342,
      "learning_rate": 2.059497430262655e-07,
      "loss": 1.7012,
      "step": 236000
    },
    {
      "epoch": 0.0008566069717715427,
      "grad_norm": 9367.254240170916,
      "learning_rate": 2.0593576777777016e-07,
      "loss": 1.7028,
      "step": 236032
    },
    {
      "epoch": 0.0008567231061223794,
      "grad_norm": 8347.18156026332,
      "learning_rate": 2.059217953738679e-07,
      "loss": 1.6936,
      "step": 236064
    },
    {
      "epoch": 0.0008568392404732161,
      "grad_norm": 8299.587941578786,
      "learning_rate": 2.0590782581359377e-07,
      "loss": 1.7127,
      "step": 236096
    },
    {
      "epoch": 0.0008569553748240528,
      "grad_norm": 12012.7712040145,
      "learning_rate": 2.0589385909598339e-07,
      "loss": 1.7191,
      "step": 236128
    },
    {
      "epoch": 0.0008570715091748896,
      "grad_norm": 9065.843148874792,
      "learning_rate": 2.058798952200728e-07,
      "loss": 1.7105,
      "step": 236160
    },
    {
      "epoch": 0.0008571876435257262,
      "grad_norm": 9275.12565952613,
      "learning_rate": 2.0586593418489848e-07,
      "loss": 1.6958,
      "step": 236192
    },
    {
      "epoch": 0.000857303777876563,
      "grad_norm": 11970.263155002065,
      "learning_rate": 2.0585197598949743e-07,
      "loss": 1.7169,
      "step": 236224
    },
    {
      "epoch": 0.0008574199122273996,
      "grad_norm": 9351.260449800337,
      "learning_rate": 2.05838020632907e-07,
      "loss": 1.7236,
      "step": 236256
    },
    {
      "epoch": 0.0008575360465782364,
      "grad_norm": 9183.19225541968,
      "learning_rate": 2.0582406811416512e-07,
      "loss": 1.6917,
      "step": 236288
    },
    {
      "epoch": 0.000857652180929073,
      "grad_norm": 9732.405047057999,
      "learning_rate": 2.058101184323101e-07,
      "loss": 1.7148,
      "step": 236320
    },
    {
      "epoch": 0.0008577683152799098,
      "grad_norm": 7934.303750172412,
      "learning_rate": 2.057961715863807e-07,
      "loss": 1.708,
      "step": 236352
    },
    {
      "epoch": 0.0008578844496307464,
      "grad_norm": 10584.20615823407,
      "learning_rate": 2.0578222757541617e-07,
      "loss": 1.7019,
      "step": 236384
    },
    {
      "epoch": 0.0008580005839815832,
      "grad_norm": 10437.3638434233,
      "learning_rate": 2.0576828639845623e-07,
      "loss": 1.6963,
      "step": 236416
    },
    {
      "epoch": 0.0008581167183324199,
      "grad_norm": 9004.413362346268,
      "learning_rate": 2.05754348054541e-07,
      "loss": 1.7226,
      "step": 236448
    },
    {
      "epoch": 0.0008582328526832566,
      "grad_norm": 13572.322424699467,
      "learning_rate": 2.0574041254271108e-07,
      "loss": 1.7158,
      "step": 236480
    },
    {
      "epoch": 0.0008583489870340933,
      "grad_norm": 9824.543144594561,
      "learning_rate": 2.0572647986200758e-07,
      "loss": 1.7328,
      "step": 236512
    },
    {
      "epoch": 0.00085846512138493,
      "grad_norm": 11052.703922570261,
      "learning_rate": 2.05712550011472e-07,
      "loss": 1.7173,
      "step": 236544
    },
    {
      "epoch": 0.0008585812557357667,
      "grad_norm": 8196.971513919028,
      "learning_rate": 2.0569862299014623e-07,
      "loss": 1.7115,
      "step": 236576
    },
    {
      "epoch": 0.0008586973900866034,
      "grad_norm": 9796.03368716135,
      "learning_rate": 2.056846987970728e-07,
      "loss": 1.7023,
      "step": 236608
    },
    {
      "epoch": 0.0008588135244374401,
      "grad_norm": 9904.963402254447,
      "learning_rate": 2.0567077743129451e-07,
      "loss": 1.706,
      "step": 236640
    },
    {
      "epoch": 0.0008589296587882768,
      "grad_norm": 9846.85371070374,
      "learning_rate": 2.0565685889185472e-07,
      "loss": 1.7235,
      "step": 236672
    },
    {
      "epoch": 0.0008590457931391135,
      "grad_norm": 9869.736774605492,
      "learning_rate": 2.056429431777972e-07,
      "loss": 1.7262,
      "step": 236704
    },
    {
      "epoch": 0.0008591619274899503,
      "grad_norm": 10851.991890892656,
      "learning_rate": 2.0562903028816624e-07,
      "loss": 1.7317,
      "step": 236736
    },
    {
      "epoch": 0.0008592780618407869,
      "grad_norm": 8785.589906204365,
      "learning_rate": 2.0561512022200645e-07,
      "loss": 1.742,
      "step": 236768
    },
    {
      "epoch": 0.0008593941961916237,
      "grad_norm": 8250.63718751467,
      "learning_rate": 2.0560121297836304e-07,
      "loss": 1.7288,
      "step": 236800
    },
    {
      "epoch": 0.0008595103305424603,
      "grad_norm": 10468.154374100528,
      "learning_rate": 2.0558730855628156e-07,
      "loss": 1.7154,
      "step": 236832
    },
    {
      "epoch": 0.0008596264648932971,
      "grad_norm": 9533.342330998084,
      "learning_rate": 2.0557384133716881e-07,
      "loss": 1.7359,
      "step": 236864
    },
    {
      "epoch": 0.0008597425992441337,
      "grad_norm": 9426.485028895977,
      "learning_rate": 2.0555994246725003e-07,
      "loss": 1.6904,
      "step": 236896
    },
    {
      "epoch": 0.0008598587335949705,
      "grad_norm": 8704.755481919063,
      "learning_rate": 2.055460464160624e-07,
      "loss": 1.7143,
      "step": 236928
    },
    {
      "epoch": 0.0008599748679458071,
      "grad_norm": 9724.561069786132,
      "learning_rate": 2.055321531826534e-07,
      "loss": 1.7185,
      "step": 236960
    },
    {
      "epoch": 0.0008600910022966439,
      "grad_norm": 9948.073180269634,
      "learning_rate": 2.0551826276607084e-07,
      "loss": 1.7159,
      "step": 236992
    },
    {
      "epoch": 0.0008602071366474806,
      "grad_norm": 10413.869597800809,
      "learning_rate": 2.0550437516536296e-07,
      "loss": 1.702,
      "step": 237024
    },
    {
      "epoch": 0.0008603232709983173,
      "grad_norm": 9851.837392080728,
      "learning_rate": 2.054904903795785e-07,
      "loss": 1.719,
      "step": 237056
    },
    {
      "epoch": 0.000860439405349154,
      "grad_norm": 9051.046127382182,
      "learning_rate": 2.0547660840776668e-07,
      "loss": 1.7335,
      "step": 237088
    },
    {
      "epoch": 0.0008605555396999907,
      "grad_norm": 8616.026346292123,
      "learning_rate": 2.0546272924897714e-07,
      "loss": 1.7285,
      "step": 237120
    },
    {
      "epoch": 0.0008606716740508274,
      "grad_norm": 9642.803119425389,
      "learning_rate": 2.0544885290225997e-07,
      "loss": 1.7202,
      "step": 237152
    },
    {
      "epoch": 0.0008607878084016641,
      "grad_norm": 9535.75482067361,
      "learning_rate": 2.0543497936666562e-07,
      "loss": 1.712,
      "step": 237184
    },
    {
      "epoch": 0.0008609039427525008,
      "grad_norm": 9379.326841516933,
      "learning_rate": 2.0542110864124515e-07,
      "loss": 1.7023,
      "step": 237216
    },
    {
      "epoch": 0.0008610200771033375,
      "grad_norm": 9320.98921788884,
      "learning_rate": 2.0540724072504996e-07,
      "loss": 1.6973,
      "step": 237248
    },
    {
      "epoch": 0.0008611362114541742,
      "grad_norm": 9720.104114668731,
      "learning_rate": 2.0539337561713198e-07,
      "loss": 1.7178,
      "step": 237280
    },
    {
      "epoch": 0.000861252345805011,
      "grad_norm": 11889.607226481454,
      "learning_rate": 2.0537951331654345e-07,
      "loss": 1.6899,
      "step": 237312
    },
    {
      "epoch": 0.0008613684801558476,
      "grad_norm": 9358.131437418477,
      "learning_rate": 2.0536565382233722e-07,
      "loss": 1.7049,
      "step": 237344
    },
    {
      "epoch": 0.0008614846145066844,
      "grad_norm": 8839.298840971494,
      "learning_rate": 2.0535179713356648e-07,
      "loss": 1.7151,
      "step": 237376
    },
    {
      "epoch": 0.000861600748857521,
      "grad_norm": 9565.99686389244,
      "learning_rate": 2.0533794324928488e-07,
      "loss": 1.7106,
      "step": 237408
    },
    {
      "epoch": 0.0008617168832083578,
      "grad_norm": 8737.148161728746,
      "learning_rate": 2.0532409216854654e-07,
      "loss": 1.6962,
      "step": 237440
    },
    {
      "epoch": 0.0008618330175591944,
      "grad_norm": 9340.064025476486,
      "learning_rate": 2.0531024389040607e-07,
      "loss": 1.7068,
      "step": 237472
    },
    {
      "epoch": 0.0008619491519100312,
      "grad_norm": 9212.891619898717,
      "learning_rate": 2.0529639841391843e-07,
      "loss": 1.7095,
      "step": 237504
    },
    {
      "epoch": 0.0008620652862608678,
      "grad_norm": 9378.2057985523,
      "learning_rate": 2.0528255573813908e-07,
      "loss": 1.7119,
      "step": 237536
    },
    {
      "epoch": 0.0008621814206117046,
      "grad_norm": 11366.691339171659,
      "learning_rate": 2.0526871586212396e-07,
      "loss": 1.7371,
      "step": 237568
    },
    {
      "epoch": 0.0008622975549625413,
      "grad_norm": 8633.224426597515,
      "learning_rate": 2.0525487878492934e-07,
      "loss": 1.7449,
      "step": 237600
    },
    {
      "epoch": 0.000862413689313378,
      "grad_norm": 9245.351047959184,
      "learning_rate": 2.0524104450561208e-07,
      "loss": 1.7458,
      "step": 237632
    },
    {
      "epoch": 0.0008625298236642147,
      "grad_norm": 10271.491809858975,
      "learning_rate": 2.0522721302322937e-07,
      "loss": 1.7529,
      "step": 237664
    },
    {
      "epoch": 0.0008626459580150514,
      "grad_norm": 9947.393829541485,
      "learning_rate": 2.0521338433683892e-07,
      "loss": 1.7645,
      "step": 237696
    },
    {
      "epoch": 0.0008627620923658881,
      "grad_norm": 9723.047053264732,
      "learning_rate": 2.0519955844549885e-07,
      "loss": 1.7175,
      "step": 237728
    },
    {
      "epoch": 0.0008628782267167248,
      "grad_norm": 8299.607822060028,
      "learning_rate": 2.051857353482677e-07,
      "loss": 1.7118,
      "step": 237760
    },
    {
      "epoch": 0.0008629943610675615,
      "grad_norm": 9121.024174948776,
      "learning_rate": 2.0517191504420452e-07,
      "loss": 1.711,
      "step": 237792
    },
    {
      "epoch": 0.0008631104954183982,
      "grad_norm": 10439.772794462531,
      "learning_rate": 2.0515809753236875e-07,
      "loss": 1.7049,
      "step": 237824
    },
    {
      "epoch": 0.000863226629769235,
      "grad_norm": 11988.1760080506,
      "learning_rate": 2.051442828118203e-07,
      "loss": 1.7034,
      "step": 237856
    },
    {
      "epoch": 0.0008633427641200717,
      "grad_norm": 19565.368588401292,
      "learning_rate": 2.0513047088161952e-07,
      "loss": 1.7084,
      "step": 237888
    },
    {
      "epoch": 0.0008634588984709083,
      "grad_norm": 9844.279760348138,
      "learning_rate": 2.0511709323426373e-07,
      "loss": 1.6909,
      "step": 237920
    },
    {
      "epoch": 0.0008635750328217451,
      "grad_norm": 8792.230433740919,
      "learning_rate": 2.0510328679481558e-07,
      "loss": 1.6982,
      "step": 237952
    },
    {
      "epoch": 0.0008636911671725817,
      "grad_norm": 9386.309604951246,
      "learning_rate": 2.0508948314292807e-07,
      "loss": 1.7207,
      "step": 237984
    },
    {
      "epoch": 0.0008638073015234185,
      "grad_norm": 10270.791692951425,
      "learning_rate": 2.050756822776633e-07,
      "loss": 1.7335,
      "step": 238016
    },
    {
      "epoch": 0.0008639234358742551,
      "grad_norm": 9388.11567887827,
      "learning_rate": 2.0506188419808383e-07,
      "loss": 1.6994,
      "step": 238048
    },
    {
      "epoch": 0.0008640395702250919,
      "grad_norm": 11262.847064574747,
      "learning_rate": 2.0504808890325264e-07,
      "loss": 1.6963,
      "step": 238080
    },
    {
      "epoch": 0.0008641557045759285,
      "grad_norm": 9044.114218650713,
      "learning_rate": 2.0503429639223313e-07,
      "loss": 1.7107,
      "step": 238112
    },
    {
      "epoch": 0.0008642718389267653,
      "grad_norm": 11528.31019707572,
      "learning_rate": 2.050205066640892e-07,
      "loss": 1.6935,
      "step": 238144
    },
    {
      "epoch": 0.0008643879732776021,
      "grad_norm": 9609.190184401597,
      "learning_rate": 2.0500671971788516e-07,
      "loss": 1.7141,
      "step": 238176
    },
    {
      "epoch": 0.0008645041076284387,
      "grad_norm": 9711.259856475883,
      "learning_rate": 2.049929355526857e-07,
      "loss": 1.7208,
      "step": 238208
    },
    {
      "epoch": 0.0008646202419792755,
      "grad_norm": 10079.761901949867,
      "learning_rate": 2.04979154167556e-07,
      "loss": 1.7203,
      "step": 238240
    },
    {
      "epoch": 0.0008647363763301121,
      "grad_norm": 9916.51813894373,
      "learning_rate": 2.0496537556156177e-07,
      "loss": 1.7205,
      "step": 238272
    },
    {
      "epoch": 0.0008648525106809489,
      "grad_norm": 10050.123382327203,
      "learning_rate": 2.0495159973376902e-07,
      "loss": 1.716,
      "step": 238304
    },
    {
      "epoch": 0.0008649686450317855,
      "grad_norm": 9537.51875489637,
      "learning_rate": 2.0493782668324423e-07,
      "loss": 1.6977,
      "step": 238336
    },
    {
      "epoch": 0.0008650847793826223,
      "grad_norm": 15950.229841604165,
      "learning_rate": 2.049240564090544e-07,
      "loss": 1.7074,
      "step": 238368
    },
    {
      "epoch": 0.0008652009137334589,
      "grad_norm": 9295.09978429495,
      "learning_rate": 2.0491028891026688e-07,
      "loss": 1.7198,
      "step": 238400
    },
    {
      "epoch": 0.0008653170480842957,
      "grad_norm": 9936.334132868118,
      "learning_rate": 2.0489652418594947e-07,
      "loss": 1.7303,
      "step": 238432
    },
    {
      "epoch": 0.0008654331824351324,
      "grad_norm": 8833.802012723627,
      "learning_rate": 2.0488276223517047e-07,
      "loss": 1.7284,
      "step": 238464
    },
    {
      "epoch": 0.0008655493167859691,
      "grad_norm": 9103.903118992424,
      "learning_rate": 2.0486900305699858e-07,
      "loss": 1.7333,
      "step": 238496
    },
    {
      "epoch": 0.0008656654511368058,
      "grad_norm": 11152.534958474687,
      "learning_rate": 2.048552466505029e-07,
      "loss": 1.7312,
      "step": 238528
    },
    {
      "epoch": 0.0008657815854876425,
      "grad_norm": 9074.256222963952,
      "learning_rate": 2.0484149301475297e-07,
      "loss": 1.713,
      "step": 238560
    },
    {
      "epoch": 0.0008658977198384792,
      "grad_norm": 11629.923645493122,
      "learning_rate": 2.0482774214881887e-07,
      "loss": 1.734,
      "step": 238592
    },
    {
      "epoch": 0.0008660138541893159,
      "grad_norm": 9250.157944597486,
      "learning_rate": 2.0481399405177103e-07,
      "loss": 1.7305,
      "step": 238624
    },
    {
      "epoch": 0.0008661299885401526,
      "grad_norm": 11247.113763094956,
      "learning_rate": 2.048002487226803e-07,
      "loss": 1.7021,
      "step": 238656
    },
    {
      "epoch": 0.0008662461228909893,
      "grad_norm": 8287.287372837991,
      "learning_rate": 2.0478650616061804e-07,
      "loss": 1.6895,
      "step": 238688
    },
    {
      "epoch": 0.000866362257241826,
      "grad_norm": 9791.927695811484,
      "learning_rate": 2.0477276636465597e-07,
      "loss": 1.7051,
      "step": 238720
    },
    {
      "epoch": 0.0008664783915926628,
      "grad_norm": 11469.225780321878,
      "learning_rate": 2.047590293338663e-07,
      "loss": 1.7126,
      "step": 238752
    },
    {
      "epoch": 0.0008665945259434994,
      "grad_norm": 9263.072276518196,
      "learning_rate": 2.0474529506732166e-07,
      "loss": 1.7112,
      "step": 238784
    },
    {
      "epoch": 0.0008667106602943362,
      "grad_norm": 9645.612474073381,
      "learning_rate": 2.047315635640951e-07,
      "loss": 1.7306,
      "step": 238816
    },
    {
      "epoch": 0.0008668267946451728,
      "grad_norm": 8623.227817934536,
      "learning_rate": 2.047178348232601e-07,
      "loss": 1.7457,
      "step": 238848
    },
    {
      "epoch": 0.0008669429289960096,
      "grad_norm": 11152.440450412636,
      "learning_rate": 2.0470410884389064e-07,
      "loss": 1.7206,
      "step": 238880
    },
    {
      "epoch": 0.0008670590633468462,
      "grad_norm": 8737.659640887827,
      "learning_rate": 2.0469081443387315e-07,
      "loss": 1.699,
      "step": 238912
    },
    {
      "epoch": 0.000867175197697683,
      "grad_norm": 10612.500365135447,
      "learning_rate": 2.046770938884343e-07,
      "loss": 1.709,
      "step": 238944
    },
    {
      "epoch": 0.0008672913320485196,
      "grad_norm": 10472.956029698587,
      "learning_rate": 2.0466337610171423e-07,
      "loss": 1.6932,
      "step": 238976
    },
    {
      "epoch": 0.0008674074663993564,
      "grad_norm": 9825.072009914227,
      "learning_rate": 2.0464966107278862e-07,
      "loss": 1.7172,
      "step": 239008
    },
    {
      "epoch": 0.0008675236007501931,
      "grad_norm": 9323.438850552944,
      "learning_rate": 2.046359488007336e-07,
      "loss": 1.7319,
      "step": 239040
    },
    {
      "epoch": 0.0008676397351010298,
      "grad_norm": 9974.406849532457,
      "learning_rate": 2.0462223928462563e-07,
      "loss": 1.7169,
      "step": 239072
    },
    {
      "epoch": 0.0008677558694518665,
      "grad_norm": 9237.735220279914,
      "learning_rate": 2.046085325235417e-07,
      "loss": 1.704,
      "step": 239104
    },
    {
      "epoch": 0.0008678720038027032,
      "grad_norm": 9616.726678033436,
      "learning_rate": 2.0459482851655915e-07,
      "loss": 1.7311,
      "step": 239136
    },
    {
      "epoch": 0.0008679881381535399,
      "grad_norm": 9342.637100947462,
      "learning_rate": 2.0458112726275588e-07,
      "loss": 1.7103,
      "step": 239168
    },
    {
      "epoch": 0.0008681042725043766,
      "grad_norm": 10890.500814930414,
      "learning_rate": 2.0456742876121009e-07,
      "loss": 1.7173,
      "step": 239200
    },
    {
      "epoch": 0.0008682204068552133,
      "grad_norm": 10378.471371064237,
      "learning_rate": 2.0455373301100045e-07,
      "loss": 1.7308,
      "step": 239232
    },
    {
      "epoch": 0.00086833654120605,
      "grad_norm": 8605.217022248771,
      "learning_rate": 2.0454004001120613e-07,
      "loss": 1.7293,
      "step": 239264
    },
    {
      "epoch": 0.0008684526755568867,
      "grad_norm": 10185.604940306688,
      "learning_rate": 2.0452634976090662e-07,
      "loss": 1.715,
      "step": 239296
    },
    {
      "epoch": 0.0008685688099077235,
      "grad_norm": 9419.885561937575,
      "learning_rate": 2.0451266225918194e-07,
      "loss": 1.7314,
      "step": 239328
    },
    {
      "epoch": 0.0008686849442585601,
      "grad_norm": 10134.678879964575,
      "learning_rate": 2.044989775051125e-07,
      "loss": 1.747,
      "step": 239360
    },
    {
      "epoch": 0.0008688010786093969,
      "grad_norm": 11650.003090128344,
      "learning_rate": 2.0448529549777911e-07,
      "loss": 1.7377,
      "step": 239392
    },
    {
      "epoch": 0.0008689172129602335,
      "grad_norm": 9245.841551746385,
      "learning_rate": 2.0447161623626305e-07,
      "loss": 1.7704,
      "step": 239424
    },
    {
      "epoch": 0.0008690333473110703,
      "grad_norm": 8956.7974187206,
      "learning_rate": 2.0445793971964603e-07,
      "loss": 1.7679,
      "step": 239456
    },
    {
      "epoch": 0.0008691494816619069,
      "grad_norm": 8968.138491348134,
      "learning_rate": 2.044442659470102e-07,
      "loss": 1.7406,
      "step": 239488
    },
    {
      "epoch": 0.0008692656160127437,
      "grad_norm": 8224.785468326818,
      "learning_rate": 2.0443059491743806e-07,
      "loss": 1.7091,
      "step": 239520
    },
    {
      "epoch": 0.0008693817503635803,
      "grad_norm": 9114.592475804939,
      "learning_rate": 2.0441692663001267e-07,
      "loss": 1.722,
      "step": 239552
    },
    {
      "epoch": 0.0008694978847144171,
      "grad_norm": 9903.432435272125,
      "learning_rate": 2.0440326108381743e-07,
      "loss": 1.7118,
      "step": 239584
    },
    {
      "epoch": 0.0008696140190652538,
      "grad_norm": 10207.529769733714,
      "learning_rate": 2.0438959827793614e-07,
      "loss": 1.7273,
      "step": 239616
    },
    {
      "epoch": 0.0008697301534160905,
      "grad_norm": 9977.647017208015,
      "learning_rate": 2.0437593821145314e-07,
      "loss": 1.724,
      "step": 239648
    },
    {
      "epoch": 0.0008698462877669272,
      "grad_norm": 10585.422806860386,
      "learning_rate": 2.0436228088345308e-07,
      "loss": 1.6974,
      "step": 239680
    },
    {
      "epoch": 0.0008699624221177639,
      "grad_norm": 10484.494265342511,
      "learning_rate": 2.0434862629302114e-07,
      "loss": 1.6995,
      "step": 239712
    },
    {
      "epoch": 0.0008700785564686006,
      "grad_norm": 9561.75025819018,
      "learning_rate": 2.0433497443924285e-07,
      "loss": 1.7148,
      "step": 239744
    },
    {
      "epoch": 0.0008701946908194373,
      "grad_norm": 9837.922341632911,
      "learning_rate": 2.0432132532120425e-07,
      "loss": 1.7034,
      "step": 239776
    },
    {
      "epoch": 0.000870310825170274,
      "grad_norm": 10110.115924162294,
      "learning_rate": 2.043076789379917e-07,
      "loss": 1.7018,
      "step": 239808
    },
    {
      "epoch": 0.0008704269595211107,
      "grad_norm": 9794.52183621028,
      "learning_rate": 2.04294035288692e-07,
      "loss": 1.7072,
      "step": 239840
    },
    {
      "epoch": 0.0008705430938719474,
      "grad_norm": 11410.832222059878,
      "learning_rate": 2.0428039437239253e-07,
      "loss": 1.6967,
      "step": 239872
    },
    {
      "epoch": 0.0008706592282227842,
      "grad_norm": 17221.078711857746,
      "learning_rate": 2.0426675618818094e-07,
      "loss": 1.6943,
      "step": 239904
    },
    {
      "epoch": 0.0008707753625736208,
      "grad_norm": 10681.251986541653,
      "learning_rate": 2.042535468017207e-07,
      "loss": 1.7151,
      "step": 239936
    },
    {
      "epoch": 0.0008708914969244576,
      "grad_norm": 11362.866891766354,
      "learning_rate": 2.0423991399364266e-07,
      "loss": 1.6997,
      "step": 239968
    },
    {
      "epoch": 0.0008710076312752942,
      "grad_norm": 9853.440515880735,
      "learning_rate": 2.042262839149466e-07,
      "loss": 1.7302,
      "step": 240000
    },
    {
      "epoch": 0.000871123765626131,
      "grad_norm": 11210.441739735326,
      "learning_rate": 2.0421265656472192e-07,
      "loss": 1.7289,
      "step": 240032
    },
    {
      "epoch": 0.0008712398999769676,
      "grad_norm": 8957.026515535164,
      "learning_rate": 2.0419903194205842e-07,
      "loss": 1.7073,
      "step": 240064
    },
    {
      "epoch": 0.0008713560343278044,
      "grad_norm": 8531.889005372726,
      "learning_rate": 2.041854100460463e-07,
      "loss": 1.7068,
      "step": 240096
    },
    {
      "epoch": 0.000871472168678641,
      "grad_norm": 9965.212190415215,
      "learning_rate": 2.041717908757763e-07,
      "loss": 1.7027,
      "step": 240128
    },
    {
      "epoch": 0.0008715883030294778,
      "grad_norm": 10143.877759515835,
      "learning_rate": 2.0415817443033945e-07,
      "loss": 1.7151,
      "step": 240160
    },
    {
      "epoch": 0.0008717044373803145,
      "grad_norm": 9838.52366973826,
      "learning_rate": 2.041445607088273e-07,
      "loss": 1.7206,
      "step": 240192
    },
    {
      "epoch": 0.0008718205717311512,
      "grad_norm": 10520.570516849359,
      "learning_rate": 2.0413094971033172e-07,
      "loss": 1.7169,
      "step": 240224
    },
    {
      "epoch": 0.0008719367060819879,
      "grad_norm": 10653.610092358365,
      "learning_rate": 2.0411734143394514e-07,
      "loss": 1.7165,
      "step": 240256
    },
    {
      "epoch": 0.0008720528404328246,
      "grad_norm": 10360.683954257074,
      "learning_rate": 2.0410373587876026e-07,
      "loss": 1.7234,
      "step": 240288
    },
    {
      "epoch": 0.0008721689747836613,
      "grad_norm": 10121.878481783902,
      "learning_rate": 2.0409013304387038e-07,
      "loss": 1.7142,
      "step": 240320
    },
    {
      "epoch": 0.000872285109134498,
      "grad_norm": 9560.967733446234,
      "learning_rate": 2.0407653292836908e-07,
      "loss": 1.734,
      "step": 240352
    },
    {
      "epoch": 0.0008724012434853347,
      "grad_norm": 9708.77870795292,
      "learning_rate": 2.0406293553135038e-07,
      "loss": 1.7214,
      "step": 240384
    },
    {
      "epoch": 0.0008725173778361714,
      "grad_norm": 10822.358522983795,
      "learning_rate": 2.040493408519088e-07,
      "loss": 1.6944,
      "step": 240416
    },
    {
      "epoch": 0.0008726335121870081,
      "grad_norm": 12343.49172641194,
      "learning_rate": 2.0403574888913925e-07,
      "loss": 1.7015,
      "step": 240448
    },
    {
      "epoch": 0.0008727496465378449,
      "grad_norm": 11247.876421796249,
      "learning_rate": 2.04022159642137e-07,
      "loss": 1.7009,
      "step": 240480
    },
    {
      "epoch": 0.0008728657808886815,
      "grad_norm": 10650.102346926062,
      "learning_rate": 2.0400857310999776e-07,
      "loss": 1.7165,
      "step": 240512
    },
    {
      "epoch": 0.0008729819152395183,
      "grad_norm": 8720.627615028634,
      "learning_rate": 2.039949892918178e-07,
      "loss": 1.72,
      "step": 240544
    },
    {
      "epoch": 0.0008730980495903549,
      "grad_norm": 10620.680015893522,
      "learning_rate": 2.039814081866936e-07,
      "loss": 1.7153,
      "step": 240576
    },
    {
      "epoch": 0.0008732141839411917,
      "grad_norm": 9442.47891181124,
      "learning_rate": 2.0396782979372223e-07,
      "loss": 1.7209,
      "step": 240608
    },
    {
      "epoch": 0.0008733303182920283,
      "grad_norm": 9500.507354873213,
      "learning_rate": 2.0395425411200107e-07,
      "loss": 1.7329,
      "step": 240640
    },
    {
      "epoch": 0.0008734464526428651,
      "grad_norm": 9551.28054241943,
      "learning_rate": 2.0394068114062795e-07,
      "loss": 1.6957,
      "step": 240672
    },
    {
      "epoch": 0.0008735625869937017,
      "grad_norm": 9254.572491476849,
      "learning_rate": 2.0392711087870118e-07,
      "loss": 1.7104,
      "step": 240704
    },
    {
      "epoch": 0.0008736787213445385,
      "grad_norm": 11755.960020347125,
      "learning_rate": 2.0391354332531946e-07,
      "loss": 1.7087,
      "step": 240736
    },
    {
      "epoch": 0.0008737948556953752,
      "grad_norm": 10008.51127790742,
      "learning_rate": 2.0389997847958182e-07,
      "loss": 1.6845,
      "step": 240768
    },
    {
      "epoch": 0.0008739109900462119,
      "grad_norm": 8783.419152015915,
      "learning_rate": 2.0388641634058782e-07,
      "loss": 1.7106,
      "step": 240800
    },
    {
      "epoch": 0.0008740271243970486,
      "grad_norm": 9690.880868115137,
      "learning_rate": 2.0387285690743743e-07,
      "loss": 1.695,
      "step": 240832
    },
    {
      "epoch": 0.0008741432587478853,
      "grad_norm": 9610.823481887492,
      "learning_rate": 2.0385930017923096e-07,
      "loss": 1.7048,
      "step": 240864
    },
    {
      "epoch": 0.000874259393098722,
      "grad_norm": 9849.892385198937,
      "learning_rate": 2.0384574615506925e-07,
      "loss": 1.7173,
      "step": 240896
    },
    {
      "epoch": 0.0008743755274495587,
      "grad_norm": 9781.417075250396,
      "learning_rate": 2.0383261827192734e-07,
      "loss": 1.6884,
      "step": 240928
    },
    {
      "epoch": 0.0008744916618003954,
      "grad_norm": 8400.726992350126,
      "learning_rate": 2.0381906956872743e-07,
      "loss": 1.6932,
      "step": 240960
    },
    {
      "epoch": 0.0008746077961512321,
      "grad_norm": 11214.512026833801,
      "learning_rate": 2.0380552356690513e-07,
      "loss": 1.7066,
      "step": 240992
    },
    {
      "epoch": 0.0008747239305020688,
      "grad_norm": 8450.399280507401,
      "learning_rate": 2.0379198026556294e-07,
      "loss": 1.6953,
      "step": 241024
    },
    {
      "epoch": 0.0008748400648529056,
      "grad_norm": 9683.142878218827,
      "learning_rate": 2.0377843966380364e-07,
      "loss": 1.7189,
      "step": 241056
    },
    {
      "epoch": 0.0008749561992037422,
      "grad_norm": 11215.310071504933,
      "learning_rate": 2.0376490176073057e-07,
      "loss": 1.7275,
      "step": 241088
    },
    {
      "epoch": 0.000875072333554579,
      "grad_norm": 9543.72862145608,
      "learning_rate": 2.0375136655544737e-07,
      "loss": 1.7285,
      "step": 241120
    },
    {
      "epoch": 0.0008751884679054156,
      "grad_norm": 10410.164167773724,
      "learning_rate": 2.037378340470582e-07,
      "loss": 1.7464,
      "step": 241152
    },
    {
      "epoch": 0.0008753046022562524,
      "grad_norm": 8551.374041637988,
      "learning_rate": 2.037243042346675e-07,
      "loss": 1.7298,
      "step": 241184
    },
    {
      "epoch": 0.000875420736607089,
      "grad_norm": 8554.733660377744,
      "learning_rate": 2.037107771173803e-07,
      "loss": 1.7526,
      "step": 241216
    },
    {
      "epoch": 0.0008755368709579258,
      "grad_norm": 8623.46044230505,
      "learning_rate": 2.0369725269430188e-07,
      "loss": 1.739,
      "step": 241248
    },
    {
      "epoch": 0.0008756530053087624,
      "grad_norm": 9660.9556463116,
      "learning_rate": 2.0368373096453808e-07,
      "loss": 1.6889,
      "step": 241280
    },
    {
      "epoch": 0.0008757691396595992,
      "grad_norm": 12149.54089667589,
      "learning_rate": 2.0367021192719508e-07,
      "loss": 1.6961,
      "step": 241312
    },
    {
      "epoch": 0.0008758852740104359,
      "grad_norm": 9781.627880879543,
      "learning_rate": 2.0365669558137942e-07,
      "loss": 1.6978,
      "step": 241344
    },
    {
      "epoch": 0.0008760014083612726,
      "grad_norm": 7763.077740174962,
      "learning_rate": 2.0364318192619817e-07,
      "loss": 1.7061,
      "step": 241376
    },
    {
      "epoch": 0.0008761175427121093,
      "grad_norm": 9788.369833634199,
      "learning_rate": 2.0362967096075877e-07,
      "loss": 1.721,
      "step": 241408
    },
    {
      "epoch": 0.000876233677062946,
      "grad_norm": 8706.101308852316,
      "learning_rate": 2.0361616268416902e-07,
      "loss": 1.7006,
      "step": 241440
    },
    {
      "epoch": 0.0008763498114137827,
      "grad_norm": 9778.80197161186,
      "learning_rate": 2.0360265709553722e-07,
      "loss": 1.6979,
      "step": 241472
    },
    {
      "epoch": 0.0008764659457646194,
      "grad_norm": 10101.220718309249,
      "learning_rate": 2.0358915419397201e-07,
      "loss": 1.7228,
      "step": 241504
    },
    {
      "epoch": 0.0008765820801154561,
      "grad_norm": 9915.137316245296,
      "learning_rate": 2.0357565397858252e-07,
      "loss": 1.699,
      "step": 241536
    },
    {
      "epoch": 0.0008766982144662928,
      "grad_norm": 9467.807877222689,
      "learning_rate": 2.0356215644847823e-07,
      "loss": 1.7186,
      "step": 241568
    },
    {
      "epoch": 0.0008768143488171295,
      "grad_norm": 9493.239278560295,
      "learning_rate": 2.0354866160276906e-07,
      "loss": 1.712,
      "step": 241600
    },
    {
      "epoch": 0.0008769304831679663,
      "grad_norm": 8328.41137312513,
      "learning_rate": 2.0353516944056535e-07,
      "loss": 1.7035,
      "step": 241632
    },
    {
      "epoch": 0.0008770466175188029,
      "grad_norm": 10172.88081125499,
      "learning_rate": 2.0352167996097782e-07,
      "loss": 1.7024,
      "step": 241664
    },
    {
      "epoch": 0.0008771627518696397,
      "grad_norm": 9960.1128507663,
      "learning_rate": 2.0350819316311762e-07,
      "loss": 1.7125,
      "step": 241696
    },
    {
      "epoch": 0.0008772788862204763,
      "grad_norm": 9203.693389069413,
      "learning_rate": 2.0349470904609631e-07,
      "loss": 1.7119,
      "step": 241728
    },
    {
      "epoch": 0.0008773950205713131,
      "grad_norm": 9736.334834012232,
      "learning_rate": 2.0348122760902592e-07,
      "loss": 1.738,
      "step": 241760
    },
    {
      "epoch": 0.0008775111549221497,
      "grad_norm": 9034.183748408042,
      "learning_rate": 2.0346774885101878e-07,
      "loss": 1.722,
      "step": 241792
    },
    {
      "epoch": 0.0008776272892729865,
      "grad_norm": 10142.56377845365,
      "learning_rate": 2.0345427277118774e-07,
      "loss": 1.6991,
      "step": 241824
    },
    {
      "epoch": 0.0008777434236238231,
      "grad_norm": 8706.909899614215,
      "learning_rate": 2.0344079936864596e-07,
      "loss": 1.7185,
      "step": 241856
    },
    {
      "epoch": 0.0008778595579746599,
      "grad_norm": 9743.44025485865,
      "learning_rate": 2.034273286425071e-07,
      "loss": 1.6961,
      "step": 241888
    },
    {
      "epoch": 0.0008779756923254967,
      "grad_norm": 7835.745146442679,
      "learning_rate": 2.034138605918852e-07,
      "loss": 1.7204,
      "step": 241920
    },
    {
      "epoch": 0.0008780918266763333,
      "grad_norm": 9947.589858855259,
      "learning_rate": 2.0340081596841804e-07,
      "loss": 1.7204,
      "step": 241952
    },
    {
      "epoch": 0.00087820796102717,
      "grad_norm": 9760.536665573261,
      "learning_rate": 2.0338735318263262e-07,
      "loss": 1.7213,
      "step": 241984
    },
    {
      "epoch": 0.0008783240953780067,
      "grad_norm": 9639.988796673988,
      "learning_rate": 2.0337389306973632e-07,
      "loss": 1.7182,
      "step": 242016
    },
    {
      "epoch": 0.0008784402297288435,
      "grad_norm": 11547.230317266562,
      "learning_rate": 2.033604356288448e-07,
      "loss": 1.7307,
      "step": 242048
    },
    {
      "epoch": 0.0008785563640796801,
      "grad_norm": 9195.65212478158,
      "learning_rate": 2.0334698085907424e-07,
      "loss": 1.7219,
      "step": 242080
    },
    {
      "epoch": 0.0008786724984305169,
      "grad_norm": 10222.69357850464,
      "learning_rate": 2.03333528759541e-07,
      "loss": 1.7319,
      "step": 242112
    },
    {
      "epoch": 0.0008787886327813535,
      "grad_norm": 9486.296643053072,
      "learning_rate": 2.0332007932936206e-07,
      "loss": 1.7162,
      "step": 242144
    },
    {
      "epoch": 0.0008789047671321903,
      "grad_norm": 9941.8778910224,
      "learning_rate": 2.0330663256765464e-07,
      "loss": 1.6945,
      "step": 242176
    },
    {
      "epoch": 0.000879020901483027,
      "grad_norm": 10519.293512399016,
      "learning_rate": 2.0329318847353648e-07,
      "loss": 1.7061,
      "step": 242208
    },
    {
      "epoch": 0.0008791370358338636,
      "grad_norm": 8421.767866665527,
      "learning_rate": 2.0327974704612572e-07,
      "loss": 1.691,
      "step": 242240
    },
    {
      "epoch": 0.0008792531701847004,
      "grad_norm": 8523.258297153736,
      "learning_rate": 2.0326630828454083e-07,
      "loss": 1.7194,
      "step": 242272
    },
    {
      "epoch": 0.000879369304535537,
      "grad_norm": 9870.321372680832,
      "learning_rate": 2.0325287218790083e-07,
      "loss": 1.7148,
      "step": 242304
    },
    {
      "epoch": 0.0008794854388863738,
      "grad_norm": 9618.396332029575,
      "learning_rate": 2.03239438755325e-07,
      "loss": 1.7114,
      "step": 242336
    },
    {
      "epoch": 0.0008796015732372104,
      "grad_norm": 9862.439454820496,
      "learning_rate": 2.0322600798593303e-07,
      "loss": 1.7402,
      "step": 242368
    },
    {
      "epoch": 0.0008797177075880472,
      "grad_norm": 9584.073455478103,
      "learning_rate": 2.032125798788452e-07,
      "loss": 1.737,
      "step": 242400
    },
    {
      "epoch": 0.0008798338419388838,
      "grad_norm": 10164.127704825436,
      "learning_rate": 2.0319915443318196e-07,
      "loss": 1.6998,
      "step": 242432
    },
    {
      "epoch": 0.0008799499762897206,
      "grad_norm": 8342.694049286478,
      "learning_rate": 2.0318573164806436e-07,
      "loss": 1.7097,
      "step": 242464
    },
    {
      "epoch": 0.0008800661106405572,
      "grad_norm": 10536.690087499015,
      "learning_rate": 2.0317231152261372e-07,
      "loss": 1.7072,
      "step": 242496
    },
    {
      "epoch": 0.000880182244991394,
      "grad_norm": 7930.53882154296,
      "learning_rate": 2.0315889405595183e-07,
      "loss": 1.6986,
      "step": 242528
    },
    {
      "epoch": 0.0008802983793422308,
      "grad_norm": 10589.369575191906,
      "learning_rate": 2.031454792472009e-07,
      "loss": 1.7283,
      "step": 242560
    },
    {
      "epoch": 0.0008804145136930674,
      "grad_norm": 9141.875081185479,
      "learning_rate": 2.031320670954835e-07,
      "loss": 1.7076,
      "step": 242592
    },
    {
      "epoch": 0.0008805306480439042,
      "grad_norm": 10234.069083214163,
      "learning_rate": 2.0311865759992263e-07,
      "loss": 1.7265,
      "step": 242624
    },
    {
      "epoch": 0.0008806467823947408,
      "grad_norm": 10189.77271581658,
      "learning_rate": 2.0310525075964172e-07,
      "loss": 1.7198,
      "step": 242656
    },
    {
      "epoch": 0.0008807629167455776,
      "grad_norm": 10872.650274886984,
      "learning_rate": 2.0309184657376453e-07,
      "loss": 1.6876,
      "step": 242688
    },
    {
      "epoch": 0.0008808790510964142,
      "grad_norm": 8919.8087423442,
      "learning_rate": 2.0307844504141532e-07,
      "loss": 1.6948,
      "step": 242720
    },
    {
      "epoch": 0.000880995185447251,
      "grad_norm": 9313.097658674045,
      "learning_rate": 2.0306504616171866e-07,
      "loss": 1.707,
      "step": 242752
    },
    {
      "epoch": 0.0008811113197980876,
      "grad_norm": 8156.5486573672815,
      "learning_rate": 2.0305164993379957e-07,
      "loss": 1.7108,
      "step": 242784
    },
    {
      "epoch": 0.0008812274541489244,
      "grad_norm": 8135.965462070251,
      "learning_rate": 2.0303825635678354e-07,
      "loss": 1.7158,
      "step": 242816
    },
    {
      "epoch": 0.0008813435884997611,
      "grad_norm": 9848.854349618538,
      "learning_rate": 2.0302486542979634e-07,
      "loss": 1.7155,
      "step": 242848
    },
    {
      "epoch": 0.0008814597228505978,
      "grad_norm": 9496.305808049781,
      "learning_rate": 2.0301147715196422e-07,
      "loss": 1.7195,
      "step": 242880
    },
    {
      "epoch": 0.0008815758572014345,
      "grad_norm": 8425.319934578152,
      "learning_rate": 2.0299809152241378e-07,
      "loss": 1.7404,
      "step": 242912
    },
    {
      "epoch": 0.0008816919915522712,
      "grad_norm": 9341.189003547675,
      "learning_rate": 2.0298470854027214e-07,
      "loss": 1.7309,
      "step": 242944
    },
    {
      "epoch": 0.0008818081259031079,
      "grad_norm": 9851.03892998094,
      "learning_rate": 2.0297174630010312e-07,
      "loss": 1.7563,
      "step": 242976
    },
    {
      "epoch": 0.0008819242602539446,
      "grad_norm": 11610.12868145741,
      "learning_rate": 2.0295836852749792e-07,
      "loss": 1.7364,
      "step": 243008
    },
    {
      "epoch": 0.0008820403946047813,
      "grad_norm": 9228.5550331566,
      "learning_rate": 2.029449933997122e-07,
      "loss": 1.6933,
      "step": 243040
    },
    {
      "epoch": 0.000882156528955618,
      "grad_norm": 10430.487236941522,
      "learning_rate": 2.029316209158746e-07,
      "loss": 1.7011,
      "step": 243072
    },
    {
      "epoch": 0.0008822726633064547,
      "grad_norm": 11554.658973764652,
      "learning_rate": 2.0291825107511425e-07,
      "loss": 1.6933,
      "step": 243104
    },
    {
      "epoch": 0.0008823887976572915,
      "grad_norm": 9941.335222192238,
      "learning_rate": 2.0290488387656052e-07,
      "loss": 1.7037,
      "step": 243136
    },
    {
      "epoch": 0.0008825049320081281,
      "grad_norm": 8736.23351336261,
      "learning_rate": 2.0289151931934324e-07,
      "loss": 1.7153,
      "step": 243168
    },
    {
      "epoch": 0.0008826210663589649,
      "grad_norm": 10209.17019154838,
      "learning_rate": 2.028781574025927e-07,
      "loss": 1.7032,
      "step": 243200
    },
    {
      "epoch": 0.0008827372007098015,
      "grad_norm": 8152.97099222118,
      "learning_rate": 2.028647981254395e-07,
      "loss": 1.7065,
      "step": 243232
    },
    {
      "epoch": 0.0008828533350606383,
      "grad_norm": 10292.518642198322,
      "learning_rate": 2.028514414870147e-07,
      "loss": 1.7206,
      "step": 243264
    },
    {
      "epoch": 0.0008829694694114749,
      "grad_norm": 9967.506809628974,
      "learning_rate": 2.0283808748644976e-07,
      "loss": 1.7008,
      "step": 243296
    },
    {
      "epoch": 0.0008830856037623117,
      "grad_norm": 8256.42174310397,
      "learning_rate": 2.028247361228765e-07,
      "loss": 1.7167,
      "step": 243328
    },
    {
      "epoch": 0.0008832017381131483,
      "grad_norm": 11268.189561770781,
      "learning_rate": 2.0281138739542717e-07,
      "loss": 1.7203,
      "step": 243360
    },
    {
      "epoch": 0.0008833178724639851,
      "grad_norm": 9194.259078359713,
      "learning_rate": 2.027980413032344e-07,
      "loss": 1.7003,
      "step": 243392
    },
    {
      "epoch": 0.0008834340068148218,
      "grad_norm": 8355.553721926513,
      "learning_rate": 2.0278469784543125e-07,
      "loss": 1.7165,
      "step": 243424
    },
    {
      "epoch": 0.0008835501411656585,
      "grad_norm": 10195.937622406289,
      "learning_rate": 2.027713570211512e-07,
      "loss": 1.7051,
      "step": 243456
    },
    {
      "epoch": 0.0008836662755164952,
      "grad_norm": 10166.507561596558,
      "learning_rate": 2.0275801882952799e-07,
      "loss": 1.71,
      "step": 243488
    },
    {
      "epoch": 0.0008837824098673319,
      "grad_norm": 8833.319647788141,
      "learning_rate": 2.0274468326969595e-07,
      "loss": 1.7406,
      "step": 243520
    },
    {
      "epoch": 0.0008838985442181686,
      "grad_norm": 11048.17070831185,
      "learning_rate": 2.0273135034078967e-07,
      "loss": 1.7237,
      "step": 243552
    },
    {
      "epoch": 0.0008840146785690053,
      "grad_norm": 10161.5122890247,
      "learning_rate": 2.0271802004194425e-07,
      "loss": 1.7024,
      "step": 243584
    },
    {
      "epoch": 0.000884130812919842,
      "grad_norm": 10694.086029203243,
      "learning_rate": 2.0270469237229505e-07,
      "loss": 1.7075,
      "step": 243616
    },
    {
      "epoch": 0.0008842469472706787,
      "grad_norm": 9373.766905572167,
      "learning_rate": 2.0269136733097792e-07,
      "loss": 1.6944,
      "step": 243648
    },
    {
      "epoch": 0.0008843630816215154,
      "grad_norm": 10273.42338268992,
      "learning_rate": 2.0267804491712914e-07,
      "loss": 1.7229,
      "step": 243680
    },
    {
      "epoch": 0.0008844792159723522,
      "grad_norm": 10114.147912701297,
      "learning_rate": 2.026647251298853e-07,
      "loss": 1.7248,
      "step": 243712
    },
    {
      "epoch": 0.0008845953503231888,
      "grad_norm": 10187.300721977339,
      "learning_rate": 2.0265140796838341e-07,
      "loss": 1.715,
      "step": 243744
    },
    {
      "epoch": 0.0008847114846740256,
      "grad_norm": 8732.951276630369,
      "learning_rate": 2.026380934317609e-07,
      "loss": 1.7283,
      "step": 243776
    },
    {
      "epoch": 0.0008848276190248622,
      "grad_norm": 8711.28555380892,
      "learning_rate": 2.0262478151915563e-07,
      "loss": 1.7212,
      "step": 243808
    },
    {
      "epoch": 0.000884943753375699,
      "grad_norm": 9533.833017207717,
      "learning_rate": 2.0261147222970576e-07,
      "loss": 1.7201,
      "step": 243840
    },
    {
      "epoch": 0.0008850598877265356,
      "grad_norm": 9000.440433667676,
      "learning_rate": 2.0259816556254993e-07,
      "loss": 1.7364,
      "step": 243872
    },
    {
      "epoch": 0.0008851760220773724,
      "grad_norm": 11188.356090150151,
      "learning_rate": 2.0258486151682714e-07,
      "loss": 1.7058,
      "step": 243904
    },
    {
      "epoch": 0.000885292156428209,
      "grad_norm": 9316.429358933603,
      "learning_rate": 2.0257156009167677e-07,
      "loss": 1.6967,
      "step": 243936
    },
    {
      "epoch": 0.0008854082907790458,
      "grad_norm": 9906.68057423878,
      "learning_rate": 2.0255867683426331e-07,
      "loss": 1.7112,
      "step": 243968
    },
    {
      "epoch": 0.0008855244251298825,
      "grad_norm": 8371.302765997654,
      "learning_rate": 2.025453805658515e-07,
      "loss": 1.7047,
      "step": 244000
    },
    {
      "epoch": 0.0008856405594807192,
      "grad_norm": 9732.188654151747,
      "learning_rate": 2.0253208691545958e-07,
      "loss": 1.7254,
      "step": 244032
    },
    {
      "epoch": 0.0008857566938315559,
      "grad_norm": 8773.255496108613,
      "learning_rate": 2.025187958822285e-07,
      "loss": 1.7344,
      "step": 244064
    },
    {
      "epoch": 0.0008858728281823926,
      "grad_norm": 7823.264919456582,
      "learning_rate": 2.0250550746529964e-07,
      "loss": 1.7256,
      "step": 244096
    },
    {
      "epoch": 0.0008859889625332293,
      "grad_norm": 9497.893029509229,
      "learning_rate": 2.0249222166381474e-07,
      "loss": 1.749,
      "step": 244128
    },
    {
      "epoch": 0.000886105096884066,
      "grad_norm": 9337.295968319737,
      "learning_rate": 2.0247893847691595e-07,
      "loss": 1.7458,
      "step": 244160
    },
    {
      "epoch": 0.0008862212312349027,
      "grad_norm": 9917.226830117379,
      "learning_rate": 2.0246565790374586e-07,
      "loss": 1.7188,
      "step": 244192
    },
    {
      "epoch": 0.0008863373655857394,
      "grad_norm": 9205.439587548222,
      "learning_rate": 2.0245237994344736e-07,
      "loss": 1.7197,
      "step": 244224
    },
    {
      "epoch": 0.0008864534999365761,
      "grad_norm": 8240.947882373726,
      "learning_rate": 2.0243910459516382e-07,
      "loss": 1.714,
      "step": 244256
    },
    {
      "epoch": 0.0008865696342874129,
      "grad_norm": 9702.753938959804,
      "learning_rate": 2.0242583185803896e-07,
      "loss": 1.7051,
      "step": 244288
    },
    {
      "epoch": 0.0008866857686382495,
      "grad_norm": 10433.298998878543,
      "learning_rate": 2.0241256173121688e-07,
      "loss": 1.7283,
      "step": 244320
    },
    {
      "epoch": 0.0008868019029890863,
      "grad_norm": 9209.253172760536,
      "learning_rate": 2.0239929421384214e-07,
      "loss": 1.7164,
      "step": 244352
    },
    {
      "epoch": 0.0008869180373399229,
      "grad_norm": 10708.237576744363,
      "learning_rate": 2.023860293050596e-07,
      "loss": 1.7212,
      "step": 244384
    },
    {
      "epoch": 0.0008870341716907597,
      "grad_norm": 9585.584593544621,
      "learning_rate": 2.0237276700401458e-07,
      "loss": 1.7294,
      "step": 244416
    },
    {
      "epoch": 0.0008871503060415963,
      "grad_norm": 7306.772611762323,
      "learning_rate": 2.0235950730985273e-07,
      "loss": 1.706,
      "step": 244448
    },
    {
      "epoch": 0.0008872664403924331,
      "grad_norm": 9535.845111997152,
      "learning_rate": 2.023462502217202e-07,
      "loss": 1.7143,
      "step": 244480
    },
    {
      "epoch": 0.0008873825747432697,
      "grad_norm": 9480.299573325728,
      "learning_rate": 2.0233299573876345e-07,
      "loss": 1.7167,
      "step": 244512
    },
    {
      "epoch": 0.0008874987090941065,
      "grad_norm": 9704.418787336004,
      "learning_rate": 2.023197438601293e-07,
      "loss": 1.7268,
      "step": 244544
    },
    {
      "epoch": 0.0008876148434449432,
      "grad_norm": 10144.48145545153,
      "learning_rate": 2.0230649458496506e-07,
      "loss": 1.7232,
      "step": 244576
    },
    {
      "epoch": 0.0008877309777957799,
      "grad_norm": 9578.638525385537,
      "learning_rate": 2.022932479124183e-07,
      "loss": 1.7074,
      "step": 244608
    },
    {
      "epoch": 0.0008878471121466166,
      "grad_norm": 9512.888940800265,
      "learning_rate": 2.0228000384163716e-07,
      "loss": 1.7195,
      "step": 244640
    },
    {
      "epoch": 0.0008879632464974533,
      "grad_norm": 9415.705071846718,
      "learning_rate": 2.0226676237177e-07,
      "loss": 1.7412,
      "step": 244672
    },
    {
      "epoch": 0.00088807938084829,
      "grad_norm": 8363.247933667877,
      "learning_rate": 2.0225352350196562e-07,
      "loss": 1.7268,
      "step": 244704
    },
    {
      "epoch": 0.0008881955151991267,
      "grad_norm": 9579.224707668152,
      "learning_rate": 2.022402872313733e-07,
      "loss": 1.7465,
      "step": 244736
    },
    {
      "epoch": 0.0008883116495499634,
      "grad_norm": 9006.895802661425,
      "learning_rate": 2.0222705355914258e-07,
      "loss": 1.7297,
      "step": 244768
    },
    {
      "epoch": 0.0008884277839008001,
      "grad_norm": 9395.290096638848,
      "learning_rate": 2.0221382248442347e-07,
      "loss": 1.6801,
      "step": 244800
    },
    {
      "epoch": 0.0008885439182516368,
      "grad_norm": 10519.21460946586,
      "learning_rate": 2.0220059400636633e-07,
      "loss": 1.7013,
      "step": 244832
    },
    {
      "epoch": 0.0008886600526024736,
      "grad_norm": 9678.457418411263,
      "learning_rate": 2.0218736812412195e-07,
      "loss": 1.6996,
      "step": 244864
    },
    {
      "epoch": 0.0008887761869533102,
      "grad_norm": 9704.698037548618,
      "learning_rate": 2.021741448368415e-07,
      "loss": 1.7049,
      "step": 244896
    },
    {
      "epoch": 0.000888892321304147,
      "grad_norm": 9650.472527291085,
      "learning_rate": 2.0216092414367642e-07,
      "loss": 1.7153,
      "step": 244928
    },
    {
      "epoch": 0.0008890084556549836,
      "grad_norm": 15524.882479426375,
      "learning_rate": 2.0214770604377878e-07,
      "loss": 1.6915,
      "step": 244960
    },
    {
      "epoch": 0.0008891245900058204,
      "grad_norm": 8802.073619324028,
      "learning_rate": 2.021349034816772e-07,
      "loss": 1.7089,
      "step": 244992
    },
    {
      "epoch": 0.000889240724356657,
      "grad_norm": 9948.951301519171,
      "learning_rate": 2.0212169048479783e-07,
      "loss": 1.7197,
      "step": 245024
    },
    {
      "epoch": 0.0008893568587074938,
      "grad_norm": 9560.70436735704,
      "learning_rate": 2.021084800786704e-07,
      "loss": 1.6982,
      "step": 245056
    },
    {
      "epoch": 0.0008894729930583304,
      "grad_norm": 9018.066089799964,
      "learning_rate": 2.020952722624484e-07,
      "loss": 1.7065,
      "step": 245088
    },
    {
      "epoch": 0.0008895891274091672,
      "grad_norm": 10737.724526174063,
      "learning_rate": 2.020820670352857e-07,
      "loss": 1.7117,
      "step": 245120
    },
    {
      "epoch": 0.0008897052617600039,
      "grad_norm": 10937.577976864897,
      "learning_rate": 2.0206886439633653e-07,
      "loss": 1.6857,
      "step": 245152
    },
    {
      "epoch": 0.0008898213961108406,
      "grad_norm": 7316.7725125221705,
      "learning_rate": 2.020556643447555e-07,
      "loss": 1.702,
      "step": 245184
    },
    {
      "epoch": 0.0008899375304616773,
      "grad_norm": 11290.518765760944,
      "learning_rate": 2.0204246687969766e-07,
      "loss": 1.6995,
      "step": 245216
    },
    {
      "epoch": 0.000890053664812514,
      "grad_norm": 8888.59235199815,
      "learning_rate": 2.020292720003184e-07,
      "loss": 1.7086,
      "step": 245248
    },
    {
      "epoch": 0.0008901697991633507,
      "grad_norm": 9219.073923122647,
      "learning_rate": 2.020160797057735e-07,
      "loss": 1.7272,
      "step": 245280
    },
    {
      "epoch": 0.0008902859335141874,
      "grad_norm": 9074.997300275081,
      "learning_rate": 2.0200288999521917e-07,
      "loss": 1.7129,
      "step": 245312
    },
    {
      "epoch": 0.0008904020678650241,
      "grad_norm": 10393.446204219272,
      "learning_rate": 2.0198970286781194e-07,
      "loss": 1.7119,
      "step": 245344
    },
    {
      "epoch": 0.0008905182022158608,
      "grad_norm": 12045.562668468418,
      "learning_rate": 2.0197651832270876e-07,
      "loss": 1.7057,
      "step": 245376
    },
    {
      "epoch": 0.0008906343365666975,
      "grad_norm": 10154.724417727937,
      "learning_rate": 2.0196333635906695e-07,
      "loss": 1.7052,
      "step": 245408
    },
    {
      "epoch": 0.0008907504709175343,
      "grad_norm": 8613.00272843333,
      "learning_rate": 2.0195015697604426e-07,
      "loss": 1.7186,
      "step": 245440
    },
    {
      "epoch": 0.0008908666052683709,
      "grad_norm": 10408.75861954729,
      "learning_rate": 2.0193698017279877e-07,
      "loss": 1.7161,
      "step": 245472
    },
    {
      "epoch": 0.0008909827396192077,
      "grad_norm": 10466.485943238065,
      "learning_rate": 2.0192380594848895e-07,
      "loss": 1.7171,
      "step": 245504
    },
    {
      "epoch": 0.0008910988739700443,
      "grad_norm": 8461.579876122425,
      "learning_rate": 2.0191063430227372e-07,
      "loss": 1.7293,
      "step": 245536
    },
    {
      "epoch": 0.0008912150083208811,
      "grad_norm": 8830.427849204138,
      "learning_rate": 2.018974652333123e-07,
      "loss": 1.7081,
      "step": 245568
    },
    {
      "epoch": 0.0008913311426717177,
      "grad_norm": 9126.745641245843,
      "learning_rate": 2.0188429874076433e-07,
      "loss": 1.7255,
      "step": 245600
    },
    {
      "epoch": 0.0008914472770225545,
      "grad_norm": 9053.38367683597,
      "learning_rate": 2.018711348237898e-07,
      "loss": 1.7335,
      "step": 245632
    },
    {
      "epoch": 0.0008915634113733911,
      "grad_norm": 10089.21324980298,
      "learning_rate": 2.018579734815492e-07,
      "loss": 1.6987,
      "step": 245664
    },
    {
      "epoch": 0.0008916795457242279,
      "grad_norm": 9374.54084208928,
      "learning_rate": 2.0184481471320326e-07,
      "loss": 1.6955,
      "step": 245696
    },
    {
      "epoch": 0.0008917956800750646,
      "grad_norm": 9081.24462835354,
      "learning_rate": 2.0183165851791314e-07,
      "loss": 1.7038,
      "step": 245728
    },
    {
      "epoch": 0.0008919118144259013,
      "grad_norm": 10828.813785452217,
      "learning_rate": 2.018185048948404e-07,
      "loss": 1.6925,
      "step": 245760
    },
    {
      "epoch": 0.000892027948776738,
      "grad_norm": 11546.786739175535,
      "learning_rate": 2.01805353843147e-07,
      "loss": 1.7121,
      "step": 245792
    },
    {
      "epoch": 0.0008921440831275747,
      "grad_norm": 8031.294914271297,
      "learning_rate": 2.0179220536199527e-07,
      "loss": 1.712,
      "step": 245824
    },
    {
      "epoch": 0.0008922602174784114,
      "grad_norm": 10345.31488162637,
      "learning_rate": 2.0177905945054783e-07,
      "loss": 1.7024,
      "step": 245856
    },
    {
      "epoch": 0.0008923763518292481,
      "grad_norm": 9316.223269115011,
      "learning_rate": 2.0176591610796787e-07,
      "loss": 1.7346,
      "step": 245888
    },
    {
      "epoch": 0.0008924924861800848,
      "grad_norm": 9827.786322463468,
      "learning_rate": 2.0175277533341877e-07,
      "loss": 1.7194,
      "step": 245920
    },
    {
      "epoch": 0.0008926086205309215,
      "grad_norm": 8560.40069155644,
      "learning_rate": 2.0173963712606443e-07,
      "loss": 1.711,
      "step": 245952
    },
    {
      "epoch": 0.0008927247548817582,
      "grad_norm": 8845.880397111414,
      "learning_rate": 2.0172691193501218e-07,
      "loss": 1.7058,
      "step": 245984
    },
    {
      "epoch": 0.000892840889232595,
      "grad_norm": 10078.147647261376,
      "learning_rate": 2.017137787793804e-07,
      "loss": 1.6986,
      "step": 246016
    },
    {
      "epoch": 0.0008929570235834316,
      "grad_norm": 10968.03154627119,
      "learning_rate": 2.0170064818846325e-07,
      "loss": 1.6992,
      "step": 246048
    },
    {
      "epoch": 0.0008930731579342684,
      "grad_norm": 9019.915742400259,
      "learning_rate": 2.016875201614261e-07,
      "loss": 1.7125,
      "step": 246080
    },
    {
      "epoch": 0.000893189292285105,
      "grad_norm": 10305.372191240838,
      "learning_rate": 2.0167439469743468e-07,
      "loss": 1.7019,
      "step": 246112
    },
    {
      "epoch": 0.0008933054266359418,
      "grad_norm": 9704.431565011935,
      "learning_rate": 2.0166127179565513e-07,
      "loss": 1.7066,
      "step": 246144
    },
    {
      "epoch": 0.0008934215609867784,
      "grad_norm": 8986.75469788733,
      "learning_rate": 2.0164815145525392e-07,
      "loss": 1.7067,
      "step": 246176
    },
    {
      "epoch": 0.0008935376953376152,
      "grad_norm": 9829.317778971234,
      "learning_rate": 2.0163503367539798e-07,
      "loss": 1.6841,
      "step": 246208
    },
    {
      "epoch": 0.0008936538296884518,
      "grad_norm": 11152.102223347849,
      "learning_rate": 2.0162191845525445e-07,
      "loss": 1.7,
      "step": 246240
    },
    {
      "epoch": 0.0008937699640392886,
      "grad_norm": 11020.554069555668,
      "learning_rate": 2.0160880579399108e-07,
      "loss": 1.6952,
      "step": 246272
    },
    {
      "epoch": 0.0008938860983901254,
      "grad_norm": 10019.82494857071,
      "learning_rate": 2.015956956907758e-07,
      "loss": 1.7125,
      "step": 246304
    },
    {
      "epoch": 0.000894002232740962,
      "grad_norm": 8807.911216627925,
      "learning_rate": 2.015825881447771e-07,
      "loss": 1.7189,
      "step": 246336
    },
    {
      "epoch": 0.0008941183670917988,
      "grad_norm": 8852.278350797607,
      "learning_rate": 2.0156948315516363e-07,
      "loss": 1.7099,
      "step": 246368
    },
    {
      "epoch": 0.0008942345014426354,
      "grad_norm": 9836.983887350838,
      "learning_rate": 2.015563807211046e-07,
      "loss": 1.7256,
      "step": 246400
    },
    {
      "epoch": 0.0008943506357934722,
      "grad_norm": 12119.779865987666,
      "learning_rate": 2.0154328084176957e-07,
      "loss": 1.7318,
      "step": 246432
    },
    {
      "epoch": 0.0008944667701443088,
      "grad_norm": 9283.227671451348,
      "learning_rate": 2.015301835163284e-07,
      "loss": 1.7443,
      "step": 246464
    },
    {
      "epoch": 0.0008945829044951456,
      "grad_norm": 9252.804115510065,
      "learning_rate": 2.0151708874395136e-07,
      "loss": 1.7561,
      "step": 246496
    },
    {
      "epoch": 0.0008946990388459822,
      "grad_norm": 10209.107306713942,
      "learning_rate": 2.0150399652380913e-07,
      "loss": 1.7315,
      "step": 246528
    },
    {
      "epoch": 0.000894815173196819,
      "grad_norm": 10572.116155245363,
      "learning_rate": 2.0149090685507276e-07,
      "loss": 1.6949,
      "step": 246560
    },
    {
      "epoch": 0.0008949313075476557,
      "grad_norm": 12167.053217603678,
      "learning_rate": 2.0147781973691364e-07,
      "loss": 1.7016,
      "step": 246592
    },
    {
      "epoch": 0.0008950474418984924,
      "grad_norm": 9930.273510835439,
      "learning_rate": 2.0146473516850358e-07,
      "loss": 1.6912,
      "step": 246624
    },
    {
      "epoch": 0.0008951635762493291,
      "grad_norm": 9300.470095645704,
      "learning_rate": 2.014516531490147e-07,
      "loss": 1.7228,
      "step": 246656
    },
    {
      "epoch": 0.0008952797106001658,
      "grad_norm": 10612.151148565497,
      "learning_rate": 2.014385736776196e-07,
      "loss": 1.7178,
      "step": 246688
    },
    {
      "epoch": 0.0008953958449510025,
      "grad_norm": 11172.884497747213,
      "learning_rate": 2.0142549675349117e-07,
      "loss": 1.6967,
      "step": 246720
    },
    {
      "epoch": 0.0008955119793018392,
      "grad_norm": 8893.947155228663,
      "learning_rate": 2.014124223758027e-07,
      "loss": 1.7072,
      "step": 246752
    },
    {
      "epoch": 0.0008956281136526759,
      "grad_norm": 8911.663368866668,
      "learning_rate": 2.0139935054372788e-07,
      "loss": 1.7215,
      "step": 246784
    },
    {
      "epoch": 0.0008957442480035126,
      "grad_norm": 9951.9226283166,
      "learning_rate": 2.0138628125644075e-07,
      "loss": 1.702,
      "step": 246816
    },
    {
      "epoch": 0.0008958603823543493,
      "grad_norm": 9477.724410426797,
      "learning_rate": 2.0137321451311572e-07,
      "loss": 1.7087,
      "step": 246848
    },
    {
      "epoch": 0.0008959765167051861,
      "grad_norm": 11472.86119501147,
      "learning_rate": 2.0136015031292755e-07,
      "loss": 1.7056,
      "step": 246880
    },
    {
      "epoch": 0.0008960926510560227,
      "grad_norm": 8117.490252534954,
      "learning_rate": 2.0134708865505149e-07,
      "loss": 1.6837,
      "step": 246912
    },
    {
      "epoch": 0.0008962087854068595,
      "grad_norm": 9630.33270453311,
      "learning_rate": 2.0133402953866303e-07,
      "loss": 1.7066,
      "step": 246944
    },
    {
      "epoch": 0.0008963249197576961,
      "grad_norm": 8738.638452299076,
      "learning_rate": 2.0132097296293811e-07,
      "loss": 1.7004,
      "step": 246976
    },
    {
      "epoch": 0.0008964410541085329,
      "grad_norm": 8920.111658493968,
      "learning_rate": 2.0130832682723774e-07,
      "loss": 1.7169,
      "step": 247008
    },
    {
      "epoch": 0.0008965571884593695,
      "grad_norm": 8155.210359028147,
      "learning_rate": 2.0129527525103735e-07,
      "loss": 1.7272,
      "step": 247040
    },
    {
      "epoch": 0.0008966733228102063,
      "grad_norm": 10065.158915784688,
      "learning_rate": 2.0128222621305617e-07,
      "loss": 1.7155,
      "step": 247072
    },
    {
      "epoch": 0.0008967894571610429,
      "grad_norm": 9751.464710493496,
      "learning_rate": 2.0126917971247161e-07,
      "loss": 1.7162,
      "step": 247104
    },
    {
      "epoch": 0.0008969055915118797,
      "grad_norm": 12172.372817162643,
      "learning_rate": 2.012561357484615e-07,
      "loss": 1.6991,
      "step": 247136
    },
    {
      "epoch": 0.0008970217258627164,
      "grad_norm": 8708.190971723117,
      "learning_rate": 2.0124309432020394e-07,
      "loss": 1.6959,
      "step": 247168
    },
    {
      "epoch": 0.0008971378602135531,
      "grad_norm": 8612.049465719527,
      "learning_rate": 2.012300554268775e-07,
      "loss": 1.718,
      "step": 247200
    },
    {
      "epoch": 0.0008972539945643898,
      "grad_norm": 8597.145340169607,
      "learning_rate": 2.0121701906766103e-07,
      "loss": 1.7122,
      "step": 247232
    },
    {
      "epoch": 0.0008973701289152265,
      "grad_norm": 9924.98705288828,
      "learning_rate": 2.0120398524173378e-07,
      "loss": 1.7163,
      "step": 247264
    },
    {
      "epoch": 0.0008974862632660632,
      "grad_norm": 10466.436451820648,
      "learning_rate": 2.0119095394827544e-07,
      "loss": 1.741,
      "step": 247296
    },
    {
      "epoch": 0.0008976023976168999,
      "grad_norm": 8837.930526995558,
      "learning_rate": 2.01177925186466e-07,
      "loss": 1.7131,
      "step": 247328
    },
    {
      "epoch": 0.0008977185319677366,
      "grad_norm": 9062.716811199609,
      "learning_rate": 2.0116489895548582e-07,
      "loss": 1.7378,
      "step": 247360
    },
    {
      "epoch": 0.0008978346663185733,
      "grad_norm": 10279.332274034146,
      "learning_rate": 2.0115187525451566e-07,
      "loss": 1.7336,
      "step": 247392
    },
    {
      "epoch": 0.00089795080066941,
      "grad_norm": 8762.789852552667,
      "learning_rate": 2.0113885408273665e-07,
      "loss": 1.6988,
      "step": 247424
    },
    {
      "epoch": 0.0008980669350202468,
      "grad_norm": 7912.175048619691,
      "learning_rate": 2.011258354393303e-07,
      "loss": 1.7037,
      "step": 247456
    },
    {
      "epoch": 0.0008981830693710834,
      "grad_norm": 9019.694784193089,
      "learning_rate": 2.0111281932347846e-07,
      "loss": 1.7136,
      "step": 247488
    },
    {
      "epoch": 0.0008982992037219202,
      "grad_norm": 9271.259029926841,
      "learning_rate": 2.0109980573436335e-07,
      "loss": 1.7121,
      "step": 247520
    },
    {
      "epoch": 0.0008984153380727568,
      "grad_norm": 9065.673278913155,
      "learning_rate": 2.0108679467116763e-07,
      "loss": 1.7288,
      "step": 247552
    },
    {
      "epoch": 0.0008985314724235936,
      "grad_norm": 8837.778793339421,
      "learning_rate": 2.010737861330742e-07,
      "loss": 1.7123,
      "step": 247584
    },
    {
      "epoch": 0.0008986476067744302,
      "grad_norm": 10035.582494304952,
      "learning_rate": 2.0106078011926647e-07,
      "loss": 1.7075,
      "step": 247616
    },
    {
      "epoch": 0.000898763741125267,
      "grad_norm": 8847.26895714152,
      "learning_rate": 2.0104777662892813e-07,
      "loss": 1.7407,
      "step": 247648
    },
    {
      "epoch": 0.0008988798754761036,
      "grad_norm": 9394.519891937001,
      "learning_rate": 2.010347756612433e-07,
      "loss": 1.7167,
      "step": 247680
    },
    {
      "epoch": 0.0008989960098269404,
      "grad_norm": 10743.932799492,
      "learning_rate": 2.0102177721539637e-07,
      "loss": 1.7162,
      "step": 247712
    },
    {
      "epoch": 0.0008991121441777771,
      "grad_norm": 11121.247771720582,
      "learning_rate": 2.010087812905722e-07,
      "loss": 1.7029,
      "step": 247744
    },
    {
      "epoch": 0.0008992282785286138,
      "grad_norm": 8943.516757964957,
      "learning_rate": 2.00995787885956e-07,
      "loss": 1.6905,
      "step": 247776
    },
    {
      "epoch": 0.0008993444128794505,
      "grad_norm": 9248.657199831769,
      "learning_rate": 2.009827970007333e-07,
      "loss": 1.6922,
      "step": 247808
    },
    {
      "epoch": 0.0008994605472302872,
      "grad_norm": 9597.787661747889,
      "learning_rate": 2.0096980863409005e-07,
      "loss": 1.6988,
      "step": 247840
    },
    {
      "epoch": 0.0008995766815811239,
      "grad_norm": 10376.447754409985,
      "learning_rate": 2.0095682278521252e-07,
      "loss": 1.7133,
      "step": 247872
    },
    {
      "epoch": 0.0008996928159319606,
      "grad_norm": 9792.867812852372,
      "learning_rate": 2.0094383945328743e-07,
      "loss": 1.7155,
      "step": 247904
    },
    {
      "epoch": 0.0008998089502827973,
      "grad_norm": 8339.246728572072,
      "learning_rate": 2.0093085863750177e-07,
      "loss": 1.706,
      "step": 247936
    },
    {
      "epoch": 0.000899925084633634,
      "grad_norm": 9054.367564882707,
      "learning_rate": 2.0091788033704297e-07,
      "loss": 1.6893,
      "step": 247968
    },
    {
      "epoch": 0.0009000412189844707,
      "grad_norm": 20889.17422972962,
      "learning_rate": 2.0090490455109878e-07,
      "loss": 1.706,
      "step": 248000
    },
    {
      "epoch": 0.0009001573533353075,
      "grad_norm": 9856.98331133821,
      "learning_rate": 2.0089233665557372e-07,
      "loss": 1.6928,
      "step": 248032
    },
    {
      "epoch": 0.0009002734876861441,
      "grad_norm": 8908.39974406178,
      "learning_rate": 2.0087936581770796e-07,
      "loss": 1.7132,
      "step": 248064
    },
    {
      "epoch": 0.0009003896220369809,
      "grad_norm": 10935.407079757022,
      "learning_rate": 2.0086639749194766e-07,
      "loss": 1.7158,
      "step": 248096
    },
    {
      "epoch": 0.0009005057563878175,
      "grad_norm": 9668.03040955085,
      "learning_rate": 2.0085343167748205e-07,
      "loss": 1.7046,
      "step": 248128
    },
    {
      "epoch": 0.0009006218907386543,
      "grad_norm": 8807.167876224456,
      "learning_rate": 2.0084046837350073e-07,
      "loss": 1.7331,
      "step": 248160
    },
    {
      "epoch": 0.0009007380250894909,
      "grad_norm": 8547.120099776299,
      "learning_rate": 2.0082750757919363e-07,
      "loss": 1.7337,
      "step": 248192
    },
    {
      "epoch": 0.0009008541594403277,
      "grad_norm": 9704.884234239995,
      "learning_rate": 2.0081454929375113e-07,
      "loss": 1.7429,
      "step": 248224
    },
    {
      "epoch": 0.0009009702937911643,
      "grad_norm": 8446.492644879294,
      "learning_rate": 2.0080159351636384e-07,
      "loss": 1.757,
      "step": 248256
    },
    {
      "epoch": 0.0009010864281420011,
      "grad_norm": 9536.339549323944,
      "learning_rate": 2.0078864024622287e-07,
      "loss": 1.7332,
      "step": 248288
    },
    {
      "epoch": 0.0009012025624928378,
      "grad_norm": 9647.112417713395,
      "learning_rate": 2.0077568948251963e-07,
      "loss": 1.698,
      "step": 248320
    },
    {
      "epoch": 0.0009013186968436745,
      "grad_norm": 9882.67372728656,
      "learning_rate": 2.007627412244459e-07,
      "loss": 1.718,
      "step": 248352
    },
    {
      "epoch": 0.0009014348311945112,
      "grad_norm": 8570.227418219425,
      "learning_rate": 2.0074979547119383e-07,
      "loss": 1.6964,
      "step": 248384
    },
    {
      "epoch": 0.0009015509655453479,
      "grad_norm": 9173.282291524665,
      "learning_rate": 2.0073685222195593e-07,
      "loss": 1.7118,
      "step": 248416
    },
    {
      "epoch": 0.0009016670998961846,
      "grad_norm": 9683.360367145282,
      "learning_rate": 2.0072391147592506e-07,
      "loss": 1.7179,
      "step": 248448
    },
    {
      "epoch": 0.0009017832342470213,
      "grad_norm": 10730.62477211835,
      "learning_rate": 2.0071097323229454e-07,
      "loss": 1.6982,
      "step": 248480
    },
    {
      "epoch": 0.000901899368597858,
      "grad_norm": 11116.061712675042,
      "learning_rate": 2.0069803749025787e-07,
      "loss": 1.705,
      "step": 248512
    },
    {
      "epoch": 0.0009020155029486947,
      "grad_norm": 9777.409677414566,
      "learning_rate": 2.0068510424900908e-07,
      "loss": 1.7153,
      "step": 248544
    },
    {
      "epoch": 0.0009021316372995314,
      "grad_norm": 8693.761901501559,
      "learning_rate": 2.0067217350774252e-07,
      "loss": 1.6901,
      "step": 248576
    },
    {
      "epoch": 0.0009022477716503682,
      "grad_norm": 8673.474736228844,
      "learning_rate": 2.006592452656528e-07,
      "loss": 1.7146,
      "step": 248608
    },
    {
      "epoch": 0.0009023639060012048,
      "grad_norm": 10338.324815945763,
      "learning_rate": 2.006463195219351e-07,
      "loss": 1.7061,
      "step": 248640
    },
    {
      "epoch": 0.0009024800403520416,
      "grad_norm": 12382.044419238691,
      "learning_rate": 2.006333962757848e-07,
      "loss": 1.685,
      "step": 248672
    },
    {
      "epoch": 0.0009025961747028782,
      "grad_norm": 8882.490979449401,
      "learning_rate": 2.006204755263976e-07,
      "loss": 1.7112,
      "step": 248704
    },
    {
      "epoch": 0.000902712309053715,
      "grad_norm": 11513.233081980057,
      "learning_rate": 2.0060755727296976e-07,
      "loss": 1.6899,
      "step": 248736
    },
    {
      "epoch": 0.0009028284434045516,
      "grad_norm": 9830.678918569154,
      "learning_rate": 2.005946415146978e-07,
      "loss": 1.7159,
      "step": 248768
    },
    {
      "epoch": 0.0009029445777553884,
      "grad_norm": 9120.176971967156,
      "learning_rate": 2.0058172825077849e-07,
      "loss": 1.7294,
      "step": 248800
    },
    {
      "epoch": 0.000903060712106225,
      "grad_norm": 9819.850915365263,
      "learning_rate": 2.005688174804091e-07,
      "loss": 1.7156,
      "step": 248832
    },
    {
      "epoch": 0.0009031768464570618,
      "grad_norm": 9473.862570250847,
      "learning_rate": 2.0055590920278726e-07,
      "loss": 1.7178,
      "step": 248864
    },
    {
      "epoch": 0.0009032929808078985,
      "grad_norm": 9569.394651700806,
      "learning_rate": 2.0054300341711093e-07,
      "loss": 1.7112,
      "step": 248896
    },
    {
      "epoch": 0.0009034091151587352,
      "grad_norm": 9049.916021709814,
      "learning_rate": 2.0053010012257837e-07,
      "loss": 1.7097,
      "step": 248928
    },
    {
      "epoch": 0.0009035252495095719,
      "grad_norm": 10851.68429323301,
      "learning_rate": 2.0051719931838833e-07,
      "loss": 1.7279,
      "step": 248960
    },
    {
      "epoch": 0.0009036413838604086,
      "grad_norm": 8370.340972744181,
      "learning_rate": 2.0050430100373982e-07,
      "loss": 1.7302,
      "step": 248992
    },
    {
      "epoch": 0.0009037575182112453,
      "grad_norm": 8769.435557662762,
      "learning_rate": 2.004918081347284e-07,
      "loss": 1.7283,
      "step": 249024
    },
    {
      "epoch": 0.000903873652562082,
      "grad_norm": 9144.418188162657,
      "learning_rate": 2.004789147190255e-07,
      "loss": 1.7536,
      "step": 249056
    },
    {
      "epoch": 0.0009039897869129187,
      "grad_norm": 11553.424773633142,
      "learning_rate": 2.0046602379048836e-07,
      "loss": 1.7249,
      "step": 249088
    },
    {
      "epoch": 0.0009041059212637554,
      "grad_norm": 8562.920062688896,
      "learning_rate": 2.0045313534831754e-07,
      "loss": 1.7482,
      "step": 249120
    },
    {
      "epoch": 0.0009042220556145921,
      "grad_norm": 11136.640426986947,
      "learning_rate": 2.004402493917138e-07,
      "loss": 1.7373,
      "step": 249152
    },
    {
      "epoch": 0.0009043381899654289,
      "grad_norm": 10216.127642115674,
      "learning_rate": 2.0042736591987836e-07,
      "loss": 1.7042,
      "step": 249184
    },
    {
      "epoch": 0.0009044543243162655,
      "grad_norm": 9978.105431393276,
      "learning_rate": 2.0041448493201276e-07,
      "loss": 1.711,
      "step": 249216
    },
    {
      "epoch": 0.0009045704586671023,
      "grad_norm": 9210.166013704638,
      "learning_rate": 2.0040160642731893e-07,
      "loss": 1.7142,
      "step": 249248
    },
    {
      "epoch": 0.0009046865930179389,
      "grad_norm": 9589.200175197095,
      "learning_rate": 2.0038873040499912e-07,
      "loss": 1.7109,
      "step": 249280
    },
    {
      "epoch": 0.0009048027273687757,
      "grad_norm": 8614.07302035454,
      "learning_rate": 2.0037585686425596e-07,
      "loss": 1.7227,
      "step": 249312
    },
    {
      "epoch": 0.0009049188617196123,
      "grad_norm": 11700.93090313758,
      "learning_rate": 2.0036298580429244e-07,
      "loss": 1.7197,
      "step": 249344
    },
    {
      "epoch": 0.0009050349960704491,
      "grad_norm": 9016.130877488415,
      "learning_rate": 2.003501172243119e-07,
      "loss": 1.717,
      "step": 249376
    },
    {
      "epoch": 0.0009051511304212857,
      "grad_norm": 9507.438982186528,
      "learning_rate": 2.00337251123518e-07,
      "loss": 1.7569,
      "step": 249408
    },
    {
      "epoch": 0.0009052672647721225,
      "grad_norm": 10746.351566927262,
      "learning_rate": 2.0032438750111487e-07,
      "loss": 1.7247,
      "step": 249440
    },
    {
      "epoch": 0.0009053833991229592,
      "grad_norm": 10068.975717519632,
      "learning_rate": 2.003115263563069e-07,
      "loss": 1.73,
      "step": 249472
    },
    {
      "epoch": 0.0009054995334737959,
      "grad_norm": 11248.465317544433,
      "learning_rate": 2.0029866768829884e-07,
      "loss": 1.716,
      "step": 249504
    },
    {
      "epoch": 0.0009056156678246326,
      "grad_norm": 10443.782360811623,
      "learning_rate": 2.0028581149629585e-07,
      "loss": 1.6848,
      "step": 249536
    },
    {
      "epoch": 0.0009057318021754693,
      "grad_norm": 8450.008875734984,
      "learning_rate": 2.0027295777950342e-07,
      "loss": 1.6879,
      "step": 249568
    },
    {
      "epoch": 0.000905847936526306,
      "grad_norm": 9898.644351627147,
      "learning_rate": 2.0026010653712737e-07,
      "loss": 1.7022,
      "step": 249600
    },
    {
      "epoch": 0.0009059640708771427,
      "grad_norm": 9141.571418525373,
      "learning_rate": 2.0024725776837393e-07,
      "loss": 1.7024,
      "step": 249632
    },
    {
      "epoch": 0.0009060802052279794,
      "grad_norm": 10055.748405762746,
      "learning_rate": 2.0023441147244962e-07,
      "loss": 1.7093,
      "step": 249664
    },
    {
      "epoch": 0.0009061963395788161,
      "grad_norm": 8890.755873377697,
      "learning_rate": 2.002215676485614e-07,
      "loss": 1.6948,
      "step": 249696
    },
    {
      "epoch": 0.0009063124739296528,
      "grad_norm": 8175.276631405203,
      "learning_rate": 2.0020872629591652e-07,
      "loss": 1.6821,
      "step": 249728
    },
    {
      "epoch": 0.0009064286082804896,
      "grad_norm": 9176.692214518258,
      "learning_rate": 2.0019588741372262e-07,
      "loss": 1.7111,
      "step": 249760
    },
    {
      "epoch": 0.0009065447426313262,
      "grad_norm": 9500.259891181924,
      "learning_rate": 2.0018305100118763e-07,
      "loss": 1.6963,
      "step": 249792
    },
    {
      "epoch": 0.000906660876982163,
      "grad_norm": 11171.54778891448,
      "learning_rate": 2.0017021705752e-07,
      "loss": 1.7125,
      "step": 249824
    },
    {
      "epoch": 0.0009067770113329996,
      "grad_norm": 10185.814645869028,
      "learning_rate": 2.001573855819283e-07,
      "loss": 1.7188,
      "step": 249856
    },
    {
      "epoch": 0.0009068931456838364,
      "grad_norm": 9687.636966773682,
      "learning_rate": 2.0014455657362162e-07,
      "loss": 1.7001,
      "step": 249888
    },
    {
      "epoch": 0.000907009280034673,
      "grad_norm": 8749.17321808181,
      "learning_rate": 2.0013173003180938e-07,
      "loss": 1.7257,
      "step": 249920
    },
    {
      "epoch": 0.0009071254143855098,
      "grad_norm": 9421.632024230197,
      "learning_rate": 2.0011890595570134e-07,
      "loss": 1.734,
      "step": 249952
    },
    {
      "epoch": 0.0009072415487363464,
      "grad_norm": 10024.630866021951,
      "learning_rate": 2.0010608434450761e-07,
      "loss": 1.7469,
      "step": 249984
    },
    {
      "epoch": 0.0009073576830871832,
      "grad_norm": 11236.505150624014,
      "learning_rate": 2.0009366575849362e-07,
      "loss": 1.7509,
      "step": 250016
    },
    {
      "epoch": 0.00090747381743802,
      "grad_norm": 9088.842610585794,
      "learning_rate": 2.0008084899779296e-07,
      "loss": 1.7243,
      "step": 250048
    },
    {
      "epoch": 0.0009075899517888566,
      "grad_norm": 9424.38623996279,
      "learning_rate": 2.0006803469966373e-07,
      "loss": 1.6846,
      "step": 250080
    },
    {
      "epoch": 0.0009077060861396933,
      "grad_norm": 9339.154565591041,
      "learning_rate": 2.0005522286331738e-07,
      "loss": 1.7054,
      "step": 250112
    },
    {
      "epoch": 0.00090782222049053,
      "grad_norm": 10787.01385926615,
      "learning_rate": 2.0004241348796584e-07,
      "loss": 1.6913,
      "step": 250144
    },
    {
      "epoch": 0.0009079383548413667,
      "grad_norm": 10098.76101311443,
      "learning_rate": 2.0002960657282133e-07,
      "loss": 1.7065,
      "step": 250176
    },
    {
      "epoch": 0.0009080544891922034,
      "grad_norm": 8980.864657704178,
      "learning_rate": 2.000168021170964e-07,
      "loss": 1.7122,
      "step": 250208
    },
    {
      "epoch": 0.0009081706235430401,
      "grad_norm": 11430.977910922582,
      "learning_rate": 2.0000400012000402e-07,
      "loss": 1.693,
      "step": 250240
    },
    {
      "epoch": 0.0009082867578938768,
      "grad_norm": 10372.25529959613,
      "learning_rate": 1.999912005807574e-07,
      "loss": 1.7103,
      "step": 250272
    },
    {
      "epoch": 0.0009084028922447135,
      "grad_norm": 8017.559354317248,
      "learning_rate": 1.999784034985703e-07,
      "loss": 1.7073,
      "step": 250304
    },
    {
      "epoch": 0.0009085190265955503,
      "grad_norm": 10393.056528278868,
      "learning_rate": 1.9996560887265655e-07,
      "loss": 1.7001,
      "step": 250336
    },
    {
      "epoch": 0.0009086351609463869,
      "grad_norm": 10669.40588786461,
      "learning_rate": 1.9995281670223059e-07,
      "loss": 1.7061,
      "step": 250368
    },
    {
      "epoch": 0.0009087512952972237,
      "grad_norm": 11828.096211986105,
      "learning_rate": 1.999400269865071e-07,
      "loss": 1.7002,
      "step": 250400
    },
    {
      "epoch": 0.0009088674296480603,
      "grad_norm": 10420.011516308416,
      "learning_rate": 1.999272397247011e-07,
      "loss": 1.6865,
      "step": 250432
    },
    {
      "epoch": 0.0009089835639988971,
      "grad_norm": 11058.26803798859,
      "learning_rate": 1.9991445491602797e-07,
      "loss": 1.7061,
      "step": 250464
    },
    {
      "epoch": 0.0009090996983497337,
      "grad_norm": 9283.52626968869,
      "learning_rate": 1.999016725597035e-07,
      "loss": 1.6916,
      "step": 250496
    },
    {
      "epoch": 0.0009092158327005705,
      "grad_norm": 10349.420853361797,
      "learning_rate": 1.9988889265494374e-07,
      "loss": 1.7151,
      "step": 250528
    },
    {
      "epoch": 0.0009093319670514071,
      "grad_norm": 8590.226772326794,
      "learning_rate": 1.9987611520096516e-07,
      "loss": 1.7229,
      "step": 250560
    },
    {
      "epoch": 0.0009094481014022439,
      "grad_norm": 11162.776536328227,
      "learning_rate": 1.9986334019698454e-07,
      "loss": 1.7091,
      "step": 250592
    },
    {
      "epoch": 0.0009095642357530807,
      "grad_norm": 10259.353195986578,
      "learning_rate": 1.9985056764221906e-07,
      "loss": 1.7157,
      "step": 250624
    },
    {
      "epoch": 0.0009096803701039173,
      "grad_norm": 8308.003610976586,
      "learning_rate": 1.998377975358862e-07,
      "loss": 1.6986,
      "step": 250656
    },
    {
      "epoch": 0.000909796504454754,
      "grad_norm": 9189.499007018827,
      "learning_rate": 1.9982502987720376e-07,
      "loss": 1.698,
      "step": 250688
    },
    {
      "epoch": 0.0009099126388055907,
      "grad_norm": 10162.955672440965,
      "learning_rate": 1.9981226466539e-07,
      "loss": 1.7109,
      "step": 250720
    },
    {
      "epoch": 0.0009100287731564275,
      "grad_norm": 9240.250213062414,
      "learning_rate": 1.9979950189966346e-07,
      "loss": 1.7064,
      "step": 250752
    },
    {
      "epoch": 0.0009101449075072641,
      "grad_norm": 10962.571413678454,
      "learning_rate": 1.99786741579243e-07,
      "loss": 1.7163,
      "step": 250784
    },
    {
      "epoch": 0.0009102610418581009,
      "grad_norm": 10418.609696115887,
      "learning_rate": 1.997739837033479e-07,
      "loss": 1.733,
      "step": 250816
    },
    {
      "epoch": 0.0009103771762089375,
      "grad_norm": 8883.347792358465,
      "learning_rate": 1.9976122827119773e-07,
      "loss": 1.7113,
      "step": 250848
    },
    {
      "epoch": 0.0009104933105597743,
      "grad_norm": 9753.891531076199,
      "learning_rate": 1.9974847528201242e-07,
      "loss": 1.73,
      "step": 250880
    },
    {
      "epoch": 0.000910609444910611,
      "grad_norm": 9060.876778767053,
      "learning_rate": 1.9973572473501232e-07,
      "loss": 1.7133,
      "step": 250912
    },
    {
      "epoch": 0.0009107255792614477,
      "grad_norm": 9483.311868751338,
      "learning_rate": 1.99722976629418e-07,
      "loss": 1.688,
      "step": 250944
    },
    {
      "epoch": 0.0009108417136122844,
      "grad_norm": 9643.839795434182,
      "learning_rate": 1.997102309644505e-07,
      "loss": 1.7026,
      "step": 250976
    },
    {
      "epoch": 0.000910957847963121,
      "grad_norm": 10308.823987245101,
      "learning_rate": 1.996974877393311e-07,
      "loss": 1.6993,
      "step": 251008
    },
    {
      "epoch": 0.0009110739823139578,
      "grad_norm": 9485.492712558478,
      "learning_rate": 1.9968514506593384e-07,
      "loss": 1.7122,
      "step": 251040
    },
    {
      "epoch": 0.0009111901166647945,
      "grad_norm": 11286.440182803433,
      "learning_rate": 1.9967240664199124e-07,
      "loss": 1.7019,
      "step": 251072
    },
    {
      "epoch": 0.0009113062510156312,
      "grad_norm": 8179.951711348912,
      "learning_rate": 1.9965967065558722e-07,
      "loss": 1.704,
      "step": 251104
    },
    {
      "epoch": 0.0009114223853664679,
      "grad_norm": 10949.06187762221,
      "learning_rate": 1.9964693710594442e-07,
      "loss": 1.7069,
      "step": 251136
    },
    {
      "epoch": 0.0009115385197173046,
      "grad_norm": 10365.320448495551,
      "learning_rate": 1.9963420599228592e-07,
      "loss": 1.7361,
      "step": 251168
    },
    {
      "epoch": 0.0009116546540681414,
      "grad_norm": 11397.369345599009,
      "learning_rate": 1.996214773138351e-07,
      "loss": 1.7064,
      "step": 251200
    },
    {
      "epoch": 0.000911770788418978,
      "grad_norm": 8945.337780095282,
      "learning_rate": 1.9960875106981576e-07,
      "loss": 1.7191,
      "step": 251232
    },
    {
      "epoch": 0.0009118869227698148,
      "grad_norm": 10001.40210170554,
      "learning_rate": 1.9959602725945196e-07,
      "loss": 1.7051,
      "step": 251264
    },
    {
      "epoch": 0.0009120030571206514,
      "grad_norm": 9071.874337754023,
      "learning_rate": 1.995833058819682e-07,
      "loss": 1.6873,
      "step": 251296
    },
    {
      "epoch": 0.0009121191914714882,
      "grad_norm": 10670.929294114923,
      "learning_rate": 1.9957058693658923e-07,
      "loss": 1.6985,
      "step": 251328
    },
    {
      "epoch": 0.0009122353258223248,
      "grad_norm": 9981.184999788353,
      "learning_rate": 1.995578704225402e-07,
      "loss": 1.6965,
      "step": 251360
    },
    {
      "epoch": 0.0009123514601731616,
      "grad_norm": 9463.833367087567,
      "learning_rate": 1.9954515633904658e-07,
      "loss": 1.7185,
      "step": 251392
    },
    {
      "epoch": 0.0009124675945239982,
      "grad_norm": 9466.631713550496,
      "learning_rate": 1.9953244468533424e-07,
      "loss": 1.7216,
      "step": 251424
    },
    {
      "epoch": 0.000912583728874835,
      "grad_norm": 10105.2633810307,
      "learning_rate": 1.995197354606293e-07,
      "loss": 1.6984,
      "step": 251456
    },
    {
      "epoch": 0.0009126998632256717,
      "grad_norm": 9782.247798946824,
      "learning_rate": 1.995070286641583e-07,
      "loss": 1.6956,
      "step": 251488
    },
    {
      "epoch": 0.0009128159975765084,
      "grad_norm": 9740.124639859594,
      "learning_rate": 1.9949432429514816e-07,
      "loss": 1.7073,
      "step": 251520
    },
    {
      "epoch": 0.0009129321319273451,
      "grad_norm": 10304.774621504344,
      "learning_rate": 1.9948162235282603e-07,
      "loss": 1.6979,
      "step": 251552
    },
    {
      "epoch": 0.0009130482662781818,
      "grad_norm": 9387.71196831262,
      "learning_rate": 1.9946892283641947e-07,
      "loss": 1.7241,
      "step": 251584
    },
    {
      "epoch": 0.0009131644006290185,
      "grad_norm": 11308.990052166462,
      "learning_rate": 1.9945622574515638e-07,
      "loss": 1.7208,
      "step": 251616
    },
    {
      "epoch": 0.0009132805349798552,
      "grad_norm": 9000.716415930456,
      "learning_rate": 1.99443531078265e-07,
      "loss": 1.708,
      "step": 251648
    },
    {
      "epoch": 0.0009133966693306919,
      "grad_norm": 9306.275194727481,
      "learning_rate": 1.9943083883497393e-07,
      "loss": 1.7299,
      "step": 251680
    },
    {
      "epoch": 0.0009135128036815286,
      "grad_norm": 12015.803427153758,
      "learning_rate": 1.994181490145121e-07,
      "loss": 1.727,
      "step": 251712
    },
    {
      "epoch": 0.0009136289380323653,
      "grad_norm": 9364.684938640487,
      "learning_rate": 1.9940546161610877e-07,
      "loss": 1.7498,
      "step": 251744
    },
    {
      "epoch": 0.0009137450723832021,
      "grad_norm": 8230.33061304344,
      "learning_rate": 1.9939277663899355e-07,
      "loss": 1.7484,
      "step": 251776
    },
    {
      "epoch": 0.0009138612067340387,
      "grad_norm": 9302.091162743998,
      "learning_rate": 1.9938009408239643e-07,
      "loss": 1.7126,
      "step": 251808
    },
    {
      "epoch": 0.0009139773410848755,
      "grad_norm": 8757.91687560461,
      "learning_rate": 1.993674139455477e-07,
      "loss": 1.6942,
      "step": 251840
    },
    {
      "epoch": 0.0009140934754357121,
      "grad_norm": 10134.226758860294,
      "learning_rate": 1.9935473622767798e-07,
      "loss": 1.7025,
      "step": 251872
    },
    {
      "epoch": 0.0009142096097865489,
      "grad_norm": 8537.103724331806,
      "learning_rate": 1.9934206092801826e-07,
      "loss": 1.7022,
      "step": 251904
    },
    {
      "epoch": 0.0009143257441373855,
      "grad_norm": 10679.644750645968,
      "learning_rate": 1.993293880457999e-07,
      "loss": 1.7082,
      "step": 251936
    },
    {
      "epoch": 0.0009144418784882223,
      "grad_norm": 9207.485867488474,
      "learning_rate": 1.9931671758025454e-07,
      "loss": 1.7084,
      "step": 251968
    },
    {
      "epoch": 0.0009145580128390589,
      "grad_norm": 9591.84049075046,
      "learning_rate": 1.9930404953061422e-07,
      "loss": 1.7081,
      "step": 252000
    },
    {
      "epoch": 0.0009146741471898957,
      "grad_norm": 19525.450468555136,
      "learning_rate": 1.9929138389611127e-07,
      "loss": 1.712,
      "step": 252032
    },
    {
      "epoch": 0.0009147902815407324,
      "grad_norm": 7944.8627426784415,
      "learning_rate": 1.9927911636506955e-07,
      "loss": 1.6975,
      "step": 252064
    },
    {
      "epoch": 0.0009149064158915691,
      "grad_norm": 9338.857210601305,
      "learning_rate": 1.9926645548312633e-07,
      "loss": 1.701,
      "step": 252096
    },
    {
      "epoch": 0.0009150225502424058,
      "grad_norm": 9609.692919131183,
      "learning_rate": 1.992537970140435e-07,
      "loss": 1.7055,
      "step": 252128
    },
    {
      "epoch": 0.0009151386845932425,
      "grad_norm": 9161.765877820717,
      "learning_rate": 1.9924114095705486e-07,
      "loss": 1.6913,
      "step": 252160
    },
    {
      "epoch": 0.0009152548189440792,
      "grad_norm": 11717.290471777167,
      "learning_rate": 1.9922848731139441e-07,
      "loss": 1.6929,
      "step": 252192
    },
    {
      "epoch": 0.0009153709532949159,
      "grad_norm": 9961.409739590075,
      "learning_rate": 1.9921583607629656e-07,
      "loss": 1.7066,
      "step": 252224
    },
    {
      "epoch": 0.0009154870876457526,
      "grad_norm": 8842.305581690784,
      "learning_rate": 1.9920318725099604e-07,
      "loss": 1.6983,
      "step": 252256
    },
    {
      "epoch": 0.0009156032219965893,
      "grad_norm": 10379.732944541493,
      "learning_rate": 1.991905408347279e-07,
      "loss": 1.7253,
      "step": 252288
    },
    {
      "epoch": 0.000915719356347426,
      "grad_norm": 8786.699607930159,
      "learning_rate": 1.9917789682672756e-07,
      "loss": 1.7197,
      "step": 252320
    },
    {
      "epoch": 0.0009158354906982628,
      "grad_norm": 9885.146028258763,
      "learning_rate": 1.9916525522623077e-07,
      "loss": 1.7104,
      "step": 252352
    },
    {
      "epoch": 0.0009159516250490994,
      "grad_norm": 9104.518109158771,
      "learning_rate": 1.991526160324736e-07,
      "loss": 1.7289,
      "step": 252384
    },
    {
      "epoch": 0.0009160677593999362,
      "grad_norm": 8948.001788108895,
      "learning_rate": 1.9913997924469252e-07,
      "loss": 1.7173,
      "step": 252416
    },
    {
      "epoch": 0.0009161838937507728,
      "grad_norm": 10430.404210767672,
      "learning_rate": 1.9912734486212425e-07,
      "loss": 1.7159,
      "step": 252448
    },
    {
      "epoch": 0.0009163000281016096,
      "grad_norm": 9517.678813660397,
      "learning_rate": 1.9911471288400597e-07,
      "loss": 1.7328,
      "step": 252480
    },
    {
      "epoch": 0.0009164161624524462,
      "grad_norm": 9776.692180896358,
      "learning_rate": 1.9910208330957502e-07,
      "loss": 1.7163,
      "step": 252512
    },
    {
      "epoch": 0.000916532296803283,
      "grad_norm": 8824.018585655858,
      "learning_rate": 1.9908945613806929e-07,
      "loss": 1.7126,
      "step": 252544
    },
    {
      "epoch": 0.0009166484311541196,
      "grad_norm": 9151.577350380643,
      "learning_rate": 1.9907683136872683e-07,
      "loss": 1.7388,
      "step": 252576
    },
    {
      "epoch": 0.0009167645655049564,
      "grad_norm": 8962.588465393243,
      "learning_rate": 1.9906420900078613e-07,
      "loss": 1.7146,
      "step": 252608
    },
    {
      "epoch": 0.0009168806998557931,
      "grad_norm": 8621.703775936634,
      "learning_rate": 1.9905158903348598e-07,
      "loss": 1.7379,
      "step": 252640
    },
    {
      "epoch": 0.0009169968342066298,
      "grad_norm": 12088.832036222522,
      "learning_rate": 1.990389714660655e-07,
      "loss": 1.7028,
      "step": 252672
    },
    {
      "epoch": 0.0009171129685574665,
      "grad_norm": 9782.26129276866,
      "learning_rate": 1.9902635629776418e-07,
      "loss": 1.6858,
      "step": 252704
    },
    {
      "epoch": 0.0009172291029083032,
      "grad_norm": 8706.642751370933,
      "learning_rate": 1.9901374352782186e-07,
      "loss": 1.6993,
      "step": 252736
    },
    {
      "epoch": 0.0009173452372591399,
      "grad_norm": 10394.37290075741,
      "learning_rate": 1.9900113315547864e-07,
      "loss": 1.6858,
      "step": 252768
    },
    {
      "epoch": 0.0009174613716099766,
      "grad_norm": 9085.530144135784,
      "learning_rate": 1.9898852517997503e-07,
      "loss": 1.7097,
      "step": 252800
    },
    {
      "epoch": 0.0009175775059608133,
      "grad_norm": 8859.739612426541,
      "learning_rate": 1.9897591960055183e-07,
      "loss": 1.7109,
      "step": 252832
    },
    {
      "epoch": 0.00091769364031165,
      "grad_norm": 8608.971831757843,
      "learning_rate": 1.9896331641645022e-07,
      "loss": 1.7015,
      "step": 252864
    },
    {
      "epoch": 0.0009178097746624867,
      "grad_norm": 9463.478747268364,
      "learning_rate": 1.989507156269117e-07,
      "loss": 1.7141,
      "step": 252896
    },
    {
      "epoch": 0.0009179259090133235,
      "grad_norm": 9475.663776221696,
      "learning_rate": 1.989381172311781e-07,
      "loss": 1.7328,
      "step": 252928
    },
    {
      "epoch": 0.0009180420433641601,
      "grad_norm": 8847.328862430739,
      "learning_rate": 1.9892552122849153e-07,
      "loss": 1.7101,
      "step": 252960
    },
    {
      "epoch": 0.0009181581777149969,
      "grad_norm": 7990.988299328187,
      "learning_rate": 1.9891292761809458e-07,
      "loss": 1.7188,
      "step": 252992
    },
    {
      "epoch": 0.0009182743120658335,
      "grad_norm": 9365.363207051823,
      "learning_rate": 1.9890033639923002e-07,
      "loss": 1.7007,
      "step": 253024
    },
    {
      "epoch": 0.0009183904464166703,
      "grad_norm": 17355.352603735828,
      "learning_rate": 1.9888774757114106e-07,
      "loss": 1.6809,
      "step": 253056
    },
    {
      "epoch": 0.0009185065807675069,
      "grad_norm": 10894.107581624114,
      "learning_rate": 1.9887555442309138e-07,
      "loss": 1.7105,
      "step": 253088
    },
    {
      "epoch": 0.0009186227151183437,
      "grad_norm": 8836.770903446575,
      "learning_rate": 1.9886297029963144e-07,
      "loss": 1.6906,
      "step": 253120
    },
    {
      "epoch": 0.0009187388494691803,
      "grad_norm": 9925.526887777796,
      "learning_rate": 1.9885038856470227e-07,
      "loss": 1.7141,
      "step": 253152
    },
    {
      "epoch": 0.0009188549838200171,
      "grad_norm": 9532.165756007393,
      "learning_rate": 1.988378092175483e-07,
      "loss": 1.7219,
      "step": 253184
    },
    {
      "epoch": 0.0009189711181708537,
      "grad_norm": 9026.258914965823,
      "learning_rate": 1.9882523225741443e-07,
      "loss": 1.6959,
      "step": 253216
    },
    {
      "epoch": 0.0009190872525216905,
      "grad_norm": 9621.350840708388,
      "learning_rate": 1.9881265768354582e-07,
      "loss": 1.703,
      "step": 253248
    },
    {
      "epoch": 0.0009192033868725272,
      "grad_norm": 10357.88376069166,
      "learning_rate": 1.9880008549518798e-07,
      "loss": 1.713,
      "step": 253280
    },
    {
      "epoch": 0.0009193195212233639,
      "grad_norm": 9241.036305523316,
      "learning_rate": 1.987875156915867e-07,
      "loss": 1.711,
      "step": 253312
    },
    {
      "epoch": 0.0009194356555742006,
      "grad_norm": 8814.707936171226,
      "learning_rate": 1.987749482719882e-07,
      "loss": 1.7129,
      "step": 253344
    },
    {
      "epoch": 0.0009195517899250373,
      "grad_norm": 10150.605302148242,
      "learning_rate": 1.98762383235639e-07,
      "loss": 1.7119,
      "step": 253376
    },
    {
      "epoch": 0.000919667924275874,
      "grad_norm": 8904.09456373864,
      "learning_rate": 1.9874982058178592e-07,
      "loss": 1.7197,
      "step": 253408
    },
    {
      "epoch": 0.0009197840586267107,
      "grad_norm": 10510.782463736941,
      "learning_rate": 1.9873726030967612e-07,
      "loss": 1.7216,
      "step": 253440
    },
    {
      "epoch": 0.0009199001929775474,
      "grad_norm": 8447.029063522867,
      "learning_rate": 1.9872470241855715e-07,
      "loss": 1.7194,
      "step": 253472
    },
    {
      "epoch": 0.0009200163273283841,
      "grad_norm": 8862.414682240953,
      "learning_rate": 1.9871214690767681e-07,
      "loss": 1.7356,
      "step": 253504
    },
    {
      "epoch": 0.0009201324616792208,
      "grad_norm": 9637.392178385187,
      "learning_rate": 1.9869959377628328e-07,
      "loss": 1.7459,
      "step": 253536
    },
    {
      "epoch": 0.0009202485960300576,
      "grad_norm": 11380.283652000946,
      "learning_rate": 1.9868704302362508e-07,
      "loss": 1.7152,
      "step": 253568
    },
    {
      "epoch": 0.0009203647303808942,
      "grad_norm": 9624.621966602117,
      "learning_rate": 1.9867449464895105e-07,
      "loss": 1.7011,
      "step": 253600
    },
    {
      "epoch": 0.000920480864731731,
      "grad_norm": 11220.39197176284,
      "learning_rate": 1.9866194865151033e-07,
      "loss": 1.6977,
      "step": 253632
    },
    {
      "epoch": 0.0009205969990825676,
      "grad_norm": 12258.986254988624,
      "learning_rate": 1.9864940503055243e-07,
      "loss": 1.7003,
      "step": 253664
    },
    {
      "epoch": 0.0009207131334334044,
      "grad_norm": 8555.280240880482,
      "learning_rate": 1.986368637853272e-07,
      "loss": 1.7021,
      "step": 253696
    },
    {
      "epoch": 0.000920829267784241,
      "grad_norm": 8134.635087082887,
      "learning_rate": 1.986243249150848e-07,
      "loss": 1.7071,
      "step": 253728
    },
    {
      "epoch": 0.0009209454021350778,
      "grad_norm": 9910.271439269462,
      "learning_rate": 1.9861178841907572e-07,
      "loss": 1.7132,
      "step": 253760
    },
    {
      "epoch": 0.0009210615364859144,
      "grad_norm": 8963.538810090577,
      "learning_rate": 1.985992542965508e-07,
      "loss": 1.7177,
      "step": 253792
    },
    {
      "epoch": 0.0009211776708367512,
      "grad_norm": 11738.883422199915,
      "learning_rate": 1.9858672254676114e-07,
      "loss": 1.7009,
      "step": 253824
    },
    {
      "epoch": 0.0009212938051875879,
      "grad_norm": 8900.386957879977,
      "learning_rate": 1.9857419316895832e-07,
      "loss": 1.711,
      "step": 253856
    },
    {
      "epoch": 0.0009214099395384246,
      "grad_norm": 8490.48078732883,
      "learning_rate": 1.985616661623941e-07,
      "loss": 1.7152,
      "step": 253888
    },
    {
      "epoch": 0.0009215260738892613,
      "grad_norm": 9046.894715867982,
      "learning_rate": 1.9854914152632062e-07,
      "loss": 1.7024,
      "step": 253920
    },
    {
      "epoch": 0.000921642208240098,
      "grad_norm": 10472.273487643455,
      "learning_rate": 1.985366192599904e-07,
      "loss": 1.7064,
      "step": 253952
    },
    {
      "epoch": 0.0009217583425909347,
      "grad_norm": 8438.458864034357,
      "learning_rate": 1.9852409936265623e-07,
      "loss": 1.7151,
      "step": 253984
    },
    {
      "epoch": 0.0009218744769417714,
      "grad_norm": 9062.55615155018,
      "learning_rate": 1.9851158183357125e-07,
      "loss": 1.7093,
      "step": 254016
    },
    {
      "epoch": 0.0009219906112926081,
      "grad_norm": 8790.531041979204,
      "learning_rate": 1.9849906667198894e-07,
      "loss": 1.7338,
      "step": 254048
    },
    {
      "epoch": 0.0009221067456434448,
      "grad_norm": 11353.90787350329,
      "learning_rate": 1.9848694486618386e-07,
      "loss": 1.7254,
      "step": 254080
    },
    {
      "epoch": 0.0009222228799942815,
      "grad_norm": 9614.382351456592,
      "learning_rate": 1.9847443436344203e-07,
      "loss": 1.7194,
      "step": 254112
    },
    {
      "epoch": 0.0009223390143451183,
      "grad_norm": 9172.866073370962,
      "learning_rate": 1.984619262259885e-07,
      "loss": 1.7371,
      "step": 254144
    },
    {
      "epoch": 0.0009224551486959549,
      "grad_norm": 10931.870837144024,
      "learning_rate": 1.9844942045307813e-07,
      "loss": 1.7201,
      "step": 254176
    },
    {
      "epoch": 0.0009225712830467917,
      "grad_norm": 9069.0521004127,
      "learning_rate": 1.9843691704396597e-07,
      "loss": 1.7264,
      "step": 254208
    },
    {
      "epoch": 0.0009226874173976283,
      "grad_norm": 10145.465982398246,
      "learning_rate": 1.9842441599790744e-07,
      "loss": 1.7296,
      "step": 254240
    },
    {
      "epoch": 0.0009228035517484651,
      "grad_norm": 8096.593234194243,
      "learning_rate": 1.984119173141583e-07,
      "loss": 1.7167,
      "step": 254272
    },
    {
      "epoch": 0.0009229196860993017,
      "grad_norm": 8908.268518629195,
      "learning_rate": 1.9839942099197467e-07,
      "loss": 1.7308,
      "step": 254304
    },
    {
      "epoch": 0.0009230358204501385,
      "grad_norm": 8559.407222465818,
      "learning_rate": 1.9838692703061295e-07,
      "loss": 1.746,
      "step": 254336
    },
    {
      "epoch": 0.0009231519548009751,
      "grad_norm": 10680.757276523045,
      "learning_rate": 1.9837443542932986e-07,
      "loss": 1.7325,
      "step": 254368
    },
    {
      "epoch": 0.0009232680891518119,
      "grad_norm": 11074.984243781117,
      "learning_rate": 1.983619461873825e-07,
      "loss": 1.7448,
      "step": 254400
    },
    {
      "epoch": 0.0009233842235026486,
      "grad_norm": 10729.320947758064,
      "learning_rate": 1.983494593040283e-07,
      "loss": 1.7178,
      "step": 254432
    },
    {
      "epoch": 0.0009235003578534853,
      "grad_norm": 10373.420458074568,
      "learning_rate": 1.9833697477852495e-07,
      "loss": 1.68,
      "step": 254464
    },
    {
      "epoch": 0.000923616492204322,
      "grad_norm": 8685.623408829098,
      "learning_rate": 1.9832449261013047e-07,
      "loss": 1.6973,
      "step": 254496
    },
    {
      "epoch": 0.0009237326265551587,
      "grad_norm": 9985.136353600787,
      "learning_rate": 1.983120127981033e-07,
      "loss": 1.6865,
      "step": 254528
    },
    {
      "epoch": 0.0009238487609059954,
      "grad_norm": 9542.493175266094,
      "learning_rate": 1.982995353417021e-07,
      "loss": 1.7032,
      "step": 254560
    },
    {
      "epoch": 0.0009239648952568321,
      "grad_norm": 9147.961193621231,
      "learning_rate": 1.9828706024018595e-07,
      "loss": 1.6999,
      "step": 254592
    },
    {
      "epoch": 0.0009240810296076688,
      "grad_norm": 8948.280058201128,
      "learning_rate": 1.9827458749281415e-07,
      "loss": 1.6922,
      "step": 254624
    },
    {
      "epoch": 0.0009241971639585055,
      "grad_norm": 12713.550723539038,
      "learning_rate": 1.9826211709884645e-07,
      "loss": 1.7129,
      "step": 254656
    },
    {
      "epoch": 0.0009243132983093422,
      "grad_norm": 9706.563346519715,
      "learning_rate": 1.9824964905754283e-07,
      "loss": 1.7204,
      "step": 254688
    },
    {
      "epoch": 0.000924429432660179,
      "grad_norm": 10966.897829377276,
      "learning_rate": 1.982371833681636e-07,
      "loss": 1.7175,
      "step": 254720
    },
    {
      "epoch": 0.0009245455670110156,
      "grad_norm": 8963.188272038025,
      "learning_rate": 1.9822472002996946e-07,
      "loss": 1.7227,
      "step": 254752
    },
    {
      "epoch": 0.0009246617013618524,
      "grad_norm": 11979.803170336314,
      "learning_rate": 1.982122590422214e-07,
      "loss": 1.6996,
      "step": 254784
    },
    {
      "epoch": 0.000924777835712689,
      "grad_norm": 8539.309573964396,
      "learning_rate": 1.9819980040418073e-07,
      "loss": 1.6812,
      "step": 254816
    },
    {
      "epoch": 0.0009248939700635258,
      "grad_norm": 9787.980588456436,
      "learning_rate": 1.9818734411510905e-07,
      "loss": 1.7082,
      "step": 254848
    },
    {
      "epoch": 0.0009250101044143624,
      "grad_norm": 9211.827614539907,
      "learning_rate": 1.9817489017426836e-07,
      "loss": 1.6902,
      "step": 254880
    },
    {
      "epoch": 0.0009251262387651992,
      "grad_norm": 10319.91472832988,
      "learning_rate": 1.9816243858092095e-07,
      "loss": 1.7256,
      "step": 254912
    },
    {
      "epoch": 0.0009252423731160358,
      "grad_norm": 8256.693648186301,
      "learning_rate": 1.9814998933432943e-07,
      "loss": 1.7206,
      "step": 254944
    },
    {
      "epoch": 0.0009253585074668726,
      "grad_norm": 8906.155736343262,
      "learning_rate": 1.981375424337567e-07,
      "loss": 1.6861,
      "step": 254976
    },
    {
      "epoch": 0.0009254746418177094,
      "grad_norm": 8653.864570236814,
      "learning_rate": 1.981250978784661e-07,
      "loss": 1.6937,
      "step": 255008
    },
    {
      "epoch": 0.000925590776168546,
      "grad_norm": 9920.546154320336,
      "learning_rate": 1.9811265566772114e-07,
      "loss": 1.693,
      "step": 255040
    },
    {
      "epoch": 0.0009257069105193828,
      "grad_norm": 19766.980143663826,
      "learning_rate": 1.9810021580078574e-07,
      "loss": 1.7045,
      "step": 255072
    },
    {
      "epoch": 0.0009258230448702194,
      "grad_norm": 11372.417509043536,
      "learning_rate": 1.9808816691408568e-07,
      "loss": 1.7144,
      "step": 255104
    },
    {
      "epoch": 0.0009259391792210562,
      "grad_norm": 10522.133814013201,
      "learning_rate": 1.9807573165937548e-07,
      "loss": 1.7012,
      "step": 255136
    },
    {
      "epoch": 0.0009260553135718928,
      "grad_norm": 10596.617007328328,
      "learning_rate": 1.9806329874629152e-07,
      "loss": 1.7147,
      "step": 255168
    },
    {
      "epoch": 0.0009261714479227296,
      "grad_norm": 10456.306422441912,
      "learning_rate": 1.9805086817409894e-07,
      "loss": 1.7271,
      "step": 255200
    },
    {
      "epoch": 0.0009262875822735662,
      "grad_norm": 10371.362880547571,
      "learning_rate": 1.9803843994206324e-07,
      "loss": 1.7168,
      "step": 255232
    },
    {
      "epoch": 0.000926403716624403,
      "grad_norm": 9828.074480792257,
      "learning_rate": 1.980260140494503e-07,
      "loss": 1.7425,
      "step": 255264
    },
    {
      "epoch": 0.0009265198509752397,
      "grad_norm": 10197.938026875825,
      "learning_rate": 1.9801359049552628e-07,
      "loss": 1.7388,
      "step": 255296
    },
    {
      "epoch": 0.0009266359853260764,
      "grad_norm": 8779.978132091219,
      "learning_rate": 1.9800116927955762e-07,
      "loss": 1.7015,
      "step": 255328
    },
    {
      "epoch": 0.0009267521196769131,
      "grad_norm": 12001.122780806803,
      "learning_rate": 1.9798875040081113e-07,
      "loss": 1.7044,
      "step": 255360
    },
    {
      "epoch": 0.0009268682540277498,
      "grad_norm": 8567.017333938342,
      "learning_rate": 1.97976333858554e-07,
      "loss": 1.6937,
      "step": 255392
    },
    {
      "epoch": 0.0009269843883785865,
      "grad_norm": 8580.145686408827,
      "learning_rate": 1.9796391965205357e-07,
      "loss": 1.7026,
      "step": 255424
    },
    {
      "epoch": 0.0009271005227294232,
      "grad_norm": 8364.448218501924,
      "learning_rate": 1.979515077805777e-07,
      "loss": 1.6986,
      "step": 255456
    },
    {
      "epoch": 0.0009272166570802599,
      "grad_norm": 9534.136772671136,
      "learning_rate": 1.979390982433944e-07,
      "loss": 1.6965,
      "step": 255488
    },
    {
      "epoch": 0.0009273327914310966,
      "grad_norm": 10327.199620419855,
      "learning_rate": 1.9792669103977215e-07,
      "loss": 1.7131,
      "step": 255520
    },
    {
      "epoch": 0.0009274489257819333,
      "grad_norm": 10938.417618650332,
      "learning_rate": 1.9791428616897967e-07,
      "loss": 1.7147,
      "step": 255552
    },
    {
      "epoch": 0.0009275650601327701,
      "grad_norm": 10025.436848337333,
      "learning_rate": 1.9790188363028597e-07,
      "loss": 1.6858,
      "step": 255584
    },
    {
      "epoch": 0.0009276811944836067,
      "grad_norm": 8646.09565063908,
      "learning_rate": 1.9788948342296046e-07,
      "loss": 1.6959,
      "step": 255616
    },
    {
      "epoch": 0.0009277973288344435,
      "grad_norm": 8867.479799807836,
      "learning_rate": 1.978770855462728e-07,
      "loss": 1.7039,
      "step": 255648
    },
    {
      "epoch": 0.0009279134631852801,
      "grad_norm": 8775.819505892314,
      "learning_rate": 1.9786468999949304e-07,
      "loss": 1.6853,
      "step": 255680
    },
    {
      "epoch": 0.0009280295975361169,
      "grad_norm": 9346.723062121826,
      "learning_rate": 1.978522967818915e-07,
      "loss": 1.6961,
      "step": 255712
    },
    {
      "epoch": 0.0009281457318869535,
      "grad_norm": 9649.266811525113,
      "learning_rate": 1.9783990589273882e-07,
      "loss": 1.6938,
      "step": 255744
    },
    {
      "epoch": 0.0009282618662377903,
      "grad_norm": 10159.983267702757,
      "learning_rate": 1.97827517331306e-07,
      "loss": 1.7008,
      "step": 255776
    },
    {
      "epoch": 0.0009283780005886269,
      "grad_norm": 8880.656957680552,
      "learning_rate": 1.978151310968643e-07,
      "loss": 1.7205,
      "step": 255808
    },
    {
      "epoch": 0.0009284941349394637,
      "grad_norm": 11208.188970569689,
      "learning_rate": 1.9780274718868535e-07,
      "loss": 1.703,
      "step": 255840
    },
    {
      "epoch": 0.0009286102692903004,
      "grad_norm": 9273.598222912184,
      "learning_rate": 1.977903656060411e-07,
      "loss": 1.7059,
      "step": 255872
    },
    {
      "epoch": 0.0009287264036411371,
      "grad_norm": 9475.193507258837,
      "learning_rate": 1.9777798634820372e-07,
      "loss": 1.7273,
      "step": 255904
    },
    {
      "epoch": 0.0009288425379919738,
      "grad_norm": 9699.518750948419,
      "learning_rate": 1.977656094144459e-07,
      "loss": 1.7233,
      "step": 255936
    },
    {
      "epoch": 0.0009289586723428105,
      "grad_norm": 10725.397335297186,
      "learning_rate": 1.9775323480404043e-07,
      "loss": 1.7153,
      "step": 255968
    },
    {
      "epoch": 0.0009290748066936472,
      "grad_norm": 13601.662986561605,
      "learning_rate": 1.9774086251626057e-07,
      "loss": 1.7151,
      "step": 256000
    },
    {
      "epoch": 0.0009291909410444839,
      "grad_norm": 11469.615861047832,
      "learning_rate": 1.9772849255037982e-07,
      "loss": 1.7097,
      "step": 256032
    },
    {
      "epoch": 0.0009293070753953206,
      "grad_norm": 9401.922994792076,
      "learning_rate": 1.97716124905672e-07,
      "loss": 1.7161,
      "step": 256064
    },
    {
      "epoch": 0.0009294232097461573,
      "grad_norm": 19504.063166427655,
      "learning_rate": 1.977037595814113e-07,
      "loss": 1.7218,
      "step": 256096
    },
    {
      "epoch": 0.000929539344096994,
      "grad_norm": 9966.842830104226,
      "learning_rate": 1.976917828856583e-07,
      "loss": 1.7242,
      "step": 256128
    },
    {
      "epoch": 0.0009296554784478308,
      "grad_norm": 8399.130669301438,
      "learning_rate": 1.9767942212765795e-07,
      "loss": 1.7238,
      "step": 256160
    },
    {
      "epoch": 0.0009297716127986674,
      "grad_norm": 9138.757245927916,
      "learning_rate": 1.976670636879517e-07,
      "loss": 1.6891,
      "step": 256192
    },
    {
      "epoch": 0.0009298877471495042,
      "grad_norm": 8245.187323523947,
      "learning_rate": 1.9765470756581507e-07,
      "loss": 1.6911,
      "step": 256224
    },
    {
      "epoch": 0.0009300038815003408,
      "grad_norm": 10083.418567132876,
      "learning_rate": 1.9764235376052373e-07,
      "loss": 1.7067,
      "step": 256256
    },
    {
      "epoch": 0.0009301200158511776,
      "grad_norm": 9610.05598318761,
      "learning_rate": 1.9763000227135374e-07,
      "loss": 1.6899,
      "step": 256288
    },
    {
      "epoch": 0.0009302361502020142,
      "grad_norm": 9414.800794493742,
      "learning_rate": 1.9761765309758148e-07,
      "loss": 1.7069,
      "step": 256320
    },
    {
      "epoch": 0.000930352284552851,
      "grad_norm": 8445.022676109284,
      "learning_rate": 1.9760530623848363e-07,
      "loss": 1.7079,
      "step": 256352
    },
    {
      "epoch": 0.0009304684189036876,
      "grad_norm": 9145.78219727542,
      "learning_rate": 1.9759296169333724e-07,
      "loss": 1.705,
      "step": 256384
    },
    {
      "epoch": 0.0009305845532545244,
      "grad_norm": 8255.079406038443,
      "learning_rate": 1.9758061946141955e-07,
      "loss": 1.7234,
      "step": 256416
    },
    {
      "epoch": 0.0009307006876053611,
      "grad_norm": 9881.657249672244,
      "learning_rate": 1.9756827954200822e-07,
      "loss": 1.7152,
      "step": 256448
    },
    {
      "epoch": 0.0009308168219561978,
      "grad_norm": 10192.853967363606,
      "learning_rate": 1.9755594193438126e-07,
      "loss": 1.7223,
      "step": 256480
    },
    {
      "epoch": 0.0009309329563070345,
      "grad_norm": 8526.073891305423,
      "learning_rate": 1.9754360663781688e-07,
      "loss": 1.7321,
      "step": 256512
    },
    {
      "epoch": 0.0009310490906578712,
      "grad_norm": 11278.58342168909,
      "learning_rate": 1.9753127365159366e-07,
      "loss": 1.6991,
      "step": 256544
    },
    {
      "epoch": 0.0009311652250087079,
      "grad_norm": 9713.049160793948,
      "learning_rate": 1.9751894297499052e-07,
      "loss": 1.6869,
      "step": 256576
    },
    {
      "epoch": 0.0009312813593595446,
      "grad_norm": 8971.806507053081,
      "learning_rate": 1.9750661460728668e-07,
      "loss": 1.7048,
      "step": 256608
    },
    {
      "epoch": 0.0009313974937103813,
      "grad_norm": 8801.46237849143,
      "learning_rate": 1.9749428854776162e-07,
      "loss": 1.6864,
      "step": 256640
    },
    {
      "epoch": 0.000931513628061218,
      "grad_norm": 11016.537931673452,
      "learning_rate": 1.974819647956952e-07,
      "loss": 1.721,
      "step": 256672
    },
    {
      "epoch": 0.0009316297624120547,
      "grad_norm": 9143.535858736488,
      "learning_rate": 1.9746964335036764e-07,
      "loss": 1.7159,
      "step": 256704
    },
    {
      "epoch": 0.0009317458967628915,
      "grad_norm": 10346.539518119089,
      "learning_rate": 1.9745732421105933e-07,
      "loss": 1.6838,
      "step": 256736
    },
    {
      "epoch": 0.0009318620311137281,
      "grad_norm": 8084.642107106535,
      "learning_rate": 1.9744500737705108e-07,
      "loss": 1.6991,
      "step": 256768
    },
    {
      "epoch": 0.0009319781654645649,
      "grad_norm": 8656.712771023422,
      "learning_rate": 1.97432692847624e-07,
      "loss": 1.6946,
      "step": 256800
    },
    {
      "epoch": 0.0009320942998154015,
      "grad_norm": 9513.047040775105,
      "learning_rate": 1.974203806220595e-07,
      "loss": 1.7084,
      "step": 256832
    },
    {
      "epoch": 0.0009322104341662383,
      "grad_norm": 9846.576054649657,
      "learning_rate": 1.9740807069963932e-07,
      "loss": 1.7221,
      "step": 256864
    },
    {
      "epoch": 0.0009323265685170749,
      "grad_norm": 8720.871172079083,
      "learning_rate": 1.9739576307964544e-07,
      "loss": 1.7055,
      "step": 256896
    },
    {
      "epoch": 0.0009324427028679117,
      "grad_norm": 9412.955221395669,
      "learning_rate": 1.9738345776136028e-07,
      "loss": 1.7185,
      "step": 256928
    },
    {
      "epoch": 0.0009325588372187483,
      "grad_norm": 9796.933397752584,
      "learning_rate": 1.973711547440665e-07,
      "loss": 1.7253,
      "step": 256960
    },
    {
      "epoch": 0.0009326749715695851,
      "grad_norm": 11124.114526558957,
      "learning_rate": 1.9735885402704702e-07,
      "loss": 1.7178,
      "step": 256992
    },
    {
      "epoch": 0.0009327911059204218,
      "grad_norm": 10188.981107058742,
      "learning_rate": 1.9734655560958518e-07,
      "loss": 1.7379,
      "step": 257024
    },
    {
      "epoch": 0.0009329072402712585,
      "grad_norm": 11543.61347239243,
      "learning_rate": 1.9733425949096462e-07,
      "loss": 1.7313,
      "step": 257056
    },
    {
      "epoch": 0.0009330233746220952,
      "grad_norm": 11827.941832795763,
      "learning_rate": 1.973219656704692e-07,
      "loss": 1.7036,
      "step": 257088
    },
    {
      "epoch": 0.0009331395089729319,
      "grad_norm": 8922.118470408248,
      "learning_rate": 1.9731005822271144e-07,
      "loss": 1.709,
      "step": 257120
    },
    {
      "epoch": 0.0009332556433237686,
      "grad_norm": 10554.971909010465,
      "learning_rate": 1.9729776892455846e-07,
      "loss": 1.6907,
      "step": 257152
    },
    {
      "epoch": 0.0009333717776746053,
      "grad_norm": 10579.238535924975,
      "learning_rate": 1.972854819224066e-07,
      "loss": 1.7123,
      "step": 257184
    },
    {
      "epoch": 0.000933487912025442,
      "grad_norm": 9180.96835851208,
      "learning_rate": 1.97273197215541e-07,
      "loss": 1.7058,
      "step": 257216
    },
    {
      "epoch": 0.0009336040463762787,
      "grad_norm": 9703.782973665477,
      "learning_rate": 1.9726091480324716e-07,
      "loss": 1.7036,
      "step": 257248
    },
    {
      "epoch": 0.0009337201807271154,
      "grad_norm": 7933.566411141966,
      "learning_rate": 1.9724863468481086e-07,
      "loss": 1.7214,
      "step": 257280
    },
    {
      "epoch": 0.0009338363150779522,
      "grad_norm": 8699.76574397265,
      "learning_rate": 1.9723635685951813e-07,
      "loss": 1.7232,
      "step": 257312
    },
    {
      "epoch": 0.0009339524494287888,
      "grad_norm": 8655.947319617882,
      "learning_rate": 1.9722408132665545e-07,
      "loss": 1.6954,
      "step": 257344
    },
    {
      "epoch": 0.0009340685837796256,
      "grad_norm": 10636.952759131724,
      "learning_rate": 1.9721180808550948e-07,
      "loss": 1.7109,
      "step": 257376
    },
    {
      "epoch": 0.0009341847181304622,
      "grad_norm": 10322.025963927817,
      "learning_rate": 1.9719953713536725e-07,
      "loss": 1.7182,
      "step": 257408
    },
    {
      "epoch": 0.000934300852481299,
      "grad_norm": 9246.480195187789,
      "learning_rate": 1.9718726847551611e-07,
      "loss": 1.7028,
      "step": 257440
    },
    {
      "epoch": 0.0009344169868321356,
      "grad_norm": 10634.041376635694,
      "learning_rate": 1.9717500210524368e-07,
      "loss": 1.697,
      "step": 257472
    },
    {
      "epoch": 0.0009345331211829724,
      "grad_norm": 8778.675298699685,
      "learning_rate": 1.9716273802383792e-07,
      "loss": 1.6927,
      "step": 257504
    },
    {
      "epoch": 0.000934649255533809,
      "grad_norm": 8297.65268012587,
      "learning_rate": 1.971504762305871e-07,
      "loss": 1.7065,
      "step": 257536
    },
    {
      "epoch": 0.0009347653898846458,
      "grad_norm": 9179.054635418617,
      "learning_rate": 1.971382167247798e-07,
      "loss": 1.7189,
      "step": 257568
    },
    {
      "epoch": 0.0009348815242354825,
      "grad_norm": 9661.529692548691,
      "learning_rate": 1.9712595950570487e-07,
      "loss": 1.6983,
      "step": 257600
    },
    {
      "epoch": 0.0009349976585863192,
      "grad_norm": 10461.03608635397,
      "learning_rate": 1.971137045726515e-07,
      "loss": 1.7023,
      "step": 257632
    },
    {
      "epoch": 0.0009351137929371559,
      "grad_norm": 9885.540349419449,
      "learning_rate": 1.9710145192490927e-07,
      "loss": 1.7197,
      "step": 257664
    },
    {
      "epoch": 0.0009352299272879926,
      "grad_norm": 9172.597669144767,
      "learning_rate": 1.970892015617679e-07,
      "loss": 1.7141,
      "step": 257696
    },
    {
      "epoch": 0.0009353460616388293,
      "grad_norm": 9028.651948103881,
      "learning_rate": 1.9707695348251756e-07,
      "loss": 1.7127,
      "step": 257728
    },
    {
      "epoch": 0.000935462195989666,
      "grad_norm": 8915.872812013415,
      "learning_rate": 1.9706470768644865e-07,
      "loss": 1.7169,
      "step": 257760
    },
    {
      "epoch": 0.0009355783303405027,
      "grad_norm": 11868.573124011158,
      "learning_rate": 1.9705246417285192e-07,
      "loss": 1.7125,
      "step": 257792
    },
    {
      "epoch": 0.0009356944646913394,
      "grad_norm": 9085.259710101853,
      "learning_rate": 1.970402229410184e-07,
      "loss": 1.724,
      "step": 257824
    },
    {
      "epoch": 0.0009358105990421761,
      "grad_norm": 10828.967633158758,
      "learning_rate": 1.9702798399023946e-07,
      "loss": 1.718,
      "step": 257856
    },
    {
      "epoch": 0.0009359267333930129,
      "grad_norm": 10306.638249206188,
      "learning_rate": 1.9701574731980677e-07,
      "loss": 1.733,
      "step": 257888
    },
    {
      "epoch": 0.0009360428677438495,
      "grad_norm": 8372.878955293692,
      "learning_rate": 1.9700351292901228e-07,
      "loss": 1.7186,
      "step": 257920
    },
    {
      "epoch": 0.0009361590020946863,
      "grad_norm": 9410.773081952406,
      "learning_rate": 1.9699128081714825e-07,
      "loss": 1.6849,
      "step": 257952
    },
    {
      "epoch": 0.0009362751364455229,
      "grad_norm": 9833.579409350392,
      "learning_rate": 1.969790509835073e-07,
      "loss": 1.6857,
      "step": 257984
    },
    {
      "epoch": 0.0009363912707963597,
      "grad_norm": 9876.187118518968,
      "learning_rate": 1.9696682342738232e-07,
      "loss": 1.707,
      "step": 258016
    },
    {
      "epoch": 0.0009365074051471963,
      "grad_norm": 10516.921792996276,
      "learning_rate": 1.9695459814806647e-07,
      "loss": 1.7008,
      "step": 258048
    },
    {
      "epoch": 0.0009366235394980331,
      "grad_norm": 8127.109080109605,
      "learning_rate": 1.9694237514485333e-07,
      "loss": 1.7012,
      "step": 258080
    },
    {
      "epoch": 0.0009367396738488697,
      "grad_norm": 9027.627152247705,
      "learning_rate": 1.9693015441703663e-07,
      "loss": 1.708,
      "step": 258112
    },
    {
      "epoch": 0.0009368558081997065,
      "grad_norm": 7899.424915777097,
      "learning_rate": 1.9691831775614637e-07,
      "loss": 1.708,
      "step": 258144
    },
    {
      "epoch": 0.0009369719425505432,
      "grad_norm": 9162.097139847405,
      "learning_rate": 1.9690610150595393e-07,
      "loss": 1.7326,
      "step": 258176
    },
    {
      "epoch": 0.0009370880769013799,
      "grad_norm": 10411.72915514037,
      "learning_rate": 1.968938875290633e-07,
      "loss": 1.7146,
      "step": 258208
    },
    {
      "epoch": 0.0009372042112522166,
      "grad_norm": 9215.712886152649,
      "learning_rate": 1.9688167582476947e-07,
      "loss": 1.7376,
      "step": 258240
    },
    {
      "epoch": 0.0009373203456030533,
      "grad_norm": 9059.617872736133,
      "learning_rate": 1.968694663923678e-07,
      "loss": 1.723,
      "step": 258272
    },
    {
      "epoch": 0.00093743647995389,
      "grad_norm": 10106.743491352692,
      "learning_rate": 1.9685725923115393e-07,
      "loss": 1.6935,
      "step": 258304
    },
    {
      "epoch": 0.0009375526143047267,
      "grad_norm": 10817.380829017715,
      "learning_rate": 1.9684505434042378e-07,
      "loss": 1.6951,
      "step": 258336
    },
    {
      "epoch": 0.0009376687486555634,
      "grad_norm": 10822.714446939824,
      "learning_rate": 1.9683285171947367e-07,
      "loss": 1.6999,
      "step": 258368
    },
    {
      "epoch": 0.0009377848830064001,
      "grad_norm": 9723.887082849122,
      "learning_rate": 1.968206513676001e-07,
      "loss": 1.6896,
      "step": 258400
    },
    {
      "epoch": 0.0009379010173572368,
      "grad_norm": 9473.244006146997,
      "learning_rate": 1.968084532840999e-07,
      "loss": 1.7156,
      "step": 258432
    },
    {
      "epoch": 0.0009380171517080736,
      "grad_norm": 9560.562117365276,
      "learning_rate": 1.967962574682703e-07,
      "loss": 1.7091,
      "step": 258464
    },
    {
      "epoch": 0.0009381332860589102,
      "grad_norm": 10057.087650010812,
      "learning_rate": 1.9678406391940878e-07,
      "loss": 1.6911,
      "step": 258496
    },
    {
      "epoch": 0.000938249420409747,
      "grad_norm": 10343.379718447932,
      "learning_rate": 1.9677187263681308e-07,
      "loss": 1.7069,
      "step": 258528
    },
    {
      "epoch": 0.0009383655547605836,
      "grad_norm": 9557.729228221524,
      "learning_rate": 1.9675968361978127e-07,
      "loss": 1.6883,
      "step": 258560
    },
    {
      "epoch": 0.0009384816891114204,
      "grad_norm": 10301.313896780352,
      "learning_rate": 1.9674749686761178e-07,
      "loss": 1.7161,
      "step": 258592
    },
    {
      "epoch": 0.000938597823462257,
      "grad_norm": 8791.714963532428,
      "learning_rate": 1.9673531237960328e-07,
      "loss": 1.7139,
      "step": 258624
    },
    {
      "epoch": 0.0009387139578130938,
      "grad_norm": 10535.061461614736,
      "learning_rate": 1.9672313015505476e-07,
      "loss": 1.7028,
      "step": 258656
    },
    {
      "epoch": 0.0009388300921639304,
      "grad_norm": 10167.283019568207,
      "learning_rate": 1.9671095019326555e-07,
      "loss": 1.7217,
      "step": 258688
    },
    {
      "epoch": 0.0009389462265147672,
      "grad_norm": 11167.437127649298,
      "learning_rate": 1.9669877249353518e-07,
      "loss": 1.7251,
      "step": 258720
    },
    {
      "epoch": 0.000939062360865604,
      "grad_norm": 9540.6354086088,
      "learning_rate": 1.9668659705516364e-07,
      "loss": 1.7268,
      "step": 258752
    },
    {
      "epoch": 0.0009391784952164406,
      "grad_norm": 9601.077231227755,
      "learning_rate": 1.9667442387745107e-07,
      "loss": 1.746,
      "step": 258784
    },
    {
      "epoch": 0.0009392946295672773,
      "grad_norm": 8880.548181278,
      "learning_rate": 1.96662252959698e-07,
      "loss": 1.7345,
      "step": 258816
    },
    {
      "epoch": 0.000939410763918114,
      "grad_norm": 9001.680509771495,
      "learning_rate": 1.9665008430120528e-07,
      "loss": 1.7231,
      "step": 258848
    },
    {
      "epoch": 0.0009395268982689507,
      "grad_norm": 10213.220256119026,
      "learning_rate": 1.96637917901274e-07,
      "loss": 1.7306,
      "step": 258880
    },
    {
      "epoch": 0.0009396430326197874,
      "grad_norm": 8241.100654645592,
      "learning_rate": 1.9662575375920556e-07,
      "loss": 1.6989,
      "step": 258912
    },
    {
      "epoch": 0.0009397591669706241,
      "grad_norm": 10181.11644172681,
      "learning_rate": 1.966135918743017e-07,
      "loss": 1.7229,
      "step": 258944
    },
    {
      "epoch": 0.0009398753013214608,
      "grad_norm": 9836.543498607629,
      "learning_rate": 1.966014322458645e-07,
      "loss": 1.7196,
      "step": 258976
    },
    {
      "epoch": 0.0009399914356722975,
      "grad_norm": 8566.522048065948,
      "learning_rate": 1.9658927487319618e-07,
      "loss": 1.7074,
      "step": 259008
    },
    {
      "epoch": 0.0009401075700231343,
      "grad_norm": 9367.558913612447,
      "learning_rate": 1.9657711975559946e-07,
      "loss": 1.7337,
      "step": 259040
    },
    {
      "epoch": 0.000940223704373971,
      "grad_norm": 8280.75322660928,
      "learning_rate": 1.965649668923772e-07,
      "loss": 1.7277,
      "step": 259072
    },
    {
      "epoch": 0.0009403398387248077,
      "grad_norm": 10008.784141942517,
      "learning_rate": 1.9655281628283269e-07,
      "loss": 1.704,
      "step": 259104
    },
    {
      "epoch": 0.0009404559730756443,
      "grad_norm": 9502.377807685822,
      "learning_rate": 1.9654104752831623e-07,
      "loss": 1.7133,
      "step": 259136
    },
    {
      "epoch": 0.0009405721074264811,
      "grad_norm": 9066.630906792225,
      "learning_rate": 1.965289013536647e-07,
      "loss": 1.7143,
      "step": 259168
    },
    {
      "epoch": 0.0009406882417773177,
      "grad_norm": 9220.592063419788,
      "learning_rate": 1.9651675743062414e-07,
      "loss": 1.6952,
      "step": 259200
    },
    {
      "epoch": 0.0009408043761281545,
      "grad_norm": 8545.640174966415,
      "learning_rate": 1.9650461575849898e-07,
      "loss": 1.7153,
      "step": 259232
    },
    {
      "epoch": 0.0009409205104789911,
      "grad_norm": 10805.146921722073,
      "learning_rate": 1.9649247633659397e-07,
      "loss": 1.6985,
      "step": 259264
    },
    {
      "epoch": 0.0009410366448298279,
      "grad_norm": 10316.705675747467,
      "learning_rate": 1.9648033916421407e-07,
      "loss": 1.7208,
      "step": 259296
    },
    {
      "epoch": 0.0009411527791806647,
      "grad_norm": 9343.6923108587,
      "learning_rate": 1.9646820424066473e-07,
      "loss": 1.7301,
      "step": 259328
    },
    {
      "epoch": 0.0009412689135315013,
      "grad_norm": 8457.916410085878,
      "learning_rate": 1.9645607156525144e-07,
      "loss": 1.7172,
      "step": 259360
    },
    {
      "epoch": 0.0009413850478823381,
      "grad_norm": 9532.535654273735,
      "learning_rate": 1.9644394113728022e-07,
      "loss": 1.7012,
      "step": 259392
    },
    {
      "epoch": 0.0009415011822331747,
      "grad_norm": 11657.321990920556,
      "learning_rate": 1.964318129560573e-07,
      "loss": 1.7101,
      "step": 259424
    },
    {
      "epoch": 0.0009416173165840115,
      "grad_norm": 8798.91902451659,
      "learning_rate": 1.964196870208892e-07,
      "loss": 1.7208,
      "step": 259456
    },
    {
      "epoch": 0.0009417334509348481,
      "grad_norm": 8319.434836573937,
      "learning_rate": 1.964075633310827e-07,
      "loss": 1.7126,
      "step": 259488
    },
    {
      "epoch": 0.0009418495852856849,
      "grad_norm": 11742.368755919735,
      "learning_rate": 1.9639544188594497e-07,
      "loss": 1.705,
      "step": 259520
    },
    {
      "epoch": 0.0009419657196365215,
      "grad_norm": 10965.740102701688,
      "learning_rate": 1.9638332268478344e-07,
      "loss": 1.7028,
      "step": 259552
    },
    {
      "epoch": 0.0009420818539873583,
      "grad_norm": 9515.021702550132,
      "learning_rate": 1.9637120572690584e-07,
      "loss": 1.7239,
      "step": 259584
    },
    {
      "epoch": 0.000942197988338195,
      "grad_norm": 9259.527201752799,
      "learning_rate": 1.9635909101162018e-07,
      "loss": 1.7059,
      "step": 259616
    },
    {
      "epoch": 0.0009423141226890317,
      "grad_norm": 8580.637738536689,
      "learning_rate": 1.963469785382348e-07,
      "loss": 1.7366,
      "step": 259648
    },
    {
      "epoch": 0.0009424302570398684,
      "grad_norm": 9932.036649147041,
      "learning_rate": 1.9633486830605832e-07,
      "loss": 1.7234,
      "step": 259680
    },
    {
      "epoch": 0.000942546391390705,
      "grad_norm": 8152.401364015391,
      "learning_rate": 1.9632276031439963e-07,
      "loss": 1.6836,
      "step": 259712
    },
    {
      "epoch": 0.0009426625257415418,
      "grad_norm": 10975.893767707485,
      "learning_rate": 1.96310654562568e-07,
      "loss": 1.6845,
      "step": 259744
    },
    {
      "epoch": 0.0009427786600923785,
      "grad_norm": 8968.453266868262,
      "learning_rate": 1.9629855104987292e-07,
      "loss": 1.6928,
      "step": 259776
    },
    {
      "epoch": 0.0009428947944432152,
      "grad_norm": 10247.938524405774,
      "learning_rate": 1.9628644977562423e-07,
      "loss": 1.7081,
      "step": 259808
    },
    {
      "epoch": 0.0009430109287940519,
      "grad_norm": 9181.54464128994,
      "learning_rate": 1.9627435073913201e-07,
      "loss": 1.7105,
      "step": 259840
    },
    {
      "epoch": 0.0009431270631448886,
      "grad_norm": 10644.471804650524,
      "learning_rate": 1.9626225393970672e-07,
      "loss": 1.7007,
      "step": 259872
    },
    {
      "epoch": 0.0009432431974957254,
      "grad_norm": 9338.021846194193,
      "learning_rate": 1.96250159376659e-07,
      "loss": 1.7017,
      "step": 259904
    },
    {
      "epoch": 0.000943359331846562,
      "grad_norm": 10008.321737434304,
      "learning_rate": 1.962380670492999e-07,
      "loss": 1.7252,
      "step": 259936
    },
    {
      "epoch": 0.0009434754661973988,
      "grad_norm": 9663.147313375699,
      "learning_rate": 1.9622597695694076e-07,
      "loss": 1.6976,
      "step": 259968
    },
    {
      "epoch": 0.0009435916005482354,
      "grad_norm": 9503.04466999919,
      "learning_rate": 1.962138890988931e-07,
      "loss": 1.7226,
      "step": 260000
    },
    {
      "epoch": 0.0009437077348990722,
      "grad_norm": 8948.87300166898,
      "learning_rate": 1.9620180347446887e-07,
      "loss": 1.7258,
      "step": 260032
    },
    {
      "epoch": 0.0009438238692499088,
      "grad_norm": 10628.12175316034,
      "learning_rate": 1.9618972008298025e-07,
      "loss": 1.6823,
      "step": 260064
    },
    {
      "epoch": 0.0009439400036007456,
      "grad_norm": 9248.75710568723,
      "learning_rate": 1.9617763892373973e-07,
      "loss": 1.6937,
      "step": 260096
    },
    {
      "epoch": 0.0009440561379515822,
      "grad_norm": 19289.85204712571,
      "learning_rate": 1.961655599960601e-07,
      "loss": 1.6947,
      "step": 260128
    },
    {
      "epoch": 0.000944172272302419,
      "grad_norm": 10871.317307483947,
      "learning_rate": 1.9615386066226835e-07,
      "loss": 1.6914,
      "step": 260160
    },
    {
      "epoch": 0.0009442884066532557,
      "grad_norm": 11193.874396293715,
      "learning_rate": 1.961417861259671e-07,
      "loss": 1.7159,
      "step": 260192
    },
    {
      "epoch": 0.0009444045410040924,
      "grad_norm": 10435.588914862448,
      "learning_rate": 1.961297138191883e-07,
      "loss": 1.7138,
      "step": 260224
    },
    {
      "epoch": 0.0009445206753549291,
      "grad_norm": 10032.233051519486,
      "learning_rate": 1.9611764374124593e-07,
      "loss": 1.6787,
      "step": 260256
    },
    {
      "epoch": 0.0009446368097057658,
      "grad_norm": 9526.994699274268,
      "learning_rate": 1.9610557589145426e-07,
      "loss": 1.7038,
      "step": 260288
    },
    {
      "epoch": 0.0009447529440566025,
      "grad_norm": 9608.448782191641,
      "learning_rate": 1.9609351026912784e-07,
      "loss": 1.6872,
      "step": 260320
    },
    {
      "epoch": 0.0009448690784074392,
      "grad_norm": 8499.192902858482,
      "learning_rate": 1.960814468735815e-07,
      "loss": 1.711,
      "step": 260352
    },
    {
      "epoch": 0.0009449852127582759,
      "grad_norm": 9074.604343992083,
      "learning_rate": 1.960693857041304e-07,
      "loss": 1.7172,
      "step": 260384
    },
    {
      "epoch": 0.0009451013471091126,
      "grad_norm": 9253.914090805036,
      "learning_rate": 1.9605732676008992e-07,
      "loss": 1.6974,
      "step": 260416
    },
    {
      "epoch": 0.0009452174814599493,
      "grad_norm": 10642.659066229642,
      "learning_rate": 1.9604527004077588e-07,
      "loss": 1.7219,
      "step": 260448
    },
    {
      "epoch": 0.0009453336158107861,
      "grad_norm": 10850.03907827064,
      "learning_rate": 1.9603321554550428e-07,
      "loss": 1.7142,
      "step": 260480
    },
    {
      "epoch": 0.0009454497501616227,
      "grad_norm": 9345.380463095122,
      "learning_rate": 1.9602116327359142e-07,
      "loss": 1.7235,
      "step": 260512
    },
    {
      "epoch": 0.0009455658845124595,
      "grad_norm": 9247.545728462228,
      "learning_rate": 1.960091132243539e-07,
      "loss": 1.7314,
      "step": 260544
    },
    {
      "epoch": 0.0009456820188632961,
      "grad_norm": 7974.451203687938,
      "learning_rate": 1.959970653971087e-07,
      "loss": 1.7069,
      "step": 260576
    },
    {
      "epoch": 0.0009457981532141329,
      "grad_norm": 8427.41894057724,
      "learning_rate": 1.9598501979117293e-07,
      "loss": 1.7013,
      "step": 260608
    },
    {
      "epoch": 0.0009459142875649695,
      "grad_norm": 10422.720949924736,
      "learning_rate": 1.9597297640586414e-07,
      "loss": 1.7215,
      "step": 260640
    },
    {
      "epoch": 0.0009460304219158063,
      "grad_norm": 8869.266711515671,
      "learning_rate": 1.9596093524050012e-07,
      "loss": 1.682,
      "step": 260672
    },
    {
      "epoch": 0.0009461465562666429,
      "grad_norm": 11381.599536093334,
      "learning_rate": 1.9594889629439895e-07,
      "loss": 1.7067,
      "step": 260704
    },
    {
      "epoch": 0.0009462626906174797,
      "grad_norm": 7766.023821750742,
      "learning_rate": 1.9593685956687898e-07,
      "loss": 1.7031,
      "step": 260736
    },
    {
      "epoch": 0.0009463788249683164,
      "grad_norm": 9514.461414079096,
      "learning_rate": 1.9592482505725893e-07,
      "loss": 1.6888,
      "step": 260768
    },
    {
      "epoch": 0.0009464949593191531,
      "grad_norm": 10728.459162433346,
      "learning_rate": 1.959127927648577e-07,
      "loss": 1.7173,
      "step": 260800
    },
    {
      "epoch": 0.0009466110936699898,
      "grad_norm": 9108.35693196089,
      "learning_rate": 1.959007626889946e-07,
      "loss": 1.7111,
      "step": 260832
    },
    {
      "epoch": 0.0009467272280208265,
      "grad_norm": 8759.78024838523,
      "learning_rate": 1.9588873482898916e-07,
      "loss": 1.7012,
      "step": 260864
    },
    {
      "epoch": 0.0009468433623716632,
      "grad_norm": 10497.059397755163,
      "learning_rate": 1.9587670918416122e-07,
      "loss": 1.7061,
      "step": 260896
    },
    {
      "epoch": 0.0009469594967224999,
      "grad_norm": 8828.447428625263,
      "learning_rate": 1.958646857538309e-07,
      "loss": 1.6894,
      "step": 260928
    },
    {
      "epoch": 0.0009470756310733366,
      "grad_norm": 10598.112662167732,
      "learning_rate": 1.9585266453731866e-07,
      "loss": 1.6834,
      "step": 260960
    },
    {
      "epoch": 0.0009471917654241733,
      "grad_norm": 8591.243216205674,
      "learning_rate": 1.958406455339452e-07,
      "loss": 1.7018,
      "step": 260992
    },
    {
      "epoch": 0.00094730789977501,
      "grad_norm": 10628.791746948475,
      "learning_rate": 1.958286287430315e-07,
      "loss": 1.6824,
      "step": 261024
    },
    {
      "epoch": 0.0009474240341258468,
      "grad_norm": 8896.904855060551,
      "learning_rate": 1.9581661416389885e-07,
      "loss": 1.7082,
      "step": 261056
    },
    {
      "epoch": 0.0009475401684766834,
      "grad_norm": 9828.111313980931,
      "learning_rate": 1.958046017958689e-07,
      "loss": 1.7096,
      "step": 261088
    },
    {
      "epoch": 0.0009476563028275202,
      "grad_norm": 8310.661586179527,
      "learning_rate": 1.9579259163826353e-07,
      "loss": 1.6935,
      "step": 261120
    },
    {
      "epoch": 0.0009477724371783568,
      "grad_norm": 9639.493140201927,
      "learning_rate": 1.9578095890533386e-07,
      "loss": 1.7104,
      "step": 261152
    },
    {
      "epoch": 0.0009478885715291936,
      "grad_norm": 9297.983437283592,
      "learning_rate": 1.9576895309752126e-07,
      "loss": 1.7155,
      "step": 261184
    },
    {
      "epoch": 0.0009480047058800302,
      "grad_norm": 8649.406222394691,
      "learning_rate": 1.9575694949812173e-07,
      "loss": 1.7243,
      "step": 261216
    },
    {
      "epoch": 0.000948120840230867,
      "grad_norm": 8080.8661664452775,
      "learning_rate": 1.9574494810645836e-07,
      "loss": 1.7233,
      "step": 261248
    },
    {
      "epoch": 0.0009482369745817036,
      "grad_norm": 9823.242641816398,
      "learning_rate": 1.9573294892185446e-07,
      "loss": 1.7085,
      "step": 261280
    },
    {
      "epoch": 0.0009483531089325404,
      "grad_norm": 9140.330409782788,
      "learning_rate": 1.9572095194363368e-07,
      "loss": 1.7188,
      "step": 261312
    },
    {
      "epoch": 0.0009484692432833771,
      "grad_norm": 9656.969296834282,
      "learning_rate": 1.957089571711199e-07,
      "loss": 1.7238,
      "step": 261344
    },
    {
      "epoch": 0.0009485853776342138,
      "grad_norm": 9807.178391362115,
      "learning_rate": 1.956969646036373e-07,
      "loss": 1.7286,
      "step": 261376
    },
    {
      "epoch": 0.0009487015119850505,
      "grad_norm": 8137.786062560259,
      "learning_rate": 1.9568497424051038e-07,
      "loss": 1.7367,
      "step": 261408
    },
    {
      "epoch": 0.0009488176463358872,
      "grad_norm": 8875.607021494361,
      "learning_rate": 1.9567298608106395e-07,
      "loss": 1.7117,
      "step": 261440
    },
    {
      "epoch": 0.0009489337806867239,
      "grad_norm": 9732.420048477152,
      "learning_rate": 1.9566100012462305e-07,
      "loss": 1.6932,
      "step": 261472
    },
    {
      "epoch": 0.0009490499150375606,
      "grad_norm": 10770.811482892086,
      "learning_rate": 1.9564901637051303e-07,
      "loss": 1.6919,
      "step": 261504
    },
    {
      "epoch": 0.0009491660493883973,
      "grad_norm": 10020.687401570813,
      "learning_rate": 1.9563703481805956e-07,
      "loss": 1.6877,
      "step": 261536
    },
    {
      "epoch": 0.000949282183739234,
      "grad_norm": 8872.395392451805,
      "learning_rate": 1.9562505546658854e-07,
      "loss": 1.706,
      "step": 261568
    },
    {
      "epoch": 0.0009493983180900707,
      "grad_norm": 10279.880933162602,
      "learning_rate": 1.956130783154262e-07,
      "loss": 1.7053,
      "step": 261600
    },
    {
      "epoch": 0.0009495144524409075,
      "grad_norm": 9087.964788664181,
      "learning_rate": 1.9560110336389913e-07,
      "loss": 1.6999,
      "step": 261632
    },
    {
      "epoch": 0.0009496305867917441,
      "grad_norm": 8909.432978590725,
      "learning_rate": 1.9558913061133403e-07,
      "loss": 1.7022,
      "step": 261664
    },
    {
      "epoch": 0.0009497467211425809,
      "grad_norm": 9871.057694087296,
      "learning_rate": 1.9557716005705802e-07,
      "loss": 1.7216,
      "step": 261696
    },
    {
      "epoch": 0.0009498628554934175,
      "grad_norm": 11892.01109989391,
      "learning_rate": 1.955651917003985e-07,
      "loss": 1.6919,
      "step": 261728
    },
    {
      "epoch": 0.0009499789898442543,
      "grad_norm": 9725.82078798494,
      "learning_rate": 1.955532255406831e-07,
      "loss": 1.722,
      "step": 261760
    },
    {
      "epoch": 0.0009500951241950909,
      "grad_norm": 9966.90443417614,
      "learning_rate": 1.9554126157723984e-07,
      "loss": 1.7308,
      "step": 261792
    },
    {
      "epoch": 0.0009502112585459277,
      "grad_norm": 11505.355100995363,
      "learning_rate": 1.9552929980939687e-07,
      "loss": 1.6978,
      "step": 261824
    },
    {
      "epoch": 0.0009503273928967643,
      "grad_norm": 8855.840106957668,
      "learning_rate": 1.955173402364828e-07,
      "loss": 1.696,
      "step": 261856
    },
    {
      "epoch": 0.0009504435272476011,
      "grad_norm": 12243.293347788413,
      "learning_rate": 1.9550538285782638e-07,
      "loss": 1.6967,
      "step": 261888
    },
    {
      "epoch": 0.0009505596615984378,
      "grad_norm": 9321.271908918869,
      "learning_rate": 1.954934276727568e-07,
      "loss": 1.6991,
      "step": 261920
    },
    {
      "epoch": 0.0009506757959492745,
      "grad_norm": 9277.680529097775,
      "learning_rate": 1.9548147468060334e-07,
      "loss": 1.7106,
      "step": 261952
    },
    {
      "epoch": 0.0009507919303001112,
      "grad_norm": 10531.706414442058,
      "learning_rate": 1.9546952388069576e-07,
      "loss": 1.7037,
      "step": 261984
    },
    {
      "epoch": 0.0009509080646509479,
      "grad_norm": 9373.569010787727,
      "learning_rate": 1.9545757527236402e-07,
      "loss": 1.6903,
      "step": 262016
    },
    {
      "epoch": 0.0009510241990017846,
      "grad_norm": 10041.183993932189,
      "learning_rate": 1.9544562885493832e-07,
      "loss": 1.7011,
      "step": 262048
    },
    {
      "epoch": 0.0009511403333526213,
      "grad_norm": 9421.251721507073,
      "learning_rate": 1.9543368462774924e-07,
      "loss": 1.6919,
      "step": 262080
    },
    {
      "epoch": 0.000951256467703458,
      "grad_norm": 10527.953837284813,
      "learning_rate": 1.954217425901276e-07,
      "loss": 1.7181,
      "step": 262112
    },
    {
      "epoch": 0.0009513726020542947,
      "grad_norm": 11070.64876147735,
      "learning_rate": 1.9540980274140452e-07,
      "loss": 1.7243,
      "step": 262144
    },
    {
      "epoch": 0.0009513726020542947,
      "eval_loss": 1.7109375,
      "eval_runtime": 24797.1786,
      "eval_samples_per_second": 214.912,
      "eval_steps_per_second": 0.42,
      "step": 262144
    }
  ],
  "logging_steps": 32,
  "max_steps": 275542936,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 262144,
  "total_flos": 2.0463160438120514e+19,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
