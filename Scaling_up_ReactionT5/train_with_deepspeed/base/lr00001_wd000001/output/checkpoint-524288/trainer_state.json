{
  "best_metric": 1.7109375,
  "best_model_checkpoint": "./output/checkpoint-262144",
  "epoch": 0.0019027452041085894,
  "eval_steps": 262144,
  "global_step": 524288,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.161343508367059e-07,
      "grad_norm": 409572.6672130356,
      "learning_rate": 0.0001,
      "loss": 93.4514,
      "step": 32
    },
    {
      "epoch": 2.322687016734118e-07,
      "grad_norm": 298229.6124800487,
      "learning_rate": 0.0001,
      "loss": 29.8428,
      "step": 64
    },
    {
      "epoch": 3.484030525101177e-07,
      "grad_norm": 116628.18664456718,
      "learning_rate": 0.0001,
      "loss": 24.1542,
      "step": 96
    },
    {
      "epoch": 4.645374033468236e-07,
      "grad_norm": 139092.62571394647,
      "learning_rate": 0.0001,
      "loss": 21.9257,
      "step": 128
    },
    {
      "epoch": 5.806717541835295e-07,
      "grad_norm": 76551.02225313519,
      "learning_rate": 0.0001,
      "loss": 19.5443,
      "step": 160
    },
    {
      "epoch": 6.968061050202354e-07,
      "grad_norm": 77397.12640660505,
      "learning_rate": 0.0001,
      "loss": 18.6562,
      "step": 192
    },
    {
      "epoch": 8.129404558569413e-07,
      "grad_norm": 95565.95644893634,
      "learning_rate": 0.0001,
      "loss": 16.8551,
      "step": 224
    },
    {
      "epoch": 9.290748066936471e-07,
      "grad_norm": 46406.155755459855,
      "learning_rate": 0.0001,
      "loss": 15.6748,
      "step": 256
    },
    {
      "epoch": 1.045209157530353e-06,
      "grad_norm": 56922.60166225715,
      "learning_rate": 0.0001,
      "loss": 15.2214,
      "step": 288
    },
    {
      "epoch": 1.161343508367059e-06,
      "grad_norm": 58975.64263320918,
      "learning_rate": 0.0001,
      "loss": 14.0696,
      "step": 320
    },
    {
      "epoch": 1.2774778592037649e-06,
      "grad_norm": 65675.34386662926,
      "learning_rate": 0.0001,
      "loss": 13.6104,
      "step": 352
    },
    {
      "epoch": 1.3936122100404707e-06,
      "grad_norm": 54383.54637204161,
      "learning_rate": 0.0001,
      "loss": 12.461,
      "step": 384
    },
    {
      "epoch": 1.5097465608771768e-06,
      "grad_norm": 52796.19152931393,
      "learning_rate": 0.0001,
      "loss": 12.1938,
      "step": 416
    },
    {
      "epoch": 1.6258809117138826e-06,
      "grad_norm": 59098.14467476962,
      "learning_rate": 0.0001,
      "loss": 12.2385,
      "step": 448
    },
    {
      "epoch": 1.7420152625505885e-06,
      "grad_norm": 39881.043165895244,
      "learning_rate": 0.0001,
      "loss": 11.5189,
      "step": 480
    },
    {
      "epoch": 1.8581496133872943e-06,
      "grad_norm": 46619.10501071422,
      "learning_rate": 0.0001,
      "loss": 11.2951,
      "step": 512
    },
    {
      "epoch": 1.974283964224e-06,
      "grad_norm": 34572.07248632919,
      "learning_rate": 0.0001,
      "loss": 10.9112,
      "step": 544
    },
    {
      "epoch": 2.090418315060706e-06,
      "grad_norm": 46754.31603606238,
      "learning_rate": 0.0001,
      "loss": 10.7267,
      "step": 576
    },
    {
      "epoch": 2.2065526658974122e-06,
      "grad_norm": 32216.06530909695,
      "learning_rate": 0.0001,
      "loss": 10.3765,
      "step": 608
    },
    {
      "epoch": 2.322687016734118e-06,
      "grad_norm": 39603.068921486374,
      "learning_rate": 0.0001,
      "loss": 10.1508,
      "step": 640
    },
    {
      "epoch": 2.438821367570824e-06,
      "grad_norm": 40816.37112728176,
      "learning_rate": 0.0001,
      "loss": 9.8823,
      "step": 672
    },
    {
      "epoch": 2.5549557184075298e-06,
      "grad_norm": 35320.400507355516,
      "learning_rate": 0.0001,
      "loss": 9.5353,
      "step": 704
    },
    {
      "epoch": 2.6710900692442356e-06,
      "grad_norm": 37960.90167527637,
      "learning_rate": 0.0001,
      "loss": 9.2725,
      "step": 736
    },
    {
      "epoch": 2.7872244200809414e-06,
      "grad_norm": 44359.30657708707,
      "learning_rate": 0.0001,
      "loss": 9.181,
      "step": 768
    },
    {
      "epoch": 2.9033587709176473e-06,
      "grad_norm": 33022.60225966451,
      "learning_rate": 0.0001,
      "loss": 8.9102,
      "step": 800
    },
    {
      "epoch": 3.0194931217543536e-06,
      "grad_norm": 32567.61557130027,
      "learning_rate": 0.0001,
      "loss": 8.7962,
      "step": 832
    },
    {
      "epoch": 3.1356274725910594e-06,
      "grad_norm": 39935.8979365683,
      "learning_rate": 0.0001,
      "loss": 8.5775,
      "step": 864
    },
    {
      "epoch": 3.2517618234277652e-06,
      "grad_norm": 45957.19573690283,
      "learning_rate": 0.0001,
      "loss": 8.8212,
      "step": 896
    },
    {
      "epoch": 3.367896174264471e-06,
      "grad_norm": 34437.40036646204,
      "learning_rate": 0.0001,
      "loss": 8.1587,
      "step": 928
    },
    {
      "epoch": 3.484030525101177e-06,
      "grad_norm": 32333.58532547852,
      "learning_rate": 0.0001,
      "loss": 8.0502,
      "step": 960
    },
    {
      "epoch": 3.6001648759378828e-06,
      "grad_norm": 27926.411978626973,
      "learning_rate": 0.0001,
      "loss": 7.6843,
      "step": 992
    },
    {
      "epoch": 3.7162992267745886e-06,
      "grad_norm": 46014.227669276384,
      "learning_rate": 0.0001,
      "loss": 7.5791,
      "step": 1024
    },
    {
      "epoch": 3.8324335776112944e-06,
      "grad_norm": 26273.29929795647,
      "learning_rate": 0.0001,
      "loss": 7.6431,
      "step": 1056
    },
    {
      "epoch": 3.948567928448e-06,
      "grad_norm": 30534.236129302466,
      "learning_rate": 0.0001,
      "loss": 7.1365,
      "step": 1088
    },
    {
      "epoch": 4.064702279284706e-06,
      "grad_norm": 23060.03321766905,
      "learning_rate": 0.0001,
      "loss": 7.2321,
      "step": 1120
    },
    {
      "epoch": 4.180836630121412e-06,
      "grad_norm": 24888.293633754805,
      "learning_rate": 0.0001,
      "loss": 7.1035,
      "step": 1152
    },
    {
      "epoch": 4.296970980958119e-06,
      "grad_norm": 30571.920907918102,
      "learning_rate": 0.0001,
      "loss": 6.8501,
      "step": 1184
    },
    {
      "epoch": 4.4131053317948245e-06,
      "grad_norm": 23334.540149743687,
      "learning_rate": 0.0001,
      "loss": 6.5739,
      "step": 1216
    },
    {
      "epoch": 4.52923968263153e-06,
      "grad_norm": 37391.70378573301,
      "learning_rate": 0.0001,
      "loss": 6.5454,
      "step": 1248
    },
    {
      "epoch": 4.645374033468236e-06,
      "grad_norm": 27152.180317609855,
      "learning_rate": 0.0001,
      "loss": 6.4446,
      "step": 1280
    },
    {
      "epoch": 4.761508384304942e-06,
      "grad_norm": 30626.713372479262,
      "learning_rate": 0.0001,
      "loss": 6.3004,
      "step": 1312
    },
    {
      "epoch": 4.877642735141648e-06,
      "grad_norm": 31318.10952148932,
      "learning_rate": 0.0001,
      "loss": 6.191,
      "step": 1344
    },
    {
      "epoch": 4.993777085978354e-06,
      "grad_norm": 24971.72232746472,
      "learning_rate": 0.0001,
      "loss": 6.0902,
      "step": 1376
    },
    {
      "epoch": 5.1099114368150595e-06,
      "grad_norm": 19424.006074957862,
      "learning_rate": 0.0001,
      "loss": 6.0366,
      "step": 1408
    },
    {
      "epoch": 5.226045787651765e-06,
      "grad_norm": 35026.2539247348,
      "learning_rate": 0.0001,
      "loss": 6.0009,
      "step": 1440
    },
    {
      "epoch": 5.342180138488471e-06,
      "grad_norm": 26796.96822403609,
      "learning_rate": 0.0001,
      "loss": 5.9144,
      "step": 1472
    },
    {
      "epoch": 5.458314489325177e-06,
      "grad_norm": 18411.54549460745,
      "learning_rate": 0.0001,
      "loss": 5.6386,
      "step": 1504
    },
    {
      "epoch": 5.574448840161883e-06,
      "grad_norm": 16159.86540785535,
      "learning_rate": 0.0001,
      "loss": 5.5917,
      "step": 1536
    },
    {
      "epoch": 5.690583190998589e-06,
      "grad_norm": 16693.284952339367,
      "learning_rate": 0.0001,
      "loss": 5.5403,
      "step": 1568
    },
    {
      "epoch": 5.806717541835295e-06,
      "grad_norm": 15610.660428053643,
      "learning_rate": 0.0001,
      "loss": 5.4474,
      "step": 1600
    },
    {
      "epoch": 5.922851892672001e-06,
      "grad_norm": 13846.19155219225,
      "learning_rate": 0.0001,
      "loss": 5.2932,
      "step": 1632
    },
    {
      "epoch": 6.038986243508707e-06,
      "grad_norm": 15878.182169253507,
      "learning_rate": 0.0001,
      "loss": 5.2903,
      "step": 1664
    },
    {
      "epoch": 6.155120594345413e-06,
      "grad_norm": 13490.37138109993,
      "learning_rate": 0.0001,
      "loss": 5.2357,
      "step": 1696
    },
    {
      "epoch": 6.271254945182119e-06,
      "grad_norm": 17380.39524867027,
      "learning_rate": 0.0001,
      "loss": 5.1873,
      "step": 1728
    },
    {
      "epoch": 6.387389296018825e-06,
      "grad_norm": 11603.54118362149,
      "learning_rate": 0.0001,
      "loss": 5.2631,
      "step": 1760
    },
    {
      "epoch": 6.5035236468555305e-06,
      "grad_norm": 8962.60179858505,
      "learning_rate": 0.0001,
      "loss": 4.9992,
      "step": 1792
    },
    {
      "epoch": 6.619657997692236e-06,
      "grad_norm": 14465.668252797726,
      "learning_rate": 0.0001,
      "loss": 4.8946,
      "step": 1824
    },
    {
      "epoch": 6.735792348528942e-06,
      "grad_norm": 19733.17369304796,
      "learning_rate": 0.0001,
      "loss": 4.9184,
      "step": 1856
    },
    {
      "epoch": 6.851926699365648e-06,
      "grad_norm": 12558.132385032417,
      "learning_rate": 0.0001,
      "loss": 4.8565,
      "step": 1888
    },
    {
      "epoch": 6.968061050202354e-06,
      "grad_norm": 14083.85621909,
      "learning_rate": 0.0001,
      "loss": 4.8656,
      "step": 1920
    },
    {
      "epoch": 7.08419540103906e-06,
      "grad_norm": 16702.6788420301,
      "learning_rate": 0.0001,
      "loss": 4.7063,
      "step": 1952
    },
    {
      "epoch": 7.2003297518757655e-06,
      "grad_norm": 13509.329887155765,
      "learning_rate": 0.0001,
      "loss": 4.5831,
      "step": 1984
    },
    {
      "epoch": 7.316464102712471e-06,
      "grad_norm": 10776.858818783887,
      "learning_rate": 0.0001,
      "loss": 4.6689,
      "step": 2016
    },
    {
      "epoch": 7.432598453549177e-06,
      "grad_norm": 15217.366091081596,
      "learning_rate": 0.0001,
      "loss": 4.6723,
      "step": 2048
    },
    {
      "epoch": 7.548732804385883e-06,
      "grad_norm": 14957.827248634743,
      "learning_rate": 0.0001,
      "loss": 4.436,
      "step": 2080
    },
    {
      "epoch": 7.664867155222589e-06,
      "grad_norm": 13795.876847812175,
      "learning_rate": 0.0001,
      "loss": 4.4759,
      "step": 2112
    },
    {
      "epoch": 7.781001506059296e-06,
      "grad_norm": 11286.501229344725,
      "learning_rate": 0.0001,
      "loss": 4.4257,
      "step": 2144
    },
    {
      "epoch": 7.897135856896e-06,
      "grad_norm": 17644.0152743076,
      "learning_rate": 0.0001,
      "loss": 4.3807,
      "step": 2176
    },
    {
      "epoch": 8.013270207732707e-06,
      "grad_norm": 12903.463469162069,
      "learning_rate": 0.0001,
      "loss": 4.4125,
      "step": 2208
    },
    {
      "epoch": 8.129404558569412e-06,
      "grad_norm": 11965.748451308844,
      "learning_rate": 0.0001,
      "loss": 4.3511,
      "step": 2240
    },
    {
      "epoch": 8.245538909406119e-06,
      "grad_norm": 12105.225462584329,
      "learning_rate": 0.0001,
      "loss": 4.3268,
      "step": 2272
    },
    {
      "epoch": 8.361673260242824e-06,
      "grad_norm": 7619.67606758975,
      "learning_rate": 0.0001,
      "loss": 4.2662,
      "step": 2304
    },
    {
      "epoch": 8.47780761107953e-06,
      "grad_norm": 13129.423406989357,
      "learning_rate": 0.0001,
      "loss": 4.2016,
      "step": 2336
    },
    {
      "epoch": 8.593941961916237e-06,
      "grad_norm": 13801.409927974752,
      "learning_rate": 0.0001,
      "loss": 4.2434,
      "step": 2368
    },
    {
      "epoch": 8.710076312752942e-06,
      "grad_norm": 9483.954330868532,
      "learning_rate": 0.0001,
      "loss": 4.117,
      "step": 2400
    },
    {
      "epoch": 8.826210663589649e-06,
      "grad_norm": 11325.919565315657,
      "learning_rate": 0.0001,
      "loss": 4.0331,
      "step": 2432
    },
    {
      "epoch": 8.942345014426354e-06,
      "grad_norm": 9246.819885776948,
      "learning_rate": 0.0001,
      "loss": 4.1495,
      "step": 2464
    },
    {
      "epoch": 9.05847936526306e-06,
      "grad_norm": 22537.95500927269,
      "learning_rate": 0.0001,
      "loss": 4.0052,
      "step": 2496
    },
    {
      "epoch": 9.174613716099766e-06,
      "grad_norm": 25145.246111342793,
      "learning_rate": 0.0001,
      "loss": 3.9738,
      "step": 2528
    },
    {
      "epoch": 9.290748066936472e-06,
      "grad_norm": 16548.68197923931,
      "learning_rate": 0.0001,
      "loss": 3.9415,
      "step": 2560
    },
    {
      "epoch": 9.406882417773177e-06,
      "grad_norm": 27505.274839564863,
      "learning_rate": 0.0001,
      "loss": 4.0287,
      "step": 2592
    },
    {
      "epoch": 9.523016768609884e-06,
      "grad_norm": 34385.620715642166,
      "learning_rate": 0.0001,
      "loss": 4.0115,
      "step": 2624
    },
    {
      "epoch": 9.639151119446589e-06,
      "grad_norm": 17275.651420424063,
      "learning_rate": 0.0001,
      "loss": 3.8689,
      "step": 2656
    },
    {
      "epoch": 9.755285470283296e-06,
      "grad_norm": 21617.040176675437,
      "learning_rate": 0.0001,
      "loss": 3.937,
      "step": 2688
    },
    {
      "epoch": 9.87141982112e-06,
      "grad_norm": 24796.554075112937,
      "learning_rate": 0.0001,
      "loss": 3.9091,
      "step": 2720
    },
    {
      "epoch": 9.987554171956707e-06,
      "grad_norm": 25231.23207455395,
      "learning_rate": 0.0001,
      "loss": 3.9054,
      "step": 2752
    },
    {
      "epoch": 1.0103688522793412e-05,
      "grad_norm": 17232.006760676486,
      "learning_rate": 0.0001,
      "loss": 3.8413,
      "step": 2784
    },
    {
      "epoch": 1.0219822873630119e-05,
      "grad_norm": 21326.664061685784,
      "learning_rate": 0.0001,
      "loss": 3.835,
      "step": 2816
    },
    {
      "epoch": 1.0335957224466826e-05,
      "grad_norm": 21415.05984114917,
      "learning_rate": 0.0001,
      "loss": 3.7126,
      "step": 2848
    },
    {
      "epoch": 1.045209157530353e-05,
      "grad_norm": 22420.70578728511,
      "learning_rate": 0.0001,
      "loss": 3.7849,
      "step": 2880
    },
    {
      "epoch": 1.0568225926140237e-05,
      "grad_norm": 22287.552220914702,
      "learning_rate": 0.0001,
      "loss": 3.763,
      "step": 2912
    },
    {
      "epoch": 1.0684360276976942e-05,
      "grad_norm": 28384.378062589287,
      "learning_rate": 0.0001,
      "loss": 3.7441,
      "step": 2944
    },
    {
      "epoch": 1.0800494627813649e-05,
      "grad_norm": 24712.028791663386,
      "learning_rate": 0.0001,
      "loss": 3.6634,
      "step": 2976
    },
    {
      "epoch": 1.0916628978650354e-05,
      "grad_norm": 20253.55057761478,
      "learning_rate": 0.0001,
      "loss": 3.6061,
      "step": 3008
    },
    {
      "epoch": 1.103276332948706e-05,
      "grad_norm": 23643.859477674112,
      "learning_rate": 0.0001,
      "loss": 3.6196,
      "step": 3040
    },
    {
      "epoch": 1.1148897680323766e-05,
      "grad_norm": 29160.01148833793,
      "learning_rate": 0.0001,
      "loss": 3.5903,
      "step": 3072
    },
    {
      "epoch": 1.1265032031160472e-05,
      "grad_norm": 20934.49555160095,
      "learning_rate": 0.0001,
      "loss": 3.6281,
      "step": 3104
    },
    {
      "epoch": 1.1381166381997177e-05,
      "grad_norm": 20391.92585314099,
      "learning_rate": 0.0001,
      "loss": 3.5786,
      "step": 3136
    },
    {
      "epoch": 1.1497300732833884e-05,
      "grad_norm": 25167.143898344922,
      "learning_rate": 0.0001,
      "loss": 3.534,
      "step": 3168
    },
    {
      "epoch": 1.161343508367059e-05,
      "grad_norm": 21929.786410268567,
      "learning_rate": 0.0001,
      "loss": 3.5526,
      "step": 3200
    },
    {
      "epoch": 1.1729569434507296e-05,
      "grad_norm": 23073.222943490146,
      "learning_rate": 0.0001,
      "loss": 3.4931,
      "step": 3232
    },
    {
      "epoch": 1.1845703785344003e-05,
      "grad_norm": 20149.36386092623,
      "learning_rate": 0.0001,
      "loss": 3.4955,
      "step": 3264
    },
    {
      "epoch": 1.1961838136180708e-05,
      "grad_norm": 21224.364725475294,
      "learning_rate": 0.0001,
      "loss": 3.4365,
      "step": 3296
    },
    {
      "epoch": 1.2077972487017414e-05,
      "grad_norm": 18944.67766946696,
      "learning_rate": 0.0001,
      "loss": 3.3932,
      "step": 3328
    },
    {
      "epoch": 1.219410683785412e-05,
      "grad_norm": 20244.963521824386,
      "learning_rate": 0.0001,
      "loss": 3.4429,
      "step": 3360
    },
    {
      "epoch": 1.2310241188690826e-05,
      "grad_norm": 24120.8867167026,
      "learning_rate": 0.0001,
      "loss": 3.397,
      "step": 3392
    },
    {
      "epoch": 1.2426375539527531e-05,
      "grad_norm": 15817.65191170927,
      "learning_rate": 0.0001,
      "loss": 3.3637,
      "step": 3424
    },
    {
      "epoch": 1.2542509890364238e-05,
      "grad_norm": 18846.486569119457,
      "learning_rate": 0.0001,
      "loss": 3.4019,
      "step": 3456
    },
    {
      "epoch": 1.2658644241200943e-05,
      "grad_norm": 28008.362536928144,
      "learning_rate": 0.0001,
      "loss": 3.3812,
      "step": 3488
    },
    {
      "epoch": 1.277477859203765e-05,
      "grad_norm": 19973.044935612597,
      "learning_rate": 0.0001,
      "loss": 3.3781,
      "step": 3520
    },
    {
      "epoch": 1.2890912942874354e-05,
      "grad_norm": 22302.577250174476,
      "learning_rate": 0.0001,
      "loss": 3.3601,
      "step": 3552
    },
    {
      "epoch": 1.3007047293711061e-05,
      "grad_norm": 17686.60589259567,
      "learning_rate": 0.0001,
      "loss": 3.2783,
      "step": 3584
    },
    {
      "epoch": 1.3123181644547766e-05,
      "grad_norm": 23607.784139982305,
      "learning_rate": 0.0001,
      "loss": 3.2929,
      "step": 3616
    },
    {
      "epoch": 1.3239315995384473e-05,
      "grad_norm": 18854.86547286933,
      "learning_rate": 0.0001,
      "loss": 3.3439,
      "step": 3648
    },
    {
      "epoch": 1.3355450346221178e-05,
      "grad_norm": 18161.94493990112,
      "learning_rate": 0.0001,
      "loss": 3.2751,
      "step": 3680
    },
    {
      "epoch": 1.3471584697057884e-05,
      "grad_norm": 18281.619293705906,
      "learning_rate": 0.0001,
      "loss": 3.3619,
      "step": 3712
    },
    {
      "epoch": 1.3587719047894591e-05,
      "grad_norm": 15669.935736945446,
      "learning_rate": 0.0001,
      "loss": 3.212,
      "step": 3744
    },
    {
      "epoch": 1.3703853398731296e-05,
      "grad_norm": 12464.307100677519,
      "learning_rate": 0.0001,
      "loss": 3.1625,
      "step": 3776
    },
    {
      "epoch": 1.3819987749568003e-05,
      "grad_norm": 17099.399755546976,
      "learning_rate": 0.0001,
      "loss": 3.202,
      "step": 3808
    },
    {
      "epoch": 1.3936122100404708e-05,
      "grad_norm": 22267.673677328756,
      "learning_rate": 0.0001,
      "loss": 3.2204,
      "step": 3840
    },
    {
      "epoch": 1.4052256451241414e-05,
      "grad_norm": 24745.124287422765,
      "learning_rate": 0.0001,
      "loss": 3.1489,
      "step": 3872
    },
    {
      "epoch": 1.416839080207812e-05,
      "grad_norm": 15584.810040549099,
      "learning_rate": 0.0001,
      "loss": 3.1428,
      "step": 3904
    },
    {
      "epoch": 1.4284525152914826e-05,
      "grad_norm": 14626.249553456963,
      "learning_rate": 0.0001,
      "loss": 3.0737,
      "step": 3936
    },
    {
      "epoch": 1.4400659503751531e-05,
      "grad_norm": 16616.824726764135,
      "learning_rate": 0.0001,
      "loss": 3.1624,
      "step": 3968
    },
    {
      "epoch": 1.4516793854588238e-05,
      "grad_norm": 21450.52055312411,
      "learning_rate": 0.0001,
      "loss": 3.1398,
      "step": 4000
    },
    {
      "epoch": 1.4632928205424943e-05,
      "grad_norm": 16308.334065746874,
      "learning_rate": 0.0001,
      "loss": 3.1596,
      "step": 4032
    },
    {
      "epoch": 1.474906255626165e-05,
      "grad_norm": 20303.278565295805,
      "learning_rate": 0.0001,
      "loss": 3.1389,
      "step": 4064
    },
    {
      "epoch": 1.4865196907098354e-05,
      "grad_norm": 11624.364326706214,
      "learning_rate": 0.0001,
      "loss": 3.1285,
      "step": 4096
    },
    {
      "epoch": 1.4981331257935061e-05,
      "grad_norm": 18035.782877380178,
      "learning_rate": 0.0001,
      "loss": 3.0929,
      "step": 4128
    },
    {
      "epoch": 1.5097465608771766e-05,
      "grad_norm": 16843.670087008948,
      "learning_rate": 0.0001,
      "loss": 3.0646,
      "step": 4160
    },
    {
      "epoch": 1.5213599959608473e-05,
      "grad_norm": 22057.891014328638,
      "learning_rate": 0.0001,
      "loss": 3.0149,
      "step": 4192
    },
    {
      "epoch": 1.5329734310445178e-05,
      "grad_norm": 16503.23586452063,
      "learning_rate": 0.0001,
      "loss": 3.0159,
      "step": 4224
    },
    {
      "epoch": 1.5445868661281884e-05,
      "grad_norm": 18436.948730741755,
      "learning_rate": 0.0001,
      "loss": 3.0184,
      "step": 4256
    },
    {
      "epoch": 1.556200301211859e-05,
      "grad_norm": 22615.68265606855,
      "learning_rate": 0.0001,
      "loss": 3.013,
      "step": 4288
    },
    {
      "epoch": 1.5678137362955298e-05,
      "grad_norm": 26030.19515869983,
      "learning_rate": 0.0001,
      "loss": 3.0253,
      "step": 4320
    },
    {
      "epoch": 1.5794271713792e-05,
      "grad_norm": 10740.673582229374,
      "learning_rate": 0.0001,
      "loss": 2.9711,
      "step": 4352
    },
    {
      "epoch": 1.5910406064628708e-05,
      "grad_norm": 16481.400547283596,
      "learning_rate": 0.0001,
      "loss": 2.9474,
      "step": 4384
    },
    {
      "epoch": 1.6026540415465414e-05,
      "grad_norm": 14825.707200670056,
      "learning_rate": 0.0001,
      "loss": 2.9869,
      "step": 4416
    },
    {
      "epoch": 1.614267476630212e-05,
      "grad_norm": 16911.679987511587,
      "learning_rate": 0.0001,
      "loss": 2.9715,
      "step": 4448
    },
    {
      "epoch": 1.6258809117138824e-05,
      "grad_norm": 19450.93190569542,
      "learning_rate": 0.0001,
      "loss": 2.9727,
      "step": 4480
    },
    {
      "epoch": 1.637494346797553e-05,
      "grad_norm": 12378.442551468259,
      "learning_rate": 0.0001,
      "loss": 2.9479,
      "step": 4512
    },
    {
      "epoch": 1.6491077818812238e-05,
      "grad_norm": 13415.24976472671,
      "learning_rate": 0.0001,
      "loss": 2.9736,
      "step": 4544
    },
    {
      "epoch": 1.6607212169648945e-05,
      "grad_norm": 19899.070631564682,
      "learning_rate": 0.0001,
      "loss": 2.9914,
      "step": 4576
    },
    {
      "epoch": 1.6723346520485648e-05,
      "grad_norm": 19685.893528107885,
      "learning_rate": 0.0001,
      "loss": 2.999,
      "step": 4608
    },
    {
      "epoch": 1.6839480871322355e-05,
      "grad_norm": 16442.116317554744,
      "learning_rate": 0.0001,
      "loss": 2.9148,
      "step": 4640
    },
    {
      "epoch": 1.695561522215906e-05,
      "grad_norm": 13557.160690941153,
      "learning_rate": 0.0001,
      "loss": 2.8907,
      "step": 4672
    },
    {
      "epoch": 1.7071749572995768e-05,
      "grad_norm": 13852.497247788935,
      "learning_rate": 0.0001,
      "loss": 2.9094,
      "step": 4704
    },
    {
      "epoch": 1.7187883923832475e-05,
      "grad_norm": 18497.08204014893,
      "learning_rate": 0.0001,
      "loss": 2.856,
      "step": 4736
    },
    {
      "epoch": 1.7304018274669178e-05,
      "grad_norm": 15590.688310655178,
      "learning_rate": 0.0001,
      "loss": 2.8007,
      "step": 4768
    },
    {
      "epoch": 1.7420152625505885e-05,
      "grad_norm": 21210.639665036037,
      "learning_rate": 0.0001,
      "loss": 2.8586,
      "step": 4800
    },
    {
      "epoch": 1.753628697634259e-05,
      "grad_norm": 19541.73216478007,
      "learning_rate": 0.0001,
      "loss": 2.8502,
      "step": 4832
    },
    {
      "epoch": 1.7652421327179298e-05,
      "grad_norm": 14852.01015014466,
      "learning_rate": 0.0001,
      "loss": 2.8688,
      "step": 4864
    },
    {
      "epoch": 1.7768555678016e-05,
      "grad_norm": 16210.071128776703,
      "learning_rate": 0.0001,
      "loss": 2.8488,
      "step": 4896
    },
    {
      "epoch": 1.7884690028852708e-05,
      "grad_norm": 15208.27445833353,
      "learning_rate": 0.0001,
      "loss": 2.7921,
      "step": 4928
    },
    {
      "epoch": 1.8000824379689415e-05,
      "grad_norm": 15372.000439110065,
      "learning_rate": 0.0001,
      "loss": 2.7917,
      "step": 4960
    },
    {
      "epoch": 1.811695873052612e-05,
      "grad_norm": 12801.62220189301,
      "learning_rate": 0.0001,
      "loss": 2.7533,
      "step": 4992
    },
    {
      "epoch": 1.8233093081362825e-05,
      "grad_norm": 13743.575080742274,
      "learning_rate": 0.0001,
      "loss": 2.8254,
      "step": 5024
    },
    {
      "epoch": 1.834922743219953e-05,
      "grad_norm": 15653.647370501229,
      "learning_rate": 0.0001,
      "loss": 2.7967,
      "step": 5056
    },
    {
      "epoch": 1.8465361783036238e-05,
      "grad_norm": 9654.409044576472,
      "learning_rate": 0.0001,
      "loss": 2.7101,
      "step": 5088
    },
    {
      "epoch": 1.8581496133872945e-05,
      "grad_norm": 16286.731654939244,
      "learning_rate": 0.0001,
      "loss": 2.7445,
      "step": 5120
    },
    {
      "epoch": 1.869763048470965e-05,
      "grad_norm": 16090.246486614182,
      "learning_rate": 0.0001,
      "loss": 2.782,
      "step": 5152
    },
    {
      "epoch": 1.8813764835546355e-05,
      "grad_norm": 16758.283802346825,
      "learning_rate": 0.0001,
      "loss": 2.7765,
      "step": 5184
    },
    {
      "epoch": 1.892989918638306e-05,
      "grad_norm": 18782.056916110123,
      "learning_rate": 0.0001,
      "loss": 2.7318,
      "step": 5216
    },
    {
      "epoch": 1.9046033537219768e-05,
      "grad_norm": 15958.73619682962,
      "learning_rate": 0.0001,
      "loss": 2.739,
      "step": 5248
    },
    {
      "epoch": 1.9162167888056475e-05,
      "grad_norm": 10285.049343586057,
      "learning_rate": 0.0001,
      "loss": 2.704,
      "step": 5280
    },
    {
      "epoch": 1.9278302238893178e-05,
      "grad_norm": 14528.576874560014,
      "learning_rate": 0.0001,
      "loss": 2.7101,
      "step": 5312
    },
    {
      "epoch": 1.9394436589729885e-05,
      "grad_norm": 13850.896144293334,
      "learning_rate": 0.0001,
      "loss": 2.7152,
      "step": 5344
    },
    {
      "epoch": 1.951057094056659e-05,
      "grad_norm": 13609.22900093903,
      "learning_rate": 0.0001,
      "loss": 2.6749,
      "step": 5376
    },
    {
      "epoch": 1.9626705291403298e-05,
      "grad_norm": 13022.109861692921,
      "learning_rate": 0.0001,
      "loss": 2.7144,
      "step": 5408
    },
    {
      "epoch": 1.974283964224e-05,
      "grad_norm": 14125.93711050704,
      "learning_rate": 0.0001,
      "loss": 2.6897,
      "step": 5440
    },
    {
      "epoch": 1.9858973993076708e-05,
      "grad_norm": 15328.001304801615,
      "learning_rate": 0.0001,
      "loss": 2.758,
      "step": 5472
    },
    {
      "epoch": 1.9975108343913415e-05,
      "grad_norm": 20883.58771858897,
      "learning_rate": 0.0001,
      "loss": 2.6389,
      "step": 5504
    },
    {
      "epoch": 2.009124269475012e-05,
      "grad_norm": 36168.23822084786,
      "learning_rate": 0.0001,
      "loss": 2.5778,
      "step": 5536
    },
    {
      "epoch": 2.0207377045586825e-05,
      "grad_norm": 28756.544994140033,
      "learning_rate": 0.0001,
      "loss": 2.6195,
      "step": 5568
    },
    {
      "epoch": 2.032351139642353e-05,
      "grad_norm": 24967.321522341957,
      "learning_rate": 0.0001,
      "loss": 2.6376,
      "step": 5600
    },
    {
      "epoch": 2.0439645747260238e-05,
      "grad_norm": 24962.49010014826,
      "learning_rate": 0.0001,
      "loss": 2.6018,
      "step": 5632
    },
    {
      "epoch": 2.0555780098096945e-05,
      "grad_norm": 19619.67176076093,
      "learning_rate": 0.0001,
      "loss": 2.5701,
      "step": 5664
    },
    {
      "epoch": 2.067191444893365e-05,
      "grad_norm": 28947.78526934315,
      "learning_rate": 0.0001,
      "loss": 2.6023,
      "step": 5696
    },
    {
      "epoch": 2.0788048799770355e-05,
      "grad_norm": 15386.200960601029,
      "learning_rate": 0.0001,
      "loss": 2.5938,
      "step": 5728
    },
    {
      "epoch": 2.090418315060706e-05,
      "grad_norm": 16819.837157356786,
      "learning_rate": 0.0001,
      "loss": 2.623,
      "step": 5760
    },
    {
      "epoch": 2.1020317501443768e-05,
      "grad_norm": 14677.377422414402,
      "learning_rate": 0.0001,
      "loss": 2.5902,
      "step": 5792
    },
    {
      "epoch": 2.1136451852280475e-05,
      "grad_norm": 8028.8776301547905,
      "learning_rate": 0.0001,
      "loss": 2.5836,
      "step": 5824
    },
    {
      "epoch": 2.1252586203117178e-05,
      "grad_norm": 12440.009967841666,
      "learning_rate": 0.0001,
      "loss": 2.6106,
      "step": 5856
    },
    {
      "epoch": 2.1368720553953885e-05,
      "grad_norm": 9361.396744076175,
      "learning_rate": 0.0001,
      "loss": 2.5657,
      "step": 5888
    },
    {
      "epoch": 2.148485490479059e-05,
      "grad_norm": 11840.09720188141,
      "learning_rate": 0.0001,
      "loss": 2.5423,
      "step": 5920
    },
    {
      "epoch": 2.1600989255627298e-05,
      "grad_norm": 17109.72881141019,
      "learning_rate": 0.0001,
      "loss": 2.5264,
      "step": 5952
    },
    {
      "epoch": 2.1717123606464e-05,
      "grad_norm": 13527.633717690615,
      "learning_rate": 0.0001,
      "loss": 2.5181,
      "step": 5984
    },
    {
      "epoch": 2.1833257957300708e-05,
      "grad_norm": 14498.226098388726,
      "learning_rate": 0.0001,
      "loss": 2.5279,
      "step": 6016
    },
    {
      "epoch": 2.1949392308137415e-05,
      "grad_norm": 17539.361105809985,
      "learning_rate": 0.0001,
      "loss": 2.6562,
      "step": 6048
    },
    {
      "epoch": 2.206552665897412e-05,
      "grad_norm": 18467.02275679542,
      "learning_rate": 0.0001,
      "loss": 2.4887,
      "step": 6080
    },
    {
      "epoch": 2.2181661009810828e-05,
      "grad_norm": 19486.593776234982,
      "learning_rate": 0.0001,
      "loss": 2.4612,
      "step": 6112
    },
    {
      "epoch": 2.229779536064753e-05,
      "grad_norm": 8924.183709169147,
      "learning_rate": 0.0001,
      "loss": 2.4452,
      "step": 6144
    },
    {
      "epoch": 2.2413929711484238e-05,
      "grad_norm": 8827.689675107526,
      "learning_rate": 0.0001,
      "loss": 2.458,
      "step": 6176
    },
    {
      "epoch": 2.2530064062320945e-05,
      "grad_norm": 8147.702191415688,
      "learning_rate": 0.0001,
      "loss": 2.4801,
      "step": 6208
    },
    {
      "epoch": 2.264619841315765e-05,
      "grad_norm": 10896.875699024928,
      "learning_rate": 0.0001,
      "loss": 2.4671,
      "step": 6240
    },
    {
      "epoch": 2.2762332763994355e-05,
      "grad_norm": 9675.749996770277,
      "learning_rate": 0.0001,
      "loss": 2.4609,
      "step": 6272
    },
    {
      "epoch": 2.287846711483106e-05,
      "grad_norm": 18169.082970805102,
      "learning_rate": 0.0001,
      "loss": 2.5135,
      "step": 6304
    },
    {
      "epoch": 2.299460146566777e-05,
      "grad_norm": 16604.1614061054,
      "learning_rate": 0.0001,
      "loss": 2.4967,
      "step": 6336
    },
    {
      "epoch": 2.3110735816504475e-05,
      "grad_norm": 14649.379099470394,
      "learning_rate": 0.0001,
      "loss": 2.4842,
      "step": 6368
    },
    {
      "epoch": 2.322687016734118e-05,
      "grad_norm": 9441.90928785063,
      "learning_rate": 0.0001,
      "loss": 2.421,
      "step": 6400
    },
    {
      "epoch": 2.3343004518177885e-05,
      "grad_norm": 13269.950828846353,
      "learning_rate": 0.0001,
      "loss": 2.4117,
      "step": 6432
    },
    {
      "epoch": 2.345913886901459e-05,
      "grad_norm": 9358.628532001898,
      "learning_rate": 0.0001,
      "loss": 2.4475,
      "step": 6464
    },
    {
      "epoch": 2.35752732198513e-05,
      "grad_norm": 9726.558178513096,
      "learning_rate": 0.0001,
      "loss": 2.4237,
      "step": 6496
    },
    {
      "epoch": 2.3691407570688005e-05,
      "grad_norm": 13905.706706241146,
      "learning_rate": 0.0001,
      "loss": 2.3553,
      "step": 6528
    },
    {
      "epoch": 2.380754192152471e-05,
      "grad_norm": 15080.404934881557,
      "learning_rate": 0.0001,
      "loss": 2.3781,
      "step": 6560
    },
    {
      "epoch": 2.3923676272361415e-05,
      "grad_norm": 8294.456401717956,
      "learning_rate": 0.0001,
      "loss": 2.3736,
      "step": 6592
    },
    {
      "epoch": 2.4039810623198122e-05,
      "grad_norm": 7958.516319013237,
      "learning_rate": 0.0001,
      "loss": 2.3908,
      "step": 6624
    },
    {
      "epoch": 2.415594497403483e-05,
      "grad_norm": 14199.886214332844,
      "learning_rate": 0.0001,
      "loss": 2.4256,
      "step": 6656
    },
    {
      "epoch": 2.4272079324871532e-05,
      "grad_norm": 16220.851765551648,
      "learning_rate": 0.0001,
      "loss": 2.3841,
      "step": 6688
    },
    {
      "epoch": 2.438821367570824e-05,
      "grad_norm": 23313.716037989314,
      "learning_rate": 0.0001,
      "loss": 2.3728,
      "step": 6720
    },
    {
      "epoch": 2.4504348026544945e-05,
      "grad_norm": 16561.34982421421,
      "learning_rate": 0.0001,
      "loss": 2.3761,
      "step": 6752
    },
    {
      "epoch": 2.4620482377381652e-05,
      "grad_norm": 15540.790166526283,
      "learning_rate": 0.0001,
      "loss": 2.3652,
      "step": 6784
    },
    {
      "epoch": 2.4736616728218355e-05,
      "grad_norm": 10463.79887994795,
      "learning_rate": 0.0001,
      "loss": 2.3655,
      "step": 6816
    },
    {
      "epoch": 2.4852751079055062e-05,
      "grad_norm": 13617.547760151238,
      "learning_rate": 0.0001,
      "loss": 2.3176,
      "step": 6848
    },
    {
      "epoch": 2.496888542989177e-05,
      "grad_norm": 10771.971755440134,
      "learning_rate": 0.0001,
      "loss": 2.3197,
      "step": 6880
    },
    {
      "epoch": 2.5085019780728475e-05,
      "grad_norm": 10909.340768350761,
      "learning_rate": 0.0001,
      "loss": 2.4162,
      "step": 6912
    },
    {
      "epoch": 2.520115413156518e-05,
      "grad_norm": 9485.776338813814,
      "learning_rate": 0.0001,
      "loss": 2.35,
      "step": 6944
    },
    {
      "epoch": 2.5317288482401885e-05,
      "grad_norm": 15615.700880844253,
      "learning_rate": 0.0001,
      "loss": 2.3233,
      "step": 6976
    },
    {
      "epoch": 2.5433422833238592e-05,
      "grad_norm": 9280.945722284987,
      "learning_rate": 0.0001,
      "loss": 2.2792,
      "step": 7008
    },
    {
      "epoch": 2.55495571840753e-05,
      "grad_norm": 8999.512542354725,
      "learning_rate": 0.0001,
      "loss": 2.286,
      "step": 7040
    },
    {
      "epoch": 2.5665691534912005e-05,
      "grad_norm": 12004.975301932112,
      "learning_rate": 0.0001,
      "loss": 2.3358,
      "step": 7072
    },
    {
      "epoch": 2.578182588574871e-05,
      "grad_norm": 8446.11603046039,
      "learning_rate": 0.0001,
      "loss": 2.2862,
      "step": 7104
    },
    {
      "epoch": 2.5897960236585415e-05,
      "grad_norm": 8882.275243427215,
      "learning_rate": 0.0001,
      "loss": 2.2808,
      "step": 7136
    },
    {
      "epoch": 2.6014094587422122e-05,
      "grad_norm": 15635.375659062369,
      "learning_rate": 0.0001,
      "loss": 2.3125,
      "step": 7168
    },
    {
      "epoch": 2.613022893825883e-05,
      "grad_norm": 16112.291333016543,
      "learning_rate": 0.0001,
      "loss": 2.3014,
      "step": 7200
    },
    {
      "epoch": 2.6246363289095532e-05,
      "grad_norm": 7468.061696317191,
      "learning_rate": 0.0001,
      "loss": 2.2694,
      "step": 7232
    },
    {
      "epoch": 2.636249763993224e-05,
      "grad_norm": 16851.66383476718,
      "learning_rate": 0.0001,
      "loss": 2.2969,
      "step": 7264
    },
    {
      "epoch": 2.6478631990768945e-05,
      "grad_norm": 13156.936649539663,
      "learning_rate": 0.0001,
      "loss": 2.2344,
      "step": 7296
    },
    {
      "epoch": 2.6594766341605652e-05,
      "grad_norm": 10638.894232954852,
      "learning_rate": 0.0001,
      "loss": 2.2271,
      "step": 7328
    },
    {
      "epoch": 2.6710900692442355e-05,
      "grad_norm": 9921.911055839999,
      "learning_rate": 0.0001,
      "loss": 2.236,
      "step": 7360
    },
    {
      "epoch": 2.6827035043279062e-05,
      "grad_norm": 5554.782624009692,
      "learning_rate": 0.0001,
      "loss": 2.2672,
      "step": 7392
    },
    {
      "epoch": 2.694316939411577e-05,
      "grad_norm": 8417.736988050885,
      "learning_rate": 0.0001,
      "loss": 2.2132,
      "step": 7424
    },
    {
      "epoch": 2.7059303744952475e-05,
      "grad_norm": 14542.259900029294,
      "learning_rate": 0.0001,
      "loss": 2.2076,
      "step": 7456
    },
    {
      "epoch": 2.7175438095789182e-05,
      "grad_norm": 9094.134758183429,
      "learning_rate": 0.0001,
      "loss": 2.2154,
      "step": 7488
    },
    {
      "epoch": 2.7291572446625885e-05,
      "grad_norm": 8125.027138416216,
      "learning_rate": 0.0001,
      "loss": 2.2301,
      "step": 7520
    },
    {
      "epoch": 2.7407706797462592e-05,
      "grad_norm": 8850.744488459713,
      "learning_rate": 0.0001,
      "loss": 2.2534,
      "step": 7552
    },
    {
      "epoch": 2.75238411482993e-05,
      "grad_norm": 9008.208395680022,
      "learning_rate": 0.0001,
      "loss": 2.2496,
      "step": 7584
    },
    {
      "epoch": 2.7639975499136005e-05,
      "grad_norm": 10577.522772369719,
      "learning_rate": 0.0001,
      "loss": 2.2281,
      "step": 7616
    },
    {
      "epoch": 2.775610984997271e-05,
      "grad_norm": 9065.412001117213,
      "learning_rate": 0.0001,
      "loss": 2.2434,
      "step": 7648
    },
    {
      "epoch": 2.7872244200809415e-05,
      "grad_norm": 7972.46555339062,
      "learning_rate": 0.0001,
      "loss": 2.1984,
      "step": 7680
    },
    {
      "epoch": 2.7988378551646122e-05,
      "grad_norm": 12220.363333387433,
      "learning_rate": 0.0001,
      "loss": 2.1947,
      "step": 7712
    },
    {
      "epoch": 2.810451290248283e-05,
      "grad_norm": 8995.745313758054,
      "learning_rate": 0.0001,
      "loss": 2.186,
      "step": 7744
    },
    {
      "epoch": 2.8220647253319532e-05,
      "grad_norm": 20462.785538630855,
      "learning_rate": 0.0001,
      "loss": 2.1525,
      "step": 7776
    },
    {
      "epoch": 2.833678160415624e-05,
      "grad_norm": 3465.70768386487,
      "learning_rate": 0.0001,
      "loss": 2.1905,
      "step": 7808
    },
    {
      "epoch": 2.8452915954992945e-05,
      "grad_norm": 4099.091042536138,
      "learning_rate": 0.0001,
      "loss": 2.1887,
      "step": 7840
    },
    {
      "epoch": 2.8569050305829652e-05,
      "grad_norm": 6118.072127925593,
      "learning_rate": 0.0001,
      "loss": 2.1552,
      "step": 7872
    },
    {
      "epoch": 2.868518465666636e-05,
      "grad_norm": 5525.807497551828,
      "learning_rate": 0.0001,
      "loss": 2.1347,
      "step": 7904
    },
    {
      "epoch": 2.8801319007503062e-05,
      "grad_norm": 4024.7498524753064,
      "learning_rate": 0.0001,
      "loss": 2.142,
      "step": 7936
    },
    {
      "epoch": 2.891745335833977e-05,
      "grad_norm": 6497.287376282505,
      "learning_rate": 0.0001,
      "loss": 2.1351,
      "step": 7968
    },
    {
      "epoch": 2.9033587709176475e-05,
      "grad_norm": 4541.016588413876,
      "learning_rate": 0.0001,
      "loss": 2.1781,
      "step": 8000
    },
    {
      "epoch": 2.9149722060013182e-05,
      "grad_norm": 4106.9287186412175,
      "learning_rate": 0.0001,
      "loss": 2.1514,
      "step": 8032
    },
    {
      "epoch": 2.9265856410849885e-05,
      "grad_norm": 6898.322078592736,
      "learning_rate": 0.0001,
      "loss": 2.1373,
      "step": 8064
    },
    {
      "epoch": 2.9381990761686592e-05,
      "grad_norm": 3538.59166901184,
      "learning_rate": 0.0001,
      "loss": 2.1532,
      "step": 8096
    },
    {
      "epoch": 2.94981251125233e-05,
      "grad_norm": 4148.488821245635,
      "learning_rate": 0.0001,
      "loss": 2.1128,
      "step": 8128
    },
    {
      "epoch": 2.9614259463360005e-05,
      "grad_norm": 4816.19460777905,
      "learning_rate": 0.0001,
      "loss": 2.1709,
      "step": 8160
    },
    {
      "epoch": 2.973039381419671e-05,
      "grad_norm": 6746.929894403825,
      "learning_rate": 0.0001,
      "loss": 2.1399,
      "step": 8192
    },
    {
      "epoch": 2.9846528165033415e-05,
      "grad_norm": 5197.007865108538,
      "learning_rate": 0.0001,
      "loss": 2.1039,
      "step": 8224
    },
    {
      "epoch": 2.9962662515870122e-05,
      "grad_norm": 4412.275150078472,
      "learning_rate": 0.0001,
      "loss": 2.1142,
      "step": 8256
    },
    {
      "epoch": 3.007879686670683e-05,
      "grad_norm": 5089.480769194438,
      "learning_rate": 0.0001,
      "loss": 2.098,
      "step": 8288
    },
    {
      "epoch": 3.0194931217543532e-05,
      "grad_norm": 6679.571075301168,
      "learning_rate": 0.0001,
      "loss": 2.0827,
      "step": 8320
    },
    {
      "epoch": 3.031106556838024e-05,
      "grad_norm": 2953.162499761908,
      "learning_rate": 0.0001,
      "loss": 2.0654,
      "step": 8352
    },
    {
      "epoch": 3.0427199919216946e-05,
      "grad_norm": 3350.802739643144,
      "learning_rate": 0.0001,
      "loss": 2.065,
      "step": 8384
    },
    {
      "epoch": 3.054333427005365e-05,
      "grad_norm": 7150.920989634832,
      "learning_rate": 0.0001,
      "loss": 2.101,
      "step": 8416
    },
    {
      "epoch": 3.0659468620890356e-05,
      "grad_norm": 6751.544906538058,
      "learning_rate": 0.0001,
      "loss": 2.0943,
      "step": 8448
    },
    {
      "epoch": 3.077560297172706e-05,
      "grad_norm": 4109.740867743367,
      "learning_rate": 0.0001,
      "loss": 2.087,
      "step": 8480
    },
    {
      "epoch": 3.089173732256377e-05,
      "grad_norm": 5390.4381605301805,
      "learning_rate": 0.0001,
      "loss": 2.0832,
      "step": 8512
    },
    {
      "epoch": 3.1007871673400476e-05,
      "grad_norm": 3120.081168815965,
      "learning_rate": 0.0001,
      "loss": 2.0723,
      "step": 8544
    },
    {
      "epoch": 3.112400602423718e-05,
      "grad_norm": 3607.5452180118273,
      "learning_rate": 0.0001,
      "loss": 2.0624,
      "step": 8576
    },
    {
      "epoch": 3.124014037507389e-05,
      "grad_norm": 6521.729026876233,
      "learning_rate": 0.0001,
      "loss": 2.137,
      "step": 8608
    },
    {
      "epoch": 3.1356274725910596e-05,
      "grad_norm": 3980.838950522867,
      "learning_rate": 0.0001,
      "loss": 2.0233,
      "step": 8640
    },
    {
      "epoch": 3.1472409076747296e-05,
      "grad_norm": 3767.3602827444047,
      "learning_rate": 0.0001,
      "loss": 2.0331,
      "step": 8672
    },
    {
      "epoch": 3.1588543427584e-05,
      "grad_norm": 4976.906418650044,
      "learning_rate": 0.0001,
      "loss": 2.0633,
      "step": 8704
    },
    {
      "epoch": 3.170467777842071e-05,
      "grad_norm": 5064.9089207013385,
      "learning_rate": 0.0001,
      "loss": 2.037,
      "step": 8736
    },
    {
      "epoch": 3.1820812129257416e-05,
      "grad_norm": 3535.76165769131,
      "learning_rate": 0.0001,
      "loss": 2.0191,
      "step": 8768
    },
    {
      "epoch": 3.193694648009412e-05,
      "grad_norm": 8398.897963423535,
      "learning_rate": 0.0001,
      "loss": 2.0463,
      "step": 8800
    },
    {
      "epoch": 3.205308083093083e-05,
      "grad_norm": 8621.803494629183,
      "learning_rate": 0.0001,
      "loss": 2.035,
      "step": 8832
    },
    {
      "epoch": 3.2169215181767536e-05,
      "grad_norm": 10823.170838529715,
      "learning_rate": 0.0001,
      "loss": 2.0283,
      "step": 8864
    },
    {
      "epoch": 3.228534953260424e-05,
      "grad_norm": 5418.9871055391895,
      "learning_rate": 0.0001,
      "loss": 2.0488,
      "step": 8896
    },
    {
      "epoch": 3.240148388344095e-05,
      "grad_norm": 5566.006804702991,
      "learning_rate": 0.0001,
      "loss": 2.0316,
      "step": 8928
    },
    {
      "epoch": 3.251761823427765e-05,
      "grad_norm": 10668.832340045465,
      "learning_rate": 0.0001,
      "loss": 2.0054,
      "step": 8960
    },
    {
      "epoch": 3.2633752585114356e-05,
      "grad_norm": 5487.171675827174,
      "learning_rate": 0.0001,
      "loss": 2.0139,
      "step": 8992
    },
    {
      "epoch": 3.274988693595106e-05,
      "grad_norm": 9551.782137381484,
      "learning_rate": 0.0001,
      "loss": 2.0085,
      "step": 9024
    },
    {
      "epoch": 3.286602128678777e-05,
      "grad_norm": 5860.839999522253,
      "learning_rate": 0.0001,
      "loss": 2.0109,
      "step": 9056
    },
    {
      "epoch": 3.2982155637624476e-05,
      "grad_norm": 12797.789809181897,
      "learning_rate": 0.0001,
      "loss": 1.9805,
      "step": 9088
    },
    {
      "epoch": 3.309828998846118e-05,
      "grad_norm": 8774.981153825916,
      "learning_rate": 0.0001,
      "loss": 1.9852,
      "step": 9120
    },
    {
      "epoch": 3.321442433929789e-05,
      "grad_norm": 7168.795191662265,
      "learning_rate": 0.0001,
      "loss": 2.002,
      "step": 9152
    },
    {
      "epoch": 3.3330558690134596e-05,
      "grad_norm": 14469.226810717979,
      "learning_rate": 0.0001,
      "loss": 1.9887,
      "step": 9184
    },
    {
      "epoch": 3.3446693040971296e-05,
      "grad_norm": 7456.30746415409,
      "learning_rate": 0.0001,
      "loss": 1.9512,
      "step": 9216
    },
    {
      "epoch": 3.3562827391808e-05,
      "grad_norm": 5534.825245660427,
      "learning_rate": 0.0001,
      "loss": 1.9451,
      "step": 9248
    },
    {
      "epoch": 3.367896174264471e-05,
      "grad_norm": 5095.24101490793,
      "learning_rate": 0.0001,
      "loss": 1.958,
      "step": 9280
    },
    {
      "epoch": 3.3795096093481416e-05,
      "grad_norm": 10993.327635434141,
      "learning_rate": 0.0001,
      "loss": 1.9985,
      "step": 9312
    },
    {
      "epoch": 3.391123044431812e-05,
      "grad_norm": 10343.740909361564,
      "learning_rate": 0.0001,
      "loss": 1.9928,
      "step": 9344
    },
    {
      "epoch": 3.402736479515483e-05,
      "grad_norm": 8252.799873376307,
      "learning_rate": 0.0001,
      "loss": 1.9843,
      "step": 9376
    },
    {
      "epoch": 3.4143499145991536e-05,
      "grad_norm": 6626.456519136001,
      "learning_rate": 0.0001,
      "loss": 2.0028,
      "step": 9408
    },
    {
      "epoch": 3.425963349682824e-05,
      "grad_norm": 4892.352195008041,
      "learning_rate": 0.0001,
      "loss": 1.989,
      "step": 9440
    },
    {
      "epoch": 3.437576784766495e-05,
      "grad_norm": 11357.49318731911,
      "learning_rate": 0.0001,
      "loss": 1.9949,
      "step": 9472
    },
    {
      "epoch": 3.449190219850165e-05,
      "grad_norm": 6459.340107549687,
      "learning_rate": 0.0001,
      "loss": 1.9793,
      "step": 9504
    },
    {
      "epoch": 3.4608036549338356e-05,
      "grad_norm": 5264.228219786828,
      "learning_rate": 0.0001,
      "loss": 1.9243,
      "step": 9536
    },
    {
      "epoch": 3.472417090017506e-05,
      "grad_norm": 9461.105960721505,
      "learning_rate": 0.0001,
      "loss": 1.9282,
      "step": 9568
    },
    {
      "epoch": 3.484030525101177e-05,
      "grad_norm": 7046.354163679257,
      "learning_rate": 0.0001,
      "loss": 1.9469,
      "step": 9600
    },
    {
      "epoch": 3.4956439601848476e-05,
      "grad_norm": 8711.152435240701,
      "learning_rate": 0.0001,
      "loss": 1.9176,
      "step": 9632
    },
    {
      "epoch": 3.507257395268518e-05,
      "grad_norm": 6237.586592585309,
      "learning_rate": 0.0001,
      "loss": 1.8922,
      "step": 9664
    },
    {
      "epoch": 3.518870830352189e-05,
      "grad_norm": 7333.434393243046,
      "learning_rate": 0.0001,
      "loss": 1.9049,
      "step": 9696
    },
    {
      "epoch": 3.5304842654358596e-05,
      "grad_norm": 5894.741385336595,
      "learning_rate": 0.0001,
      "loss": 1.9283,
      "step": 9728
    },
    {
      "epoch": 3.54209770051953e-05,
      "grad_norm": 8139.839679010884,
      "learning_rate": 0.0001,
      "loss": 1.953,
      "step": 9760
    },
    {
      "epoch": 3.5537111356032e-05,
      "grad_norm": 13547.099099069144,
      "learning_rate": 0.0001,
      "loss": 1.9172,
      "step": 9792
    },
    {
      "epoch": 3.565324570686871e-05,
      "grad_norm": 11891.228195607046,
      "learning_rate": 0.0001,
      "loss": 1.8989,
      "step": 9824
    },
    {
      "epoch": 3.5769380057705416e-05,
      "grad_norm": 11841.149944156608,
      "learning_rate": 0.0001,
      "loss": 1.914,
      "step": 9856
    },
    {
      "epoch": 3.588551440854212e-05,
      "grad_norm": 9462.854537611789,
      "learning_rate": 0.0001,
      "loss": 1.905,
      "step": 9888
    },
    {
      "epoch": 3.600164875937883e-05,
      "grad_norm": 12326.463401965708,
      "learning_rate": 0.0001,
      "loss": 1.9209,
      "step": 9920
    },
    {
      "epoch": 3.6117783110215536e-05,
      "grad_norm": 20852.57144814519,
      "learning_rate": 0.0001,
      "loss": 1.9202,
      "step": 9952
    },
    {
      "epoch": 3.623391746105224e-05,
      "grad_norm": 15668.550571128142,
      "learning_rate": 0.0001,
      "loss": 1.8957,
      "step": 9984
    },
    {
      "epoch": 3.635005181188895e-05,
      "grad_norm": 12767.800828647038,
      "learning_rate": 9.99800059980007e-07,
      "loss": 1.9118,
      "step": 10016
    },
    {
      "epoch": 3.646618616272565e-05,
      "grad_norm": 8591.33272548561,
      "learning_rate": 9.982048454657787e-07,
      "loss": 1.903,
      "step": 10048
    },
    {
      "epoch": 3.6582320513562356e-05,
      "grad_norm": 5879.11591993218,
      "learning_rate": 9.966172423210911e-07,
      "loss": 1.844,
      "step": 10080
    },
    {
      "epoch": 3.669845486439906e-05,
      "grad_norm": 5426.173237190275,
      "learning_rate": 9.950371902099892e-07,
      "loss": 1.8407,
      "step": 10112
    },
    {
      "epoch": 3.681458921523577e-05,
      "grad_norm": 5429.302625568039,
      "learning_rate": 9.934646294640047e-07,
      "loss": 1.8543,
      "step": 10144
    },
    {
      "epoch": 3.6930723566072476e-05,
      "grad_norm": 4471.081692387202,
      "learning_rate": 9.918995010726928e-07,
      "loss": 1.8578,
      "step": 10176
    },
    {
      "epoch": 3.704685791690918e-05,
      "grad_norm": 4939.131401370083,
      "learning_rate": 9.903417466743302e-07,
      "loss": 1.8649,
      "step": 10208
    },
    {
      "epoch": 3.716299226774589e-05,
      "grad_norm": 4566.186264269122,
      "learning_rate": 9.88791308546776e-07,
      "loss": 1.8767,
      "step": 10240
    },
    {
      "epoch": 3.7279126618582596e-05,
      "grad_norm": 4359.418195126501,
      "learning_rate": 9.872481295984873e-07,
      "loss": 1.8524,
      "step": 10272
    },
    {
      "epoch": 3.73952609694193e-05,
      "grad_norm": 5109.624643748306,
      "learning_rate": 9.85712153359689e-07,
      "loss": 1.8732,
      "step": 10304
    },
    {
      "epoch": 3.7511395320256e-05,
      "grad_norm": 5342.124343367534,
      "learning_rate": 9.841833239736953e-07,
      "loss": 1.8943,
      "step": 10336
    },
    {
      "epoch": 3.762752967109271e-05,
      "grad_norm": 5553.342416959357,
      "learning_rate": 9.82661586188378e-07,
      "loss": 1.8693,
      "step": 10368
    },
    {
      "epoch": 3.7743664021929416e-05,
      "grad_norm": 5692.346704128272,
      "learning_rate": 9.811468853477788e-07,
      "loss": 1.8477,
      "step": 10400
    },
    {
      "epoch": 3.785979837276612e-05,
      "grad_norm": 4949.559980442706,
      "learning_rate": 9.796391673838652e-07,
      "loss": 1.8173,
      "step": 10432
    },
    {
      "epoch": 3.797593272360283e-05,
      "grad_norm": 4947.146702898551,
      "learning_rate": 9.781383788084238e-07,
      "loss": 1.8383,
      "step": 10464
    },
    {
      "epoch": 3.8092067074439536e-05,
      "grad_norm": 4645.385236985196,
      "learning_rate": 9.7664446670509e-07,
      "loss": 1.8722,
      "step": 10496
    },
    {
      "epoch": 3.820820142527624e-05,
      "grad_norm": 5407.556102344201,
      "learning_rate": 9.751573787215115e-07,
      "loss": 1.8371,
      "step": 10528
    },
    {
      "epoch": 3.832433577611295e-05,
      "grad_norm": 5508.991286978044,
      "learning_rate": 9.736770630616433e-07,
      "loss": 1.8478,
      "step": 10560
    },
    {
      "epoch": 3.844047012694965e-05,
      "grad_norm": 5839.9708047215445,
      "learning_rate": 9.722034684781694e-07,
      "loss": 1.8643,
      "step": 10592
    },
    {
      "epoch": 3.8556604477786356e-05,
      "grad_norm": 5747.7397296676545,
      "learning_rate": 9.707365442650517e-07,
      "loss": 1.8792,
      "step": 10624
    },
    {
      "epoch": 3.867273882862306e-05,
      "grad_norm": 4278.391344886533,
      "learning_rate": 9.692762402502016e-07,
      "loss": 1.8666,
      "step": 10656
    },
    {
      "epoch": 3.878887317945977e-05,
      "grad_norm": 6041.881660542517,
      "learning_rate": 9.678225067882721e-07,
      "loss": 1.8605,
      "step": 10688
    },
    {
      "epoch": 3.8905007530296476e-05,
      "grad_norm": 4696.7383895635485,
      "learning_rate": 9.663752947535696e-07,
      "loss": 1.8468,
      "step": 10720
    },
    {
      "epoch": 3.902114188113318e-05,
      "grad_norm": 5702.050201462628,
      "learning_rate": 9.649345555330812e-07,
      "loss": 1.8642,
      "step": 10752
    },
    {
      "epoch": 3.913727623196989e-05,
      "grad_norm": 10613.930468963888,
      "learning_rate": 9.635002410196155e-07,
      "loss": 1.8508,
      "step": 10784
    },
    {
      "epoch": 3.9253410582806596e-05,
      "grad_norm": 10906.772666559067,
      "learning_rate": 9.620723036050563e-07,
      "loss": 1.8499,
      "step": 10816
    },
    {
      "epoch": 3.93695449336433e-05,
      "grad_norm": 9533.134217034814,
      "learning_rate": 9.60650696173725e-07,
      "loss": 1.8496,
      "step": 10848
    },
    {
      "epoch": 3.948567928448e-05,
      "grad_norm": 9724.994807196557,
      "learning_rate": 9.59235372095852e-07,
      "loss": 1.8233,
      "step": 10880
    },
    {
      "epoch": 3.960181363531671e-05,
      "grad_norm": 11131.417340123406,
      "learning_rate": 9.578262852211515e-07,
      "loss": 1.8475,
      "step": 10912
    },
    {
      "epoch": 3.9717947986153416e-05,
      "grad_norm": 9726.443954498478,
      "learning_rate": 9.564233898725014e-07,
      "loss": 1.8898,
      "step": 10944
    },
    {
      "epoch": 3.983408233699012e-05,
      "grad_norm": 10848.796430941084,
      "learning_rate": 9.550266408397246e-07,
      "loss": 1.841,
      "step": 10976
    },
    {
      "epoch": 3.995021668782683e-05,
      "grad_norm": 8704.614638224946,
      "learning_rate": 9.536359933734689e-07,
      "loss": 1.825,
      "step": 11008
    },
    {
      "epoch": 4.0066351038663536e-05,
      "grad_norm": 11354.616506073642,
      "learning_rate": 9.522514031791859e-07,
      "loss": 1.8374,
      "step": 11040
    },
    {
      "epoch": 4.018248538950024e-05,
      "grad_norm": 10805.221052805908,
      "learning_rate": 9.508728264112049e-07,
      "loss": 1.864,
      "step": 11072
    },
    {
      "epoch": 4.029861974033695e-05,
      "grad_norm": 9809.475215321154,
      "learning_rate": 9.495002196669013e-07,
      "loss": 1.8677,
      "step": 11104
    },
    {
      "epoch": 4.041475409117365e-05,
      "grad_norm": 10952.251549339067,
      "learning_rate": 9.481335399809572e-07,
      "loss": 1.8795,
      "step": 11136
    },
    {
      "epoch": 4.0530888442010356e-05,
      "grad_norm": 10235.984368882164,
      "learning_rate": 9.467727448197129e-07,
      "loss": 1.8802,
      "step": 11168
    },
    {
      "epoch": 4.064702279284706e-05,
      "grad_norm": 12907.898976983048,
      "learning_rate": 9.454177920756062e-07,
      "loss": 1.8878,
      "step": 11200
    },
    {
      "epoch": 4.076315714368377e-05,
      "grad_norm": 10790.377194519198,
      "learning_rate": 9.440686400617013e-07,
      "loss": 1.8543,
      "step": 11232
    },
    {
      "epoch": 4.0879291494520476e-05,
      "grad_norm": 12416.475828511082,
      "learning_rate": 9.427252475063007e-07,
      "loss": 1.8699,
      "step": 11264
    },
    {
      "epoch": 4.099542584535718e-05,
      "grad_norm": 8186.713015612554,
      "learning_rate": 9.413875735476427e-07,
      "loss": 1.8331,
      "step": 11296
    },
    {
      "epoch": 4.111156019619389e-05,
      "grad_norm": 11959.511026793696,
      "learning_rate": 9.400555777286816e-07,
      "loss": 1.8138,
      "step": 11328
    },
    {
      "epoch": 4.1227694547030596e-05,
      "grad_norm": 10862.981174613164,
      "learning_rate": 9.387292199919476e-07,
      "loss": 1.8518,
      "step": 11360
    },
    {
      "epoch": 4.13438288978673e-05,
      "grad_norm": 8790.45470951304,
      "learning_rate": 9.374084606744877e-07,
      "loss": 1.8502,
      "step": 11392
    },
    {
      "epoch": 4.1459963248704e-05,
      "grad_norm": 11112.632451404123,
      "learning_rate": 9.360932605028841e-07,
      "loss": 1.8204,
      "step": 11424
    },
    {
      "epoch": 4.157609759954071e-05,
      "grad_norm": 10016.66511369927,
      "learning_rate": 9.34783580588349e-07,
      "loss": 1.8267,
      "step": 11456
    },
    {
      "epoch": 4.1692231950377416e-05,
      "grad_norm": 9884.840312316634,
      "learning_rate": 9.334793824218948e-07,
      "loss": 1.8503,
      "step": 11488
    },
    {
      "epoch": 4.180836630121412e-05,
      "grad_norm": 9103.180652936642,
      "learning_rate": 9.321806278695785e-07,
      "loss": 1.8392,
      "step": 11520
    },
    {
      "epoch": 4.192450065205083e-05,
      "grad_norm": 8728.402602996724,
      "learning_rate": 9.308872791678188e-07,
      "loss": 1.8663,
      "step": 11552
    },
    {
      "epoch": 4.2040635002887536e-05,
      "grad_norm": 10994.572206320718,
      "learning_rate": 9.295992989187835e-07,
      "loss": 1.8632,
      "step": 11584
    },
    {
      "epoch": 4.215676935372424e-05,
      "grad_norm": 10237.175782411867,
      "learning_rate": 9.283166500858485e-07,
      "loss": 1.8473,
      "step": 11616
    },
    {
      "epoch": 4.227290370456095e-05,
      "grad_norm": 9769.24817987546,
      "learning_rate": 9.270392959891236e-07,
      "loss": 1.8459,
      "step": 11648
    },
    {
      "epoch": 4.2389038055397656e-05,
      "grad_norm": 11205.546394531593,
      "learning_rate": 9.257672003010474e-07,
      "loss": 1.8572,
      "step": 11680
    },
    {
      "epoch": 4.2505172406234356e-05,
      "grad_norm": 10854.247095031513,
      "learning_rate": 9.245003270420486e-07,
      "loss": 1.87,
      "step": 11712
    },
    {
      "epoch": 4.262130675707106e-05,
      "grad_norm": 9792.301465947625,
      "learning_rate": 9.232386405762704e-07,
      "loss": 1.8551,
      "step": 11744
    },
    {
      "epoch": 4.273744110790777e-05,
      "grad_norm": 9167.511003538528,
      "learning_rate": 9.219821056073614e-07,
      "loss": 1.8452,
      "step": 11776
    },
    {
      "epoch": 4.2853575458744476e-05,
      "grad_norm": 9576.716556315114,
      "learning_rate": 9.207697168976312e-07,
      "loss": 1.836,
      "step": 11808
    },
    {
      "epoch": 4.296970980958118e-05,
      "grad_norm": 9821.707947195335,
      "learning_rate": 9.195232220821039e-07,
      "loss": 1.874,
      "step": 11840
    },
    {
      "epoch": 4.308584416041789e-05,
      "grad_norm": 9465.380076890731,
      "learning_rate": 9.182817759372944e-07,
      "loss": 1.8436,
      "step": 11872
    },
    {
      "epoch": 4.3201978511254596e-05,
      "grad_norm": 8536.898968595095,
      "learning_rate": 9.170453444739675e-07,
      "loss": 1.8259,
      "step": 11904
    },
    {
      "epoch": 4.33181128620913e-05,
      "grad_norm": 10005.187254619475,
      "learning_rate": 9.158138940223838e-07,
      "loss": 1.8477,
      "step": 11936
    },
    {
      "epoch": 4.3434247212928e-05,
      "grad_norm": 10467.600775726976,
      "learning_rate": 9.145873912284495e-07,
      "loss": 1.8491,
      "step": 11968
    },
    {
      "epoch": 4.355038156376471e-05,
      "grad_norm": 8635.849350237648,
      "learning_rate": 9.133658030499223e-07,
      "loss": 1.8573,
      "step": 12000
    },
    {
      "epoch": 4.3666515914601416e-05,
      "grad_norm": 11022.999954640298,
      "learning_rate": 9.121490967526715e-07,
      "loss": 1.8669,
      "step": 12032
    },
    {
      "epoch": 4.378265026543812e-05,
      "grad_norm": 13902.190331023381,
      "learning_rate": 9.109372399069946e-07,
      "loss": 1.8693,
      "step": 12064
    },
    {
      "epoch": 4.389878461627483e-05,
      "grad_norm": 10048.817442863612,
      "learning_rate": 9.097302003839872e-07,
      "loss": 1.8379,
      "step": 12096
    },
    {
      "epoch": 4.4014918967111537e-05,
      "grad_norm": 10540.281495292238,
      "learning_rate": 9.085279463519642e-07,
      "loss": 1.8508,
      "step": 12128
    },
    {
      "epoch": 4.413105331794824e-05,
      "grad_norm": 12325.637833394263,
      "learning_rate": 9.073304462729352e-07,
      "loss": 1.8605,
      "step": 12160
    },
    {
      "epoch": 4.424718766878495e-05,
      "grad_norm": 9590.65659900301,
      "learning_rate": 9.061376688991289e-07,
      "loss": 1.8393,
      "step": 12192
    },
    {
      "epoch": 4.4363322019621657e-05,
      "grad_norm": 10535.104935405247,
      "learning_rate": 9.049495832695686e-07,
      "loss": 1.8237,
      "step": 12224
    },
    {
      "epoch": 4.4479456370458356e-05,
      "grad_norm": 9897.198997696267,
      "learning_rate": 9.037661587066969e-07,
      "loss": 1.8319,
      "step": 12256
    },
    {
      "epoch": 4.459559072129506e-05,
      "grad_norm": 12743.76616232423,
      "learning_rate": 9.025873648130485e-07,
      "loss": 1.8783,
      "step": 12288
    },
    {
      "epoch": 4.471172507213177e-05,
      "grad_norm": 9500.560194009615,
      "learning_rate": 9.01413171467971e-07,
      "loss": 1.8395,
      "step": 12320
    },
    {
      "epoch": 4.4827859422968477e-05,
      "grad_norm": 9775.840321936525,
      "learning_rate": 9.002435488243919e-07,
      "loss": 1.8486,
      "step": 12352
    },
    {
      "epoch": 4.494399377380518e-05,
      "grad_norm": 13272.169980828305,
      "learning_rate": 8.990784673056328e-07,
      "loss": 1.836,
      "step": 12384
    },
    {
      "epoch": 4.506012812464189e-05,
      "grad_norm": 9083.409271853823,
      "learning_rate": 8.979178976022672e-07,
      "loss": 1.8349,
      "step": 12416
    },
    {
      "epoch": 4.51762624754786e-05,
      "grad_norm": 10255.60042123327,
      "learning_rate": 8.967618106690242e-07,
      "loss": 1.8592,
      "step": 12448
    },
    {
      "epoch": 4.52923968263153e-05,
      "grad_norm": 8655.596224408808,
      "learning_rate": 8.956101777217353e-07,
      "loss": 1.8648,
      "step": 12480
    },
    {
      "epoch": 4.5408531177152e-05,
      "grad_norm": 11037.237516697736,
      "learning_rate": 8.944629702343244e-07,
      "loss": 1.8437,
      "step": 12512
    },
    {
      "epoch": 4.552466552798871e-05,
      "grad_norm": 11907.569357345772,
      "learning_rate": 8.933201599358393e-07,
      "loss": 1.8523,
      "step": 12544
    },
    {
      "epoch": 4.5640799878825417e-05,
      "grad_norm": 9795.927214919473,
      "learning_rate": 8.92181718807527e-07,
      "loss": 1.8342,
      "step": 12576
    },
    {
      "epoch": 4.575693422966212e-05,
      "grad_norm": 10163.15246367976,
      "learning_rate": 8.910476190799474e-07,
      "loss": 1.8638,
      "step": 12608
    },
    {
      "epoch": 4.587306858049883e-05,
      "grad_norm": 10120.788408024348,
      "learning_rate": 8.89917833230128e-07,
      "loss": 1.8319,
      "step": 12640
    },
    {
      "epoch": 4.598920293133554e-05,
      "grad_norm": 9238.27132097775,
      "learning_rate": 8.887923339787595e-07,
      "loss": 1.8218,
      "step": 12672
    },
    {
      "epoch": 4.610533728217224e-05,
      "grad_norm": 10092.7399649451,
      "learning_rate": 8.876710942874287e-07,
      "loss": 1.8437,
      "step": 12704
    },
    {
      "epoch": 4.622147163300895e-05,
      "grad_norm": 9822.811104770364,
      "learning_rate": 8.865540873558902e-07,
      "loss": 1.8597,
      "step": 12736
    },
    {
      "epoch": 4.633760598384566e-05,
      "grad_norm": 10649.300070896677,
      "learning_rate": 8.854412866193763e-07,
      "loss": 1.8351,
      "step": 12768
    },
    {
      "epoch": 4.645374033468236e-05,
      "grad_norm": 10707.643344826161,
      "learning_rate": 8.843672471386924e-07,
      "loss": 1.8422,
      "step": 12800
    },
    {
      "epoch": 4.656987468551906e-05,
      "grad_norm": 10933.47675718936,
      "learning_rate": 8.832626506143878e-07,
      "loss": 1.8385,
      "step": 12832
    },
    {
      "epoch": 4.668600903635577e-05,
      "grad_norm": 11129.902335600254,
      "learning_rate": 8.821621827824619e-07,
      "loss": 1.8515,
      "step": 12864
    },
    {
      "epoch": 4.680214338719248e-05,
      "grad_norm": 10370.75059964321,
      "learning_rate": 8.810658179868875e-07,
      "loss": 1.8939,
      "step": 12896
    },
    {
      "epoch": 4.691827773802918e-05,
      "grad_norm": 8360.058133769167,
      "learning_rate": 8.799735307942847e-07,
      "loss": 1.8831,
      "step": 12928
    },
    {
      "epoch": 4.703441208886589e-05,
      "grad_norm": 10362.950159100448,
      "learning_rate": 8.78885295991442e-07,
      "loss": 1.8513,
      "step": 12960
    },
    {
      "epoch": 4.71505464397026e-05,
      "grad_norm": 10459.178744050607,
      "learning_rate": 8.778010885828722e-07,
      "loss": 1.8448,
      "step": 12992
    },
    {
      "epoch": 4.7266680790539303e-05,
      "grad_norm": 10409.371738966767,
      "learning_rate": 8.76720883788401e-07,
      "loss": 1.8348,
      "step": 13024
    },
    {
      "epoch": 4.738281514137601e-05,
      "grad_norm": 11323.327779411844,
      "learning_rate": 8.75644657040788e-07,
      "loss": 1.8508,
      "step": 13056
    },
    {
      "epoch": 4.749894949221271e-05,
      "grad_norm": 9978.762047468614,
      "learning_rate": 8.745723839833802e-07,
      "loss": 1.8447,
      "step": 13088
    },
    {
      "epoch": 4.761508384304942e-05,
      "grad_norm": 9750.57772647344,
      "learning_rate": 8.735040404677968e-07,
      "loss": 1.8243,
      "step": 13120
    },
    {
      "epoch": 4.7731218193886123e-05,
      "grad_norm": 11339.081267898207,
      "learning_rate": 8.724396025516448e-07,
      "loss": 1.837,
      "step": 13152
    },
    {
      "epoch": 4.784735254472283e-05,
      "grad_norm": 9787.396282975365,
      "learning_rate": 8.713790464962657e-07,
      "loss": 1.8558,
      "step": 13184
    },
    {
      "epoch": 4.796348689555954e-05,
      "grad_norm": 10000.983651621473,
      "learning_rate": 8.703223487645115e-07,
      "loss": 1.8207,
      "step": 13216
    },
    {
      "epoch": 4.8079621246396243e-05,
      "grad_norm": 9318.210557827077,
      "learning_rate": 8.692694860185511e-07,
      "loss": 1.8101,
      "step": 13248
    },
    {
      "epoch": 4.819575559723295e-05,
      "grad_norm": 9185.179257913262,
      "learning_rate": 8.682204351177055e-07,
      "loss": 1.827,
      "step": 13280
    },
    {
      "epoch": 4.831188994806966e-05,
      "grad_norm": 12183.286092019673,
      "learning_rate": 8.671751731163108e-07,
      "loss": 1.8354,
      "step": 13312
    },
    {
      "epoch": 4.842802429890636e-05,
      "grad_norm": 10089.568870868567,
      "learning_rate": 8.661336772616117e-07,
      "loss": 1.8477,
      "step": 13344
    },
    {
      "epoch": 4.8544158649743063e-05,
      "grad_norm": 9205.619370797382,
      "learning_rate": 8.650959249916797e-07,
      "loss": 1.8573,
      "step": 13376
    },
    {
      "epoch": 4.866029300057977e-05,
      "grad_norm": 11243.981323356953,
      "learning_rate": 8.640618939333617e-07,
      "loss": 1.8604,
      "step": 13408
    },
    {
      "epoch": 4.877642735141648e-05,
      "grad_norm": 10507.317926093225,
      "learning_rate": 8.630315619002528e-07,
      "loss": 1.8577,
      "step": 13440
    },
    {
      "epoch": 4.8892561702253184e-05,
      "grad_norm": 9744.97234475296,
      "learning_rate": 8.62004906890698e-07,
      "loss": 1.8551,
      "step": 13472
    },
    {
      "epoch": 4.900869605308989e-05,
      "grad_norm": 9449.173297172616,
      "learning_rate": 8.609819070858184e-07,
      "loss": 1.8723,
      "step": 13504
    },
    {
      "epoch": 4.91248304039266e-05,
      "grad_norm": 11593.991374845851,
      "learning_rate": 8.599625408475633e-07,
      "loss": 1.8466,
      "step": 13536
    },
    {
      "epoch": 4.9240964754763304e-05,
      "grad_norm": 9916.692492963568,
      "learning_rate": 8.589467867167886e-07,
      "loss": 1.8214,
      "step": 13568
    },
    {
      "epoch": 4.935709910560001e-05,
      "grad_norm": 10185.534644779329,
      "learning_rate": 8.579346234113591e-07,
      "loss": 1.848,
      "step": 13600
    },
    {
      "epoch": 4.947323345643671e-05,
      "grad_norm": 10435.034067984638,
      "learning_rate": 8.569260298242756e-07,
      "loss": 1.8516,
      "step": 13632
    },
    {
      "epoch": 4.958936780727342e-05,
      "grad_norm": 9962.973652479464,
      "learning_rate": 8.55920985021826e-07,
      "loss": 1.8131,
      "step": 13664
    },
    {
      "epoch": 4.9705502158110124e-05,
      "grad_norm": 9643.938925563558,
      "learning_rate": 8.549194682417608e-07,
      "loss": 1.8476,
      "step": 13696
    },
    {
      "epoch": 4.982163650894683e-05,
      "grad_norm": 9899.39594116732,
      "learning_rate": 8.53921458891492e-07,
      "loss": 1.8503,
      "step": 13728
    },
    {
      "epoch": 4.993777085978354e-05,
      "grad_norm": 9742.34078648453,
      "learning_rate": 8.529269365463138e-07,
      "loss": 1.8477,
      "step": 13760
    },
    {
      "epoch": 5.0053905210620244e-05,
      "grad_norm": 9106.185590026156,
      "learning_rate": 8.519667991599624e-07,
      "loss": 1.8581,
      "step": 13792
    },
    {
      "epoch": 5.017003956145695e-05,
      "grad_norm": 8776.47514666338,
      "learning_rate": 8.509790828082779e-07,
      "loss": 1.8451,
      "step": 13824
    },
    {
      "epoch": 5.028617391229366e-05,
      "grad_norm": 12213.201218353852,
      "learning_rate": 8.499947937978321e-07,
      "loss": 1.8363,
      "step": 13856
    },
    {
      "epoch": 5.040230826313036e-05,
      "grad_norm": 10391.334466756423,
      "learning_rate": 8.490139123531099e-07,
      "loss": 1.8426,
      "step": 13888
    },
    {
      "epoch": 5.0518442613967064e-05,
      "grad_norm": 10180.276617066946,
      "learning_rate": 8.480364188579739e-07,
      "loss": 1.8353,
      "step": 13920
    },
    {
      "epoch": 5.063457696480377e-05,
      "grad_norm": 9448.887553569466,
      "learning_rate": 8.470622938540152e-07,
      "loss": 1.8449,
      "step": 13952
    },
    {
      "epoch": 5.075071131564048e-05,
      "grad_norm": 8310.823906208096,
      "learning_rate": 8.460915180389269e-07,
      "loss": 1.8267,
      "step": 13984
    },
    {
      "epoch": 5.0866845666477184e-05,
      "grad_norm": 11446.211600350573,
      "learning_rate": 8.451240722648987e-07,
      "loss": 1.8425,
      "step": 14016
    },
    {
      "epoch": 5.098298001731389e-05,
      "grad_norm": 10985.48797277572,
      "learning_rate": 8.441599375370293e-07,
      "loss": 1.8694,
      "step": 14048
    },
    {
      "epoch": 5.10991143681506e-05,
      "grad_norm": 10063.695841985687,
      "learning_rate": 8.431990950117611e-07,
      "loss": 1.8633,
      "step": 14080
    },
    {
      "epoch": 5.1215248718987304e-05,
      "grad_norm": 9363.139857974995,
      "learning_rate": 8.42241525995332e-07,
      "loss": 1.8181,
      "step": 14112
    },
    {
      "epoch": 5.133138306982401e-05,
      "grad_norm": 11386.258560211952,
      "learning_rate": 8.412872119422498e-07,
      "loss": 1.8254,
      "step": 14144
    },
    {
      "epoch": 5.144751742066071e-05,
      "grad_norm": 10122.297466484573,
      "learning_rate": 8.403361344537815e-07,
      "loss": 1.8182,
      "step": 14176
    },
    {
      "epoch": 5.156365177149742e-05,
      "grad_norm": 10054.888363378283,
      "learning_rate": 8.393882752764651e-07,
      "loss": 1.8366,
      "step": 14208
    },
    {
      "epoch": 5.1679786122334124e-05,
      "grad_norm": 10648.396217271407,
      "learning_rate": 8.384436163006371e-07,
      "loss": 1.8451,
      "step": 14240
    },
    {
      "epoch": 5.179592047317083e-05,
      "grad_norm": 9742.041367187885,
      "learning_rate": 8.375021395589801e-07,
      "loss": 1.8477,
      "step": 14272
    },
    {
      "epoch": 5.191205482400754e-05,
      "grad_norm": 11393.923907065555,
      "learning_rate": 8.365638272250869e-07,
      "loss": 1.871,
      "step": 14304
    },
    {
      "epoch": 5.2028189174844244e-05,
      "grad_norm": 12283.75138139811,
      "learning_rate": 8.35628661612043e-07,
      "loss": 1.8563,
      "step": 14336
    },
    {
      "epoch": 5.214432352568095e-05,
      "grad_norm": 10091.717296872717,
      "learning_rate": 8.346966251710266e-07,
      "loss": 1.8389,
      "step": 14368
    },
    {
      "epoch": 5.226045787651766e-05,
      "grad_norm": 9875.999189955415,
      "learning_rate": 8.33767700489925e-07,
      "loss": 1.8469,
      "step": 14400
    },
    {
      "epoch": 5.2376592227354364e-05,
      "grad_norm": 9718.941917719232,
      "learning_rate": 8.328418702919685e-07,
      "loss": 1.8305,
      "step": 14432
    },
    {
      "epoch": 5.2492726578191064e-05,
      "grad_norm": 10965.842968053117,
      "learning_rate": 8.319191174343811e-07,
      "loss": 1.8198,
      "step": 14464
    },
    {
      "epoch": 5.260886092902777e-05,
      "grad_norm": 9495.057029844527,
      "learning_rate": 8.30999424907047e-07,
      "loss": 1.845,
      "step": 14496
    },
    {
      "epoch": 5.272499527986448e-05,
      "grad_norm": 10146.88636971953,
      "learning_rate": 8.300827758311938e-07,
      "loss": 1.8446,
      "step": 14528
    },
    {
      "epoch": 5.2841129630701184e-05,
      "grad_norm": 9964.878022334244,
      "learning_rate": 8.291691534580914e-07,
      "loss": 1.8098,
      "step": 14560
    },
    {
      "epoch": 5.295726398153789e-05,
      "grad_norm": 10013.951068384546,
      "learning_rate": 8.282585411677661e-07,
      "loss": 1.8378,
      "step": 14592
    },
    {
      "epoch": 5.30733983323746e-05,
      "grad_norm": 11259.349537162438,
      "learning_rate": 8.273509224677321e-07,
      "loss": 1.8737,
      "step": 14624
    },
    {
      "epoch": 5.3189532683211304e-05,
      "grad_norm": 9513.798978326167,
      "learning_rate": 8.264462809917356e-07,
      "loss": 1.8634,
      "step": 14656
    },
    {
      "epoch": 5.330566703404801e-05,
      "grad_norm": 10844.11895914094,
      "learning_rate": 8.255446004985157e-07,
      "loss": 1.865,
      "step": 14688
    },
    {
      "epoch": 5.342180138488471e-05,
      "grad_norm": 13146.547835838883,
      "learning_rate": 8.246458648705801e-07,
      "loss": 1.8544,
      "step": 14720
    },
    {
      "epoch": 5.353793573572142e-05,
      "grad_norm": 10484.733091500231,
      "learning_rate": 8.237500581129945e-07,
      "loss": 1.8336,
      "step": 14752
    },
    {
      "epoch": 5.3654070086558124e-05,
      "grad_norm": 10235.834504328408,
      "learning_rate": 8.228571643521874e-07,
      "loss": 1.8367,
      "step": 14784
    },
    {
      "epoch": 5.377020443739483e-05,
      "grad_norm": 9137.068785994774,
      "learning_rate": 8.219949365267865e-07,
      "loss": 1.8337,
      "step": 14816
    },
    {
      "epoch": 5.388633878823154e-05,
      "grad_norm": 11032.85652041211,
      "learning_rate": 8.211077318035777e-07,
      "loss": 1.8443,
      "step": 14848
    },
    {
      "epoch": 5.4002473139068244e-05,
      "grad_norm": 9758.641093922863,
      "learning_rate": 8.202233936559833e-07,
      "loss": 1.8274,
      "step": 14880
    },
    {
      "epoch": 5.411860748990495e-05,
      "grad_norm": 11350.098501775215,
      "learning_rate": 8.193419066805777e-07,
      "loss": 1.8449,
      "step": 14912
    },
    {
      "epoch": 5.423474184074166e-05,
      "grad_norm": 9721.275327856936,
      "learning_rate": 8.18463255589564e-07,
      "loss": 1.8568,
      "step": 14944
    },
    {
      "epoch": 5.4350876191578364e-05,
      "grad_norm": 10579.480232979313,
      "learning_rate": 8.175874252096607e-07,
      "loss": 1.8188,
      "step": 14976
    },
    {
      "epoch": 5.4467010542415064e-05,
      "grad_norm": 9646.387095695465,
      "learning_rate": 8.167144004810015e-07,
      "loss": 1.8026,
      "step": 15008
    },
    {
      "epoch": 5.458314489325177e-05,
      "grad_norm": 9625.590475394223,
      "learning_rate": 8.15844166456047e-07,
      "loss": 1.8186,
      "step": 15040
    },
    {
      "epoch": 5.469927924408848e-05,
      "grad_norm": 8958.9258284685,
      "learning_rate": 8.149767082985105e-07,
      "loss": 1.8177,
      "step": 15072
    },
    {
      "epoch": 5.4815413594925184e-05,
      "grad_norm": 10157.979031283732,
      "learning_rate": 8.141120112822951e-07,
      "loss": 1.8307,
      "step": 15104
    },
    {
      "epoch": 5.493154794576189e-05,
      "grad_norm": 12084.653077353938,
      "learning_rate": 8.132500607904444e-07,
      "loss": 1.8526,
      "step": 15136
    },
    {
      "epoch": 5.50476822965986e-05,
      "grad_norm": 9715.074780978272,
      "learning_rate": 8.123908423141034e-07,
      "loss": 1.8456,
      "step": 15168
    },
    {
      "epoch": 5.5163816647435304e-05,
      "grad_norm": 9888.716094620171,
      "learning_rate": 8.115343414514944e-07,
      "loss": 1.8592,
      "step": 15200
    },
    {
      "epoch": 5.527995099827201e-05,
      "grad_norm": 10567.146256203707,
      "learning_rate": 8.106805439069019e-07,
      "loss": 1.8738,
      "step": 15232
    },
    {
      "epoch": 5.539608534910871e-05,
      "grad_norm": 10053.140205925709,
      "learning_rate": 8.098294354896712e-07,
      "loss": 1.87,
      "step": 15264
    },
    {
      "epoch": 5.551221969994542e-05,
      "grad_norm": 10255.628893441883,
      "learning_rate": 8.08981002113217e-07,
      "loss": 1.8666,
      "step": 15296
    },
    {
      "epoch": 5.5628354050782124e-05,
      "grad_norm": 8537.280421773668,
      "learning_rate": 8.081352297940449e-07,
      "loss": 1.813,
      "step": 15328
    },
    {
      "epoch": 5.574448840161883e-05,
      "grad_norm": 10372.16650464116,
      "learning_rate": 8.072921046507833e-07,
      "loss": 1.8191,
      "step": 15360
    },
    {
      "epoch": 5.586062275245554e-05,
      "grad_norm": 9814.089361728882,
      "learning_rate": 8.064516129032258e-07,
      "loss": 1.8485,
      "step": 15392
    },
    {
      "epoch": 5.5976757103292244e-05,
      "grad_norm": 9922.589682134396,
      "learning_rate": 8.056137408713863e-07,
      "loss": 1.8245,
      "step": 15424
    },
    {
      "epoch": 5.609289145412895e-05,
      "grad_norm": 9747.338918904996,
      "learning_rate": 8.047784749745631e-07,
      "loss": 1.8233,
      "step": 15456
    },
    {
      "epoch": 5.620902580496566e-05,
      "grad_norm": 10137.664326658287,
      "learning_rate": 8.039458017304144e-07,
      "loss": 1.8531,
      "step": 15488
    },
    {
      "epoch": 5.6325160155802364e-05,
      "grad_norm": 11155.779488677606,
      "learning_rate": 8.031157077540445e-07,
      "loss": 1.8401,
      "step": 15520
    },
    {
      "epoch": 5.6441294506639064e-05,
      "grad_norm": 10539.273789023608,
      "learning_rate": 8.022881797571e-07,
      "loss": 1.8519,
      "step": 15552
    },
    {
      "epoch": 5.655742885747577e-05,
      "grad_norm": 10395.860329958266,
      "learning_rate": 8.014632045468768e-07,
      "loss": 1.8421,
      "step": 15584
    },
    {
      "epoch": 5.667356320831248e-05,
      "grad_norm": 10671.360831684027,
      "learning_rate": 8.006407690254357e-07,
      "loss": 1.8293,
      "step": 15616
    },
    {
      "epoch": 5.6789697559149184e-05,
      "grad_norm": 10531.457638902604,
      "learning_rate": 7.9982086018873e-07,
      "loss": 1.8527,
      "step": 15648
    },
    {
      "epoch": 5.690583190998589e-05,
      "grad_norm": 9329.291077032594,
      "learning_rate": 7.990034651257415e-07,
      "loss": 1.827,
      "step": 15680
    },
    {
      "epoch": 5.70219662608226e-05,
      "grad_norm": 10526.398054415386,
      "learning_rate": 7.981885710176262e-07,
      "loss": 1.8368,
      "step": 15712
    },
    {
      "epoch": 5.7138100611659304e-05,
      "grad_norm": 11449.263731786425,
      "learning_rate": 7.97376165136871e-07,
      "loss": 1.8452,
      "step": 15744
    },
    {
      "epoch": 5.725423496249601e-05,
      "grad_norm": 10360.951886771792,
      "learning_rate": 7.965662348464577e-07,
      "loss": 1.8271,
      "step": 15776
    },
    {
      "epoch": 5.737036931333272e-05,
      "grad_norm": 11274.664340901683,
      "learning_rate": 7.957839637921133e-07,
      "loss": 1.8259,
      "step": 15808
    },
    {
      "epoch": 5.748650366416942e-05,
      "grad_norm": 9451.442535401673,
      "learning_rate": 7.949788707361304e-07,
      "loss": 1.8746,
      "step": 15840
    },
    {
      "epoch": 5.7602638015006124e-05,
      "grad_norm": 9722.536706024823,
      "learning_rate": 7.94176216279449e-07,
      "loss": 1.8408,
      "step": 15872
    },
    {
      "epoch": 5.771877236584283e-05,
      "grad_norm": 10514.799284817565,
      "learning_rate": 7.933759881361526e-07,
      "loss": 1.8149,
      "step": 15904
    },
    {
      "epoch": 5.783490671667954e-05,
      "grad_norm": 9785.8394632244,
      "learning_rate": 7.925781741068082e-07,
      "loss": 1.8063,
      "step": 15936
    },
    {
      "epoch": 5.7951041067516244e-05,
      "grad_norm": 11188.475141859146,
      "learning_rate": 7.917827620776833e-07,
      "loss": 1.8257,
      "step": 15968
    },
    {
      "epoch": 5.806717541835295e-05,
      "grad_norm": 8854.495355467752,
      "learning_rate": 7.909897400199752e-07,
      "loss": 1.8387,
      "step": 16000
    },
    {
      "epoch": 5.818330976918966e-05,
      "grad_norm": 10208.538680927843,
      "learning_rate": 7.901990959890454e-07,
      "loss": 1.8509,
      "step": 16032
    },
    {
      "epoch": 5.8299444120026364e-05,
      "grad_norm": 11389.02840456551,
      "learning_rate": 7.894108181236647e-07,
      "loss": 1.8537,
      "step": 16064
    },
    {
      "epoch": 5.8415578470863064e-05,
      "grad_norm": 11821.753592424433,
      "learning_rate": 7.886248946452661e-07,
      "loss": 1.8477,
      "step": 16096
    },
    {
      "epoch": 5.853171282169977e-05,
      "grad_norm": 9960.057228751248,
      "learning_rate": 7.878413138572051e-07,
      "loss": 1.8504,
      "step": 16128
    },
    {
      "epoch": 5.864784717253648e-05,
      "grad_norm": 9402.50232650862,
      "learning_rate": 7.870600641440283e-07,
      "loss": 1.8616,
      "step": 16160
    },
    {
      "epoch": 5.8763981523373184e-05,
      "grad_norm": 10499.561514653838,
      "learning_rate": 7.862811339707515e-07,
      "loss": 1.831,
      "step": 16192
    },
    {
      "epoch": 5.888011587420989e-05,
      "grad_norm": 8773.538966688413,
      "learning_rate": 7.855045118821426e-07,
      "loss": 1.8114,
      "step": 16224
    },
    {
      "epoch": 5.89962502250466e-05,
      "grad_norm": 10786.308729125085,
      "learning_rate": 7.847301865020156e-07,
      "loss": 1.835,
      "step": 16256
    },
    {
      "epoch": 5.9112384575883304e-05,
      "grad_norm": 11223.256746595438,
      "learning_rate": 7.839581465325298e-07,
      "loss": 1.8481,
      "step": 16288
    },
    {
      "epoch": 5.922851892672001e-05,
      "grad_norm": 10730.557115080279,
      "learning_rate": 7.831883807534978e-07,
      "loss": 1.8226,
      "step": 16320
    },
    {
      "epoch": 5.934465327755672e-05,
      "grad_norm": 9943.939159105912,
      "learning_rate": 7.824208780217005e-07,
      "loss": 1.8335,
      "step": 16352
    },
    {
      "epoch": 5.946078762839342e-05,
      "grad_norm": 11142.48356516625,
      "learning_rate": 7.816556272702094e-07,
      "loss": 1.8274,
      "step": 16384
    },
    {
      "epoch": 5.9576921979230124e-05,
      "grad_norm": 9560.365212689314,
      "learning_rate": 7.808926175077171e-07,
      "loss": 1.8367,
      "step": 16416
    },
    {
      "epoch": 5.969305633006683e-05,
      "grad_norm": 9511.074387260358,
      "learning_rate": 7.801318378178729e-07,
      "loss": 1.869,
      "step": 16448
    },
    {
      "epoch": 5.980919068090354e-05,
      "grad_norm": 10697.237493857936,
      "learning_rate": 7.793732773586288e-07,
      "loss": 1.8722,
      "step": 16480
    },
    {
      "epoch": 5.9925325031740244e-05,
      "grad_norm": 10463.648789977615,
      "learning_rate": 7.786169253615883e-07,
      "loss": 1.8414,
      "step": 16512
    },
    {
      "epoch": 6.004145938257695e-05,
      "grad_norm": 9787.47791823818,
      "learning_rate": 7.778627711313662e-07,
      "loss": 1.823,
      "step": 16544
    },
    {
      "epoch": 6.015759373341366e-05,
      "grad_norm": 13504.253848324977,
      "learning_rate": 7.771108040449519e-07,
      "loss": 1.8412,
      "step": 16576
    },
    {
      "epoch": 6.0273728084250364e-05,
      "grad_norm": 11198.751716151224,
      "learning_rate": 7.763610135510823e-07,
      "loss": 1.847,
      "step": 16608
    },
    {
      "epoch": 6.0389862435087064e-05,
      "grad_norm": 10099.112238211832,
      "learning_rate": 7.756133891696192e-07,
      "loss": 1.8478,
      "step": 16640
    },
    {
      "epoch": 6.050599678592377e-05,
      "grad_norm": 10329.066366327597,
      "learning_rate": 7.748679204909338e-07,
      "loss": 1.8096,
      "step": 16672
    },
    {
      "epoch": 6.062213113676048e-05,
      "grad_norm": 9459.262761970407,
      "learning_rate": 7.74124597175299e-07,
      "loss": 1.813,
      "step": 16704
    },
    {
      "epoch": 6.0738265487597184e-05,
      "grad_norm": 11328.58949737345,
      "learning_rate": 7.733834089522855e-07,
      "loss": 1.8446,
      "step": 16736
    },
    {
      "epoch": 6.085439983843389e-05,
      "grad_norm": 9825.471693511716,
      "learning_rate": 7.726443456201674e-07,
      "loss": 1.8182,
      "step": 16768
    },
    {
      "epoch": 6.09705341892706e-05,
      "grad_norm": 9695.014491995358,
      "learning_rate": 7.719303947779722e-07,
      "loss": 1.8008,
      "step": 16800
    },
    {
      "epoch": 6.10866685401073e-05,
      "grad_norm": 10248.851838132894,
      "learning_rate": 7.711954852744357e-07,
      "loss": 1.821,
      "step": 16832
    },
    {
      "epoch": 6.120280289094401e-05,
      "grad_norm": 9570.309921836388,
      "learning_rate": 7.704626707744303e-07,
      "loss": 1.8228,
      "step": 16864
    },
    {
      "epoch": 6.131893724178071e-05,
      "grad_norm": 10187.21787339409,
      "learning_rate": 7.697319413431387e-07,
      "loss": 1.8494,
      "step": 16896
    },
    {
      "epoch": 6.143507159261742e-05,
      "grad_norm": 9596.036994509765,
      "learning_rate": 7.690032871115763e-07,
      "loss": 1.8511,
      "step": 16928
    },
    {
      "epoch": 6.155120594345412e-05,
      "grad_norm": 12223.484773173319,
      "learning_rate": 7.6827669827603e-07,
      "loss": 1.849,
      "step": 16960
    },
    {
      "epoch": 6.166734029429084e-05,
      "grad_norm": 9314.470140593077,
      "learning_rate": 7.675521650975067e-07,
      "loss": 1.8517,
      "step": 16992
    },
    {
      "epoch": 6.178347464512754e-05,
      "grad_norm": 10020.939377124281,
      "learning_rate": 7.668296779011828e-07,
      "loss": 1.8601,
      "step": 17024
    },
    {
      "epoch": 6.189960899596424e-05,
      "grad_norm": 10780.405929277431,
      "learning_rate": 7.661092270758628e-07,
      "loss": 1.882,
      "step": 17056
    },
    {
      "epoch": 6.201574334680095e-05,
      "grad_norm": 12654.138295435214,
      "learning_rate": 7.65390803073442e-07,
      "loss": 1.8515,
      "step": 17088
    },
    {
      "epoch": 6.213187769763765e-05,
      "grad_norm": 9672.622188424399,
      "learning_rate": 7.646743964083746e-07,
      "loss": 1.8097,
      "step": 17120
    },
    {
      "epoch": 6.224801204847436e-05,
      "grad_norm": 9810.011722724901,
      "learning_rate": 7.639599976571487e-07,
      "loss": 1.8297,
      "step": 17152
    },
    {
      "epoch": 6.236414639931106e-05,
      "grad_norm": 10393.672017145818,
      "learning_rate": 7.632475974577645e-07,
      "loss": 1.864,
      "step": 17184
    },
    {
      "epoch": 6.248028075014778e-05,
      "grad_norm": 9233.280565432851,
      "learning_rate": 7.625371865092198e-07,
      "loss": 1.8252,
      "step": 17216
    },
    {
      "epoch": 6.259641510098448e-05,
      "grad_norm": 10490.23803352431,
      "learning_rate": 7.618287555709996e-07,
      "loss": 1.8134,
      "step": 17248
    },
    {
      "epoch": 6.271254945182119e-05,
      "grad_norm": 9467.084556504184,
      "learning_rate": 7.611222954625718e-07,
      "loss": 1.8157,
      "step": 17280
    },
    {
      "epoch": 6.282868380265789e-05,
      "grad_norm": 9787.419271697723,
      "learning_rate": 7.604177970628867e-07,
      "loss": 1.8208,
      "step": 17312
    },
    {
      "epoch": 6.294481815349459e-05,
      "grad_norm": 8547.034222465709,
      "learning_rate": 7.597152513098829e-07,
      "loss": 1.8348,
      "step": 17344
    },
    {
      "epoch": 6.30609525043313e-05,
      "grad_norm": 11207.975196260919,
      "learning_rate": 7.590146491999979e-07,
      "loss": 1.8595,
      "step": 17376
    },
    {
      "epoch": 6.3177086855168e-05,
      "grad_norm": 10282.676986077118,
      "learning_rate": 7.58315981787683e-07,
      "loss": 1.8324,
      "step": 17408
    },
    {
      "epoch": 6.329322120600472e-05,
      "grad_norm": 11692.007697568455,
      "learning_rate": 7.576192401849239e-07,
      "loss": 1.8385,
      "step": 17440
    },
    {
      "epoch": 6.340935555684142e-05,
      "grad_norm": 10377.755826767172,
      "learning_rate": 7.569244155607655e-07,
      "loss": 1.8342,
      "step": 17472
    },
    {
      "epoch": 6.352548990767813e-05,
      "grad_norm": 10370.098360189262,
      "learning_rate": 7.562314991408423e-07,
      "loss": 1.853,
      "step": 17504
    },
    {
      "epoch": 6.364162425851483e-05,
      "grad_norm": 10076.938920128474,
      "learning_rate": 7.555404822069124e-07,
      "loss": 1.8134,
      "step": 17536
    },
    {
      "epoch": 6.375775860935154e-05,
      "grad_norm": 11114.18589011359,
      "learning_rate": 7.548513560963972e-07,
      "loss": 1.8192,
      "step": 17568
    },
    {
      "epoch": 6.387389296018824e-05,
      "grad_norm": 10159.216308357649,
      "learning_rate": 7.541641122019254e-07,
      "loss": 1.8348,
      "step": 17600
    },
    {
      "epoch": 6.399002731102494e-05,
      "grad_norm": 11721.444450237352,
      "learning_rate": 7.534787419708808e-07,
      "loss": 1.8587,
      "step": 17632
    },
    {
      "epoch": 6.410616166186166e-05,
      "grad_norm": 9942.804332782578,
      "learning_rate": 7.527952369049562e-07,
      "loss": 1.8218,
      "step": 17664
    },
    {
      "epoch": 6.422229601269836e-05,
      "grad_norm": 8899.077705020898,
      "learning_rate": 7.521135885597101e-07,
      "loss": 1.8196,
      "step": 17696
    },
    {
      "epoch": 6.433843036353507e-05,
      "grad_norm": 10059.72703407006,
      "learning_rate": 7.514337885441285e-07,
      "loss": 1.8129,
      "step": 17728
    },
    {
      "epoch": 6.445456471437177e-05,
      "grad_norm": 10693.657372480193,
      "learning_rate": 7.50755828520192e-07,
      "loss": 1.835,
      "step": 17760
    },
    {
      "epoch": 6.457069906520848e-05,
      "grad_norm": 9693.828449070057,
      "learning_rate": 7.500797002024448e-07,
      "loss": 1.8512,
      "step": 17792
    },
    {
      "epoch": 6.468683341604518e-05,
      "grad_norm": 9696.752033541952,
      "learning_rate": 7.494264398639804e-07,
      "loss": 1.8553,
      "step": 17824
    },
    {
      "epoch": 6.48029677668819e-05,
      "grad_norm": 11527.948906895796,
      "learning_rate": 7.487538937058607e-07,
      "loss": 1.8356,
      "step": 17856
    },
    {
      "epoch": 6.49191021177186e-05,
      "grad_norm": 11557.3832678509,
      "learning_rate": 7.480831549620176e-07,
      "loss": 1.8498,
      "step": 17888
    },
    {
      "epoch": 6.50352364685553e-05,
      "grad_norm": 8576.247780935437,
      "learning_rate": 7.474142155514584e-07,
      "loss": 1.8311,
      "step": 17920
    },
    {
      "epoch": 6.515137081939201e-05,
      "grad_norm": 9429.940614871337,
      "learning_rate": 7.467470674436817e-07,
      "loss": 1.8488,
      "step": 17952
    },
    {
      "epoch": 6.526750517022871e-05,
      "grad_norm": 9439.95021173311,
      "learning_rate": 7.460817026582743e-07,
      "loss": 1.8383,
      "step": 17984
    },
    {
      "epoch": 6.538363952106542e-05,
      "grad_norm": 10815.249049374685,
      "learning_rate": 7.454181132645084e-07,
      "loss": 1.8222,
      "step": 18016
    },
    {
      "epoch": 6.549977387190212e-05,
      "grad_norm": 11703.916267643066,
      "learning_rate": 7.44756291380946e-07,
      "loss": 1.8407,
      "step": 18048
    },
    {
      "epoch": 6.561590822273884e-05,
      "grad_norm": 10474.987732689715,
      "learning_rate": 7.440962291750448e-07,
      "loss": 1.8509,
      "step": 18080
    },
    {
      "epoch": 6.573204257357554e-05,
      "grad_norm": 10403.290729379814,
      "learning_rate": 7.4343791886277e-07,
      "loss": 1.8033,
      "step": 18112
    },
    {
      "epoch": 6.584817692441225e-05,
      "grad_norm": 8467.3945225199,
      "learning_rate": 7.427813527082075e-07,
      "loss": 1.8106,
      "step": 18144
    },
    {
      "epoch": 6.596431127524895e-05,
      "grad_norm": 9975.289669979515,
      "learning_rate": 7.421265230231825e-07,
      "loss": 1.8213,
      "step": 18176
    },
    {
      "epoch": 6.608044562608565e-05,
      "grad_norm": 12565.74430744156,
      "learning_rate": 7.41473422166882e-07,
      "loss": 1.8386,
      "step": 18208
    },
    {
      "epoch": 6.619657997692236e-05,
      "grad_norm": 10356.472662060187,
      "learning_rate": 7.408220425454791e-07,
      "loss": 1.8615,
      "step": 18240
    },
    {
      "epoch": 6.631271432775906e-05,
      "grad_norm": 10067.479922999599,
      "learning_rate": 7.401723766117641e-07,
      "loss": 1.845,
      "step": 18272
    },
    {
      "epoch": 6.642884867859578e-05,
      "grad_norm": 11924.747712215969,
      "learning_rate": 7.395244168647747e-07,
      "loss": 1.8468,
      "step": 18304
    },
    {
      "epoch": 6.654498302943248e-05,
      "grad_norm": 10046.46186475617,
      "learning_rate": 7.388781558494347e-07,
      "loss": 1.8466,
      "step": 18336
    },
    {
      "epoch": 6.666111738026919e-05,
      "grad_norm": 10064.420301239412,
      "learning_rate": 7.382335861561919e-07,
      "loss": 1.8285,
      "step": 18368
    },
    {
      "epoch": 6.677725173110589e-05,
      "grad_norm": 9436.642835246017,
      "learning_rate": 7.375907004206624e-07,
      "loss": 1.8417,
      "step": 18400
    },
    {
      "epoch": 6.689338608194259e-05,
      "grad_norm": 8490.434146732427,
      "learning_rate": 7.369494913232764e-07,
      "loss": 1.8248,
      "step": 18432
    },
    {
      "epoch": 6.70095204327793e-05,
      "grad_norm": 8812.895778346638,
      "learning_rate": 7.363099515889292e-07,
      "loss": 1.7926,
      "step": 18464
    },
    {
      "epoch": 6.7125654783616e-05,
      "grad_norm": 9861.36613253965,
      "learning_rate": 7.356720739866336e-07,
      "loss": 1.8236,
      "step": 18496
    },
    {
      "epoch": 6.724178913445272e-05,
      "grad_norm": 11360.484144612852,
      "learning_rate": 7.35035851329177e-07,
      "loss": 1.8223,
      "step": 18528
    },
    {
      "epoch": 6.735792348528942e-05,
      "grad_norm": 9853.054957727578,
      "learning_rate": 7.344012764727807e-07,
      "loss": 1.7958,
      "step": 18560
    },
    {
      "epoch": 6.747405783612613e-05,
      "grad_norm": 10109.564184474028,
      "learning_rate": 7.337683423167642e-07,
      "loss": 1.8302,
      "step": 18592
    },
    {
      "epoch": 6.759019218696283e-05,
      "grad_norm": 10030.009571281575,
      "learning_rate": 7.331370418032097e-07,
      "loss": 1.8345,
      "step": 18624
    },
    {
      "epoch": 6.770632653779954e-05,
      "grad_norm": 9457.217243988847,
      "learning_rate": 7.325073679166336e-07,
      "loss": 1.8299,
      "step": 18656
    },
    {
      "epoch": 6.782246088863624e-05,
      "grad_norm": 9344.07309474835,
      "learning_rate": 7.318793136836571e-07,
      "loss": 1.8423,
      "step": 18688
    },
    {
      "epoch": 6.793859523947294e-05,
      "grad_norm": 8840.458924739145,
      "learning_rate": 7.312528721726834e-07,
      "loss": 1.8565,
      "step": 18720
    },
    {
      "epoch": 6.805472959030966e-05,
      "grad_norm": 9568.016304333934,
      "learning_rate": 7.306280364935755e-07,
      "loss": 1.8388,
      "step": 18752
    },
    {
      "epoch": 6.817086394114636e-05,
      "grad_norm": 11001.828302604981,
      "learning_rate": 7.300047997973381e-07,
      "loss": 1.8537,
      "step": 18784
    },
    {
      "epoch": 6.828699829198307e-05,
      "grad_norm": 10325.192298451395,
      "learning_rate": 7.294025576341292e-07,
      "loss": 1.8483,
      "step": 18816
    },
    {
      "epoch": 6.840313264281977e-05,
      "grad_norm": 10203.376499963137,
      "learning_rate": 7.287824490777102e-07,
      "loss": 1.8554,
      "step": 18848
    },
    {
      "epoch": 6.851926699365649e-05,
      "grad_norm": 8726.489213882063,
      "learning_rate": 7.281639194105152e-07,
      "loss": 1.8184,
      "step": 18880
    },
    {
      "epoch": 6.863540134449318e-05,
      "grad_norm": 10406.902324899567,
      "learning_rate": 7.2754696194375e-07,
      "loss": 1.8388,
      "step": 18912
    },
    {
      "epoch": 6.87515356953299e-05,
      "grad_norm": 9840.30324735981,
      "learning_rate": 7.269315700282241e-07,
      "loss": 1.8435,
      "step": 18944
    },
    {
      "epoch": 6.88676700461666e-05,
      "grad_norm": 11232.637624351637,
      "learning_rate": 7.263177370540502e-07,
      "loss": 1.8291,
      "step": 18976
    },
    {
      "epoch": 6.89838043970033e-05,
      "grad_norm": 9581.670000579232,
      "learning_rate": 7.257054564503449e-07,
      "loss": 1.8029,
      "step": 19008
    },
    {
      "epoch": 6.909993874784001e-05,
      "grad_norm": 10435.535252204365,
      "learning_rate": 7.250947216849342e-07,
      "loss": 1.8063,
      "step": 19040
    },
    {
      "epoch": 6.921607309867671e-05,
      "grad_norm": 10252.21000565244,
      "learning_rate": 7.244855262640613e-07,
      "loss": 1.8118,
      "step": 19072
    },
    {
      "epoch": 6.933220744951343e-05,
      "grad_norm": 10808.259434340018,
      "learning_rate": 7.23877863732095e-07,
      "loss": 1.8229,
      "step": 19104
    },
    {
      "epoch": 6.944834180035012e-05,
      "grad_norm": 10387.904119696139,
      "learning_rate": 7.232717276712438e-07,
      "loss": 1.8383,
      "step": 19136
    },
    {
      "epoch": 6.956447615118684e-05,
      "grad_norm": 9871.576773747951,
      "learning_rate": 7.226671117012703e-07,
      "loss": 1.8422,
      "step": 19168
    },
    {
      "epoch": 6.968061050202354e-05,
      "grad_norm": 9855.15459036539,
      "learning_rate": 7.220640094792102e-07,
      "loss": 1.8569,
      "step": 19200
    },
    {
      "epoch": 6.979674485286025e-05,
      "grad_norm": 9580.75560694458,
      "learning_rate": 7.214624146990914e-07,
      "loss": 1.8503,
      "step": 19232
    },
    {
      "epoch": 6.991287920369695e-05,
      "grad_norm": 10986.95799573294,
      "learning_rate": 7.208623210916582e-07,
      "loss": 1.8224,
      "step": 19264
    },
    {
      "epoch": 7.002901355453365e-05,
      "grad_norm": 9189.476372459967,
      "learning_rate": 7.202637224240964e-07,
      "loss": 1.837,
      "step": 19296
    },
    {
      "epoch": 7.014514790537037e-05,
      "grad_norm": 8908.679587907514,
      "learning_rate": 7.196666124997617e-07,
      "loss": 1.8134,
      "step": 19328
    },
    {
      "epoch": 7.026128225620706e-05,
      "grad_norm": 8469.993270363324,
      "learning_rate": 7.190709851579098e-07,
      "loss": 1.8053,
      "step": 19360
    },
    {
      "epoch": 7.037741660704378e-05,
      "grad_norm": 11294.72540613538,
      "learning_rate": 7.184768342734299e-07,
      "loss": 1.8434,
      "step": 19392
    },
    {
      "epoch": 7.049355095788048e-05,
      "grad_norm": 8739.458678888526,
      "learning_rate": 7.178841537565801e-07,
      "loss": 1.8448,
      "step": 19424
    },
    {
      "epoch": 7.060968530871719e-05,
      "grad_norm": 9997.669028328553,
      "learning_rate": 7.172929375527248e-07,
      "loss": 1.7981,
      "step": 19456
    },
    {
      "epoch": 7.072581965955389e-05,
      "grad_norm": 9537.678229003115,
      "learning_rate": 7.167031796420754e-07,
      "loss": 1.8115,
      "step": 19488
    },
    {
      "epoch": 7.08419540103906e-05,
      "grad_norm": 10215.158148555507,
      "learning_rate": 7.161148740394328e-07,
      "loss": 1.8417,
      "step": 19520
    },
    {
      "epoch": 7.09580883612273e-05,
      "grad_norm": 9932.44330464564,
      "learning_rate": 7.155280147939323e-07,
      "loss": 1.8335,
      "step": 19552
    },
    {
      "epoch": 7.1074222712064e-05,
      "grad_norm": 8079.167655148642,
      "learning_rate": 7.149425959887901e-07,
      "loss": 1.8551,
      "step": 19584
    },
    {
      "epoch": 7.119035706290072e-05,
      "grad_norm": 9116.314935323375,
      "learning_rate": 7.143586117410541e-07,
      "loss": 1.8428,
      "step": 19616
    },
    {
      "epoch": 7.130649141373742e-05,
      "grad_norm": 10908.382189857486,
      "learning_rate": 7.137760562013538e-07,
      "loss": 1.8369,
      "step": 19648
    },
    {
      "epoch": 7.142262576457413e-05,
      "grad_norm": 10207.783598803415,
      "learning_rate": 7.131949235536565e-07,
      "loss": 1.8301,
      "step": 19680
    },
    {
      "epoch": 7.153876011541083e-05,
      "grad_norm": 10042.24865256781,
      "learning_rate": 7.126152080150215e-07,
      "loss": 1.8254,
      "step": 19712
    },
    {
      "epoch": 7.165489446624755e-05,
      "grad_norm": 11390.728422712922,
      "learning_rate": 7.120369038353586e-07,
      "loss": 1.8389,
      "step": 19744
    },
    {
      "epoch": 7.177102881708425e-05,
      "grad_norm": 9124.144233844618,
      "learning_rate": 7.114600052971897e-07,
      "loss": 1.8266,
      "step": 19776
    },
    {
      "epoch": 7.188716316792095e-05,
      "grad_norm": 11722.444284363222,
      "learning_rate": 7.109024699115808e-07,
      "loss": 1.822,
      "step": 19808
    },
    {
      "epoch": 7.200329751875766e-05,
      "grad_norm": 10412.45235283216,
      "learning_rate": 7.103283221464939e-07,
      "loss": 1.8499,
      "step": 19840
    },
    {
      "epoch": 7.211943186959436e-05,
      "grad_norm": 9714.525207131845,
      "learning_rate": 7.097555632389488e-07,
      "loss": 1.8177,
      "step": 19872
    },
    {
      "epoch": 7.223556622043107e-05,
      "grad_norm": 10906.382901769037,
      "learning_rate": 7.091841875985731e-07,
      "loss": 1.7975,
      "step": 19904
    },
    {
      "epoch": 7.235170057126777e-05,
      "grad_norm": 10662.471008166916,
      "learning_rate": 7.086141896664472e-07,
      "loss": 1.8136,
      "step": 19936
    },
    {
      "epoch": 7.246783492210449e-05,
      "grad_norm": 10660.904089241212,
      "learning_rate": 7.080455639148767e-07,
      "loss": 1.8324,
      "step": 19968
    },
    {
      "epoch": 7.258396927294119e-05,
      "grad_norm": 9109.2303736375,
      "learning_rate": 7.07478304847167e-07,
      "loss": 1.8381,
      "step": 20000
    },
    {
      "epoch": 7.27001036237779e-05,
      "grad_norm": 9800.843637156957,
      "learning_rate": 7.069124069974004e-07,
      "loss": 1.8574,
      "step": 20032
    },
    {
      "epoch": 7.28162379746146e-05,
      "grad_norm": 10333.174342862892,
      "learning_rate": 7.063478649302151e-07,
      "loss": 1.8383,
      "step": 20064
    },
    {
      "epoch": 7.29323723254513e-05,
      "grad_norm": 9600.482175390984,
      "learning_rate": 7.057846732405855e-07,
      "loss": 1.8332,
      "step": 20096
    },
    {
      "epoch": 7.304850667628801e-05,
      "grad_norm": 11628.136050115685,
      "learning_rate": 7.052228265536053e-07,
      "loss": 1.8423,
      "step": 20128
    },
    {
      "epoch": 7.316464102712471e-05,
      "grad_norm": 11883.980477937517,
      "learning_rate": 7.046623195242716e-07,
      "loss": 1.8345,
      "step": 20160
    },
    {
      "epoch": 7.328077537796143e-05,
      "grad_norm": 9516.219102143456,
      "learning_rate": 7.041031468372717e-07,
      "loss": 1.8391,
      "step": 20192
    },
    {
      "epoch": 7.339690972879813e-05,
      "grad_norm": 9625.18488134124,
      "learning_rate": 7.03545303206771e-07,
      "loss": 1.79,
      "step": 20224
    },
    {
      "epoch": 7.351304407963484e-05,
      "grad_norm": 10843.474258741982,
      "learning_rate": 7.029887833762038e-07,
      "loss": 1.8028,
      "step": 20256
    },
    {
      "epoch": 7.362917843047154e-05,
      "grad_norm": 11747.40499003929,
      "learning_rate": 7.024335821180645e-07,
      "loss": 1.8235,
      "step": 20288
    },
    {
      "epoch": 7.374531278130825e-05,
      "grad_norm": 11240.494295181152,
      "learning_rate": 7.018796942337019e-07,
      "loss": 1.8229,
      "step": 20320
    },
    {
      "epoch": 7.386144713214495e-05,
      "grad_norm": 8726.174075733305,
      "learning_rate": 7.013271145531147e-07,
      "loss": 1.8064,
      "step": 20352
    },
    {
      "epoch": 7.397758148298165e-05,
      "grad_norm": 9509.669815508843,
      "learning_rate": 7.007758379347481e-07,
      "loss": 1.8095,
      "step": 20384
    },
    {
      "epoch": 7.409371583381837e-05,
      "grad_norm": 9760.982634960477,
      "learning_rate": 7.002258592652943e-07,
      "loss": 1.8306,
      "step": 20416
    },
    {
      "epoch": 7.420985018465507e-05,
      "grad_norm": 9773.473691579673,
      "learning_rate": 6.996771734594916e-07,
      "loss": 1.8512,
      "step": 20448
    },
    {
      "epoch": 7.432598453549178e-05,
      "grad_norm": 9974.754232561321,
      "learning_rate": 6.991297754599284e-07,
      "loss": 1.851,
      "step": 20480
    },
    {
      "epoch": 7.444211888632848e-05,
      "grad_norm": 8764.707981444675,
      "learning_rate": 6.985836602368465e-07,
      "loss": 1.8431,
      "step": 20512
    },
    {
      "epoch": 7.455825323716519e-05,
      "grad_norm": 10192.356940374488,
      "learning_rate": 6.980388227879474e-07,
      "loss": 1.8458,
      "step": 20544
    },
    {
      "epoch": 7.467438758800189e-05,
      "grad_norm": 10582.607996141594,
      "learning_rate": 6.974952581381996e-07,
      "loss": 1.8545,
      "step": 20576
    },
    {
      "epoch": 7.47905219388386e-05,
      "grad_norm": 10109.603355226158,
      "learning_rate": 6.96952961339648e-07,
      "loss": 1.8513,
      "step": 20608
    },
    {
      "epoch": 7.49066562896753e-05,
      "grad_norm": 12423.808675281507,
      "learning_rate": 6.964119274712248e-07,
      "loss": 1.8552,
      "step": 20640
    },
    {
      "epoch": 7.5022790640512e-05,
      "grad_norm": 9557.386148942607,
      "learning_rate": 6.958721516385616e-07,
      "loss": 1.793,
      "step": 20672
    },
    {
      "epoch": 7.513892499134872e-05,
      "grad_norm": 9636.104088271359,
      "learning_rate": 6.953336289738034e-07,
      "loss": 1.8099,
      "step": 20704
    },
    {
      "epoch": 7.525505934218542e-05,
      "grad_norm": 11636.78959163566,
      "learning_rate": 6.947963546354251e-07,
      "loss": 1.8415,
      "step": 20736
    },
    {
      "epoch": 7.537119369302213e-05,
      "grad_norm": 9679.29553221721,
      "learning_rate": 6.942603238080475e-07,
      "loss": 1.8323,
      "step": 20768
    },
    {
      "epoch": 7.548732804385883e-05,
      "grad_norm": 10367.763307483441,
      "learning_rate": 6.93725531702257e-07,
      "loss": 1.7936,
      "step": 20800
    },
    {
      "epoch": 7.560346239469555e-05,
      "grad_norm": 9518.885649066282,
      "learning_rate": 6.932086286156005e-07,
      "loss": 1.7916,
      "step": 20832
    },
    {
      "epoch": 7.571959674553225e-05,
      "grad_norm": 9613.921572386576,
      "learning_rate": 6.92676261346048e-07,
      "loss": 1.8178,
      "step": 20864
    },
    {
      "epoch": 7.583573109636895e-05,
      "grad_norm": 9675.007493537149,
      "learning_rate": 6.921451187307275e-07,
      "loss": 1.8266,
      "step": 20896
    },
    {
      "epoch": 7.595186544720566e-05,
      "grad_norm": 10495.563062551717,
      "learning_rate": 6.916151960815171e-07,
      "loss": 1.8565,
      "step": 20928
    },
    {
      "epoch": 7.606799979804236e-05,
      "grad_norm": 10547.095713986862,
      "learning_rate": 6.91086488735382e-07,
      "loss": 1.827,
      "step": 20960
    },
    {
      "epoch": 7.618413414887907e-05,
      "grad_norm": 11326.475003283238,
      "learning_rate": 6.905589920542014e-07,
      "loss": 1.8199,
      "step": 20992
    },
    {
      "epoch": 7.630026849971577e-05,
      "grad_norm": 10804.737294353807,
      "learning_rate": 6.900327014245992e-07,
      "loss": 1.8286,
      "step": 21024
    },
    {
      "epoch": 7.641640285055249e-05,
      "grad_norm": 11150.293628420734,
      "learning_rate": 6.895076122577726e-07,
      "loss": 1.8413,
      "step": 21056
    },
    {
      "epoch": 7.653253720138919e-05,
      "grad_norm": 9852.172247783734,
      "learning_rate": 6.889837199893257e-07,
      "loss": 1.8372,
      "step": 21088
    },
    {
      "epoch": 7.66486715522259e-05,
      "grad_norm": 10127.001036832176,
      "learning_rate": 6.884610200791017e-07,
      "loss": 1.7996,
      "step": 21120
    },
    {
      "epoch": 7.67648059030626e-05,
      "grad_norm": 10781.32858232231,
      "learning_rate": 6.879395080110184e-07,
      "loss": 1.8211,
      "step": 21152
    },
    {
      "epoch": 7.68809402538993e-05,
      "grad_norm": 10880.824233485255,
      "learning_rate": 6.874191792929041e-07,
      "loss": 1.8584,
      "step": 21184
    },
    {
      "epoch": 7.699707460473601e-05,
      "grad_norm": 9850.554197607362,
      "learning_rate": 6.869000294563347e-07,
      "loss": 1.8171,
      "step": 21216
    },
    {
      "epoch": 7.711320895557271e-05,
      "grad_norm": 10489.938036041965,
      "learning_rate": 6.86382054056473e-07,
      "loss": 1.8052,
      "step": 21248
    },
    {
      "epoch": 7.722934330640943e-05,
      "grad_norm": 9980.868198708968,
      "learning_rate": 6.858652486719089e-07,
      "loss": 1.8153,
      "step": 21280
    },
    {
      "epoch": 7.734547765724613e-05,
      "grad_norm": 9805.363328301506,
      "learning_rate": 6.853496089045006e-07,
      "loss": 1.8184,
      "step": 21312
    },
    {
      "epoch": 7.746161200808284e-05,
      "grad_norm": 10012.720109940155,
      "learning_rate": 6.848351303792173e-07,
      "loss": 1.8531,
      "step": 21344
    },
    {
      "epoch": 7.757774635891954e-05,
      "grad_norm": 9377.096778854317,
      "learning_rate": 6.843218087439837e-07,
      "loss": 1.8767,
      "step": 21376
    },
    {
      "epoch": 7.769388070975625e-05,
      "grad_norm": 10370.026518770335,
      "learning_rate": 6.838096396695252e-07,
      "loss": 1.8248,
      "step": 21408
    },
    {
      "epoch": 7.781001506059295e-05,
      "grad_norm": 10640.04896605274,
      "learning_rate": 6.832986188492143e-07,
      "loss": 1.8282,
      "step": 21440
    },
    {
      "epoch": 7.792614941142965e-05,
      "grad_norm": 10540.862583299338,
      "learning_rate": 6.82788741998919e-07,
      "loss": 1.8368,
      "step": 21472
    },
    {
      "epoch": 7.804228376226637e-05,
      "grad_norm": 9207.784098250784,
      "learning_rate": 6.822800048568512e-07,
      "loss": 1.8501,
      "step": 21504
    },
    {
      "epoch": 7.815841811310307e-05,
      "grad_norm": 9471.371495195403,
      "learning_rate": 6.817724031834181e-07,
      "loss": 1.8246,
      "step": 21536
    },
    {
      "epoch": 7.827455246393978e-05,
      "grad_norm": 10538.809420423162,
      "learning_rate": 6.812659327610731e-07,
      "loss": 1.7954,
      "step": 21568
    },
    {
      "epoch": 7.839068681477648e-05,
      "grad_norm": 9807.573604108205,
      "learning_rate": 6.807605893941687e-07,
      "loss": 1.7999,
      "step": 21600
    },
    {
      "epoch": 7.850682116561319e-05,
      "grad_norm": 9287.58838450542,
      "learning_rate": 6.802563689088104e-07,
      "loss": 1.8306,
      "step": 21632
    },
    {
      "epoch": 7.862295551644989e-05,
      "grad_norm": 9912.536002456687,
      "learning_rate": 6.797532671527126e-07,
      "loss": 1.8285,
      "step": 21664
    },
    {
      "epoch": 7.87390898672866e-05,
      "grad_norm": 9210.251896663847,
      "learning_rate": 6.79251279995054e-07,
      "loss": 1.808,
      "step": 21696
    },
    {
      "epoch": 7.88552242181233e-05,
      "grad_norm": 12123.555914004768,
      "learning_rate": 6.787504033263361e-07,
      "loss": 1.8219,
      "step": 21728
    },
    {
      "epoch": 7.897135856896e-05,
      "grad_norm": 9807.218158071126,
      "learning_rate": 6.78250633058241e-07,
      "loss": 1.8351,
      "step": 21760
    },
    {
      "epoch": 7.908749291979672e-05,
      "grad_norm": 11971.852154115502,
      "learning_rate": 6.777519651234921e-07,
      "loss": 1.8646,
      "step": 21792
    },
    {
      "epoch": 7.920362727063342e-05,
      "grad_norm": 9597.808916622585,
      "learning_rate": 6.772699279427875e-07,
      "loss": 1.8244,
      "step": 21824
    },
    {
      "epoch": 7.931976162147013e-05,
      "grad_norm": 11953.409890069026,
      "learning_rate": 6.767734184214317e-07,
      "loss": 1.8183,
      "step": 21856
    },
    {
      "epoch": 7.943589597230683e-05,
      "grad_norm": 11684.465584698344,
      "learning_rate": 6.762779992812971e-07,
      "loss": 1.8236,
      "step": 21888
    },
    {
      "epoch": 7.955203032314355e-05,
      "grad_norm": 11367.67381657303,
      "learning_rate": 6.75783666537253e-07,
      "loss": 1.8171,
      "step": 21920
    },
    {
      "epoch": 7.966816467398025e-05,
      "grad_norm": 9820.829598358787,
      "learning_rate": 6.752904162245299e-07,
      "loss": 1.8471,
      "step": 21952
    },
    {
      "epoch": 7.978429902481696e-05,
      "grad_norm": 11060.802321712472,
      "learning_rate": 6.747982443985856e-07,
      "loss": 1.8269,
      "step": 21984
    },
    {
      "epoch": 7.990043337565366e-05,
      "grad_norm": 10338.662195854935,
      "learning_rate": 6.743071471349733e-07,
      "loss": 1.7938,
      "step": 22016
    },
    {
      "epoch": 8.001656772649036e-05,
      "grad_norm": 10262.771360602359,
      "learning_rate": 6.738171205292091e-07,
      "loss": 1.8081,
      "step": 22048
    },
    {
      "epoch": 8.013270207732707e-05,
      "grad_norm": 9494.751708180684,
      "learning_rate": 6.73328160696643e-07,
      "loss": 1.8378,
      "step": 22080
    },
    {
      "epoch": 8.024883642816377e-05,
      "grad_norm": 8903.589500869859,
      "learning_rate": 6.728402637723285e-07,
      "loss": 1.8112,
      "step": 22112
    },
    {
      "epoch": 8.036497077900049e-05,
      "grad_norm": 9273.72632764198,
      "learning_rate": 6.723534259108946e-07,
      "loss": 1.8039,
      "step": 22144
    },
    {
      "epoch": 8.048110512983719e-05,
      "grad_norm": 9279.34081710549,
      "learning_rate": 6.71867643286419e-07,
      "loss": 1.8159,
      "step": 22176
    },
    {
      "epoch": 8.05972394806739e-05,
      "grad_norm": 8600.778569408702,
      "learning_rate": 6.713829120923008e-07,
      "loss": 1.8179,
      "step": 22208
    },
    {
      "epoch": 8.07133738315106e-05,
      "grad_norm": 9026.444039598318,
      "learning_rate": 6.708992285411361e-07,
      "loss": 1.8435,
      "step": 22240
    },
    {
      "epoch": 8.08295081823473e-05,
      "grad_norm": 10772.361301033308,
      "learning_rate": 6.704165888645932e-07,
      "loss": 1.8677,
      "step": 22272
    },
    {
      "epoch": 8.094564253318401e-05,
      "grad_norm": 11429.946981504332,
      "learning_rate": 6.699349893132901e-07,
      "loss": 1.8584,
      "step": 22304
    },
    {
      "epoch": 8.106177688402071e-05,
      "grad_norm": 9526.454954493827,
      "learning_rate": 6.694544261566708e-07,
      "loss": 1.8525,
      "step": 22336
    },
    {
      "epoch": 8.117791123485743e-05,
      "grad_norm": 10018.029247312068,
      "learning_rate": 6.689748956828851e-07,
      "loss": 1.8584,
      "step": 22368
    },
    {
      "epoch": 8.129404558569413e-05,
      "grad_norm": 9340.112097828376,
      "learning_rate": 6.684963941986679e-07,
      "loss": 1.8334,
      "step": 22400
    },
    {
      "epoch": 8.141017993653084e-05,
      "grad_norm": 10896.034508021714,
      "learning_rate": 6.680189180292189e-07,
      "loss": 1.8025,
      "step": 22432
    },
    {
      "epoch": 8.152631428736754e-05,
      "grad_norm": 12488.20323345196,
      "learning_rate": 6.675424635180858e-07,
      "loss": 1.7878,
      "step": 22464
    },
    {
      "epoch": 8.164244863820425e-05,
      "grad_norm": 10414.84286967403,
      "learning_rate": 6.670670270270451e-07,
      "loss": 1.8014,
      "step": 22496
    },
    {
      "epoch": 8.175858298904095e-05,
      "grad_norm": 9674.779894137126,
      "learning_rate": 6.665926049359858e-07,
      "loss": 1.844,
      "step": 22528
    },
    {
      "epoch": 8.187471733987765e-05,
      "grad_norm": 9994.867432837715,
      "learning_rate": 6.661191936427943e-07,
      "loss": 1.8023,
      "step": 22560
    },
    {
      "epoch": 8.199085169071437e-05,
      "grad_norm": 9604.15847432767,
      "learning_rate": 6.656467895632386e-07,
      "loss": 1.8117,
      "step": 22592
    },
    {
      "epoch": 8.210698604155107e-05,
      "grad_norm": 8753.63238890005,
      "learning_rate": 6.651753891308553e-07,
      "loss": 1.8248,
      "step": 22624
    },
    {
      "epoch": 8.222312039238778e-05,
      "grad_norm": 11232.192484105675,
      "learning_rate": 6.647049887968355e-07,
      "loss": 1.824,
      "step": 22656
    },
    {
      "epoch": 8.233925474322448e-05,
      "grad_norm": 9399.19305046981,
      "learning_rate": 6.642355850299131e-07,
      "loss": 1.8199,
      "step": 22688
    },
    {
      "epoch": 8.245538909406119e-05,
      "grad_norm": 10364.010420681754,
      "learning_rate": 6.637671743162539e-07,
      "loss": 1.8406,
      "step": 22720
    },
    {
      "epoch": 8.257152344489789e-05,
      "grad_norm": 10003.482893472654,
      "learning_rate": 6.632997531593441e-07,
      "loss": 1.8143,
      "step": 22752
    },
    {
      "epoch": 8.26876577957346e-05,
      "grad_norm": 11921.320899967419,
      "learning_rate": 6.628333180798819e-07,
      "loss": 1.8219,
      "step": 22784
    },
    {
      "epoch": 8.28037921465713e-05,
      "grad_norm": 9374.686448089877,
      "learning_rate": 6.623823961658694e-07,
      "loss": 1.8135,
      "step": 22816
    },
    {
      "epoch": 8.2919926497408e-05,
      "grad_norm": 10769.455696552172,
      "learning_rate": 6.619178923246148e-07,
      "loss": 1.8389,
      "step": 22848
    },
    {
      "epoch": 8.303606084824472e-05,
      "grad_norm": 9501.043521634872,
      "learning_rate": 6.614543643319656e-07,
      "loss": 1.837,
      "step": 22880
    },
    {
      "epoch": 8.315219519908142e-05,
      "grad_norm": 10485.194609543496,
      "learning_rate": 6.609918087758631e-07,
      "loss": 1.8379,
      "step": 22912
    },
    {
      "epoch": 8.326832954991813e-05,
      "grad_norm": 10328.163437901241,
      "learning_rate": 6.605302222609272e-07,
      "loss": 1.8316,
      "step": 22944
    },
    {
      "epoch": 8.338446390075483e-05,
      "grad_norm": 10912.83473713407,
      "learning_rate": 6.600696014083522e-07,
      "loss": 1.8439,
      "step": 22976
    },
    {
      "epoch": 8.350059825159155e-05,
      "grad_norm": 11341.068203657009,
      "learning_rate": 6.596099428558033e-07,
      "loss": 1.8021,
      "step": 23008
    },
    {
      "epoch": 8.361673260242825e-05,
      "grad_norm": 8816.414917640843,
      "learning_rate": 6.591512432573124e-07,
      "loss": 1.8033,
      "step": 23040
    },
    {
      "epoch": 8.373286695326496e-05,
      "grad_norm": 10981.835547849003,
      "learning_rate": 6.586934992831776e-07,
      "loss": 1.8115,
      "step": 23072
    },
    {
      "epoch": 8.384900130410166e-05,
      "grad_norm": 10136.117797263409,
      "learning_rate": 6.582367076198594e-07,
      "loss": 1.8239,
      "step": 23104
    },
    {
      "epoch": 8.396513565493836e-05,
      "grad_norm": 10567.363152650712,
      "learning_rate": 6.57780864969882e-07,
      "loss": 1.845,
      "step": 23136
    },
    {
      "epoch": 8.408127000577507e-05,
      "grad_norm": 10657.302472952524,
      "learning_rate": 6.573259680517322e-07,
      "loss": 1.8387,
      "step": 23168
    },
    {
      "epoch": 8.419740435661177e-05,
      "grad_norm": 9586.484131317384,
      "learning_rate": 6.568720135997607e-07,
      "loss": 1.8576,
      "step": 23200
    },
    {
      "epoch": 8.431353870744849e-05,
      "grad_norm": 11441.311113679236,
      "learning_rate": 6.564189983640828e-07,
      "loss": 1.846,
      "step": 23232
    },
    {
      "epoch": 8.442967305828519e-05,
      "grad_norm": 10023.286886046913,
      "learning_rate": 6.559669191104821e-07,
      "loss": 1.8115,
      "step": 23264
    },
    {
      "epoch": 8.45458074091219e-05,
      "grad_norm": 9895.608520955142,
      "learning_rate": 6.555157726203121e-07,
      "loss": 1.827,
      "step": 23296
    },
    {
      "epoch": 8.46619417599586e-05,
      "grad_norm": 8148.121869486244,
      "learning_rate": 6.550655556904012e-07,
      "loss": 1.8036,
      "step": 23328
    },
    {
      "epoch": 8.477807611079531e-05,
      "grad_norm": 10024.908079379084,
      "learning_rate": 6.546162651329555e-07,
      "loss": 1.7869,
      "step": 23360
    },
    {
      "epoch": 8.489421046163201e-05,
      "grad_norm": 10684.895039259862,
      "learning_rate": 6.54167897775466e-07,
      "loss": 1.8116,
      "step": 23392
    },
    {
      "epoch": 8.501034481246871e-05,
      "grad_norm": 10937.597176711162,
      "learning_rate": 6.537204504606135e-07,
      "loss": 1.8309,
      "step": 23424
    },
    {
      "epoch": 8.512647916330543e-05,
      "grad_norm": 10567.737411574912,
      "learning_rate": 6.532739200461747e-07,
      "loss": 1.7999,
      "step": 23456
    },
    {
      "epoch": 8.524261351414213e-05,
      "grad_norm": 9334.583011575825,
      "learning_rate": 6.528283034049304e-07,
      "loss": 1.8304,
      "step": 23488
    },
    {
      "epoch": 8.535874786497884e-05,
      "grad_norm": 8820.839642573716,
      "learning_rate": 6.523835974245735e-07,
      "loss": 1.8579,
      "step": 23520
    },
    {
      "epoch": 8.547488221581554e-05,
      "grad_norm": 9662.568499110368,
      "learning_rate": 6.519397990076168e-07,
      "loss": 1.8239,
      "step": 23552
    },
    {
      "epoch": 8.559101656665225e-05,
      "grad_norm": 8812.673941545778,
      "learning_rate": 6.514969050713037e-07,
      "loss": 1.8279,
      "step": 23584
    },
    {
      "epoch": 8.570715091748895e-05,
      "grad_norm": 10091.299817169243,
      "learning_rate": 6.51054912547517e-07,
      "loss": 1.8264,
      "step": 23616
    },
    {
      "epoch": 8.582328526832565e-05,
      "grad_norm": 11852.490033743965,
      "learning_rate": 6.506138183826906e-07,
      "loss": 1.8167,
      "step": 23648
    },
    {
      "epoch": 8.593941961916237e-05,
      "grad_norm": 10132.01125147421,
      "learning_rate": 6.501736195377201e-07,
      "loss": 1.8161,
      "step": 23680
    },
    {
      "epoch": 8.605555396999907e-05,
      "grad_norm": 11128.630284091569,
      "learning_rate": 6.497343129878754e-07,
      "loss": 1.8256,
      "step": 23712
    },
    {
      "epoch": 8.617168832083578e-05,
      "grad_norm": 9554.086978879772,
      "learning_rate": 6.492958957227136e-07,
      "loss": 1.833,
      "step": 23744
    },
    {
      "epoch": 8.628782267167248e-05,
      "grad_norm": 9098.860807815448,
      "learning_rate": 6.488583647459915e-07,
      "loss": 1.8137,
      "step": 23776
    },
    {
      "epoch": 8.640395702250919e-05,
      "grad_norm": 10414.489713855404,
      "learning_rate": 6.484217170755803e-07,
      "loss": 1.8177,
      "step": 23808
    },
    {
      "epoch": 8.652009137334589e-05,
      "grad_norm": 10100.458009417196,
      "learning_rate": 6.479995541764601e-07,
      "loss": 1.8289,
      "step": 23840
    },
    {
      "epoch": 8.66362257241826e-05,
      "grad_norm": 10588.226763722054,
      "learning_rate": 6.475646368546418e-07,
      "loss": 1.8343,
      "step": 23872
    },
    {
      "epoch": 8.67523600750193e-05,
      "grad_norm": 11458.251524556441,
      "learning_rate": 6.47130594068258e-07,
      "loss": 1.7897,
      "step": 23904
    },
    {
      "epoch": 8.6868494425856e-05,
      "grad_norm": 10498.756878792841,
      "learning_rate": 6.466974228903602e-07,
      "loss": 1.805,
      "step": 23936
    },
    {
      "epoch": 8.698462877669272e-05,
      "grad_norm": 10467.186059299796,
      "learning_rate": 6.462651204076959e-07,
      "loss": 1.8164,
      "step": 23968
    },
    {
      "epoch": 8.710076312752942e-05,
      "grad_norm": 12636.328738996941,
      "learning_rate": 6.458336837206266e-07,
      "loss": 1.8232,
      "step": 24000
    },
    {
      "epoch": 8.721689747836613e-05,
      "grad_norm": 8687.609222334992,
      "learning_rate": 6.45403109943046e-07,
      "loss": 1.8471,
      "step": 24032
    },
    {
      "epoch": 8.733303182920283e-05,
      "grad_norm": 10040.285902303778,
      "learning_rate": 6.449733962022988e-07,
      "loss": 1.8566,
      "step": 24064
    },
    {
      "epoch": 8.744916618003955e-05,
      "grad_norm": 10391.039697739587,
      "learning_rate": 6.445445396391006e-07,
      "loss": 1.8576,
      "step": 24096
    },
    {
      "epoch": 8.756530053087625e-05,
      "grad_norm": 9506.86835924428,
      "learning_rate": 6.441165374074573e-07,
      "loss": 1.858,
      "step": 24128
    },
    {
      "epoch": 8.768143488171296e-05,
      "grad_norm": 10222.495390069882,
      "learning_rate": 6.436893866745868e-07,
      "loss": 1.8219,
      "step": 24160
    },
    {
      "epoch": 8.779756923254966e-05,
      "grad_norm": 9951.272682426103,
      "learning_rate": 6.432630846208391e-07,
      "loss": 1.8189,
      "step": 24192
    },
    {
      "epoch": 8.791370358338636e-05,
      "grad_norm": 10867.954729386758,
      "learning_rate": 6.428376284396189e-07,
      "loss": 1.7973,
      "step": 24224
    },
    {
      "epoch": 8.802983793422307e-05,
      "grad_norm": 9856.404821231725,
      "learning_rate": 6.424130153373078e-07,
      "loss": 1.7946,
      "step": 24256
    },
    {
      "epoch": 8.814597228505977e-05,
      "grad_norm": 9200.823224038162,
      "learning_rate": 6.419892425331871e-07,
      "loss": 1.8159,
      "step": 24288
    },
    {
      "epoch": 8.826210663589649e-05,
      "grad_norm": 10446.998994926726,
      "learning_rate": 6.415663072593617e-07,
      "loss": 1.8383,
      "step": 24320
    },
    {
      "epoch": 8.837824098673319e-05,
      "grad_norm": 9157.736073943166,
      "learning_rate": 6.411442067606836e-07,
      "loss": 1.7908,
      "step": 24352
    },
    {
      "epoch": 8.84943753375699e-05,
      "grad_norm": 10952.61576062997,
      "learning_rate": 6.407229382946771e-07,
      "loss": 1.8069,
      "step": 24384
    },
    {
      "epoch": 8.86105096884066e-05,
      "grad_norm": 10972.993484004262,
      "learning_rate": 6.403024991314634e-07,
      "loss": 1.8177,
      "step": 24416
    },
    {
      "epoch": 8.872664403924331e-05,
      "grad_norm": 9286.426223257255,
      "learning_rate": 6.398828865536871e-07,
      "loss": 1.8283,
      "step": 24448
    },
    {
      "epoch": 8.884277839008001e-05,
      "grad_norm": 8841.434046578643,
      "learning_rate": 6.394640978564413e-07,
      "loss": 1.8301,
      "step": 24480
    },
    {
      "epoch": 8.895891274091671e-05,
      "grad_norm": 9618.25826228429,
      "learning_rate": 6.390461303471954e-07,
      "loss": 1.8198,
      "step": 24512
    },
    {
      "epoch": 8.907504709175343e-05,
      "grad_norm": 9465.522383894087,
      "learning_rate": 6.386289813457219e-07,
      "loss": 1.8221,
      "step": 24544
    },
    {
      "epoch": 8.919118144259013e-05,
      "grad_norm": 10284.765918580744,
      "learning_rate": 6.382126481840243e-07,
      "loss": 1.8228,
      "step": 24576
    },
    {
      "epoch": 8.930731579342684e-05,
      "grad_norm": 11123.510417130017,
      "learning_rate": 6.377971282062655e-07,
      "loss": 1.8272,
      "step": 24608
    },
    {
      "epoch": 8.942345014426354e-05,
      "grad_norm": 10683.602295106273,
      "learning_rate": 6.373824187686963e-07,
      "loss": 1.8429,
      "step": 24640
    },
    {
      "epoch": 8.953958449510025e-05,
      "grad_norm": 9080.780032574294,
      "learning_rate": 6.369685172395854e-07,
      "loss": 1.8233,
      "step": 24672
    },
    {
      "epoch": 8.965571884593695e-05,
      "grad_norm": 9489.327162660164,
      "learning_rate": 6.365554209991489e-07,
      "loss": 1.825,
      "step": 24704
    },
    {
      "epoch": 8.977185319677367e-05,
      "grad_norm": 8918.65987690976,
      "learning_rate": 6.361431274394804e-07,
      "loss": 1.8577,
      "step": 24736
    },
    {
      "epoch": 8.988798754761037e-05,
      "grad_norm": 10470.835687756733,
      "learning_rate": 6.35731633964482e-07,
      "loss": 1.8296,
      "step": 24768
    },
    {
      "epoch": 9.000412189844707e-05,
      "grad_norm": 10647.722667312479,
      "learning_rate": 6.353209379897959e-07,
      "loss": 1.7959,
      "step": 24800
    },
    {
      "epoch": 9.012025624928378e-05,
      "grad_norm": 7669.2718037633795,
      "learning_rate": 6.349238343433312e-07,
      "loss": 1.7966,
      "step": 24832
    },
    {
      "epoch": 9.023639060012048e-05,
      "grad_norm": 9242.668878630242,
      "learning_rate": 6.345147009400132e-07,
      "loss": 1.8224,
      "step": 24864
    },
    {
      "epoch": 9.03525249509572e-05,
      "grad_norm": 10174.51993953523,
      "learning_rate": 6.341063574331939e-07,
      "loss": 1.8289,
      "step": 24896
    },
    {
      "epoch": 9.046865930179389e-05,
      "grad_norm": 10530.515466965517,
      "learning_rate": 6.33698801284441e-07,
      "loss": 1.8567,
      "step": 24928
    },
    {
      "epoch": 9.05847936526306e-05,
      "grad_norm": 10975.549371216002,
      "learning_rate": 6.33292029966728e-07,
      "loss": 1.8311,
      "step": 24960
    },
    {
      "epoch": 9.07009280034673e-05,
      "grad_norm": 10410.917634867736,
      "learning_rate": 6.328860409643689e-07,
      "loss": 1.8151,
      "step": 24992
    },
    {
      "epoch": 9.0817062354304e-05,
      "grad_norm": 8765.683088042826,
      "learning_rate": 6.324808317729518e-07,
      "loss": 1.8377,
      "step": 25024
    },
    {
      "epoch": 9.093319670514072e-05,
      "grad_norm": 10911.573672023665,
      "learning_rate": 6.320763998992752e-07,
      "loss": 1.8278,
      "step": 25056
    },
    {
      "epoch": 9.104933105597742e-05,
      "grad_norm": 11607.040966585755,
      "learning_rate": 6.316727428612831e-07,
      "loss": 1.8237,
      "step": 25088
    },
    {
      "epoch": 9.116546540681413e-05,
      "grad_norm": 10276.1511277326,
      "learning_rate": 6.312698581880009e-07,
      "loss": 1.7881,
      "step": 25120
    },
    {
      "epoch": 9.128159975765083e-05,
      "grad_norm": 9577.261090729438,
      "learning_rate": 6.308677434194719e-07,
      "loss": 1.7972,
      "step": 25152
    },
    {
      "epoch": 9.139773410848755e-05,
      "grad_norm": 11208.320926882849,
      "learning_rate": 6.304663961066943e-07,
      "loss": 1.8313,
      "step": 25184
    },
    {
      "epoch": 9.151386845932425e-05,
      "grad_norm": 9344.652802539,
      "learning_rate": 6.300658138115584e-07,
      "loss": 1.8318,
      "step": 25216
    },
    {
      "epoch": 9.163000281016096e-05,
      "grad_norm": 9900.941268384537,
      "learning_rate": 6.29665994106785e-07,
      "loss": 1.8029,
      "step": 25248
    },
    {
      "epoch": 9.174613716099766e-05,
      "grad_norm": 10792.874037993772,
      "learning_rate": 6.292669345758626e-07,
      "loss": 1.8137,
      "step": 25280
    },
    {
      "epoch": 9.186227151183436e-05,
      "grad_norm": 10326.61609628246,
      "learning_rate": 6.288686328129869e-07,
      "loss": 1.8353,
      "step": 25312
    },
    {
      "epoch": 9.197840586267107e-05,
      "grad_norm": 9563.111522930181,
      "learning_rate": 6.284710864229996e-07,
      "loss": 1.834,
      "step": 25344
    },
    {
      "epoch": 9.209454021350777e-05,
      "grad_norm": 9581.99300772026,
      "learning_rate": 6.280742930213278e-07,
      "loss": 1.8364,
      "step": 25376
    },
    {
      "epoch": 9.221067456434449e-05,
      "grad_norm": 11107.688688471604,
      "learning_rate": 6.276782502339241e-07,
      "loss": 1.8148,
      "step": 25408
    },
    {
      "epoch": 9.232680891518119e-05,
      "grad_norm": 9992.266809888535,
      "learning_rate": 6.272829556972067e-07,
      "loss": 1.8069,
      "step": 25440
    },
    {
      "epoch": 9.24429432660179e-05,
      "grad_norm": 9248.785866263745,
      "learning_rate": 6.268884070580009e-07,
      "loss": 1.8427,
      "step": 25472
    },
    {
      "epoch": 9.25590776168546e-05,
      "grad_norm": 9096.220533826123,
      "learning_rate": 6.264946019734787e-07,
      "loss": 1.8289,
      "step": 25504
    },
    {
      "epoch": 9.267521196769131e-05,
      "grad_norm": 9847.94313549789,
      "learning_rate": 6.261015381111024e-07,
      "loss": 1.8251,
      "step": 25536
    },
    {
      "epoch": 9.279134631852801e-05,
      "grad_norm": 9536.904004969327,
      "learning_rate": 6.257092131485649e-07,
      "loss": 1.8007,
      "step": 25568
    },
    {
      "epoch": 9.290748066936471e-05,
      "grad_norm": 9450.986086118211,
      "learning_rate": 6.253176247737328e-07,
      "loss": 1.8024,
      "step": 25600
    },
    {
      "epoch": 9.302361502020143e-05,
      "grad_norm": 10581.813833176238,
      "learning_rate": 6.249267706845892e-07,
      "loss": 1.8343,
      "step": 25632
    },
    {
      "epoch": 9.313974937103813e-05,
      "grad_norm": 10098.268366408174,
      "learning_rate": 6.245366485891767e-07,
      "loss": 1.8364,
      "step": 25664
    },
    {
      "epoch": 9.325588372187484e-05,
      "grad_norm": 8270.402892241707,
      "learning_rate": 6.241472562055411e-07,
      "loss": 1.7897,
      "step": 25696
    },
    {
      "epoch": 9.337201807271154e-05,
      "grad_norm": 10001.841630419869,
      "learning_rate": 6.237585912616748e-07,
      "loss": 1.7969,
      "step": 25728
    },
    {
      "epoch": 9.348815242354825e-05,
      "grad_norm": 10843.947712894967,
      "learning_rate": 6.233706514954623e-07,
      "loss": 1.8137,
      "step": 25760
    },
    {
      "epoch": 9.360428677438495e-05,
      "grad_norm": 10066.68634655913,
      "learning_rate": 6.229834346546233e-07,
      "loss": 1.8476,
      "step": 25792
    },
    {
      "epoch": 9.372042112522167e-05,
      "grad_norm": 8930.57400170896,
      "learning_rate": 6.226090056150256e-07,
      "loss": 1.8473,
      "step": 25824
    },
    {
      "epoch": 9.383655547605837e-05,
      "grad_norm": 11784.06177852102,
      "learning_rate": 6.222232054892992e-07,
      "loss": 1.8396,
      "step": 25856
    },
    {
      "epoch": 9.395268982689507e-05,
      "grad_norm": 11187.366982449445,
      "learning_rate": 6.218381216599018e-07,
      "loss": 1.8321,
      "step": 25888
    },
    {
      "epoch": 9.406882417773178e-05,
      "grad_norm": 9475.73374467645,
      "learning_rate": 6.214537519130523e-07,
      "loss": 1.8286,
      "step": 25920
    },
    {
      "epoch": 9.418495852856848e-05,
      "grad_norm": 10803.581350644794,
      "learning_rate": 6.210700940445369e-07,
      "loss": 1.8323,
      "step": 25952
    },
    {
      "epoch": 9.43010928794052e-05,
      "grad_norm": 9534.030941841966,
      "learning_rate": 6.206871458596551e-07,
      "loss": 1.8328,
      "step": 25984
    },
    {
      "epoch": 9.44172272302419e-05,
      "grad_norm": 9087.20242979103,
      "learning_rate": 6.203049051731675e-07,
      "loss": 1.7802,
      "step": 26016
    },
    {
      "epoch": 9.453336158107861e-05,
      "grad_norm": 11261.359243004372,
      "learning_rate": 6.199233698092436e-07,
      "loss": 1.7969,
      "step": 26048
    },
    {
      "epoch": 9.464949593191531e-05,
      "grad_norm": 11805.873284090423,
      "learning_rate": 6.195425376014091e-07,
      "loss": 1.8557,
      "step": 26080
    },
    {
      "epoch": 9.476563028275202e-05,
      "grad_norm": 8539.696833026333,
      "learning_rate": 6.19162406392495e-07,
      "loss": 1.802,
      "step": 26112
    },
    {
      "epoch": 9.488176463358872e-05,
      "grad_norm": 9686.564509670083,
      "learning_rate": 6.187829740345857e-07,
      "loss": 1.7816,
      "step": 26144
    },
    {
      "epoch": 9.499789898442542e-05,
      "grad_norm": 10326.910477001338,
      "learning_rate": 6.184042383889684e-07,
      "loss": 1.794,
      "step": 26176
    },
    {
      "epoch": 9.511403333526213e-05,
      "grad_norm": 11024.947800329941,
      "learning_rate": 6.180261973260823e-07,
      "loss": 1.8007,
      "step": 26208
    },
    {
      "epoch": 9.523016768609883e-05,
      "grad_norm": 10201.96383055733,
      "learning_rate": 6.176488487254683e-07,
      "loss": 1.827,
      "step": 26240
    },
    {
      "epoch": 9.534630203693555e-05,
      "grad_norm": 9346.182536201612,
      "learning_rate": 6.17272190475719e-07,
      "loss": 1.8443,
      "step": 26272
    },
    {
      "epoch": 9.546243638777225e-05,
      "grad_norm": 9466.492697932006,
      "learning_rate": 6.168962204744293e-07,
      "loss": 1.8189,
      "step": 26304
    },
    {
      "epoch": 9.557857073860896e-05,
      "grad_norm": 10795.54408077703,
      "learning_rate": 6.16520936628147e-07,
      "loss": 1.8273,
      "step": 26336
    },
    {
      "epoch": 9.569470508944566e-05,
      "grad_norm": 10499.855903773156,
      "learning_rate": 6.161463368523236e-07,
      "loss": 1.8231,
      "step": 26368
    },
    {
      "epoch": 9.581083944028236e-05,
      "grad_norm": 10628.057301313349,
      "learning_rate": 6.15772419071266e-07,
      "loss": 1.8374,
      "step": 26400
    },
    {
      "epoch": 9.592697379111907e-05,
      "grad_norm": 10937.085900732425,
      "learning_rate": 6.153991812180881e-07,
      "loss": 1.8423,
      "step": 26432
    },
    {
      "epoch": 9.604310814195577e-05,
      "grad_norm": 10977.404975676172,
      "learning_rate": 6.150266212346628e-07,
      "loss": 1.8114,
      "step": 26464
    },
    {
      "epoch": 9.615924249279249e-05,
      "grad_norm": 9933.136060680938,
      "learning_rate": 6.146547370715743e-07,
      "loss": 1.8194,
      "step": 26496
    },
    {
      "epoch": 9.627537684362919e-05,
      "grad_norm": 9607.565664620774,
      "learning_rate": 6.142835266880706e-07,
      "loss": 1.8353,
      "step": 26528
    },
    {
      "epoch": 9.63915111944659e-05,
      "grad_norm": 9564.294432941722,
      "learning_rate": 6.139129880520171e-07,
      "loss": 1.8126,
      "step": 26560
    },
    {
      "epoch": 9.65076455453026e-05,
      "grad_norm": 9553.58759838418,
      "learning_rate": 6.135431191398489e-07,
      "loss": 1.8177,
      "step": 26592
    },
    {
      "epoch": 9.662377989613931e-05,
      "grad_norm": 11203.33146880873,
      "learning_rate": 6.131739179365251e-07,
      "loss": 1.7975,
      "step": 26624
    },
    {
      "epoch": 9.673991424697601e-05,
      "grad_norm": 10717.056685489724,
      "learning_rate": 6.128053824354824e-07,
      "loss": 1.839,
      "step": 26656
    },
    {
      "epoch": 9.685604859781271e-05,
      "grad_norm": 8998.928714019241,
      "learning_rate": 6.1243751063859e-07,
      "loss": 1.8308,
      "step": 26688
    },
    {
      "epoch": 9.697218294864943e-05,
      "grad_norm": 9320.658453135165,
      "learning_rate": 6.120703005561024e-07,
      "loss": 1.8233,
      "step": 26720
    },
    {
      "epoch": 9.708831729948613e-05,
      "grad_norm": 9967.832161508339,
      "learning_rate": 6.117037502066161e-07,
      "loss": 1.8054,
      "step": 26752
    },
    {
      "epoch": 9.720445165032284e-05,
      "grad_norm": 8828.435648516672,
      "learning_rate": 6.113378576170238e-07,
      "loss": 1.8166,
      "step": 26784
    },
    {
      "epoch": 9.732058600115954e-05,
      "grad_norm": 9766.506540211807,
      "learning_rate": 6.109726208224694e-07,
      "loss": 1.8162,
      "step": 26816
    },
    {
      "epoch": 9.743672035199625e-05,
      "grad_norm": 10922.250592254326,
      "learning_rate": 6.106194212060931e-07,
      "loss": 1.8246,
      "step": 26848
    },
    {
      "epoch": 9.755285470283295e-05,
      "grad_norm": 10155.30836557906,
      "learning_rate": 6.102554697976864e-07,
      "loss": 1.8348,
      "step": 26880
    },
    {
      "epoch": 9.766898905366967e-05,
      "grad_norm": 9236.065612586346,
      "learning_rate": 6.098921683993328e-07,
      "loss": 1.8032,
      "step": 26912
    },
    {
      "epoch": 9.778512340450637e-05,
      "grad_norm": 10647.197753399718,
      "learning_rate": 6.095295150784911e-07,
      "loss": 1.8162,
      "step": 26944
    },
    {
      "epoch": 9.790125775534307e-05,
      "grad_norm": 10387.463212931249,
      "learning_rate": 6.091675079106547e-07,
      "loss": 1.8289,
      "step": 26976
    },
    {
      "epoch": 9.801739210617978e-05,
      "grad_norm": 9049.57910623472,
      "learning_rate": 6.088061449793081e-07,
      "loss": 1.8087,
      "step": 27008
    },
    {
      "epoch": 9.813352645701648e-05,
      "grad_norm": 8923.101478746054,
      "learning_rate": 6.084454243758851e-07,
      "loss": 1.801,
      "step": 27040
    },
    {
      "epoch": 9.82496608078532e-05,
      "grad_norm": 9719.127841529815,
      "learning_rate": 6.080853441997254e-07,
      "loss": 1.8162,
      "step": 27072
    },
    {
      "epoch": 9.83657951586899e-05,
      "grad_norm": 10303.620043460454,
      "learning_rate": 6.077259025580334e-07,
      "loss": 1.8049,
      "step": 27104
    },
    {
      "epoch": 9.848192950952661e-05,
      "grad_norm": 10867.274359286233,
      "learning_rate": 6.073670975658364e-07,
      "loss": 1.8115,
      "step": 27136
    },
    {
      "epoch": 9.859806386036331e-05,
      "grad_norm": 9624.104321961602,
      "learning_rate": 6.070089273459422e-07,
      "loss": 1.8432,
      "step": 27168
    },
    {
      "epoch": 9.871419821120002e-05,
      "grad_norm": 10044.808211210408,
      "learning_rate": 6.06651390028899e-07,
      "loss": 1.8347,
      "step": 27200
    },
    {
      "epoch": 9.883033256203672e-05,
      "grad_norm": 10667.64847564823,
      "learning_rate": 6.062944837529535e-07,
      "loss": 1.8323,
      "step": 27232
    },
    {
      "epoch": 9.894646691287342e-05,
      "grad_norm": 10416.763892879593,
      "learning_rate": 6.059382066640109e-07,
      "loss": 1.8294,
      "step": 27264
    },
    {
      "epoch": 9.906260126371013e-05,
      "grad_norm": 8913.168348011834,
      "learning_rate": 6.055825569155937e-07,
      "loss": 1.8273,
      "step": 27296
    },
    {
      "epoch": 9.917873561454683e-05,
      "grad_norm": 9320.67261521399,
      "learning_rate": 6.052275326688025e-07,
      "loss": 1.8147,
      "step": 27328
    },
    {
      "epoch": 9.929486996538355e-05,
      "grad_norm": 10167.694330574655,
      "learning_rate": 6.048731320922748e-07,
      "loss": 1.79,
      "step": 27360
    },
    {
      "epoch": 9.941100431622025e-05,
      "grad_norm": 9335.766492366869,
      "learning_rate": 6.045193533621463e-07,
      "loss": 1.8076,
      "step": 27392
    },
    {
      "epoch": 9.952713866705696e-05,
      "grad_norm": 10254.399348572299,
      "learning_rate": 6.041661946620115e-07,
      "loss": 1.8305,
      "step": 27424
    },
    {
      "epoch": 9.964327301789366e-05,
      "grad_norm": 9819.619748238727,
      "learning_rate": 6.038136541828836e-07,
      "loss": 1.8043,
      "step": 27456
    },
    {
      "epoch": 9.975940736873037e-05,
      "grad_norm": 9660.278774445384,
      "learning_rate": 6.034617301231568e-07,
      "loss": 1.8181,
      "step": 27488
    },
    {
      "epoch": 9.987554171956707e-05,
      "grad_norm": 10349.591682767006,
      "learning_rate": 6.031104206885666e-07,
      "loss": 1.8375,
      "step": 27520
    },
    {
      "epoch": 9.999167607040377e-05,
      "grad_norm": 10678.933935557425,
      "learning_rate": 6.02759724092152e-07,
      "loss": 1.8021,
      "step": 27552
    },
    {
      "epoch": 0.00010010781042124049,
      "grad_norm": 10322.505897310013,
      "learning_rate": 6.024096385542169e-07,
      "loss": 1.8239,
      "step": 27584
    },
    {
      "epoch": 0.00010022394477207719,
      "grad_norm": 10581.927990682983,
      "learning_rate": 6.020601623022926e-07,
      "loss": 1.8479,
      "step": 27616
    },
    {
      "epoch": 0.0001003400791229139,
      "grad_norm": 9497.727096521567,
      "learning_rate": 6.017112935711001e-07,
      "loss": 1.8252,
      "step": 27648
    },
    {
      "epoch": 0.0001004562134737506,
      "grad_norm": 9824.134974642804,
      "learning_rate": 6.013630306025118e-07,
      "loss": 1.8344,
      "step": 27680
    },
    {
      "epoch": 0.00010057234782458731,
      "grad_norm": 9568.909342239585,
      "learning_rate": 6.010153716455158e-07,
      "loss": 1.7946,
      "step": 27712
    },
    {
      "epoch": 0.00010068848217542401,
      "grad_norm": 10589.896127913626,
      "learning_rate": 6.006683149561772e-07,
      "loss": 1.8191,
      "step": 27744
    },
    {
      "epoch": 0.00010080461652626071,
      "grad_norm": 9776.206421715942,
      "learning_rate": 6.003218587976029e-07,
      "loss": 1.8177,
      "step": 27776
    },
    {
      "epoch": 0.00010092075087709743,
      "grad_norm": 9343.83433072312,
      "learning_rate": 5.99976001439904e-07,
      "loss": 1.8233,
      "step": 27808
    },
    {
      "epoch": 0.00010103688522793413,
      "grad_norm": 10391.749900762625,
      "learning_rate": 5.996415215231645e-07,
      "loss": 1.812,
      "step": 27840
    },
    {
      "epoch": 0.00010115301957877084,
      "grad_norm": 9674.23754101583,
      "learning_rate": 5.992968380262321e-07,
      "loss": 1.8172,
      "step": 27872
    },
    {
      "epoch": 0.00010126915392960754,
      "grad_norm": 9598.452531528194,
      "learning_rate": 5.989527482354798e-07,
      "loss": 1.7834,
      "step": 27904
    },
    {
      "epoch": 0.00010138528828044425,
      "grad_norm": 10634.280229521883,
      "learning_rate": 5.986092504484634e-07,
      "loss": 1.7897,
      "step": 27936
    },
    {
      "epoch": 0.00010150142263128095,
      "grad_norm": 9966.763265975569,
      "learning_rate": 5.982663429695645e-07,
      "loss": 1.7926,
      "step": 27968
    },
    {
      "epoch": 0.00010161755698211767,
      "grad_norm": 10382.707354057515,
      "learning_rate": 5.979240241099565e-07,
      "loss": 1.7965,
      "step": 28000
    },
    {
      "epoch": 0.00010173369133295437,
      "grad_norm": 9271.245655250432,
      "learning_rate": 5.975822921875696e-07,
      "loss": 1.8218,
      "step": 28032
    },
    {
      "epoch": 0.00010184982568379107,
      "grad_norm": 10747.162323143724,
      "learning_rate": 5.972411455270552e-07,
      "loss": 1.8353,
      "step": 28064
    },
    {
      "epoch": 0.00010196596003462778,
      "grad_norm": 10634.958297990643,
      "learning_rate": 5.969005824597525e-07,
      "loss": 1.8348,
      "step": 28096
    },
    {
      "epoch": 0.00010208209438546448,
      "grad_norm": 8977.986411217162,
      "learning_rate": 5.965606013236534e-07,
      "loss": 1.8473,
      "step": 28128
    },
    {
      "epoch": 0.0001021982287363012,
      "grad_norm": 11054.349189346245,
      "learning_rate": 5.96221200463369e-07,
      "loss": 1.8249,
      "step": 28160
    },
    {
      "epoch": 0.0001023143630871379,
      "grad_norm": 10556.71227229387,
      "learning_rate": 5.958823782300952e-07,
      "loss": 1.8434,
      "step": 28192
    },
    {
      "epoch": 0.00010243049743797461,
      "grad_norm": 9464.946698212305,
      "learning_rate": 5.955441329815798e-07,
      "loss": 1.8329,
      "step": 28224
    },
    {
      "epoch": 0.00010254663178881131,
      "grad_norm": 8801.115383859025,
      "learning_rate": 5.952064630820885e-07,
      "loss": 1.8167,
      "step": 28256
    },
    {
      "epoch": 0.00010266276613964802,
      "grad_norm": 9872.487224605562,
      "learning_rate": 5.948693669023713e-07,
      "loss": 1.8057,
      "step": 28288
    },
    {
      "epoch": 0.00010277890049048472,
      "grad_norm": 8041.694411005681,
      "learning_rate": 5.945328428196306e-07,
      "loss": 1.8267,
      "step": 28320
    },
    {
      "epoch": 0.00010289503484132142,
      "grad_norm": 11768.666874374514,
      "learning_rate": 5.941968892174876e-07,
      "loss": 1.7963,
      "step": 28352
    },
    {
      "epoch": 0.00010301116919215813,
      "grad_norm": 9610.858026211812,
      "learning_rate": 5.938615044859497e-07,
      "loss": 1.8149,
      "step": 28384
    },
    {
      "epoch": 0.00010312730354299483,
      "grad_norm": 9430.116648271112,
      "learning_rate": 5.935266870213785e-07,
      "loss": 1.8249,
      "step": 28416
    },
    {
      "epoch": 0.00010324343789383155,
      "grad_norm": 10204.771041037618,
      "learning_rate": 5.931924352264574e-07,
      "loss": 1.8172,
      "step": 28448
    },
    {
      "epoch": 0.00010335957224466825,
      "grad_norm": 9427.780756890776,
      "learning_rate": 5.928587475101592e-07,
      "loss": 1.8094,
      "step": 28480
    },
    {
      "epoch": 0.00010347570659550496,
      "grad_norm": 10505.791831175791,
      "learning_rate": 5.925256222877149e-07,
      "loss": 1.8151,
      "step": 28512
    },
    {
      "epoch": 0.00010359184094634166,
      "grad_norm": 9433.338115428705,
      "learning_rate": 5.921930579805819e-07,
      "loss": 1.8207,
      "step": 28544
    },
    {
      "epoch": 0.00010370797529717837,
      "grad_norm": 10281.159078625327,
      "learning_rate": 5.918610530164123e-07,
      "loss": 1.8038,
      "step": 28576
    },
    {
      "epoch": 0.00010382410964801507,
      "grad_norm": 11015.064593546422,
      "learning_rate": 5.915296058290223e-07,
      "loss": 1.8118,
      "step": 28608
    },
    {
      "epoch": 0.00010394024399885177,
      "grad_norm": 10194.089660190359,
      "learning_rate": 5.9119871485836e-07,
      "loss": 1.8298,
      "step": 28640
    },
    {
      "epoch": 0.00010405637834968849,
      "grad_norm": 9345.202726533009,
      "learning_rate": 5.908683785504763e-07,
      "loss": 1.8165,
      "step": 28672
    },
    {
      "epoch": 0.00010417251270052519,
      "grad_norm": 9563.472277368717,
      "learning_rate": 5.905385953574926e-07,
      "loss": 1.8013,
      "step": 28704
    },
    {
      "epoch": 0.0001042886470513619,
      "grad_norm": 10887.8806018435,
      "learning_rate": 5.902093637375712e-07,
      "loss": 1.8276,
      "step": 28736
    },
    {
      "epoch": 0.0001044047814021986,
      "grad_norm": 10303.87732846233,
      "learning_rate": 5.898806821548847e-07,
      "loss": 1.8277,
      "step": 28768
    },
    {
      "epoch": 0.00010452091575303531,
      "grad_norm": 9883.094454673597,
      "learning_rate": 5.895525490795865e-07,
      "loss": 1.7952,
      "step": 28800
    },
    {
      "epoch": 0.00010463705010387201,
      "grad_norm": 8770.06681844557,
      "learning_rate": 5.892351917887075e-07,
      "loss": 1.8047,
      "step": 28832
    },
    {
      "epoch": 0.00010475318445470873,
      "grad_norm": 9838.825742943109,
      "learning_rate": 5.88908134139538e-07,
      "loss": 1.8124,
      "step": 28864
    },
    {
      "epoch": 0.00010486931880554543,
      "grad_norm": 10311.595123936935,
      "learning_rate": 5.885816204909636e-07,
      "loss": 1.8008,
      "step": 28896
    },
    {
      "epoch": 0.00010498545315638213,
      "grad_norm": 10055.1543001587,
      "learning_rate": 5.882556493365786e-07,
      "loss": 1.8328,
      "step": 28928
    },
    {
      "epoch": 0.00010510158750721884,
      "grad_norm": 10068.699320170406,
      "learning_rate": 5.879302191758113e-07,
      "loss": 1.8252,
      "step": 28960
    },
    {
      "epoch": 0.00010521772185805554,
      "grad_norm": 9545.43545366056,
      "learning_rate": 5.876053285138941e-07,
      "loss": 1.8268,
      "step": 28992
    },
    {
      "epoch": 0.00010533385620889225,
      "grad_norm": 9198.93059001969,
      "learning_rate": 5.872809758618353e-07,
      "loss": 1.8599,
      "step": 29024
    },
    {
      "epoch": 0.00010544999055972895,
      "grad_norm": 11306.784600406962,
      "learning_rate": 5.869571597363899e-07,
      "loss": 1.8207,
      "step": 29056
    },
    {
      "epoch": 0.00010556612491056567,
      "grad_norm": 10713.41327495584,
      "learning_rate": 5.86633878660032e-07,
      "loss": 1.8226,
      "step": 29088
    },
    {
      "epoch": 0.00010568225926140237,
      "grad_norm": 10617.089996792907,
      "learning_rate": 5.863111311609255e-07,
      "loss": 1.7971,
      "step": 29120
    },
    {
      "epoch": 0.00010579839361223907,
      "grad_norm": 11615.44239364132,
      "learning_rate": 5.859889157728963e-07,
      "loss": 1.8049,
      "step": 29152
    },
    {
      "epoch": 0.00010591452796307578,
      "grad_norm": 9863.957116695105,
      "learning_rate": 5.856672310354047e-07,
      "loss": 1.8148,
      "step": 29184
    },
    {
      "epoch": 0.00010603066231391248,
      "grad_norm": 9821.45294750222,
      "learning_rate": 5.853460754935173e-07,
      "loss": 1.8399,
      "step": 29216
    },
    {
      "epoch": 0.0001061467966647492,
      "grad_norm": 9277.485435181237,
      "learning_rate": 5.850254476978789e-07,
      "loss": 1.7856,
      "step": 29248
    },
    {
      "epoch": 0.0001062629310155859,
      "grad_norm": 9352.403541336313,
      "learning_rate": 5.847053462046862e-07,
      "loss": 1.7817,
      "step": 29280
    },
    {
      "epoch": 0.00010637906536642261,
      "grad_norm": 10201.314327085505,
      "learning_rate": 5.843857695756592e-07,
      "loss": 1.8159,
      "step": 29312
    },
    {
      "epoch": 0.00010649519971725931,
      "grad_norm": 10583.878306178694,
      "learning_rate": 5.840667163780147e-07,
      "loss": 1.8299,
      "step": 29344
    },
    {
      "epoch": 0.00010661133406809602,
      "grad_norm": 9847.323291128409,
      "learning_rate": 5.837481851844397e-07,
      "loss": 1.8343,
      "step": 29376
    },
    {
      "epoch": 0.00010672746841893272,
      "grad_norm": 10284.015558136813,
      "learning_rate": 5.834301745730637e-07,
      "loss": 1.8356,
      "step": 29408
    },
    {
      "epoch": 0.00010684360276976942,
      "grad_norm": 7760.81065353356,
      "learning_rate": 5.831126831274325e-07,
      "loss": 1.8142,
      "step": 29440
    },
    {
      "epoch": 0.00010695973712060613,
      "grad_norm": 8680.066704812814,
      "learning_rate": 5.827957094364819e-07,
      "loss": 1.7879,
      "step": 29472
    },
    {
      "epoch": 0.00010707587147144283,
      "grad_norm": 8510.314330270063,
      "learning_rate": 5.82479252094511e-07,
      "loss": 1.7955,
      "step": 29504
    },
    {
      "epoch": 0.00010719200582227955,
      "grad_norm": 10768.028138893398,
      "learning_rate": 5.821633097011562e-07,
      "loss": 1.7994,
      "step": 29536
    },
    {
      "epoch": 0.00010730814017311625,
      "grad_norm": 8783.590951313705,
      "learning_rate": 5.818478808613652e-07,
      "loss": 1.7886,
      "step": 29568
    },
    {
      "epoch": 0.00010742427452395296,
      "grad_norm": 10031.081696407422,
      "learning_rate": 5.815329641853709e-07,
      "loss": 1.799,
      "step": 29600
    },
    {
      "epoch": 0.00010754040887478966,
      "grad_norm": 9586.13926458405,
      "learning_rate": 5.812185582886661e-07,
      "loss": 1.8028,
      "step": 29632
    },
    {
      "epoch": 0.00010765654322562637,
      "grad_norm": 9876.175778103587,
      "learning_rate": 5.809046617919773e-07,
      "loss": 1.8127,
      "step": 29664
    },
    {
      "epoch": 0.00010777267757646307,
      "grad_norm": 10081.437893475315,
      "learning_rate": 5.8059127332124e-07,
      "loss": 1.7909,
      "step": 29696
    },
    {
      "epoch": 0.00010788881192729977,
      "grad_norm": 9867.2303104772,
      "learning_rate": 5.80278391507573e-07,
      "loss": 1.8026,
      "step": 29728
    },
    {
      "epoch": 0.00010800494627813649,
      "grad_norm": 9560.255906616727,
      "learning_rate": 5.799660149872534e-07,
      "loss": 1.7963,
      "step": 29760
    },
    {
      "epoch": 0.00010812108062897319,
      "grad_norm": 9160.12030488683,
      "learning_rate": 5.796541424016917e-07,
      "loss": 1.7912,
      "step": 29792
    },
    {
      "epoch": 0.0001082372149798099,
      "grad_norm": 11495.369415551637,
      "learning_rate": 5.793427723974065e-07,
      "loss": 1.7943,
      "step": 29824
    },
    {
      "epoch": 0.0001083533493306466,
      "grad_norm": 9330.545536033784,
      "learning_rate": 5.790416107014432e-07,
      "loss": 1.7948,
      "step": 29856
    },
    {
      "epoch": 0.00010846948368148331,
      "grad_norm": 10055.99025456966,
      "learning_rate": 5.787312262183146e-07,
      "loss": 1.8185,
      "step": 29888
    },
    {
      "epoch": 0.00010858561803232001,
      "grad_norm": 12463.569633134803,
      "learning_rate": 5.784213403281707e-07,
      "loss": 1.8378,
      "step": 29920
    },
    {
      "epoch": 0.00010870175238315673,
      "grad_norm": 9531.082834599645,
      "learning_rate": 5.781119516975626e-07,
      "loss": 1.8596,
      "step": 29952
    },
    {
      "epoch": 0.00010881788673399343,
      "grad_norm": 8837.851492302867,
      "learning_rate": 5.778030589980296e-07,
      "loss": 1.8576,
      "step": 29984
    },
    {
      "epoch": 0.00010893402108483013,
      "grad_norm": 9628.93649371518,
      "learning_rate": 5.774946609060736e-07,
      "loss": 1.8492,
      "step": 30016
    },
    {
      "epoch": 0.00010905015543566684,
      "grad_norm": 8674.553129700687,
      "learning_rate": 5.771867561031365e-07,
      "loss": 1.8294,
      "step": 30048
    },
    {
      "epoch": 0.00010916628978650354,
      "grad_norm": 10394.689894364334,
      "learning_rate": 5.76879343275576e-07,
      "loss": 1.8455,
      "step": 30080
    },
    {
      "epoch": 0.00010928242413734025,
      "grad_norm": 8721.357405817056,
      "learning_rate": 5.765724211146422e-07,
      "loss": 1.8297,
      "step": 30112
    },
    {
      "epoch": 0.00010939855848817695,
      "grad_norm": 9805.411057166344,
      "learning_rate": 5.762659883164542e-07,
      "loss": 1.8129,
      "step": 30144
    },
    {
      "epoch": 0.00010951469283901367,
      "grad_norm": 10842.715158114226,
      "learning_rate": 5.759600435819766e-07,
      "loss": 1.8165,
      "step": 30176
    },
    {
      "epoch": 0.00010963082718985037,
      "grad_norm": 10060.295224296353,
      "learning_rate": 5.756545856169969e-07,
      "loss": 1.7894,
      "step": 30208
    },
    {
      "epoch": 0.00010974696154068708,
      "grad_norm": 11656.060054752636,
      "learning_rate": 5.753496131321016e-07,
      "loss": 1.794,
      "step": 30240
    },
    {
      "epoch": 0.00010986309589152378,
      "grad_norm": 9521.39044467771,
      "learning_rate": 5.750451248426545e-07,
      "loss": 1.7977,
      "step": 30272
    },
    {
      "epoch": 0.00010997923024236048,
      "grad_norm": 9850.21218045581,
      "learning_rate": 5.747411194687732e-07,
      "loss": 1.7862,
      "step": 30304
    },
    {
      "epoch": 0.0001100953645931972,
      "grad_norm": 11501.547026378668,
      "learning_rate": 5.744375957353065e-07,
      "loss": 1.7906,
      "step": 30336
    },
    {
      "epoch": 0.0001102114989440339,
      "grad_norm": 8614.893382973465,
      "learning_rate": 5.741345523718123e-07,
      "loss": 1.8089,
      "step": 30368
    },
    {
      "epoch": 0.00011032763329487061,
      "grad_norm": 9836.919436490267,
      "learning_rate": 5.738319881125352e-07,
      "loss": 1.7918,
      "step": 30400
    },
    {
      "epoch": 0.00011044376764570731,
      "grad_norm": 9720.766739306113,
      "learning_rate": 5.735299016963839e-07,
      "loss": 1.7975,
      "step": 30432
    },
    {
      "epoch": 0.00011055990199654402,
      "grad_norm": 9746.82327735555,
      "learning_rate": 5.732282918669097e-07,
      "loss": 1.8028,
      "step": 30464
    },
    {
      "epoch": 0.00011067603634738072,
      "grad_norm": 10341.197996363864,
      "learning_rate": 5.729271573722839e-07,
      "loss": 1.8042,
      "step": 30496
    },
    {
      "epoch": 0.00011079217069821742,
      "grad_norm": 11195.446485067043,
      "learning_rate": 5.726264969652766e-07,
      "loss": 1.792,
      "step": 30528
    },
    {
      "epoch": 0.00011090830504905413,
      "grad_norm": 11468.455606575804,
      "learning_rate": 5.723263094032348e-07,
      "loss": 1.8182,
      "step": 30560
    },
    {
      "epoch": 0.00011102443939989083,
      "grad_norm": 10042.38437822413,
      "learning_rate": 5.720265934480606e-07,
      "loss": 1.8064,
      "step": 30592
    },
    {
      "epoch": 0.00011114057375072755,
      "grad_norm": 9493.605321478242,
      "learning_rate": 5.7172734786619e-07,
      "loss": 1.8043,
      "step": 30624
    },
    {
      "epoch": 0.00011125670810156425,
      "grad_norm": 8690.676613474925,
      "learning_rate": 5.714285714285715e-07,
      "loss": 1.8112,
      "step": 30656
    },
    {
      "epoch": 0.00011137284245240096,
      "grad_norm": 10781.732792088664,
      "learning_rate": 5.711302629106447e-07,
      "loss": 1.8281,
      "step": 30688
    },
    {
      "epoch": 0.00011148897680323766,
      "grad_norm": 8434.466432442541,
      "learning_rate": 5.708324210923199e-07,
      "loss": 1.8185,
      "step": 30720
    },
    {
      "epoch": 0.00011160511115407437,
      "grad_norm": 10168.407151565087,
      "learning_rate": 5.705350447579562e-07,
      "loss": 1.8281,
      "step": 30752
    },
    {
      "epoch": 0.00011172124550491107,
      "grad_norm": 8919.43316584636,
      "learning_rate": 5.702381326963412e-07,
      "loss": 1.8406,
      "step": 30784
    },
    {
      "epoch": 0.00011183737985574777,
      "grad_norm": 9213.6746198246,
      "learning_rate": 5.699416837006704e-07,
      "loss": 1.8263,
      "step": 30816
    },
    {
      "epoch": 0.00011195351420658449,
      "grad_norm": 8766.959450117241,
      "learning_rate": 5.696549391872194e-07,
      "loss": 1.8364,
      "step": 30848
    },
    {
      "epoch": 0.00011206964855742119,
      "grad_norm": 11739.940033918401,
      "learning_rate": 5.693593983428462e-07,
      "loss": 1.8431,
      "step": 30880
    },
    {
      "epoch": 0.0001121857829082579,
      "grad_norm": 9159.25040601031,
      "learning_rate": 5.690643170074818e-07,
      "loss": 1.8205,
      "step": 30912
    },
    {
      "epoch": 0.0001123019172590946,
      "grad_norm": 10638.06711766757,
      "learning_rate": 5.687696939916126e-07,
      "loss": 1.8257,
      "step": 30944
    },
    {
      "epoch": 0.00011241805160993131,
      "grad_norm": 10273.723570351694,
      "learning_rate": 5.684755281100316e-07,
      "loss": 1.7919,
      "step": 30976
    },
    {
      "epoch": 0.00011253418596076801,
      "grad_norm": 9909.799997981796,
      "learning_rate": 5.681818181818182e-07,
      "loss": 1.781,
      "step": 31008
    },
    {
      "epoch": 0.00011265032031160473,
      "grad_norm": 9949.925426856224,
      "learning_rate": 5.678885630303183e-07,
      "loss": 1.7824,
      "step": 31040
    },
    {
      "epoch": 0.00011276645466244143,
      "grad_norm": 10411.511609751968,
      "learning_rate": 5.675957614831248e-07,
      "loss": 1.7935,
      "step": 31072
    },
    {
      "epoch": 0.00011288258901327813,
      "grad_norm": 8970.69941531874,
      "learning_rate": 5.673034123720573e-07,
      "loss": 1.7927,
      "step": 31104
    },
    {
      "epoch": 0.00011299872336411484,
      "grad_norm": 10463.243474181416,
      "learning_rate": 5.670115145331432e-07,
      "loss": 1.7998,
      "step": 31136
    },
    {
      "epoch": 0.00011311485771495154,
      "grad_norm": 9645.06599251659,
      "learning_rate": 5.667200668065976e-07,
      "loss": 1.8327,
      "step": 31168
    },
    {
      "epoch": 0.00011323099206578825,
      "grad_norm": 9949.81326457939,
      "learning_rate": 5.664290680368047e-07,
      "loss": 1.8344,
      "step": 31200
    },
    {
      "epoch": 0.00011334712641662495,
      "grad_norm": 10443.76742368385,
      "learning_rate": 5.661385170722979e-07,
      "loss": 1.8079,
      "step": 31232
    },
    {
      "epoch": 0.00011346326076746167,
      "grad_norm": 9768.62487763759,
      "learning_rate": 5.658484127657409e-07,
      "loss": 1.7901,
      "step": 31264
    },
    {
      "epoch": 0.00011357939511829837,
      "grad_norm": 10363.358335983563,
      "learning_rate": 5.655587539739085e-07,
      "loss": 1.7843,
      "step": 31296
    },
    {
      "epoch": 0.00011369552946913508,
      "grad_norm": 8673.214859554673,
      "learning_rate": 5.652695395576682e-07,
      "loss": 1.7807,
      "step": 31328
    },
    {
      "epoch": 0.00011381166381997178,
      "grad_norm": 9922.225254447714,
      "learning_rate": 5.649807683819609e-07,
      "loss": 1.7811,
      "step": 31360
    },
    {
      "epoch": 0.00011392779817080848,
      "grad_norm": 9649.803314057754,
      "learning_rate": 5.646924393157821e-07,
      "loss": 1.7988,
      "step": 31392
    },
    {
      "epoch": 0.0001140439325216452,
      "grad_norm": 9243.401862950674,
      "learning_rate": 5.644045512321636e-07,
      "loss": 1.7973,
      "step": 31424
    },
    {
      "epoch": 0.0001141600668724819,
      "grad_norm": 11036.068684092175,
      "learning_rate": 5.641171030081553e-07,
      "loss": 1.8256,
      "step": 31456
    },
    {
      "epoch": 0.00011427620122331861,
      "grad_norm": 9690.055520996772,
      "learning_rate": 5.638300935248058e-07,
      "loss": 1.8336,
      "step": 31488
    },
    {
      "epoch": 0.00011439233557415531,
      "grad_norm": 9032.625642635701,
      "learning_rate": 5.635435216671452e-07,
      "loss": 1.828,
      "step": 31520
    },
    {
      "epoch": 0.00011450846992499202,
      "grad_norm": 10251.346838342755,
      "learning_rate": 5.632573863241661e-07,
      "loss": 1.8296,
      "step": 31552
    },
    {
      "epoch": 0.00011462460427582872,
      "grad_norm": 10590.269590525068,
      "learning_rate": 5.629716863888063e-07,
      "loss": 1.8232,
      "step": 31584
    },
    {
      "epoch": 0.00011474073862666544,
      "grad_norm": 10748.524922053257,
      "learning_rate": 5.626864207579296e-07,
      "loss": 1.8249,
      "step": 31616
    },
    {
      "epoch": 0.00011485687297750213,
      "grad_norm": 10836.755603039132,
      "learning_rate": 5.624015883323094e-07,
      "loss": 1.8261,
      "step": 31648
    },
    {
      "epoch": 0.00011497300732833883,
      "grad_norm": 9098.921254742234,
      "learning_rate": 5.621171880166099e-07,
      "loss": 1.8233,
      "step": 31680
    },
    {
      "epoch": 0.00011508914167917555,
      "grad_norm": 11223.916606960334,
      "learning_rate": 5.618332187193684e-07,
      "loss": 1.8097,
      "step": 31712
    },
    {
      "epoch": 0.00011520527603001225,
      "grad_norm": 8170.470733072851,
      "learning_rate": 5.615496793529785e-07,
      "loss": 1.8122,
      "step": 31744
    },
    {
      "epoch": 0.00011532141038084896,
      "grad_norm": 11828.51554507158,
      "learning_rate": 5.612665688336716e-07,
      "loss": 1.8242,
      "step": 31776
    },
    {
      "epoch": 0.00011543754473168566,
      "grad_norm": 11255.78162545809,
      "learning_rate": 5.609838860815003e-07,
      "loss": 1.8279,
      "step": 31808
    },
    {
      "epoch": 0.00011555367908252238,
      "grad_norm": 9290.609129653449,
      "learning_rate": 5.607104440741931e-07,
      "loss": 1.7856,
      "step": 31840
    },
    {
      "epoch": 0.00011566981343335907,
      "grad_norm": 9148.542178948514,
      "learning_rate": 5.604286003472598e-07,
      "loss": 1.7806,
      "step": 31872
    },
    {
      "epoch": 0.00011578594778419577,
      "grad_norm": 10851.14980082756,
      "learning_rate": 5.601471812037091e-07,
      "loss": 1.7871,
      "step": 31904
    },
    {
      "epoch": 0.00011590208213503249,
      "grad_norm": 8783.167651821295,
      "learning_rate": 5.598661855785879e-07,
      "loss": 1.7927,
      "step": 31936
    },
    {
      "epoch": 0.00011601821648586919,
      "grad_norm": 8861.202062925775,
      "learning_rate": 5.59585612410679e-07,
      "loss": 1.8091,
      "step": 31968
    },
    {
      "epoch": 0.0001161343508367059,
      "grad_norm": 9985.073159471593,
      "learning_rate": 5.593054606424843e-07,
      "loss": 1.7977,
      "step": 32000
    },
    {
      "epoch": 0.0001162504851875426,
      "grad_norm": 9696.654268354627,
      "learning_rate": 5.59025729220208e-07,
      "loss": 1.7811,
      "step": 32032
    },
    {
      "epoch": 0.00011636661953837932,
      "grad_norm": 10531.521447540237,
      "learning_rate": 5.5874641709374e-07,
      "loss": 1.789,
      "step": 32064
    },
    {
      "epoch": 0.00011648275388921602,
      "grad_norm": 10203.760483272821,
      "learning_rate": 5.584675232166391e-07,
      "loss": 1.8147,
      "step": 32096
    },
    {
      "epoch": 0.00011659888824005273,
      "grad_norm": 9268.749322319598,
      "learning_rate": 5.581890465461167e-07,
      "loss": 1.7855,
      "step": 32128
    },
    {
      "epoch": 0.00011671502259088943,
      "grad_norm": 7885.6871609264335,
      "learning_rate": 5.579109860430209e-07,
      "loss": 1.7932,
      "step": 32160
    },
    {
      "epoch": 0.00011683115694172613,
      "grad_norm": 11367.42327882621,
      "learning_rate": 5.576333406718192e-07,
      "loss": 1.8091,
      "step": 32192
    },
    {
      "epoch": 0.00011694729129256284,
      "grad_norm": 12152.906154496546,
      "learning_rate": 5.573561094005829e-07,
      "loss": 1.8246,
      "step": 32224
    },
    {
      "epoch": 0.00011706342564339954,
      "grad_norm": 10691.624104877612,
      "learning_rate": 5.570792912009714e-07,
      "loss": 1.8142,
      "step": 32256
    },
    {
      "epoch": 0.00011717955999423626,
      "grad_norm": 9662.98101001963,
      "learning_rate": 5.568028850482151e-07,
      "loss": 1.8181,
      "step": 32288
    },
    {
      "epoch": 0.00011729569434507296,
      "grad_norm": 10701.256748625368,
      "learning_rate": 5.565268899211007e-07,
      "loss": 1.834,
      "step": 32320
    },
    {
      "epoch": 0.00011741182869590967,
      "grad_norm": 9266.966925591134,
      "learning_rate": 5.562513048019543e-07,
      "loss": 1.8442,
      "step": 32352
    },
    {
      "epoch": 0.00011752796304674637,
      "grad_norm": 12549.634416986019,
      "learning_rate": 5.559761286766264e-07,
      "loss": 1.8518,
      "step": 32384
    },
    {
      "epoch": 0.00011764409739758308,
      "grad_norm": 11049.114806173387,
      "learning_rate": 5.557013605344756e-07,
      "loss": 1.8337,
      "step": 32416
    },
    {
      "epoch": 0.00011776023174841978,
      "grad_norm": 11342.197494312997,
      "learning_rate": 5.554269993683536e-07,
      "loss": 1.8078,
      "step": 32448
    },
    {
      "epoch": 0.00011787636609925648,
      "grad_norm": 8692.961175571878,
      "learning_rate": 5.551530441745892e-07,
      "loss": 1.7825,
      "step": 32480
    },
    {
      "epoch": 0.0001179925004500932,
      "grad_norm": 10400.918805567131,
      "learning_rate": 5.548794939529733e-07,
      "loss": 1.7939,
      "step": 32512
    },
    {
      "epoch": 0.0001181086348009299,
      "grad_norm": 10179.525529217952,
      "learning_rate": 5.546063477067431e-07,
      "loss": 1.7923,
      "step": 32544
    },
    {
      "epoch": 0.00011822476915176661,
      "grad_norm": 10993.163148066165,
      "learning_rate": 5.543336044425674e-07,
      "loss": 1.7934,
      "step": 32576
    },
    {
      "epoch": 0.00011834090350260331,
      "grad_norm": 9228.172083354319,
      "learning_rate": 5.540612631705309e-07,
      "loss": 1.7943,
      "step": 32608
    },
    {
      "epoch": 0.00011845703785344002,
      "grad_norm": 10436.660193759306,
      "learning_rate": 5.537893229041196e-07,
      "loss": 1.7828,
      "step": 32640
    },
    {
      "epoch": 0.00011857317220427672,
      "grad_norm": 10005.745149662767,
      "learning_rate": 5.535177826602049e-07,
      "loss": 1.8153,
      "step": 32672
    },
    {
      "epoch": 0.00011868930655511344,
      "grad_norm": 10548.689018072342,
      "learning_rate": 5.532466414590304e-07,
      "loss": 1.8111,
      "step": 32704
    },
    {
      "epoch": 0.00011880544090595014,
      "grad_norm": 9772.495177793642,
      "learning_rate": 5.529758983241947e-07,
      "loss": 1.8061,
      "step": 32736
    },
    {
      "epoch": 0.00011892157525678684,
      "grad_norm": 8976.58275737488,
      "learning_rate": 5.527055522826388e-07,
      "loss": 1.7943,
      "step": 32768
    },
    {
      "epoch": 0.00011903770960762355,
      "grad_norm": 9656.50060839847,
      "learning_rate": 5.524356023646303e-07,
      "loss": 1.7719,
      "step": 32800
    },
    {
      "epoch": 0.00011915384395846025,
      "grad_norm": 10163.183261163798,
      "learning_rate": 5.521660476037487e-07,
      "loss": 1.794,
      "step": 32832
    },
    {
      "epoch": 0.00011926997830929696,
      "grad_norm": 9550.67945226935,
      "learning_rate": 5.519052923473255e-07,
      "loss": 1.7799,
      "step": 32864
    },
    {
      "epoch": 0.00011938611266013366,
      "grad_norm": 9671.507535022656,
      "learning_rate": 5.516365127405419e-07,
      "loss": 1.7894,
      "step": 32896
    },
    {
      "epoch": 0.00011950224701097038,
      "grad_norm": 10049.430730145863,
      "learning_rate": 5.51368125441196e-07,
      "loss": 1.8324,
      "step": 32928
    },
    {
      "epoch": 0.00011961838136180708,
      "grad_norm": 9547.929409039427,
      "learning_rate": 5.511001294958703e-07,
      "loss": 1.8449,
      "step": 32960
    },
    {
      "epoch": 0.00011973451571264378,
      "grad_norm": 10570.067265632702,
      "learning_rate": 5.508325239543881e-07,
      "loss": 1.8204,
      "step": 32992
    },
    {
      "epoch": 0.00011985065006348049,
      "grad_norm": 9558.26009271562,
      "learning_rate": 5.50565307869799e-07,
      "loss": 1.8326,
      "step": 33024
    },
    {
      "epoch": 0.00011996678441431719,
      "grad_norm": 9977.276983225433,
      "learning_rate": 5.502984802983655e-07,
      "loss": 1.8118,
      "step": 33056
    },
    {
      "epoch": 0.0001200829187651539,
      "grad_norm": 8847.00101729394,
      "learning_rate": 5.500320402995483e-07,
      "loss": 1.809,
      "step": 33088
    },
    {
      "epoch": 0.0001201990531159906,
      "grad_norm": 9641.611483564353,
      "learning_rate": 5.49765986935993e-07,
      "loss": 1.8127,
      "step": 33120
    },
    {
      "epoch": 0.00012031518746682732,
      "grad_norm": 10371.755299851613,
      "learning_rate": 5.495003192735157e-07,
      "loss": 1.8181,
      "step": 33152
    },
    {
      "epoch": 0.00012043132181766402,
      "grad_norm": 8656.88696934412,
      "learning_rate": 5.492350363810898e-07,
      "loss": 1.8057,
      "step": 33184
    },
    {
      "epoch": 0.00012054745616850073,
      "grad_norm": 9893.605106330047,
      "learning_rate": 5.489701373308314e-07,
      "loss": 1.8099,
      "step": 33216
    },
    {
      "epoch": 0.00012066359051933743,
      "grad_norm": 8800.088635917255,
      "learning_rate": 5.487056211979868e-07,
      "loss": 1.7805,
      "step": 33248
    },
    {
      "epoch": 0.00012077972487017413,
      "grad_norm": 8772.321471537623,
      "learning_rate": 5.484414870609185e-07,
      "loss": 1.7914,
      "step": 33280
    },
    {
      "epoch": 0.00012089585922101084,
      "grad_norm": 9293.978265522252,
      "learning_rate": 5.481777340010908e-07,
      "loss": 1.8055,
      "step": 33312
    },
    {
      "epoch": 0.00012101199357184754,
      "grad_norm": 10624.6000395309,
      "learning_rate": 5.479143611030578e-07,
      "loss": 1.8121,
      "step": 33344
    },
    {
      "epoch": 0.00012112812792268426,
      "grad_norm": 9534.01379273179,
      "learning_rate": 5.476513674544496e-07,
      "loss": 1.7911,
      "step": 33376
    },
    {
      "epoch": 0.00012124426227352096,
      "grad_norm": 10173.44798974271,
      "learning_rate": 5.473887521459583e-07,
      "loss": 1.7846,
      "step": 33408
    },
    {
      "epoch": 0.00012136039662435767,
      "grad_norm": 10246.452068887063,
      "learning_rate": 5.471265142713258e-07,
      "loss": 1.8124,
      "step": 33440
    },
    {
      "epoch": 0.00012147653097519437,
      "grad_norm": 10662.671710223472,
      "learning_rate": 5.4686465292733e-07,
      "loss": 1.8128,
      "step": 33472
    },
    {
      "epoch": 0.00012159266532603108,
      "grad_norm": 12042.39893044571,
      "learning_rate": 5.466031672137719e-07,
      "loss": 1.8252,
      "step": 33504
    },
    {
      "epoch": 0.00012170879967686778,
      "grad_norm": 9873.41612614398,
      "learning_rate": 5.46342056233463e-07,
      "loss": 1.8331,
      "step": 33536
    },
    {
      "epoch": 0.00012182493402770448,
      "grad_norm": 10790.770315413076,
      "learning_rate": 5.460813190922116e-07,
      "loss": 1.7804,
      "step": 33568
    },
    {
      "epoch": 0.0001219410683785412,
      "grad_norm": 9059.212990100188,
      "learning_rate": 5.458209548988108e-07,
      "loss": 1.7938,
      "step": 33600
    },
    {
      "epoch": 0.0001220572027293779,
      "grad_norm": 9084.657616002927,
      "learning_rate": 5.455609627650249e-07,
      "loss": 1.7938,
      "step": 33632
    },
    {
      "epoch": 0.0001221733370802146,
      "grad_norm": 9513.983182663294,
      "learning_rate": 5.453013418055771e-07,
      "loss": 1.7886,
      "step": 33664
    },
    {
      "epoch": 0.00012228947143105132,
      "grad_norm": 9836.94169953243,
      "learning_rate": 5.450420911381371e-07,
      "loss": 1.8054,
      "step": 33696
    },
    {
      "epoch": 0.00012240560578188802,
      "grad_norm": 9729.546443694075,
      "learning_rate": 5.44783209883308e-07,
      "loss": 1.8183,
      "step": 33728
    },
    {
      "epoch": 0.00012252174013272472,
      "grad_norm": 8775.219541413195,
      "learning_rate": 5.445246971646138e-07,
      "loss": 1.8105,
      "step": 33760
    },
    {
      "epoch": 0.00012263787448356142,
      "grad_norm": 9295.936531624988,
      "learning_rate": 5.442665521084873e-07,
      "loss": 1.8188,
      "step": 33792
    },
    {
      "epoch": 0.00012275400883439812,
      "grad_norm": 10940.787266006044,
      "learning_rate": 5.440087738442574e-07,
      "loss": 1.8157,
      "step": 33824
    },
    {
      "epoch": 0.00012287014318523485,
      "grad_norm": 10552.453648322746,
      "learning_rate": 5.437594001094744e-07,
      "loss": 1.8052,
      "step": 33856
    },
    {
      "epoch": 0.00012298627753607155,
      "grad_norm": 9902.140778639738,
      "learning_rate": 5.435023414335118e-07,
      "loss": 1.8144,
      "step": 33888
    },
    {
      "epoch": 0.00012310241188690825,
      "grad_norm": 10707.862718582079,
      "learning_rate": 5.432456469815829e-07,
      "loss": 1.8204,
      "step": 33920
    },
    {
      "epoch": 0.00012321854623774495,
      "grad_norm": 10494.653877093802,
      "learning_rate": 5.429893158943901e-07,
      "loss": 1.8204,
      "step": 33952
    },
    {
      "epoch": 0.00012333468058858168,
      "grad_norm": 11241.862923910789,
      "learning_rate": 5.427333473154712e-07,
      "loss": 1.7875,
      "step": 33984
    },
    {
      "epoch": 0.00012345081493941838,
      "grad_norm": 9081.825587402567,
      "learning_rate": 5.424777403911877e-07,
      "loss": 1.7832,
      "step": 34016
    },
    {
      "epoch": 0.00012356694929025508,
      "grad_norm": 9423.152869395679,
      "learning_rate": 5.422224942707123e-07,
      "loss": 1.7953,
      "step": 34048
    },
    {
      "epoch": 0.00012368308364109178,
      "grad_norm": 9773.522701666989,
      "learning_rate": 5.419676081060178e-07,
      "loss": 1.8032,
      "step": 34080
    },
    {
      "epoch": 0.00012379921799192848,
      "grad_norm": 8816.419794905414,
      "learning_rate": 5.417130810518645e-07,
      "loss": 1.8157,
      "step": 34112
    },
    {
      "epoch": 0.0001239153523427652,
      "grad_norm": 8872.46707517137,
      "learning_rate": 5.41458912265789e-07,
      "loss": 1.8096,
      "step": 34144
    },
    {
      "epoch": 0.0001240314866936019,
      "grad_norm": 8920.463328773903,
      "learning_rate": 5.412051009080921e-07,
      "loss": 1.7865,
      "step": 34176
    },
    {
      "epoch": 0.0001241476210444386,
      "grad_norm": 9532.572580368847,
      "learning_rate": 5.409516461418276e-07,
      "loss": 1.8067,
      "step": 34208
    },
    {
      "epoch": 0.0001242637553952753,
      "grad_norm": 9504.629503562988,
      "learning_rate": 5.406985471327902e-07,
      "loss": 1.823,
      "step": 34240
    },
    {
      "epoch": 0.00012437988974611203,
      "grad_norm": 9704.495659229284,
      "learning_rate": 5.404458030495039e-07,
      "loss": 1.8136,
      "step": 34272
    },
    {
      "epoch": 0.00012449602409694873,
      "grad_norm": 9979.85090068985,
      "learning_rate": 5.401934130632113e-07,
      "loss": 1.7986,
      "step": 34304
    },
    {
      "epoch": 0.00012461215844778543,
      "grad_norm": 9182.268565011589,
      "learning_rate": 5.399413763478615e-07,
      "loss": 1.783,
      "step": 34336
    },
    {
      "epoch": 0.00012472829279862213,
      "grad_norm": 10230.90738888785,
      "learning_rate": 5.39689692080099e-07,
      "loss": 1.8075,
      "step": 34368
    },
    {
      "epoch": 0.00012484442714945883,
      "grad_norm": 11646.107504226466,
      "learning_rate": 5.394383594392522e-07,
      "loss": 1.7869,
      "step": 34400
    },
    {
      "epoch": 0.00012496056150029556,
      "grad_norm": 9571.761593353649,
      "learning_rate": 5.391873776073225e-07,
      "loss": 1.7882,
      "step": 34432
    },
    {
      "epoch": 0.00012507669585113226,
      "grad_norm": 9274.247139256102,
      "learning_rate": 5.389367457689729e-07,
      "loss": 1.8004,
      "step": 34464
    },
    {
      "epoch": 0.00012519283020196896,
      "grad_norm": 9723.652194520328,
      "learning_rate": 5.386864631115168e-07,
      "loss": 1.8163,
      "step": 34496
    },
    {
      "epoch": 0.00012530896455280566,
      "grad_norm": 9197.315804081101,
      "learning_rate": 5.384365288249069e-07,
      "loss": 1.8153,
      "step": 34528
    },
    {
      "epoch": 0.00012542509890364238,
      "grad_norm": 9670.798260743524,
      "learning_rate": 5.381869421017248e-07,
      "loss": 1.8326,
      "step": 34560
    },
    {
      "epoch": 0.00012554123325447908,
      "grad_norm": 9104.74250047743,
      "learning_rate": 5.37937702137169e-07,
      "loss": 1.8039,
      "step": 34592
    },
    {
      "epoch": 0.00012565736760531578,
      "grad_norm": 10501.331629845807,
      "learning_rate": 5.37688808129045e-07,
      "loss": 1.8222,
      "step": 34624
    },
    {
      "epoch": 0.00012577350195615248,
      "grad_norm": 9026.963387540685,
      "learning_rate": 5.374402592777538e-07,
      "loss": 1.8342,
      "step": 34656
    },
    {
      "epoch": 0.00012588963630698918,
      "grad_norm": 10691.265032726482,
      "learning_rate": 5.371920547862814e-07,
      "loss": 1.8403,
      "step": 34688
    },
    {
      "epoch": 0.0001260057706578259,
      "grad_norm": 8968.398073234706,
      "learning_rate": 5.369441938601881e-07,
      "loss": 1.8208,
      "step": 34720
    },
    {
      "epoch": 0.0001261219050086626,
      "grad_norm": 9965.9238407686,
      "learning_rate": 5.366966757075974e-07,
      "loss": 1.7938,
      "step": 34752
    },
    {
      "epoch": 0.0001262380393594993,
      "grad_norm": 7794.61827160253,
      "learning_rate": 5.364494995391862e-07,
      "loss": 1.766,
      "step": 34784
    },
    {
      "epoch": 0.000126354173710336,
      "grad_norm": 11785.600875644823,
      "learning_rate": 5.362026645681733e-07,
      "loss": 1.7832,
      "step": 34816
    },
    {
      "epoch": 0.00012647030806117274,
      "grad_norm": 10136.13239850388,
      "learning_rate": 5.359638678202763e-07,
      "loss": 1.7912,
      "step": 34848
    },
    {
      "epoch": 0.00012658644241200944,
      "grad_norm": 10028.032808083548,
      "learning_rate": 5.357177022921594e-07,
      "loss": 1.7986,
      "step": 34880
    },
    {
      "epoch": 0.00012670257676284614,
      "grad_norm": 9763.269227057093,
      "learning_rate": 5.354718756405593e-07,
      "loss": 1.7873,
      "step": 34912
    },
    {
      "epoch": 0.00012681871111368284,
      "grad_norm": 8561.631094598739,
      "learning_rate": 5.352263870886816e-07,
      "loss": 1.7864,
      "step": 34944
    },
    {
      "epoch": 0.00012693484546451954,
      "grad_norm": 9396.92013374595,
      "learning_rate": 5.349812358622224e-07,
      "loss": 1.8073,
      "step": 34976
    },
    {
      "epoch": 0.00012705097981535626,
      "grad_norm": 10369.76836771198,
      "learning_rate": 5.347364211893584e-07,
      "loss": 1.7988,
      "step": 35008
    },
    {
      "epoch": 0.00012716711416619296,
      "grad_norm": 8767.541616667697,
      "learning_rate": 5.34491942300736e-07,
      "loss": 1.8077,
      "step": 35040
    },
    {
      "epoch": 0.00012728324851702966,
      "grad_norm": 10053.234205965759,
      "learning_rate": 5.342477984294618e-07,
      "loss": 1.7877,
      "step": 35072
    },
    {
      "epoch": 0.00012739938286786636,
      "grad_norm": 9637.1439752657,
      "learning_rate": 5.340039888110923e-07,
      "loss": 1.7836,
      "step": 35104
    },
    {
      "epoch": 0.0001275155172187031,
      "grad_norm": 10628.680915334697,
      "learning_rate": 5.337605126836239e-07,
      "loss": 1.7993,
      "step": 35136
    },
    {
      "epoch": 0.0001276316515695398,
      "grad_norm": 10524.607356096472,
      "learning_rate": 5.335173692874823e-07,
      "loss": 1.8044,
      "step": 35168
    },
    {
      "epoch": 0.0001277477859203765,
      "grad_norm": 9187.142754959237,
      "learning_rate": 5.332745578655137e-07,
      "loss": 1.8083,
      "step": 35200
    },
    {
      "epoch": 0.0001278639202712132,
      "grad_norm": 11053.249476963778,
      "learning_rate": 5.330320776629739e-07,
      "loss": 1.8321,
      "step": 35232
    },
    {
      "epoch": 0.0001279800546220499,
      "grad_norm": 8618.088303098317,
      "learning_rate": 5.327899279275186e-07,
      "loss": 1.8352,
      "step": 35264
    },
    {
      "epoch": 0.00012809618897288662,
      "grad_norm": 8650.504378358524,
      "learning_rate": 5.325481079091945e-07,
      "loss": 1.8271,
      "step": 35296
    },
    {
      "epoch": 0.00012821232332372332,
      "grad_norm": 9901.661274755868,
      "learning_rate": 5.323066168604281e-07,
      "loss": 1.8172,
      "step": 35328
    },
    {
      "epoch": 0.00012832845767456002,
      "grad_norm": 9310.255904109188,
      "learning_rate": 5.32065454036017e-07,
      "loss": 1.8042,
      "step": 35360
    },
    {
      "epoch": 0.00012844459202539672,
      "grad_norm": 9143.634835228275,
      "learning_rate": 5.318246186931198e-07,
      "loss": 1.8033,
      "step": 35392
    },
    {
      "epoch": 0.00012856072637623344,
      "grad_norm": 8824.745322104201,
      "learning_rate": 5.315841100912473e-07,
      "loss": 1.8146,
      "step": 35424
    },
    {
      "epoch": 0.00012867686072707014,
      "grad_norm": 10104.371628161744,
      "learning_rate": 5.313439274922516e-07,
      "loss": 1.8217,
      "step": 35456
    },
    {
      "epoch": 0.00012879299507790684,
      "grad_norm": 9235.457108340657,
      "learning_rate": 5.311040701603173e-07,
      "loss": 1.8263,
      "step": 35488
    },
    {
      "epoch": 0.00012890912942874354,
      "grad_norm": 10607.588604390727,
      "learning_rate": 5.308645373619525e-07,
      "loss": 1.7899,
      "step": 35520
    },
    {
      "epoch": 0.00012902526377958024,
      "grad_norm": 11341.193587978296,
      "learning_rate": 5.306253283659786e-07,
      "loss": 1.7765,
      "step": 35552
    },
    {
      "epoch": 0.00012914139813041697,
      "grad_norm": 10145.023607661049,
      "learning_rate": 5.303864424435213e-07,
      "loss": 1.7763,
      "step": 35584
    },
    {
      "epoch": 0.00012925753248125367,
      "grad_norm": 9289.70607715874,
      "learning_rate": 5.30147878868001e-07,
      "loss": 1.787,
      "step": 35616
    },
    {
      "epoch": 0.00012937366683209037,
      "grad_norm": 9923.036833550503,
      "learning_rate": 5.299096369151238e-07,
      "loss": 1.7826,
      "step": 35648
    },
    {
      "epoch": 0.00012948980118292707,
      "grad_norm": 10214.920655590038,
      "learning_rate": 5.296717158628724e-07,
      "loss": 1.7813,
      "step": 35680
    },
    {
      "epoch": 0.0001296059355337638,
      "grad_norm": 9477.113695635397,
      "learning_rate": 5.294341149914966e-07,
      "loss": 1.7779,
      "step": 35712
    },
    {
      "epoch": 0.0001297220698846005,
      "grad_norm": 10719.827890409435,
      "learning_rate": 5.29196833583504e-07,
      "loss": 1.804,
      "step": 35744
    },
    {
      "epoch": 0.0001298382042354372,
      "grad_norm": 9178.578212337683,
      "learning_rate": 5.289598709236513e-07,
      "loss": 1.8221,
      "step": 35776
    },
    {
      "epoch": 0.0001299543385862739,
      "grad_norm": 9756.75632574679,
      "learning_rate": 5.28723226298935e-07,
      "loss": 1.8139,
      "step": 35808
    },
    {
      "epoch": 0.0001300704729371106,
      "grad_norm": 10201.26737224351,
      "learning_rate": 5.284868989985828e-07,
      "loss": 1.797,
      "step": 35840
    },
    {
      "epoch": 0.00013018660728794732,
      "grad_norm": 9764.11060977906,
      "learning_rate": 5.282582588624333e-07,
      "loss": 1.811,
      "step": 35872
    },
    {
      "epoch": 0.00013030274163878402,
      "grad_norm": 9763.852825601172,
      "learning_rate": 5.280225542258686e-07,
      "loss": 1.8111,
      "step": 35904
    },
    {
      "epoch": 0.00013041887598962072,
      "grad_norm": 9507.237032913401,
      "learning_rate": 5.27787164816617e-07,
      "loss": 1.7874,
      "step": 35936
    },
    {
      "epoch": 0.00013053501034045742,
      "grad_norm": 10122.094743678308,
      "learning_rate": 5.275520899326718e-07,
      "loss": 1.8122,
      "step": 35968
    },
    {
      "epoch": 0.00013065114469129415,
      "grad_norm": 9845.089029561896,
      "learning_rate": 5.273173288742128e-07,
      "loss": 1.8148,
      "step": 36000
    },
    {
      "epoch": 0.00013076727904213085,
      "grad_norm": 9872.12905102035,
      "learning_rate": 5.270828809435982e-07,
      "loss": 1.8124,
      "step": 36032
    },
    {
      "epoch": 0.00013088341339296755,
      "grad_norm": 10383.39193134883,
      "learning_rate": 5.268487454453553e-07,
      "loss": 1.8386,
      "step": 36064
    },
    {
      "epoch": 0.00013099954774380425,
      "grad_norm": 10928.88750056473,
      "learning_rate": 5.266149216861721e-07,
      "loss": 1.8496,
      "step": 36096
    },
    {
      "epoch": 0.00013111568209464095,
      "grad_norm": 10804.480366958885,
      "learning_rate": 5.263814089748883e-07,
      "loss": 1.8061,
      "step": 36128
    },
    {
      "epoch": 0.00013123181644547768,
      "grad_norm": 9299.121786491454,
      "learning_rate": 5.261482066224877e-07,
      "loss": 1.8061,
      "step": 36160
    },
    {
      "epoch": 0.00013134795079631438,
      "grad_norm": 10260.993031865873,
      "learning_rate": 5.259153139420881e-07,
      "loss": 1.8105,
      "step": 36192
    },
    {
      "epoch": 0.00013146408514715108,
      "grad_norm": 9651.759839531856,
      "learning_rate": 5.256827302489347e-07,
      "loss": 1.798,
      "step": 36224
    },
    {
      "epoch": 0.00013158021949798778,
      "grad_norm": 9012.44572799193,
      "learning_rate": 5.254504548603896e-07,
      "loss": 1.8008,
      "step": 36256
    },
    {
      "epoch": 0.0001316963538488245,
      "grad_norm": 10047.174926316353,
      "learning_rate": 5.252184870959254e-07,
      "loss": 1.7734,
      "step": 36288
    },
    {
      "epoch": 0.0001318124881996612,
      "grad_norm": 8267.63146251694,
      "learning_rate": 5.249868262771151e-07,
      "loss": 1.777,
      "step": 36320
    },
    {
      "epoch": 0.0001319286225504979,
      "grad_norm": 8835.988456307534,
      "learning_rate": 5.247554717276248e-07,
      "loss": 1.7944,
      "step": 36352
    },
    {
      "epoch": 0.0001320447569013346,
      "grad_norm": 10059.45058141845,
      "learning_rate": 5.245244227732052e-07,
      "loss": 1.8259,
      "step": 36384
    },
    {
      "epoch": 0.0001321608912521713,
      "grad_norm": 10215.203081681733,
      "learning_rate": 5.242936787416832e-07,
      "loss": 1.7902,
      "step": 36416
    },
    {
      "epoch": 0.00013227702560300803,
      "grad_norm": 10589.26409152213,
      "learning_rate": 5.240632389629537e-07,
      "loss": 1.7939,
      "step": 36448
    },
    {
      "epoch": 0.00013239315995384473,
      "grad_norm": 9930.997130197953,
      "learning_rate": 5.238331027689714e-07,
      "loss": 1.8072,
      "step": 36480
    },
    {
      "epoch": 0.00013250929430468143,
      "grad_norm": 9522.016593138242,
      "learning_rate": 5.236032694937432e-07,
      "loss": 1.8057,
      "step": 36512
    },
    {
      "epoch": 0.00013262542865551813,
      "grad_norm": 9539.63081046641,
      "learning_rate": 5.233737384733188e-07,
      "loss": 1.7882,
      "step": 36544
    },
    {
      "epoch": 0.00013274156300635483,
      "grad_norm": 10920.089102200585,
      "learning_rate": 5.231445090457846e-07,
      "loss": 1.7872,
      "step": 36576
    },
    {
      "epoch": 0.00013285769735719156,
      "grad_norm": 9876.935962129146,
      "learning_rate": 5.229155805512536e-07,
      "loss": 1.7694,
      "step": 36608
    },
    {
      "epoch": 0.00013297383170802826,
      "grad_norm": 8165.817656548546,
      "learning_rate": 5.226869523318588e-07,
      "loss": 1.7909,
      "step": 36640
    },
    {
      "epoch": 0.00013308996605886496,
      "grad_norm": 10663.324059597928,
      "learning_rate": 5.224586237317451e-07,
      "loss": 1.8105,
      "step": 36672
    },
    {
      "epoch": 0.00013320610040970166,
      "grad_norm": 8540.328916382554,
      "learning_rate": 5.222305940970607e-07,
      "loss": 1.8111,
      "step": 36704
    },
    {
      "epoch": 0.00013332223476053838,
      "grad_norm": 9084.050087928841,
      "learning_rate": 5.220028627759501e-07,
      "loss": 1.8036,
      "step": 36736
    },
    {
      "epoch": 0.00013343836911137508,
      "grad_norm": 9954.136426631896,
      "learning_rate": 5.217754291185455e-07,
      "loss": 1.8101,
      "step": 36768
    },
    {
      "epoch": 0.00013355450346221178,
      "grad_norm": 10751.017533238424,
      "learning_rate": 5.215482924769597e-07,
      "loss": 1.8231,
      "step": 36800
    },
    {
      "epoch": 0.00013367063781304848,
      "grad_norm": 10517.416602949605,
      "learning_rate": 5.213214522052779e-07,
      "loss": 1.8134,
      "step": 36832
    },
    {
      "epoch": 0.00013378677216388518,
      "grad_norm": 10342.300034325053,
      "learning_rate": 5.211019827066655e-07,
      "loss": 1.8096,
      "step": 36864
    },
    {
      "epoch": 0.0001339029065147219,
      "grad_norm": 10278.419528312706,
      "learning_rate": 5.208757240332232e-07,
      "loss": 1.8139,
      "step": 36896
    },
    {
      "epoch": 0.0001340190408655586,
      "grad_norm": 10677.809419539197,
      "learning_rate": 5.206497598236704e-07,
      "loss": 1.8159,
      "step": 36928
    },
    {
      "epoch": 0.0001341351752163953,
      "grad_norm": 9587.099352776104,
      "learning_rate": 5.204240894398452e-07,
      "loss": 1.8381,
      "step": 36960
    },
    {
      "epoch": 0.000134251309567232,
      "grad_norm": 9781.598540116027,
      "learning_rate": 5.201987122455202e-07,
      "loss": 1.831,
      "step": 36992
    },
    {
      "epoch": 0.00013436744391806874,
      "grad_norm": 9405.350392196986,
      "learning_rate": 5.199736276063948e-07,
      "loss": 1.8147,
      "step": 37024
    },
    {
      "epoch": 0.00013448357826890544,
      "grad_norm": 8526.987041153516,
      "learning_rate": 5.19748834890088e-07,
      "loss": 1.791,
      "step": 37056
    },
    {
      "epoch": 0.00013459971261974214,
      "grad_norm": 10087.934377264753,
      "learning_rate": 5.195243334661312e-07,
      "loss": 1.786,
      "step": 37088
    },
    {
      "epoch": 0.00013471584697057884,
      "grad_norm": 11440.525774631164,
      "learning_rate": 5.193001227059598e-07,
      "loss": 1.7845,
      "step": 37120
    },
    {
      "epoch": 0.00013483198132141554,
      "grad_norm": 9066.434028878168,
      "learning_rate": 5.190762019829071e-07,
      "loss": 1.7771,
      "step": 37152
    },
    {
      "epoch": 0.00013494811567225226,
      "grad_norm": 8858.133099022614,
      "learning_rate": 5.188525706721958e-07,
      "loss": 1.774,
      "step": 37184
    },
    {
      "epoch": 0.00013506425002308896,
      "grad_norm": 9525.947092021876,
      "learning_rate": 5.186292281509316e-07,
      "loss": 1.7846,
      "step": 37216
    },
    {
      "epoch": 0.00013518038437392566,
      "grad_norm": 10042.790050578575,
      "learning_rate": 5.18406173798095e-07,
      "loss": 1.8192,
      "step": 37248
    },
    {
      "epoch": 0.00013529651872476236,
      "grad_norm": 10213.965733249745,
      "learning_rate": 5.18183406994535e-07,
      "loss": 1.8017,
      "step": 37280
    },
    {
      "epoch": 0.0001354126530755991,
      "grad_norm": 9189.48834266631,
      "learning_rate": 5.179609271229609e-07,
      "loss": 1.8129,
      "step": 37312
    },
    {
      "epoch": 0.0001355287874264358,
      "grad_norm": 10606.450395867601,
      "learning_rate": 5.177387335679361e-07,
      "loss": 1.7714,
      "step": 37344
    },
    {
      "epoch": 0.0001356449217772725,
      "grad_norm": 8959.763724563276,
      "learning_rate": 5.175168257158703e-07,
      "loss": 1.7747,
      "step": 37376
    },
    {
      "epoch": 0.0001357610561281092,
      "grad_norm": 9045.634416667523,
      "learning_rate": 5.172952029550126e-07,
      "loss": 1.7751,
      "step": 37408
    },
    {
      "epoch": 0.0001358771904789459,
      "grad_norm": 9448.961424410621,
      "learning_rate": 5.17073864675444e-07,
      "loss": 1.7735,
      "step": 37440
    },
    {
      "epoch": 0.00013599332482978262,
      "grad_norm": 9154.839594444024,
      "learning_rate": 5.168528102690716e-07,
      "loss": 1.7962,
      "step": 37472
    },
    {
      "epoch": 0.00013610945918061932,
      "grad_norm": 9757.293989626427,
      "learning_rate": 5.166320391296196e-07,
      "loss": 1.8143,
      "step": 37504
    },
    {
      "epoch": 0.00013622559353145602,
      "grad_norm": 9583.803942068096,
      "learning_rate": 5.164115506526242e-07,
      "loss": 1.8156,
      "step": 37536
    },
    {
      "epoch": 0.00013634172788229272,
      "grad_norm": 10468.593792864445,
      "learning_rate": 5.161913442354258e-07,
      "loss": 1.8221,
      "step": 37568
    },
    {
      "epoch": 0.00013645786223312944,
      "grad_norm": 10525.196150191216,
      "learning_rate": 5.159714192771618e-07,
      "loss": 1.8399,
      "step": 37600
    },
    {
      "epoch": 0.00013657399658396614,
      "grad_norm": 9312.44897972601,
      "learning_rate": 5.157517751787604e-07,
      "loss": 1.8475,
      "step": 37632
    },
    {
      "epoch": 0.00013669013093480284,
      "grad_norm": 10995.919788721632,
      "learning_rate": 5.155324113429333e-07,
      "loss": 1.8329,
      "step": 37664
    },
    {
      "epoch": 0.00013680626528563954,
      "grad_norm": 9409.946120993467,
      "learning_rate": 5.153133271741687e-07,
      "loss": 1.8158,
      "step": 37696
    },
    {
      "epoch": 0.00013692239963647624,
      "grad_norm": 9624.85573917864,
      "learning_rate": 5.150945220787257e-07,
      "loss": 1.816,
      "step": 37728
    },
    {
      "epoch": 0.00013703853398731297,
      "grad_norm": 9577.610557962775,
      "learning_rate": 5.148759954646255e-07,
      "loss": 1.8064,
      "step": 37760
    },
    {
      "epoch": 0.00013715466833814967,
      "grad_norm": 9115.252272976322,
      "learning_rate": 5.146577467416465e-07,
      "loss": 1.7828,
      "step": 37792
    },
    {
      "epoch": 0.00013727080268898637,
      "grad_norm": 10480.795198838683,
      "learning_rate": 5.144397753213169e-07,
      "loss": 1.8067,
      "step": 37824
    },
    {
      "epoch": 0.00013738693703982307,
      "grad_norm": 9934.34164904751,
      "learning_rate": 5.14228879393665e-07,
      "loss": 1.7747,
      "step": 37856
    },
    {
      "epoch": 0.0001375030713906598,
      "grad_norm": 10789.343538881316,
      "learning_rate": 5.140114521999305e-07,
      "loss": 1.7872,
      "step": 37888
    },
    {
      "epoch": 0.0001376192057414965,
      "grad_norm": 8397.037691948275,
      "learning_rate": 5.137943005720629e-07,
      "loss": 1.7958,
      "step": 37920
    },
    {
      "epoch": 0.0001377353400923332,
      "grad_norm": 10017.169560309938,
      "learning_rate": 5.135774239284693e-07,
      "loss": 1.7707,
      "step": 37952
    },
    {
      "epoch": 0.0001378514744431699,
      "grad_norm": 11404.494552587588,
      "learning_rate": 5.133608216892743e-07,
      "loss": 1.7849,
      "step": 37984
    },
    {
      "epoch": 0.0001379676087940066,
      "grad_norm": 9464.45434243306,
      "learning_rate": 5.131444932763126e-07,
      "loss": 1.7922,
      "step": 38016
    },
    {
      "epoch": 0.00013808374314484332,
      "grad_norm": 9724.235085599279,
      "learning_rate": 5.12928438113123e-07,
      "loss": 1.798,
      "step": 38048
    },
    {
      "epoch": 0.00013819987749568002,
      "grad_norm": 10962.019704415789,
      "learning_rate": 5.127126556249418e-07,
      "loss": 1.7995,
      "step": 38080
    },
    {
      "epoch": 0.00013831601184651672,
      "grad_norm": 10222.165621823979,
      "learning_rate": 5.124971452386966e-07,
      "loss": 1.788,
      "step": 38112
    },
    {
      "epoch": 0.00013843214619735342,
      "grad_norm": 9690.273886738185,
      "learning_rate": 5.122819063829999e-07,
      "loss": 1.7788,
      "step": 38144
    },
    {
      "epoch": 0.00013854828054819015,
      "grad_norm": 9930.969136997657,
      "learning_rate": 5.120669384881421e-07,
      "loss": 1.7896,
      "step": 38176
    },
    {
      "epoch": 0.00013866441489902685,
      "grad_norm": 9068.446724770456,
      "learning_rate": 5.118522409860862e-07,
      "loss": 1.8128,
      "step": 38208
    },
    {
      "epoch": 0.00013878054924986355,
      "grad_norm": 10765.338266863702,
      "learning_rate": 5.116378133104606e-07,
      "loss": 1.8326,
      "step": 38240
    },
    {
      "epoch": 0.00013889668360070025,
      "grad_norm": 9745.511582261857,
      "learning_rate": 5.114236548965533e-07,
      "loss": 1.8047,
      "step": 38272
    },
    {
      "epoch": 0.00013901281795153695,
      "grad_norm": 10723.35637755269,
      "learning_rate": 5.112097651813055e-07,
      "loss": 1.7938,
      "step": 38304
    },
    {
      "epoch": 0.00013912895230237368,
      "grad_norm": 8198.645619856978,
      "learning_rate": 5.109961436033055e-07,
      "loss": 1.8162,
      "step": 38336
    },
    {
      "epoch": 0.00013924508665321038,
      "grad_norm": 10059.390737017824,
      "learning_rate": 5.107827896027823e-07,
      "loss": 1.8069,
      "step": 38368
    },
    {
      "epoch": 0.00013936122100404708,
      "grad_norm": 10858.433404501775,
      "learning_rate": 5.105697026215995e-07,
      "loss": 1.8164,
      "step": 38400
    },
    {
      "epoch": 0.00013947735535488378,
      "grad_norm": 8943.205745145306,
      "learning_rate": 5.103568821032497e-07,
      "loss": 1.8267,
      "step": 38432
    },
    {
      "epoch": 0.0001395934897057205,
      "grad_norm": 9247.66705715555,
      "learning_rate": 5.101443274928474e-07,
      "loss": 1.8088,
      "step": 38464
    },
    {
      "epoch": 0.0001397096240565572,
      "grad_norm": 8950.790467886063,
      "learning_rate": 5.099320382371235e-07,
      "loss": 1.8232,
      "step": 38496
    },
    {
      "epoch": 0.0001398257584073939,
      "grad_norm": 9236.24739815906,
      "learning_rate": 5.097200137844197e-07,
      "loss": 1.8159,
      "step": 38528
    },
    {
      "epoch": 0.0001399418927582306,
      "grad_norm": 8704.177847447741,
      "learning_rate": 5.095082535846815e-07,
      "loss": 1.7829,
      "step": 38560
    },
    {
      "epoch": 0.0001400580271090673,
      "grad_norm": 9678.680695218745,
      "learning_rate": 5.092967570894525e-07,
      "loss": 1.782,
      "step": 38592
    },
    {
      "epoch": 0.00014017416145990403,
      "grad_norm": 8952.346731444219,
      "learning_rate": 5.090855237518695e-07,
      "loss": 1.779,
      "step": 38624
    },
    {
      "epoch": 0.00014029029581074073,
      "grad_norm": 10617.411643145424,
      "learning_rate": 5.088745530266547e-07,
      "loss": 1.8049,
      "step": 38656
    },
    {
      "epoch": 0.00014040643016157743,
      "grad_norm": 10830.907348878947,
      "learning_rate": 5.086638443701114e-07,
      "loss": 1.7939,
      "step": 38688
    },
    {
      "epoch": 0.00014052256451241413,
      "grad_norm": 11389.804388135908,
      "learning_rate": 5.084533972401172e-07,
      "loss": 1.7765,
      "step": 38720
    },
    {
      "epoch": 0.00014063869886325086,
      "grad_norm": 8654.964355790265,
      "learning_rate": 5.082432110961187e-07,
      "loss": 1.7968,
      "step": 38752
    },
    {
      "epoch": 0.00014075483321408756,
      "grad_norm": 8510.807893496363,
      "learning_rate": 5.080332853991251e-07,
      "loss": 1.8042,
      "step": 38784
    },
    {
      "epoch": 0.00014087096756492426,
      "grad_norm": 9154.983014730286,
      "learning_rate": 5.078236196117028e-07,
      "loss": 1.8248,
      "step": 38816
    },
    {
      "epoch": 0.00014098710191576096,
      "grad_norm": 10810.65788932385,
      "learning_rate": 5.076142131979696e-07,
      "loss": 1.8261,
      "step": 38848
    },
    {
      "epoch": 0.00014110323626659766,
      "grad_norm": 9860.611948555728,
      "learning_rate": 5.074115975726061e-07,
      "loss": 1.7626,
      "step": 38880
    },
    {
      "epoch": 0.00014121937061743438,
      "grad_norm": 9071.39206516839,
      "learning_rate": 5.072027002407504e-07,
      "loss": 1.779,
      "step": 38912
    },
    {
      "epoch": 0.00014133550496827108,
      "grad_norm": 9159.410898087277,
      "learning_rate": 5.069940607007664e-07,
      "loss": 1.7906,
      "step": 38944
    },
    {
      "epoch": 0.00014145163931910778,
      "grad_norm": 9885.632503790539,
      "learning_rate": 5.067856784228719e-07,
      "loss": 1.7966,
      "step": 38976
    },
    {
      "epoch": 0.00014156777366994448,
      "grad_norm": 9532.571321527052,
      "learning_rate": 5.06577552878808e-07,
      "loss": 1.7988,
      "step": 39008
    },
    {
      "epoch": 0.0001416839080207812,
      "grad_norm": 10009.545843843265,
      "learning_rate": 5.063696835418334e-07,
      "loss": 1.8066,
      "step": 39040
    },
    {
      "epoch": 0.0001418000423716179,
      "grad_norm": 8955.200165267106,
      "learning_rate": 5.061620698867178e-07,
      "loss": 1.796,
      "step": 39072
    },
    {
      "epoch": 0.0001419161767224546,
      "grad_norm": 11524.657912493542,
      "learning_rate": 5.059547113897378e-07,
      "loss": 1.8124,
      "step": 39104
    },
    {
      "epoch": 0.0001420323110732913,
      "grad_norm": 10358.70349030225,
      "learning_rate": 5.057476075286704e-07,
      "loss": 1.814,
      "step": 39136
    },
    {
      "epoch": 0.000142148445424128,
      "grad_norm": 9843.661005946922,
      "learning_rate": 5.055407577827876e-07,
      "loss": 1.8116,
      "step": 39168
    },
    {
      "epoch": 0.00014226457977496474,
      "grad_norm": 10496.754831851606,
      "learning_rate": 5.053341616328513e-07,
      "loss": 1.8069,
      "step": 39200
    },
    {
      "epoch": 0.00014238071412580144,
      "grad_norm": 10111.633893688992,
      "learning_rate": 5.051278185611073e-07,
      "loss": 1.8034,
      "step": 39232
    },
    {
      "epoch": 0.00014249684847663814,
      "grad_norm": 9307.878383391137,
      "learning_rate": 5.049217280512801e-07,
      "loss": 1.8145,
      "step": 39264
    },
    {
      "epoch": 0.00014261298282747484,
      "grad_norm": 8759.9964611865,
      "learning_rate": 5.047158895885676e-07,
      "loss": 1.7964,
      "step": 39296
    },
    {
      "epoch": 0.00014272911717831154,
      "grad_norm": 10278.685713650359,
      "learning_rate": 5.045103026596355e-07,
      "loss": 1.7833,
      "step": 39328
    },
    {
      "epoch": 0.00014284525152914826,
      "grad_norm": 9890.985036890916,
      "learning_rate": 5.043049667526119e-07,
      "loss": 1.7938,
      "step": 39360
    },
    {
      "epoch": 0.00014296138587998496,
      "grad_norm": 10696.772223432637,
      "learning_rate": 5.040998813570823e-07,
      "loss": 1.8041,
      "step": 39392
    },
    {
      "epoch": 0.00014307752023082166,
      "grad_norm": 9447.086111600762,
      "learning_rate": 5.038950459640838e-07,
      "loss": 1.8195,
      "step": 39424
    },
    {
      "epoch": 0.00014319365458165836,
      "grad_norm": 9480.944151296326,
      "learning_rate": 5.036904600661e-07,
      "loss": 1.8047,
      "step": 39456
    },
    {
      "epoch": 0.0001433097889324951,
      "grad_norm": 10462.966978825843,
      "learning_rate": 5.034861231570557e-07,
      "loss": 1.7836,
      "step": 39488
    },
    {
      "epoch": 0.0001434259232833318,
      "grad_norm": 9080.353737602958,
      "learning_rate": 5.032820347323117e-07,
      "loss": 1.809,
      "step": 39520
    },
    {
      "epoch": 0.0001435420576341685,
      "grad_norm": 9895.220563484172,
      "learning_rate": 5.030781942886598e-07,
      "loss": 1.7979,
      "step": 39552
    },
    {
      "epoch": 0.0001436581919850052,
      "grad_norm": 10133.763762788236,
      "learning_rate": 5.028746013243171e-07,
      "loss": 1.7986,
      "step": 39584
    },
    {
      "epoch": 0.0001437743263358419,
      "grad_norm": 11658.413614210125,
      "learning_rate": 5.026712553389207e-07,
      "loss": 1.782,
      "step": 39616
    },
    {
      "epoch": 0.00014389046068667862,
      "grad_norm": 9427.97104365515,
      "learning_rate": 5.024681558335236e-07,
      "loss": 1.7668,
      "step": 39648
    },
    {
      "epoch": 0.00014400659503751532,
      "grad_norm": 9042.502750898117,
      "learning_rate": 5.02265302310588e-07,
      "loss": 1.7647,
      "step": 39680
    },
    {
      "epoch": 0.00014412272938835202,
      "grad_norm": 10459.765771756078,
      "learning_rate": 5.020626942739816e-07,
      "loss": 1.7738,
      "step": 39712
    },
    {
      "epoch": 0.00014423886373918872,
      "grad_norm": 9894.699894387904,
      "learning_rate": 5.018603312289718e-07,
      "loss": 1.8015,
      "step": 39744
    },
    {
      "epoch": 0.00014435499809002544,
      "grad_norm": 10942.750294144522,
      "learning_rate": 5.016582126822206e-07,
      "loss": 1.8217,
      "step": 39776
    },
    {
      "epoch": 0.00014447113244086214,
      "grad_norm": 11714.93294901853,
      "learning_rate": 5.014563381417797e-07,
      "loss": 1.8101,
      "step": 39808
    },
    {
      "epoch": 0.00014458726679169884,
      "grad_norm": 9773.831899516177,
      "learning_rate": 5.012547071170856e-07,
      "loss": 1.8062,
      "step": 39840
    },
    {
      "epoch": 0.00014470340114253554,
      "grad_norm": 10003.390825115252,
      "learning_rate": 5.010596088201202e-07,
      "loss": 1.8076,
      "step": 39872
    },
    {
      "epoch": 0.00014481953549337224,
      "grad_norm": 9989.027079751062,
      "learning_rate": 5.008584557887765e-07,
      "loss": 1.8034,
      "step": 39904
    },
    {
      "epoch": 0.00014493566984420897,
      "grad_norm": 9356.380924267673,
      "learning_rate": 5.006575448249273e-07,
      "loss": 1.8076,
      "step": 39936
    },
    {
      "epoch": 0.00014505180419504567,
      "grad_norm": 10181.599776066627,
      "learning_rate": 5.004568754434552e-07,
      "loss": 1.82,
      "step": 39968
    },
    {
      "epoch": 0.00014516793854588237,
      "grad_norm": 8822.133868855086,
      "learning_rate": 5.002564471606027e-07,
      "loss": 1.8128,
      "step": 40000
    },
    {
      "epoch": 0.00014528407289671907,
      "grad_norm": 11085.410051053592,
      "learning_rate": 5.000562594939676e-07,
      "loss": 1.8198,
      "step": 40032
    },
    {
      "epoch": 0.0001454002072475558,
      "grad_norm": 10579.649143520783,
      "learning_rate": 4.998563119624979e-07,
      "loss": 1.7869,
      "step": 40064
    },
    {
      "epoch": 0.0001455163415983925,
      "grad_norm": 9860.949244367906,
      "learning_rate": 4.996566040864867e-07,
      "loss": 1.7834,
      "step": 40096
    },
    {
      "epoch": 0.0001456324759492292,
      "grad_norm": 10446.8403835801,
      "learning_rate": 4.994571353875677e-07,
      "loss": 1.7794,
      "step": 40128
    },
    {
      "epoch": 0.0001457486103000659,
      "grad_norm": 9485.708091650302,
      "learning_rate": 4.992579053887108e-07,
      "loss": 1.7831,
      "step": 40160
    },
    {
      "epoch": 0.0001458647446509026,
      "grad_norm": 10208.547888901732,
      "learning_rate": 4.990589136142163e-07,
      "loss": 1.8023,
      "step": 40192
    },
    {
      "epoch": 0.00014598087900173932,
      "grad_norm": 10451.42755799417,
      "learning_rate": 4.988601595897107e-07,
      "loss": 1.7773,
      "step": 40224
    },
    {
      "epoch": 0.00014609701335257602,
      "grad_norm": 9266.076192218581,
      "learning_rate": 4.986616428421421e-07,
      "loss": 1.7853,
      "step": 40256
    },
    {
      "epoch": 0.00014621314770341272,
      "grad_norm": 10853.0421541612,
      "learning_rate": 4.984633628997752e-07,
      "loss": 1.7934,
      "step": 40288
    },
    {
      "epoch": 0.00014632928205424942,
      "grad_norm": 9969.080599533741,
      "learning_rate": 4.982653192921868e-07,
      "loss": 1.8021,
      "step": 40320
    },
    {
      "epoch": 0.00014644541640508615,
      "grad_norm": 8676.266478157526,
      "learning_rate": 4.980675115502606e-07,
      "loss": 1.8127,
      "step": 40352
    },
    {
      "epoch": 0.00014656155075592285,
      "grad_norm": 11879.585851367041,
      "learning_rate": 4.978699392061833e-07,
      "loss": 1.808,
      "step": 40384
    },
    {
      "epoch": 0.00014667768510675955,
      "grad_norm": 10675.083231525645,
      "learning_rate": 4.976726017934395e-07,
      "loss": 1.7758,
      "step": 40416
    },
    {
      "epoch": 0.00014679381945759625,
      "grad_norm": 9832.935268779105,
      "learning_rate": 4.974754988468071e-07,
      "loss": 1.7712,
      "step": 40448
    },
    {
      "epoch": 0.00014690995380843295,
      "grad_norm": 9878.628042395361,
      "learning_rate": 4.972786299023524e-07,
      "loss": 1.7829,
      "step": 40480
    },
    {
      "epoch": 0.00014702608815926968,
      "grad_norm": 9231.069060515147,
      "learning_rate": 4.970819944974265e-07,
      "loss": 1.7994,
      "step": 40512
    },
    {
      "epoch": 0.00014714222251010638,
      "grad_norm": 9447.05403816449,
      "learning_rate": 4.968855921706597e-07,
      "loss": 1.8081,
      "step": 40544
    },
    {
      "epoch": 0.00014725835686094308,
      "grad_norm": 8465.400758381142,
      "learning_rate": 4.966894224619574e-07,
      "loss": 1.8259,
      "step": 40576
    },
    {
      "epoch": 0.00014737449121177978,
      "grad_norm": 10202.346788852063,
      "learning_rate": 4.964934849124953e-07,
      "loss": 1.8166,
      "step": 40608
    },
    {
      "epoch": 0.0001474906255626165,
      "grad_norm": 10222.85234169016,
      "learning_rate": 4.962977790647154e-07,
      "loss": 1.814,
      "step": 40640
    },
    {
      "epoch": 0.0001476067599134532,
      "grad_norm": 10527.252728038782,
      "learning_rate": 4.961023044623213e-07,
      "loss": 1.8307,
      "step": 40672
    },
    {
      "epoch": 0.0001477228942642899,
      "grad_norm": 11449.687681330002,
      "learning_rate": 4.959070606502732e-07,
      "loss": 1.8156,
      "step": 40704
    },
    {
      "epoch": 0.0001478390286151266,
      "grad_norm": 10871.91289516247,
      "learning_rate": 4.957120471747842e-07,
      "loss": 1.7922,
      "step": 40736
    },
    {
      "epoch": 0.0001479551629659633,
      "grad_norm": 8138.031088660205,
      "learning_rate": 4.955172635833157e-07,
      "loss": 1.8059,
      "step": 40768
    },
    {
      "epoch": 0.00014807129731680003,
      "grad_norm": 9389.520967546749,
      "learning_rate": 4.953227094245726e-07,
      "loss": 1.8069,
      "step": 40800
    },
    {
      "epoch": 0.00014818743166763673,
      "grad_norm": 9783.936017779348,
      "learning_rate": 4.951283842484991e-07,
      "loss": 1.76,
      "step": 40832
    },
    {
      "epoch": 0.00014830356601847343,
      "grad_norm": 9368.258749628983,
      "learning_rate": 4.949403496715409e-07,
      "loss": 1.7609,
      "step": 40864
    },
    {
      "epoch": 0.00014841970036931013,
      "grad_norm": 9174.153584936324,
      "learning_rate": 4.947464739946428e-07,
      "loss": 1.7719,
      "step": 40896
    },
    {
      "epoch": 0.00014853583472014686,
      "grad_norm": 10421.355765926044,
      "learning_rate": 4.945528259715703e-07,
      "loss": 1.786,
      "step": 40928
    },
    {
      "epoch": 0.00014865196907098356,
      "grad_norm": 10515.437128336605,
      "learning_rate": 4.943594051571437e-07,
      "loss": 1.7996,
      "step": 40960
    },
    {
      "epoch": 0.00014876810342182026,
      "grad_norm": 9140.664636666199,
      "learning_rate": 4.941662111074008e-07,
      "loss": 1.7956,
      "step": 40992
    },
    {
      "epoch": 0.00014888423777265696,
      "grad_norm": 8807.422324380726,
      "learning_rate": 4.939732433795933e-07,
      "loss": 1.794,
      "step": 41024
    },
    {
      "epoch": 0.00014900037212349366,
      "grad_norm": 11279.460359432094,
      "learning_rate": 4.937805015321818e-07,
      "loss": 1.8004,
      "step": 41056
    },
    {
      "epoch": 0.00014911650647433038,
      "grad_norm": 9397.239488275267,
      "learning_rate": 4.935879851248323e-07,
      "loss": 1.809,
      "step": 41088
    },
    {
      "epoch": 0.00014923264082516708,
      "grad_norm": 9125.297803359625,
      "learning_rate": 4.933956937184114e-07,
      "loss": 1.8032,
      "step": 41120
    },
    {
      "epoch": 0.00014934877517600378,
      "grad_norm": 9173.754520369508,
      "learning_rate": 4.932036268749824e-07,
      "loss": 1.7928,
      "step": 41152
    },
    {
      "epoch": 0.00014946490952684048,
      "grad_norm": 9113.61761322034,
      "learning_rate": 4.930117841578009e-07,
      "loss": 1.8066,
      "step": 41184
    },
    {
      "epoch": 0.0001495810438776772,
      "grad_norm": 8577.589055206598,
      "learning_rate": 4.928201651313108e-07,
      "loss": 1.782,
      "step": 41216
    },
    {
      "epoch": 0.0001496971782285139,
      "grad_norm": 11021.215722414656,
      "learning_rate": 4.926287693611402e-07,
      "loss": 1.806,
      "step": 41248
    },
    {
      "epoch": 0.0001498133125793506,
      "grad_norm": 10450.119138076847,
      "learning_rate": 4.924375964140972e-07,
      "loss": 1.8196,
      "step": 41280
    },
    {
      "epoch": 0.0001499294469301873,
      "grad_norm": 9836.991816607351,
      "learning_rate": 4.922466458581653e-07,
      "loss": 1.8143,
      "step": 41312
    },
    {
      "epoch": 0.000150045581281024,
      "grad_norm": 10159.278025529176,
      "learning_rate": 4.920559172625004e-07,
      "loss": 1.7997,
      "step": 41344
    },
    {
      "epoch": 0.00015016171563186074,
      "grad_norm": 9895.829020349936,
      "learning_rate": 4.918654101974254e-07,
      "loss": 1.8078,
      "step": 41376
    },
    {
      "epoch": 0.00015027784998269744,
      "grad_norm": 9242.73509303388,
      "learning_rate": 4.916751242344272e-07,
      "loss": 1.8034,
      "step": 41408
    },
    {
      "epoch": 0.00015039398433353414,
      "grad_norm": 8437.014045265065,
      "learning_rate": 4.914850589461523e-07,
      "loss": 1.794,
      "step": 41440
    },
    {
      "epoch": 0.00015051011868437084,
      "grad_norm": 10506.424415565934,
      "learning_rate": 4.912952139064024e-07,
      "loss": 1.7926,
      "step": 41472
    },
    {
      "epoch": 0.00015062625303520756,
      "grad_norm": 9612.212544466545,
      "learning_rate": 4.91105588690131e-07,
      "loss": 1.8053,
      "step": 41504
    },
    {
      "epoch": 0.00015074238738604426,
      "grad_norm": 10835.274615809236,
      "learning_rate": 4.909161828734387e-07,
      "loss": 1.8096,
      "step": 41536
    },
    {
      "epoch": 0.00015085852173688096,
      "grad_norm": 9587.784102700685,
      "learning_rate": 4.907269960335703e-07,
      "loss": 1.7996,
      "step": 41568
    },
    {
      "epoch": 0.00015097465608771766,
      "grad_norm": 9987.287319387584,
      "learning_rate": 4.905380277489092e-07,
      "loss": 1.785,
      "step": 41600
    },
    {
      "epoch": 0.00015109079043855436,
      "grad_norm": 9237.191239765472,
      "learning_rate": 4.903492775989753e-07,
      "loss": 1.7738,
      "step": 41632
    },
    {
      "epoch": 0.0001512069247893911,
      "grad_norm": 8488.112393223832,
      "learning_rate": 4.901607451644199e-07,
      "loss": 1.7727,
      "step": 41664
    },
    {
      "epoch": 0.0001513230591402278,
      "grad_norm": 11551.25603560063,
      "learning_rate": 4.899724300270217e-07,
      "loss": 1.788,
      "step": 41696
    },
    {
      "epoch": 0.0001514391934910645,
      "grad_norm": 8852.224127302696,
      "learning_rate": 4.897843317696839e-07,
      "loss": 1.7879,
      "step": 41728
    },
    {
      "epoch": 0.0001515553278419012,
      "grad_norm": 10863.91117415823,
      "learning_rate": 4.895964499764291e-07,
      "loss": 1.7945,
      "step": 41760
    },
    {
      "epoch": 0.0001516714621927379,
      "grad_norm": 9503.417595791527,
      "learning_rate": 4.894087842323964e-07,
      "loss": 1.8108,
      "step": 41792
    },
    {
      "epoch": 0.00015178759654357462,
      "grad_norm": 9018.78761253418,
      "learning_rate": 4.892213341238371e-07,
      "loss": 1.7961,
      "step": 41824
    },
    {
      "epoch": 0.00015190373089441132,
      "grad_norm": 9411.261658247528,
      "learning_rate": 4.890340992381108e-07,
      "loss": 1.8028,
      "step": 41856
    },
    {
      "epoch": 0.00015201986524524802,
      "grad_norm": 10006.55595097534,
      "learning_rate": 4.888529202935496e-07,
      "loss": 1.8021,
      "step": 41888
    },
    {
      "epoch": 0.00015213599959608472,
      "grad_norm": 10256.964365737067,
      "learning_rate": 4.886661079261583e-07,
      "loss": 1.7893,
      "step": 41920
    },
    {
      "epoch": 0.00015225213394692144,
      "grad_norm": 9874.413400298774,
      "learning_rate": 4.884795095630671e-07,
      "loss": 1.7886,
      "step": 41952
    },
    {
      "epoch": 0.00015236826829775814,
      "grad_norm": 9142.649178438382,
      "learning_rate": 4.882931247959974e-07,
      "loss": 1.7732,
      "step": 41984
    },
    {
      "epoch": 0.00015248440264859484,
      "grad_norm": 8692.706252945627,
      "learning_rate": 4.881069532177604e-07,
      "loss": 1.8026,
      "step": 42016
    },
    {
      "epoch": 0.00015260053699943154,
      "grad_norm": 9405.153906236728,
      "learning_rate": 4.87920994422253e-07,
      "loss": 1.799,
      "step": 42048
    },
    {
      "epoch": 0.00015271667135026824,
      "grad_norm": 9221.74229741864,
      "learning_rate": 4.877352480044544e-07,
      "loss": 1.8089,
      "step": 42080
    },
    {
      "epoch": 0.00015283280570110497,
      "grad_norm": 11429.857041975634,
      "learning_rate": 4.875497135604223e-07,
      "loss": 1.8216,
      "step": 42112
    },
    {
      "epoch": 0.00015294894005194167,
      "grad_norm": 9886.230120728527,
      "learning_rate": 4.873643906872892e-07,
      "loss": 1.8056,
      "step": 42144
    },
    {
      "epoch": 0.00015306507440277837,
      "grad_norm": 9884.176546379571,
      "learning_rate": 4.871792789832586e-07,
      "loss": 1.8079,
      "step": 42176
    },
    {
      "epoch": 0.00015318120875361507,
      "grad_norm": 9683.72459335766,
      "learning_rate": 4.869943780476017e-07,
      "loss": 1.819,
      "step": 42208
    },
    {
      "epoch": 0.0001532973431044518,
      "grad_norm": 9734.389040920852,
      "learning_rate": 4.868096874806532e-07,
      "loss": 1.8055,
      "step": 42240
    },
    {
      "epoch": 0.0001534134774552885,
      "grad_norm": 9620.132223623541,
      "learning_rate": 4.866252068838083e-07,
      "loss": 1.8087,
      "step": 42272
    },
    {
      "epoch": 0.0001535296118061252,
      "grad_norm": 9043.284137966693,
      "learning_rate": 4.864409358595189e-07,
      "loss": 1.7971,
      "step": 42304
    },
    {
      "epoch": 0.0001536457461569619,
      "grad_norm": 9398.973454585346,
      "learning_rate": 4.862568740112892e-07,
      "loss": 1.8082,
      "step": 42336
    },
    {
      "epoch": 0.0001537618805077986,
      "grad_norm": 8372.555762728607,
      "learning_rate": 4.860730209436735e-07,
      "loss": 1.7953,
      "step": 42368
    },
    {
      "epoch": 0.00015387801485863532,
      "grad_norm": 10577.102344215073,
      "learning_rate": 4.858893762622718e-07,
      "loss": 1.7904,
      "step": 42400
    },
    {
      "epoch": 0.00015399414920947202,
      "grad_norm": 7830.004980841329,
      "learning_rate": 4.85705939573726e-07,
      "loss": 1.7688,
      "step": 42432
    },
    {
      "epoch": 0.00015411028356030872,
      "grad_norm": 9423.719860012818,
      "learning_rate": 4.855227104857171e-07,
      "loss": 1.7857,
      "step": 42464
    },
    {
      "epoch": 0.00015422641791114542,
      "grad_norm": 8852.732120650664,
      "learning_rate": 4.853396886069614e-07,
      "loss": 1.7768,
      "step": 42496
    },
    {
      "epoch": 0.00015434255226198215,
      "grad_norm": 10279.898832187018,
      "learning_rate": 4.851568735472064e-07,
      "loss": 1.7976,
      "step": 42528
    },
    {
      "epoch": 0.00015445868661281885,
      "grad_norm": 9312.31013229263,
      "learning_rate": 4.849742649172284e-07,
      "loss": 1.7887,
      "step": 42560
    },
    {
      "epoch": 0.00015457482096365555,
      "grad_norm": 9781.39785511253,
      "learning_rate": 4.847918623288281e-07,
      "loss": 1.7785,
      "step": 42592
    },
    {
      "epoch": 0.00015469095531449225,
      "grad_norm": 10840.554413866479,
      "learning_rate": 4.846096653948275e-07,
      "loss": 1.7999,
      "step": 42624
    },
    {
      "epoch": 0.00015480708966532895,
      "grad_norm": 9362.187564880336,
      "learning_rate": 4.844276737290663e-07,
      "loss": 1.7683,
      "step": 42656
    },
    {
      "epoch": 0.00015492322401616568,
      "grad_norm": 10136.307414438455,
      "learning_rate": 4.84245886946399e-07,
      "loss": 1.7756,
      "step": 42688
    },
    {
      "epoch": 0.00015503935836700238,
      "grad_norm": 9818.386629176914,
      "learning_rate": 4.840643046626906e-07,
      "loss": 1.7857,
      "step": 42720
    },
    {
      "epoch": 0.00015515549271783908,
      "grad_norm": 9992.76488265385,
      "learning_rate": 4.83882926494814e-07,
      "loss": 1.7775,
      "step": 42752
    },
    {
      "epoch": 0.00015527162706867578,
      "grad_norm": 8237.222104568991,
      "learning_rate": 4.837017520606458e-07,
      "loss": 1.8046,
      "step": 42784
    },
    {
      "epoch": 0.0001553877614195125,
      "grad_norm": 10210.265912306104,
      "learning_rate": 4.835207809790639e-07,
      "loss": 1.8228,
      "step": 42816
    },
    {
      "epoch": 0.0001555038957703492,
      "grad_norm": 9467.83048010472,
      "learning_rate": 4.83340012869943e-07,
      "loss": 1.8232,
      "step": 42848
    },
    {
      "epoch": 0.0001556200301211859,
      "grad_norm": 10626.931165675254,
      "learning_rate": 4.831650869636815e-07,
      "loss": 1.8134,
      "step": 42880
    },
    {
      "epoch": 0.0001557361644720226,
      "grad_norm": 10414.579492231072,
      "learning_rate": 4.829847173495704e-07,
      "loss": 1.8244,
      "step": 42912
    },
    {
      "epoch": 0.0001558522988228593,
      "grad_norm": 10358.829084409106,
      "learning_rate": 4.828045495852676e-07,
      "loss": 1.8369,
      "step": 42944
    },
    {
      "epoch": 0.00015596843317369603,
      "grad_norm": 10029.89202334701,
      "learning_rate": 4.826245832945737e-07,
      "loss": 1.8379,
      "step": 42976
    },
    {
      "epoch": 0.00015608456752453273,
      "grad_norm": 9621.807938220341,
      "learning_rate": 4.8244481810227e-07,
      "loss": 1.7863,
      "step": 43008
    },
    {
      "epoch": 0.00015620070187536943,
      "grad_norm": 10512.363863565606,
      "learning_rate": 4.822652536341157e-07,
      "loss": 1.7954,
      "step": 43040
    },
    {
      "epoch": 0.00015631683622620613,
      "grad_norm": 8567.592660718645,
      "learning_rate": 4.820858895168439e-07,
      "loss": 1.7928,
      "step": 43072
    },
    {
      "epoch": 0.00015643297057704286,
      "grad_norm": 11134.259292831292,
      "learning_rate": 4.819067253781591e-07,
      "loss": 1.7754,
      "step": 43104
    },
    {
      "epoch": 0.00015654910492787956,
      "grad_norm": 9457.10833183167,
      "learning_rate": 4.817277608467333e-07,
      "loss": 1.8034,
      "step": 43136
    },
    {
      "epoch": 0.00015666523927871626,
      "grad_norm": 9536.631585628125,
      "learning_rate": 4.815489955522035e-07,
      "loss": 1.7621,
      "step": 43168
    },
    {
      "epoch": 0.00015678137362955296,
      "grad_norm": 9342.473762339394,
      "learning_rate": 4.813704291251676e-07,
      "loss": 1.7769,
      "step": 43200
    },
    {
      "epoch": 0.00015689750798038966,
      "grad_norm": 11379.030011384979,
      "learning_rate": 4.811920611971818e-07,
      "loss": 1.7866,
      "step": 43232
    },
    {
      "epoch": 0.00015701364233122638,
      "grad_norm": 9638.617950723019,
      "learning_rate": 4.810138914007576e-07,
      "loss": 1.7807,
      "step": 43264
    },
    {
      "epoch": 0.00015712977668206308,
      "grad_norm": 12030.539472525743,
      "learning_rate": 4.808359193693577e-07,
      "loss": 1.775,
      "step": 43296
    },
    {
      "epoch": 0.00015724591103289978,
      "grad_norm": 9291.419052007072,
      "learning_rate": 4.806581447373939e-07,
      "loss": 1.7896,
      "step": 43328
    },
    {
      "epoch": 0.00015736204538373648,
      "grad_norm": 10357.819172007205,
      "learning_rate": 4.804805671402233e-07,
      "loss": 1.7824,
      "step": 43360
    },
    {
      "epoch": 0.0001574781797345732,
      "grad_norm": 10048.464957395234,
      "learning_rate": 4.80303186214145e-07,
      "loss": 1.7918,
      "step": 43392
    },
    {
      "epoch": 0.0001575943140854099,
      "grad_norm": 9580.501761390162,
      "learning_rate": 4.801260015963979e-07,
      "loss": 1.7823,
      "step": 43424
    },
    {
      "epoch": 0.0001577104484362466,
      "grad_norm": 10074.033750191627,
      "learning_rate": 4.799490129251565e-07,
      "loss": 1.7894,
      "step": 43456
    },
    {
      "epoch": 0.0001578265827870833,
      "grad_norm": 9853.123362670336,
      "learning_rate": 4.797722198395283e-07,
      "loss": 1.7796,
      "step": 43488
    },
    {
      "epoch": 0.00015794271713792,
      "grad_norm": 8481.543019993473,
      "learning_rate": 4.79595621979551e-07,
      "loss": 1.8011,
      "step": 43520
    },
    {
      "epoch": 0.00015805885148875674,
      "grad_norm": 10121.355739227824,
      "learning_rate": 4.794192189861888e-07,
      "loss": 1.8298,
      "step": 43552
    },
    {
      "epoch": 0.00015817498583959344,
      "grad_norm": 10784.212349541342,
      "learning_rate": 4.792430105013297e-07,
      "loss": 1.8042,
      "step": 43584
    },
    {
      "epoch": 0.00015829112019043014,
      "grad_norm": 9246.48246632199,
      "learning_rate": 4.790669961677824e-07,
      "loss": 1.8032,
      "step": 43616
    },
    {
      "epoch": 0.00015840725454126684,
      "grad_norm": 9326.921035368532,
      "learning_rate": 4.788911756292733e-07,
      "loss": 1.8075,
      "step": 43648
    },
    {
      "epoch": 0.00015852338889210356,
      "grad_norm": 8212.505707760574,
      "learning_rate": 4.787155485304435e-07,
      "loss": 1.8005,
      "step": 43680
    },
    {
      "epoch": 0.00015863952324294026,
      "grad_norm": 10396.62608734199,
      "learning_rate": 4.785401145168453e-07,
      "loss": 1.8254,
      "step": 43712
    },
    {
      "epoch": 0.00015875565759377696,
      "grad_norm": 9203.541709581155,
      "learning_rate": 4.783648732349399e-07,
      "loss": 1.8275,
      "step": 43744
    },
    {
      "epoch": 0.00015887179194461366,
      "grad_norm": 9248.587351590511,
      "learning_rate": 4.781898243320941e-07,
      "loss": 1.8033,
      "step": 43776
    },
    {
      "epoch": 0.00015898792629545036,
      "grad_norm": 10988.569879652221,
      "learning_rate": 4.780149674565772e-07,
      "loss": 1.8117,
      "step": 43808
    },
    {
      "epoch": 0.0001591040606462871,
      "grad_norm": 9050.490152472406,
      "learning_rate": 4.778403022575583e-07,
      "loss": 1.8097,
      "step": 43840
    },
    {
      "epoch": 0.0001592201949971238,
      "grad_norm": 9220.06366572379,
      "learning_rate": 4.776712778010304e-07,
      "loss": 1.7659,
      "step": 43872
    },
    {
      "epoch": 0.0001593363293479605,
      "grad_norm": 10008.516373569062,
      "learning_rate": 4.77496988943325e-07,
      "loss": 1.7711,
      "step": 43904
    },
    {
      "epoch": 0.0001594524636987972,
      "grad_norm": 10308.83970192572,
      "learning_rate": 4.773228907258593e-07,
      "loss": 1.7651,
      "step": 43936
    },
    {
      "epoch": 0.00015956859804963392,
      "grad_norm": 9001.221361570884,
      "learning_rate": 4.771489828013436e-07,
      "loss": 1.7734,
      "step": 43968
    },
    {
      "epoch": 0.00015968473240047062,
      "grad_norm": 9263.143526902733,
      "learning_rate": 4.769752648233735e-07,
      "loss": 1.7822,
      "step": 44000
    },
    {
      "epoch": 0.00015980086675130732,
      "grad_norm": 10366.877543407176,
      "learning_rate": 4.768017364464267e-07,
      "loss": 1.7827,
      "step": 44032
    },
    {
      "epoch": 0.00015991700110214402,
      "grad_norm": 10993.934327619027,
      "learning_rate": 4.766283973258601e-07,
      "loss": 1.8064,
      "step": 44064
    },
    {
      "epoch": 0.00016003313545298072,
      "grad_norm": 9256.216181572252,
      "learning_rate": 4.7645524711790723e-07,
      "loss": 1.8112,
      "step": 44096
    },
    {
      "epoch": 0.00016014926980381745,
      "grad_norm": 11287.300297236712,
      "learning_rate": 4.7628228547967503e-07,
      "loss": 1.8161,
      "step": 44128
    },
    {
      "epoch": 0.00016026540415465414,
      "grad_norm": 8915.46656098266,
      "learning_rate": 4.761095120691411e-07,
      "loss": 1.7962,
      "step": 44160
    },
    {
      "epoch": 0.00016038153850549084,
      "grad_norm": 8626.647784626426,
      "learning_rate": 4.759369265451512e-07,
      "loss": 1.7628,
      "step": 44192
    },
    {
      "epoch": 0.00016049767285632754,
      "grad_norm": 9496.914867471436,
      "learning_rate": 4.7576452856741555e-07,
      "loss": 1.7681,
      "step": 44224
    },
    {
      "epoch": 0.00016061380720716427,
      "grad_norm": 9855.935470567976,
      "learning_rate": 4.755923177965072e-07,
      "loss": 1.7658,
      "step": 44256
    },
    {
      "epoch": 0.00016072994155800097,
      "grad_norm": 9674.581334610815,
      "learning_rate": 4.754202938938583e-07,
      "loss": 1.7764,
      "step": 44288
    },
    {
      "epoch": 0.00016084607590883767,
      "grad_norm": 9058.808862096606,
      "learning_rate": 4.752484565217575e-07,
      "loss": 1.8111,
      "step": 44320
    },
    {
      "epoch": 0.00016096221025967437,
      "grad_norm": 9818.080973387823,
      "learning_rate": 4.750768053433476e-07,
      "loss": 1.8201,
      "step": 44352
    },
    {
      "epoch": 0.00016107834461051107,
      "grad_norm": 8919.59147046545,
      "learning_rate": 4.7490534002262207e-07,
      "loss": 1.8087,
      "step": 44384
    },
    {
      "epoch": 0.0001611944789613478,
      "grad_norm": 9291.761835088111,
      "learning_rate": 4.747340602244231e-07,
      "loss": 1.8032,
      "step": 44416
    },
    {
      "epoch": 0.0001613106133121845,
      "grad_norm": 9926.61583824014,
      "learning_rate": 4.7456296561443806e-07,
      "loss": 1.8063,
      "step": 44448
    },
    {
      "epoch": 0.0001614267476630212,
      "grad_norm": 9939.215864443231,
      "learning_rate": 4.743920558591973e-07,
      "loss": 1.811,
      "step": 44480
    },
    {
      "epoch": 0.0001615428820138579,
      "grad_norm": 9425.86706886958,
      "learning_rate": 4.742213306260712e-07,
      "loss": 1.8052,
      "step": 44512
    },
    {
      "epoch": 0.0001616590163646946,
      "grad_norm": 11371.434562094617,
      "learning_rate": 4.7405078958326735e-07,
      "loss": 1.8043,
      "step": 44544
    },
    {
      "epoch": 0.00016177515071553133,
      "grad_norm": 9558.782244616728,
      "learning_rate": 4.738804323998283e-07,
      "loss": 1.8031,
      "step": 44576
    },
    {
      "epoch": 0.00016189128506636803,
      "grad_norm": 9193.902762157102,
      "learning_rate": 4.737102587456283e-07,
      "loss": 1.7961,
      "step": 44608
    },
    {
      "epoch": 0.00016200741941720472,
      "grad_norm": 9496.009161747897,
      "learning_rate": 4.7354026829137077e-07,
      "loss": 1.8,
      "step": 44640
    },
    {
      "epoch": 0.00016212355376804142,
      "grad_norm": 10136.621823862228,
      "learning_rate": 4.733704607085861e-07,
      "loss": 1.8174,
      "step": 44672
    },
    {
      "epoch": 0.00016223968811887815,
      "grad_norm": 9493.865598374563,
      "learning_rate": 4.732008356696281e-07,
      "loss": 1.799,
      "step": 44704
    },
    {
      "epoch": 0.00016235582246971485,
      "grad_norm": 9108.216620173238,
      "learning_rate": 4.730313928476723e-07,
      "loss": 1.7938,
      "step": 44736
    },
    {
      "epoch": 0.00016247195682055155,
      "grad_norm": 9571.692431331045,
      "learning_rate": 4.728621319167125e-07,
      "loss": 1.7701,
      "step": 44768
    },
    {
      "epoch": 0.00016258809117138825,
      "grad_norm": 9175.987576277554,
      "learning_rate": 4.726930525515586e-07,
      "loss": 1.7698,
      "step": 44800
    },
    {
      "epoch": 0.00016270422552222495,
      "grad_norm": 9116.620426451898,
      "learning_rate": 4.7252415442783404e-07,
      "loss": 1.7767,
      "step": 44832
    },
    {
      "epoch": 0.00016282035987306168,
      "grad_norm": 10518.109621029818,
      "learning_rate": 4.7235543722197265e-07,
      "loss": 1.7881,
      "step": 44864
    },
    {
      "epoch": 0.00016293649422389838,
      "grad_norm": 9790.694970225555,
      "learning_rate": 4.7219216464988567e-07,
      "loss": 1.7964,
      "step": 44896
    },
    {
      "epoch": 0.00016305262857473508,
      "grad_norm": 10531.302483548747,
      "learning_rate": 4.72023802683612e-07,
      "loss": 1.7833,
      "step": 44928
    },
    {
      "epoch": 0.00016316876292557178,
      "grad_norm": 10808.449472519173,
      "learning_rate": 4.71855620679366e-07,
      "loss": 1.7931,
      "step": 44960
    },
    {
      "epoch": 0.0001632848972764085,
      "grad_norm": 11591.646129864388,
      "learning_rate": 4.7168761831677354e-07,
      "loss": 1.7836,
      "step": 44992
    },
    {
      "epoch": 0.0001634010316272452,
      "grad_norm": 8383.13246942931,
      "learning_rate": 4.7151979527625876e-07,
      "loss": 1.764,
      "step": 45024
    },
    {
      "epoch": 0.0001635171659780819,
      "grad_norm": 10343.430572107109,
      "learning_rate": 4.7135215123904087e-07,
      "loss": 1.7843,
      "step": 45056
    },
    {
      "epoch": 0.0001636333003289186,
      "grad_norm": 10182.552332298616,
      "learning_rate": 4.711846858871321e-07,
      "loss": 1.8012,
      "step": 45088
    },
    {
      "epoch": 0.0001637494346797553,
      "grad_norm": 10221.727055639863,
      "learning_rate": 4.7101739890333483e-07,
      "loss": 1.7946,
      "step": 45120
    },
    {
      "epoch": 0.00016386556903059203,
      "grad_norm": 9929.900402320256,
      "learning_rate": 4.708502899712393e-07,
      "loss": 1.7872,
      "step": 45152
    },
    {
      "epoch": 0.00016398170338142873,
      "grad_norm": 10737.433399094962,
      "learning_rate": 4.7068335877522084e-07,
      "loss": 1.7934,
      "step": 45184
    },
    {
      "epoch": 0.00016409783773226543,
      "grad_norm": 9072.096780788883,
      "learning_rate": 4.7051660500043773e-07,
      "loss": 1.8097,
      "step": 45216
    },
    {
      "epoch": 0.00016421397208310213,
      "grad_norm": 10105.483461962618,
      "learning_rate": 4.7035002833282833e-07,
      "loss": 1.83,
      "step": 45248
    },
    {
      "epoch": 0.00016433010643393886,
      "grad_norm": 9341.065035636995,
      "learning_rate": 4.701836284591087e-07,
      "loss": 1.8398,
      "step": 45280
    },
    {
      "epoch": 0.00016444624078477556,
      "grad_norm": 9372.636448726687,
      "learning_rate": 4.700174050667704e-07,
      "loss": 1.818,
      "step": 45312
    },
    {
      "epoch": 0.00016456237513561226,
      "grad_norm": 8867.135163061404,
      "learning_rate": 4.698513578440777e-07,
      "loss": 1.8059,
      "step": 45344
    },
    {
      "epoch": 0.00016467850948644896,
      "grad_norm": 8757.201379436241,
      "learning_rate": 4.6968548648006514e-07,
      "loss": 1.7771,
      "step": 45376
    },
    {
      "epoch": 0.00016479464383728566,
      "grad_norm": 9168.40705902612,
      "learning_rate": 4.695197906645353e-07,
      "loss": 1.782,
      "step": 45408
    },
    {
      "epoch": 0.00016491077818812239,
      "grad_norm": 9734.27973709406,
      "learning_rate": 4.6935427008805626e-07,
      "loss": 1.7672,
      "step": 45440
    },
    {
      "epoch": 0.00016502691253895909,
      "grad_norm": 8986.435444602048,
      "learning_rate": 4.69188924441959e-07,
      "loss": 1.7927,
      "step": 45472
    },
    {
      "epoch": 0.00016514304688979579,
      "grad_norm": 9558.387939396476,
      "learning_rate": 4.6902375341833534e-07,
      "loss": 1.7951,
      "step": 45504
    },
    {
      "epoch": 0.00016525918124063249,
      "grad_norm": 10037.821676041072,
      "learning_rate": 4.688587567100352e-07,
      "loss": 1.7915,
      "step": 45536
    },
    {
      "epoch": 0.0001653753155914692,
      "grad_norm": 9751.48204120789,
      "learning_rate": 4.686939340106642e-07,
      "loss": 1.7946,
      "step": 45568
    },
    {
      "epoch": 0.0001654914499423059,
      "grad_norm": 9800.148264184578,
      "learning_rate": 4.685292850145818e-07,
      "loss": 1.7886,
      "step": 45600
    },
    {
      "epoch": 0.0001656075842931426,
      "grad_norm": 12365.586439793302,
      "learning_rate": 4.6836480941689804e-07,
      "loss": 1.7823,
      "step": 45632
    },
    {
      "epoch": 0.0001657237186439793,
      "grad_norm": 10127.843106999635,
      "learning_rate": 4.6820050691347205e-07,
      "loss": 1.7848,
      "step": 45664
    },
    {
      "epoch": 0.000165839852994816,
      "grad_norm": 9109.628861814295,
      "learning_rate": 4.6803637720090906e-07,
      "loss": 1.7701,
      "step": 45696
    },
    {
      "epoch": 0.00016595598734565274,
      "grad_norm": 10968.76674927496,
      "learning_rate": 4.6787241997655827e-07,
      "loss": 1.7682,
      "step": 45728
    },
    {
      "epoch": 0.00016607212169648944,
      "grad_norm": 8978.927998374862,
      "learning_rate": 4.677086349385106e-07,
      "loss": 1.7657,
      "step": 45760
    },
    {
      "epoch": 0.00016618825604732614,
      "grad_norm": 10179.626122800386,
      "learning_rate": 4.675450217855963e-07,
      "loss": 1.7731,
      "step": 45792
    },
    {
      "epoch": 0.00016630439039816284,
      "grad_norm": 10311.806728212083,
      "learning_rate": 4.6738158021738244e-07,
      "loss": 1.8113,
      "step": 45824
    },
    {
      "epoch": 0.00016642052474899957,
      "grad_norm": 10116.505622002096,
      "learning_rate": 4.672183099341708e-07,
      "loss": 1.827,
      "step": 45856
    },
    {
      "epoch": 0.00016653665909983627,
      "grad_norm": 8727.731434914802,
      "learning_rate": 4.670603049048292e-07,
      "loss": 1.8286,
      "step": 45888
    },
    {
      "epoch": 0.00016665279345067297,
      "grad_norm": 9460.304646257433,
      "learning_rate": 4.668973709659678e-07,
      "loss": 1.8109,
      "step": 45920
    },
    {
      "epoch": 0.00016676892780150967,
      "grad_norm": 8586.514077319154,
      "learning_rate": 4.667346074266851e-07,
      "loss": 1.7944,
      "step": 45952
    },
    {
      "epoch": 0.00016688506215234637,
      "grad_norm": 9343.351325942956,
      "learning_rate": 4.665720139901759e-07,
      "loss": 1.7925,
      "step": 45984
    },
    {
      "epoch": 0.0001670011965031831,
      "grad_norm": 8622.041637570535,
      "learning_rate": 4.664095903603581e-07,
      "loss": 1.7948,
      "step": 46016
    },
    {
      "epoch": 0.0001671173308540198,
      "grad_norm": 8339.587399865775,
      "learning_rate": 4.6624733624187085e-07,
      "loss": 1.7931,
      "step": 46048
    },
    {
      "epoch": 0.0001672334652048565,
      "grad_norm": 9917.453402965904,
      "learning_rate": 4.6608525134007197e-07,
      "loss": 1.8126,
      "step": 46080
    },
    {
      "epoch": 0.0001673495995556932,
      "grad_norm": 11015.313522546692,
      "learning_rate": 4.6592333536103557e-07,
      "loss": 1.8015,
      "step": 46112
    },
    {
      "epoch": 0.00016746573390652992,
      "grad_norm": 8784.060109083954,
      "learning_rate": 4.657615880115504e-07,
      "loss": 1.7768,
      "step": 46144
    },
    {
      "epoch": 0.00016758186825736662,
      "grad_norm": 9605.089067780684,
      "learning_rate": 4.656000089991171e-07,
      "loss": 1.7915,
      "step": 46176
    },
    {
      "epoch": 0.00016769800260820332,
      "grad_norm": 10538.184188938812,
      "learning_rate": 4.6543859803194586e-07,
      "loss": 1.7916,
      "step": 46208
    },
    {
      "epoch": 0.00016781413695904002,
      "grad_norm": 10337.058188865922,
      "learning_rate": 4.652773548189549e-07,
      "loss": 1.7807,
      "step": 46240
    },
    {
      "epoch": 0.00016793027130987672,
      "grad_norm": 8596.774395085637,
      "learning_rate": 4.651162790697675e-07,
      "loss": 1.7822,
      "step": 46272
    },
    {
      "epoch": 0.00016804640566071345,
      "grad_norm": 9465.454135961993,
      "learning_rate": 4.649553704947102e-07,
      "loss": 1.7829,
      "step": 46304
    },
    {
      "epoch": 0.00016816254001155015,
      "grad_norm": 8359.872965542001,
      "learning_rate": 4.6479462880481076e-07,
      "loss": 1.7794,
      "step": 46336
    },
    {
      "epoch": 0.00016827867436238685,
      "grad_norm": 9875.928614565822,
      "learning_rate": 4.6463405371179554e-07,
      "loss": 1.7952,
      "step": 46368
    },
    {
      "epoch": 0.00016839480871322355,
      "grad_norm": 9956.543376091926,
      "learning_rate": 4.6447364492808766e-07,
      "loss": 1.8196,
      "step": 46400
    },
    {
      "epoch": 0.00016851094306406027,
      "grad_norm": 11409.890797023432,
      "learning_rate": 4.643134021668046e-07,
      "loss": 1.7955,
      "step": 46432
    },
    {
      "epoch": 0.00016862707741489697,
      "grad_norm": 9000.847960053541,
      "learning_rate": 4.641533251417565e-07,
      "loss": 1.793,
      "step": 46464
    },
    {
      "epoch": 0.00016874321176573367,
      "grad_norm": 9086.732636101933,
      "learning_rate": 4.639934135674433e-07,
      "loss": 1.8063,
      "step": 46496
    },
    {
      "epoch": 0.00016885934611657037,
      "grad_norm": 9412.900190695746,
      "learning_rate": 4.638336671590532e-07,
      "loss": 1.7743,
      "step": 46528
    },
    {
      "epoch": 0.00016897548046740707,
      "grad_norm": 10430.464802682573,
      "learning_rate": 4.6367408563246046e-07,
      "loss": 1.7743,
      "step": 46560
    },
    {
      "epoch": 0.0001690916148182438,
      "grad_norm": 11358.538814477855,
      "learning_rate": 4.6351466870422276e-07,
      "loss": 1.7872,
      "step": 46592
    },
    {
      "epoch": 0.0001692077491690805,
      "grad_norm": 10074.528078277413,
      "learning_rate": 4.633554160915799e-07,
      "loss": 1.7957,
      "step": 46624
    },
    {
      "epoch": 0.0001693238835199172,
      "grad_norm": 9399.266992696825,
      "learning_rate": 4.6319632751245095e-07,
      "loss": 1.8028,
      "step": 46656
    },
    {
      "epoch": 0.0001694400178707539,
      "grad_norm": 12444.566846620255,
      "learning_rate": 4.6303740268543256e-07,
      "loss": 1.8143,
      "step": 46688
    },
    {
      "epoch": 0.00016955615222159063,
      "grad_norm": 8722.553525201207,
      "learning_rate": 4.628786413297968e-07,
      "loss": 1.7941,
      "step": 46720
    },
    {
      "epoch": 0.00016967228657242733,
      "grad_norm": 10102.781993094772,
      "learning_rate": 4.6272004316548906e-07,
      "loss": 1.7932,
      "step": 46752
    },
    {
      "epoch": 0.00016978842092326403,
      "grad_norm": 8835.09094463662,
      "learning_rate": 4.625616079131259e-07,
      "loss": 1.8102,
      "step": 46784
    },
    {
      "epoch": 0.00016990455527410073,
      "grad_norm": 10521.637895308886,
      "learning_rate": 4.624033352939931e-07,
      "loss": 1.8082,
      "step": 46816
    },
    {
      "epoch": 0.00017002068962493743,
      "grad_norm": 9548.382271358849,
      "learning_rate": 4.6224522503004356e-07,
      "loss": 1.7879,
      "step": 46848
    },
    {
      "epoch": 0.00017013682397577415,
      "grad_norm": 10153.918061516943,
      "learning_rate": 4.6209221027413665e-07,
      "loss": 1.7801,
      "step": 46880
    },
    {
      "epoch": 0.00017025295832661085,
      "grad_norm": 8211.751457515016,
      "learning_rate": 4.6193441883696855e-07,
      "loss": 1.7703,
      "step": 46912
    },
    {
      "epoch": 0.00017036909267744755,
      "grad_norm": 8838.703637977687,
      "learning_rate": 4.617767889334413e-07,
      "loss": 1.7781,
      "step": 46944
    },
    {
      "epoch": 0.00017048522702828425,
      "grad_norm": 10038.956220643659,
      "learning_rate": 4.616193202881352e-07,
      "loss": 1.7832,
      "step": 46976
    },
    {
      "epoch": 0.00017060136137912098,
      "grad_norm": 9762.317757581957,
      "learning_rate": 4.6146201262628753e-07,
      "loss": 1.796,
      "step": 47008
    },
    {
      "epoch": 0.00017071749572995768,
      "grad_norm": 9866.026555812628,
      "learning_rate": 4.613048656737905e-07,
      "loss": 1.7966,
      "step": 47040
    },
    {
      "epoch": 0.00017083363008079438,
      "grad_norm": 9048.9577300372,
      "learning_rate": 4.611478791571895e-07,
      "loss": 1.8012,
      "step": 47072
    },
    {
      "epoch": 0.00017094976443163108,
      "grad_norm": 10291.782547255845,
      "learning_rate": 4.609910528036807e-07,
      "loss": 1.8113,
      "step": 47104
    },
    {
      "epoch": 0.00017106589878246778,
      "grad_norm": 10156.43234605538,
      "learning_rate": 4.608343863411092e-07,
      "loss": 1.8008,
      "step": 47136
    },
    {
      "epoch": 0.0001711820331333045,
      "grad_norm": 9523.86035176913,
      "learning_rate": 4.606778794979673e-07,
      "loss": 1.7936,
      "step": 47168
    },
    {
      "epoch": 0.0001712981674841412,
      "grad_norm": 10141.776570207016,
      "learning_rate": 4.605215320033921e-07,
      "loss": 1.7814,
      "step": 47200
    },
    {
      "epoch": 0.0001714143018349779,
      "grad_norm": 9937.079047688008,
      "learning_rate": 4.6036534358716396e-07,
      "loss": 1.7866,
      "step": 47232
    },
    {
      "epoch": 0.0001715304361858146,
      "grad_norm": 10161.899330341745,
      "learning_rate": 4.6020931397970415e-07,
      "loss": 1.8014,
      "step": 47264
    },
    {
      "epoch": 0.0001716465705366513,
      "grad_norm": 9705.337603607615,
      "learning_rate": 4.600534429120733e-07,
      "loss": 1.7601,
      "step": 47296
    },
    {
      "epoch": 0.00017176270488748803,
      "grad_norm": 7933.874841463029,
      "learning_rate": 4.5989773011596903e-07,
      "loss": 1.7799,
      "step": 47328
    },
    {
      "epoch": 0.00017187883923832473,
      "grad_norm": 9579.848224267438,
      "learning_rate": 4.5974217532372415e-07,
      "loss": 1.7836,
      "step": 47360
    },
    {
      "epoch": 0.00017199497358916143,
      "grad_norm": 9333.081698988819,
      "learning_rate": 4.595867782683051e-07,
      "loss": 1.804,
      "step": 47392
    },
    {
      "epoch": 0.00017211110793999813,
      "grad_norm": 9142.484782596031,
      "learning_rate": 4.5943153868330937e-07,
      "loss": 1.8184,
      "step": 47424
    },
    {
      "epoch": 0.00017222724229083486,
      "grad_norm": 10718.83902295393,
      "learning_rate": 4.5927645630296423e-07,
      "loss": 1.7882,
      "step": 47456
    },
    {
      "epoch": 0.00017234337664167156,
      "grad_norm": 8814.573954536883,
      "learning_rate": 4.591215308621242e-07,
      "loss": 1.7948,
      "step": 47488
    },
    {
      "epoch": 0.00017245951099250826,
      "grad_norm": 10386.650952063423,
      "learning_rate": 4.589667620962697e-07,
      "loss": 1.8005,
      "step": 47520
    },
    {
      "epoch": 0.00017257564534334496,
      "grad_norm": 10462.978925717092,
      "learning_rate": 4.588121497415048e-07,
      "loss": 1.8217,
      "step": 47552
    },
    {
      "epoch": 0.00017269177969418166,
      "grad_norm": 9928.560016437428,
      "learning_rate": 4.5865769353455556e-07,
      "loss": 1.7955,
      "step": 47584
    },
    {
      "epoch": 0.00017280791404501839,
      "grad_norm": 10077.650321379482,
      "learning_rate": 4.585033932127678e-07,
      "loss": 1.8076,
      "step": 47616
    },
    {
      "epoch": 0.00017292404839585509,
      "grad_norm": 9646.603962016892,
      "learning_rate": 4.5834924851410564e-07,
      "loss": 1.7942,
      "step": 47648
    },
    {
      "epoch": 0.00017304018274669179,
      "grad_norm": 9786.188635010058,
      "learning_rate": 4.581952591771495e-07,
      "loss": 1.7859,
      "step": 47680
    },
    {
      "epoch": 0.00017315631709752849,
      "grad_norm": 10213.597603195458,
      "learning_rate": 4.58041424941094e-07,
      "loss": 1.7759,
      "step": 47712
    },
    {
      "epoch": 0.0001732724514483652,
      "grad_norm": 10090.43398472038,
      "learning_rate": 4.578877455457463e-07,
      "loss": 1.7809,
      "step": 47744
    },
    {
      "epoch": 0.0001733885857992019,
      "grad_norm": 12012.326169397833,
      "learning_rate": 4.5773422073152446e-07,
      "loss": 1.7786,
      "step": 47776
    },
    {
      "epoch": 0.0001735047201500386,
      "grad_norm": 8474.833685683749,
      "learning_rate": 4.575808502394551e-07,
      "loss": 1.7699,
      "step": 47808
    },
    {
      "epoch": 0.0001736208545008753,
      "grad_norm": 12210.503347528309,
      "learning_rate": 4.574276338111721e-07,
      "loss": 1.7967,
      "step": 47840
    },
    {
      "epoch": 0.000173736988851712,
      "grad_norm": 8305.931856209754,
      "learning_rate": 4.5727457118891426e-07,
      "loss": 1.7905,
      "step": 47872
    },
    {
      "epoch": 0.00017385312320254874,
      "grad_norm": 8964.951645156822,
      "learning_rate": 4.571264382023913e-07,
      "loss": 1.7923,
      "step": 47904
    },
    {
      "epoch": 0.00017396925755338544,
      "grad_norm": 9588.549629636382,
      "learning_rate": 4.569736776348049e-07,
      "loss": 1.7986,
      "step": 47936
    },
    {
      "epoch": 0.00017408539190422214,
      "grad_norm": 9997.212911606914,
      "learning_rate": 4.568210701115629e-07,
      "loss": 1.7771,
      "step": 47968
    },
    {
      "epoch": 0.00017420152625505884,
      "grad_norm": 9485.62902500409,
      "learning_rate": 4.5666861537728825e-07,
      "loss": 1.7803,
      "step": 48000
    },
    {
      "epoch": 0.00017431766060589557,
      "grad_norm": 10068.763975781734,
      "learning_rate": 4.5651631317719975e-07,
      "loss": 1.794,
      "step": 48032
    },
    {
      "epoch": 0.00017443379495673227,
      "grad_norm": 9943.681410825671,
      "learning_rate": 4.563641632571109e-07,
      "loss": 1.7752,
      "step": 48064
    },
    {
      "epoch": 0.00017454992930756897,
      "grad_norm": 8601.262814261636,
      "learning_rate": 4.5621216536342764e-07,
      "loss": 1.7995,
      "step": 48096
    },
    {
      "epoch": 0.00017466606365840567,
      "grad_norm": 12189.22097592787,
      "learning_rate": 4.56060319243147e-07,
      "loss": 1.8185,
      "step": 48128
    },
    {
      "epoch": 0.00017478219800924237,
      "grad_norm": 9138.77902129163,
      "learning_rate": 4.5590862464385483e-07,
      "loss": 1.8041,
      "step": 48160
    },
    {
      "epoch": 0.0001748983323600791,
      "grad_norm": 9072.855228647706,
      "learning_rate": 4.557570813137245e-07,
      "loss": 1.8037,
      "step": 48192
    },
    {
      "epoch": 0.0001750144667109158,
      "grad_norm": 9333.765156677127,
      "learning_rate": 4.556056890015147e-07,
      "loss": 1.8078,
      "step": 48224
    },
    {
      "epoch": 0.0001751306010617525,
      "grad_norm": 9175.8553824698,
      "learning_rate": 4.5545444745656807e-07,
      "loss": 1.8015,
      "step": 48256
    },
    {
      "epoch": 0.0001752467354125892,
      "grad_norm": 9257.115965569406,
      "learning_rate": 4.5530335642880943e-07,
      "loss": 1.8107,
      "step": 48288
    },
    {
      "epoch": 0.00017536286976342592,
      "grad_norm": 10064.637002892852,
      "learning_rate": 4.5515241566874365e-07,
      "loss": 1.7991,
      "step": 48320
    },
    {
      "epoch": 0.00017547900411426262,
      "grad_norm": 9818.608760919236,
      "learning_rate": 4.550016249274546e-07,
      "loss": 1.8135,
      "step": 48352
    },
    {
      "epoch": 0.00017559513846509932,
      "grad_norm": 10826.60260654283,
      "learning_rate": 4.5485098395660253e-07,
      "loss": 1.7915,
      "step": 48384
    },
    {
      "epoch": 0.00017571127281593602,
      "grad_norm": 9417.721380461411,
      "learning_rate": 4.5470049250842316e-07,
      "loss": 1.7862,
      "step": 48416
    },
    {
      "epoch": 0.00017582740716677272,
      "grad_norm": 8970.419053756632,
      "learning_rate": 4.5455015033572566e-07,
      "loss": 1.7784,
      "step": 48448
    },
    {
      "epoch": 0.00017594354151760945,
      "grad_norm": 9542.39089536789,
      "learning_rate": 4.543999571918909e-07,
      "loss": 1.7586,
      "step": 48480
    },
    {
      "epoch": 0.00017605967586844615,
      "grad_norm": 8817.566841255019,
      "learning_rate": 4.542499128308697e-07,
      "loss": 1.7639,
      "step": 48512
    },
    {
      "epoch": 0.00017617581021928285,
      "grad_norm": 9348.475704626931,
      "learning_rate": 4.5410001700718123e-07,
      "loss": 1.7695,
      "step": 48544
    },
    {
      "epoch": 0.00017629194457011955,
      "grad_norm": 9851.197592171216,
      "learning_rate": 4.5395026947591153e-07,
      "loss": 1.7548,
      "step": 48576
    },
    {
      "epoch": 0.00017640807892095627,
      "grad_norm": 9021.802924027992,
      "learning_rate": 4.5380066999271144e-07,
      "loss": 1.7794,
      "step": 48608
    },
    {
      "epoch": 0.00017652421327179297,
      "grad_norm": 10473.277996883306,
      "learning_rate": 4.53651218313795e-07,
      "loss": 1.801,
      "step": 48640
    },
    {
      "epoch": 0.00017664034762262967,
      "grad_norm": 10262.956396672453,
      "learning_rate": 4.535019141959383e-07,
      "loss": 1.7985,
      "step": 48672
    },
    {
      "epoch": 0.00017675648197346637,
      "grad_norm": 10104.429721661683,
      "learning_rate": 4.533527573964769e-07,
      "loss": 1.7958,
      "step": 48704
    },
    {
      "epoch": 0.00017687261632430307,
      "grad_norm": 10786.141849614254,
      "learning_rate": 4.53203747673305e-07,
      "loss": 1.78,
      "step": 48736
    },
    {
      "epoch": 0.0001769887506751398,
      "grad_norm": 9684.866442032126,
      "learning_rate": 4.5305488478487336e-07,
      "loss": 1.7895,
      "step": 48768
    },
    {
      "epoch": 0.0001771048850259765,
      "grad_norm": 9617.467650062566,
      "learning_rate": 4.529061684901879e-07,
      "loss": 1.793,
      "step": 48800
    },
    {
      "epoch": 0.0001772210193768132,
      "grad_norm": 9223.923243392694,
      "learning_rate": 4.527575985488077e-07,
      "loss": 1.7941,
      "step": 48832
    },
    {
      "epoch": 0.0001773371537276499,
      "grad_norm": 9396.991007764134,
      "learning_rate": 4.526091747208436e-07,
      "loss": 1.805,
      "step": 48864
    },
    {
      "epoch": 0.00017745328807848663,
      "grad_norm": 10127.391174433818,
      "learning_rate": 4.5246552824733447e-07,
      "loss": 1.8054,
      "step": 48896
    },
    {
      "epoch": 0.00017756942242932333,
      "grad_norm": 9504.557643572898,
      "learning_rate": 4.52317391381241e-07,
      "loss": 1.8147,
      "step": 48928
    },
    {
      "epoch": 0.00017768555678016003,
      "grad_norm": 10626.605666909825,
      "learning_rate": 4.5216939991962764e-07,
      "loss": 1.8325,
      "step": 48960
    },
    {
      "epoch": 0.00017780169113099673,
      "grad_norm": 9917.045124430968,
      "learning_rate": 4.520215536247793e-07,
      "loss": 1.804,
      "step": 48992
    },
    {
      "epoch": 0.00017791782548183343,
      "grad_norm": 9705.963012499069,
      "learning_rate": 4.5187385225952426e-07,
      "loss": 1.7983,
      "step": 49024
    },
    {
      "epoch": 0.00017803395983267015,
      "grad_norm": 9135.358887312528,
      "learning_rate": 4.517262955872331e-07,
      "loss": 1.7928,
      "step": 49056
    },
    {
      "epoch": 0.00017815009418350685,
      "grad_norm": 8534.614343952513,
      "learning_rate": 4.5157888337181707e-07,
      "loss": 1.7892,
      "step": 49088
    },
    {
      "epoch": 0.00017826622853434355,
      "grad_norm": 9022.548863818914,
      "learning_rate": 4.5143161537772624e-07,
      "loss": 1.7824,
      "step": 49120
    },
    {
      "epoch": 0.00017838236288518025,
      "grad_norm": 8622.1839460777,
      "learning_rate": 4.512844913699479e-07,
      "loss": 1.7917,
      "step": 49152
    },
    {
      "epoch": 0.00017849849723601698,
      "grad_norm": 9691.026571008873,
      "learning_rate": 4.5113751111400537e-07,
      "loss": 1.7579,
      "step": 49184
    },
    {
      "epoch": 0.00017861463158685368,
      "grad_norm": 11916.507709895546,
      "learning_rate": 4.509906743759561e-07,
      "loss": 1.7746,
      "step": 49216
    },
    {
      "epoch": 0.00017873076593769038,
      "grad_norm": 10226.124583633822,
      "learning_rate": 4.5084398092239026e-07,
      "loss": 1.7901,
      "step": 49248
    },
    {
      "epoch": 0.00017884690028852708,
      "grad_norm": 8992.070284422825,
      "learning_rate": 4.5069743052042893e-07,
      "loss": 1.7968,
      "step": 49280
    },
    {
      "epoch": 0.00017896303463936378,
      "grad_norm": 10047.11043036753,
      "learning_rate": 4.50551022937723e-07,
      "loss": 1.7701,
      "step": 49312
    },
    {
      "epoch": 0.0001790791689902005,
      "grad_norm": 10337.542067629036,
      "learning_rate": 4.504047579424512e-07,
      "loss": 1.7733,
      "step": 49344
    },
    {
      "epoch": 0.0001791953033410372,
      "grad_norm": 9783.527789095302,
      "learning_rate": 4.502586353033188e-07,
      "loss": 1.7957,
      "step": 49376
    },
    {
      "epoch": 0.0001793114376918739,
      "grad_norm": 9188.580086172184,
      "learning_rate": 4.501126547895558e-07,
      "loss": 1.7978,
      "step": 49408
    },
    {
      "epoch": 0.0001794275720427106,
      "grad_norm": 9988.62713289469,
      "learning_rate": 4.49966816170916e-07,
      "loss": 1.7879,
      "step": 49440
    },
    {
      "epoch": 0.00017954370639354733,
      "grad_norm": 9209.480441371272,
      "learning_rate": 4.498211192176746e-07,
      "loss": 1.7768,
      "step": 49472
    },
    {
      "epoch": 0.00017965984074438403,
      "grad_norm": 10126.784484721691,
      "learning_rate": 4.496755637006275e-07,
      "loss": 1.7657,
      "step": 49504
    },
    {
      "epoch": 0.00017977597509522073,
      "grad_norm": 9565.786951422242,
      "learning_rate": 4.495301493910893e-07,
      "loss": 1.7893,
      "step": 49536
    },
    {
      "epoch": 0.00017989210944605743,
      "grad_norm": 10152.202913653766,
      "learning_rate": 4.493848760608918e-07,
      "loss": 1.7882,
      "step": 49568
    },
    {
      "epoch": 0.00018000824379689413,
      "grad_norm": 9524.01532968107,
      "learning_rate": 4.492397434823828e-07,
      "loss": 1.7921,
      "step": 49600
    },
    {
      "epoch": 0.00018012437814773086,
      "grad_norm": 9480.002742615638,
      "learning_rate": 4.490947514284244e-07,
      "loss": 1.803,
      "step": 49632
    },
    {
      "epoch": 0.00018024051249856756,
      "grad_norm": 10078.465557811864,
      "learning_rate": 4.489498996723915e-07,
      "loss": 1.8004,
      "step": 49664
    },
    {
      "epoch": 0.00018035664684940426,
      "grad_norm": 9406.679010150181,
      "learning_rate": 4.4880518798817036e-07,
      "loss": 1.7975,
      "step": 49696
    },
    {
      "epoch": 0.00018047278120024096,
      "grad_norm": 10679.758517869212,
      "learning_rate": 4.48660616150157e-07,
      "loss": 1.7969,
      "step": 49728
    },
    {
      "epoch": 0.0001805889155510777,
      "grad_norm": 9910.262458683927,
      "learning_rate": 4.48516183933256e-07,
      "loss": 1.799,
      "step": 49760
    },
    {
      "epoch": 0.0001807050499019144,
      "grad_norm": 9085.491951457554,
      "learning_rate": 4.483718911128787e-07,
      "loss": 1.8022,
      "step": 49792
    },
    {
      "epoch": 0.00018082118425275109,
      "grad_norm": 9340.93292985235,
      "learning_rate": 4.482277374649419e-07,
      "loss": 1.8113,
      "step": 49824
    },
    {
      "epoch": 0.00018093731860358779,
      "grad_norm": 8732.09699900316,
      "learning_rate": 4.4808372276586657e-07,
      "loss": 1.818,
      "step": 49856
    },
    {
      "epoch": 0.00018105345295442449,
      "grad_norm": 9364.789372965097,
      "learning_rate": 4.479443408191003e-07,
      "loss": 1.7989,
      "step": 49888
    },
    {
      "epoch": 0.0001811695873052612,
      "grad_norm": 9942.247834368241,
      "learning_rate": 4.478005990241576e-07,
      "loss": 1.7837,
      "step": 49920
    },
    {
      "epoch": 0.0001812857216560979,
      "grad_norm": 8577.958731539806,
      "learning_rate": 4.476569955172804e-07,
      "loss": 1.7647,
      "step": 49952
    },
    {
      "epoch": 0.0001814018560069346,
      "grad_norm": 8526.477349996305,
      "learning_rate": 4.475135300768755e-07,
      "loss": 1.79,
      "step": 49984
    },
    {
      "epoch": 0.0001815179903577713,
      "grad_norm": 8872.128267783328,
      "learning_rate": 4.473702024818462e-07,
      "loss": 1.7842,
      "step": 50016
    },
    {
      "epoch": 0.000181634124708608,
      "grad_norm": 10489.143339663158,
      "learning_rate": 4.472270125115915e-07,
      "loss": 1.7786,
      "step": 50048
    },
    {
      "epoch": 0.00018175025905944474,
      "grad_norm": 10406.077743319045,
      "learning_rate": 4.4708395994600416e-07,
      "loss": 1.7645,
      "step": 50080
    },
    {
      "epoch": 0.00018186639341028144,
      "grad_norm": 9604.146812705436,
      "learning_rate": 4.4694104456546943e-07,
      "loss": 1.7795,
      "step": 50112
    },
    {
      "epoch": 0.00018198252776111814,
      "grad_norm": 9632.673564488729,
      "learning_rate": 4.4679826615086383e-07,
      "loss": 1.7914,
      "step": 50144
    },
    {
      "epoch": 0.00018209866211195484,
      "grad_norm": 10369.06649607379,
      "learning_rate": 4.466556244835534e-07,
      "loss": 1.8019,
      "step": 50176
    },
    {
      "epoch": 0.00018221479646279157,
      "grad_norm": 9886.433532877263,
      "learning_rate": 4.4651311934539243e-07,
      "loss": 1.8053,
      "step": 50208
    },
    {
      "epoch": 0.00018233093081362827,
      "grad_norm": 9609.09173647541,
      "learning_rate": 4.463707505187224e-07,
      "loss": 1.7578,
      "step": 50240
    },
    {
      "epoch": 0.00018244706516446497,
      "grad_norm": 10642.350210362372,
      "learning_rate": 4.4622851778637e-07,
      "loss": 1.7544,
      "step": 50272
    },
    {
      "epoch": 0.00018256319951530167,
      "grad_norm": 9987.15294766231,
      "learning_rate": 4.4608642093164596e-07,
      "loss": 1.763,
      "step": 50304
    },
    {
      "epoch": 0.00018267933386613837,
      "grad_norm": 9236.389987435567,
      "learning_rate": 4.4594445973834397e-07,
      "loss": 1.7616,
      "step": 50336
    },
    {
      "epoch": 0.0001827954682169751,
      "grad_norm": 11085.58514468226,
      "learning_rate": 4.4580263399073887e-07,
      "loss": 1.7914,
      "step": 50368
    },
    {
      "epoch": 0.0001829116025678118,
      "grad_norm": 12003.359029871597,
      "learning_rate": 4.456609434735855e-07,
      "loss": 1.8005,
      "step": 50400
    },
    {
      "epoch": 0.0001830277369186485,
      "grad_norm": 8600.675438592018,
      "learning_rate": 4.455193879721174e-07,
      "loss": 1.8035,
      "step": 50432
    },
    {
      "epoch": 0.0001831438712694852,
      "grad_norm": 10633.500740583979,
      "learning_rate": 4.453779672720451e-07,
      "loss": 1.8176,
      "step": 50464
    },
    {
      "epoch": 0.00018326000562032192,
      "grad_norm": 9786.777304097606,
      "learning_rate": 4.45236681159555e-07,
      "loss": 1.8259,
      "step": 50496
    },
    {
      "epoch": 0.00018337613997115862,
      "grad_norm": 9185.612554424446,
      "learning_rate": 4.450955294213082e-07,
      "loss": 1.8026,
      "step": 50528
    },
    {
      "epoch": 0.00018349227432199532,
      "grad_norm": 10007.778574688791,
      "learning_rate": 4.4495451184443883e-07,
      "loss": 1.8179,
      "step": 50560
    },
    {
      "epoch": 0.00018360840867283202,
      "grad_norm": 10826.056992275628,
      "learning_rate": 4.448136282165527e-07,
      "loss": 1.8251,
      "step": 50592
    },
    {
      "epoch": 0.00018372454302366872,
      "grad_norm": 9404.128029753741,
      "learning_rate": 4.4467287832572624e-07,
      "loss": 1.8122,
      "step": 50624
    },
    {
      "epoch": 0.00018384067737450545,
      "grad_norm": 9730.40225273344,
      "learning_rate": 4.445322619605049e-07,
      "loss": 1.7912,
      "step": 50656
    },
    {
      "epoch": 0.00018395681172534215,
      "grad_norm": 10132.698949440864,
      "learning_rate": 4.443917789099019e-07,
      "loss": 1.7916,
      "step": 50688
    },
    {
      "epoch": 0.00018407294607617885,
      "grad_norm": 10851.131738210537,
      "learning_rate": 4.442514289633969e-07,
      "loss": 1.7737,
      "step": 50720
    },
    {
      "epoch": 0.00018418908042701555,
      "grad_norm": 9326.273639562587,
      "learning_rate": 4.4411121191093467e-07,
      "loss": 1.7688,
      "step": 50752
    },
    {
      "epoch": 0.00018430521477785227,
      "grad_norm": 10098.605547302062,
      "learning_rate": 4.439711275429239e-07,
      "loss": 1.7956,
      "step": 50784
    },
    {
      "epoch": 0.00018442134912868897,
      "grad_norm": 10777.456657300923,
      "learning_rate": 4.438311756502356e-07,
      "loss": 1.7751,
      "step": 50816
    },
    {
      "epoch": 0.00018453748347952567,
      "grad_norm": 10192.120780289057,
      "learning_rate": 4.43691356024202e-07,
      "loss": 1.7573,
      "step": 50848
    },
    {
      "epoch": 0.00018465361783036237,
      "grad_norm": 11309.406880999552,
      "learning_rate": 4.435516684566152e-07,
      "loss": 1.7652,
      "step": 50880
    },
    {
      "epoch": 0.00018476975218119907,
      "grad_norm": 9409.878851504944,
      "learning_rate": 4.434164718621521e-07,
      "loss": 1.7887,
      "step": 50912
    },
    {
      "epoch": 0.0001848858865320358,
      "grad_norm": 10285.796031421194,
      "learning_rate": 4.432770436779451e-07,
      "loss": 1.7827,
      "step": 50944
    },
    {
      "epoch": 0.0001850020208828725,
      "grad_norm": 8257.347758209049,
      "learning_rate": 4.4313774693676424e-07,
      "loss": 1.7982,
      "step": 50976
    },
    {
      "epoch": 0.0001851181552337092,
      "grad_norm": 10360.358584527854,
      "learning_rate": 4.4299858143221383e-07,
      "loss": 1.7669,
      "step": 51008
    },
    {
      "epoch": 0.0001852342895845459,
      "grad_norm": 10242.138350949961,
      "learning_rate": 4.428595469583518e-07,
      "loss": 1.7604,
      "step": 51040
    },
    {
      "epoch": 0.00018535042393538263,
      "grad_norm": 8321.451676240149,
      "learning_rate": 4.4272064330968815e-07,
      "loss": 1.7824,
      "step": 51072
    },
    {
      "epoch": 0.00018546655828621933,
      "grad_norm": 10277.107180525072,
      "learning_rate": 4.4258187028118375e-07,
      "loss": 1.7891,
      "step": 51104
    },
    {
      "epoch": 0.00018558269263705603,
      "grad_norm": 8572.04736337825,
      "learning_rate": 4.4244322766824906e-07,
      "loss": 1.7934,
      "step": 51136
    },
    {
      "epoch": 0.00018569882698789273,
      "grad_norm": 9189.107900117398,
      "learning_rate": 4.4230471526674305e-07,
      "loss": 1.8079,
      "step": 51168
    },
    {
      "epoch": 0.00018581496133872943,
      "grad_norm": 8934.084172426405,
      "learning_rate": 4.421663328729715e-07,
      "loss": 1.8125,
      "step": 51200
    },
    {
      "epoch": 0.00018593109568956615,
      "grad_norm": 8644.879177871719,
      "learning_rate": 4.4202808028368623e-07,
      "loss": 1.8053,
      "step": 51232
    },
    {
      "epoch": 0.00018604723004040285,
      "grad_norm": 8941.116037721466,
      "learning_rate": 4.418899572960836e-07,
      "loss": 1.7984,
      "step": 51264
    },
    {
      "epoch": 0.00018616336439123955,
      "grad_norm": 8711.92458645046,
      "learning_rate": 4.417519637078032e-07,
      "loss": 1.7998,
      "step": 51296
    },
    {
      "epoch": 0.00018627949874207625,
      "grad_norm": 10066.904390129073,
      "learning_rate": 4.4161409931692674e-07,
      "loss": 1.7982,
      "step": 51328
    },
    {
      "epoch": 0.00018639563309291298,
      "grad_norm": 8769.57102713696,
      "learning_rate": 4.4147636392197683e-07,
      "loss": 1.8116,
      "step": 51360
    },
    {
      "epoch": 0.00018651176744374968,
      "grad_norm": 10695.378441177292,
      "learning_rate": 4.413387573219156e-07,
      "loss": 1.8227,
      "step": 51392
    },
    {
      "epoch": 0.00018662790179458638,
      "grad_norm": 9994.516796724092,
      "learning_rate": 4.412012793161435e-07,
      "loss": 1.8094,
      "step": 51424
    },
    {
      "epoch": 0.00018674403614542308,
      "grad_norm": 10028.09064578098,
      "learning_rate": 4.410639297044983e-07,
      "loss": 1.7617,
      "step": 51456
    },
    {
      "epoch": 0.00018686017049625978,
      "grad_norm": 9796.596653940593,
      "learning_rate": 4.4092670828725353e-07,
      "loss": 1.771,
      "step": 51488
    },
    {
      "epoch": 0.0001869763048470965,
      "grad_norm": 10743.476346136757,
      "learning_rate": 4.407896148651174e-07,
      "loss": 1.7778,
      "step": 51520
    },
    {
      "epoch": 0.0001870924391979332,
      "grad_norm": 10448.406289956378,
      "learning_rate": 4.4065264923923174e-07,
      "loss": 1.7937,
      "step": 51552
    },
    {
      "epoch": 0.0001872085735487699,
      "grad_norm": 9365.918961853129,
      "learning_rate": 4.405158112111706e-07,
      "loss": 1.7666,
      "step": 51584
    },
    {
      "epoch": 0.0001873247078996066,
      "grad_norm": 8160.27511791116,
      "learning_rate": 4.4037910058293887e-07,
      "loss": 1.7672,
      "step": 51616
    },
    {
      "epoch": 0.00018744084225044333,
      "grad_norm": 10184.980215984713,
      "learning_rate": 4.402425171569716e-07,
      "loss": 1.7731,
      "step": 51648
    },
    {
      "epoch": 0.00018755697660128003,
      "grad_norm": 8477.200481290978,
      "learning_rate": 4.401060607361324e-07,
      "loss": 1.7929,
      "step": 51680
    },
    {
      "epoch": 0.00018767311095211673,
      "grad_norm": 10832.015324952232,
      "learning_rate": 4.3996973112371225e-07,
      "loss": 1.8095,
      "step": 51712
    },
    {
      "epoch": 0.00018778924530295343,
      "grad_norm": 10217.31647743183,
      "learning_rate": 4.398335281234285e-07,
      "loss": 1.7994,
      "step": 51744
    },
    {
      "epoch": 0.00018790537965379013,
      "grad_norm": 9192.97111928456,
      "learning_rate": 4.396974515394236e-07,
      "loss": 1.7839,
      "step": 51776
    },
    {
      "epoch": 0.00018802151400462686,
      "grad_norm": 10002.561971815021,
      "learning_rate": 4.395615011762637e-07,
      "loss": 1.7802,
      "step": 51808
    },
    {
      "epoch": 0.00018813764835546356,
      "grad_norm": 10043.37254113378,
      "learning_rate": 4.3942567683893806e-07,
      "loss": 1.7832,
      "step": 51840
    },
    {
      "epoch": 0.00018825378270630026,
      "grad_norm": 10034.081921132596,
      "learning_rate": 4.3928997833285704e-07,
      "loss": 1.7703,
      "step": 51872
    },
    {
      "epoch": 0.00018836991705713696,
      "grad_norm": 11291.026436954258,
      "learning_rate": 4.391586402162007e-07,
      "loss": 1.7835,
      "step": 51904
    },
    {
      "epoch": 0.0001884860514079737,
      "grad_norm": 9343.882704743248,
      "learning_rate": 4.3902318887334586e-07,
      "loss": 1.7886,
      "step": 51936
    },
    {
      "epoch": 0.0001886021857588104,
      "grad_norm": 10157.309190922564,
      "learning_rate": 4.388878627865203e-07,
      "loss": 1.8077,
      "step": 51968
    },
    {
      "epoch": 0.0001887183201096471,
      "grad_norm": 9220.96307334543,
      "learning_rate": 4.387526617627962e-07,
      "loss": 1.7941,
      "step": 52000
    },
    {
      "epoch": 0.0001888344544604838,
      "grad_norm": 8792.520685218773,
      "learning_rate": 4.386175856096614e-07,
      "loss": 1.8026,
      "step": 52032
    },
    {
      "epoch": 0.0001889505888113205,
      "grad_norm": 9711.278391643398,
      "learning_rate": 4.384826341350183e-07,
      "loss": 1.7821,
      "step": 52064
    },
    {
      "epoch": 0.00018906672316215721,
      "grad_norm": 9381.160216092676,
      "learning_rate": 4.3834780714718287e-07,
      "loss": 1.796,
      "step": 52096
    },
    {
      "epoch": 0.00018918285751299391,
      "grad_norm": 9067.560421634917,
      "learning_rate": 4.3821310445488347e-07,
      "loss": 1.8098,
      "step": 52128
    },
    {
      "epoch": 0.00018929899186383061,
      "grad_norm": 8512.369470364876,
      "learning_rate": 4.380785258672594e-07,
      "loss": 1.7924,
      "step": 52160
    },
    {
      "epoch": 0.0001894151262146673,
      "grad_norm": 9577.45415024264,
      "learning_rate": 4.3794407119386034e-07,
      "loss": 1.7858,
      "step": 52192
    },
    {
      "epoch": 0.00018953126056550404,
      "grad_norm": 9817.593187742095,
      "learning_rate": 4.378097402446446e-07,
      "loss": 1.7727,
      "step": 52224
    },
    {
      "epoch": 0.00018964739491634074,
      "grad_norm": 10464.495974484389,
      "learning_rate": 4.376755328299784e-07,
      "loss": 1.7753,
      "step": 52256
    },
    {
      "epoch": 0.00018976352926717744,
      "grad_norm": 10092.708853424832,
      "learning_rate": 4.375414487606347e-07,
      "loss": 1.7876,
      "step": 52288
    },
    {
      "epoch": 0.00018987966361801414,
      "grad_norm": 10026.747478619376,
      "learning_rate": 4.374074878477919e-07,
      "loss": 1.817,
      "step": 52320
    },
    {
      "epoch": 0.00018999579796885084,
      "grad_norm": 9336.116430293701,
      "learning_rate": 4.372736499030328e-07,
      "loss": 1.7995,
      "step": 52352
    },
    {
      "epoch": 0.00019011193231968757,
      "grad_norm": 10136.99659662565,
      "learning_rate": 4.371399347383436e-07,
      "loss": 1.7937,
      "step": 52384
    },
    {
      "epoch": 0.00019022806667052427,
      "grad_norm": 9885.791116547021,
      "learning_rate": 4.3700634216611283e-07,
      "loss": 1.7937,
      "step": 52416
    },
    {
      "epoch": 0.00019034420102136097,
      "grad_norm": 10690.513364661212,
      "learning_rate": 4.3687287199912974e-07,
      "loss": 1.7821,
      "step": 52448
    },
    {
      "epoch": 0.00019046033537219767,
      "grad_norm": 10063.775931527887,
      "learning_rate": 4.36739524050584e-07,
      "loss": 1.7812,
      "step": 52480
    },
    {
      "epoch": 0.00019057646972303437,
      "grad_norm": 9259.536489479373,
      "learning_rate": 4.3660629813406394e-07,
      "loss": 1.7614,
      "step": 52512
    },
    {
      "epoch": 0.0001906926040738711,
      "grad_norm": 8440.532210708043,
      "learning_rate": 4.364731940635558e-07,
      "loss": 1.7641,
      "step": 52544
    },
    {
      "epoch": 0.0001908087384247078,
      "grad_norm": 9194.394161661769,
      "learning_rate": 4.363402116534423e-07,
      "loss": 1.7784,
      "step": 52576
    },
    {
      "epoch": 0.0001909248727755445,
      "grad_norm": 8571.693298292934,
      "learning_rate": 4.3620735071850214e-07,
      "loss": 1.767,
      "step": 52608
    },
    {
      "epoch": 0.0001910410071263812,
      "grad_norm": 9274.767921624778,
      "learning_rate": 4.3607461107390845e-07,
      "loss": 1.7951,
      "step": 52640
    },
    {
      "epoch": 0.00019115714147721792,
      "grad_norm": 10387.0894864731,
      "learning_rate": 4.359419925352277e-07,
      "loss": 1.7913,
      "step": 52672
    },
    {
      "epoch": 0.00019127327582805462,
      "grad_norm": 9933.889771886943,
      "learning_rate": 4.3580949491841877e-07,
      "loss": 1.8031,
      "step": 52704
    },
    {
      "epoch": 0.00019138941017889132,
      "grad_norm": 8588.186537331383,
      "learning_rate": 4.3567711803983206e-07,
      "loss": 1.7911,
      "step": 52736
    },
    {
      "epoch": 0.00019150554452972802,
      "grad_norm": 9583.184856820828,
      "learning_rate": 4.3554486171620797e-07,
      "loss": 1.7877,
      "step": 52768
    },
    {
      "epoch": 0.00019162167888056472,
      "grad_norm": 9544.839233847786,
      "learning_rate": 4.354127257646762e-07,
      "loss": 1.7886,
      "step": 52800
    },
    {
      "epoch": 0.00019173781323140145,
      "grad_norm": 9917.563410435045,
      "learning_rate": 4.352807100027546e-07,
      "loss": 1.7939,
      "step": 52832
    },
    {
      "epoch": 0.00019185394758223815,
      "grad_norm": 10007.131856830907,
      "learning_rate": 4.3514881424834796e-07,
      "loss": 1.788,
      "step": 52864
    },
    {
      "epoch": 0.00019197008193307485,
      "grad_norm": 9256.75213020204,
      "learning_rate": 4.350211545055486e-07,
      "loss": 1.8044,
      "step": 52896
    },
    {
      "epoch": 0.00019208621628391155,
      "grad_norm": 9567.03005117053,
      "learning_rate": 4.3488949448528033e-07,
      "loss": 1.8245,
      "step": 52928
    },
    {
      "epoch": 0.00019220235063474827,
      "grad_norm": 10926.590227513796,
      "learning_rate": 4.347579539342021e-07,
      "loss": 1.8087,
      "step": 52960
    },
    {
      "epoch": 0.00019231848498558497,
      "grad_norm": 9995.675965136124,
      "learning_rate": 4.3462653267174455e-07,
      "loss": 1.7734,
      "step": 52992
    },
    {
      "epoch": 0.00019243461933642167,
      "grad_norm": 9121.815608748075,
      "learning_rate": 4.344952305177202e-07,
      "loss": 1.7673,
      "step": 53024
    },
    {
      "epoch": 0.00019255075368725837,
      "grad_norm": 9416.018160560227,
      "learning_rate": 4.343640472923223e-07,
      "loss": 1.7835,
      "step": 53056
    },
    {
      "epoch": 0.00019266688803809507,
      "grad_norm": 10324.691375532733,
      "learning_rate": 4.3423298281612404e-07,
      "loss": 1.7748,
      "step": 53088
    },
    {
      "epoch": 0.0001927830223889318,
      "grad_norm": 10064.864032862044,
      "learning_rate": 4.341020369100772e-07,
      "loss": 1.7742,
      "step": 53120
    },
    {
      "epoch": 0.0001928991567397685,
      "grad_norm": 9279.715728404615,
      "learning_rate": 4.339712093955114e-07,
      "loss": 1.783,
      "step": 53152
    },
    {
      "epoch": 0.0001930152910906052,
      "grad_norm": 9462.138447518088,
      "learning_rate": 4.33840500094133e-07,
      "loss": 1.786,
      "step": 53184
    },
    {
      "epoch": 0.0001931314254414419,
      "grad_norm": 13003.809903255276,
      "learning_rate": 4.3370990882802375e-07,
      "loss": 1.7977,
      "step": 53216
    },
    {
      "epoch": 0.00019324755979227863,
      "grad_norm": 10540.095824991346,
      "learning_rate": 4.3357943541964046e-07,
      "loss": 1.816,
      "step": 53248
    },
    {
      "epoch": 0.00019336369414311533,
      "grad_norm": 8929.001735916507,
      "learning_rate": 4.334490796918133e-07,
      "loss": 1.7795,
      "step": 53280
    },
    {
      "epoch": 0.00019347982849395203,
      "grad_norm": 9077.451845093974,
      "learning_rate": 4.333188414677453e-07,
      "loss": 1.7603,
      "step": 53312
    },
    {
      "epoch": 0.00019359596284478873,
      "grad_norm": 9265.941290554349,
      "learning_rate": 4.331887205710109e-07,
      "loss": 1.7655,
      "step": 53344
    },
    {
      "epoch": 0.00019371209719562543,
      "grad_norm": 10710.741057461897,
      "learning_rate": 4.330587168255553e-07,
      "loss": 1.7579,
      "step": 53376
    },
    {
      "epoch": 0.00019382823154646215,
      "grad_norm": 10638.845144093413,
      "learning_rate": 4.3292883005569325e-07,
      "loss": 1.7778,
      "step": 53408
    },
    {
      "epoch": 0.00019394436589729885,
      "grad_norm": 10944.07127169775,
      "learning_rate": 4.3279906008610825e-07,
      "loss": 1.8032,
      "step": 53440
    },
    {
      "epoch": 0.00019406050024813555,
      "grad_norm": 11281.410638745494,
      "learning_rate": 4.3266940674185135e-07,
      "loss": 1.7886,
      "step": 53472
    },
    {
      "epoch": 0.00019417663459897225,
      "grad_norm": 8056.8846336533825,
      "learning_rate": 4.325398698483402e-07,
      "loss": 1.8161,
      "step": 53504
    },
    {
      "epoch": 0.00019429276894980898,
      "grad_norm": 10319.232335789324,
      "learning_rate": 4.3241044923135824e-07,
      "loss": 1.8345,
      "step": 53536
    },
    {
      "epoch": 0.00019440890330064568,
      "grad_norm": 9423.048551291668,
      "learning_rate": 4.3228114471705354e-07,
      "loss": 1.8266,
      "step": 53568
    },
    {
      "epoch": 0.00019452503765148238,
      "grad_norm": 10035.90075678312,
      "learning_rate": 4.3215195613193773e-07,
      "loss": 1.7781,
      "step": 53600
    },
    {
      "epoch": 0.00019464117200231908,
      "grad_norm": 9973.256439097511,
      "learning_rate": 4.320228833028854e-07,
      "loss": 1.7859,
      "step": 53632
    },
    {
      "epoch": 0.00019475730635315578,
      "grad_norm": 10231.740321177038,
      "learning_rate": 4.3189392605713257e-07,
      "loss": 1.7939,
      "step": 53664
    },
    {
      "epoch": 0.0001948734407039925,
      "grad_norm": 11805.64432803225,
      "learning_rate": 4.317650842222763e-07,
      "loss": 1.7863,
      "step": 53696
    },
    {
      "epoch": 0.0001949895750548292,
      "grad_norm": 9790.67862816465,
      "learning_rate": 4.3163635762627333e-07,
      "loss": 1.7592,
      "step": 53728
    },
    {
      "epoch": 0.0001951057094056659,
      "grad_norm": 10189.359548077593,
      "learning_rate": 4.315077460974393e-07,
      "loss": 1.7634,
      "step": 53760
    },
    {
      "epoch": 0.0001952218437565026,
      "grad_norm": 8970.494300761804,
      "learning_rate": 4.313792494644477e-07,
      "loss": 1.7584,
      "step": 53792
    },
    {
      "epoch": 0.00019533797810733933,
      "grad_norm": 9432.953196109902,
      "learning_rate": 4.312508675563289e-07,
      "loss": 1.7801,
      "step": 53824
    },
    {
      "epoch": 0.00019545411245817603,
      "grad_norm": 8431.906427374535,
      "learning_rate": 4.311226002024692e-07,
      "loss": 1.7946,
      "step": 53856
    },
    {
      "epoch": 0.00019557024680901273,
      "grad_norm": 9530.420977060772,
      "learning_rate": 4.309944472326101e-07,
      "loss": 1.7722,
      "step": 53888
    },
    {
      "epoch": 0.00019568638115984943,
      "grad_norm": 9877.610338538365,
      "learning_rate": 4.3087040796082297e-07,
      "loss": 1.78,
      "step": 53920
    },
    {
      "epoch": 0.00019580251551068613,
      "grad_norm": 11473.484910871675,
      "learning_rate": 4.307424796882752e-07,
      "loss": 1.7825,
      "step": 53952
    },
    {
      "epoch": 0.00019591864986152286,
      "grad_norm": 11807.370155966146,
      "learning_rate": 4.306146652963548e-07,
      "loss": 1.7935,
      "step": 53984
    },
    {
      "epoch": 0.00019603478421235956,
      "grad_norm": 10081.823644559548,
      "learning_rate": 4.304869646162027e-07,
      "loss": 1.7872,
      "step": 54016
    },
    {
      "epoch": 0.00019615091856319626,
      "grad_norm": 9700.108968460097,
      "learning_rate": 4.303593774793103e-07,
      "loss": 1.7771,
      "step": 54048
    },
    {
      "epoch": 0.00019626705291403296,
      "grad_norm": 10116.073744294275,
      "learning_rate": 4.302319037175182e-07,
      "loss": 1.7824,
      "step": 54080
    },
    {
      "epoch": 0.0001963831872648697,
      "grad_norm": 9315.456295855829,
      "learning_rate": 4.301045431630156e-07,
      "loss": 1.8008,
      "step": 54112
    },
    {
      "epoch": 0.0001964993216157064,
      "grad_norm": 9108.241872062907,
      "learning_rate": 4.299772956483391e-07,
      "loss": 1.809,
      "step": 54144
    },
    {
      "epoch": 0.0001966154559665431,
      "grad_norm": 11911.620376758152,
      "learning_rate": 4.298501610063722e-07,
      "loss": 1.7977,
      "step": 54176
    },
    {
      "epoch": 0.0001967315903173798,
      "grad_norm": 9071.890872359521,
      "learning_rate": 4.297231390703436e-07,
      "loss": 1.7824,
      "step": 54208
    },
    {
      "epoch": 0.0001968477246682165,
      "grad_norm": 11956.59834568344,
      "learning_rate": 4.295962296738272e-07,
      "loss": 1.7862,
      "step": 54240
    },
    {
      "epoch": 0.00019696385901905321,
      "grad_norm": 9187.48474828666,
      "learning_rate": 4.2946943265074063e-07,
      "loss": 1.7879,
      "step": 54272
    },
    {
      "epoch": 0.00019707999336988991,
      "grad_norm": 9370.513966693608,
      "learning_rate": 4.293427478353443e-07,
      "loss": 1.7951,
      "step": 54304
    },
    {
      "epoch": 0.00019719612772072661,
      "grad_norm": 10541.967368570251,
      "learning_rate": 4.292161750622408e-07,
      "loss": 1.7868,
      "step": 54336
    },
    {
      "epoch": 0.00019731226207156331,
      "grad_norm": 9649.517086362404,
      "learning_rate": 4.290897141663739e-07,
      "loss": 1.7898,
      "step": 54368
    },
    {
      "epoch": 0.00019742839642240004,
      "grad_norm": 9410.02125396112,
      "learning_rate": 4.289633649830273e-07,
      "loss": 1.7952,
      "step": 54400
    },
    {
      "epoch": 0.00019754453077323674,
      "grad_norm": 9753.871949128716,
      "learning_rate": 4.2883712734782437e-07,
      "loss": 1.8051,
      "step": 54432
    },
    {
      "epoch": 0.00019766066512407344,
      "grad_norm": 10898.959491621206,
      "learning_rate": 4.2871100109672657e-07,
      "loss": 1.7978,
      "step": 54464
    },
    {
      "epoch": 0.00019777679947491014,
      "grad_norm": 8403.87303569015,
      "learning_rate": 4.2858498606603323e-07,
      "loss": 1.7679,
      "step": 54496
    },
    {
      "epoch": 0.00019789293382574684,
      "grad_norm": 9139.647367376927,
      "learning_rate": 4.2845908209237997e-07,
      "loss": 1.7563,
      "step": 54528
    },
    {
      "epoch": 0.00019800906817658357,
      "grad_norm": 9227.917858325354,
      "learning_rate": 4.2833328901273836e-07,
      "loss": 1.7611,
      "step": 54560
    },
    {
      "epoch": 0.00019812520252742027,
      "grad_norm": 10207.812400313791,
      "learning_rate": 4.2820760666441475e-07,
      "loss": 1.7713,
      "step": 54592
    },
    {
      "epoch": 0.00019824133687825697,
      "grad_norm": 9483.50757894989,
      "learning_rate": 4.2808203488504956e-07,
      "loss": 1.772,
      "step": 54624
    },
    {
      "epoch": 0.00019835747122909367,
      "grad_norm": 11349.275571594868,
      "learning_rate": 4.27956573512616e-07,
      "loss": 1.7753,
      "step": 54656
    },
    {
      "epoch": 0.0001984736055799304,
      "grad_norm": 9919.367217721097,
      "learning_rate": 4.278312223854199e-07,
      "loss": 1.8038,
      "step": 54688
    },
    {
      "epoch": 0.0001985897399307671,
      "grad_norm": 10506.167331620032,
      "learning_rate": 4.277059813420981e-07,
      "loss": 1.8053,
      "step": 54720
    },
    {
      "epoch": 0.0001987058742816038,
      "grad_norm": 9975.169973489174,
      "learning_rate": 4.2758085022161816e-07,
      "loss": 1.8105,
      "step": 54752
    },
    {
      "epoch": 0.0001988220086324405,
      "grad_norm": 10199.56253963865,
      "learning_rate": 4.2745582886327697e-07,
      "loss": 1.7975,
      "step": 54784
    },
    {
      "epoch": 0.0001989381429832772,
      "grad_norm": 11608.841113565126,
      "learning_rate": 4.273309171067003e-07,
      "loss": 1.7635,
      "step": 54816
    },
    {
      "epoch": 0.00019905427733411392,
      "grad_norm": 8826.012463168177,
      "learning_rate": 4.2720611479184177e-07,
      "loss": 1.7701,
      "step": 54848
    },
    {
      "epoch": 0.00019917041168495062,
      "grad_norm": 9075.210410783873,
      "learning_rate": 4.270814217589821e-07,
      "loss": 1.7729,
      "step": 54880
    },
    {
      "epoch": 0.00019928654603578732,
      "grad_norm": 10898.823606243015,
      "learning_rate": 4.26960729445746e-07,
      "loss": 1.79,
      "step": 54912
    },
    {
      "epoch": 0.00019940268038662402,
      "grad_norm": 8162.447549601774,
      "learning_rate": 4.268362510963233e-07,
      "loss": 1.7897,
      "step": 54944
    },
    {
      "epoch": 0.00019951881473746075,
      "grad_norm": 11021.790054251624,
      "learning_rate": 4.2671188155665045e-07,
      "loss": 1.8085,
      "step": 54976
    },
    {
      "epoch": 0.00019963494908829745,
      "grad_norm": 11369.735089262194,
      "learning_rate": 4.2658762066829746e-07,
      "loss": 1.7964,
      "step": 55008
    },
    {
      "epoch": 0.00019975108343913415,
      "grad_norm": 8627.047698952405,
      "learning_rate": 4.264634682731569e-07,
      "loss": 1.794,
      "step": 55040
    },
    {
      "epoch": 0.00019986721778997085,
      "grad_norm": 9084.457276029207,
      "learning_rate": 4.263394242134436e-07,
      "loss": 1.8115,
      "step": 55072
    },
    {
      "epoch": 0.00019998335214080755,
      "grad_norm": 10967.751820678657,
      "learning_rate": 4.262154883316931e-07,
      "loss": 1.7927,
      "step": 55104
    },
    {
      "epoch": 0.00020009948649164427,
      "grad_norm": 9387.522356830903,
      "learning_rate": 4.2609166047076134e-07,
      "loss": 1.7866,
      "step": 55136
    },
    {
      "epoch": 0.00020021562084248097,
      "grad_norm": 10834.250597064847,
      "learning_rate": 4.2596794047382387e-07,
      "loss": 1.7803,
      "step": 55168
    },
    {
      "epoch": 0.00020033175519331767,
      "grad_norm": 9002.312036360438,
      "learning_rate": 4.2584432818437433e-07,
      "loss": 1.7965,
      "step": 55200
    },
    {
      "epoch": 0.00020044788954415437,
      "grad_norm": 9700.395043502094,
      "learning_rate": 4.257208234462245e-07,
      "loss": 1.7861,
      "step": 55232
    },
    {
      "epoch": 0.00020056402389499107,
      "grad_norm": 9538.639944981674,
      "learning_rate": 4.25597426103503e-07,
      "loss": 1.786,
      "step": 55264
    },
    {
      "epoch": 0.0002006801582458278,
      "grad_norm": 10036.086388627791,
      "learning_rate": 4.254741360006543e-07,
      "loss": 1.7834,
      "step": 55296
    },
    {
      "epoch": 0.0002007962925966645,
      "grad_norm": 9091.894742021599,
      "learning_rate": 4.2535095298243836e-07,
      "loss": 1.7753,
      "step": 55328
    },
    {
      "epoch": 0.0002009124269475012,
      "grad_norm": 9400.52743201146,
      "learning_rate": 4.2522787689392943e-07,
      "loss": 1.7765,
      "step": 55360
    },
    {
      "epoch": 0.0002010285612983379,
      "grad_norm": 8514.644913324337,
      "learning_rate": 4.251049075805155e-07,
      "loss": 1.7778,
      "step": 55392
    },
    {
      "epoch": 0.00020114469564917463,
      "grad_norm": 10223.453721712638,
      "learning_rate": 4.2498204488789725e-07,
      "loss": 1.7612,
      "step": 55424
    },
    {
      "epoch": 0.00020126083000001133,
      "grad_norm": 8876.983496661465,
      "learning_rate": 4.248592886620874e-07,
      "loss": 1.765,
      "step": 55456
    },
    {
      "epoch": 0.00020137696435084803,
      "grad_norm": 9015.574413202965,
      "learning_rate": 4.247366387494098e-07,
      "loss": 1.7854,
      "step": 55488
    },
    {
      "epoch": 0.00020149309870168473,
      "grad_norm": 10208.051430121224,
      "learning_rate": 4.2461409499649876e-07,
      "loss": 1.795,
      "step": 55520
    },
    {
      "epoch": 0.00020160923305252143,
      "grad_norm": 9799.095876661275,
      "learning_rate": 4.2449165725029806e-07,
      "loss": 1.7741,
      "step": 55552
    },
    {
      "epoch": 0.00020172536740335815,
      "grad_norm": 10015.919129066488,
      "learning_rate": 4.2436932535806033e-07,
      "loss": 1.7713,
      "step": 55584
    },
    {
      "epoch": 0.00020184150175419485,
      "grad_norm": 9302.087292645667,
      "learning_rate": 4.2424709916734593e-07,
      "loss": 1.7685,
      "step": 55616
    },
    {
      "epoch": 0.00020195763610503155,
      "grad_norm": 9501.303910516703,
      "learning_rate": 4.2412497852602273e-07,
      "loss": 1.767,
      "step": 55648
    },
    {
      "epoch": 0.00020207377045586825,
      "grad_norm": 9459.505906758555,
      "learning_rate": 4.240029632822647e-07,
      "loss": 1.8032,
      "step": 55680
    },
    {
      "epoch": 0.00020218990480670498,
      "grad_norm": 9369.447262245516,
      "learning_rate": 4.238810532845515e-07,
      "loss": 1.804,
      "step": 55712
    },
    {
      "epoch": 0.00020230603915754168,
      "grad_norm": 9674.9605683951,
      "learning_rate": 4.237592483816676e-07,
      "loss": 1.7939,
      "step": 55744
    },
    {
      "epoch": 0.00020242217350837838,
      "grad_norm": 9363.207676859463,
      "learning_rate": 4.236375484227013e-07,
      "loss": 1.7983,
      "step": 55776
    },
    {
      "epoch": 0.00020253830785921508,
      "grad_norm": 10117.767342650255,
      "learning_rate": 4.235159532570442e-07,
      "loss": 1.8086,
      "step": 55808
    },
    {
      "epoch": 0.00020265444221005178,
      "grad_norm": 10605.52676673818,
      "learning_rate": 4.2339446273439045e-07,
      "loss": 1.8154,
      "step": 55840
    },
    {
      "epoch": 0.0002027705765608885,
      "grad_norm": 10322.5869819537,
      "learning_rate": 4.232730767047357e-07,
      "loss": 1.8069,
      "step": 55872
    },
    {
      "epoch": 0.0002028867109117252,
      "grad_norm": 10064.503564508286,
      "learning_rate": 4.2315558349314486e-07,
      "loss": 1.8087,
      "step": 55904
    },
    {
      "epoch": 0.0002030028452625619,
      "grad_norm": 8614.68200225638,
      "learning_rate": 4.2303440274687715e-07,
      "loss": 1.7903,
      "step": 55936
    },
    {
      "epoch": 0.0002031189796133986,
      "grad_norm": 9337.224212794721,
      "learning_rate": 4.229133260500526e-07,
      "loss": 1.7857,
      "step": 55968
    },
    {
      "epoch": 0.00020323511396423533,
      "grad_norm": 10418.659414723183,
      "learning_rate": 4.227923532538564e-07,
      "loss": 1.787,
      "step": 56000
    },
    {
      "epoch": 0.00020335124831507203,
      "grad_norm": 9568.271944295899,
      "learning_rate": 4.226714842097715e-07,
      "loss": 1.7579,
      "step": 56032
    },
    {
      "epoch": 0.00020346738266590873,
      "grad_norm": 9913.880471339162,
      "learning_rate": 4.2255071876957797e-07,
      "loss": 1.7649,
      "step": 56064
    },
    {
      "epoch": 0.00020358351701674543,
      "grad_norm": 10764.250833197822,
      "learning_rate": 4.224300567853522e-07,
      "loss": 1.7775,
      "step": 56096
    },
    {
      "epoch": 0.00020369965136758213,
      "grad_norm": 10221.11324660871,
      "learning_rate": 4.2230949810946594e-07,
      "loss": 1.7813,
      "step": 56128
    },
    {
      "epoch": 0.00020381578571841886,
      "grad_norm": 10000.340794192965,
      "learning_rate": 4.221890425945858e-07,
      "loss": 1.7584,
      "step": 56160
    },
    {
      "epoch": 0.00020393192006925556,
      "grad_norm": 10829.153521859407,
      "learning_rate": 4.220686900936724e-07,
      "loss": 1.7592,
      "step": 56192
    },
    {
      "epoch": 0.00020404805442009226,
      "grad_norm": 9651.256912962166,
      "learning_rate": 4.219484404599795e-07,
      "loss": 1.7727,
      "step": 56224
    },
    {
      "epoch": 0.00020416418877092896,
      "grad_norm": 10185.926761959365,
      "learning_rate": 4.2182829354705354e-07,
      "loss": 1.7826,
      "step": 56256
    },
    {
      "epoch": 0.0002042803231217657,
      "grad_norm": 8187.795307651505,
      "learning_rate": 4.2170824920873257e-07,
      "loss": 1.7962,
      "step": 56288
    },
    {
      "epoch": 0.0002043964574726024,
      "grad_norm": 8630.902154467978,
      "learning_rate": 4.2158830729914557e-07,
      "loss": 1.7663,
      "step": 56320
    },
    {
      "epoch": 0.0002045125918234391,
      "grad_norm": 8514.573858978498,
      "learning_rate": 4.21468467672712e-07,
      "loss": 1.7584,
      "step": 56352
    },
    {
      "epoch": 0.0002046287261742758,
      "grad_norm": 8658.057172368406,
      "learning_rate": 4.2134873018414067e-07,
      "loss": 1.7726,
      "step": 56384
    },
    {
      "epoch": 0.0002047448605251125,
      "grad_norm": 9782.135656389151,
      "learning_rate": 4.212290946884291e-07,
      "loss": 1.7807,
      "step": 56416
    },
    {
      "epoch": 0.00020486099487594921,
      "grad_norm": 9275.58645046231,
      "learning_rate": 4.2110956104086313e-07,
      "loss": 1.805,
      "step": 56448
    },
    {
      "epoch": 0.00020497712922678591,
      "grad_norm": 9782.703614032269,
      "learning_rate": 4.2099012909701545e-07,
      "loss": 1.8172,
      "step": 56480
    },
    {
      "epoch": 0.00020509326357762261,
      "grad_norm": 10168.021243093466,
      "learning_rate": 4.2087079871274584e-07,
      "loss": 1.8092,
      "step": 56512
    },
    {
      "epoch": 0.00020520939792845931,
      "grad_norm": 9889.00733137558,
      "learning_rate": 4.207515697441995e-07,
      "loss": 1.7927,
      "step": 56544
    },
    {
      "epoch": 0.00020532553227929604,
      "grad_norm": 8956.844477828115,
      "learning_rate": 4.206324420478069e-07,
      "loss": 1.8013,
      "step": 56576
    },
    {
      "epoch": 0.00020544166663013274,
      "grad_norm": 9875.56266751419,
      "learning_rate": 4.205134154802829e-07,
      "loss": 1.8115,
      "step": 56608
    },
    {
      "epoch": 0.00020555780098096944,
      "grad_norm": 10965.064705691437,
      "learning_rate": 4.2039448989862604e-07,
      "loss": 1.8024,
      "step": 56640
    },
    {
      "epoch": 0.00020567393533180614,
      "grad_norm": 10425.906195626354,
      "learning_rate": 4.202756651601177e-07,
      "loss": 1.8094,
      "step": 56672
    },
    {
      "epoch": 0.00020579006968264284,
      "grad_norm": 9588.379320823724,
      "learning_rate": 4.201569411223216e-07,
      "loss": 1.8045,
      "step": 56704
    },
    {
      "epoch": 0.00020590620403347957,
      "grad_norm": 9203.479124765809,
      "learning_rate": 4.2003831764308285e-07,
      "loss": 1.7901,
      "step": 56736
    },
    {
      "epoch": 0.00020602233838431627,
      "grad_norm": 9387.48869506643,
      "learning_rate": 4.1991979458052733e-07,
      "loss": 1.7513,
      "step": 56768
    },
    {
      "epoch": 0.00020613847273515297,
      "grad_norm": 11782.572724154941,
      "learning_rate": 4.1980137179306116e-07,
      "loss": 1.7625,
      "step": 56800
    },
    {
      "epoch": 0.00020625460708598967,
      "grad_norm": 11238.73160103043,
      "learning_rate": 4.1968304913936964e-07,
      "loss": 1.7512,
      "step": 56832
    },
    {
      "epoch": 0.0002063707414368264,
      "grad_norm": 11766.560924926196,
      "learning_rate": 4.1956482647841677e-07,
      "loss": 1.7722,
      "step": 56864
    },
    {
      "epoch": 0.0002064868757876631,
      "grad_norm": 10670.074413986062,
      "learning_rate": 4.1944670366944467e-07,
      "loss": 1.7754,
      "step": 56896
    },
    {
      "epoch": 0.0002066030101384998,
      "grad_norm": 10832.160541646343,
      "learning_rate": 4.193323672858569e-07,
      "loss": 1.7774,
      "step": 56928
    },
    {
      "epoch": 0.0002067191444893365,
      "grad_norm": 10169.25975673746,
      "learning_rate": 4.1921444065019675e-07,
      "loss": 1.7779,
      "step": 56960
    },
    {
      "epoch": 0.0002068352788401732,
      "grad_norm": 9463.377726795015,
      "learning_rate": 4.1909661345027243e-07,
      "loss": 1.8007,
      "step": 56992
    },
    {
      "epoch": 0.00020695141319100992,
      "grad_norm": 9853.658102451089,
      "learning_rate": 4.1897888554642216e-07,
      "loss": 1.7919,
      "step": 57024
    },
    {
      "epoch": 0.00020706754754184662,
      "grad_norm": 9333.261594962396,
      "learning_rate": 4.188612567992584e-07,
      "loss": 1.7919,
      "step": 57056
    },
    {
      "epoch": 0.00020718368189268332,
      "grad_norm": 10510.519777822597,
      "learning_rate": 4.187437270696678e-07,
      "loss": 1.771,
      "step": 57088
    },
    {
      "epoch": 0.00020729981624352002,
      "grad_norm": 9533.128762373872,
      "learning_rate": 4.186262962188096e-07,
      "loss": 1.7598,
      "step": 57120
    },
    {
      "epoch": 0.00020741595059435675,
      "grad_norm": 10255.6682863673,
      "learning_rate": 4.185089641081159e-07,
      "loss": 1.7514,
      "step": 57152
    },
    {
      "epoch": 0.00020753208494519345,
      "grad_norm": 9534.018355342096,
      "learning_rate": 4.183917305992902e-07,
      "loss": 1.768,
      "step": 57184
    },
    {
      "epoch": 0.00020764821929603015,
      "grad_norm": 10620.851378302967,
      "learning_rate": 4.182745955543071e-07,
      "loss": 1.7979,
      "step": 57216
    },
    {
      "epoch": 0.00020776435364686685,
      "grad_norm": 9423.482052829517,
      "learning_rate": 4.1815755883541167e-07,
      "loss": 1.8096,
      "step": 57248
    },
    {
      "epoch": 0.00020788048799770355,
      "grad_norm": 10096.999158165756,
      "learning_rate": 4.180406203051185e-07,
      "loss": 1.7954,
      "step": 57280
    },
    {
      "epoch": 0.00020799662234854028,
      "grad_norm": 7908.521480023936,
      "learning_rate": 4.179237798262112e-07,
      "loss": 1.7894,
      "step": 57312
    },
    {
      "epoch": 0.00020811275669937698,
      "grad_norm": 9954.118946446239,
      "learning_rate": 4.178070372617418e-07,
      "loss": 1.7993,
      "step": 57344
    },
    {
      "epoch": 0.00020822889105021367,
      "grad_norm": 9595.5496976463,
      "learning_rate": 4.1769039247502976e-07,
      "loss": 1.7906,
      "step": 57376
    },
    {
      "epoch": 0.00020834502540105037,
      "grad_norm": 9334.434744535954,
      "learning_rate": 4.1757384532966165e-07,
      "loss": 1.7858,
      "step": 57408
    },
    {
      "epoch": 0.0002084611597518871,
      "grad_norm": 9040.889558002575,
      "learning_rate": 4.174573956894903e-07,
      "loss": 1.8008,
      "step": 57440
    },
    {
      "epoch": 0.0002085772941027238,
      "grad_norm": 9416.563598256002,
      "learning_rate": 4.1734104341863415e-07,
      "loss": 1.7907,
      "step": 57472
    },
    {
      "epoch": 0.0002086934284535605,
      "grad_norm": 8991.255863337446,
      "learning_rate": 4.1722478838147667e-07,
      "loss": 1.8037,
      "step": 57504
    },
    {
      "epoch": 0.0002088095628043972,
      "grad_norm": 9666.472469313716,
      "learning_rate": 4.171086304426655e-07,
      "loss": 1.7897,
      "step": 57536
    },
    {
      "epoch": 0.0002089256971552339,
      "grad_norm": 9165.441615110534,
      "learning_rate": 4.16992569467112e-07,
      "loss": 1.7893,
      "step": 57568
    },
    {
      "epoch": 0.00020904183150607063,
      "grad_norm": 9065.563744191533,
      "learning_rate": 4.168766053199905e-07,
      "loss": 1.768,
      "step": 57600
    },
    {
      "epoch": 0.00020915796585690733,
      "grad_norm": 10364.93058346268,
      "learning_rate": 4.167607378667376e-07,
      "loss": 1.7879,
      "step": 57632
    },
    {
      "epoch": 0.00020927410020774403,
      "grad_norm": 10467.854316907547,
      "learning_rate": 4.166449669730516e-07,
      "loss": 1.7835,
      "step": 57664
    },
    {
      "epoch": 0.00020939023455858073,
      "grad_norm": 8776.263213919692,
      "learning_rate": 4.165292925048919e-07,
      "loss": 1.7574,
      "step": 57696
    },
    {
      "epoch": 0.00020950636890941746,
      "grad_norm": 9818.179260942428,
      "learning_rate": 4.16413714328478e-07,
      "loss": 1.7723,
      "step": 57728
    },
    {
      "epoch": 0.00020962250326025416,
      "grad_norm": 10788.721332947664,
      "learning_rate": 4.162982323102893e-07,
      "loss": 1.7677,
      "step": 57760
    },
    {
      "epoch": 0.00020973863761109086,
      "grad_norm": 9663.332551454492,
      "learning_rate": 4.1618284631706424e-07,
      "loss": 1.7749,
      "step": 57792
    },
    {
      "epoch": 0.00020985477196192756,
      "grad_norm": 9624.777503921843,
      "learning_rate": 4.1606755621579966e-07,
      "loss": 1.8018,
      "step": 57824
    },
    {
      "epoch": 0.00020997090631276425,
      "grad_norm": 10778.87971915449,
      "learning_rate": 4.159523618737501e-07,
      "loss": 1.7971,
      "step": 57856
    },
    {
      "epoch": 0.00021008704066360098,
      "grad_norm": 8333.552183792935,
      "learning_rate": 4.158372631584273e-07,
      "loss": 1.7523,
      "step": 57888
    },
    {
      "epoch": 0.00021020317501443768,
      "grad_norm": 9204.551808752016,
      "learning_rate": 4.157258523440887e-07,
      "loss": 1.7533,
      "step": 57920
    },
    {
      "epoch": 0.00021031930936527438,
      "grad_norm": 9419.998726114563,
      "learning_rate": 4.1561094150769633e-07,
      "loss": 1.7694,
      "step": 57952
    },
    {
      "epoch": 0.00021043544371611108,
      "grad_norm": 11524.507798600336,
      "learning_rate": 4.154961259062146e-07,
      "loss": 1.7762,
      "step": 57984
    },
    {
      "epoch": 0.00021055157806694778,
      "grad_norm": 9254.50376843621,
      "learning_rate": 4.1538140540816947e-07,
      "loss": 1.7829,
      "step": 58016
    },
    {
      "epoch": 0.0002106677124177845,
      "grad_norm": 9976.710981079887,
      "learning_rate": 4.1526677988234083e-07,
      "loss": 1.7848,
      "step": 58048
    },
    {
      "epoch": 0.0002107838467686212,
      "grad_norm": 9656.372196637823,
      "learning_rate": 4.1515224919776177e-07,
      "loss": 1.788,
      "step": 58080
    },
    {
      "epoch": 0.0002108999811194579,
      "grad_norm": 8512.477782643547,
      "learning_rate": 4.1503781322371823e-07,
      "loss": 1.8013,
      "step": 58112
    },
    {
      "epoch": 0.0002110161154702946,
      "grad_norm": 9556.891335575601,
      "learning_rate": 4.1492347182974813e-07,
      "loss": 1.8201,
      "step": 58144
    },
    {
      "epoch": 0.00021113224982113134,
      "grad_norm": 9834.426673680577,
      "learning_rate": 4.14809224885641e-07,
      "loss": 1.8052,
      "step": 58176
    },
    {
      "epoch": 0.00021124838417196804,
      "grad_norm": 10509.25268513418,
      "learning_rate": 4.1469507226143703e-07,
      "loss": 1.8103,
      "step": 58208
    },
    {
      "epoch": 0.00021136451852280474,
      "grad_norm": 10035.919489513655,
      "learning_rate": 4.145810138274267e-07,
      "loss": 1.8116,
      "step": 58240
    },
    {
      "epoch": 0.00021148065287364144,
      "grad_norm": 10100.117326051219,
      "learning_rate": 4.1446704945415e-07,
      "loss": 1.801,
      "step": 58272
    },
    {
      "epoch": 0.00021159678722447814,
      "grad_norm": 10061.351400284158,
      "learning_rate": 4.1435317901239616e-07,
      "loss": 1.763,
      "step": 58304
    },
    {
      "epoch": 0.00021171292157531486,
      "grad_norm": 9360.28332904512,
      "learning_rate": 4.1423940237320233e-07,
      "loss": 1.7677,
      "step": 58336
    },
    {
      "epoch": 0.00021182905592615156,
      "grad_norm": 8533.91047527451,
      "learning_rate": 4.1412571940785383e-07,
      "loss": 1.7695,
      "step": 58368
    },
    {
      "epoch": 0.00021194519027698826,
      "grad_norm": 10120.707386344098,
      "learning_rate": 4.1401212998788286e-07,
      "loss": 1.7838,
      "step": 58400
    },
    {
      "epoch": 0.00021206132462782496,
      "grad_norm": 10249.449448628935,
      "learning_rate": 4.138986339850682e-07,
      "loss": 1.8049,
      "step": 58432
    },
    {
      "epoch": 0.0002121774589786617,
      "grad_norm": 10416.306543108261,
      "learning_rate": 4.137852312714346e-07,
      "loss": 1.7724,
      "step": 58464
    },
    {
      "epoch": 0.0002122935933294984,
      "grad_norm": 8124.00160019679,
      "learning_rate": 4.1367192171925205e-07,
      "loss": 1.7633,
      "step": 58496
    },
    {
      "epoch": 0.0002124097276803351,
      "grad_norm": 9658.495535019934,
      "learning_rate": 4.1355870520103535e-07,
      "loss": 1.7632,
      "step": 58528
    },
    {
      "epoch": 0.0002125258620311718,
      "grad_norm": 9559.419752265301,
      "learning_rate": 4.1344558158954324e-07,
      "loss": 1.7779,
      "step": 58560
    },
    {
      "epoch": 0.0002126419963820085,
      "grad_norm": 9465.870905521584,
      "learning_rate": 4.1333255075777806e-07,
      "loss": 1.7662,
      "step": 58592
    },
    {
      "epoch": 0.00021275813073284522,
      "grad_norm": 8927.376098272101,
      "learning_rate": 4.132196125789852e-07,
      "loss": 1.7521,
      "step": 58624
    },
    {
      "epoch": 0.00021287426508368192,
      "grad_norm": 9817.579844340456,
      "learning_rate": 4.1310676692665197e-07,
      "loss": 1.765,
      "step": 58656
    },
    {
      "epoch": 0.00021299039943451862,
      "grad_norm": 10688.626104415853,
      "learning_rate": 4.1299401367450784e-07,
      "loss": 1.763,
      "step": 58688
    },
    {
      "epoch": 0.00021310653378535532,
      "grad_norm": 9716.226222150244,
      "learning_rate": 4.128813526965231e-07,
      "loss": 1.7996,
      "step": 58720
    },
    {
      "epoch": 0.00021322266813619204,
      "grad_norm": 10378.562135479075,
      "learning_rate": 4.127687838669086e-07,
      "loss": 1.8044,
      "step": 58752
    },
    {
      "epoch": 0.00021333880248702874,
      "grad_norm": 8820.612450391412,
      "learning_rate": 4.126563070601153e-07,
      "loss": 1.8113,
      "step": 58784
    },
    {
      "epoch": 0.00021345493683786544,
      "grad_norm": 10317.290826568766,
      "learning_rate": 4.125439221508333e-07,
      "loss": 1.8017,
      "step": 58816
    },
    {
      "epoch": 0.00021357107118870214,
      "grad_norm": 9364.99813133991,
      "learning_rate": 4.124316290139916e-07,
      "loss": 1.7998,
      "step": 58848
    },
    {
      "epoch": 0.00021368720553953884,
      "grad_norm": 9218.523417554463,
      "learning_rate": 4.123194275247572e-07,
      "loss": 1.7905,
      "step": 58880
    },
    {
      "epoch": 0.00021380333989037557,
      "grad_norm": 9630.215054711914,
      "learning_rate": 4.122073175585351e-07,
      "loss": 1.7799,
      "step": 58912
    },
    {
      "epoch": 0.00021391947424121227,
      "grad_norm": 8633.92471590991,
      "learning_rate": 4.120987981889611e-07,
      "loss": 1.7843,
      "step": 58944
    },
    {
      "epoch": 0.00021403560859204897,
      "grad_norm": 10138.615881864744,
      "learning_rate": 4.119868680454722e-07,
      "loss": 1.7891,
      "step": 58976
    },
    {
      "epoch": 0.00021415174294288567,
      "grad_norm": 8421.689497957046,
      "learning_rate": 4.1187502905649723e-07,
      "loss": 1.8008,
      "step": 59008
    },
    {
      "epoch": 0.0002142678772937224,
      "grad_norm": 9488.060497277616,
      "learning_rate": 4.117632810983782e-07,
      "loss": 1.7972,
      "step": 59040
    },
    {
      "epoch": 0.0002143840116445591,
      "grad_norm": 8933.260435025948,
      "learning_rate": 4.1165162404769174e-07,
      "loss": 1.7846,
      "step": 59072
    },
    {
      "epoch": 0.0002145001459953958,
      "grad_norm": 9723.82877265946,
      "learning_rate": 4.115400577812488e-07,
      "loss": 1.7609,
      "step": 59104
    },
    {
      "epoch": 0.0002146162803462325,
      "grad_norm": 9014.92251769254,
      "learning_rate": 4.114285821760937e-07,
      "loss": 1.768,
      "step": 59136
    },
    {
      "epoch": 0.0002147324146970692,
      "grad_norm": 10539.40415773112,
      "learning_rate": 4.11317197109504e-07,
      "loss": 1.7816,
      "step": 59168
    },
    {
      "epoch": 0.00021484854904790592,
      "grad_norm": 9178.453791352878,
      "learning_rate": 4.1120590245898947e-07,
      "loss": 1.7698,
      "step": 59200
    },
    {
      "epoch": 0.00021496468339874262,
      "grad_norm": 9326.363921700675,
      "learning_rate": 4.1109469810229194e-07,
      "loss": 1.7692,
      "step": 59232
    },
    {
      "epoch": 0.00021508081774957932,
      "grad_norm": 9524.933385593833,
      "learning_rate": 4.109835839173844e-07,
      "loss": 1.7907,
      "step": 59264
    },
    {
      "epoch": 0.00021519695210041602,
      "grad_norm": 10839.350165023732,
      "learning_rate": 4.108725597824708e-07,
      "loss": 1.7964,
      "step": 59296
    },
    {
      "epoch": 0.00021531308645125275,
      "grad_norm": 9938.20446559639,
      "learning_rate": 4.107616255759851e-07,
      "loss": 1.7886,
      "step": 59328
    },
    {
      "epoch": 0.00021542922080208945,
      "grad_norm": 9674.36974691375,
      "learning_rate": 4.106507811765909e-07,
      "loss": 1.7801,
      "step": 59360
    },
    {
      "epoch": 0.00021554535515292615,
      "grad_norm": 11475.70651419772,
      "learning_rate": 4.105400264631811e-07,
      "loss": 1.7856,
      "step": 59392
    },
    {
      "epoch": 0.00021566148950376285,
      "grad_norm": 9673.96020252306,
      "learning_rate": 4.104293613148768e-07,
      "loss": 1.7742,
      "step": 59424
    },
    {
      "epoch": 0.00021577762385459955,
      "grad_norm": 10368.027584839847,
      "learning_rate": 4.103187856110272e-07,
      "loss": 1.7651,
      "step": 59456
    },
    {
      "epoch": 0.00021589375820543628,
      "grad_norm": 9049.031881919746,
      "learning_rate": 4.1020829923120907e-07,
      "loss": 1.7782,
      "step": 59488
    },
    {
      "epoch": 0.00021600989255627298,
      "grad_norm": 10025.143190997323,
      "learning_rate": 4.100979020552258e-07,
      "loss": 1.7911,
      "step": 59520
    },
    {
      "epoch": 0.00021612602690710968,
      "grad_norm": 9043.1185992444,
      "learning_rate": 4.099875939631072e-07,
      "loss": 1.7913,
      "step": 59552
    },
    {
      "epoch": 0.00021624216125794638,
      "grad_norm": 10430.360492331989,
      "learning_rate": 4.0987737483510886e-07,
      "loss": 1.7996,
      "step": 59584
    },
    {
      "epoch": 0.0002163582956087831,
      "grad_norm": 8857.870398690648,
      "learning_rate": 4.097672445517116e-07,
      "loss": 1.7884,
      "step": 59616
    },
    {
      "epoch": 0.0002164744299596198,
      "grad_norm": 10020.25209263719,
      "learning_rate": 4.096572029936208e-07,
      "loss": 1.7905,
      "step": 59648
    },
    {
      "epoch": 0.0002165905643104565,
      "grad_norm": 8976.241306916832,
      "learning_rate": 4.095472500417661e-07,
      "loss": 1.7991,
      "step": 59680
    },
    {
      "epoch": 0.0002167066986612932,
      "grad_norm": 10150.181279169354,
      "learning_rate": 4.094373855773008e-07,
      "loss": 1.7825,
      "step": 59712
    },
    {
      "epoch": 0.0002168228330121299,
      "grad_norm": 8031.345341846532,
      "learning_rate": 4.0932760948160095e-07,
      "loss": 1.7775,
      "step": 59744
    },
    {
      "epoch": 0.00021693896736296663,
      "grad_norm": 8456.756825166489,
      "learning_rate": 4.092179216362654e-07,
      "loss": 1.7892,
      "step": 59776
    },
    {
      "epoch": 0.00021705510171380333,
      "grad_norm": 10035.245986023461,
      "learning_rate": 4.091083219231149e-07,
      "loss": 1.753,
      "step": 59808
    },
    {
      "epoch": 0.00021717123606464003,
      "grad_norm": 10899.0560141693,
      "learning_rate": 4.089988102241916e-07,
      "loss": 1.7673,
      "step": 59840
    },
    {
      "epoch": 0.00021728737041547673,
      "grad_norm": 10136.808768049243,
      "learning_rate": 4.088893864217586e-07,
      "loss": 1.7647,
      "step": 59872
    },
    {
      "epoch": 0.00021740350476631346,
      "grad_norm": 10146.152374176134,
      "learning_rate": 4.087800503982993e-07,
      "loss": 1.7778,
      "step": 59904
    },
    {
      "epoch": 0.00021751963911715016,
      "grad_norm": 8312.835256397182,
      "learning_rate": 4.086742147220762e-07,
      "loss": 1.7838,
      "step": 59936
    },
    {
      "epoch": 0.00021763577346798686,
      "grad_norm": 10902.083929231145,
      "learning_rate": 4.0856505117089494e-07,
      "loss": 1.7982,
      "step": 59968
    },
    {
      "epoch": 0.00021775190781882356,
      "grad_norm": 8901.249912231428,
      "learning_rate": 4.084559750511033e-07,
      "loss": 1.8098,
      "step": 60000
    },
    {
      "epoch": 0.00021786804216966026,
      "grad_norm": 8508.262102215704,
      "learning_rate": 4.0834698624605423e-07,
      "loss": 1.7944,
      "step": 60032
    },
    {
      "epoch": 0.00021798417652049698,
      "grad_norm": 8484.970830827882,
      "learning_rate": 4.082380846393183e-07,
      "loss": 1.7787,
      "step": 60064
    },
    {
      "epoch": 0.00021810031087133368,
      "grad_norm": 10710.223153604224,
      "learning_rate": 4.081292701146833e-07,
      "loss": 1.7895,
      "step": 60096
    },
    {
      "epoch": 0.00021821644522217038,
      "grad_norm": 10188.914564368475,
      "learning_rate": 4.08020542556154e-07,
      "loss": 1.7722,
      "step": 60128
    },
    {
      "epoch": 0.00021833257957300708,
      "grad_norm": 9626.812244974968,
      "learning_rate": 4.079119018479511e-07,
      "loss": 1.7702,
      "step": 60160
    },
    {
      "epoch": 0.0002184487139238438,
      "grad_norm": 9382.280000085268,
      "learning_rate": 4.07803347874511e-07,
      "loss": 1.7674,
      "step": 60192
    },
    {
      "epoch": 0.0002185648482746805,
      "grad_norm": 12090.624136081646,
      "learning_rate": 4.076948805204855e-07,
      "loss": 1.7656,
      "step": 60224
    },
    {
      "epoch": 0.0002186809826255172,
      "grad_norm": 10724.236103331556,
      "learning_rate": 4.075864996707407e-07,
      "loss": 1.7839,
      "step": 60256
    },
    {
      "epoch": 0.0002187971169763539,
      "grad_norm": 8483.079216888169,
      "learning_rate": 4.074782052103571e-07,
      "loss": 1.7955,
      "step": 60288
    },
    {
      "epoch": 0.0002189132513271906,
      "grad_norm": 7685.41788063603,
      "learning_rate": 4.073699970246286e-07,
      "loss": 1.7929,
      "step": 60320
    },
    {
      "epoch": 0.00021902938567802734,
      "grad_norm": 10760.387818289822,
      "learning_rate": 4.072618749990626e-07,
      "loss": 1.777,
      "step": 60352
    },
    {
      "epoch": 0.00021914552002886404,
      "grad_norm": 10380.206597173294,
      "learning_rate": 4.071538390193787e-07,
      "loss": 1.7886,
      "step": 60384
    },
    {
      "epoch": 0.00021926165437970074,
      "grad_norm": 10168.74476029367,
      "learning_rate": 4.0704588897150867e-07,
      "loss": 1.7996,
      "step": 60416
    },
    {
      "epoch": 0.00021937778873053744,
      "grad_norm": 10242.838864299292,
      "learning_rate": 4.069380247415961e-07,
      "loss": 1.7896,
      "step": 60448
    },
    {
      "epoch": 0.00021949392308137416,
      "grad_norm": 9700.408651185784,
      "learning_rate": 4.068302462159954e-07,
      "loss": 1.7857,
      "step": 60480
    },
    {
      "epoch": 0.00021961005743221086,
      "grad_norm": 9122.604014205594,
      "learning_rate": 4.0672255328127177e-07,
      "loss": 1.7924,
      "step": 60512
    },
    {
      "epoch": 0.00021972619178304756,
      "grad_norm": 9064.027471273463,
      "learning_rate": 4.0661494582420033e-07,
      "loss": 1.7864,
      "step": 60544
    },
    {
      "epoch": 0.00021984232613388426,
      "grad_norm": 10068.218213765533,
      "learning_rate": 4.0650742373176587e-07,
      "loss": 1.788,
      "step": 60576
    },
    {
      "epoch": 0.00021995846048472096,
      "grad_norm": 10025.68710862253,
      "learning_rate": 4.063999868911623e-07,
      "loss": 1.7876,
      "step": 60608
    },
    {
      "epoch": 0.0002200745948355577,
      "grad_norm": 8825.5915382483,
      "learning_rate": 4.062926351897919e-07,
      "loss": 1.7565,
      "step": 60640
    },
    {
      "epoch": 0.0002201907291863944,
      "grad_norm": 10000.429890759697,
      "learning_rate": 4.0618536851526543e-07,
      "loss": 1.7677,
      "step": 60672
    },
    {
      "epoch": 0.0002203068635372311,
      "grad_norm": 8471.349479274244,
      "learning_rate": 4.0607818675540085e-07,
      "loss": 1.7784,
      "step": 60704
    },
    {
      "epoch": 0.0002204229978880678,
      "grad_norm": 11235.051223737253,
      "learning_rate": 4.0597108979822337e-07,
      "loss": 1.7645,
      "step": 60736
    },
    {
      "epoch": 0.0002205391322389045,
      "grad_norm": 8485.01019445469,
      "learning_rate": 4.05864077531965e-07,
      "loss": 1.783,
      "step": 60768
    },
    {
      "epoch": 0.00022065526658974122,
      "grad_norm": 10003.233977069616,
      "learning_rate": 4.057571498450636e-07,
      "loss": 1.7922,
      "step": 60800
    },
    {
      "epoch": 0.00022077140094057792,
      "grad_norm": 9529.686248770207,
      "learning_rate": 4.056503066261628e-07,
      "loss": 1.7822,
      "step": 60832
    },
    {
      "epoch": 0.00022088753529141462,
      "grad_norm": 8586.909455677287,
      "learning_rate": 4.0554354776411125e-07,
      "loss": 1.7783,
      "step": 60864
    },
    {
      "epoch": 0.00022100366964225132,
      "grad_norm": 9561.723066477087,
      "learning_rate": 4.054368731479625e-07,
      "loss": 1.7829,
      "step": 60896
    },
    {
      "epoch": 0.00022111980399308804,
      "grad_norm": 11118.076002618438,
      "learning_rate": 4.053336123470733e-07,
      "loss": 1.7773,
      "step": 60928
    },
    {
      "epoch": 0.00022123593834392474,
      "grad_norm": 9409.856747049873,
      "learning_rate": 4.0522710326660824e-07,
      "loss": 1.7645,
      "step": 60960
    },
    {
      "epoch": 0.00022135207269476144,
      "grad_norm": 9330.464511480659,
      "learning_rate": 4.0512067810387423e-07,
      "loss": 1.787,
      "step": 60992
    },
    {
      "epoch": 0.00022146820704559814,
      "grad_norm": 12271.849575349268,
      "learning_rate": 4.0501433674873213e-07,
      "loss": 1.7983,
      "step": 61024
    },
    {
      "epoch": 0.00022158434139643484,
      "grad_norm": 7914.588555319853,
      "learning_rate": 4.049080790912451e-07,
      "loss": 1.7866,
      "step": 61056
    },
    {
      "epoch": 0.00022170047574727157,
      "grad_norm": 10753.715078985495,
      "learning_rate": 4.048019050216781e-07,
      "loss": 1.7912,
      "step": 61088
    },
    {
      "epoch": 0.00022181661009810827,
      "grad_norm": 9851.926613612182,
      "learning_rate": 4.046958144304975e-07,
      "loss": 1.786,
      "step": 61120
    },
    {
      "epoch": 0.00022193274444894497,
      "grad_norm": 9837.783998441924,
      "learning_rate": 4.0458980720837025e-07,
      "loss": 1.8002,
      "step": 61152
    },
    {
      "epoch": 0.00022204887879978167,
      "grad_norm": 8680.971950190831,
      "learning_rate": 4.0448388324616396e-07,
      "loss": 1.8114,
      "step": 61184
    },
    {
      "epoch": 0.0002221650131506184,
      "grad_norm": 9891.857257360723,
      "learning_rate": 4.0437804243494603e-07,
      "loss": 1.8085,
      "step": 61216
    },
    {
      "epoch": 0.0002222811475014551,
      "grad_norm": 9868.956581118391,
      "learning_rate": 4.0427228466598313e-07,
      "loss": 1.7817,
      "step": 61248
    },
    {
      "epoch": 0.0002223972818522918,
      "grad_norm": 9354.119627201697,
      "learning_rate": 4.041666098307412e-07,
      "loss": 1.794,
      "step": 61280
    },
    {
      "epoch": 0.0002225134162031285,
      "grad_norm": 9771.887944506936,
      "learning_rate": 4.040610178208843e-07,
      "loss": 1.7722,
      "step": 61312
    },
    {
      "epoch": 0.0002226295505539652,
      "grad_norm": 9837.26771009105,
      "learning_rate": 4.0395550852827484e-07,
      "loss": 1.7566,
      "step": 61344
    },
    {
      "epoch": 0.00022274568490480192,
      "grad_norm": 9487.112837950228,
      "learning_rate": 4.0385008184497263e-07,
      "loss": 1.7626,
      "step": 61376
    },
    {
      "epoch": 0.00022286181925563862,
      "grad_norm": 9529.268807206563,
      "learning_rate": 4.037447376632346e-07,
      "loss": 1.7608,
      "step": 61408
    },
    {
      "epoch": 0.00022297795360647532,
      "grad_norm": 9942.245621588716,
      "learning_rate": 4.036394758755142e-07,
      "loss": 1.7634,
      "step": 61440
    },
    {
      "epoch": 0.00022309408795731202,
      "grad_norm": 9357.247565390157,
      "learning_rate": 4.0353429637446123e-07,
      "loss": 1.7556,
      "step": 61472
    },
    {
      "epoch": 0.00022321022230814875,
      "grad_norm": 8678.906843606515,
      "learning_rate": 4.034291990529212e-07,
      "loss": 1.7751,
      "step": 61504
    },
    {
      "epoch": 0.00022332635665898545,
      "grad_norm": 9482.495241232658,
      "learning_rate": 4.0332418380393466e-07,
      "loss": 1.7902,
      "step": 61536
    },
    {
      "epoch": 0.00022344249100982215,
      "grad_norm": 9221.540977515635,
      "learning_rate": 4.032192505207372e-07,
      "loss": 1.776,
      "step": 61568
    },
    {
      "epoch": 0.00022355862536065885,
      "grad_norm": 9988.18131593535,
      "learning_rate": 4.031143990967585e-07,
      "loss": 1.7813,
      "step": 61600
    },
    {
      "epoch": 0.00022367475971149555,
      "grad_norm": 9661.916787056283,
      "learning_rate": 4.030096294256224e-07,
      "loss": 1.7726,
      "step": 61632
    },
    {
      "epoch": 0.00022379089406233228,
      "grad_norm": 9468.045310411226,
      "learning_rate": 4.02904941401146e-07,
      "loss": 1.7638,
      "step": 61664
    },
    {
      "epoch": 0.00022390702841316898,
      "grad_norm": 10121.144006484641,
      "learning_rate": 4.028003349173393e-07,
      "loss": 1.7713,
      "step": 61696
    },
    {
      "epoch": 0.00022402316276400568,
      "grad_norm": 8679.36345592233,
      "learning_rate": 4.0269580986840514e-07,
      "loss": 1.782,
      "step": 61728
    },
    {
      "epoch": 0.00022413929711484238,
      "grad_norm": 9972.807227656614,
      "learning_rate": 4.0259136614873797e-07,
      "loss": 1.8038,
      "step": 61760
    },
    {
      "epoch": 0.0002242554314656791,
      "grad_norm": 10688.428790051417,
      "learning_rate": 4.0248700365292437e-07,
      "loss": 1.8214,
      "step": 61792
    },
    {
      "epoch": 0.0002243715658165158,
      "grad_norm": 9201.142211703936,
      "learning_rate": 4.023827222757418e-07,
      "loss": 1.8136,
      "step": 61824
    },
    {
      "epoch": 0.0002244877001673525,
      "grad_norm": 9533.704526573078,
      "learning_rate": 4.0227852191215846e-07,
      "loss": 1.8057,
      "step": 61856
    },
    {
      "epoch": 0.0002246038345181892,
      "grad_norm": 10719.159388683425,
      "learning_rate": 4.0217440245733293e-07,
      "loss": 1.7895,
      "step": 61888
    },
    {
      "epoch": 0.0002247199688690259,
      "grad_norm": 9625.145193710066,
      "learning_rate": 4.020703638066137e-07,
      "loss": 1.7769,
      "step": 61920
    },
    {
      "epoch": 0.00022483610321986263,
      "grad_norm": 9830.383919257682,
      "learning_rate": 4.0196965332101803e-07,
      "loss": 1.7953,
      "step": 61952
    },
    {
      "epoch": 0.00022495223757069933,
      "grad_norm": 10129.81026475817,
      "learning_rate": 4.018657734482848e-07,
      "loss": 1.7879,
      "step": 61984
    },
    {
      "epoch": 0.00022506837192153603,
      "grad_norm": 9907.047895311702,
      "learning_rate": 4.0176197407008756e-07,
      "loss": 1.7825,
      "step": 62016
    },
    {
      "epoch": 0.00022518450627237273,
      "grad_norm": 9797.062621010442,
      "learning_rate": 4.0165825508252375e-07,
      "loss": 1.7752,
      "step": 62048
    },
    {
      "epoch": 0.00022530064062320946,
      "grad_norm": 9141.656305068573,
      "learning_rate": 4.015546163818787e-07,
      "loss": 1.7576,
      "step": 62080
    },
    {
      "epoch": 0.00022541677497404616,
      "grad_norm": 8248.68231901314,
      "learning_rate": 4.0145105786462477e-07,
      "loss": 1.7735,
      "step": 62112
    },
    {
      "epoch": 0.00022553290932488286,
      "grad_norm": 9628.356142145969,
      "learning_rate": 4.0134757942742117e-07,
      "loss": 1.7937,
      "step": 62144
    },
    {
      "epoch": 0.00022564904367571956,
      "grad_norm": 9312.730426679385,
      "learning_rate": 4.012441809671135e-07,
      "loss": 1.7557,
      "step": 62176
    },
    {
      "epoch": 0.00022576517802655626,
      "grad_norm": 8879.2340885912,
      "learning_rate": 4.011408623807334e-07,
      "loss": 1.7596,
      "step": 62208
    },
    {
      "epoch": 0.00022588131237739298,
      "grad_norm": 9808.55891555941,
      "learning_rate": 4.0103762356549773e-07,
      "loss": 1.7619,
      "step": 62240
    },
    {
      "epoch": 0.00022599744672822968,
      "grad_norm": 10613.52985580198,
      "learning_rate": 4.009344644188087e-07,
      "loss": 1.7665,
      "step": 62272
    },
    {
      "epoch": 0.00022611358107906638,
      "grad_norm": 9714.114782109587,
      "learning_rate": 4.0083138483825304e-07,
      "loss": 1.7753,
      "step": 62304
    },
    {
      "epoch": 0.00022622971542990308,
      "grad_norm": 9014.903770978368,
      "learning_rate": 4.007283847216017e-07,
      "loss": 1.7883,
      "step": 62336
    },
    {
      "epoch": 0.0002263458497807398,
      "grad_norm": 9922.855032701023,
      "learning_rate": 4.006254639668094e-07,
      "loss": 1.7907,
      "step": 62368
    },
    {
      "epoch": 0.0002264619841315765,
      "grad_norm": 9390.30489387858,
      "learning_rate": 4.005226224720144e-07,
      "loss": 1.774,
      "step": 62400
    },
    {
      "epoch": 0.0002265781184824132,
      "grad_norm": 11176.80741535793,
      "learning_rate": 4.004198601355376e-07,
      "loss": 1.78,
      "step": 62432
    },
    {
      "epoch": 0.0002266942528332499,
      "grad_norm": 10303.86655581292,
      "learning_rate": 4.0031717685588264e-07,
      "loss": 1.7756,
      "step": 62464
    },
    {
      "epoch": 0.0002268103871840866,
      "grad_norm": 9424.48460129253,
      "learning_rate": 4.002145725317353e-07,
      "loss": 1.7645,
      "step": 62496
    },
    {
      "epoch": 0.00022692652153492334,
      "grad_norm": 9228.667292735176,
      "learning_rate": 4.0011204706196275e-07,
      "loss": 1.7812,
      "step": 62528
    },
    {
      "epoch": 0.00022704265588576004,
      "grad_norm": 8184.30778991113,
      "learning_rate": 4.0000960034561387e-07,
      "loss": 1.8096,
      "step": 62560
    },
    {
      "epoch": 0.00022715879023659674,
      "grad_norm": 9269.56600925847,
      "learning_rate": 3.999072322819179e-07,
      "loss": 1.7923,
      "step": 62592
    },
    {
      "epoch": 0.00022727492458743344,
      "grad_norm": 9219.582420044848,
      "learning_rate": 3.9980494277028493e-07,
      "loss": 1.7865,
      "step": 62624
    },
    {
      "epoch": 0.00022739105893827016,
      "grad_norm": 8693.983781903438,
      "learning_rate": 3.9970273171030475e-07,
      "loss": 1.7919,
      "step": 62656
    },
    {
      "epoch": 0.00022750719328910686,
      "grad_norm": 10035.434420093632,
      "learning_rate": 3.9960059900174685e-07,
      "loss": 1.7956,
      "step": 62688
    },
    {
      "epoch": 0.00022762332763994356,
      "grad_norm": 9442.157910138974,
      "learning_rate": 3.994985445445601e-07,
      "loss": 1.8204,
      "step": 62720
    },
    {
      "epoch": 0.00022773946199078026,
      "grad_norm": 9749.252381593165,
      "learning_rate": 3.9939656823887174e-07,
      "loss": 1.8026,
      "step": 62752
    },
    {
      "epoch": 0.00022785559634161696,
      "grad_norm": 9840.26351273176,
      "learning_rate": 3.992946699849878e-07,
      "loss": 1.7797,
      "step": 62784
    },
    {
      "epoch": 0.0002279717306924537,
      "grad_norm": 9354.835861734828,
      "learning_rate": 3.99192849683392e-07,
      "loss": 1.7684,
      "step": 62816
    },
    {
      "epoch": 0.0002280878650432904,
      "grad_norm": 9909.055555399818,
      "learning_rate": 3.9909110723474565e-07,
      "loss": 1.7584,
      "step": 62848
    },
    {
      "epoch": 0.0002282039993941271,
      "grad_norm": 8974.23266914782,
      "learning_rate": 3.989894425398872e-07,
      "loss": 1.7683,
      "step": 62880
    },
    {
      "epoch": 0.0002283201337449638,
      "grad_norm": 9725.237272169763,
      "learning_rate": 3.9888785549983197e-07,
      "loss": 1.7658,
      "step": 62912
    },
    {
      "epoch": 0.00022843626809580052,
      "grad_norm": 8818.474017651806,
      "learning_rate": 3.9878951701418524e-07,
      "loss": 1.7914,
      "step": 62944
    },
    {
      "epoch": 0.00022855240244663722,
      "grad_norm": 9683.601602709605,
      "learning_rate": 3.986880825684355e-07,
      "loss": 1.7779,
      "step": 62976
    },
    {
      "epoch": 0.00022866853679747392,
      "grad_norm": 8489.8077716754,
      "learning_rate": 3.9858672548466506e-07,
      "loss": 1.773,
      "step": 63008
    },
    {
      "epoch": 0.00022878467114831062,
      "grad_norm": 10012.898980814696,
      "learning_rate": 3.9848544566458645e-07,
      "loss": 1.7815,
      "step": 63040
    },
    {
      "epoch": 0.00022890080549914732,
      "grad_norm": 8930.867147147583,
      "learning_rate": 3.9838424301008714e-07,
      "loss": 1.7871,
      "step": 63072
    },
    {
      "epoch": 0.00022901693984998404,
      "grad_norm": 11436.111926699565,
      "learning_rate": 3.982831174232289e-07,
      "loss": 1.7689,
      "step": 63104
    },
    {
      "epoch": 0.00022913307420082074,
      "grad_norm": 10857.062954593199,
      "learning_rate": 3.9818206880624717e-07,
      "loss": 1.7762,
      "step": 63136
    },
    {
      "epoch": 0.00022924920855165744,
      "grad_norm": 9073.180258321776,
      "learning_rate": 3.980810970615513e-07,
      "loss": 1.7516,
      "step": 63168
    },
    {
      "epoch": 0.00022936534290249414,
      "grad_norm": 10718.852643823404,
      "learning_rate": 3.979802020917235e-07,
      "loss": 1.755,
      "step": 63200
    },
    {
      "epoch": 0.00022948147725333087,
      "grad_norm": 10816.9124984905,
      "learning_rate": 3.9787938379951895e-07,
      "loss": 1.7486,
      "step": 63232
    },
    {
      "epoch": 0.00022959761160416757,
      "grad_norm": 10695.592924190787,
      "learning_rate": 3.9777864208786506e-07,
      "loss": 1.771,
      "step": 63264
    },
    {
      "epoch": 0.00022971374595500427,
      "grad_norm": 9636.644748043793,
      "learning_rate": 3.976779768598611e-07,
      "loss": 1.7863,
      "step": 63296
    },
    {
      "epoch": 0.00022982988030584097,
      "grad_norm": 9844.024786640877,
      "learning_rate": 3.975773880187782e-07,
      "loss": 1.8051,
      "step": 63328
    },
    {
      "epoch": 0.00022994601465667767,
      "grad_norm": 11812.930965683327,
      "learning_rate": 3.9747687546805837e-07,
      "loss": 1.8062,
      "step": 63360
    },
    {
      "epoch": 0.0002300621490075144,
      "grad_norm": 9525.365084866826,
      "learning_rate": 3.9737643911131456e-07,
      "loss": 1.7977,
      "step": 63392
    },
    {
      "epoch": 0.0002301782833583511,
      "grad_norm": 9952.075662895655,
      "learning_rate": 3.972760788523301e-07,
      "loss": 1.7922,
      "step": 63424
    },
    {
      "epoch": 0.0002302944177091878,
      "grad_norm": 9894.673213401238,
      "learning_rate": 3.971757945950585e-07,
      "loss": 1.7983,
      "step": 63456
    },
    {
      "epoch": 0.0002304105520600245,
      "grad_norm": 10008.92801452783,
      "learning_rate": 3.970755862436225e-07,
      "loss": 1.8054,
      "step": 63488
    },
    {
      "epoch": 0.0002305266864108612,
      "grad_norm": 8710.285873609431,
      "learning_rate": 3.9697545370231437e-07,
      "loss": 1.8065,
      "step": 63520
    },
    {
      "epoch": 0.00023064282076169792,
      "grad_norm": 10027.965895434627,
      "learning_rate": 3.9687539687559534e-07,
      "loss": 1.8106,
      "step": 63552
    },
    {
      "epoch": 0.00023075895511253462,
      "grad_norm": 10836.861999674997,
      "learning_rate": 3.9677541566809483e-07,
      "loss": 1.7872,
      "step": 63584
    },
    {
      "epoch": 0.00023087508946337132,
      "grad_norm": 10445.846255809052,
      "learning_rate": 3.9667550998461057e-07,
      "loss": 1.7548,
      "step": 63616
    },
    {
      "epoch": 0.00023099122381420802,
      "grad_norm": 10476.778226153307,
      "learning_rate": 3.965756797301079e-07,
      "loss": 1.7681,
      "step": 63648
    },
    {
      "epoch": 0.00023110735816504475,
      "grad_norm": 8022.793528441324,
      "learning_rate": 3.964759248097195e-07,
      "loss": 1.7672,
      "step": 63680
    },
    {
      "epoch": 0.00023122349251588145,
      "grad_norm": 11067.394092558556,
      "learning_rate": 3.963762451287451e-07,
      "loss": 1.7691,
      "step": 63712
    },
    {
      "epoch": 0.00023133962686671815,
      "grad_norm": 9205.284460569375,
      "learning_rate": 3.962766405926509e-07,
      "loss": 1.7644,
      "step": 63744
    },
    {
      "epoch": 0.00023145576121755485,
      "grad_norm": 9605.512688034929,
      "learning_rate": 3.9617711110706935e-07,
      "loss": 1.754,
      "step": 63776
    },
    {
      "epoch": 0.00023157189556839155,
      "grad_norm": 8672.046355964663,
      "learning_rate": 3.960776565777987e-07,
      "loss": 1.7671,
      "step": 63808
    },
    {
      "epoch": 0.00023168802991922828,
      "grad_norm": 12007.652726490718,
      "learning_rate": 3.9597827691080275e-07,
      "loss": 1.7776,
      "step": 63840
    },
    {
      "epoch": 0.00023180416427006498,
      "grad_norm": 11108.77040900567,
      "learning_rate": 3.9587897201221024e-07,
      "loss": 1.7915,
      "step": 63872
    },
    {
      "epoch": 0.00023192029862090168,
      "grad_norm": 9535.79624362853,
      "learning_rate": 3.9577974178831475e-07,
      "loss": 1.7581,
      "step": 63904
    },
    {
      "epoch": 0.00023203643297173838,
      "grad_norm": 8630.133486800769,
      "learning_rate": 3.956836836314237e-07,
      "loss": 1.7602,
      "step": 63936
    },
    {
      "epoch": 0.0002321525673225751,
      "grad_norm": 10383.768680012088,
      "learning_rate": 3.9558460015012845e-07,
      "loss": 1.7718,
      "step": 63968
    },
    {
      "epoch": 0.0002322687016734118,
      "grad_norm": 10372.021982236636,
      "learning_rate": 3.954855910663057e-07,
      "loss": 1.7656,
      "step": 64000
    },
    {
      "epoch": 0.0002323848360242485,
      "grad_norm": 9027.782895041284,
      "learning_rate": 3.95386656286899e-07,
      "loss": 1.7748,
      "step": 64032
    },
    {
      "epoch": 0.0002325009703750852,
      "grad_norm": 9393.168794395211,
      "learning_rate": 3.952877957190148e-07,
      "loss": 1.7845,
      "step": 64064
    },
    {
      "epoch": 0.0002326171047259219,
      "grad_norm": 9786.010014301028,
      "learning_rate": 3.95189009269922e-07,
      "loss": 1.8107,
      "step": 64096
    },
    {
      "epoch": 0.00023273323907675863,
      "grad_norm": 9558.5511454404,
      "learning_rate": 3.950902968470517e-07,
      "loss": 1.8061,
      "step": 64128
    },
    {
      "epoch": 0.00023284937342759533,
      "grad_norm": 9880.326614034579,
      "learning_rate": 3.9499165835799667e-07,
      "loss": 1.7963,
      "step": 64160
    },
    {
      "epoch": 0.00023296550777843203,
      "grad_norm": 9726.91009519467,
      "learning_rate": 3.948930937105112e-07,
      "loss": 1.7918,
      "step": 64192
    },
    {
      "epoch": 0.00023308164212926873,
      "grad_norm": 9166.317581231844,
      "learning_rate": 3.9479460281251055e-07,
      "loss": 1.7957,
      "step": 64224
    },
    {
      "epoch": 0.00023319777648010546,
      "grad_norm": 9086.029495879924,
      "learning_rate": 3.9469618557207074e-07,
      "loss": 1.8009,
      "step": 64256
    },
    {
      "epoch": 0.00023331391083094216,
      "grad_norm": 8367.524723596578,
      "learning_rate": 3.94597841897428e-07,
      "loss": 1.8105,
      "step": 64288
    },
    {
      "epoch": 0.00023343004518177886,
      "grad_norm": 8966.659021062416,
      "learning_rate": 3.9449957169697876e-07,
      "loss": 1.7915,
      "step": 64320
    },
    {
      "epoch": 0.00023354617953261556,
      "grad_norm": 9598.091893704706,
      "learning_rate": 3.9440137487927885e-07,
      "loss": 1.7604,
      "step": 64352
    },
    {
      "epoch": 0.00023366231388345226,
      "grad_norm": 9741.357810900901,
      "learning_rate": 3.9430325135304335e-07,
      "loss": 1.7695,
      "step": 64384
    },
    {
      "epoch": 0.00023377844823428898,
      "grad_norm": 9400.675933144383,
      "learning_rate": 3.942052010271464e-07,
      "loss": 1.7613,
      "step": 64416
    },
    {
      "epoch": 0.00023389458258512568,
      "grad_norm": 9329.30201033282,
      "learning_rate": 3.9410722381062055e-07,
      "loss": 1.7804,
      "step": 64448
    },
    {
      "epoch": 0.00023401071693596238,
      "grad_norm": 9893.289442849633,
      "learning_rate": 3.9400931961265653e-07,
      "loss": 1.7611,
      "step": 64480
    },
    {
      "epoch": 0.00023412685128679908,
      "grad_norm": 9478.523830217446,
      "learning_rate": 3.9391148834260297e-07,
      "loss": 1.7544,
      "step": 64512
    },
    {
      "epoch": 0.0002342429856376358,
      "grad_norm": 10269.991236607751,
      "learning_rate": 3.9381372990996604e-07,
      "loss": 1.7662,
      "step": 64544
    },
    {
      "epoch": 0.0002343591199884725,
      "grad_norm": 10271.433492945374,
      "learning_rate": 3.9371604422440884e-07,
      "loss": 1.785,
      "step": 64576
    },
    {
      "epoch": 0.0002344752543393092,
      "grad_norm": 9303.892088798108,
      "learning_rate": 3.936184311957514e-07,
      "loss": 1.7962,
      "step": 64608
    },
    {
      "epoch": 0.0002345913886901459,
      "grad_norm": 9929.320319135646,
      "learning_rate": 3.9352089073397024e-07,
      "loss": 1.7729,
      "step": 64640
    },
    {
      "epoch": 0.0002347075230409826,
      "grad_norm": 9470.25886657804,
      "learning_rate": 3.934234227491977e-07,
      "loss": 1.7679,
      "step": 64672
    },
    {
      "epoch": 0.00023482365739181934,
      "grad_norm": 10707.212989382439,
      "learning_rate": 3.933260271517221e-07,
      "loss": 1.7911,
      "step": 64704
    },
    {
      "epoch": 0.00023493979174265604,
      "grad_norm": 9548.651108926328,
      "learning_rate": 3.932287038519871e-07,
      "loss": 1.7818,
      "step": 64736
    },
    {
      "epoch": 0.00023505592609349274,
      "grad_norm": 8619.259364933858,
      "learning_rate": 3.9313145276059133e-07,
      "loss": 1.7492,
      "step": 64768
    },
    {
      "epoch": 0.00023517206044432944,
      "grad_norm": 9579.984133598553,
      "learning_rate": 3.9303427378828815e-07,
      "loss": 1.7782,
      "step": 64800
    },
    {
      "epoch": 0.00023528819479516616,
      "grad_norm": 10287.40598984992,
      "learning_rate": 3.9293716684598523e-07,
      "loss": 1.7823,
      "step": 64832
    },
    {
      "epoch": 0.00023540432914600286,
      "grad_norm": 11057.588344661779,
      "learning_rate": 3.928401318447444e-07,
      "loss": 1.7923,
      "step": 64864
    },
    {
      "epoch": 0.00023552046349683956,
      "grad_norm": 9925.564769825443,
      "learning_rate": 3.9274316869578083e-07,
      "loss": 1.8046,
      "step": 64896
    },
    {
      "epoch": 0.00023563659784767626,
      "grad_norm": 10263.536817296463,
      "learning_rate": 3.926462773104635e-07,
      "loss": 1.784,
      "step": 64928
    },
    {
      "epoch": 0.00023575273219851296,
      "grad_norm": 8349.373748970638,
      "learning_rate": 3.92552482132207e-07,
      "loss": 1.7803,
      "step": 64960
    },
    {
      "epoch": 0.0002358688665493497,
      "grad_norm": 9748.439157116385,
      "learning_rate": 3.924557317731462e-07,
      "loss": 1.788,
      "step": 64992
    },
    {
      "epoch": 0.0002359850009001864,
      "grad_norm": 10370.853581070363,
      "learning_rate": 3.923590529155073e-07,
      "loss": 1.7914,
      "step": 65024
    },
    {
      "epoch": 0.0002361011352510231,
      "grad_norm": 8607.622377869513,
      "learning_rate": 3.9226244547126417e-07,
      "loss": 1.7835,
      "step": 65056
    },
    {
      "epoch": 0.0002362172696018598,
      "grad_norm": 9339.32770599683,
      "learning_rate": 3.9216590935254235e-07,
      "loss": 1.7827,
      "step": 65088
    },
    {
      "epoch": 0.00023633340395269652,
      "grad_norm": 8637.6626468044,
      "learning_rate": 3.9206944447161856e-07,
      "loss": 1.7551,
      "step": 65120
    },
    {
      "epoch": 0.00023644953830353322,
      "grad_norm": 9849.290634355348,
      "learning_rate": 3.919730507409207e-07,
      "loss": 1.7663,
      "step": 65152
    },
    {
      "epoch": 0.00023656567265436992,
      "grad_norm": 8309.269161605009,
      "learning_rate": 3.9187672807302724e-07,
      "loss": 1.7796,
      "step": 65184
    },
    {
      "epoch": 0.00023668180700520662,
      "grad_norm": 11680.631746613708,
      "learning_rate": 3.9178047638066673e-07,
      "loss": 1.7928,
      "step": 65216
    },
    {
      "epoch": 0.00023679794135604332,
      "grad_norm": 10131.897749187956,
      "learning_rate": 3.916842955767181e-07,
      "loss": 1.772,
      "step": 65248
    },
    {
      "epoch": 0.00023691407570688004,
      "grad_norm": 11619.624950918167,
      "learning_rate": 3.9158818557420957e-07,
      "loss": 1.7905,
      "step": 65280
    },
    {
      "epoch": 0.00023703021005771674,
      "grad_norm": 10788.011586942239,
      "learning_rate": 3.914921462863188e-07,
      "loss": 1.8071,
      "step": 65312
    },
    {
      "epoch": 0.00023714634440855344,
      "grad_norm": 10268.10732316331,
      "learning_rate": 3.913961776263725e-07,
      "loss": 1.7733,
      "step": 65344
    },
    {
      "epoch": 0.00023726247875939014,
      "grad_norm": 10680.798191146576,
      "learning_rate": 3.9130027950784604e-07,
      "loss": 1.7656,
      "step": 65376
    },
    {
      "epoch": 0.00023737861311022687,
      "grad_norm": 9552.785352974282,
      "learning_rate": 3.9120445184436284e-07,
      "loss": 1.7733,
      "step": 65408
    },
    {
      "epoch": 0.00023749474746106357,
      "grad_norm": 9686.584950332082,
      "learning_rate": 3.911086945496947e-07,
      "loss": 1.7429,
      "step": 65440
    },
    {
      "epoch": 0.00023761088181190027,
      "grad_norm": 9267.794991258708,
      "learning_rate": 3.910130075377609e-07,
      "loss": 1.7753,
      "step": 65472
    },
    {
      "epoch": 0.00023772701616273697,
      "grad_norm": 9050.89332607561,
      "learning_rate": 3.90917390722628e-07,
      "loss": 1.7784,
      "step": 65504
    },
    {
      "epoch": 0.00023784315051357367,
      "grad_norm": 10423.335454642147,
      "learning_rate": 3.908218440185098e-07,
      "loss": 1.7701,
      "step": 65536
    },
    {
      "epoch": 0.0002379592848644104,
      "grad_norm": 9439.993644065657,
      "learning_rate": 3.9072636733976663e-07,
      "loss": 1.7847,
      "step": 65568
    },
    {
      "epoch": 0.0002380754192152471,
      "grad_norm": 10941.583432026646,
      "learning_rate": 3.906309606009052e-07,
      "loss": 1.7895,
      "step": 65600
    },
    {
      "epoch": 0.0002381915535660838,
      "grad_norm": 9652.24077610997,
      "learning_rate": 3.9053562371657844e-07,
      "loss": 1.7809,
      "step": 65632
    },
    {
      "epoch": 0.0002383076879169205,
      "grad_norm": 9535.24367806088,
      "learning_rate": 3.904403566015848e-07,
      "loss": 1.7857,
      "step": 65664
    },
    {
      "epoch": 0.00023842382226775722,
      "grad_norm": 9577.784503735716,
      "learning_rate": 3.903451591708682e-07,
      "loss": 1.7773,
      "step": 65696
    },
    {
      "epoch": 0.00023853995661859392,
      "grad_norm": 9442.44290424888,
      "learning_rate": 3.902500313395178e-07,
      "loss": 1.7776,
      "step": 65728
    },
    {
      "epoch": 0.00023865609096943062,
      "grad_norm": 9260.814327044896,
      "learning_rate": 3.9015497302276753e-07,
      "loss": 1.78,
      "step": 65760
    },
    {
      "epoch": 0.00023877222532026732,
      "grad_norm": 10032.745885349632,
      "learning_rate": 3.900599841359955e-07,
      "loss": 1.7963,
      "step": 65792
    },
    {
      "epoch": 0.00023888835967110402,
      "grad_norm": 11614.613984115012,
      "learning_rate": 3.8996506459472436e-07,
      "loss": 1.8042,
      "step": 65824
    },
    {
      "epoch": 0.00023900449402194075,
      "grad_norm": 11401.780913523991,
      "learning_rate": 3.898702143146203e-07,
      "loss": 1.7923,
      "step": 65856
    },
    {
      "epoch": 0.00023912062837277745,
      "grad_norm": 7539.373581405818,
      "learning_rate": 3.8977543321149305e-07,
      "loss": 1.781,
      "step": 65888
    },
    {
      "epoch": 0.00023923676272361415,
      "grad_norm": 10161.759394907951,
      "learning_rate": 3.8968072120129574e-07,
      "loss": 1.7821,
      "step": 65920
    },
    {
      "epoch": 0.00023935289707445085,
      "grad_norm": 8574.798423286695,
      "learning_rate": 3.8958903475017306e-07,
      "loss": 1.759,
      "step": 65952
    },
    {
      "epoch": 0.00023946903142528755,
      "grad_norm": 9552.050355813666,
      "learning_rate": 3.894944585216177e-07,
      "loss": 1.767,
      "step": 65984
    },
    {
      "epoch": 0.00023958516577612428,
      "grad_norm": 10105.015487370614,
      "learning_rate": 3.8939995113731845e-07,
      "loss": 1.7644,
      "step": 66016
    },
    {
      "epoch": 0.00023970130012696098,
      "grad_norm": 8725.069856453872,
      "learning_rate": 3.8930551251379325e-07,
      "loss": 1.7626,
      "step": 66048
    },
    {
      "epoch": 0.00023981743447779768,
      "grad_norm": 9523.158929682944,
      "learning_rate": 3.8921114256770233e-07,
      "loss": 1.797,
      "step": 66080
    },
    {
      "epoch": 0.00023993356882863438,
      "grad_norm": 9673.386583818514,
      "learning_rate": 3.891168412158467e-07,
      "loss": 1.7929,
      "step": 66112
    },
    {
      "epoch": 0.0002400497031794711,
      "grad_norm": 9498.394601194455,
      "learning_rate": 3.8902260837516885e-07,
      "loss": 1.8025,
      "step": 66144
    },
    {
      "epoch": 0.0002401658375303078,
      "grad_norm": 10586.831726253138,
      "learning_rate": 3.889284439627517e-07,
      "loss": 1.757,
      "step": 66176
    },
    {
      "epoch": 0.0002402819718811445,
      "grad_norm": 10360.863284495168,
      "learning_rate": 3.8883434789581883e-07,
      "loss": 1.7473,
      "step": 66208
    },
    {
      "epoch": 0.0002403981062319812,
      "grad_norm": 11126.537107294434,
      "learning_rate": 3.8874032009173364e-07,
      "loss": 1.7683,
      "step": 66240
    },
    {
      "epoch": 0.0002405142405828179,
      "grad_norm": 9974.03719664209,
      "learning_rate": 3.886463604679996e-07,
      "loss": 1.7444,
      "step": 66272
    },
    {
      "epoch": 0.00024063037493365463,
      "grad_norm": 9221.348057632355,
      "learning_rate": 3.885524689422597e-07,
      "loss": 1.7636,
      "step": 66304
    },
    {
      "epoch": 0.00024074650928449133,
      "grad_norm": 10397.341775665547,
      "learning_rate": 3.8845864543229595e-07,
      "loss": 1.7827,
      "step": 66336
    },
    {
      "epoch": 0.00024086264363532803,
      "grad_norm": 8964.386426298233,
      "learning_rate": 3.8836488985602936e-07,
      "loss": 1.7828,
      "step": 66368
    },
    {
      "epoch": 0.00024097877798616473,
      "grad_norm": 9778.328384749615,
      "learning_rate": 3.882712021315196e-07,
      "loss": 1.7987,
      "step": 66400
    },
    {
      "epoch": 0.00024109491233700146,
      "grad_norm": 9324.717261129155,
      "learning_rate": 3.881775821769645e-07,
      "loss": 1.8286,
      "step": 66432
    },
    {
      "epoch": 0.00024121104668783816,
      "grad_norm": 9162.585879542958,
      "learning_rate": 3.880840299107001e-07,
      "loss": 1.8076,
      "step": 66464
    },
    {
      "epoch": 0.00024132718103867486,
      "grad_norm": 9915.355565989552,
      "learning_rate": 3.8799054525119994e-07,
      "loss": 1.7967,
      "step": 66496
    },
    {
      "epoch": 0.00024144331538951156,
      "grad_norm": 9311.187464550372,
      "learning_rate": 3.87897128117075e-07,
      "loss": 1.7799,
      "step": 66528
    },
    {
      "epoch": 0.00024155944974034826,
      "grad_norm": 8588.58009219219,
      "learning_rate": 3.8780377842707365e-07,
      "loss": 1.7789,
      "step": 66560
    },
    {
      "epoch": 0.00024167558409118498,
      "grad_norm": 9197.50781462022,
      "learning_rate": 3.877104961000806e-07,
      "loss": 1.775,
      "step": 66592
    },
    {
      "epoch": 0.00024179171844202168,
      "grad_norm": 8841.882718064066,
      "learning_rate": 3.8761728105511745e-07,
      "loss": 1.7506,
      "step": 66624
    },
    {
      "epoch": 0.00024190785279285838,
      "grad_norm": 9779.32605039836,
      "learning_rate": 3.8752413321134174e-07,
      "loss": 1.7595,
      "step": 66656
    },
    {
      "epoch": 0.00024202398714369508,
      "grad_norm": 10838.4766457284,
      "learning_rate": 3.874310524880471e-07,
      "loss": 1.7643,
      "step": 66688
    },
    {
      "epoch": 0.0002421401214945318,
      "grad_norm": 11424.815797202158,
      "learning_rate": 3.873380388046629e-07,
      "loss": 1.7821,
      "step": 66720
    },
    {
      "epoch": 0.0002422562558453685,
      "grad_norm": 9728.367077778263,
      "learning_rate": 3.8724509208075365e-07,
      "loss": 1.7772,
      "step": 66752
    },
    {
      "epoch": 0.0002423723901962052,
      "grad_norm": 10112.010482589503,
      "learning_rate": 3.8715221223601885e-07,
      "loss": 1.7606,
      "step": 66784
    },
    {
      "epoch": 0.0002424885245470419,
      "grad_norm": 9047.180334225686,
      "learning_rate": 3.87059399190293e-07,
      "loss": 1.7625,
      "step": 66816
    },
    {
      "epoch": 0.0002426046588978786,
      "grad_norm": 10897.61928129259,
      "learning_rate": 3.869666528635449e-07,
      "loss": 1.7883,
      "step": 66848
    },
    {
      "epoch": 0.00024272079324871534,
      "grad_norm": 11251.043596040325,
      "learning_rate": 3.868739731758776e-07,
      "loss": 1.7747,
      "step": 66880
    },
    {
      "epoch": 0.00024283692759955204,
      "grad_norm": 9836.485957901836,
      "learning_rate": 3.867813600475279e-07,
      "loss": 1.7823,
      "step": 66912
    },
    {
      "epoch": 0.00024295306195038874,
      "grad_norm": 8164.527298013033,
      "learning_rate": 3.8669170447614275e-07,
      "loss": 1.7663,
      "step": 66944
    },
    {
      "epoch": 0.00024306919630122544,
      "grad_norm": 9317.375918143476,
      "learning_rate": 3.8659922215386916e-07,
      "loss": 1.7662,
      "step": 66976
    },
    {
      "epoch": 0.00024318533065206216,
      "grad_norm": 9087.961487594455,
      "learning_rate": 3.8650680615490247e-07,
      "loss": 1.7946,
      "step": 67008
    },
    {
      "epoch": 0.00024330146500289886,
      "grad_norm": 10422.270098207971,
      "learning_rate": 3.864144564000081e-07,
      "loss": 1.8004,
      "step": 67040
    },
    {
      "epoch": 0.00024341759935373556,
      "grad_norm": 8611.771826981949,
      "learning_rate": 3.863221728100837e-07,
      "loss": 1.798,
      "step": 67072
    },
    {
      "epoch": 0.00024353373370457226,
      "grad_norm": 9779.665331697193,
      "learning_rate": 3.8622995530615967e-07,
      "loss": 1.7833,
      "step": 67104
    },
    {
      "epoch": 0.00024364986805540896,
      "grad_norm": 9476.135288185791,
      "learning_rate": 3.861378038093977e-07,
      "loss": 1.7729,
      "step": 67136
    },
    {
      "epoch": 0.0002437660024062457,
      "grad_norm": 10042.023700430109,
      "learning_rate": 3.8604571824109154e-07,
      "loss": 1.7808,
      "step": 67168
    },
    {
      "epoch": 0.0002438821367570824,
      "grad_norm": 9491.166208638431,
      "learning_rate": 3.859536985226658e-07,
      "loss": 1.7747,
      "step": 67200
    },
    {
      "epoch": 0.0002439982711079191,
      "grad_norm": 9075.86326472584,
      "learning_rate": 3.858617445756766e-07,
      "loss": 1.7831,
      "step": 67232
    },
    {
      "epoch": 0.0002441144054587558,
      "grad_norm": 10004.3812402367,
      "learning_rate": 3.857698563218106e-07,
      "loss": 1.7906,
      "step": 67264
    },
    {
      "epoch": 0.0002442305398095925,
      "grad_norm": 11166.936374852326,
      "learning_rate": 3.8567803368288496e-07,
      "loss": 1.7956,
      "step": 67296
    },
    {
      "epoch": 0.0002443466741604292,
      "grad_norm": 9437.724619843493,
      "learning_rate": 3.8558627658084696e-07,
      "loss": 1.7909,
      "step": 67328
    },
    {
      "epoch": 0.0002444628085112659,
      "grad_norm": 9957.027568506577,
      "learning_rate": 3.85494584937774e-07,
      "loss": 1.7888,
      "step": 67360
    },
    {
      "epoch": 0.00024457894286210264,
      "grad_norm": 10299.736986933209,
      "learning_rate": 3.85402958675873e-07,
      "loss": 1.7405,
      "step": 67392
    },
    {
      "epoch": 0.00024469507721293934,
      "grad_norm": 9824.417132837958,
      "learning_rate": 3.853113977174803e-07,
      "loss": 1.756,
      "step": 67424
    },
    {
      "epoch": 0.00024481121156377604,
      "grad_norm": 10209.574623851868,
      "learning_rate": 3.8521990198506127e-07,
      "loss": 1.7537,
      "step": 67456
    },
    {
      "epoch": 0.00024492734591461274,
      "grad_norm": 11170.171260996853,
      "learning_rate": 3.851284714012101e-07,
      "loss": 1.7546,
      "step": 67488
    },
    {
      "epoch": 0.00024504348026544944,
      "grad_norm": 9652.460100927638,
      "learning_rate": 3.850371058886496e-07,
      "loss": 1.7543,
      "step": 67520
    },
    {
      "epoch": 0.00024515961461628614,
      "grad_norm": 9173.45333012601,
      "learning_rate": 3.849458053702308e-07,
      "loss": 1.7646,
      "step": 67552
    },
    {
      "epoch": 0.00024527574896712284,
      "grad_norm": 9737.19014911386,
      "learning_rate": 3.848545697689328e-07,
      "loss": 1.7784,
      "step": 67584
    },
    {
      "epoch": 0.00024539188331795954,
      "grad_norm": 9072.969965782979,
      "learning_rate": 3.847633990078622e-07,
      "loss": 1.807,
      "step": 67616
    },
    {
      "epoch": 0.00024550801766879624,
      "grad_norm": 8607.736636305737,
      "learning_rate": 3.8467229301025336e-07,
      "loss": 1.8167,
      "step": 67648
    },
    {
      "epoch": 0.000245624152019633,
      "grad_norm": 10140.105916606592,
      "learning_rate": 3.845812516994676e-07,
      "loss": 1.7968,
      "step": 67680
    },
    {
      "epoch": 0.0002457402863704697,
      "grad_norm": 10885.705673037464,
      "learning_rate": 3.844902749989931e-07,
      "loss": 1.7559,
      "step": 67712
    },
    {
      "epoch": 0.0002458564207213064,
      "grad_norm": 10521.584671521681,
      "learning_rate": 3.84399362832445e-07,
      "loss": 1.7608,
      "step": 67744
    },
    {
      "epoch": 0.0002459725550721431,
      "grad_norm": 10300.262715096154,
      "learning_rate": 3.843085151235645e-07,
      "loss": 1.7646,
      "step": 67776
    },
    {
      "epoch": 0.0002460886894229798,
      "grad_norm": 8877.173424012848,
      "learning_rate": 3.84217731796219e-07,
      "loss": 1.7668,
      "step": 67808
    },
    {
      "epoch": 0.0002462048237738165,
      "grad_norm": 10955.450150495872,
      "learning_rate": 3.841270127744016e-07,
      "loss": 1.7936,
      "step": 67840
    },
    {
      "epoch": 0.0002463209581246532,
      "grad_norm": 10006.059863902476,
      "learning_rate": 3.8403635798223133e-07,
      "loss": 1.8026,
      "step": 67872
    },
    {
      "epoch": 0.0002464370924754899,
      "grad_norm": 8581.752152095747,
      "learning_rate": 3.8394576734395207e-07,
      "loss": 1.7825,
      "step": 67904
    },
    {
      "epoch": 0.0002465532268263266,
      "grad_norm": 9295.50934591537,
      "learning_rate": 3.8385524078393306e-07,
      "loss": 1.7876,
      "step": 67936
    },
    {
      "epoch": 0.00024666936117716335,
      "grad_norm": 8044.547594489078,
      "learning_rate": 3.837676042135401e-07,
      "loss": 1.7944,
      "step": 67968
    },
    {
      "epoch": 0.00024678549552800005,
      "grad_norm": 9036.932001514673,
      "learning_rate": 3.836772035870571e-07,
      "loss": 1.7696,
      "step": 68000
    },
    {
      "epoch": 0.00024690162987883675,
      "grad_norm": 9277.817631318261,
      "learning_rate": 3.835868668150391e-07,
      "loss": 1.7812,
      "step": 68032
    },
    {
      "epoch": 0.00024701776422967345,
      "grad_norm": 9736.276701080347,
      "learning_rate": 3.8349659382234886e-07,
      "loss": 1.7799,
      "step": 68064
    },
    {
      "epoch": 0.00024713389858051015,
      "grad_norm": 10956.579210684327,
      "learning_rate": 3.834063845339729e-07,
      "loss": 1.7774,
      "step": 68096
    },
    {
      "epoch": 0.00024725003293134685,
      "grad_norm": 9114.325208154469,
      "learning_rate": 3.8331623887502107e-07,
      "loss": 1.7787,
      "step": 68128
    },
    {
      "epoch": 0.00024736616728218355,
      "grad_norm": 11158.875929053069,
      "learning_rate": 3.8322615677072645e-07,
      "loss": 1.7739,
      "step": 68160
    },
    {
      "epoch": 0.00024748230163302025,
      "grad_norm": 9038.538266777432,
      "learning_rate": 3.8313613814644506e-07,
      "loss": 1.7587,
      "step": 68192
    },
    {
      "epoch": 0.00024759843598385695,
      "grad_norm": 7780.126220055816,
      "learning_rate": 3.830461829276558e-07,
      "loss": 1.7759,
      "step": 68224
    },
    {
      "epoch": 0.0002477145703346937,
      "grad_norm": 9433.750579700525,
      "learning_rate": 3.829562910399597e-07,
      "loss": 1.7943,
      "step": 68256
    },
    {
      "epoch": 0.0002478307046855304,
      "grad_norm": 10572.886833783856,
      "learning_rate": 3.8286646240908003e-07,
      "loss": 1.7658,
      "step": 68288
    },
    {
      "epoch": 0.0002479468390363671,
      "grad_norm": 10509.595710587539,
      "learning_rate": 3.827766969608621e-07,
      "loss": 1.7442,
      "step": 68320
    },
    {
      "epoch": 0.0002480629733872038,
      "grad_norm": 10517.775430194351,
      "learning_rate": 3.8268699462127256e-07,
      "loss": 1.76,
      "step": 68352
    },
    {
      "epoch": 0.0002481791077380405,
      "grad_norm": 10357.423038574798,
      "learning_rate": 3.825973553164e-07,
      "loss": 1.7778,
      "step": 68384
    },
    {
      "epoch": 0.0002482952420888772,
      "grad_norm": 9750.25702225331,
      "learning_rate": 3.8250777897245337e-07,
      "loss": 1.7797,
      "step": 68416
    },
    {
      "epoch": 0.0002484113764397139,
      "grad_norm": 9570.778129284994,
      "learning_rate": 3.824182655157633e-07,
      "loss": 1.7786,
      "step": 68448
    },
    {
      "epoch": 0.0002485275107905506,
      "grad_norm": 8064.804399363942,
      "learning_rate": 3.8232881487278047e-07,
      "loss": 1.7541,
      "step": 68480
    },
    {
      "epoch": 0.0002486436451413873,
      "grad_norm": 8624.883883276343,
      "learning_rate": 3.822394269700762e-07,
      "loss": 1.7652,
      "step": 68512
    },
    {
      "epoch": 0.00024875977949222406,
      "grad_norm": 9267.510668998444,
      "learning_rate": 3.821501017343418e-07,
      "loss": 1.7724,
      "step": 68544
    },
    {
      "epoch": 0.00024887591384306076,
      "grad_norm": 10622.320085555697,
      "learning_rate": 3.820608390923884e-07,
      "loss": 1.7886,
      "step": 68576
    },
    {
      "epoch": 0.00024899204819389746,
      "grad_norm": 8508.364472682162,
      "learning_rate": 3.819716389711471e-07,
      "loss": 1.7875,
      "step": 68608
    },
    {
      "epoch": 0.00024910818254473416,
      "grad_norm": 8770.04663613598,
      "learning_rate": 3.818825012976679e-07,
      "loss": 1.7876,
      "step": 68640
    },
    {
      "epoch": 0.00024922431689557086,
      "grad_norm": 9150.34972009267,
      "learning_rate": 3.8179342599912016e-07,
      "loss": 1.7911,
      "step": 68672
    },
    {
      "epoch": 0.00024934045124640756,
      "grad_norm": 8838.026702833615,
      "learning_rate": 3.817044130027921e-07,
      "loss": 1.7937,
      "step": 68704
    },
    {
      "epoch": 0.00024945658559724426,
      "grad_norm": 10786.634414867318,
      "learning_rate": 3.816154622360904e-07,
      "loss": 1.8086,
      "step": 68736
    },
    {
      "epoch": 0.00024957271994808096,
      "grad_norm": 11211.854975872639,
      "learning_rate": 3.815265736265404e-07,
      "loss": 1.786,
      "step": 68768
    },
    {
      "epoch": 0.00024968885429891766,
      "grad_norm": 9908.042995465856,
      "learning_rate": 3.814377471017851e-07,
      "loss": 1.7961,
      "step": 68800
    },
    {
      "epoch": 0.0002498049886497544,
      "grad_norm": 9456.915987783756,
      "learning_rate": 3.813489825895857e-07,
      "loss": 1.8003,
      "step": 68832
    },
    {
      "epoch": 0.0002499211230005911,
      "grad_norm": 9834.102297617206,
      "learning_rate": 3.8126028001782117e-07,
      "loss": 1.793,
      "step": 68864
    },
    {
      "epoch": 0.0002500372573514278,
      "grad_norm": 8260.320453832284,
      "learning_rate": 3.811716393144874e-07,
      "loss": 1.7744,
      "step": 68896
    },
    {
      "epoch": 0.0002501533917022645,
      "grad_norm": 9423.615548185315,
      "learning_rate": 3.810830604076976e-07,
      "loss": 1.7421,
      "step": 68928
    },
    {
      "epoch": 0.0002502695260531012,
      "grad_norm": 9845.569358853758,
      "learning_rate": 3.809973084540216e-07,
      "loss": 1.75,
      "step": 68960
    },
    {
      "epoch": 0.0002503856604039379,
      "grad_norm": 9567.745711503834,
      "learning_rate": 3.8090885099955055e-07,
      "loss": 1.7718,
      "step": 68992
    },
    {
      "epoch": 0.0002505017947547746,
      "grad_norm": 9456.180624332426,
      "learning_rate": 3.808204551288973e-07,
      "loss": 1.7837,
      "step": 69024
    },
    {
      "epoch": 0.0002506179291056113,
      "grad_norm": 10502.48180193615,
      "learning_rate": 3.807321207706377e-07,
      "loss": 1.7429,
      "step": 69056
    },
    {
      "epoch": 0.000250734063456448,
      "grad_norm": 9528.961538383917,
      "learning_rate": 3.8064384785346323e-07,
      "loss": 1.753,
      "step": 69088
    },
    {
      "epoch": 0.00025085019780728477,
      "grad_norm": 9537.399226204176,
      "learning_rate": 3.805556363061814e-07,
      "loss": 1.7593,
      "step": 69120
    },
    {
      "epoch": 0.00025096633215812147,
      "grad_norm": 9982.874736267104,
      "learning_rate": 3.8046748605771476e-07,
      "loss": 1.7758,
      "step": 69152
    },
    {
      "epoch": 0.00025108246650895816,
      "grad_norm": 8998.102355496963,
      "learning_rate": 3.803793970371013e-07,
      "loss": 1.7862,
      "step": 69184
    },
    {
      "epoch": 0.00025119860085979486,
      "grad_norm": 9412.339241655074,
      "learning_rate": 3.8029136917349396e-07,
      "loss": 1.751,
      "step": 69216
    },
    {
      "epoch": 0.00025131473521063156,
      "grad_norm": 9165.393608569138,
      "learning_rate": 3.802034023961601e-07,
      "loss": 1.7517,
      "step": 69248
    },
    {
      "epoch": 0.00025143086956146826,
      "grad_norm": 9176.221989468215,
      "learning_rate": 3.8011549663448206e-07,
      "loss": 1.7582,
      "step": 69280
    },
    {
      "epoch": 0.00025154700391230496,
      "grad_norm": 9108.925403141691,
      "learning_rate": 3.8002765181795605e-07,
      "loss": 1.7673,
      "step": 69312
    },
    {
      "epoch": 0.00025166313826314166,
      "grad_norm": 10556.823385848606,
      "learning_rate": 3.7993986787619226e-07,
      "loss": 1.7864,
      "step": 69344
    },
    {
      "epoch": 0.00025177927261397836,
      "grad_norm": 11083.75279406754,
      "learning_rate": 3.7985214473891496e-07,
      "loss": 1.8043,
      "step": 69376
    },
    {
      "epoch": 0.0002518954069648151,
      "grad_norm": 8857.07118634597,
      "learning_rate": 3.797644823359616e-07,
      "loss": 1.8047,
      "step": 69408
    },
    {
      "epoch": 0.0002520115413156518,
      "grad_norm": 8201.088342409195,
      "learning_rate": 3.796768805972832e-07,
      "loss": 1.8047,
      "step": 69440
    },
    {
      "epoch": 0.0002521276756664885,
      "grad_norm": 8411.110033758921,
      "learning_rate": 3.795893394529435e-07,
      "loss": 1.7992,
      "step": 69472
    },
    {
      "epoch": 0.0002522438100173252,
      "grad_norm": 10277.271038558825,
      "learning_rate": 3.795018588331196e-07,
      "loss": 1.7958,
      "step": 69504
    },
    {
      "epoch": 0.0002523599443681619,
      "grad_norm": 9235.419102563781,
      "learning_rate": 3.794144386681006e-07,
      "loss": 1.785,
      "step": 69536
    },
    {
      "epoch": 0.0002524760787189986,
      "grad_norm": 11126.762691816519,
      "learning_rate": 3.793270788882883e-07,
      "loss": 1.7876,
      "step": 69568
    },
    {
      "epoch": 0.0002525922130698353,
      "grad_norm": 10831.67586294937,
      "learning_rate": 3.792397794241965e-07,
      "loss": 1.8156,
      "step": 69600
    },
    {
      "epoch": 0.000252708347420672,
      "grad_norm": 10059.387058861987,
      "learning_rate": 3.7915254020645105e-07,
      "loss": 1.7727,
      "step": 69632
    },
    {
      "epoch": 0.0002528244817715087,
      "grad_norm": 9539.75670549307,
      "learning_rate": 3.790653611657892e-07,
      "loss": 1.7439,
      "step": 69664
    },
    {
      "epoch": 0.00025294061612234547,
      "grad_norm": 8985.577443881946,
      "learning_rate": 3.7897824223305984e-07,
      "loss": 1.7519,
      "step": 69696
    },
    {
      "epoch": 0.00025305675047318217,
      "grad_norm": 9892.709740005517,
      "learning_rate": 3.7889118333922307e-07,
      "loss": 1.7399,
      "step": 69728
    },
    {
      "epoch": 0.00025317288482401887,
      "grad_norm": 10520.868500271257,
      "learning_rate": 3.788041844153497e-07,
      "loss": 1.7732,
      "step": 69760
    },
    {
      "epoch": 0.00025328901917485557,
      "grad_norm": 8506.589328279579,
      "learning_rate": 3.787172453926216e-07,
      "loss": 1.7733,
      "step": 69792
    },
    {
      "epoch": 0.00025340515352569227,
      "grad_norm": 9301.169818899125,
      "learning_rate": 3.7863036620233097e-07,
      "loss": 1.7561,
      "step": 69824
    },
    {
      "epoch": 0.00025352128787652897,
      "grad_norm": 9113.334515971637,
      "learning_rate": 3.7854354677588027e-07,
      "loss": 1.7717,
      "step": 69856
    },
    {
      "epoch": 0.00025363742222736567,
      "grad_norm": 9372.438743464798,
      "learning_rate": 3.784567870447821e-07,
      "loss": 1.7837,
      "step": 69888
    },
    {
      "epoch": 0.00025375355657820237,
      "grad_norm": 10059.798705739593,
      "learning_rate": 3.783700869406589e-07,
      "loss": 1.7762,
      "step": 69920
    },
    {
      "epoch": 0.00025386969092903907,
      "grad_norm": 8683.531079002367,
      "learning_rate": 3.7828615301144077e-07,
      "loss": 1.7771,
      "step": 69952
    },
    {
      "epoch": 0.0002539858252798758,
      "grad_norm": 12286.630376144632,
      "learning_rate": 3.7819957009852387e-07,
      "loss": 1.7676,
      "step": 69984
    },
    {
      "epoch": 0.0002541019596307125,
      "grad_norm": 9252.972711512772,
      "learning_rate": 3.7811304661023057e-07,
      "loss": 1.774,
      "step": 70016
    },
    {
      "epoch": 0.0002542180939815492,
      "grad_norm": 9275.141292724333,
      "learning_rate": 3.780265824786171e-07,
      "loss": 1.7513,
      "step": 70048
    },
    {
      "epoch": 0.0002543342283323859,
      "grad_norm": 9526.095107650353,
      "learning_rate": 3.7794017763584827e-07,
      "loss": 1.764,
      "step": 70080
    },
    {
      "epoch": 0.0002544503626832226,
      "grad_norm": 10012.921451804163,
      "learning_rate": 3.7785383201419716e-07,
      "loss": 1.7971,
      "step": 70112
    },
    {
      "epoch": 0.0002545664970340593,
      "grad_norm": 9374.934773106423,
      "learning_rate": 3.777675455460454e-07,
      "loss": 1.7834,
      "step": 70144
    },
    {
      "epoch": 0.000254682631384896,
      "grad_norm": 9191.761746259528,
      "learning_rate": 3.7768131816388255e-07,
      "loss": 1.7883,
      "step": 70176
    },
    {
      "epoch": 0.0002547987657357327,
      "grad_norm": 9509.237719186538,
      "learning_rate": 3.7759514980030587e-07,
      "loss": 1.8025,
      "step": 70208
    },
    {
      "epoch": 0.0002549149000865694,
      "grad_norm": 7458.503603270566,
      "learning_rate": 3.775090403880204e-07,
      "loss": 1.7782,
      "step": 70240
    },
    {
      "epoch": 0.0002550310344374062,
      "grad_norm": 9938.767126761748,
      "learning_rate": 3.774229898598384e-07,
      "loss": 1.7807,
      "step": 70272
    },
    {
      "epoch": 0.0002551471687882429,
      "grad_norm": 10403.81891422568,
      "learning_rate": 3.7733699814867946e-07,
      "loss": 1.7844,
      "step": 70304
    },
    {
      "epoch": 0.0002552633031390796,
      "grad_norm": 10630.878420902009,
      "learning_rate": 3.7725106518756996e-07,
      "loss": 1.7803,
      "step": 70336
    },
    {
      "epoch": 0.0002553794374899163,
      "grad_norm": 9170.07557220768,
      "learning_rate": 3.771651909096432e-07,
      "loss": 1.8039,
      "step": 70368
    },
    {
      "epoch": 0.000255495571840753,
      "grad_norm": 8776.040565084006,
      "learning_rate": 3.770793752481385e-07,
      "loss": 1.7946,
      "step": 70400
    },
    {
      "epoch": 0.0002556117061915897,
      "grad_norm": 10287.931959339543,
      "learning_rate": 3.7699361813640215e-07,
      "loss": 1.7715,
      "step": 70432
    },
    {
      "epoch": 0.0002557278405424264,
      "grad_norm": 9900.261713712422,
      "learning_rate": 3.769079195078859e-07,
      "loss": 1.7646,
      "step": 70464
    },
    {
      "epoch": 0.0002558439748932631,
      "grad_norm": 8903.908804564431,
      "learning_rate": 3.768222792961477e-07,
      "loss": 1.7551,
      "step": 70496
    },
    {
      "epoch": 0.0002559601092440998,
      "grad_norm": 11074.543602334139,
      "learning_rate": 3.767366974348509e-07,
      "loss": 1.7733,
      "step": 70528
    },
    {
      "epoch": 0.00025607624359493653,
      "grad_norm": 10796.584459911384,
      "learning_rate": 3.7665117385776453e-07,
      "loss": 1.7654,
      "step": 70560
    },
    {
      "epoch": 0.00025619237794577323,
      "grad_norm": 11258.099662021117,
      "learning_rate": 3.765657084987626e-07,
      "loss": 1.7656,
      "step": 70592
    },
    {
      "epoch": 0.00025630851229660993,
      "grad_norm": 8952.663402585848,
      "learning_rate": 3.764803012918241e-07,
      "loss": 1.7778,
      "step": 70624
    },
    {
      "epoch": 0.00025642464664744663,
      "grad_norm": 10022.522237441033,
      "learning_rate": 3.7639495217103307e-07,
      "loss": 1.7598,
      "step": 70656
    },
    {
      "epoch": 0.00025654078099828333,
      "grad_norm": 11611.927230223242,
      "learning_rate": 3.763096610705777e-07,
      "loss": 1.7824,
      "step": 70688
    },
    {
      "epoch": 0.00025665691534912003,
      "grad_norm": 8728.26546342399,
      "learning_rate": 3.7622442792475074e-07,
      "loss": 1.7905,
      "step": 70720
    },
    {
      "epoch": 0.00025677304969995673,
      "grad_norm": 8848.613450705145,
      "learning_rate": 3.761392526679492e-07,
      "loss": 1.7581,
      "step": 70752
    },
    {
      "epoch": 0.00025688918405079343,
      "grad_norm": 8978.715164209187,
      "learning_rate": 3.7605413523467376e-07,
      "loss": 1.7611,
      "step": 70784
    },
    {
      "epoch": 0.00025700531840163013,
      "grad_norm": 8464.65711059816,
      "learning_rate": 3.7596907555952887e-07,
      "loss": 1.749,
      "step": 70816
    },
    {
      "epoch": 0.0002571214527524669,
      "grad_norm": 9706.892293623125,
      "learning_rate": 3.758840735772226e-07,
      "loss": 1.763,
      "step": 70848
    },
    {
      "epoch": 0.0002572375871033036,
      "grad_norm": 8870.384320873589,
      "learning_rate": 3.757991292225662e-07,
      "loss": 1.768,
      "step": 70880
    },
    {
      "epoch": 0.0002573537214541403,
      "grad_norm": 10331.707022559243,
      "learning_rate": 3.75714242430474e-07,
      "loss": 1.7748,
      "step": 70912
    },
    {
      "epoch": 0.000257469855804977,
      "grad_norm": 9625.38871942323,
      "learning_rate": 3.7562941313596316e-07,
      "loss": 1.7776,
      "step": 70944
    },
    {
      "epoch": 0.0002575859901558137,
      "grad_norm": 10145.048644535915,
      "learning_rate": 3.7554728952613627e-07,
      "loss": 1.7823,
      "step": 70976
    },
    {
      "epoch": 0.0002577021245066504,
      "grad_norm": 9002.597069734933,
      "learning_rate": 3.754625732404823e-07,
      "loss": 1.8014,
      "step": 71008
    },
    {
      "epoch": 0.0002578182588574871,
      "grad_norm": 9346.647420332061,
      "learning_rate": 3.75377914260096e-07,
      "loss": 1.7988,
      "step": 71040
    },
    {
      "epoch": 0.0002579343932083238,
      "grad_norm": 9425.243020739572,
      "learning_rate": 3.7529331252040086e-07,
      "loss": 1.7919,
      "step": 71072
    },
    {
      "epoch": 0.0002580505275591605,
      "grad_norm": 10376.54296960216,
      "learning_rate": 3.75208767956922e-07,
      "loss": 1.7917,
      "step": 71104
    },
    {
      "epoch": 0.00025816666190999724,
      "grad_norm": 10391.355830689276,
      "learning_rate": 3.7512428050528644e-07,
      "loss": 1.8066,
      "step": 71136
    },
    {
      "epoch": 0.00025828279626083394,
      "grad_norm": 9536.284811183023,
      "learning_rate": 3.750398501012224e-07,
      "loss": 1.7916,
      "step": 71168
    },
    {
      "epoch": 0.00025839893061167064,
      "grad_norm": 8586.456545048137,
      "learning_rate": 3.7495547668055933e-07,
      "loss": 1.7769,
      "step": 71200
    },
    {
      "epoch": 0.00025851506496250734,
      "grad_norm": 10018.81889246432,
      "learning_rate": 3.748711601792278e-07,
      "loss": 1.7564,
      "step": 71232
    },
    {
      "epoch": 0.00025863119931334404,
      "grad_norm": 8749.298486164476,
      "learning_rate": 3.747869005332592e-07,
      "loss": 1.7597,
      "step": 71264
    },
    {
      "epoch": 0.00025874733366418074,
      "grad_norm": 10521.120282555465,
      "learning_rate": 3.747026976787853e-07,
      "loss": 1.7909,
      "step": 71296
    },
    {
      "epoch": 0.00025886346801501744,
      "grad_norm": 8457.721442563594,
      "learning_rate": 3.746185515520385e-07,
      "loss": 1.78,
      "step": 71328
    },
    {
      "epoch": 0.00025897960236585414,
      "grad_norm": 10023.694727993266,
      "learning_rate": 3.745344620893514e-07,
      "loss": 1.7594,
      "step": 71360
    },
    {
      "epoch": 0.00025909573671669084,
      "grad_norm": 10508.618938756892,
      "learning_rate": 3.7445042922715633e-07,
      "loss": 1.765,
      "step": 71392
    },
    {
      "epoch": 0.0002592118710675276,
      "grad_norm": 9394.018096640011,
      "learning_rate": 3.7436645290198567e-07,
      "loss": 1.7599,
      "step": 71424
    },
    {
      "epoch": 0.0002593280054183643,
      "grad_norm": 11239.643766596875,
      "learning_rate": 3.742825330504713e-07,
      "loss": 1.7668,
      "step": 71456
    },
    {
      "epoch": 0.000259444139769201,
      "grad_norm": 8442.958723101754,
      "learning_rate": 3.7419866960934454e-07,
      "loss": 1.752,
      "step": 71488
    },
    {
      "epoch": 0.0002595602741200377,
      "grad_norm": 9809.57843130886,
      "learning_rate": 3.7411486251543587e-07,
      "loss": 1.7492,
      "step": 71520
    },
    {
      "epoch": 0.0002596764084708744,
      "grad_norm": 9212.885975632174,
      "learning_rate": 3.7403111170567466e-07,
      "loss": 1.7561,
      "step": 71552
    },
    {
      "epoch": 0.0002597925428217111,
      "grad_norm": 9166.09600647953,
      "learning_rate": 3.7394741711708927e-07,
      "loss": 1.763,
      "step": 71584
    },
    {
      "epoch": 0.0002599086771725478,
      "grad_norm": 10244.126317065795,
      "learning_rate": 3.738637786868065e-07,
      "loss": 1.7879,
      "step": 71616
    },
    {
      "epoch": 0.0002600248115233845,
      "grad_norm": 10155.115065817816,
      "learning_rate": 3.737801963520516e-07,
      "loss": 1.7899,
      "step": 71648
    },
    {
      "epoch": 0.0002601409458742212,
      "grad_norm": 8574.436541254474,
      "learning_rate": 3.736966700501479e-07,
      "loss": 1.7809,
      "step": 71680
    },
    {
      "epoch": 0.00026025708022505795,
      "grad_norm": 8774.436734058774,
      "learning_rate": 3.73613199718517e-07,
      "loss": 1.7817,
      "step": 71712
    },
    {
      "epoch": 0.00026037321457589465,
      "grad_norm": 9403.207006122964,
      "learning_rate": 3.73529785294678e-07,
      "loss": 1.8003,
      "step": 71744
    },
    {
      "epoch": 0.00026048934892673135,
      "grad_norm": 8812.136630806403,
      "learning_rate": 3.734464267162477e-07,
      "loss": 1.7907,
      "step": 71776
    },
    {
      "epoch": 0.00026060548327756805,
      "grad_norm": 9880.12530284915,
      "learning_rate": 3.733631239209405e-07,
      "loss": 1.7766,
      "step": 71808
    },
    {
      "epoch": 0.00026072161762840475,
      "grad_norm": 10428.77135620491,
      "learning_rate": 3.7327987684656776e-07,
      "loss": 1.7825,
      "step": 71840
    },
    {
      "epoch": 0.00026083775197924145,
      "grad_norm": 9080.286228968776,
      "learning_rate": 3.73196685431038e-07,
      "loss": 1.7849,
      "step": 71872
    },
    {
      "epoch": 0.00026095388633007815,
      "grad_norm": 9540.02327041187,
      "learning_rate": 3.731135496123564e-07,
      "loss": 1.7876,
      "step": 71904
    },
    {
      "epoch": 0.00026107002068091485,
      "grad_norm": 9293.995158165299,
      "learning_rate": 3.7303046932862507e-07,
      "loss": 1.7915,
      "step": 71936
    },
    {
      "epoch": 0.00026118615503175155,
      "grad_norm": 8783.338772926842,
      "learning_rate": 3.729500382043046e-07,
      "loss": 1.7723,
      "step": 71968
    },
    {
      "epoch": 0.0002613022893825883,
      "grad_norm": 9421.7590714261,
      "learning_rate": 3.728670670744904e-07,
      "loss": 1.7493,
      "step": 72000
    },
    {
      "epoch": 0.000261418423733425,
      "grad_norm": 9602.212140960019,
      "learning_rate": 3.7278415129643374e-07,
      "loss": 1.7574,
      "step": 72032
    },
    {
      "epoch": 0.0002615345580842617,
      "grad_norm": 9660.343989734527,
      "learning_rate": 3.7270129080861817e-07,
      "loss": 1.7655,
      "step": 72064
    },
    {
      "epoch": 0.0002616506924350984,
      "grad_norm": 9802.87621058228,
      "learning_rate": 3.726184855496231e-07,
      "loss": 1.757,
      "step": 72096
    },
    {
      "epoch": 0.0002617668267859351,
      "grad_norm": 10049.770843158565,
      "learning_rate": 3.7253573545812297e-07,
      "loss": 1.7673,
      "step": 72128
    },
    {
      "epoch": 0.0002618829611367718,
      "grad_norm": 10415.763054140585,
      "learning_rate": 3.724530404728878e-07,
      "loss": 1.7825,
      "step": 72160
    },
    {
      "epoch": 0.0002619990954876085,
      "grad_norm": 10652.12805030056,
      "learning_rate": 3.723704005327827e-07,
      "loss": 1.7764,
      "step": 72192
    },
    {
      "epoch": 0.0002621152298384452,
      "grad_norm": 7916.612154198285,
      "learning_rate": 3.722878155767676e-07,
      "loss": 1.7885,
      "step": 72224
    },
    {
      "epoch": 0.0002622313641892819,
      "grad_norm": 10506.492468945095,
      "learning_rate": 3.7220528554389724e-07,
      "loss": 1.7734,
      "step": 72256
    },
    {
      "epoch": 0.00026234749854011865,
      "grad_norm": 10472.602923819848,
      "learning_rate": 3.721228103733208e-07,
      "loss": 1.7516,
      "step": 72288
    },
    {
      "epoch": 0.00026246363289095535,
      "grad_norm": 9847.810416534227,
      "learning_rate": 3.7204039000428194e-07,
      "loss": 1.7724,
      "step": 72320
    },
    {
      "epoch": 0.00026257976724179205,
      "grad_norm": 9568.167745185072,
      "learning_rate": 3.7195802437611824e-07,
      "loss": 1.7716,
      "step": 72352
    },
    {
      "epoch": 0.00026269590159262875,
      "grad_norm": 12299.241602635506,
      "learning_rate": 3.718757134282616e-07,
      "loss": 1.7768,
      "step": 72384
    },
    {
      "epoch": 0.00026281203594346545,
      "grad_norm": 10831.355778479441,
      "learning_rate": 3.7179345710023726e-07,
      "loss": 1.7776,
      "step": 72416
    },
    {
      "epoch": 0.00026292817029430215,
      "grad_norm": 9889.883214679534,
      "learning_rate": 3.717112553316644e-07,
      "loss": 1.797,
      "step": 72448
    },
    {
      "epoch": 0.00026304430464513885,
      "grad_norm": 9333.63476894184,
      "learning_rate": 3.716291080622555e-07,
      "loss": 1.7754,
      "step": 72480
    },
    {
      "epoch": 0.00026316043899597555,
      "grad_norm": 10080.960866901527,
      "learning_rate": 3.7154701523181633e-07,
      "loss": 1.7854,
      "step": 72512
    },
    {
      "epoch": 0.00026327657334681225,
      "grad_norm": 9417.88394492096,
      "learning_rate": 3.714649767802456e-07,
      "loss": 1.8062,
      "step": 72544
    },
    {
      "epoch": 0.000263392707697649,
      "grad_norm": 9793.177727377359,
      "learning_rate": 3.713829926475348e-07,
      "loss": 1.7779,
      "step": 72576
    },
    {
      "epoch": 0.0002635088420484857,
      "grad_norm": 9553.146811391522,
      "learning_rate": 3.713010627737682e-07,
      "loss": 1.7735,
      "step": 72608
    },
    {
      "epoch": 0.0002636249763993224,
      "grad_norm": 8794.117579382255,
      "learning_rate": 3.7121918709912273e-07,
      "loss": 1.7704,
      "step": 72640
    },
    {
      "epoch": 0.0002637411107501591,
      "grad_norm": 9021.229738788388,
      "learning_rate": 3.7113736556386724e-07,
      "loss": 1.7814,
      "step": 72672
    },
    {
      "epoch": 0.0002638572451009958,
      "grad_norm": 9334.113991161668,
      "learning_rate": 3.7105559810836284e-07,
      "loss": 1.7497,
      "step": 72704
    },
    {
      "epoch": 0.0002639733794518325,
      "grad_norm": 10866.643363983196,
      "learning_rate": 3.709738846730629e-07,
      "loss": 1.7623,
      "step": 72736
    },
    {
      "epoch": 0.0002640895138026692,
      "grad_norm": 10962.713715134589,
      "learning_rate": 3.70892225198512e-07,
      "loss": 1.7583,
      "step": 72768
    },
    {
      "epoch": 0.0002642056481535059,
      "grad_norm": 9029.947951123528,
      "learning_rate": 3.708106196253468e-07,
      "loss": 1.7697,
      "step": 72800
    },
    {
      "epoch": 0.0002643217825043426,
      "grad_norm": 9022.768200502549,
      "learning_rate": 3.707290678942949e-07,
      "loss": 1.785,
      "step": 72832
    },
    {
      "epoch": 0.0002644379168551793,
      "grad_norm": 10631.581349921564,
      "learning_rate": 3.706475699461755e-07,
      "loss": 1.7822,
      "step": 72864
    },
    {
      "epoch": 0.00026455405120601606,
      "grad_norm": 9564.885258067658,
      "learning_rate": 3.705661257218984e-07,
      "loss": 1.7811,
      "step": 72896
    },
    {
      "epoch": 0.00026467018555685276,
      "grad_norm": 8558.215818732313,
      "learning_rate": 3.704847351624647e-07,
      "loss": 1.7887,
      "step": 72928
    },
    {
      "epoch": 0.00026478631990768946,
      "grad_norm": 18136.32355247336,
      "learning_rate": 3.7040339820896597e-07,
      "loss": 1.7964,
      "step": 72960
    },
    {
      "epoch": 0.00026490245425852616,
      "grad_norm": 9095.1577226566,
      "learning_rate": 3.703246540990896e-07,
      "loss": 1.7735,
      "step": 72992
    },
    {
      "epoch": 0.00026501858860936286,
      "grad_norm": 9605.535487415576,
      "learning_rate": 3.7024342251047307e-07,
      "loss": 1.767,
      "step": 73024
    },
    {
      "epoch": 0.00026513472296019956,
      "grad_norm": 9754.842079705852,
      "learning_rate": 3.701622443534395e-07,
      "loss": 1.7559,
      "step": 73056
    },
    {
      "epoch": 0.00026525085731103626,
      "grad_norm": 10817.847290473275,
      "learning_rate": 3.700811195694384e-07,
      "loss": 1.7495,
      "step": 73088
    },
    {
      "epoch": 0.00026536699166187296,
      "grad_norm": 10076.818545552956,
      "learning_rate": 3.700000481000094e-07,
      "loss": 1.7666,
      "step": 73120
    },
    {
      "epoch": 0.00026548312601270966,
      "grad_norm": 9636.429629276603,
      "learning_rate": 3.6991902988678144e-07,
      "loss": 1.7915,
      "step": 73152
    },
    {
      "epoch": 0.0002655992603635464,
      "grad_norm": 10015.028507198569,
      "learning_rate": 3.698380648714729e-07,
      "loss": 1.7891,
      "step": 73184
    },
    {
      "epoch": 0.0002657153947143831,
      "grad_norm": 10092.676354664307,
      "learning_rate": 3.697571529958917e-07,
      "loss": 1.7671,
      "step": 73216
    },
    {
      "epoch": 0.0002658315290652198,
      "grad_norm": 9839.789123756667,
      "learning_rate": 3.696762942019345e-07,
      "loss": 1.7759,
      "step": 73248
    },
    {
      "epoch": 0.0002659476634160565,
      "grad_norm": 9751.513113358356,
      "learning_rate": 3.695954884315871e-07,
      "loss": 1.7892,
      "step": 73280
    },
    {
      "epoch": 0.0002660637977668932,
      "grad_norm": 9169.014014603752,
      "learning_rate": 3.6951473562692387e-07,
      "loss": 1.7958,
      "step": 73312
    },
    {
      "epoch": 0.0002661799321177299,
      "grad_norm": 9078.509789607544,
      "learning_rate": 3.6943403573010785e-07,
      "loss": 1.7666,
      "step": 73344
    },
    {
      "epoch": 0.0002662960664685666,
      "grad_norm": 8666.767217365423,
      "learning_rate": 3.693533886833904e-07,
      "loss": 1.7738,
      "step": 73376
    },
    {
      "epoch": 0.0002664122008194033,
      "grad_norm": 10040.04611543194,
      "learning_rate": 3.6927279442911107e-07,
      "loss": 1.7776,
      "step": 73408
    },
    {
      "epoch": 0.00026652833517024,
      "grad_norm": 8409.88668175737,
      "learning_rate": 3.6919225290969745e-07,
      "loss": 1.7846,
      "step": 73440
    },
    {
      "epoch": 0.00026664446952107677,
      "grad_norm": 8881.673941324349,
      "learning_rate": 3.691117640676651e-07,
      "loss": 1.7755,
      "step": 73472
    },
    {
      "epoch": 0.00026676060387191347,
      "grad_norm": 9271.890422130753,
      "learning_rate": 3.690313278456172e-07,
      "loss": 1.7661,
      "step": 73504
    },
    {
      "epoch": 0.00026687673822275017,
      "grad_norm": 10247.099296874214,
      "learning_rate": 3.689509441862443e-07,
      "loss": 1.7723,
      "step": 73536
    },
    {
      "epoch": 0.00026699287257358687,
      "grad_norm": 10230.189538811097,
      "learning_rate": 3.688706130323245e-07,
      "loss": 1.7651,
      "step": 73568
    },
    {
      "epoch": 0.00026710900692442357,
      "grad_norm": 9262.652967697753,
      "learning_rate": 3.68790334326723e-07,
      "loss": 1.765,
      "step": 73600
    },
    {
      "epoch": 0.00026722514127526027,
      "grad_norm": 7946.191792299001,
      "learning_rate": 3.6871010801239203e-07,
      "loss": 1.7555,
      "step": 73632
    },
    {
      "epoch": 0.00026734127562609697,
      "grad_norm": 9239.45117417696,
      "learning_rate": 3.686299340323706e-07,
      "loss": 1.7706,
      "step": 73664
    },
    {
      "epoch": 0.00026745740997693367,
      "grad_norm": 9470.73312896103,
      "learning_rate": 3.685498123297844e-07,
      "loss": 1.7772,
      "step": 73696
    },
    {
      "epoch": 0.00026757354432777037,
      "grad_norm": 10147.593902004553,
      "learning_rate": 3.684697428478455e-07,
      "loss": 1.7911,
      "step": 73728
    },
    {
      "epoch": 0.0002676896786786071,
      "grad_norm": 10264.012081052904,
      "learning_rate": 3.683897255298526e-07,
      "loss": 1.7767,
      "step": 73760
    },
    {
      "epoch": 0.0002678058130294438,
      "grad_norm": 9814.627043347087,
      "learning_rate": 3.683097603191904e-07,
      "loss": 1.7705,
      "step": 73792
    },
    {
      "epoch": 0.0002679219473802805,
      "grad_norm": 9361.061691923624,
      "learning_rate": 3.682298471593294e-07,
      "loss": 1.7551,
      "step": 73824
    },
    {
      "epoch": 0.0002680380817311172,
      "grad_norm": 10018.990767537416,
      "learning_rate": 3.681499859938261e-07,
      "loss": 1.7514,
      "step": 73856
    },
    {
      "epoch": 0.0002681542160819539,
      "grad_norm": 10388.253558707547,
      "learning_rate": 3.680701767663227e-07,
      "loss": 1.8008,
      "step": 73888
    },
    {
      "epoch": 0.0002682703504327906,
      "grad_norm": 8582.097762202433,
      "learning_rate": 3.679904194205468e-07,
      "loss": 1.7626,
      "step": 73920
    },
    {
      "epoch": 0.0002683864847836273,
      "grad_norm": 9763.19097426656,
      "learning_rate": 3.6791071390031143e-07,
      "loss": 1.7754,
      "step": 73952
    },
    {
      "epoch": 0.000268502619134464,
      "grad_norm": 10239.472056702924,
      "learning_rate": 3.678335485461642e-07,
      "loss": 1.7785,
      "step": 73984
    },
    {
      "epoch": 0.0002686187534853007,
      "grad_norm": 9229.556002322106,
      "learning_rate": 3.6775394489359144e-07,
      "loss": 1.7801,
      "step": 74016
    },
    {
      "epoch": 0.0002687348878361375,
      "grad_norm": 9830.602931661922,
      "learning_rate": 3.676743929002549e-07,
      "loss": 1.7973,
      "step": 74048
    },
    {
      "epoch": 0.0002688510221869742,
      "grad_norm": 10669.447970724634,
      "learning_rate": 3.675948925103043e-07,
      "loss": 1.8096,
      "step": 74080
    },
    {
      "epoch": 0.00026896715653781087,
      "grad_norm": 9383.164498185033,
      "learning_rate": 3.675154436679742e-07,
      "loss": 1.7996,
      "step": 74112
    },
    {
      "epoch": 0.00026908329088864757,
      "grad_norm": 9597.682011819312,
      "learning_rate": 3.674360463175832e-07,
      "loss": 1.7898,
      "step": 74144
    },
    {
      "epoch": 0.00026919942523948427,
      "grad_norm": 9119.548344079327,
      "learning_rate": 3.673567004035343e-07,
      "loss": 1.7787,
      "step": 74176
    },
    {
      "epoch": 0.00026931555959032097,
      "grad_norm": 10309.409197427367,
      "learning_rate": 3.6727740587031434e-07,
      "loss": 1.7635,
      "step": 74208
    },
    {
      "epoch": 0.00026943169394115767,
      "grad_norm": 9209.207674930563,
      "learning_rate": 3.671981626624942e-07,
      "loss": 1.7448,
      "step": 74240
    },
    {
      "epoch": 0.00026954782829199437,
      "grad_norm": 8842.008595336243,
      "learning_rate": 3.6711897072472816e-07,
      "loss": 1.7417,
      "step": 74272
    },
    {
      "epoch": 0.00026966396264283107,
      "grad_norm": 10301.755869753466,
      "learning_rate": 3.6703983000175426e-07,
      "loss": 1.7672,
      "step": 74304
    },
    {
      "epoch": 0.0002697800969936678,
      "grad_norm": 10008.863871589023,
      "learning_rate": 3.6696074043839385e-07,
      "loss": 1.7485,
      "step": 74336
    },
    {
      "epoch": 0.0002698962313445045,
      "grad_norm": 9800.48917146486,
      "learning_rate": 3.668817019795513e-07,
      "loss": 1.7576,
      "step": 74368
    },
    {
      "epoch": 0.0002700123656953412,
      "grad_norm": 11690.62461975407,
      "learning_rate": 3.668027145702142e-07,
      "loss": 1.7683,
      "step": 74400
    },
    {
      "epoch": 0.0002701285000461779,
      "grad_norm": 9522.34928995991,
      "learning_rate": 3.6672377815545303e-07,
      "loss": 1.7691,
      "step": 74432
    },
    {
      "epoch": 0.0002702446343970146,
      "grad_norm": 9913.7207949387,
      "learning_rate": 3.6664489268042084e-07,
      "loss": 1.7771,
      "step": 74464
    },
    {
      "epoch": 0.0002703607687478513,
      "grad_norm": 10489.059919744954,
      "learning_rate": 3.6656605809035324e-07,
      "loss": 1.7874,
      "step": 74496
    },
    {
      "epoch": 0.000270476903098688,
      "grad_norm": 9602.030826861575,
      "learning_rate": 3.664872743305685e-07,
      "loss": 1.7512,
      "step": 74528
    },
    {
      "epoch": 0.0002705930374495247,
      "grad_norm": 9742.631779965823,
      "learning_rate": 3.6640854134646656e-07,
      "loss": 1.7584,
      "step": 74560
    },
    {
      "epoch": 0.0002707091718003614,
      "grad_norm": 9673.924849821813,
      "learning_rate": 3.663298590835301e-07,
      "loss": 1.765,
      "step": 74592
    },
    {
      "epoch": 0.0002708253061511982,
      "grad_norm": 10413.446979746906,
      "learning_rate": 3.6625122748732325e-07,
      "loss": 1.7627,
      "step": 74624
    },
    {
      "epoch": 0.0002709414405020349,
      "grad_norm": 10270.179550523933,
      "learning_rate": 3.66172646503492e-07,
      "loss": 1.795,
      "step": 74656
    },
    {
      "epoch": 0.0002710575748528716,
      "grad_norm": 11269.843299709184,
      "learning_rate": 3.660941160777641e-07,
      "loss": 1.8204,
      "step": 74688
    },
    {
      "epoch": 0.0002711737092037083,
      "grad_norm": 9827.866401208352,
      "learning_rate": 3.660156361559486e-07,
      "loss": 1.8177,
      "step": 74720
    },
    {
      "epoch": 0.000271289843554545,
      "grad_norm": 9456.132190277376,
      "learning_rate": 3.659372066839358e-07,
      "loss": 1.7916,
      "step": 74752
    },
    {
      "epoch": 0.0002714059779053817,
      "grad_norm": 10007.073798069043,
      "learning_rate": 3.6585882760769707e-07,
      "loss": 1.7674,
      "step": 74784
    },
    {
      "epoch": 0.0002715221122562184,
      "grad_norm": 10377.625643662426,
      "learning_rate": 3.6578049887328514e-07,
      "loss": 1.7833,
      "step": 74816
    },
    {
      "epoch": 0.0002716382466070551,
      "grad_norm": 10459.220143012577,
      "learning_rate": 3.657022204268331e-07,
      "loss": 1.7684,
      "step": 74848
    },
    {
      "epoch": 0.0002717543809578918,
      "grad_norm": 9336.827405494867,
      "learning_rate": 3.6562399221455486e-07,
      "loss": 1.774,
      "step": 74880
    },
    {
      "epoch": 0.00027187051530872853,
      "grad_norm": 9429.290535347822,
      "learning_rate": 3.65545814182745e-07,
      "loss": 1.7806,
      "step": 74912
    },
    {
      "epoch": 0.00027198664965956523,
      "grad_norm": 9543.936399620441,
      "learning_rate": 3.654676862777782e-07,
      "loss": 1.7636,
      "step": 74944
    },
    {
      "epoch": 0.00027210278401040193,
      "grad_norm": 10258.732475310973,
      "learning_rate": 3.6539204762093485e-07,
      "loss": 1.7616,
      "step": 74976
    },
    {
      "epoch": 0.00027221891836123863,
      "grad_norm": 9661.863484856323,
      "learning_rate": 3.6531401824678776e-07,
      "loss": 1.7773,
      "step": 75008
    },
    {
      "epoch": 0.00027233505271207533,
      "grad_norm": 9824.04804548512,
      "learning_rate": 3.652360388407557e-07,
      "loss": 1.7556,
      "step": 75040
    },
    {
      "epoch": 0.00027245118706291203,
      "grad_norm": 10295.992812740304,
      "learning_rate": 3.651581093495309e-07,
      "loss": 1.7523,
      "step": 75072
    },
    {
      "epoch": 0.00027256732141374873,
      "grad_norm": 8403.087884819484,
      "learning_rate": 3.650802297198849e-07,
      "loss": 1.7594,
      "step": 75104
    },
    {
      "epoch": 0.00027268345576458543,
      "grad_norm": 9316.89969893419,
      "learning_rate": 3.6500239989866905e-07,
      "loss": 1.7468,
      "step": 75136
    },
    {
      "epoch": 0.00027279959011542213,
      "grad_norm": 9029.715610139669,
      "learning_rate": 3.649246198328137e-07,
      "loss": 1.7567,
      "step": 75168
    },
    {
      "epoch": 0.0002729157244662589,
      "grad_norm": 9092.416400495524,
      "learning_rate": 3.6484688946932844e-07,
      "loss": 1.767,
      "step": 75200
    },
    {
      "epoch": 0.0002730318588170956,
      "grad_norm": 9900.813400928228,
      "learning_rate": 3.647692087553018e-07,
      "loss": 1.7721,
      "step": 75232
    },
    {
      "epoch": 0.0002731479931679323,
      "grad_norm": 10548.017823268976,
      "learning_rate": 3.646915776379011e-07,
      "loss": 1.7912,
      "step": 75264
    },
    {
      "epoch": 0.000273264127518769,
      "grad_norm": 9171.040508033971,
      "learning_rate": 3.6461399606437257e-07,
      "loss": 1.7919,
      "step": 75296
    },
    {
      "epoch": 0.0002733802618696057,
      "grad_norm": 11219.526371465063,
      "learning_rate": 3.6453646398204067e-07,
      "loss": 1.7764,
      "step": 75328
    },
    {
      "epoch": 0.0002734963962204424,
      "grad_norm": 10703.803062463361,
      "learning_rate": 3.644589813383083e-07,
      "loss": 1.7518,
      "step": 75360
    },
    {
      "epoch": 0.0002736125305712791,
      "grad_norm": 8763.703440897576,
      "learning_rate": 3.6438154808065674e-07,
      "loss": 1.7604,
      "step": 75392
    },
    {
      "epoch": 0.0002737286649221158,
      "grad_norm": 8852.840674043558,
      "learning_rate": 3.6430416415664515e-07,
      "loss": 1.788,
      "step": 75424
    },
    {
      "epoch": 0.0002738447992729525,
      "grad_norm": 10731.8809162234,
      "learning_rate": 3.6422682951391073e-07,
      "loss": 1.7876,
      "step": 75456
    },
    {
      "epoch": 0.00027396093362378924,
      "grad_norm": 9409.243965377877,
      "learning_rate": 3.641495441001684e-07,
      "loss": 1.7931,
      "step": 75488
    },
    {
      "epoch": 0.00027407706797462594,
      "grad_norm": 10437.018635606626,
      "learning_rate": 3.6407230786321075e-07,
      "loss": 1.7788,
      "step": 75520
    },
    {
      "epoch": 0.00027419320232546264,
      "grad_norm": 9048.252317436776,
      "learning_rate": 3.6399512075090785e-07,
      "loss": 1.7805,
      "step": 75552
    },
    {
      "epoch": 0.00027430933667629934,
      "grad_norm": 9302.023435790732,
      "learning_rate": 3.6391798271120706e-07,
      "loss": 1.8007,
      "step": 75584
    },
    {
      "epoch": 0.00027442547102713604,
      "grad_norm": 9581.172057739075,
      "learning_rate": 3.638408936921329e-07,
      "loss": 1.8046,
      "step": 75616
    },
    {
      "epoch": 0.00027454160537797274,
      "grad_norm": 8441.603520658855,
      "learning_rate": 3.637638536417871e-07,
      "loss": 1.7744,
      "step": 75648
    },
    {
      "epoch": 0.00027465773972880944,
      "grad_norm": 9434.655478606517,
      "learning_rate": 3.636868625083481e-07,
      "loss": 1.7743,
      "step": 75680
    },
    {
      "epoch": 0.00027477387407964614,
      "grad_norm": 9834.040166686325,
      "learning_rate": 3.6360992024007104e-07,
      "loss": 1.77,
      "step": 75712
    },
    {
      "epoch": 0.00027489000843048284,
      "grad_norm": 9228.162005513339,
      "learning_rate": 3.63533026785288e-07,
      "loss": 1.7465,
      "step": 75744
    },
    {
      "epoch": 0.0002750061427813196,
      "grad_norm": 9608.504358119426,
      "learning_rate": 3.6345618209240703e-07,
      "loss": 1.7579,
      "step": 75776
    },
    {
      "epoch": 0.0002751222771321563,
      "grad_norm": 11758.774425934023,
      "learning_rate": 3.633793861099128e-07,
      "loss": 1.7541,
      "step": 75808
    },
    {
      "epoch": 0.000275238411482993,
      "grad_norm": 10294.827633331215,
      "learning_rate": 3.633026387863661e-07,
      "loss": 1.7743,
      "step": 75840
    },
    {
      "epoch": 0.0002753545458338297,
      "grad_norm": 10583.827474028476,
      "learning_rate": 3.6322594007040373e-07,
      "loss": 1.7917,
      "step": 75872
    },
    {
      "epoch": 0.0002754706801846664,
      "grad_norm": 9956.418834098935,
      "learning_rate": 3.631492899107383e-07,
      "loss": 1.7841,
      "step": 75904
    },
    {
      "epoch": 0.0002755868145355031,
      "grad_norm": 10254.613400806487,
      "learning_rate": 3.6307268825615816e-07,
      "loss": 1.7639,
      "step": 75936
    },
    {
      "epoch": 0.0002757029488863398,
      "grad_norm": 18446.072102211896,
      "learning_rate": 3.629961350555273e-07,
      "loss": 1.7607,
      "step": 75968
    },
    {
      "epoch": 0.0002758190832371765,
      "grad_norm": 9382.039117377415,
      "learning_rate": 3.629220203005604e-07,
      "loss": 1.7575,
      "step": 76000
    },
    {
      "epoch": 0.0002759352175880132,
      "grad_norm": 10085.119731564915,
      "learning_rate": 3.628455623444957e-07,
      "loss": 1.7718,
      "step": 76032
    },
    {
      "epoch": 0.00027605135193884995,
      "grad_norm": 9593.931936385623,
      "learning_rate": 3.6276915269101404e-07,
      "loss": 1.7444,
      "step": 76064
    },
    {
      "epoch": 0.00027616748628968665,
      "grad_norm": 9848.22095609151,
      "learning_rate": 3.6269279128927804e-07,
      "loss": 1.7512,
      "step": 76096
    },
    {
      "epoch": 0.00027628362064052335,
      "grad_norm": 9672.809416090033,
      "learning_rate": 3.62616478088525e-07,
      "loss": 1.753,
      "step": 76128
    },
    {
      "epoch": 0.00027639975499136005,
      "grad_norm": 9732.37237265406,
      "learning_rate": 3.625402130380671e-07,
      "loss": 1.7618,
      "step": 76160
    },
    {
      "epoch": 0.00027651588934219675,
      "grad_norm": 9301.615773616968,
      "learning_rate": 3.6246399608729105e-07,
      "loss": 1.7909,
      "step": 76192
    },
    {
      "epoch": 0.00027663202369303345,
      "grad_norm": 9109.785068814741,
      "learning_rate": 3.623878271856582e-07,
      "loss": 1.8004,
      "step": 76224
    },
    {
      "epoch": 0.00027674815804387015,
      "grad_norm": 9426.302350338652,
      "learning_rate": 3.623117062827038e-07,
      "loss": 1.7904,
      "step": 76256
    },
    {
      "epoch": 0.00027686429239470685,
      "grad_norm": 10128.616884846617,
      "learning_rate": 3.6223563332803774e-07,
      "loss": 1.7815,
      "step": 76288
    },
    {
      "epoch": 0.00027698042674554355,
      "grad_norm": 9335.703937036564,
      "learning_rate": 3.621596082713435e-07,
      "loss": 1.7858,
      "step": 76320
    },
    {
      "epoch": 0.0002770965610963803,
      "grad_norm": 10956.283402687244,
      "learning_rate": 3.6208363106237896e-07,
      "loss": 1.7895,
      "step": 76352
    },
    {
      "epoch": 0.000277212695447217,
      "grad_norm": 8835.56132908374,
      "learning_rate": 3.620077016509752e-07,
      "loss": 1.7892,
      "step": 76384
    },
    {
      "epoch": 0.0002773288297980537,
      "grad_norm": 8587.031151684498,
      "learning_rate": 3.6193181998703727e-07,
      "loss": 1.7937,
      "step": 76416
    },
    {
      "epoch": 0.0002774449641488904,
      "grad_norm": 9192.342574121136,
      "learning_rate": 3.618559860205436e-07,
      "loss": 1.8091,
      "step": 76448
    },
    {
      "epoch": 0.0002775610984997271,
      "grad_norm": 10133.055807603154,
      "learning_rate": 3.6178019970154597e-07,
      "loss": 1.7938,
      "step": 76480
    },
    {
      "epoch": 0.0002776772328505638,
      "grad_norm": 10565.57901868137,
      "learning_rate": 3.617044609801693e-07,
      "loss": 1.7626,
      "step": 76512
    },
    {
      "epoch": 0.0002777933672014005,
      "grad_norm": 8322.006729148925,
      "learning_rate": 3.616287698066116e-07,
      "loss": 1.7594,
      "step": 76544
    },
    {
      "epoch": 0.0002779095015522372,
      "grad_norm": 9649.117265325362,
      "learning_rate": 3.6155312613114385e-07,
      "loss": 1.7419,
      "step": 76576
    },
    {
      "epoch": 0.0002780256359030739,
      "grad_norm": 11703.13325567132,
      "learning_rate": 3.614775299041098e-07,
      "loss": 1.7515,
      "step": 76608
    },
    {
      "epoch": 0.00027814177025391065,
      "grad_norm": 8772.365701451348,
      "learning_rate": 3.614019810759257e-07,
      "loss": 1.7595,
      "step": 76640
    },
    {
      "epoch": 0.00027825790460474735,
      "grad_norm": 10431.486471256145,
      "learning_rate": 3.6132647959708057e-07,
      "loss": 1.7502,
      "step": 76672
    },
    {
      "epoch": 0.00027837403895558405,
      "grad_norm": 10546.927514684076,
      "learning_rate": 3.612510254181356e-07,
      "loss": 1.7596,
      "step": 76704
    },
    {
      "epoch": 0.00027849017330642075,
      "grad_norm": 8467.666856932907,
      "learning_rate": 3.611756184897242e-07,
      "loss": 1.7808,
      "step": 76736
    },
    {
      "epoch": 0.00027860630765725745,
      "grad_norm": 10205.60591047881,
      "learning_rate": 3.611002587625521e-07,
      "loss": 1.767,
      "step": 76768
    },
    {
      "epoch": 0.00027872244200809415,
      "grad_norm": 10497.915221604717,
      "learning_rate": 3.610249461873968e-07,
      "loss": 1.7552,
      "step": 76800
    },
    {
      "epoch": 0.00027883857635893085,
      "grad_norm": 8677.568438220467,
      "learning_rate": 3.609496807151078e-07,
      "loss": 1.7668,
      "step": 76832
    },
    {
      "epoch": 0.00027895471070976755,
      "grad_norm": 9249.959999913513,
      "learning_rate": 3.60874462296606e-07,
      "loss": 1.7574,
      "step": 76864
    },
    {
      "epoch": 0.00027907084506060425,
      "grad_norm": 10430.484648375646,
      "learning_rate": 3.6079929088288417e-07,
      "loss": 1.7403,
      "step": 76896
    },
    {
      "epoch": 0.000279186979411441,
      "grad_norm": 8842.354550683885,
      "learning_rate": 3.607241664250064e-07,
      "loss": 1.7617,
      "step": 76928
    },
    {
      "epoch": 0.0002793031137622777,
      "grad_norm": 8491.843616082435,
      "learning_rate": 3.6064908887410805e-07,
      "loss": 1.7844,
      "step": 76960
    },
    {
      "epoch": 0.0002794192481131144,
      "grad_norm": 10147.926684796259,
      "learning_rate": 3.6057640218174795e-07,
      "loss": 1.7879,
      "step": 76992
    },
    {
      "epoch": 0.0002795353824639511,
      "grad_norm": 8414.552156829262,
      "learning_rate": 3.6050141683644015e-07,
      "loss": 1.7988,
      "step": 77024
    },
    {
      "epoch": 0.0002796515168147878,
      "grad_norm": 9223.166918146933,
      "learning_rate": 3.604264782534637e-07,
      "loss": 1.8044,
      "step": 77056
    },
    {
      "epoch": 0.0002797676511656245,
      "grad_norm": 10877.903290616257,
      "learning_rate": 3.603515863842355e-07,
      "loss": 1.7919,
      "step": 77088
    },
    {
      "epoch": 0.0002798837855164612,
      "grad_norm": 10113.616563821273,
      "learning_rate": 3.6027674118024324e-07,
      "loss": 1.7908,
      "step": 77120
    },
    {
      "epoch": 0.0002799999198672979,
      "grad_norm": 10557.983424878068,
      "learning_rate": 3.602019425930452e-07,
      "loss": 1.8026,
      "step": 77152
    },
    {
      "epoch": 0.0002801160542181346,
      "grad_norm": 8467.364879347057,
      "learning_rate": 3.6012719057426976e-07,
      "loss": 1.7829,
      "step": 77184
    },
    {
      "epoch": 0.00028023218856897136,
      "grad_norm": 8534.296807587605,
      "learning_rate": 3.6005248507561576e-07,
      "loss": 1.779,
      "step": 77216
    },
    {
      "epoch": 0.00028034832291980806,
      "grad_norm": 9209.594127864702,
      "learning_rate": 3.59977826048852e-07,
      "loss": 1.7788,
      "step": 77248
    },
    {
      "epoch": 0.00028046445727064476,
      "grad_norm": 8891.087222606693,
      "learning_rate": 3.599032134458175e-07,
      "loss": 1.7564,
      "step": 77280
    },
    {
      "epoch": 0.00028058059162148146,
      "grad_norm": 9401.959902062974,
      "learning_rate": 3.598286472184208e-07,
      "loss": 1.7666,
      "step": 77312
    },
    {
      "epoch": 0.00028069672597231816,
      "grad_norm": 8510.70666866154,
      "learning_rate": 3.597541273186405e-07,
      "loss": 1.7508,
      "step": 77344
    },
    {
      "epoch": 0.00028081286032315486,
      "grad_norm": 9373.906122849748,
      "learning_rate": 3.5967965369852437e-07,
      "loss": 1.754,
      "step": 77376
    },
    {
      "epoch": 0.00028092899467399156,
      "grad_norm": 8943.878465185,
      "learning_rate": 3.596052263101901e-07,
      "loss": 1.7471,
      "step": 77408
    },
    {
      "epoch": 0.00028104512902482826,
      "grad_norm": 9206.606540957424,
      "learning_rate": 3.5953084510582426e-07,
      "loss": 1.7656,
      "step": 77440
    },
    {
      "epoch": 0.00028116126337566496,
      "grad_norm": 8598.992615417226,
      "learning_rate": 3.594565100376831e-07,
      "loss": 1.7728,
      "step": 77472
    },
    {
      "epoch": 0.0002812773977265017,
      "grad_norm": 10207.398591218038,
      "learning_rate": 3.593822210580915e-07,
      "loss": 1.7546,
      "step": 77504
    },
    {
      "epoch": 0.0002813935320773384,
      "grad_norm": 9845.038547410568,
      "learning_rate": 3.593079781194435e-07,
      "loss": 1.7733,
      "step": 77536
    },
    {
      "epoch": 0.0002815096664281751,
      "grad_norm": 9805.236968069667,
      "learning_rate": 3.59233781174202e-07,
      "loss": 1.7664,
      "step": 77568
    },
    {
      "epoch": 0.0002816258007790118,
      "grad_norm": 10135.367975559644,
      "learning_rate": 3.5915963017489846e-07,
      "loss": 1.7814,
      "step": 77600
    },
    {
      "epoch": 0.0002817419351298485,
      "grad_norm": 11228.920607075284,
      "learning_rate": 3.590855250741329e-07,
      "loss": 1.7646,
      "step": 77632
    },
    {
      "epoch": 0.0002818580694806852,
      "grad_norm": 9321.048653450962,
      "learning_rate": 3.590114658245738e-07,
      "loss": 1.7661,
      "step": 77664
    },
    {
      "epoch": 0.0002819742038315219,
      "grad_norm": 9717.186012421498,
      "learning_rate": 3.589374523789582e-07,
      "loss": 1.7604,
      "step": 77696
    },
    {
      "epoch": 0.0002820903381823586,
      "grad_norm": 10474.013270948248,
      "learning_rate": 3.588634846900908e-07,
      "loss": 1.7802,
      "step": 77728
    },
    {
      "epoch": 0.0002822064725331953,
      "grad_norm": 8277.770714389231,
      "learning_rate": 3.5878956271084476e-07,
      "loss": 1.7866,
      "step": 77760
    },
    {
      "epoch": 0.00028232260688403207,
      "grad_norm": 10569.537549013203,
      "learning_rate": 3.5871568639416106e-07,
      "loss": 1.7733,
      "step": 77792
    },
    {
      "epoch": 0.00028243874123486877,
      "grad_norm": 9341.305797371158,
      "learning_rate": 3.586418556930484e-07,
      "loss": 1.7793,
      "step": 77824
    },
    {
      "epoch": 0.00028255487558570547,
      "grad_norm": 10127.277719110896,
      "learning_rate": 3.5856807056058324e-07,
      "loss": 1.7804,
      "step": 77856
    },
    {
      "epoch": 0.00028267100993654217,
      "grad_norm": 8867.021596906145,
      "learning_rate": 3.5849433094990956e-07,
      "loss": 1.7837,
      "step": 77888
    },
    {
      "epoch": 0.00028278714428737887,
      "grad_norm": 9873.831475167075,
      "learning_rate": 3.584206368142387e-07,
      "loss": 1.7808,
      "step": 77920
    },
    {
      "epoch": 0.00028290327863821557,
      "grad_norm": 9807.419028470233,
      "learning_rate": 3.583469881068495e-07,
      "loss": 1.7829,
      "step": 77952
    },
    {
      "epoch": 0.00028301941298905227,
      "grad_norm": 10628.954605228117,
      "learning_rate": 3.5827568419855185e-07,
      "loss": 1.78,
      "step": 77984
    },
    {
      "epoch": 0.00028313554733988897,
      "grad_norm": 9329.88681603373,
      "learning_rate": 3.5820212479181497e-07,
      "loss": 1.7559,
      "step": 78016
    },
    {
      "epoch": 0.00028325168169072567,
      "grad_norm": 8761.70862332228,
      "learning_rate": 3.581286106750512e-07,
      "loss": 1.771,
      "step": 78048
    },
    {
      "epoch": 0.0002833678160415624,
      "grad_norm": 9351.362681449158,
      "learning_rate": 3.5805514180180496e-07,
      "loss": 1.7677,
      "step": 78080
    },
    {
      "epoch": 0.0002834839503923991,
      "grad_norm": 9399.970425485391,
      "learning_rate": 3.5798171812568753e-07,
      "loss": 1.7647,
      "step": 78112
    },
    {
      "epoch": 0.0002836000847432358,
      "grad_norm": 9074.877630028957,
      "learning_rate": 3.5790833960037676e-07,
      "loss": 1.7696,
      "step": 78144
    },
    {
      "epoch": 0.0002837162190940725,
      "grad_norm": 9514.936888913136,
      "learning_rate": 3.578350061796169e-07,
      "loss": 1.7847,
      "step": 78176
    },
    {
      "epoch": 0.0002838323534449092,
      "grad_norm": 7475.219194110631,
      "learning_rate": 3.5776171781721864e-07,
      "loss": 1.7703,
      "step": 78208
    },
    {
      "epoch": 0.0002839484877957459,
      "grad_norm": 10630.319092106314,
      "learning_rate": 3.576884744670585e-07,
      "loss": 1.7788,
      "step": 78240
    },
    {
      "epoch": 0.0002840646221465826,
      "grad_norm": 9560.69610436395,
      "learning_rate": 3.576152760830795e-07,
      "loss": 1.7643,
      "step": 78272
    },
    {
      "epoch": 0.0002841807564974193,
      "grad_norm": 9053.92202307928,
      "learning_rate": 3.5754212261929044e-07,
      "loss": 1.7546,
      "step": 78304
    },
    {
      "epoch": 0.000284296890848256,
      "grad_norm": 10681.939337030519,
      "learning_rate": 3.574690140297658e-07,
      "loss": 1.7566,
      "step": 78336
    },
    {
      "epoch": 0.0002844130251990927,
      "grad_norm": 8929.638514520058,
      "learning_rate": 3.5739595026864606e-07,
      "loss": 1.7553,
      "step": 78368
    },
    {
      "epoch": 0.0002845291595499295,
      "grad_norm": 8611.253451153321,
      "learning_rate": 3.57322931290137e-07,
      "loss": 1.7555,
      "step": 78400
    },
    {
      "epoch": 0.0002846452939007662,
      "grad_norm": 9193.164199556102,
      "learning_rate": 3.572499570485101e-07,
      "loss": 1.767,
      "step": 78432
    },
    {
      "epoch": 0.0002847614282516029,
      "grad_norm": 10400.148460478822,
      "learning_rate": 3.57177027498102e-07,
      "loss": 1.7794,
      "step": 78464
    },
    {
      "epoch": 0.0002848775626024396,
      "grad_norm": 10066.48478864395,
      "learning_rate": 3.571041425933147e-07,
      "loss": 1.7712,
      "step": 78496
    },
    {
      "epoch": 0.0002849936969532763,
      "grad_norm": 8404.871563563598,
      "learning_rate": 3.5703130228861533e-07,
      "loss": 1.7723,
      "step": 78528
    },
    {
      "epoch": 0.000285109831304113,
      "grad_norm": 9815.000356597038,
      "learning_rate": 3.5695850653853573e-07,
      "loss": 1.7718,
      "step": 78560
    },
    {
      "epoch": 0.0002852259656549497,
      "grad_norm": 9108.241872062907,
      "learning_rate": 3.5688575529767297e-07,
      "loss": 1.7721,
      "step": 78592
    },
    {
      "epoch": 0.0002853421000057864,
      "grad_norm": 9504.558958731332,
      "learning_rate": 3.5681304852068864e-07,
      "loss": 1.7695,
      "step": 78624
    },
    {
      "epoch": 0.0002854582343566231,
      "grad_norm": 9054.33321675318,
      "learning_rate": 3.5674038616230906e-07,
      "loss": 1.7861,
      "step": 78656
    },
    {
      "epoch": 0.00028557436870745983,
      "grad_norm": 8925.995070578965,
      "learning_rate": 3.56667768177325e-07,
      "loss": 1.7922,
      "step": 78688
    },
    {
      "epoch": 0.00028569050305829653,
      "grad_norm": 9602.91018389738,
      "learning_rate": 3.565951945205917e-07,
      "loss": 1.7834,
      "step": 78720
    },
    {
      "epoch": 0.00028580663740913323,
      "grad_norm": 9689.684411785556,
      "learning_rate": 3.5652266514702856e-07,
      "loss": 1.782,
      "step": 78752
    },
    {
      "epoch": 0.00028592277175996993,
      "grad_norm": 8681.614941933327,
      "learning_rate": 3.564501800116192e-07,
      "loss": 1.7743,
      "step": 78784
    },
    {
      "epoch": 0.00028603890611080663,
      "grad_norm": 8332.592993780507,
      "learning_rate": 3.563777390694114e-07,
      "loss": 1.7734,
      "step": 78816
    },
    {
      "epoch": 0.00028615504046164333,
      "grad_norm": 8995.86371617534,
      "learning_rate": 3.563053422755166e-07,
      "loss": 1.7668,
      "step": 78848
    },
    {
      "epoch": 0.00028627117481248,
      "grad_norm": 9129.677102723841,
      "learning_rate": 3.562329895851103e-07,
      "loss": 1.7602,
      "step": 78880
    },
    {
      "epoch": 0.0002863873091633167,
      "grad_norm": 10489.424388401874,
      "learning_rate": 3.561606809534316e-07,
      "loss": 1.7549,
      "step": 78912
    },
    {
      "epoch": 0.0002865034435141534,
      "grad_norm": 9079.07330072844,
      "learning_rate": 3.5608841633578306e-07,
      "loss": 1.761,
      "step": 78944
    },
    {
      "epoch": 0.0002866195778649902,
      "grad_norm": 9799.963061154875,
      "learning_rate": 3.5601619568753087e-07,
      "loss": 1.7872,
      "step": 78976
    },
    {
      "epoch": 0.0002867357122158269,
      "grad_norm": 9798.764309850503,
      "learning_rate": 3.5594627382227607e-07,
      "loss": 1.7982,
      "step": 79008
    },
    {
      "epoch": 0.0002868518465666636,
      "grad_norm": 11093.884531578648,
      "learning_rate": 3.5587413960858116e-07,
      "loss": 1.7806,
      "step": 79040
    },
    {
      "epoch": 0.0002869679809175003,
      "grad_norm": 10896.121328252544,
      "learning_rate": 3.558020492321485e-07,
      "loss": 1.7476,
      "step": 79072
    },
    {
      "epoch": 0.000287084115268337,
      "grad_norm": 9485.617112238928,
      "learning_rate": 3.5573000264859486e-07,
      "loss": 1.7472,
      "step": 79104
    },
    {
      "epoch": 0.0002872002496191737,
      "grad_norm": 9843.904103555662,
      "learning_rate": 3.556579998135999e-07,
      "loss": 1.7458,
      "step": 79136
    },
    {
      "epoch": 0.0002873163839700104,
      "grad_norm": 9351.588421225562,
      "learning_rate": 3.5558604068290624e-07,
      "loss": 1.7368,
      "step": 79168
    },
    {
      "epoch": 0.0002874325183208471,
      "grad_norm": 11245.795125290164,
      "learning_rate": 3.555141252123188e-07,
      "loss": 1.7708,
      "step": 79200
    },
    {
      "epoch": 0.0002875486526716838,
      "grad_norm": 9473.69473859064,
      "learning_rate": 3.5544225335770543e-07,
      "loss": 1.7652,
      "step": 79232
    },
    {
      "epoch": 0.00028766478702252053,
      "grad_norm": 9940.615876292575,
      "learning_rate": 3.5537042507499613e-07,
      "loss": 1.7914,
      "step": 79264
    },
    {
      "epoch": 0.00028778092137335723,
      "grad_norm": 8749.186019282022,
      "learning_rate": 3.552986403201834e-07,
      "loss": 1.8056,
      "step": 79296
    },
    {
      "epoch": 0.00028789705572419393,
      "grad_norm": 10059.88409475974,
      "learning_rate": 3.552268990493218e-07,
      "loss": 1.7959,
      "step": 79328
    },
    {
      "epoch": 0.00028801319007503063,
      "grad_norm": 10115.855870859372,
      "learning_rate": 3.5515520121852806e-07,
      "loss": 1.7788,
      "step": 79360
    },
    {
      "epoch": 0.00028812932442586733,
      "grad_norm": 8673.97625083214,
      "learning_rate": 3.550835467839809e-07,
      "loss": 1.7931,
      "step": 79392
    },
    {
      "epoch": 0.00028824545877670403,
      "grad_norm": 10225.235547409166,
      "learning_rate": 3.5501193570192083e-07,
      "loss": 1.7906,
      "step": 79424
    },
    {
      "epoch": 0.00028836159312754073,
      "grad_norm": 10331.817652281714,
      "learning_rate": 3.5494036792865017e-07,
      "loss": 1.7732,
      "step": 79456
    },
    {
      "epoch": 0.00028847772747837743,
      "grad_norm": 8311.810512758337,
      "learning_rate": 3.548688434205329e-07,
      "loss": 1.7629,
      "step": 79488
    },
    {
      "epoch": 0.00028859386182921413,
      "grad_norm": 10614.413313980194,
      "learning_rate": 3.5479736213399435e-07,
      "loss": 1.7536,
      "step": 79520
    },
    {
      "epoch": 0.0002887099961800509,
      "grad_norm": 9969.188532674061,
      "learning_rate": 3.5472592402552146e-07,
      "loss": 1.7472,
      "step": 79552
    },
    {
      "epoch": 0.0002888261305308876,
      "grad_norm": 9891.833803698888,
      "learning_rate": 3.5465452905166253e-07,
      "loss": 1.7774,
      "step": 79584
    },
    {
      "epoch": 0.0002889422648817243,
      "grad_norm": 10151.45733380188,
      "learning_rate": 3.545831771690268e-07,
      "loss": 1.7781,
      "step": 79616
    },
    {
      "epoch": 0.000289058399232561,
      "grad_norm": 9720.761389932375,
      "learning_rate": 3.545118683342848e-07,
      "loss": 1.7569,
      "step": 79648
    },
    {
      "epoch": 0.0002891745335833977,
      "grad_norm": 8854.023379232744,
      "learning_rate": 3.5444060250416794e-07,
      "loss": 1.7496,
      "step": 79680
    },
    {
      "epoch": 0.0002892906679342344,
      "grad_norm": 9858.680439085141,
      "learning_rate": 3.5436937963546856e-07,
      "loss": 1.7672,
      "step": 79712
    },
    {
      "epoch": 0.0002894068022850711,
      "grad_norm": 9171.561262947547,
      "learning_rate": 3.542981996850397e-07,
      "loss": 1.7616,
      "step": 79744
    },
    {
      "epoch": 0.0002895229366359078,
      "grad_norm": 8817.022173046862,
      "learning_rate": 3.54227062609795e-07,
      "loss": 1.7707,
      "step": 79776
    },
    {
      "epoch": 0.0002896390709867445,
      "grad_norm": 8439.53517677366,
      "learning_rate": 3.5415596836670875e-07,
      "loss": 1.7803,
      "step": 79808
    },
    {
      "epoch": 0.00028975520533758124,
      "grad_norm": 10028.719359918294,
      "learning_rate": 3.5408491691281553e-07,
      "loss": 1.757,
      "step": 79840
    },
    {
      "epoch": 0.00028987133968841794,
      "grad_norm": 9328.17817154025,
      "learning_rate": 3.540139082052104e-07,
      "loss": 1.7708,
      "step": 79872
    },
    {
      "epoch": 0.00028998747403925464,
      "grad_norm": 10243.238159878936,
      "learning_rate": 3.539429422010484e-07,
      "loss": 1.7868,
      "step": 79904
    },
    {
      "epoch": 0.00029010360839009134,
      "grad_norm": 10531.057116928005,
      "learning_rate": 3.538720188575449e-07,
      "loss": 1.7618,
      "step": 79936
    },
    {
      "epoch": 0.00029021974274092804,
      "grad_norm": 8470.270951982588,
      "learning_rate": 3.5380113813197504e-07,
      "loss": 1.7825,
      "step": 79968
    },
    {
      "epoch": 0.00029033587709176474,
      "grad_norm": 10927.044065070846,
      "learning_rate": 3.537325130298445e-07,
      "loss": 1.7836,
      "step": 80000
    },
    {
      "epoch": 0.00029045201144260144,
      "grad_norm": 8388.29410547818,
      "learning_rate": 3.5366171608370584e-07,
      "loss": 1.7803,
      "step": 80032
    },
    {
      "epoch": 0.00029056814579343814,
      "grad_norm": 8915.031127259175,
      "learning_rate": 3.5359096162901415e-07,
      "loss": 1.7686,
      "step": 80064
    },
    {
      "epoch": 0.00029068428014427484,
      "grad_norm": 9138.493420690305,
      "learning_rate": 3.535202496232818e-07,
      "loss": 1.7659,
      "step": 80096
    },
    {
      "epoch": 0.0002908004144951116,
      "grad_norm": 10307.516674737908,
      "learning_rate": 3.534495800240803e-07,
      "loss": 1.7682,
      "step": 80128
    },
    {
      "epoch": 0.0002909165488459483,
      "grad_norm": 10014.714673918572,
      "learning_rate": 3.5337895278904085e-07,
      "loss": 1.7852,
      "step": 80160
    },
    {
      "epoch": 0.000291032683196785,
      "grad_norm": 9715.924865909576,
      "learning_rate": 3.5330836787585376e-07,
      "loss": 1.8082,
      "step": 80192
    },
    {
      "epoch": 0.0002911488175476217,
      "grad_norm": 9573.080068609059,
      "learning_rate": 3.5323782524226847e-07,
      "loss": 1.7851,
      "step": 80224
    },
    {
      "epoch": 0.0002912649518984584,
      "grad_norm": 9868.43320897497,
      "learning_rate": 3.531673248460934e-07,
      "loss": 1.7665,
      "step": 80256
    },
    {
      "epoch": 0.0002913810862492951,
      "grad_norm": 8910.08035878465,
      "learning_rate": 3.53096866645196e-07,
      "loss": 1.7367,
      "step": 80288
    },
    {
      "epoch": 0.0002914972206001318,
      "grad_norm": 9178.11603761905,
      "learning_rate": 3.5302645059750246e-07,
      "loss": 1.749,
      "step": 80320
    },
    {
      "epoch": 0.0002916133549509685,
      "grad_norm": 9668.997569551871,
      "learning_rate": 3.5295607666099774e-07,
      "loss": 1.7377,
      "step": 80352
    },
    {
      "epoch": 0.0002917294893018052,
      "grad_norm": 10609.495746735563,
      "learning_rate": 3.528857447937253e-07,
      "loss": 1.7477,
      "step": 80384
    },
    {
      "epoch": 0.00029184562365264195,
      "grad_norm": 9024.225617746932,
      "learning_rate": 3.528154549537872e-07,
      "loss": 1.7624,
      "step": 80416
    },
    {
      "epoch": 0.00029196175800347865,
      "grad_norm": 10395.703535595849,
      "learning_rate": 3.5274520709934385e-07,
      "loss": 1.7534,
      "step": 80448
    },
    {
      "epoch": 0.00029207789235431535,
      "grad_norm": 9468.678999733807,
      "learning_rate": 3.52675001188614e-07,
      "loss": 1.7706,
      "step": 80480
    },
    {
      "epoch": 0.00029219402670515205,
      "grad_norm": 8925.82018640304,
      "learning_rate": 3.526048371798744e-07,
      "loss": 1.7919,
      "step": 80512
    },
    {
      "epoch": 0.00029231016105598875,
      "grad_norm": 8081.882082782451,
      "learning_rate": 3.5253471503146023e-07,
      "loss": 1.7992,
      "step": 80544
    },
    {
      "epoch": 0.00029242629540682545,
      "grad_norm": 9846.579406067876,
      "learning_rate": 3.524646347017643e-07,
      "loss": 1.7833,
      "step": 80576
    },
    {
      "epoch": 0.00029254242975766215,
      "grad_norm": 9639.891700636475,
      "learning_rate": 3.523945961492376e-07,
      "loss": 1.7632,
      "step": 80608
    },
    {
      "epoch": 0.00029265856410849885,
      "grad_norm": 9071.147005754014,
      "learning_rate": 3.5232459933238857e-07,
      "loss": 1.7626,
      "step": 80640
    },
    {
      "epoch": 0.00029277469845933555,
      "grad_norm": 9611.533072304335,
      "learning_rate": 3.5225464420978344e-07,
      "loss": 1.7517,
      "step": 80672
    },
    {
      "epoch": 0.0002928908328101723,
      "grad_norm": 9063.930273341692,
      "learning_rate": 3.521847307400462e-07,
      "loss": 1.7592,
      "step": 80704
    },
    {
      "epoch": 0.000293006967161009,
      "grad_norm": 10157.734983745146,
      "learning_rate": 3.521148588818581e-07,
      "loss": 1.7888,
      "step": 80736
    },
    {
      "epoch": 0.0002931231015118457,
      "grad_norm": 9236.826511307874,
      "learning_rate": 3.520450285939578e-07,
      "loss": 1.7895,
      "step": 80768
    },
    {
      "epoch": 0.0002932392358626824,
      "grad_norm": 10561.931736192959,
      "learning_rate": 3.5197523983514123e-07,
      "loss": 1.7822,
      "step": 80800
    },
    {
      "epoch": 0.0002933553702135191,
      "grad_norm": 10315.68805267007,
      "learning_rate": 3.519054925642614e-07,
      "loss": 1.7904,
      "step": 80832
    },
    {
      "epoch": 0.0002934715045643558,
      "grad_norm": 9200.110434119799,
      "learning_rate": 3.518357867402284e-07,
      "loss": 1.7681,
      "step": 80864
    },
    {
      "epoch": 0.0002935876389151925,
      "grad_norm": 10203.170389638703,
      "learning_rate": 3.5176612232200936e-07,
      "loss": 1.7663,
      "step": 80896
    },
    {
      "epoch": 0.0002937037732660292,
      "grad_norm": 9507.822463634879,
      "learning_rate": 3.5169649926862825e-07,
      "loss": 1.7656,
      "step": 80928
    },
    {
      "epoch": 0.0002938199076168659,
      "grad_norm": 9019.666956157527,
      "learning_rate": 3.5162691753916566e-07,
      "loss": 1.7725,
      "step": 80960
    },
    {
      "epoch": 0.00029393604196770265,
      "grad_norm": 9100.8204025791,
      "learning_rate": 3.5155954960722464e-07,
      "loss": 1.7695,
      "step": 80992
    },
    {
      "epoch": 0.00029405217631853935,
      "grad_norm": 9357.25237449541,
      "learning_rate": 3.514900491148644e-07,
      "loss": 1.7749,
      "step": 81024
    },
    {
      "epoch": 0.00029416831066937605,
      "grad_norm": 10615.880933770877,
      "learning_rate": 3.514205898252768e-07,
      "loss": 1.7499,
      "step": 81056
    },
    {
      "epoch": 0.00029428444502021275,
      "grad_norm": 8848.963329113756,
      "learning_rate": 3.513511716977668e-07,
      "loss": 1.7532,
      "step": 81088
    },
    {
      "epoch": 0.00029440057937104945,
      "grad_norm": 10333.692757189949,
      "learning_rate": 3.512817946916953e-07,
      "loss": 1.7642,
      "step": 81120
    },
    {
      "epoch": 0.00029451671372188615,
      "grad_norm": 10383.787940823908,
      "learning_rate": 3.512124587664799e-07,
      "loss": 1.7866,
      "step": 81152
    },
    {
      "epoch": 0.00029463284807272285,
      "grad_norm": 9673.910274547723,
      "learning_rate": 3.511431638815938e-07,
      "loss": 1.7653,
      "step": 81184
    },
    {
      "epoch": 0.00029474898242355955,
      "grad_norm": 10560.437396244532,
      "learning_rate": 3.5107390999656626e-07,
      "loss": 1.7427,
      "step": 81216
    },
    {
      "epoch": 0.00029486511677439625,
      "grad_norm": 9319.036216261851,
      "learning_rate": 3.5100469707098236e-07,
      "loss": 1.7627,
      "step": 81248
    },
    {
      "epoch": 0.000294981251125233,
      "grad_norm": 8646.727820395412,
      "learning_rate": 3.509355250644833e-07,
      "loss": 1.7671,
      "step": 81280
    },
    {
      "epoch": 0.0002950973854760697,
      "grad_norm": 9451.54452986389,
      "learning_rate": 3.508663939367653e-07,
      "loss": 1.7686,
      "step": 81312
    },
    {
      "epoch": 0.0002952135198269064,
      "grad_norm": 9650.688369230456,
      "learning_rate": 3.507973036475806e-07,
      "loss": 1.7735,
      "step": 81344
    },
    {
      "epoch": 0.0002953296541777431,
      "grad_norm": 9774.57835407748,
      "learning_rate": 3.507282541567367e-07,
      "loss": 1.7603,
      "step": 81376
    },
    {
      "epoch": 0.0002954457885285798,
      "grad_norm": 10176.747220993553,
      "learning_rate": 3.506592454240967e-07,
      "loss": 1.7643,
      "step": 81408
    },
    {
      "epoch": 0.0002955619228794165,
      "grad_norm": 9898.148715795292,
      "learning_rate": 3.5059027740957856e-07,
      "loss": 1.7697,
      "step": 81440
    },
    {
      "epoch": 0.0002956780572302532,
      "grad_norm": 9094.924298750375,
      "learning_rate": 3.505213500731557e-07,
      "loss": 1.7692,
      "step": 81472
    },
    {
      "epoch": 0.0002957941915810899,
      "grad_norm": 8789.342637535528,
      "learning_rate": 3.5045246337485655e-07,
      "loss": 1.777,
      "step": 81504
    },
    {
      "epoch": 0.0002959103259319266,
      "grad_norm": 9262.23806647184,
      "learning_rate": 3.5038361727476445e-07,
      "loss": 1.7832,
      "step": 81536
    },
    {
      "epoch": 0.00029602646028276336,
      "grad_norm": 10758.477587465617,
      "learning_rate": 3.503148117330176e-07,
      "loss": 1.786,
      "step": 81568
    },
    {
      "epoch": 0.00029614259463360006,
      "grad_norm": 10239.889550185588,
      "learning_rate": 3.5024604670980906e-07,
      "loss": 1.7963,
      "step": 81600
    },
    {
      "epoch": 0.00029625872898443676,
      "grad_norm": 10173.062075894357,
      "learning_rate": 3.501773221653866e-07,
      "loss": 1.7743,
      "step": 81632
    },
    {
      "epoch": 0.00029637486333527346,
      "grad_norm": 8340.734020456473,
      "learning_rate": 3.5010863806005224e-07,
      "loss": 1.7757,
      "step": 81664
    },
    {
      "epoch": 0.00029649099768611016,
      "grad_norm": 10065.77200218642,
      "learning_rate": 3.50039994354163e-07,
      "loss": 1.778,
      "step": 81696
    },
    {
      "epoch": 0.00029660713203694686,
      "grad_norm": 10056.171239592135,
      "learning_rate": 3.4997139100812984e-07,
      "loss": 1.8016,
      "step": 81728
    },
    {
      "epoch": 0.00029672326638778356,
      "grad_norm": 10144.61049030469,
      "learning_rate": 3.4990282798241826e-07,
      "loss": 1.8017,
      "step": 81760
    },
    {
      "epoch": 0.00029683940073862026,
      "grad_norm": 9844.04439242327,
      "learning_rate": 3.4983430523754785e-07,
      "loss": 1.7551,
      "step": 81792
    },
    {
      "epoch": 0.00029695553508945696,
      "grad_norm": 9967.564396581543,
      "learning_rate": 3.497658227340924e-07,
      "loss": 1.7412,
      "step": 81824
    },
    {
      "epoch": 0.0002970716694402937,
      "grad_norm": 9845.324778797294,
      "learning_rate": 3.496973804326796e-07,
      "loss": 1.7525,
      "step": 81856
    },
    {
      "epoch": 0.0002971878037911304,
      "grad_norm": 10036.66069965504,
      "learning_rate": 3.4962897829399104e-07,
      "loss": 1.7637,
      "step": 81888
    },
    {
      "epoch": 0.0002973039381419671,
      "grad_norm": 7875.989334680437,
      "learning_rate": 3.4956061627876217e-07,
      "loss": 1.75,
      "step": 81920
    },
    {
      "epoch": 0.0002974200724928038,
      "grad_norm": 9324.589535202073,
      "learning_rate": 3.494922943477822e-07,
      "loss": 1.7566,
      "step": 81952
    },
    {
      "epoch": 0.0002975362068436405,
      "grad_norm": 9320.419518455165,
      "learning_rate": 3.49424012461894e-07,
      "loss": 1.7383,
      "step": 81984
    },
    {
      "epoch": 0.0002976523411944772,
      "grad_norm": 9872.053687050127,
      "learning_rate": 3.4935790253556936e-07,
      "loss": 1.7608,
      "step": 82016
    },
    {
      "epoch": 0.0002977684755453139,
      "grad_norm": 10331.063352820947,
      "learning_rate": 3.492896993742306e-07,
      "loss": 1.7692,
      "step": 82048
    },
    {
      "epoch": 0.0002978846098961506,
      "grad_norm": 9506.746236226147,
      "learning_rate": 3.4922153614205067e-07,
      "loss": 1.7725,
      "step": 82080
    },
    {
      "epoch": 0.0002980007442469873,
      "grad_norm": 10917.344182538169,
      "learning_rate": 3.4915341280008426e-07,
      "loss": 1.7432,
      "step": 82112
    },
    {
      "epoch": 0.00029811687859782407,
      "grad_norm": 10032.113037640675,
      "learning_rate": 3.4908532930943904e-07,
      "loss": 1.7439,
      "step": 82144
    },
    {
      "epoch": 0.00029823301294866077,
      "grad_norm": 10727.560952984606,
      "learning_rate": 3.49017285631276e-07,
      "loss": 1.7584,
      "step": 82176
    },
    {
      "epoch": 0.00029834914729949747,
      "grad_norm": 10796.558710996758,
      "learning_rate": 3.4894928172680916e-07,
      "loss": 1.7496,
      "step": 82208
    },
    {
      "epoch": 0.00029846528165033417,
      "grad_norm": 9247.35691968251,
      "learning_rate": 3.4888131755730517e-07,
      "loss": 1.7744,
      "step": 82240
    },
    {
      "epoch": 0.00029858141600117087,
      "grad_norm": 10483.76707104846,
      "learning_rate": 3.488133930840836e-07,
      "loss": 1.7882,
      "step": 82272
    },
    {
      "epoch": 0.00029869755035200757,
      "grad_norm": 8635.80361055067,
      "learning_rate": 3.4874550826851676e-07,
      "loss": 1.7943,
      "step": 82304
    },
    {
      "epoch": 0.00029881368470284427,
      "grad_norm": 8253.985643311962,
      "learning_rate": 3.4867766307202953e-07,
      "loss": 1.8118,
      "step": 82336
    },
    {
      "epoch": 0.00029892981905368097,
      "grad_norm": 9351.118008024496,
      "learning_rate": 3.4860985745609914e-07,
      "loss": 1.8142,
      "step": 82368
    },
    {
      "epoch": 0.00029904595340451767,
      "grad_norm": 9387.916488763627,
      "learning_rate": 3.485420913822555e-07,
      "loss": 1.7763,
      "step": 82400
    },
    {
      "epoch": 0.0002991620877553544,
      "grad_norm": 8651.244303566973,
      "learning_rate": 3.4847436481208077e-07,
      "loss": 1.7762,
      "step": 82432
    },
    {
      "epoch": 0.0002992782221061911,
      "grad_norm": 9696.271242080638,
      "learning_rate": 3.4840667770720914e-07,
      "loss": 1.8001,
      "step": 82464
    },
    {
      "epoch": 0.0002993943564570278,
      "grad_norm": 10989.16393544113,
      "learning_rate": 3.4833903002932723e-07,
      "loss": 1.7709,
      "step": 82496
    },
    {
      "epoch": 0.0002995104908078645,
      "grad_norm": 10086.47569768549,
      "learning_rate": 3.4827142174017355e-07,
      "loss": 1.7777,
      "step": 82528
    },
    {
      "epoch": 0.0002996266251587012,
      "grad_norm": 8523.977475333919,
      "learning_rate": 3.482038528015386e-07,
      "loss": 1.7451,
      "step": 82560
    },
    {
      "epoch": 0.0002997427595095379,
      "grad_norm": 8444.835581584759,
      "learning_rate": 3.481363231752648e-07,
      "loss": 1.7406,
      "step": 82592
    },
    {
      "epoch": 0.0002998588938603746,
      "grad_norm": 9481.989453695885,
      "learning_rate": 3.4806883282324627e-07,
      "loss": 1.7467,
      "step": 82624
    },
    {
      "epoch": 0.0002999750282112113,
      "grad_norm": 11277.796238627474,
      "learning_rate": 3.480013817074289e-07,
      "loss": 1.7615,
      "step": 82656
    },
    {
      "epoch": 0.000300091162562048,
      "grad_norm": 10075.632188602362,
      "learning_rate": 3.479339697898101e-07,
      "loss": 1.7602,
      "step": 82688
    },
    {
      "epoch": 0.0003002072969128848,
      "grad_norm": 8721.163110503094,
      "learning_rate": 3.478665970324389e-07,
      "loss": 1.7486,
      "step": 82720
    },
    {
      "epoch": 0.0003003234312637215,
      "grad_norm": 9966.547847675241,
      "learning_rate": 3.477992633974157e-07,
      "loss": 1.7634,
      "step": 82752
    },
    {
      "epoch": 0.0003004395656145582,
      "grad_norm": 9538.765748250662,
      "learning_rate": 3.477319688468921e-07,
      "loss": 1.7664,
      "step": 82784
    },
    {
      "epoch": 0.0003005556999653949,
      "grad_norm": 10403.010718056576,
      "learning_rate": 3.476647133430712e-07,
      "loss": 1.7623,
      "step": 82816
    },
    {
      "epoch": 0.0003006718343162316,
      "grad_norm": 8861.303402998907,
      "learning_rate": 3.47597496848207e-07,
      "loss": 1.7653,
      "step": 82848
    },
    {
      "epoch": 0.0003007879686670683,
      "grad_norm": 9856.77421877969,
      "learning_rate": 3.4753031932460477e-07,
      "loss": 1.7552,
      "step": 82880
    },
    {
      "epoch": 0.000300904103017905,
      "grad_norm": 10171.489763058311,
      "learning_rate": 3.474631807346205e-07,
      "loss": 1.7602,
      "step": 82912
    },
    {
      "epoch": 0.0003010202373687417,
      "grad_norm": 10359.700092184135,
      "learning_rate": 3.4739608104066147e-07,
      "loss": 1.7697,
      "step": 82944
    },
    {
      "epoch": 0.0003011363717195784,
      "grad_norm": 9927.248460676301,
      "learning_rate": 3.473290202051854e-07,
      "loss": 1.7758,
      "step": 82976
    },
    {
      "epoch": 0.00030125250607041513,
      "grad_norm": 9113.410338616384,
      "learning_rate": 3.4726409204140304e-07,
      "loss": 1.7752,
      "step": 83008
    },
    {
      "epoch": 0.00030136864042125183,
      "grad_norm": 8643.430916019403,
      "learning_rate": 3.471971075990494e-07,
      "loss": 1.784,
      "step": 83040
    },
    {
      "epoch": 0.00030148477477208853,
      "grad_norm": 8396.418522203381,
      "learning_rate": 3.4713016190402377e-07,
      "loss": 1.776,
      "step": 83072
    },
    {
      "epoch": 0.00030160090912292523,
      "grad_norm": 8715.35369333913,
      "learning_rate": 3.4706325491898483e-07,
      "loss": 1.7891,
      "step": 83104
    },
    {
      "epoch": 0.00030171704347376193,
      "grad_norm": 8994.133532475487,
      "learning_rate": 3.469963866066415e-07,
      "loss": 1.7886,
      "step": 83136
    },
    {
      "epoch": 0.00030183317782459863,
      "grad_norm": 9798.220144495632,
      "learning_rate": 3.4692955692975294e-07,
      "loss": 1.7737,
      "step": 83168
    },
    {
      "epoch": 0.00030194931217543533,
      "grad_norm": 10678.794782183988,
      "learning_rate": 3.468627658511285e-07,
      "loss": 1.7754,
      "step": 83200
    },
    {
      "epoch": 0.00030206544652627203,
      "grad_norm": 10390.283345510843,
      "learning_rate": 3.4679601333362785e-07,
      "loss": 1.7801,
      "step": 83232
    },
    {
      "epoch": 0.00030218158087710873,
      "grad_norm": 10750.994837688277,
      "learning_rate": 3.467292993401603e-07,
      "loss": 1.7928,
      "step": 83264
    },
    {
      "epoch": 0.0003022977152279455,
      "grad_norm": 9140.57066052224,
      "learning_rate": 3.4666262383368545e-07,
      "loss": 1.7952,
      "step": 83296
    },
    {
      "epoch": 0.0003024138495787822,
      "grad_norm": 10345.328994285295,
      "learning_rate": 3.4659598677721254e-07,
      "loss": 1.7594,
      "step": 83328
    },
    {
      "epoch": 0.0003025299839296189,
      "grad_norm": 8623.98631724332,
      "learning_rate": 3.465293881338005e-07,
      "loss": 1.7375,
      "step": 83360
    },
    {
      "epoch": 0.0003026461182804556,
      "grad_norm": 10352.957645040378,
      "learning_rate": 3.4646282786655825e-07,
      "loss": 1.7515,
      "step": 83392
    },
    {
      "epoch": 0.0003027622526312923,
      "grad_norm": 9248.884257033385,
      "learning_rate": 3.463963059386439e-07,
      "loss": 1.7459,
      "step": 83424
    },
    {
      "epoch": 0.000302878386982129,
      "grad_norm": 10728.33854797657,
      "learning_rate": 3.463298223132653e-07,
      "loss": 1.7482,
      "step": 83456
    },
    {
      "epoch": 0.0003029945213329657,
      "grad_norm": 8533.21861902061,
      "learning_rate": 3.462633769536796e-07,
      "loss": 1.7642,
      "step": 83488
    },
    {
      "epoch": 0.0003031106556838024,
      "grad_norm": 11198.845119029016,
      "learning_rate": 3.4619696982319334e-07,
      "loss": 1.7635,
      "step": 83520
    },
    {
      "epoch": 0.0003032267900346391,
      "grad_norm": 9582.141723017876,
      "learning_rate": 3.4613060088516224e-07,
      "loss": 1.7864,
      "step": 83552
    },
    {
      "epoch": 0.0003033429243854758,
      "grad_norm": 9266.701678590933,
      "learning_rate": 3.460642701029914e-07,
      "loss": 1.7852,
      "step": 83584
    },
    {
      "epoch": 0.00030345905873631254,
      "grad_norm": 9739.672479092918,
      "learning_rate": 3.459979774401346e-07,
      "loss": 1.7744,
      "step": 83616
    },
    {
      "epoch": 0.00030357519308714924,
      "grad_norm": 8219.440248581408,
      "learning_rate": 3.4593172286009496e-07,
      "loss": 1.7372,
      "step": 83648
    },
    {
      "epoch": 0.00030369132743798594,
      "grad_norm": 10911.931634683202,
      "learning_rate": 3.458655063264243e-07,
      "loss": 1.7441,
      "step": 83680
    },
    {
      "epoch": 0.00030380746178882264,
      "grad_norm": 8776.858549617853,
      "learning_rate": 3.4579932780272326e-07,
      "loss": 1.7537,
      "step": 83712
    },
    {
      "epoch": 0.00030392359613965934,
      "grad_norm": 9150.644348896967,
      "learning_rate": 3.457331872526414e-07,
      "loss": 1.7571,
      "step": 83744
    },
    {
      "epoch": 0.00030403973049049604,
      "grad_norm": 9517.522366666653,
      "learning_rate": 3.4566708463987665e-07,
      "loss": 1.7528,
      "step": 83776
    },
    {
      "epoch": 0.00030415586484133273,
      "grad_norm": 8982.745237398198,
      "learning_rate": 3.456010199281758e-07,
      "loss": 1.7696,
      "step": 83808
    },
    {
      "epoch": 0.00030427199919216943,
      "grad_norm": 8654.401076908789,
      "learning_rate": 3.4553499308133396e-07,
      "loss": 1.7709,
      "step": 83840
    },
    {
      "epoch": 0.00030438813354300613,
      "grad_norm": 7756.462531334758,
      "learning_rate": 3.454690040631945e-07,
      "loss": 1.7882,
      "step": 83872
    },
    {
      "epoch": 0.0003045042678938429,
      "grad_norm": 8999.12951345851,
      "learning_rate": 3.454030528376496e-07,
      "loss": 1.7983,
      "step": 83904
    },
    {
      "epoch": 0.0003046204022446796,
      "grad_norm": 9400.804433664174,
      "learning_rate": 3.45337139368639e-07,
      "loss": 1.7757,
      "step": 83936
    },
    {
      "epoch": 0.0003047365365955163,
      "grad_norm": 9928.59003081505,
      "learning_rate": 3.4527126362015115e-07,
      "loss": 1.7787,
      "step": 83968
    },
    {
      "epoch": 0.000304852670946353,
      "grad_norm": 10680.258236578366,
      "learning_rate": 3.4520542555622227e-07,
      "loss": 1.7866,
      "step": 84000
    },
    {
      "epoch": 0.0003049688052971897,
      "grad_norm": 10201.883551580071,
      "learning_rate": 3.451416808343938e-07,
      "loss": 1.7827,
      "step": 84032
    },
    {
      "epoch": 0.0003050849396480264,
      "grad_norm": 9586.349774549226,
      "learning_rate": 3.450759168570269e-07,
      "loss": 1.7787,
      "step": 84064
    },
    {
      "epoch": 0.0003052010739988631,
      "grad_norm": 9175.285717622095,
      "learning_rate": 3.4501019045773406e-07,
      "loss": 1.7713,
      "step": 84096
    },
    {
      "epoch": 0.0003053172083496998,
      "grad_norm": 9596.158085400635,
      "learning_rate": 3.449445016007417e-07,
      "loss": 1.7692,
      "step": 84128
    },
    {
      "epoch": 0.0003054333427005365,
      "grad_norm": 9386.983541053005,
      "learning_rate": 3.448788502503236e-07,
      "loss": 1.7682,
      "step": 84160
    },
    {
      "epoch": 0.00030554947705137324,
      "grad_norm": 9620.766393588403,
      "learning_rate": 3.4481323637080134e-07,
      "loss": 1.7829,
      "step": 84192
    },
    {
      "epoch": 0.00030566561140220994,
      "grad_norm": 10339.418358882669,
      "learning_rate": 3.447476599265439e-07,
      "loss": 1.753,
      "step": 84224
    },
    {
      "epoch": 0.00030578174575304664,
      "grad_norm": 11313.908166500203,
      "learning_rate": 3.4468212088196766e-07,
      "loss": 1.7401,
      "step": 84256
    },
    {
      "epoch": 0.00030589788010388334,
      "grad_norm": 9715.571419118898,
      "learning_rate": 3.446166192015365e-07,
      "loss": 1.7561,
      "step": 84288
    },
    {
      "epoch": 0.00030601401445472004,
      "grad_norm": 10895.87187883558,
      "learning_rate": 3.445511548497613e-07,
      "loss": 1.7721,
      "step": 84320
    },
    {
      "epoch": 0.00030613014880555674,
      "grad_norm": 8685.210187439334,
      "learning_rate": 3.444857277912002e-07,
      "loss": 1.7671,
      "step": 84352
    },
    {
      "epoch": 0.00030624628315639344,
      "grad_norm": 8238.442814027418,
      "learning_rate": 3.4442033799045856e-07,
      "loss": 1.7391,
      "step": 84384
    },
    {
      "epoch": 0.00030636241750723014,
      "grad_norm": 10293.9409362984,
      "learning_rate": 3.443549854121887e-07,
      "loss": 1.7459,
      "step": 84416
    },
    {
      "epoch": 0.00030647855185806684,
      "grad_norm": 10171.463709810894,
      "learning_rate": 3.442896700210897e-07,
      "loss": 1.7537,
      "step": 84448
    },
    {
      "epoch": 0.0003065946862089036,
      "grad_norm": 10077.299340597163,
      "learning_rate": 3.442243917819077e-07,
      "loss": 1.7669,
      "step": 84480
    },
    {
      "epoch": 0.0003067108205597403,
      "grad_norm": 8920.979766819337,
      "learning_rate": 3.441591506594355e-07,
      "loss": 1.7717,
      "step": 84512
    },
    {
      "epoch": 0.000306826954910577,
      "grad_norm": 9583.15177799037,
      "learning_rate": 3.4409394661851275e-07,
      "loss": 1.7756,
      "step": 84544
    },
    {
      "epoch": 0.0003069430892614137,
      "grad_norm": 8975.657413248346,
      "learning_rate": 3.440287796240255e-07,
      "loss": 1.76,
      "step": 84576
    },
    {
      "epoch": 0.0003070592236122504,
      "grad_norm": 10303.516487102837,
      "learning_rate": 3.439636496409064e-07,
      "loss": 1.7725,
      "step": 84608
    },
    {
      "epoch": 0.0003071753579630871,
      "grad_norm": 9470.897528745625,
      "learning_rate": 3.438985566341346e-07,
      "loss": 1.7746,
      "step": 84640
    },
    {
      "epoch": 0.0003072914923139238,
      "grad_norm": 9906.67068191933,
      "learning_rate": 3.4383350056873574e-07,
      "loss": 1.79,
      "step": 84672
    },
    {
      "epoch": 0.0003074076266647605,
      "grad_norm": 10470.783924807158,
      "learning_rate": 3.437684814097813e-07,
      "loss": 1.7901,
      "step": 84704
    },
    {
      "epoch": 0.0003075237610155972,
      "grad_norm": 9233.505943031607,
      "learning_rate": 3.437034991223896e-07,
      "loss": 1.7894,
      "step": 84736
    },
    {
      "epoch": 0.00030763989536643395,
      "grad_norm": 9416.351628948443,
      "learning_rate": 3.436385536717247e-07,
      "loss": 1.7724,
      "step": 84768
    },
    {
      "epoch": 0.00030775602971727065,
      "grad_norm": 10995.68569940047,
      "learning_rate": 3.435736450229968e-07,
      "loss": 1.7906,
      "step": 84800
    },
    {
      "epoch": 0.00030787216406810735,
      "grad_norm": 8067.601502305379,
      "learning_rate": 3.435087731414622e-07,
      "loss": 1.7678,
      "step": 84832
    },
    {
      "epoch": 0.00030798829841894405,
      "grad_norm": 9689.507830638251,
      "learning_rate": 3.434439379924228e-07,
      "loss": 1.7532,
      "step": 84864
    },
    {
      "epoch": 0.00030810443276978075,
      "grad_norm": 9841.509233852297,
      "learning_rate": 3.433791395412266e-07,
      "loss": 1.7554,
      "step": 84896
    },
    {
      "epoch": 0.00030822056712061745,
      "grad_norm": 9711.62705214734,
      "learning_rate": 3.433143777532674e-07,
      "loss": 1.765,
      "step": 84928
    },
    {
      "epoch": 0.00030833670147145415,
      "grad_norm": 10954.964536683812,
      "learning_rate": 3.4324965259398446e-07,
      "loss": 1.7557,
      "step": 84960
    },
    {
      "epoch": 0.00030845283582229085,
      "grad_norm": 10822.956435281443,
      "learning_rate": 3.4318496402886274e-07,
      "loss": 1.7479,
      "step": 84992
    },
    {
      "epoch": 0.00030856897017312755,
      "grad_norm": 10335.226267479586,
      "learning_rate": 3.4312233184555054e-07,
      "loss": 1.7701,
      "step": 85024
    },
    {
      "epoch": 0.0003086851045239643,
      "grad_norm": 10210.55052384542,
      "learning_rate": 3.43057715224494e-07,
      "loss": 1.7705,
      "step": 85056
    },
    {
      "epoch": 0.000308801238874801,
      "grad_norm": 9769.221668075712,
      "learning_rate": 3.429931350953999e-07,
      "loss": 1.7677,
      "step": 85088
    },
    {
      "epoch": 0.0003089173732256377,
      "grad_norm": 10115.940687845101,
      "learning_rate": 3.4292859142393336e-07,
      "loss": 1.776,
      "step": 85120
    },
    {
      "epoch": 0.0003090335075764744,
      "grad_norm": 10495.918635355365,
      "learning_rate": 3.428640841758048e-07,
      "loss": 1.7479,
      "step": 85152
    },
    {
      "epoch": 0.0003091496419273111,
      "grad_norm": 9873.2306769365,
      "learning_rate": 3.427996133167695e-07,
      "loss": 1.7433,
      "step": 85184
    },
    {
      "epoch": 0.0003092657762781478,
      "grad_norm": 10268.138487574075,
      "learning_rate": 3.4273517881262815e-07,
      "loss": 1.7491,
      "step": 85216
    },
    {
      "epoch": 0.0003093819106289845,
      "grad_norm": 10666.271513513988,
      "learning_rate": 3.4267078062922627e-07,
      "loss": 1.7612,
      "step": 85248
    },
    {
      "epoch": 0.0003094980449798212,
      "grad_norm": 11997.262020977952,
      "learning_rate": 3.4260641873245416e-07,
      "loss": 1.783,
      "step": 85280
    },
    {
      "epoch": 0.0003096141793306579,
      "grad_norm": 8918.095200209515,
      "learning_rate": 3.4254209308824714e-07,
      "loss": 1.7982,
      "step": 85312
    },
    {
      "epoch": 0.00030973031368149466,
      "grad_norm": 9010.470686928624,
      "learning_rate": 3.424778036625852e-07,
      "loss": 1.7771,
      "step": 85344
    },
    {
      "epoch": 0.00030984644803233136,
      "grad_norm": 8495.339663603805,
      "learning_rate": 3.42413550421493e-07,
      "loss": 1.7693,
      "step": 85376
    },
    {
      "epoch": 0.00030996258238316806,
      "grad_norm": 8890.883758097392,
      "learning_rate": 3.4234933333103986e-07,
      "loss": 1.7847,
      "step": 85408
    },
    {
      "epoch": 0.00031007871673400476,
      "grad_norm": 10603.71991331344,
      "learning_rate": 3.4228515235733945e-07,
      "loss": 1.7801,
      "step": 85440
    },
    {
      "epoch": 0.00031019485108484146,
      "grad_norm": 8723.579425900816,
      "learning_rate": 3.422210074665502e-07,
      "loss": 1.7687,
      "step": 85472
    },
    {
      "epoch": 0.00031031098543567816,
      "grad_norm": 9329.105423351159,
      "learning_rate": 3.4215689862487464e-07,
      "loss": 1.766,
      "step": 85504
    },
    {
      "epoch": 0.00031042711978651486,
      "grad_norm": 9332.22577952334,
      "learning_rate": 3.420928257985598e-07,
      "loss": 1.7741,
      "step": 85536
    },
    {
      "epoch": 0.00031054325413735156,
      "grad_norm": 10018.463455041396,
      "learning_rate": 3.4202878895389674e-07,
      "loss": 1.7654,
      "step": 85568
    },
    {
      "epoch": 0.00031065938848818826,
      "grad_norm": 9349.145308529545,
      "learning_rate": 3.4196478805722086e-07,
      "loss": 1.7457,
      "step": 85600
    },
    {
      "epoch": 0.000310775522839025,
      "grad_norm": 10455.22194886364,
      "learning_rate": 3.4190082307491147e-07,
      "loss": 1.7534,
      "step": 85632
    },
    {
      "epoch": 0.0003108916571898617,
      "grad_norm": 9678.839909823904,
      "learning_rate": 3.41836893973392e-07,
      "loss": 1.7544,
      "step": 85664
    },
    {
      "epoch": 0.0003110077915406984,
      "grad_norm": 10825.431169242174,
      "learning_rate": 3.417730007191299e-07,
      "loss": 1.7626,
      "step": 85696
    },
    {
      "epoch": 0.0003111239258915351,
      "grad_norm": 9562.808164969116,
      "learning_rate": 3.417091432786362e-07,
      "loss": 1.7762,
      "step": 85728
    },
    {
      "epoch": 0.0003112400602423718,
      "grad_norm": 9060.393368943756,
      "learning_rate": 3.41645321618466e-07,
      "loss": 1.7603,
      "step": 85760
    },
    {
      "epoch": 0.0003113561945932085,
      "grad_norm": 9636.844296760222,
      "learning_rate": 3.4158153570521785e-07,
      "loss": 1.7689,
      "step": 85792
    },
    {
      "epoch": 0.0003114723289440452,
      "grad_norm": 8989.90233539831,
      "learning_rate": 3.415177855055342e-07,
      "loss": 1.7747,
      "step": 85824
    },
    {
      "epoch": 0.0003115884632948819,
      "grad_norm": 10484.683781593034,
      "learning_rate": 3.414540709861009e-07,
      "loss": 1.7905,
      "step": 85856
    },
    {
      "epoch": 0.0003117045976457186,
      "grad_norm": 10791.978502573103,
      "learning_rate": 3.413903921136473e-07,
      "loss": 1.7948,
      "step": 85888
    },
    {
      "epoch": 0.00031182073199655536,
      "grad_norm": 10777.723043389082,
      "learning_rate": 3.413267488549462e-07,
      "loss": 1.7532,
      "step": 85920
    },
    {
      "epoch": 0.00031193686634739206,
      "grad_norm": 9955.621125776132,
      "learning_rate": 3.412631411768137e-07,
      "loss": 1.745,
      "step": 85952
    },
    {
      "epoch": 0.00031205300069822876,
      "grad_norm": 9298.615810968855,
      "learning_rate": 3.4119956904610933e-07,
      "loss": 1.7442,
      "step": 85984
    },
    {
      "epoch": 0.00031216913504906546,
      "grad_norm": 8589.898835259935,
      "learning_rate": 3.411380174117552e-07,
      "loss": 1.7571,
      "step": 86016
    },
    {
      "epoch": 0.00031228526939990216,
      "grad_norm": 9031.669723810764,
      "learning_rate": 3.410745151683678e-07,
      "loss": 1.7879,
      "step": 86048
    },
    {
      "epoch": 0.00031240140375073886,
      "grad_norm": 9938.11098750663,
      "learning_rate": 3.4101104837427654e-07,
      "loss": 1.7794,
      "step": 86080
    },
    {
      "epoch": 0.00031251753810157556,
      "grad_norm": 8835.401066165587,
      "learning_rate": 3.409476169965118e-07,
      "loss": 1.7577,
      "step": 86112
    },
    {
      "epoch": 0.00031263367245241226,
      "grad_norm": 8285.643849454307,
      "learning_rate": 3.40884221002147e-07,
      "loss": 1.7712,
      "step": 86144
    },
    {
      "epoch": 0.00031274980680324896,
      "grad_norm": 11082.787375024389,
      "learning_rate": 3.4082086035829827e-07,
      "loss": 1.7835,
      "step": 86176
    },
    {
      "epoch": 0.0003128659411540857,
      "grad_norm": 8595.491957997518,
      "learning_rate": 3.4075753503212446e-07,
      "loss": 1.7675,
      "step": 86208
    },
    {
      "epoch": 0.0003129820755049224,
      "grad_norm": 8676.06362355648,
      "learning_rate": 3.406942449908274e-07,
      "loss": 1.7744,
      "step": 86240
    },
    {
      "epoch": 0.0003130982098557591,
      "grad_norm": 10441.065654424361,
      "learning_rate": 3.406309902016512e-07,
      "loss": 1.7645,
      "step": 86272
    },
    {
      "epoch": 0.0003132143442065958,
      "grad_norm": 10518.148030903538,
      "learning_rate": 3.4056777063188276e-07,
      "loss": 1.7693,
      "step": 86304
    },
    {
      "epoch": 0.0003133304785574325,
      "grad_norm": 9153.07128782465,
      "learning_rate": 3.405045862488514e-07,
      "loss": 1.7707,
      "step": 86336
    },
    {
      "epoch": 0.0003134466129082692,
      "grad_norm": 10483.328860624377,
      "learning_rate": 3.40441437019929e-07,
      "loss": 1.7544,
      "step": 86368
    },
    {
      "epoch": 0.0003135627472591059,
      "grad_norm": 9628.125258844528,
      "learning_rate": 3.4037832291252933e-07,
      "loss": 1.7459,
      "step": 86400
    },
    {
      "epoch": 0.0003136788816099426,
      "grad_norm": 9490.619632036676,
      "learning_rate": 3.403152438941089e-07,
      "loss": 1.7661,
      "step": 86432
    },
    {
      "epoch": 0.0003137950159607793,
      "grad_norm": 9451.637953286192,
      "learning_rate": 3.4025219993216625e-07,
      "loss": 1.7828,
      "step": 86464
    },
    {
      "epoch": 0.00031391115031161607,
      "grad_norm": 10097.406597735877,
      "learning_rate": 3.4018919099424204e-07,
      "loss": 1.7684,
      "step": 86496
    },
    {
      "epoch": 0.00031402728466245277,
      "grad_norm": 10630.8648754464,
      "learning_rate": 3.4012621704791893e-07,
      "loss": 1.7458,
      "step": 86528
    },
    {
      "epoch": 0.00031414341901328947,
      "grad_norm": 10592.365741419619,
      "learning_rate": 3.4006327806082175e-07,
      "loss": 1.7613,
      "step": 86560
    },
    {
      "epoch": 0.00031425955336412617,
      "grad_norm": 9392.90466256312,
      "learning_rate": 3.400003740006171e-07,
      "loss": 1.7706,
      "step": 86592
    },
    {
      "epoch": 0.00031437568771496287,
      "grad_norm": 11042.593898174468,
      "learning_rate": 3.3993750483501356e-07,
      "loss": 1.7824,
      "step": 86624
    },
    {
      "epoch": 0.00031449182206579957,
      "grad_norm": 9176.365075562328,
      "learning_rate": 3.398746705317612e-07,
      "loss": 1.7835,
      "step": 86656
    },
    {
      "epoch": 0.00031460795641663627,
      "grad_norm": 10239.596085783853,
      "learning_rate": 3.3981187105865225e-07,
      "loss": 1.7529,
      "step": 86688
    },
    {
      "epoch": 0.00031472409076747297,
      "grad_norm": 9930.794630843999,
      "learning_rate": 3.3974910638352015e-07,
      "loss": 1.7491,
      "step": 86720
    },
    {
      "epoch": 0.00031484022511830967,
      "grad_norm": 9917.881527826394,
      "learning_rate": 3.396863764742402e-07,
      "loss": 1.7609,
      "step": 86752
    },
    {
      "epoch": 0.0003149563594691464,
      "grad_norm": 9980.462113549653,
      "learning_rate": 3.396236812987292e-07,
      "loss": 1.7625,
      "step": 86784
    },
    {
      "epoch": 0.0003150724938199831,
      "grad_norm": 9888.409073253393,
      "learning_rate": 3.395610208249452e-07,
      "loss": 1.7637,
      "step": 86816
    },
    {
      "epoch": 0.0003151886281708198,
      "grad_norm": 8881.806122630689,
      "learning_rate": 3.3949839502088756e-07,
      "loss": 1.7729,
      "step": 86848
    },
    {
      "epoch": 0.0003153047625216565,
      "grad_norm": 8964.208609799307,
      "learning_rate": 3.3943580385459737e-07,
      "loss": 1.762,
      "step": 86880
    },
    {
      "epoch": 0.0003154208968724932,
      "grad_norm": 8698.26051575831,
      "learning_rate": 3.3937324729415657e-07,
      "loss": 1.7743,
      "step": 86912
    },
    {
      "epoch": 0.0003155370312233299,
      "grad_norm": 9512.528896145335,
      "learning_rate": 3.393107253076883e-07,
      "loss": 1.7809,
      "step": 86944
    },
    {
      "epoch": 0.0003156531655741666,
      "grad_norm": 9940.634185000472,
      "learning_rate": 3.392482378633568e-07,
      "loss": 1.7884,
      "step": 86976
    },
    {
      "epoch": 0.0003157692999250033,
      "grad_norm": 9929.472493541638,
      "learning_rate": 3.391857849293676e-07,
      "loss": 1.7791,
      "step": 87008
    },
    {
      "epoch": 0.00031588543427584,
      "grad_norm": 11185.49846899994,
      "learning_rate": 3.391253165291205e-07,
      "loss": 1.798,
      "step": 87040
    },
    {
      "epoch": 0.0003160015686266768,
      "grad_norm": 8830.646748681549,
      "learning_rate": 3.3906293144461015e-07,
      "loss": 1.7929,
      "step": 87072
    },
    {
      "epoch": 0.0003161177029775135,
      "grad_norm": 9426.333751782822,
      "learning_rate": 3.390005807762925e-07,
      "loss": 1.7487,
      "step": 87104
    },
    {
      "epoch": 0.0003162338373283502,
      "grad_norm": 10022.960640449508,
      "learning_rate": 3.389382644925351e-07,
      "loss": 1.7421,
      "step": 87136
    },
    {
      "epoch": 0.0003163499716791869,
      "grad_norm": 9956.426467362675,
      "learning_rate": 3.3887598256174607e-07,
      "loss": 1.7412,
      "step": 87168
    },
    {
      "epoch": 0.0003164661060300236,
      "grad_norm": 8941.201149733743,
      "learning_rate": 3.388137349523742e-07,
      "loss": 1.7393,
      "step": 87200
    },
    {
      "epoch": 0.0003165822403808603,
      "grad_norm": 8871.804551499092,
      "learning_rate": 3.387515216329088e-07,
      "loss": 1.7608,
      "step": 87232
    },
    {
      "epoch": 0.000316698374731697,
      "grad_norm": 10057.81765593312,
      "learning_rate": 3.3868934257187966e-07,
      "loss": 1.7665,
      "step": 87264
    },
    {
      "epoch": 0.0003168145090825337,
      "grad_norm": 10430.71943827462,
      "learning_rate": 3.3862719773785713e-07,
      "loss": 1.7529,
      "step": 87296
    },
    {
      "epoch": 0.0003169306434333704,
      "grad_norm": 8677.466450525751,
      "learning_rate": 3.3856508709945174e-07,
      "loss": 1.769,
      "step": 87328
    },
    {
      "epoch": 0.00031704677778420713,
      "grad_norm": 10222.163665291218,
      "learning_rate": 3.385030106253144e-07,
      "loss": 1.7615,
      "step": 87360
    },
    {
      "epoch": 0.00031716291213504383,
      "grad_norm": 8984.347722567287,
      "learning_rate": 3.3844096828413617e-07,
      "loss": 1.7785,
      "step": 87392
    },
    {
      "epoch": 0.00031727904648588053,
      "grad_norm": 9779.122660034487,
      "learning_rate": 3.383789600446483e-07,
      "loss": 1.7513,
      "step": 87424
    },
    {
      "epoch": 0.00031739518083671723,
      "grad_norm": 11263.562136375864,
      "learning_rate": 3.3831698587562216e-07,
      "loss": 1.7494,
      "step": 87456
    },
    {
      "epoch": 0.00031751131518755393,
      "grad_norm": 9511.29076413922,
      "learning_rate": 3.3825504574586914e-07,
      "loss": 1.753,
      "step": 87488
    },
    {
      "epoch": 0.00031762744953839063,
      "grad_norm": 8045.032007394377,
      "learning_rate": 3.381931396242406e-07,
      "loss": 1.751,
      "step": 87520
    },
    {
      "epoch": 0.00031774358388922733,
      "grad_norm": 8804.743039975669,
      "learning_rate": 3.3813126747962766e-07,
      "loss": 1.7919,
      "step": 87552
    },
    {
      "epoch": 0.00031785971824006403,
      "grad_norm": 11822.201994552453,
      "learning_rate": 3.3806942928096136e-07,
      "loss": 1.8037,
      "step": 87584
    },
    {
      "epoch": 0.00031797585259090073,
      "grad_norm": 10384.696914209871,
      "learning_rate": 3.380076249972126e-07,
      "loss": 1.8047,
      "step": 87616
    },
    {
      "epoch": 0.0003180919869417375,
      "grad_norm": 8103.441244310962,
      "learning_rate": 3.3794585459739177e-07,
      "loss": 1.7887,
      "step": 87648
    },
    {
      "epoch": 0.0003182081212925742,
      "grad_norm": 10445.753012588417,
      "learning_rate": 3.37884118050549e-07,
      "loss": 1.7879,
      "step": 87680
    },
    {
      "epoch": 0.0003183242556434109,
      "grad_norm": 8845.233292570638,
      "learning_rate": 3.37822415325774e-07,
      "loss": 1.7587,
      "step": 87712
    },
    {
      "epoch": 0.0003184403899942476,
      "grad_norm": 11821.064926646837,
      "learning_rate": 3.3776074639219604e-07,
      "loss": 1.7621,
      "step": 87744
    },
    {
      "epoch": 0.0003185565243450843,
      "grad_norm": 10035.81048047441,
      "learning_rate": 3.3769911121898355e-07,
      "loss": 1.7698,
      "step": 87776
    },
    {
      "epoch": 0.000318672658695921,
      "grad_norm": 8998.893820909323,
      "learning_rate": 3.3763750977534465e-07,
      "loss": 1.7626,
      "step": 87808
    },
    {
      "epoch": 0.0003187887930467577,
      "grad_norm": 10576.851516401279,
      "learning_rate": 3.3757594203052657e-07,
      "loss": 1.7844,
      "step": 87840
    },
    {
      "epoch": 0.0003189049273975944,
      "grad_norm": 10866.374004238949,
      "learning_rate": 3.3751440795381586e-07,
      "loss": 1.7612,
      "step": 87872
    },
    {
      "epoch": 0.0003190210617484311,
      "grad_norm": 10019.693408483115,
      "learning_rate": 3.3745290751453827e-07,
      "loss": 1.7684,
      "step": 87904
    },
    {
      "epoch": 0.00031913719609926784,
      "grad_norm": 9113.031987214794,
      "learning_rate": 3.3739144068205847e-07,
      "loss": 1.7263,
      "step": 87936
    },
    {
      "epoch": 0.00031925333045010454,
      "grad_norm": 9398.819925926871,
      "learning_rate": 3.373300074257805e-07,
      "loss": 1.746,
      "step": 87968
    },
    {
      "epoch": 0.00031936946480094124,
      "grad_norm": 10250.505353395998,
      "learning_rate": 3.372686077151471e-07,
      "loss": 1.7546,
      "step": 88000
    },
    {
      "epoch": 0.00031948559915177794,
      "grad_norm": 9556.208976367145,
      "learning_rate": 3.372091587062434e-07,
      "loss": 1.7392,
      "step": 88032
    },
    {
      "epoch": 0.00031960173350261464,
      "grad_norm": 10012.348276003986,
      "learning_rate": 3.3714782494944904e-07,
      "loss": 1.7397,
      "step": 88064
    },
    {
      "epoch": 0.00031971786785345134,
      "grad_norm": 10101.633333278336,
      "learning_rate": 3.370865246478118e-07,
      "loss": 1.7576,
      "step": 88096
    },
    {
      "epoch": 0.00031983400220428804,
      "grad_norm": 10563.069250932704,
      "learning_rate": 3.3702525777092857e-07,
      "loss": 1.7652,
      "step": 88128
    },
    {
      "epoch": 0.00031995013655512474,
      "grad_norm": 9730.776536330488,
      "learning_rate": 3.3696402428843484e-07,
      "loss": 1.7814,
      "step": 88160
    },
    {
      "epoch": 0.00032006627090596144,
      "grad_norm": 9198.815358512204,
      "learning_rate": 3.36902824170005e-07,
      "loss": 1.7763,
      "step": 88192
    },
    {
      "epoch": 0.0003201824052567982,
      "grad_norm": 9537.274872834483,
      "learning_rate": 3.368416573853517e-07,
      "loss": 1.772,
      "step": 88224
    },
    {
      "epoch": 0.0003202985396076349,
      "grad_norm": 8422.986999871246,
      "learning_rate": 3.367805239042261e-07,
      "loss": 1.7668,
      "step": 88256
    },
    {
      "epoch": 0.0003204146739584716,
      "grad_norm": 9681.583548159877,
      "learning_rate": 3.367194236964179e-07,
      "loss": 1.7683,
      "step": 88288
    },
    {
      "epoch": 0.0003205308083093083,
      "grad_norm": 9098.430414087916,
      "learning_rate": 3.366583567317551e-07,
      "loss": 1.7758,
      "step": 88320
    },
    {
      "epoch": 0.000320646942660145,
      "grad_norm": 10250.945907573603,
      "learning_rate": 3.3659732298010395e-07,
      "loss": 1.7768,
      "step": 88352
    },
    {
      "epoch": 0.0003207630770109817,
      "grad_norm": 7717.517735645315,
      "learning_rate": 3.3653632241136893e-07,
      "loss": 1.7745,
      "step": 88384
    },
    {
      "epoch": 0.0003208792113618184,
      "grad_norm": 9365.075546945684,
      "learning_rate": 3.3647535499549277e-07,
      "loss": 1.7798,
      "step": 88416
    },
    {
      "epoch": 0.0003209953457126551,
      "grad_norm": 9610.343178055611,
      "learning_rate": 3.3641442070245625e-07,
      "loss": 1.7902,
      "step": 88448
    },
    {
      "epoch": 0.0003211114800634918,
      "grad_norm": 9143.102974373634,
      "learning_rate": 3.363535195022781e-07,
      "loss": 1.802,
      "step": 88480
    },
    {
      "epoch": 0.00032122761441432854,
      "grad_norm": 10723.14757895274,
      "learning_rate": 3.3629265136501523e-07,
      "loss": 1.7757,
      "step": 88512
    },
    {
      "epoch": 0.00032134374876516524,
      "grad_norm": 10405.132195219818,
      "learning_rate": 3.3623181626076223e-07,
      "loss": 1.7671,
      "step": 88544
    },
    {
      "epoch": 0.00032145988311600194,
      "grad_norm": 8957.537049881514,
      "learning_rate": 3.3617101415965173e-07,
      "loss": 1.7615,
      "step": 88576
    },
    {
      "epoch": 0.00032157601746683864,
      "grad_norm": 9859.432843728893,
      "learning_rate": 3.3611024503185405e-07,
      "loss": 1.7666,
      "step": 88608
    },
    {
      "epoch": 0.00032169215181767534,
      "grad_norm": 9120.339138431202,
      "learning_rate": 3.3604950884757733e-07,
      "loss": 1.7401,
      "step": 88640
    },
    {
      "epoch": 0.00032180828616851204,
      "grad_norm": 8304.230849392374,
      "learning_rate": 3.359888055770673e-07,
      "loss": 1.7323,
      "step": 88672
    },
    {
      "epoch": 0.00032192442051934874,
      "grad_norm": 10902.76753856561,
      "learning_rate": 3.359281351906073e-07,
      "loss": 1.7467,
      "step": 88704
    },
    {
      "epoch": 0.00032204055487018544,
      "grad_norm": 8676.1793434668,
      "learning_rate": 3.358674976585184e-07,
      "loss": 1.7619,
      "step": 88736
    },
    {
      "epoch": 0.00032215668922102214,
      "grad_norm": 9116.727702416038,
      "learning_rate": 3.3580689295115875e-07,
      "loss": 1.7757,
      "step": 88768
    },
    {
      "epoch": 0.0003222728235718589,
      "grad_norm": 9249.616316366857,
      "learning_rate": 3.357463210389244e-07,
      "loss": 1.7754,
      "step": 88800
    },
    {
      "epoch": 0.0003223889579226956,
      "grad_norm": 9031.925043975953,
      "learning_rate": 3.356857818922484e-07,
      "loss": 1.7745,
      "step": 88832
    },
    {
      "epoch": 0.0003225050922735323,
      "grad_norm": 10344.8735130015,
      "learning_rate": 3.356252754816014e-07,
      "loss": 1.7659,
      "step": 88864
    },
    {
      "epoch": 0.000322621226624369,
      "grad_norm": 10517.788550831398,
      "learning_rate": 3.3556480177749103e-07,
      "loss": 1.7583,
      "step": 88896
    },
    {
      "epoch": 0.0003227373609752057,
      "grad_norm": 8965.03965412312,
      "learning_rate": 3.3550436075046234e-07,
      "loss": 1.7475,
      "step": 88928
    },
    {
      "epoch": 0.0003228534953260424,
      "grad_norm": 9122.23799294888,
      "learning_rate": 3.3544395237109724e-07,
      "loss": 1.735,
      "step": 88960
    },
    {
      "epoch": 0.0003229696296768791,
      "grad_norm": 9716.70355624787,
      "learning_rate": 3.35383576610015e-07,
      "loss": 1.7499,
      "step": 88992
    },
    {
      "epoch": 0.0003230857640277158,
      "grad_norm": 8975.914660913393,
      "learning_rate": 3.3532511866900266e-07,
      "loss": 1.7549,
      "step": 89024
    },
    {
      "epoch": 0.0003232018983785525,
      "grad_norm": 9791.168469595445,
      "learning_rate": 3.352648070394461e-07,
      "loss": 1.7663,
      "step": 89056
    },
    {
      "epoch": 0.0003233180327293892,
      "grad_norm": 9554.680319089697,
      "learning_rate": 3.352045279411655e-07,
      "loss": 1.7885,
      "step": 89088
    },
    {
      "epoch": 0.00032343416708022595,
      "grad_norm": 10163.76632946665,
      "learning_rate": 3.351442813449267e-07,
      "loss": 1.7853,
      "step": 89120
    },
    {
      "epoch": 0.00032355030143106265,
      "grad_norm": 7511.147981500564,
      "learning_rate": 3.350840672215319e-07,
      "loss": 1.7726,
      "step": 89152
    },
    {
      "epoch": 0.00032366643578189935,
      "grad_norm": 9414.486921760526,
      "learning_rate": 3.3502388554182035e-07,
      "loss": 1.7763,
      "step": 89184
    },
    {
      "epoch": 0.00032378257013273605,
      "grad_norm": 10061.543420370455,
      "learning_rate": 3.349637362766677e-07,
      "loss": 1.7814,
      "step": 89216
    },
    {
      "epoch": 0.00032389870448357275,
      "grad_norm": 9959.310919938187,
      "learning_rate": 3.3490361939698646e-07,
      "loss": 1.7739,
      "step": 89248
    },
    {
      "epoch": 0.00032401483883440945,
      "grad_norm": 9382.753007513307,
      "learning_rate": 3.348435348737253e-07,
      "loss": 1.7759,
      "step": 89280
    },
    {
      "epoch": 0.00032413097318524615,
      "grad_norm": 9354.67006366339,
      "learning_rate": 3.347834826778697e-07,
      "loss": 1.7874,
      "step": 89312
    },
    {
      "epoch": 0.00032424710753608285,
      "grad_norm": 9284.204327781676,
      "learning_rate": 3.3472346278044137e-07,
      "loss": 1.7908,
      "step": 89344
    },
    {
      "epoch": 0.00032436324188691955,
      "grad_norm": 10117.896224018114,
      "learning_rate": 3.3466347515249834e-07,
      "loss": 1.7636,
      "step": 89376
    },
    {
      "epoch": 0.0003244793762377563,
      "grad_norm": 8459.435796789287,
      "learning_rate": 3.3460351976513513e-07,
      "loss": 1.78,
      "step": 89408
    },
    {
      "epoch": 0.000324595510588593,
      "grad_norm": 10285.19012950174,
      "learning_rate": 3.3454359658948236e-07,
      "loss": 1.7627,
      "step": 89440
    },
    {
      "epoch": 0.0003247116449394297,
      "grad_norm": 9465.812590580907,
      "learning_rate": 3.3448370559670677e-07,
      "loss": 1.7395,
      "step": 89472
    },
    {
      "epoch": 0.0003248277792902664,
      "grad_norm": 9382.364840486645,
      "learning_rate": 3.344238467580114e-07,
      "loss": 1.746,
      "step": 89504
    },
    {
      "epoch": 0.0003249439136411031,
      "grad_norm": 9804.8292182985,
      "learning_rate": 3.343640200446352e-07,
      "loss": 1.74,
      "step": 89536
    },
    {
      "epoch": 0.0003250600479919398,
      "grad_norm": 9595.83597192032,
      "learning_rate": 3.343042254278531e-07,
      "loss": 1.7466,
      "step": 89568
    },
    {
      "epoch": 0.0003251761823427765,
      "grad_norm": 9832.631997588438,
      "learning_rate": 3.342444628789762e-07,
      "loss": 1.7727,
      "step": 89600
    },
    {
      "epoch": 0.0003252923166936132,
      "grad_norm": 9129.238522461772,
      "learning_rate": 3.341847323693512e-07,
      "loss": 1.771,
      "step": 89632
    },
    {
      "epoch": 0.0003254084510444499,
      "grad_norm": 8339.319996258688,
      "learning_rate": 3.3412503387036084e-07,
      "loss": 1.7571,
      "step": 89664
    },
    {
      "epoch": 0.00032552458539528666,
      "grad_norm": 8261.056106818305,
      "learning_rate": 3.3406536735342355e-07,
      "loss": 1.7474,
      "step": 89696
    },
    {
      "epoch": 0.00032564071974612336,
      "grad_norm": 9849.141637726609,
      "learning_rate": 3.340057327899934e-07,
      "loss": 1.7493,
      "step": 89728
    },
    {
      "epoch": 0.00032575685409696006,
      "grad_norm": 9983.617580817085,
      "learning_rate": 3.3394613015156033e-07,
      "loss": 1.7395,
      "step": 89760
    },
    {
      "epoch": 0.00032587298844779676,
      "grad_norm": 9445.04960283428,
      "learning_rate": 3.3388655940964964e-07,
      "loss": 1.7374,
      "step": 89792
    },
    {
      "epoch": 0.00032598912279863346,
      "grad_norm": 10660.27157252572,
      "learning_rate": 3.3382702053582236e-07,
      "loss": 1.7604,
      "step": 89824
    },
    {
      "epoch": 0.00032610525714947016,
      "grad_norm": 9017.372344535852,
      "learning_rate": 3.3376751350167494e-07,
      "loss": 1.7729,
      "step": 89856
    },
    {
      "epoch": 0.00032622139150030686,
      "grad_norm": 9526.778259201796,
      "learning_rate": 3.3370803827883914e-07,
      "loss": 1.7765,
      "step": 89888
    },
    {
      "epoch": 0.00032633752585114356,
      "grad_norm": 9631.264922116929,
      "learning_rate": 3.3364859483898233e-07,
      "loss": 1.7853,
      "step": 89920
    },
    {
      "epoch": 0.00032645366020198026,
      "grad_norm": 9289.850160255546,
      "learning_rate": 3.33589183153807e-07,
      "loss": 1.7848,
      "step": 89952
    },
    {
      "epoch": 0.000326569794552817,
      "grad_norm": 9327.536116252779,
      "learning_rate": 3.3352980319505096e-07,
      "loss": 1.8005,
      "step": 89984
    },
    {
      "epoch": 0.0003266859289036537,
      "grad_norm": 10152.566375060052,
      "learning_rate": 3.334704549344873e-07,
      "loss": 1.8126,
      "step": 90016
    },
    {
      "epoch": 0.0003268020632544904,
      "grad_norm": 8991.202144318633,
      "learning_rate": 3.334129915082791e-07,
      "loss": 1.7926,
      "step": 90048
    },
    {
      "epoch": 0.0003269181976053271,
      "grad_norm": 10430.506890846676,
      "learning_rate": 3.333537055711779e-07,
      "loss": 1.7682,
      "step": 90080
    },
    {
      "epoch": 0.0003270343319561638,
      "grad_norm": 8907.292967001815,
      "learning_rate": 3.33294451248677e-07,
      "loss": 1.7702,
      "step": 90112
    },
    {
      "epoch": 0.0003271504663070005,
      "grad_norm": 8685.262114639949,
      "learning_rate": 3.3323522851268854e-07,
      "loss": 1.7519,
      "step": 90144
    },
    {
      "epoch": 0.0003272666006578372,
      "grad_norm": 9254.316614423778,
      "learning_rate": 3.3317603733515976e-07,
      "loss": 1.7629,
      "step": 90176
    },
    {
      "epoch": 0.0003273827350086739,
      "grad_norm": 10338.384206441546,
      "learning_rate": 3.3311687768807256e-07,
      "loss": 1.7549,
      "step": 90208
    },
    {
      "epoch": 0.0003274988693595106,
      "grad_norm": 9480.518551218598,
      "learning_rate": 3.3305774954344367e-07,
      "loss": 1.7451,
      "step": 90240
    },
    {
      "epoch": 0.00032761500371034736,
      "grad_norm": 9036.755280519663,
      "learning_rate": 3.3299865287332465e-07,
      "loss": 1.7426,
      "step": 90272
    },
    {
      "epoch": 0.00032773113806118406,
      "grad_norm": 9364.761182219225,
      "learning_rate": 3.3293958764980175e-07,
      "loss": 1.7478,
      "step": 90304
    },
    {
      "epoch": 0.00032784727241202076,
      "grad_norm": 9097.728507710042,
      "learning_rate": 3.328805538449958e-07,
      "loss": 1.7625,
      "step": 90336
    },
    {
      "epoch": 0.00032796340676285746,
      "grad_norm": 9074.771622470727,
      "learning_rate": 3.3282155143106237e-07,
      "loss": 1.765,
      "step": 90368
    },
    {
      "epoch": 0.00032807954111369416,
      "grad_norm": 11038.771399028064,
      "learning_rate": 3.327625803801915e-07,
      "loss": 1.754,
      "step": 90400
    },
    {
      "epoch": 0.00032819567546453086,
      "grad_norm": 9753.48204489043,
      "learning_rate": 3.327036406646075e-07,
      "loss": 1.7575,
      "step": 90432
    },
    {
      "epoch": 0.00032831180981536756,
      "grad_norm": 9574.66490275247,
      "learning_rate": 3.3264473225656946e-07,
      "loss": 1.7519,
      "step": 90464
    },
    {
      "epoch": 0.00032842794416620426,
      "grad_norm": 8795.69462862371,
      "learning_rate": 3.325858551283706e-07,
      "loss": 1.754,
      "step": 90496
    },
    {
      "epoch": 0.00032854407851704096,
      "grad_norm": 8986.84494135734,
      "learning_rate": 3.325270092523385e-07,
      "loss": 1.745,
      "step": 90528
    },
    {
      "epoch": 0.0003286602128678777,
      "grad_norm": 8358.941440158556,
      "learning_rate": 3.3246819460083506e-07,
      "loss": 1.769,
      "step": 90560
    },
    {
      "epoch": 0.0003287763472187144,
      "grad_norm": 9943.719123145023,
      "learning_rate": 3.3240941114625636e-07,
      "loss": 1.7871,
      "step": 90592
    },
    {
      "epoch": 0.0003288924815695511,
      "grad_norm": 10478.637411419482,
      "learning_rate": 3.3235065886103264e-07,
      "loss": 1.7885,
      "step": 90624
    },
    {
      "epoch": 0.0003290086159203878,
      "grad_norm": 8993.815875366807,
      "learning_rate": 3.3229193771762813e-07,
      "loss": 1.7728,
      "step": 90656
    },
    {
      "epoch": 0.0003291247502712245,
      "grad_norm": 9846.474292862395,
      "learning_rate": 3.322332476885413e-07,
      "loss": 1.7624,
      "step": 90688
    },
    {
      "epoch": 0.0003292408846220612,
      "grad_norm": 10090.839410078826,
      "learning_rate": 3.321745887463044e-07,
      "loss": 1.7668,
      "step": 90720
    },
    {
      "epoch": 0.0003293570189728979,
      "grad_norm": 10014.250046808298,
      "learning_rate": 3.321159608634838e-07,
      "loss": 1.7867,
      "step": 90752
    },
    {
      "epoch": 0.0003294731533237346,
      "grad_norm": 8623.571649844396,
      "learning_rate": 3.3205736401267965e-07,
      "loss": 1.7837,
      "step": 90784
    },
    {
      "epoch": 0.0003295892876745713,
      "grad_norm": 8460.504476684591,
      "learning_rate": 3.31998798166526e-07,
      "loss": 1.7672,
      "step": 90816
    },
    {
      "epoch": 0.00032970542202540807,
      "grad_norm": 9614.626877835666,
      "learning_rate": 3.319402632976905e-07,
      "loss": 1.7738,
      "step": 90848
    },
    {
      "epoch": 0.00032982155637624477,
      "grad_norm": 8442.560275177193,
      "learning_rate": 3.3188175937887477e-07,
      "loss": 1.7632,
      "step": 90880
    },
    {
      "epoch": 0.00032993769072708147,
      "grad_norm": 9380.217481487303,
      "learning_rate": 3.3182328638281397e-07,
      "loss": 1.7538,
      "step": 90912
    },
    {
      "epoch": 0.00033005382507791817,
      "grad_norm": 8560.16845628636,
      "learning_rate": 3.3176484428227674e-07,
      "loss": 1.7621,
      "step": 90944
    },
    {
      "epoch": 0.00033016995942875487,
      "grad_norm": 9543.405052705244,
      "learning_rate": 3.3170643305006555e-07,
      "loss": 1.7531,
      "step": 90976
    },
    {
      "epoch": 0.00033028609377959157,
      "grad_norm": 11127.501786115336,
      "learning_rate": 3.316480526590162e-07,
      "loss": 1.7565,
      "step": 91008
    },
    {
      "epoch": 0.00033040222813042827,
      "grad_norm": 8514.497401491177,
      "learning_rate": 3.315915260401258e-07,
      "loss": 1.7599,
      "step": 91040
    },
    {
      "epoch": 0.00033051836248126497,
      "grad_norm": 10555.630724878547,
      "learning_rate": 3.3153320628835954e-07,
      "loss": 1.76,
      "step": 91072
    },
    {
      "epoch": 0.00033063449683210167,
      "grad_norm": 9349.180926690851,
      "learning_rate": 3.314749172973085e-07,
      "loss": 1.7625,
      "step": 91104
    },
    {
      "epoch": 0.0003307506311829384,
      "grad_norm": 10452.274202296838,
      "learning_rate": 3.3141665903994093e-07,
      "loss": 1.767,
      "step": 91136
    },
    {
      "epoch": 0.0003308667655337751,
      "grad_norm": 8949.487694834828,
      "learning_rate": 3.313584314892583e-07,
      "loss": 1.7744,
      "step": 91168
    },
    {
      "epoch": 0.0003309828998846118,
      "grad_norm": 9848.60629733974,
      "learning_rate": 3.3130023461829547e-07,
      "loss": 1.7654,
      "step": 91200
    },
    {
      "epoch": 0.0003310990342354485,
      "grad_norm": 10102.325276885515,
      "learning_rate": 3.312420684001202e-07,
      "loss": 1.7468,
      "step": 91232
    },
    {
      "epoch": 0.0003312151685862852,
      "grad_norm": 11711.759731142029,
      "learning_rate": 3.311839328078336e-07,
      "loss": 1.7591,
      "step": 91264
    },
    {
      "epoch": 0.0003313313029371219,
      "grad_norm": 8711.375436749355,
      "learning_rate": 3.3112582781456954e-07,
      "loss": 1.7408,
      "step": 91296
    },
    {
      "epoch": 0.0003314474372879586,
      "grad_norm": 8914.711885417273,
      "learning_rate": 3.3106775339349515e-07,
      "loss": 1.764,
      "step": 91328
    },
    {
      "epoch": 0.0003315635716387953,
      "grad_norm": 9536.540882311574,
      "learning_rate": 3.310097095178102e-07,
      "loss": 1.7788,
      "step": 91360
    },
    {
      "epoch": 0.000331679705989632,
      "grad_norm": 9801.528962360924,
      "learning_rate": 3.3095169616074763e-07,
      "loss": 1.7696,
      "step": 91392
    },
    {
      "epoch": 0.0003317958403404688,
      "grad_norm": 8602.331428165275,
      "learning_rate": 3.3089371329557293e-07,
      "loss": 1.7587,
      "step": 91424
    },
    {
      "epoch": 0.0003319119746913055,
      "grad_norm": 9486.193335579874,
      "learning_rate": 3.308357608955846e-07,
      "loss": 1.7584,
      "step": 91456
    },
    {
      "epoch": 0.0003320281090421422,
      "grad_norm": 10318.060379741923,
      "learning_rate": 3.307778389341138e-07,
      "loss": 1.7631,
      "step": 91488
    },
    {
      "epoch": 0.0003321442433929789,
      "grad_norm": 9182.490947449935,
      "learning_rate": 3.3071994738452427e-07,
      "loss": 1.7671,
      "step": 91520
    },
    {
      "epoch": 0.0003322603777438156,
      "grad_norm": 9234.465008867597,
      "learning_rate": 3.306620862202124e-07,
      "loss": 1.7841,
      "step": 91552
    },
    {
      "epoch": 0.0003323765120946523,
      "grad_norm": 9972.967061010479,
      "learning_rate": 3.306042554146072e-07,
      "loss": 1.7794,
      "step": 91584
    },
    {
      "epoch": 0.000332492646445489,
      "grad_norm": 9140.097045436662,
      "learning_rate": 3.3054645494117024e-07,
      "loss": 1.77,
      "step": 91616
    },
    {
      "epoch": 0.0003326087807963257,
      "grad_norm": 8083.304151150072,
      "learning_rate": 3.3048868477339543e-07,
      "loss": 1.7649,
      "step": 91648
    },
    {
      "epoch": 0.0003327249151471624,
      "grad_norm": 10379.47763618189,
      "learning_rate": 3.304309448848092e-07,
      "loss": 1.7646,
      "step": 91680
    },
    {
      "epoch": 0.00033284104949799913,
      "grad_norm": 8869.55444202244,
      "learning_rate": 3.303732352489703e-07,
      "loss": 1.7543,
      "step": 91712
    },
    {
      "epoch": 0.00033295718384883583,
      "grad_norm": 9421.309038557221,
      "learning_rate": 3.3031555583946966e-07,
      "loss": 1.7695,
      "step": 91744
    },
    {
      "epoch": 0.00033307331819967253,
      "grad_norm": 8041.896915529321,
      "learning_rate": 3.302579066299309e-07,
      "loss": 1.7775,
      "step": 91776
    },
    {
      "epoch": 0.00033318945255050923,
      "grad_norm": 11391.611299548453,
      "learning_rate": 3.3020028759400934e-07,
      "loss": 1.7548,
      "step": 91808
    },
    {
      "epoch": 0.00033330558690134593,
      "grad_norm": 10108.533622637855,
      "learning_rate": 3.301426987053928e-07,
      "loss": 1.7603,
      "step": 91840
    },
    {
      "epoch": 0.00033342172125218263,
      "grad_norm": 8388.615976429008,
      "learning_rate": 3.3008513993780106e-07,
      "loss": 1.7823,
      "step": 91872
    },
    {
      "epoch": 0.00033353785560301933,
      "grad_norm": 9875.914641186406,
      "learning_rate": 3.30027611264986e-07,
      "loss": 1.7759,
      "step": 91904
    },
    {
      "epoch": 0.00033365398995385603,
      "grad_norm": 9861.551399247484,
      "learning_rate": 3.299701126607315e-07,
      "loss": 1.769,
      "step": 91936
    },
    {
      "epoch": 0.00033377012430469273,
      "grad_norm": 8925.184255801109,
      "learning_rate": 3.299126440988535e-07,
      "loss": 1.7519,
      "step": 91968
    },
    {
      "epoch": 0.0003338862586555295,
      "grad_norm": 8402.865344630962,
      "learning_rate": 3.2985520555319964e-07,
      "loss": 1.7328,
      "step": 92000
    },
    {
      "epoch": 0.0003340023930063662,
      "grad_norm": 18205.03358964218,
      "learning_rate": 3.297977969976497e-07,
      "loss": 1.7321,
      "step": 92032
    },
    {
      "epoch": 0.0003341185273572029,
      "grad_norm": 8560.851359531947,
      "learning_rate": 3.297422110338025e-07,
      "loss": 1.7421,
      "step": 92064
    },
    {
      "epoch": 0.0003342346617080396,
      "grad_norm": 10025.412909202294,
      "learning_rate": 3.2968486144505914e-07,
      "loss": 1.7527,
      "step": 92096
    },
    {
      "epoch": 0.0003343507960588763,
      "grad_norm": 8949.9039100987,
      "learning_rate": 3.29627541769062e-07,
      "loss": 1.7736,
      "step": 92128
    },
    {
      "epoch": 0.000334466930409713,
      "grad_norm": 10162.94071615101,
      "learning_rate": 3.295702519798167e-07,
      "loss": 1.7841,
      "step": 92160
    },
    {
      "epoch": 0.0003345830647605497,
      "grad_norm": 9114.691656880117,
      "learning_rate": 3.2951299205136036e-07,
      "loss": 1.7965,
      "step": 92192
    },
    {
      "epoch": 0.0003346991991113864,
      "grad_norm": 10065.554728876099,
      "learning_rate": 3.294557619577617e-07,
      "loss": 1.7593,
      "step": 92224
    },
    {
      "epoch": 0.0003348153334622231,
      "grad_norm": 8032.064989776913,
      "learning_rate": 3.293985616731212e-07,
      "loss": 1.7675,
      "step": 92256
    },
    {
      "epoch": 0.00033493146781305984,
      "grad_norm": 8841.776631424253,
      "learning_rate": 3.2934139117157036e-07,
      "loss": 1.7812,
      "step": 92288
    },
    {
      "epoch": 0.00033504760216389654,
      "grad_norm": 9605.9288983419,
      "learning_rate": 3.2928425042727254e-07,
      "loss": 1.7795,
      "step": 92320
    },
    {
      "epoch": 0.00033516373651473324,
      "grad_norm": 9898.520444995807,
      "learning_rate": 3.2922713941442213e-07,
      "loss": 1.7765,
      "step": 92352
    },
    {
      "epoch": 0.00033527987086556994,
      "grad_norm": 9502.570599579883,
      "learning_rate": 3.2917005810724504e-07,
      "loss": 1.7763,
      "step": 92384
    },
    {
      "epoch": 0.00033539600521640664,
      "grad_norm": 9289.404501904306,
      "learning_rate": 3.2911300647999844e-07,
      "loss": 1.7312,
      "step": 92416
    },
    {
      "epoch": 0.00033551213956724334,
      "grad_norm": 8406.568741169016,
      "learning_rate": 3.290559845069706e-07,
      "loss": 1.7545,
      "step": 92448
    },
    {
      "epoch": 0.00033562827391808004,
      "grad_norm": 8960.72474747439,
      "learning_rate": 3.289989921624811e-07,
      "loss": 1.7626,
      "step": 92480
    },
    {
      "epoch": 0.00033574440826891674,
      "grad_norm": 10196.423294469487,
      "learning_rate": 3.289420294208805e-07,
      "loss": 1.7578,
      "step": 92512
    },
    {
      "epoch": 0.00033586054261975344,
      "grad_norm": 7751.59596470301,
      "learning_rate": 3.2888509625655053e-07,
      "loss": 1.7631,
      "step": 92544
    },
    {
      "epoch": 0.0003359766769705902,
      "grad_norm": 9618.455697252028,
      "learning_rate": 3.28828192643904e-07,
      "loss": 1.7485,
      "step": 92576
    },
    {
      "epoch": 0.0003360928113214269,
      "grad_norm": 8833.012623108834,
      "learning_rate": 3.2877131855738454e-07,
      "loss": 1.7573,
      "step": 92608
    },
    {
      "epoch": 0.0003362089456722636,
      "grad_norm": 8503.980244567834,
      "learning_rate": 3.287144739714668e-07,
      "loss": 1.7607,
      "step": 92640
    },
    {
      "epoch": 0.0003363250800231003,
      "grad_norm": 10196.689756975054,
      "learning_rate": 3.286576588606564e-07,
      "loss": 1.7651,
      "step": 92672
    },
    {
      "epoch": 0.000336441214373937,
      "grad_norm": 10255.96031583586,
      "learning_rate": 3.286008731994898e-07,
      "loss": 1.7632,
      "step": 92704
    },
    {
      "epoch": 0.0003365573487247737,
      "grad_norm": 9152.095934811872,
      "learning_rate": 3.2854411696253394e-07,
      "loss": 1.7541,
      "step": 92736
    },
    {
      "epoch": 0.0003366734830756104,
      "grad_norm": 9543.033322796267,
      "learning_rate": 3.2848739012438674e-07,
      "loss": 1.7751,
      "step": 92768
    },
    {
      "epoch": 0.0003367896174264471,
      "grad_norm": 10066.072322410564,
      "learning_rate": 3.2843069265967694e-07,
      "loss": 1.7538,
      "step": 92800
    },
    {
      "epoch": 0.0003369057517772838,
      "grad_norm": 9267.429201240224,
      "learning_rate": 3.283740245430636e-07,
      "loss": 1.742,
      "step": 92832
    },
    {
      "epoch": 0.00033702188612812054,
      "grad_norm": 8738.506165243576,
      "learning_rate": 3.2831738574923665e-07,
      "loss": 1.7627,
      "step": 92864
    },
    {
      "epoch": 0.00033713802047895724,
      "grad_norm": 9981.340791697276,
      "learning_rate": 3.282607762529165e-07,
      "loss": 1.7801,
      "step": 92896
    },
    {
      "epoch": 0.00033725415482979394,
      "grad_norm": 9159.461774580426,
      "learning_rate": 3.282041960288538e-07,
      "loss": 1.784,
      "step": 92928
    },
    {
      "epoch": 0.00033737028918063064,
      "grad_norm": 10441.19820710248,
      "learning_rate": 3.281476450518301e-07,
      "loss": 1.7783,
      "step": 92960
    },
    {
      "epoch": 0.00033748642353146734,
      "grad_norm": 9328.215263382379,
      "learning_rate": 3.2809112329665716e-07,
      "loss": 1.7684,
      "step": 92992
    },
    {
      "epoch": 0.00033760255788230404,
      "grad_norm": 9460.859897493461,
      "learning_rate": 3.2803463073817695e-07,
      "loss": 1.764,
      "step": 93024
    },
    {
      "epoch": 0.00033771869223314074,
      "grad_norm": 9416.009876800259,
      "learning_rate": 3.279799313907905e-07,
      "loss": 1.79,
      "step": 93056
    },
    {
      "epoch": 0.00033783482658397744,
      "grad_norm": 9954.191278049662,
      "learning_rate": 3.279234962398958e-07,
      "loss": 1.7719,
      "step": 93088
    },
    {
      "epoch": 0.00033795096093481414,
      "grad_norm": 9156.894779345233,
      "learning_rate": 3.278670902111847e-07,
      "loss": 1.7734,
      "step": 93120
    },
    {
      "epoch": 0.0003380670952856509,
      "grad_norm": 9604.641586233189,
      "learning_rate": 3.278107132796193e-07,
      "loss": 1.7581,
      "step": 93152
    },
    {
      "epoch": 0.0003381832296364876,
      "grad_norm": 9676.914590922046,
      "learning_rate": 3.2775436542019193e-07,
      "loss": 1.7276,
      "step": 93184
    },
    {
      "epoch": 0.0003382993639873243,
      "grad_norm": 9510.154783177823,
      "learning_rate": 3.2769804660792487e-07,
      "loss": 1.7388,
      "step": 93216
    },
    {
      "epoch": 0.000338415498338161,
      "grad_norm": 8830.725677994986,
      "learning_rate": 3.2764175681787045e-07,
      "loss": 1.7338,
      "step": 93248
    },
    {
      "epoch": 0.0003385316326889977,
      "grad_norm": 10549.349932578783,
      "learning_rate": 3.275854960251111e-07,
      "loss": 1.7461,
      "step": 93280
    },
    {
      "epoch": 0.0003386477670398344,
      "grad_norm": 9246.400813289461,
      "learning_rate": 3.2752926420475904e-07,
      "loss": 1.75,
      "step": 93312
    },
    {
      "epoch": 0.0003387639013906711,
      "grad_norm": 10119.417572172817,
      "learning_rate": 3.274730613319565e-07,
      "loss": 1.7439,
      "step": 93344
    },
    {
      "epoch": 0.0003388800357415078,
      "grad_norm": 9069.242085202048,
      "learning_rate": 3.274168873818755e-07,
      "loss": 1.7691,
      "step": 93376
    },
    {
      "epoch": 0.0003389961700923445,
      "grad_norm": 8372.696041299958,
      "learning_rate": 3.2736074232971777e-07,
      "loss": 1.7865,
      "step": 93408
    },
    {
      "epoch": 0.00033911230444318125,
      "grad_norm": 9617.480751215466,
      "learning_rate": 3.2730462615071503e-07,
      "loss": 1.7671,
      "step": 93440
    },
    {
      "epoch": 0.00033922843879401795,
      "grad_norm": 10030.34206794564,
      "learning_rate": 3.2724853882012855e-07,
      "loss": 1.7648,
      "step": 93472
    },
    {
      "epoch": 0.00033934457314485465,
      "grad_norm": 8166.968103280434,
      "learning_rate": 3.271924803132493e-07,
      "loss": 1.7688,
      "step": 93504
    },
    {
      "epoch": 0.00033946070749569135,
      "grad_norm": 9004.154541099348,
      "learning_rate": 3.2713645060539784e-07,
      "loss": 1.7723,
      "step": 93536
    },
    {
      "epoch": 0.00033957684184652805,
      "grad_norm": 8317.08723051526,
      "learning_rate": 3.2708044967192427e-07,
      "loss": 1.7539,
      "step": 93568
    },
    {
      "epoch": 0.00033969297619736475,
      "grad_norm": 9359.065551645634,
      "learning_rate": 3.2702447748820847e-07,
      "loss": 1.763,
      "step": 93600
    },
    {
      "epoch": 0.00033980911054820145,
      "grad_norm": 9740.842982001095,
      "learning_rate": 3.269685340296594e-07,
      "loss": 1.7837,
      "step": 93632
    },
    {
      "epoch": 0.00033992524489903815,
      "grad_norm": 8936.926652938359,
      "learning_rate": 3.269126192717157e-07,
      "loss": 1.7673,
      "step": 93664
    },
    {
      "epoch": 0.00034004137924987485,
      "grad_norm": 8870.619144118407,
      "learning_rate": 3.268567331898454e-07,
      "loss": 1.7792,
      "step": 93696
    },
    {
      "epoch": 0.0003401575136007116,
      "grad_norm": 9434.658340395797,
      "learning_rate": 3.268008757595459e-07,
      "loss": 1.7795,
      "step": 93728
    },
    {
      "epoch": 0.0003402736479515483,
      "grad_norm": 7794.945028670824,
      "learning_rate": 3.267450469563438e-07,
      "loss": 1.7526,
      "step": 93760
    },
    {
      "epoch": 0.000340389782302385,
      "grad_norm": 10206.678401909212,
      "learning_rate": 3.266892467557949e-07,
      "loss": 1.7655,
      "step": 93792
    },
    {
      "epoch": 0.0003405059166532217,
      "grad_norm": 10202.209760635193,
      "learning_rate": 3.266334751334844e-07,
      "loss": 1.7577,
      "step": 93824
    },
    {
      "epoch": 0.0003406220510040584,
      "grad_norm": 10502.928734405466,
      "learning_rate": 3.2657773206502665e-07,
      "loss": 1.7617,
      "step": 93856
    },
    {
      "epoch": 0.0003407381853548951,
      "grad_norm": 9181.380832968427,
      "learning_rate": 3.2652201752606495e-07,
      "loss": 1.7681,
      "step": 93888
    },
    {
      "epoch": 0.0003408543197057318,
      "grad_norm": 11233.748439412377,
      "learning_rate": 3.264663314922718e-07,
      "loss": 1.7668,
      "step": 93920
    },
    {
      "epoch": 0.0003409704540565685,
      "grad_norm": 8307.476632528074,
      "learning_rate": 3.2641067393934866e-07,
      "loss": 1.7343,
      "step": 93952
    },
    {
      "epoch": 0.0003410865884074052,
      "grad_norm": 9183.071708311985,
      "learning_rate": 3.263550448430261e-07,
      "loss": 1.7528,
      "step": 93984
    },
    {
      "epoch": 0.00034120272275824196,
      "grad_norm": 9528.038203114007,
      "learning_rate": 3.262994441790635e-07,
      "loss": 1.7545,
      "step": 94016
    },
    {
      "epoch": 0.00034131885710907866,
      "grad_norm": 9228.72753959071,
      "learning_rate": 3.262456081264776e-07,
      "loss": 1.7525,
      "step": 94048
    },
    {
      "epoch": 0.00034143499145991536,
      "grad_norm": 10926.195861323373,
      "learning_rate": 3.261900633679957e-07,
      "loss": 1.7464,
      "step": 94080
    },
    {
      "epoch": 0.00034155112581075206,
      "grad_norm": 8902.556711417232,
      "learning_rate": 3.2613454697007967e-07,
      "loss": 1.759,
      "step": 94112
    },
    {
      "epoch": 0.00034166726016158876,
      "grad_norm": 9066.838368472221,
      "learning_rate": 3.260790589086034e-07,
      "loss": 1.7772,
      "step": 94144
    },
    {
      "epoch": 0.00034178339451242546,
      "grad_norm": 9989.798396364164,
      "learning_rate": 3.2602359915946955e-07,
      "loss": 1.7559,
      "step": 94176
    },
    {
      "epoch": 0.00034189952886326216,
      "grad_norm": 8802.228127014205,
      "learning_rate": 3.259681676986093e-07,
      "loss": 1.7695,
      "step": 94208
    },
    {
      "epoch": 0.00034201566321409886,
      "grad_norm": 9796.756299918867,
      "learning_rate": 3.2591276450198265e-07,
      "loss": 1.7539,
      "step": 94240
    },
    {
      "epoch": 0.00034213179756493556,
      "grad_norm": 9321.05863086377,
      "learning_rate": 3.25857389545578e-07,
      "loss": 1.7475,
      "step": 94272
    },
    {
      "epoch": 0.00034224793191577226,
      "grad_norm": 10904.062729093226,
      "learning_rate": 3.258020428054124e-07,
      "loss": 1.7712,
      "step": 94304
    },
    {
      "epoch": 0.000342364066266609,
      "grad_norm": 9661.941833813738,
      "learning_rate": 3.257467242575315e-07,
      "loss": 1.7624,
      "step": 94336
    },
    {
      "epoch": 0.0003424802006174457,
      "grad_norm": 8409.01991911067,
      "learning_rate": 3.256914338780093e-07,
      "loss": 1.758,
      "step": 94368
    },
    {
      "epoch": 0.0003425963349682824,
      "grad_norm": 9622.571277990099,
      "learning_rate": 3.2563617164294817e-07,
      "loss": 1.7703,
      "step": 94400
    },
    {
      "epoch": 0.0003427124693191191,
      "grad_norm": 10235.444006001888,
      "learning_rate": 3.25580937528479e-07,
      "loss": 1.7807,
      "step": 94432
    },
    {
      "epoch": 0.0003428286036699558,
      "grad_norm": 10048.315281677818,
      "learning_rate": 3.2552573151076087e-07,
      "loss": 1.7755,
      "step": 94464
    },
    {
      "epoch": 0.0003429447380207925,
      "grad_norm": 9798.937289318674,
      "learning_rate": 3.254705535659811e-07,
      "loss": 1.7806,
      "step": 94496
    },
    {
      "epoch": 0.0003430608723716292,
      "grad_norm": 9077.045003744335,
      "learning_rate": 3.2541540367035565e-07,
      "loss": 1.7568,
      "step": 94528
    },
    {
      "epoch": 0.0003431770067224659,
      "grad_norm": 8312.676945485131,
      "learning_rate": 3.2536028180012814e-07,
      "loss": 1.762,
      "step": 94560
    },
    {
      "epoch": 0.0003432931410733026,
      "grad_norm": 9160.451408091198,
      "learning_rate": 3.2530518793157076e-07,
      "loss": 1.7757,
      "step": 94592
    },
    {
      "epoch": 0.00034340927542413937,
      "grad_norm": 9840.598457411012,
      "learning_rate": 3.2525012204098353e-07,
      "loss": 1.7832,
      "step": 94624
    },
    {
      "epoch": 0.00034352540977497606,
      "grad_norm": 9839.44612262296,
      "learning_rate": 3.2519508410469474e-07,
      "loss": 1.7823,
      "step": 94656
    },
    {
      "epoch": 0.00034364154412581276,
      "grad_norm": 11245.28985842517,
      "learning_rate": 3.251400740990607e-07,
      "loss": 1.7511,
      "step": 94688
    },
    {
      "epoch": 0.00034375767847664946,
      "grad_norm": 9548.263925971045,
      "learning_rate": 3.2508509200046554e-07,
      "loss": 1.7633,
      "step": 94720
    },
    {
      "epoch": 0.00034387381282748616,
      "grad_norm": 11295.929178248241,
      "learning_rate": 3.250301377853215e-07,
      "loss": 1.75,
      "step": 94752
    },
    {
      "epoch": 0.00034398994717832286,
      "grad_norm": 10160.020177145318,
      "learning_rate": 3.249752114300686e-07,
      "loss": 1.752,
      "step": 94784
    },
    {
      "epoch": 0.00034410608152915956,
      "grad_norm": 10164.944859663528,
      "learning_rate": 3.2492031291117493e-07,
      "loss": 1.7449,
      "step": 94816
    },
    {
      "epoch": 0.00034422221587999626,
      "grad_norm": 9863.655103459367,
      "learning_rate": 3.248654422051361e-07,
      "loss": 1.739,
      "step": 94848
    },
    {
      "epoch": 0.00034433835023083296,
      "grad_norm": 10284.510002912146,
      "learning_rate": 3.2481059928847577e-07,
      "loss": 1.747,
      "step": 94880
    },
    {
      "epoch": 0.0003444544845816697,
      "grad_norm": 8637.096155537462,
      "learning_rate": 3.247557841377451e-07,
      "loss": 1.7736,
      "step": 94912
    },
    {
      "epoch": 0.0003445706189325064,
      "grad_norm": 11347.26451617305,
      "learning_rate": 3.2470099672952317e-07,
      "loss": 1.7617,
      "step": 94944
    },
    {
      "epoch": 0.0003446867532833431,
      "grad_norm": 9899.3328058006,
      "learning_rate": 3.246462370404165e-07,
      "loss": 1.755,
      "step": 94976
    },
    {
      "epoch": 0.0003448028876341798,
      "grad_norm": 9657.941602639767,
      "learning_rate": 3.2459150504705933e-07,
      "loss": 1.7366,
      "step": 95008
    },
    {
      "epoch": 0.0003449190219850165,
      "grad_norm": 9970.049247621599,
      "learning_rate": 3.2453680072611355e-07,
      "loss": 1.7539,
      "step": 95040
    },
    {
      "epoch": 0.0003450351563358532,
      "grad_norm": 9724.287531742364,
      "learning_rate": 3.2448383228197815e-07,
      "loss": 1.7492,
      "step": 95072
    },
    {
      "epoch": 0.0003451512906866899,
      "grad_norm": 9258.283858253644,
      "learning_rate": 3.244291823729958e-07,
      "loss": 1.7471,
      "step": 95104
    },
    {
      "epoch": 0.0003452674250375266,
      "grad_norm": 9067.846712422966,
      "learning_rate": 3.243745600673013e-07,
      "loss": 1.7697,
      "step": 95136
    },
    {
      "epoch": 0.0003453835593883633,
      "grad_norm": 10110.204547881314,
      "learning_rate": 3.243199653416655e-07,
      "loss": 1.7723,
      "step": 95168
    },
    {
      "epoch": 0.00034549969373920007,
      "grad_norm": 9586.218441074667,
      "learning_rate": 3.242653981728867e-07,
      "loss": 1.7828,
      "step": 95200
    },
    {
      "epoch": 0.00034561582809003677,
      "grad_norm": 9601.53675199965,
      "learning_rate": 3.2421085853779017e-07,
      "loss": 1.7999,
      "step": 95232
    },
    {
      "epoch": 0.00034573196244087347,
      "grad_norm": 9913.77476040282,
      "learning_rate": 3.2415634641322867e-07,
      "loss": 1.7956,
      "step": 95264
    },
    {
      "epoch": 0.00034584809679171017,
      "grad_norm": 9393.49892212694,
      "learning_rate": 3.241018617760822e-07,
      "loss": 1.789,
      "step": 95296
    },
    {
      "epoch": 0.00034596423114254687,
      "grad_norm": 10264.82323276928,
      "learning_rate": 3.240474046032579e-07,
      "loss": 1.7972,
      "step": 95328
    },
    {
      "epoch": 0.00034608036549338357,
      "grad_norm": 9273.422561276931,
      "learning_rate": 3.239929748716901e-07,
      "loss": 1.7777,
      "step": 95360
    },
    {
      "epoch": 0.00034619649984422027,
      "grad_norm": 9940.91625555713,
      "learning_rate": 3.239385725583401e-07,
      "loss": 1.7564,
      "step": 95392
    },
    {
      "epoch": 0.00034631263419505697,
      "grad_norm": 9906.090046027242,
      "learning_rate": 3.2388419764019654e-07,
      "loss": 1.7621,
      "step": 95424
    },
    {
      "epoch": 0.00034642876854589367,
      "grad_norm": 10679.703928480414,
      "learning_rate": 3.238298500942749e-07,
      "loss": 1.7304,
      "step": 95456
    },
    {
      "epoch": 0.0003465449028967304,
      "grad_norm": 8901.136444297435,
      "learning_rate": 3.2377552989761763e-07,
      "loss": 1.7452,
      "step": 95488
    },
    {
      "epoch": 0.0003466610372475671,
      "grad_norm": 9746.911408235945,
      "learning_rate": 3.237212370272942e-07,
      "loss": 1.7473,
      "step": 95520
    },
    {
      "epoch": 0.0003467771715984038,
      "grad_norm": 12382.715049616543,
      "learning_rate": 3.236669714604009e-07,
      "loss": 1.761,
      "step": 95552
    },
    {
      "epoch": 0.0003468933059492405,
      "grad_norm": 10454.571440283911,
      "learning_rate": 3.236127331740611e-07,
      "loss": 1.7404,
      "step": 95584
    },
    {
      "epoch": 0.0003470094403000772,
      "grad_norm": 9838.197497509389,
      "learning_rate": 3.2355852214542476e-07,
      "loss": 1.7447,
      "step": 95616
    },
    {
      "epoch": 0.0003471255746509139,
      "grad_norm": 9166.433766738295,
      "learning_rate": 3.235043383516688e-07,
      "loss": 1.7578,
      "step": 95648
    },
    {
      "epoch": 0.0003472417090017506,
      "grad_norm": 9089.591079911132,
      "learning_rate": 3.234501817699966e-07,
      "loss": 1.7523,
      "step": 95680
    },
    {
      "epoch": 0.0003473578433525873,
      "grad_norm": 8114.23822672221,
      "learning_rate": 3.233960523776387e-07,
      "loss": 1.7512,
      "step": 95712
    },
    {
      "epoch": 0.000347473977703424,
      "grad_norm": 9265.121801681831,
      "learning_rate": 3.233419501518519e-07,
      "loss": 1.7523,
      "step": 95744
    },
    {
      "epoch": 0.0003475901120542608,
      "grad_norm": 9388.268317426808,
      "learning_rate": 3.232878750699198e-07,
      "loss": 1.7378,
      "step": 95776
    },
    {
      "epoch": 0.0003477062464050975,
      "grad_norm": 8671.62868208735,
      "learning_rate": 3.232338271091526e-07,
      "loss": 1.748,
      "step": 95808
    },
    {
      "epoch": 0.0003478223807559342,
      "grad_norm": 8081.769236992603,
      "learning_rate": 3.23179806246887e-07,
      "loss": 1.7623,
      "step": 95840
    },
    {
      "epoch": 0.0003479385151067709,
      "grad_norm": 11093.693704082514,
      "learning_rate": 3.2312581246048614e-07,
      "loss": 1.78,
      "step": 95872
    },
    {
      "epoch": 0.0003480546494576076,
      "grad_norm": 10801.640616128645,
      "learning_rate": 3.230718457273398e-07,
      "loss": 1.7849,
      "step": 95904
    },
    {
      "epoch": 0.0003481707838084443,
      "grad_norm": 10005.05202385275,
      "learning_rate": 3.2301790602486393e-07,
      "loss": 1.7754,
      "step": 95936
    },
    {
      "epoch": 0.000348286918159281,
      "grad_norm": 9413.760247637498,
      "learning_rate": 3.2296399333050115e-07,
      "loss": 1.7803,
      "step": 95968
    },
    {
      "epoch": 0.0003484030525101177,
      "grad_norm": 9060.77248362412,
      "learning_rate": 3.229101076217202e-07,
      "loss": 1.7727,
      "step": 96000
    },
    {
      "epoch": 0.0003485191868609544,
      "grad_norm": 9965.889122401473,
      "learning_rate": 3.228562488760163e-07,
      "loss": 1.7673,
      "step": 96032
    },
    {
      "epoch": 0.00034863532121179113,
      "grad_norm": 9225.553425133909,
      "learning_rate": 3.228040989072512e-07,
      "loss": 1.7784,
      "step": 96064
    },
    {
      "epoch": 0.00034875145556262783,
      "grad_norm": 9532.532507156742,
      "learning_rate": 3.2275029317943933e-07,
      "loss": 1.7807,
      "step": 96096
    },
    {
      "epoch": 0.00034886758991346453,
      "grad_norm": 9914.168447227434,
      "learning_rate": 3.2269651434804775e-07,
      "loss": 1.7826,
      "step": 96128
    },
    {
      "epoch": 0.00034898372426430123,
      "grad_norm": 9941.965097504619,
      "learning_rate": 3.226427623906755e-07,
      "loss": 1.7854,
      "step": 96160
    },
    {
      "epoch": 0.00034909985861513793,
      "grad_norm": 8911.553512154882,
      "learning_rate": 3.2258903728494806e-07,
      "loss": 1.7691,
      "step": 96192
    },
    {
      "epoch": 0.00034921599296597463,
      "grad_norm": 11006.965521886585,
      "learning_rate": 3.2253533900851655e-07,
      "loss": 1.7406,
      "step": 96224
    },
    {
      "epoch": 0.00034933212731681133,
      "grad_norm": 8712.892516265767,
      "learning_rate": 3.2248166753905846e-07,
      "loss": 1.7459,
      "step": 96256
    },
    {
      "epoch": 0.00034944826166764803,
      "grad_norm": 7899.052474822534,
      "learning_rate": 3.224280228542772e-07,
      "loss": 1.7362,
      "step": 96288
    },
    {
      "epoch": 0.00034956439601848473,
      "grad_norm": 9135.981392275271,
      "learning_rate": 3.2237440493190187e-07,
      "loss": 1.7387,
      "step": 96320
    },
    {
      "epoch": 0.0003496805303693215,
      "grad_norm": 9310.419539419263,
      "learning_rate": 3.2232081374968787e-07,
      "loss": 1.7371,
      "step": 96352
    },
    {
      "epoch": 0.0003497966647201582,
      "grad_norm": 9746.992356619554,
      "learning_rate": 3.222672492854163e-07,
      "loss": 1.744,
      "step": 96384
    },
    {
      "epoch": 0.0003499127990709949,
      "grad_norm": 11268.79248189441,
      "learning_rate": 3.2221371151689406e-07,
      "loss": 1.7561,
      "step": 96416
    },
    {
      "epoch": 0.0003500289334218316,
      "grad_norm": 8227.69578922313,
      "learning_rate": 3.2216020042195386e-07,
      "loss": 1.7908,
      "step": 96448
    },
    {
      "epoch": 0.0003501450677726683,
      "grad_norm": 10576.50093367367,
      "learning_rate": 3.2210671597845436e-07,
      "loss": 1.8009,
      "step": 96480
    },
    {
      "epoch": 0.000350261202123505,
      "grad_norm": 10005.995602637451,
      "learning_rate": 3.220532581642795e-07,
      "loss": 1.7573,
      "step": 96512
    },
    {
      "epoch": 0.0003503773364743417,
      "grad_norm": 9334.840223592475,
      "learning_rate": 3.219998269573395e-07,
      "loss": 1.7288,
      "step": 96544
    },
    {
      "epoch": 0.0003504934708251784,
      "grad_norm": 11482.212678747943,
      "learning_rate": 3.2194642233556977e-07,
      "loss": 1.7501,
      "step": 96576
    },
    {
      "epoch": 0.0003506096051760151,
      "grad_norm": 8805.568465465474,
      "learning_rate": 3.2189304427693145e-07,
      "loss": 1.7272,
      "step": 96608
    },
    {
      "epoch": 0.00035072573952685184,
      "grad_norm": 8734.674235482396,
      "learning_rate": 3.2183969275941133e-07,
      "loss": 1.7517,
      "step": 96640
    },
    {
      "epoch": 0.00035084187387768854,
      "grad_norm": 9398.979093497335,
      "learning_rate": 3.2178636776102165e-07,
      "loss": 1.7645,
      "step": 96672
    },
    {
      "epoch": 0.00035095800822852524,
      "grad_norm": 10771.263249962838,
      "learning_rate": 3.2173306925980017e-07,
      "loss": 1.7575,
      "step": 96704
    },
    {
      "epoch": 0.00035107414257936194,
      "grad_norm": 9475.842970416932,
      "learning_rate": 3.216797972338102e-07,
      "loss": 1.7733,
      "step": 96736
    },
    {
      "epoch": 0.00035119027693019864,
      "grad_norm": 9448.227294048338,
      "learning_rate": 3.216265516611402e-07,
      "loss": 1.7859,
      "step": 96768
    },
    {
      "epoch": 0.00035130641128103534,
      "grad_norm": 11674.876787358398,
      "learning_rate": 3.2157333251990433e-07,
      "loss": 1.7779,
      "step": 96800
    },
    {
      "epoch": 0.00035142254563187204,
      "grad_norm": 9120.008662276588,
      "learning_rate": 3.2152013978824183e-07,
      "loss": 1.7676,
      "step": 96832
    },
    {
      "epoch": 0.00035153867998270874,
      "grad_norm": 8806.057119960102,
      "learning_rate": 3.2146697344431744e-07,
      "loss": 1.7693,
      "step": 96864
    },
    {
      "epoch": 0.00035165481433354544,
      "grad_norm": 10796.291215042322,
      "learning_rate": 3.2141383346632107e-07,
      "loss": 1.7735,
      "step": 96896
    },
    {
      "epoch": 0.0003517709486843822,
      "grad_norm": 8723.956785771006,
      "learning_rate": 3.213607198324678e-07,
      "loss": 1.7715,
      "step": 96928
    },
    {
      "epoch": 0.0003518870830352189,
      "grad_norm": 10104.703756172172,
      "learning_rate": 3.2130763252099805e-07,
      "loss": 1.7578,
      "step": 96960
    },
    {
      "epoch": 0.0003520032173860556,
      "grad_norm": 9538.717523860323,
      "learning_rate": 3.212545715101773e-07,
      "loss": 1.756,
      "step": 96992
    },
    {
      "epoch": 0.0003521193517368923,
      "grad_norm": 9880.937809742554,
      "learning_rate": 3.2120153677829623e-07,
      "loss": 1.7607,
      "step": 97024
    },
    {
      "epoch": 0.000352235486087729,
      "grad_norm": 11155.074002443911,
      "learning_rate": 3.21150184421269e-07,
      "loss": 1.7918,
      "step": 97056
    },
    {
      "epoch": 0.0003523516204385657,
      "grad_norm": 9221.367360646685,
      "learning_rate": 3.210972013627044e-07,
      "loss": 1.786,
      "step": 97088
    },
    {
      "epoch": 0.0003524677547894024,
      "grad_norm": 8316.10822440401,
      "learning_rate": 3.210442445187773e-07,
      "loss": 1.7328,
      "step": 97120
    },
    {
      "epoch": 0.0003525838891402391,
      "grad_norm": 9195.85830686837,
      "learning_rate": 3.2099131386787765e-07,
      "loss": 1.7349,
      "step": 97152
    },
    {
      "epoch": 0.0003527000234910758,
      "grad_norm": 8912.969202235583,
      "learning_rate": 3.209384093884202e-07,
      "loss": 1.7545,
      "step": 97184
    },
    {
      "epoch": 0.00035281615784191255,
      "grad_norm": 9643.654079237807,
      "learning_rate": 3.2088553105884486e-07,
      "loss": 1.7551,
      "step": 97216
    },
    {
      "epoch": 0.00035293229219274925,
      "grad_norm": 8440.274640081328,
      "learning_rate": 3.2083267885761606e-07,
      "loss": 1.7607,
      "step": 97248
    },
    {
      "epoch": 0.00035304842654358595,
      "grad_norm": 9767.964271023928,
      "learning_rate": 3.2077985276322336e-07,
      "loss": 1.7451,
      "step": 97280
    },
    {
      "epoch": 0.00035316456089442265,
      "grad_norm": 9360.944503627825,
      "learning_rate": 3.2072705275418097e-07,
      "loss": 1.7353,
      "step": 97312
    },
    {
      "epoch": 0.00035328069524525935,
      "grad_norm": 9640.156845197074,
      "learning_rate": 3.206742788090278e-07,
      "loss": 1.7572,
      "step": 97344
    },
    {
      "epoch": 0.00035339682959609605,
      "grad_norm": 9553.96462208229,
      "learning_rate": 3.2062153090632754e-07,
      "loss": 1.7534,
      "step": 97376
    },
    {
      "epoch": 0.00035351296394693275,
      "grad_norm": 8890.923574072605,
      "learning_rate": 3.2056880902466855e-07,
      "loss": 1.7615,
      "step": 97408
    },
    {
      "epoch": 0.00035362909829776945,
      "grad_norm": 11489.298847188194,
      "learning_rate": 3.205161131426638e-07,
      "loss": 1.7649,
      "step": 97440
    },
    {
      "epoch": 0.00035374523264860615,
      "grad_norm": 9330.737591423305,
      "learning_rate": 3.20463443238951e-07,
      "loss": 1.7585,
      "step": 97472
    },
    {
      "epoch": 0.0003538613669994429,
      "grad_norm": 9040.82573662384,
      "learning_rate": 3.204107992921921e-07,
      "loss": 1.7647,
      "step": 97504
    },
    {
      "epoch": 0.0003539775013502796,
      "grad_norm": 9592.423572799526,
      "learning_rate": 3.2035818128107397e-07,
      "loss": 1.7633,
      "step": 97536
    },
    {
      "epoch": 0.0003540936357011163,
      "grad_norm": 9027.381015554844,
      "learning_rate": 3.203055891843077e-07,
      "loss": 1.7708,
      "step": 97568
    },
    {
      "epoch": 0.000354209770051953,
      "grad_norm": 9564.495804798076,
      "learning_rate": 3.202530229806289e-07,
      "loss": 1.7754,
      "step": 97600
    },
    {
      "epoch": 0.0003543259044027897,
      "grad_norm": 9088.338241945004,
      "learning_rate": 3.202004826487977e-07,
      "loss": 1.7894,
      "step": 97632
    },
    {
      "epoch": 0.0003544420387536264,
      "grad_norm": 9047.542262957382,
      "learning_rate": 3.2014796816759844e-07,
      "loss": 1.8021,
      "step": 97664
    },
    {
      "epoch": 0.0003545581731044631,
      "grad_norm": 8766.60892249677,
      "learning_rate": 3.2009547951583997e-07,
      "loss": 1.7993,
      "step": 97696
    },
    {
      "epoch": 0.0003546743074552998,
      "grad_norm": 9335.99485861041,
      "learning_rate": 3.2004301667235546e-07,
      "loss": 1.7383,
      "step": 97728
    },
    {
      "epoch": 0.0003547904418061365,
      "grad_norm": 8654.0177952209,
      "learning_rate": 3.199905796160021e-07,
      "loss": 1.7497,
      "step": 97760
    },
    {
      "epoch": 0.00035490657615697325,
      "grad_norm": 7963.147367718369,
      "learning_rate": 3.199381683256617e-07,
      "loss": 1.7406,
      "step": 97792
    },
    {
      "epoch": 0.00035502271050780995,
      "grad_norm": 9925.018488647767,
      "learning_rate": 3.1988578278023994e-07,
      "loss": 1.7542,
      "step": 97824
    },
    {
      "epoch": 0.00035513884485864665,
      "grad_norm": 9535.067697714578,
      "learning_rate": 3.1983342295866686e-07,
      "loss": 1.7625,
      "step": 97856
    },
    {
      "epoch": 0.00035525497920948335,
      "grad_norm": 9757.604111665936,
      "learning_rate": 3.197810888398966e-07,
      "loss": 1.7453,
      "step": 97888
    },
    {
      "epoch": 0.00035537111356032005,
      "grad_norm": 10748.240041978965,
      "learning_rate": 3.197287804029073e-07,
      "loss": 1.764,
      "step": 97920
    },
    {
      "epoch": 0.00035548724791115675,
      "grad_norm": 8309.835497770096,
      "learning_rate": 3.1967649762670137e-07,
      "loss": 1.751,
      "step": 97952
    },
    {
      "epoch": 0.00035560338226199345,
      "grad_norm": 11946.105976425959,
      "learning_rate": 3.196242404903051e-07,
      "loss": 1.7695,
      "step": 97984
    },
    {
      "epoch": 0.00035571951661283015,
      "grad_norm": 9655.830363050089,
      "learning_rate": 3.195720089727686e-07,
      "loss": 1.7621,
      "step": 98016
    },
    {
      "epoch": 0.00035583565096366685,
      "grad_norm": 9088.215336357298,
      "learning_rate": 3.195198030531665e-07,
      "loss": 1.7282,
      "step": 98048
    },
    {
      "epoch": 0.0003559517853145036,
      "grad_norm": 10402.264945674091,
      "learning_rate": 3.19469252959357e-07,
      "loss": 1.7398,
      "step": 98080
    },
    {
      "epoch": 0.0003560679196653403,
      "grad_norm": 9619.26244573876,
      "learning_rate": 3.194170973746275e-07,
      "loss": 1.7402,
      "step": 98112
    },
    {
      "epoch": 0.000356184054016177,
      "grad_norm": 9599.460401501743,
      "learning_rate": 3.193649673258497e-07,
      "loss": 1.7432,
      "step": 98144
    },
    {
      "epoch": 0.0003563001883670137,
      "grad_norm": 11253.269569329617,
      "learning_rate": 3.1931286279219256e-07,
      "loss": 1.7715,
      "step": 98176
    },
    {
      "epoch": 0.0003564163227178504,
      "grad_norm": 10289.991010686064,
      "learning_rate": 3.192607837528489e-07,
      "loss": 1.7944,
      "step": 98208
    },
    {
      "epoch": 0.0003565324570686871,
      "grad_norm": 8638.567705354864,
      "learning_rate": 3.1920873018703526e-07,
      "loss": 1.7751,
      "step": 98240
    },
    {
      "epoch": 0.0003566485914195238,
      "grad_norm": 9452.919125857368,
      "learning_rate": 3.191567020739917e-07,
      "loss": 1.7883,
      "step": 98272
    },
    {
      "epoch": 0.0003567647257703605,
      "grad_norm": 10195.759510698554,
      "learning_rate": 3.191046993929823e-07,
      "loss": 1.7791,
      "step": 98304
    },
    {
      "epoch": 0.0003568808601211972,
      "grad_norm": 8501.913549313473,
      "learning_rate": 3.1905272212329455e-07,
      "loss": 1.7596,
      "step": 98336
    },
    {
      "epoch": 0.00035699699447203396,
      "grad_norm": 11176.900464797922,
      "learning_rate": 3.190007702442397e-07,
      "loss": 1.7589,
      "step": 98368
    },
    {
      "epoch": 0.00035711312882287066,
      "grad_norm": 9451.230501897624,
      "learning_rate": 3.1894884373515235e-07,
      "loss": 1.7531,
      "step": 98400
    },
    {
      "epoch": 0.00035722926317370736,
      "grad_norm": 9768.32145253216,
      "learning_rate": 3.1889694257539086e-07,
      "loss": 1.7754,
      "step": 98432
    },
    {
      "epoch": 0.00035734539752454406,
      "grad_norm": 10095.509298693158,
      "learning_rate": 3.1884506674433705e-07,
      "loss": 1.7641,
      "step": 98464
    },
    {
      "epoch": 0.00035746153187538076,
      "grad_norm": 8557.504309084514,
      "learning_rate": 3.1879321622139607e-07,
      "loss": 1.7405,
      "step": 98496
    },
    {
      "epoch": 0.00035757766622621746,
      "grad_norm": 8988.47528783386,
      "learning_rate": 3.1874139098599666e-07,
      "loss": 1.7401,
      "step": 98528
    },
    {
      "epoch": 0.00035769380057705416,
      "grad_norm": 10416.419922410962,
      "learning_rate": 3.186895910175909e-07,
      "loss": 1.748,
      "step": 98560
    },
    {
      "epoch": 0.00035780993492789086,
      "grad_norm": 9889.529816932652,
      "learning_rate": 3.186378162956542e-07,
      "loss": 1.7651,
      "step": 98592
    },
    {
      "epoch": 0.00035792606927872756,
      "grad_norm": 10294.479297176715,
      "learning_rate": 3.185860667996855e-07,
      "loss": 1.7695,
      "step": 98624
    },
    {
      "epoch": 0.0003580422036295643,
      "grad_norm": 9383.22204788952,
      "learning_rate": 3.1853434250920683e-07,
      "loss": 1.7389,
      "step": 98656
    },
    {
      "epoch": 0.000358158337980401,
      "grad_norm": 10404.58187530859,
      "learning_rate": 3.1848264340376344e-07,
      "loss": 1.7574,
      "step": 98688
    },
    {
      "epoch": 0.0003582744723312377,
      "grad_norm": 8260.14685099484,
      "learning_rate": 3.18430969462924e-07,
      "loss": 1.7688,
      "step": 98720
    },
    {
      "epoch": 0.0003583906066820744,
      "grad_norm": 10176.563172309205,
      "learning_rate": 3.183793206662803e-07,
      "loss": 1.777,
      "step": 98752
    },
    {
      "epoch": 0.0003585067410329111,
      "grad_norm": 10026.577282403003,
      "learning_rate": 3.183276969934472e-07,
      "loss": 1.7795,
      "step": 98784
    },
    {
      "epoch": 0.0003586228753837478,
      "grad_norm": 9678.22235743734,
      "learning_rate": 3.1827609842406296e-07,
      "loss": 1.7472,
      "step": 98816
    },
    {
      "epoch": 0.0003587390097345845,
      "grad_norm": 8863.826713107606,
      "learning_rate": 3.182245249377886e-07,
      "loss": 1.7594,
      "step": 98848
    },
    {
      "epoch": 0.0003588551440854212,
      "grad_norm": 8486.441185797496,
      "learning_rate": 3.181729765143084e-07,
      "loss": 1.7348,
      "step": 98880
    },
    {
      "epoch": 0.0003589712784362579,
      "grad_norm": 9103.78712404898,
      "learning_rate": 3.181214531333297e-07,
      "loss": 1.7636,
      "step": 98912
    },
    {
      "epoch": 0.00035908741278709467,
      "grad_norm": 10461.898298110147,
      "learning_rate": 3.180699547745826e-07,
      "loss": 1.772,
      "step": 98944
    },
    {
      "epoch": 0.00035920354713793137,
      "grad_norm": 9877.178443259998,
      "learning_rate": 3.180184814178205e-07,
      "loss": 1.7478,
      "step": 98976
    },
    {
      "epoch": 0.00035931968148876807,
      "grad_norm": 9379.36586342595,
      "learning_rate": 3.1796703304281944e-07,
      "loss": 1.7668,
      "step": 99008
    },
    {
      "epoch": 0.00035943581583960477,
      "grad_norm": 11276.755029706019,
      "learning_rate": 3.1791560962937845e-07,
      "loss": 1.7851,
      "step": 99040
    },
    {
      "epoch": 0.00035955195019044147,
      "grad_norm": 9155.05193868391,
      "learning_rate": 3.17865816982241e-07,
      "loss": 1.7647,
      "step": 99072
    },
    {
      "epoch": 0.00035966808454127817,
      "grad_norm": 8998.930158635525,
      "learning_rate": 3.178144426529253e-07,
      "loss": 1.7559,
      "step": 99104
    },
    {
      "epoch": 0.00035978421889211487,
      "grad_norm": 9069.55456458585,
      "learning_rate": 3.177630932253328e-07,
      "loss": 1.7601,
      "step": 99136
    },
    {
      "epoch": 0.00035990035324295157,
      "grad_norm": 9614.345323525675,
      "learning_rate": 3.1771176867935283e-07,
      "loss": 1.7531,
      "step": 99168
    },
    {
      "epoch": 0.00036001648759378827,
      "grad_norm": 8108.8402376665435,
      "learning_rate": 3.1766046899489796e-07,
      "loss": 1.7708,
      "step": 99200
    },
    {
      "epoch": 0.000360132621944625,
      "grad_norm": 9950.18964643388,
      "learning_rate": 3.1760919415190306e-07,
      "loss": 1.7535,
      "step": 99232
    },
    {
      "epoch": 0.0003602487562954617,
      "grad_norm": 9187.47429928378,
      "learning_rate": 3.175579441303258e-07,
      "loss": 1.7351,
      "step": 99264
    },
    {
      "epoch": 0.0003603648906462984,
      "grad_norm": 8763.467806753215,
      "learning_rate": 3.175067189101466e-07,
      "loss": 1.7379,
      "step": 99296
    },
    {
      "epoch": 0.0003604810249971351,
      "grad_norm": 9702.578007931706,
      "learning_rate": 3.174555184713682e-07,
      "loss": 1.7564,
      "step": 99328
    },
    {
      "epoch": 0.0003605971593479718,
      "grad_norm": 9189.765394176284,
      "learning_rate": 3.174043427940162e-07,
      "loss": 1.7632,
      "step": 99360
    },
    {
      "epoch": 0.0003607132936988085,
      "grad_norm": 9332.313646679477,
      "learning_rate": 3.173531918581385e-07,
      "loss": 1.7587,
      "step": 99392
    },
    {
      "epoch": 0.0003608294280496452,
      "grad_norm": 9678.937131730941,
      "learning_rate": 3.1730206564380564e-07,
      "loss": 1.7677,
      "step": 99424
    },
    {
      "epoch": 0.0003609455624004819,
      "grad_norm": 7504.004664177655,
      "learning_rate": 3.1725096413111066e-07,
      "loss": 1.7677,
      "step": 99456
    },
    {
      "epoch": 0.0003610616967513186,
      "grad_norm": 9166.364601083682,
      "learning_rate": 3.171998873001689e-07,
      "loss": 1.76,
      "step": 99488
    },
    {
      "epoch": 0.0003611778311021554,
      "grad_norm": 8036.549010613947,
      "learning_rate": 3.171488351311181e-07,
      "loss": 1.7808,
      "step": 99520
    },
    {
      "epoch": 0.0003612939654529921,
      "grad_norm": 11392.260881844306,
      "learning_rate": 3.1709780760411865e-07,
      "loss": 1.7557,
      "step": 99552
    },
    {
      "epoch": 0.0003614100998038288,
      "grad_norm": 9769.842168633022,
      "learning_rate": 3.170468046993528e-07,
      "loss": 1.7454,
      "step": 99584
    },
    {
      "epoch": 0.00036152623415466547,
      "grad_norm": 10352.864820908268,
      "learning_rate": 3.169958263970256e-07,
      "loss": 1.7647,
      "step": 99616
    },
    {
      "epoch": 0.00036164236850550217,
      "grad_norm": 9554.777025132507,
      "learning_rate": 3.1694487267736396e-07,
      "loss": 1.7559,
      "step": 99648
    },
    {
      "epoch": 0.00036175850285633887,
      "grad_norm": 9741.846642192639,
      "learning_rate": 3.1689394352061735e-07,
      "loss": 1.7489,
      "step": 99680
    },
    {
      "epoch": 0.00036187463720717557,
      "grad_norm": 10160.918954504066,
      "learning_rate": 3.1684303890705723e-07,
      "loss": 1.7572,
      "step": 99712
    },
    {
      "epoch": 0.00036199077155801227,
      "grad_norm": 9144.700213785032,
      "learning_rate": 3.1679215881697737e-07,
      "loss": 1.7584,
      "step": 99744
    },
    {
      "epoch": 0.00036210690590884897,
      "grad_norm": 9054.183342521843,
      "learning_rate": 3.1674130323069364e-07,
      "loss": 1.7589,
      "step": 99776
    },
    {
      "epoch": 0.00036222304025968567,
      "grad_norm": 9243.086497485567,
      "learning_rate": 3.166904721285439e-07,
      "loss": 1.7739,
      "step": 99808
    },
    {
      "epoch": 0.0003623391746105224,
      "grad_norm": 10637.518695635745,
      "learning_rate": 3.1663966549088837e-07,
      "loss": 1.7723,
      "step": 99840
    },
    {
      "epoch": 0.0003624553089613591,
      "grad_norm": 10887.338701445824,
      "learning_rate": 3.165888832981091e-07,
      "loss": 1.7752,
      "step": 99872
    },
    {
      "epoch": 0.0003625714433121958,
      "grad_norm": 9537.326984013916,
      "learning_rate": 3.1653812553061016e-07,
      "loss": 1.7735,
      "step": 99904
    },
    {
      "epoch": 0.0003626875776630325,
      "grad_norm": 9561.130895453738,
      "learning_rate": 3.164873921688177e-07,
      "loss": 1.7771,
      "step": 99936
    },
    {
      "epoch": 0.0003628037120138692,
      "grad_norm": 9406.539002204796,
      "learning_rate": 3.1643668319317976e-07,
      "loss": 1.7793,
      "step": 99968
    },
    {
      "epoch": 0.0003629198463647059,
      "grad_norm": 8426.897293784943,
      "learning_rate": 3.1638599858416636e-07,
      "loss": 1.7555,
      "step": 100000
    },
    {
      "epoch": 0.0003630359807155426,
      "grad_norm": 9552.949282813135,
      "learning_rate": 3.163353383222693e-07,
      "loss": 1.7484,
      "step": 100032
    },
    {
      "epoch": 0.0003631521150663793,
      "grad_norm": 10054.882595038094,
      "learning_rate": 3.162862843929012e-07,
      "loss": 1.7323,
      "step": 100064
    },
    {
      "epoch": 0.000363268249417216,
      "grad_norm": 8341.429733564864,
      "learning_rate": 3.1623567200746427e-07,
      "loss": 1.7308,
      "step": 100096
    },
    {
      "epoch": 0.0003633843837680528,
      "grad_norm": 10199.983529398467,
      "learning_rate": 3.1618508391135756e-07,
      "loss": 1.7536,
      "step": 100128
    },
    {
      "epoch": 0.0003635005181188895,
      "grad_norm": 9472.897339251596,
      "learning_rate": 3.161345200851597e-07,
      "loss": 1.758,
      "step": 100160
    },
    {
      "epoch": 0.0003636166524697262,
      "grad_norm": 9994.513394858202,
      "learning_rate": 3.1608398050947067e-07,
      "loss": 1.7556,
      "step": 100192
    },
    {
      "epoch": 0.0003637327868205629,
      "grad_norm": 10698.132734267228,
      "learning_rate": 3.160334651649124e-07,
      "loss": 1.7639,
      "step": 100224
    },
    {
      "epoch": 0.0003638489211713996,
      "grad_norm": 9692.591603900373,
      "learning_rate": 3.1598297403212853e-07,
      "loss": 1.7612,
      "step": 100256
    },
    {
      "epoch": 0.0003639650555222363,
      "grad_norm": 9300.498481264323,
      "learning_rate": 3.159325070917842e-07,
      "loss": 1.7593,
      "step": 100288
    },
    {
      "epoch": 0.000364081189873073,
      "grad_norm": 11205.638223680078,
      "learning_rate": 3.158820643245661e-07,
      "loss": 1.7405,
      "step": 100320
    },
    {
      "epoch": 0.0003641973242239097,
      "grad_norm": 9091.472378003466,
      "learning_rate": 3.158316457111828e-07,
      "loss": 1.7411,
      "step": 100352
    },
    {
      "epoch": 0.0003643134585747464,
      "grad_norm": 9082.119246079077,
      "learning_rate": 3.157812512323641e-07,
      "loss": 1.7433,
      "step": 100384
    },
    {
      "epoch": 0.00036442959292558313,
      "grad_norm": 8282.638227038533,
      "learning_rate": 3.1573088086886146e-07,
      "loss": 1.7502,
      "step": 100416
    },
    {
      "epoch": 0.00036454572727641983,
      "grad_norm": 9480.259912048825,
      "learning_rate": 3.156805346014478e-07,
      "loss": 1.7853,
      "step": 100448
    },
    {
      "epoch": 0.00036466186162725653,
      "grad_norm": 9778.206890836376,
      "learning_rate": 3.156302124109176e-07,
      "loss": 1.7934,
      "step": 100480
    },
    {
      "epoch": 0.00036477799597809323,
      "grad_norm": 9369.411080745684,
      "learning_rate": 3.155799142780865e-07,
      "loss": 1.7823,
      "step": 100512
    },
    {
      "epoch": 0.00036489413032892993,
      "grad_norm": 10149.372394389713,
      "learning_rate": 3.155296401837918e-07,
      "loss": 1.7672,
      "step": 100544
    },
    {
      "epoch": 0.00036501026467976663,
      "grad_norm": 10956.011317993423,
      "learning_rate": 3.1547939010889204e-07,
      "loss": 1.7909,
      "step": 100576
    },
    {
      "epoch": 0.00036512639903060333,
      "grad_norm": 9121.335757442546,
      "learning_rate": 3.1542916403426714e-07,
      "loss": 1.7743,
      "step": 100608
    },
    {
      "epoch": 0.00036524253338144003,
      "grad_norm": 8898.057540834405,
      "learning_rate": 3.1537896194081823e-07,
      "loss": 1.7593,
      "step": 100640
    },
    {
      "epoch": 0.00036535866773227673,
      "grad_norm": 10288.115182092393,
      "learning_rate": 3.153287838094678e-07,
      "loss": 1.7606,
      "step": 100672
    },
    {
      "epoch": 0.0003654748020831135,
      "grad_norm": 8291.557995937796,
      "learning_rate": 3.152786296211596e-07,
      "loss": 1.7628,
      "step": 100704
    },
    {
      "epoch": 0.0003655909364339502,
      "grad_norm": 9245.425355277062,
      "learning_rate": 3.152284993568586e-07,
      "loss": 1.77,
      "step": 100736
    },
    {
      "epoch": 0.0003657070707847869,
      "grad_norm": 8018.388117321336,
      "learning_rate": 3.151783929975507e-07,
      "loss": 1.7682,
      "step": 100768
    },
    {
      "epoch": 0.0003658232051356236,
      "grad_norm": 10235.833136584437,
      "learning_rate": 3.151283105242433e-07,
      "loss": 1.7513,
      "step": 100800
    },
    {
      "epoch": 0.0003659393394864603,
      "grad_norm": 9987.629148101165,
      "learning_rate": 3.1507825191796465e-07,
      "loss": 1.7276,
      "step": 100832
    },
    {
      "epoch": 0.000366055473837297,
      "grad_norm": 9312.541114003203,
      "learning_rate": 3.150282171597644e-07,
      "loss": 1.7389,
      "step": 100864
    },
    {
      "epoch": 0.0003661716081881337,
      "grad_norm": 8942.484106779279,
      "learning_rate": 3.1497820623071286e-07,
      "loss": 1.7371,
      "step": 100896
    },
    {
      "epoch": 0.0003662877425389704,
      "grad_norm": 9034.923242618057,
      "learning_rate": 3.149282191119017e-07,
      "loss": 1.7281,
      "step": 100928
    },
    {
      "epoch": 0.0003664038768898071,
      "grad_norm": 9748.094377877145,
      "learning_rate": 3.1487825578444335e-07,
      "loss": 1.7447,
      "step": 100960
    },
    {
      "epoch": 0.00036652001124064384,
      "grad_norm": 8546.498347276503,
      "learning_rate": 3.148283162294714e-07,
      "loss": 1.7502,
      "step": 100992
    },
    {
      "epoch": 0.00036663614559148054,
      "grad_norm": 9752.565816235234,
      "learning_rate": 3.1477840042814025e-07,
      "loss": 1.7611,
      "step": 101024
    },
    {
      "epoch": 0.00036675227994231724,
      "grad_norm": 9296.339494661326,
      "learning_rate": 3.147285083616253e-07,
      "loss": 1.7745,
      "step": 101056
    },
    {
      "epoch": 0.00036686841429315394,
      "grad_norm": 10690.8860250215,
      "learning_rate": 3.1468019803828005e-07,
      "loss": 1.7641,
      "step": 101088
    },
    {
      "epoch": 0.00036698454864399064,
      "grad_norm": 8755.321010676878,
      "learning_rate": 3.146303526447527e-07,
      "loss": 1.7508,
      "step": 101120
    },
    {
      "epoch": 0.00036710068299482734,
      "grad_norm": 10005.65620036987,
      "learning_rate": 3.1458053093027873e-07,
      "loss": 1.7628,
      "step": 101152
    },
    {
      "epoch": 0.00036721681734566404,
      "grad_norm": 8496.432898575731,
      "learning_rate": 3.1453073287611625e-07,
      "loss": 1.7815,
      "step": 101184
    },
    {
      "epoch": 0.00036733295169650074,
      "grad_norm": 9044.410760243036,
      "learning_rate": 3.1448095846354405e-07,
      "loss": 1.7799,
      "step": 101216
    },
    {
      "epoch": 0.00036744908604733744,
      "grad_norm": 8923.140590621668,
      "learning_rate": 3.1443120767386164e-07,
      "loss": 1.7635,
      "step": 101248
    },
    {
      "epoch": 0.0003675652203981742,
      "grad_norm": 10093.606788457731,
      "learning_rate": 3.143814804883892e-07,
      "loss": 1.7716,
      "step": 101280
    },
    {
      "epoch": 0.0003676813547490109,
      "grad_norm": 10503.396212654268,
      "learning_rate": 3.143317768884678e-07,
      "loss": 1.7685,
      "step": 101312
    },
    {
      "epoch": 0.0003677974890998476,
      "grad_norm": 9343.81271216413,
      "learning_rate": 3.142820968554588e-07,
      "loss": 1.785,
      "step": 101344
    },
    {
      "epoch": 0.0003679136234506843,
      "grad_norm": 10112.706264892697,
      "learning_rate": 3.142324403707446e-07,
      "loss": 1.797,
      "step": 101376
    },
    {
      "epoch": 0.000368029757801521,
      "grad_norm": 10291.335967696323,
      "learning_rate": 3.1418280741572784e-07,
      "loss": 1.7644,
      "step": 101408
    },
    {
      "epoch": 0.0003681458921523577,
      "grad_norm": 9297.903849793243,
      "learning_rate": 3.1413319797183174e-07,
      "loss": 1.7539,
      "step": 101440
    },
    {
      "epoch": 0.0003682620265031944,
      "grad_norm": 10005.924045284373,
      "learning_rate": 3.140836120205003e-07,
      "loss": 1.748,
      "step": 101472
    },
    {
      "epoch": 0.0003683781608540311,
      "grad_norm": 9567.719791047395,
      "learning_rate": 3.1403404954319777e-07,
      "loss": 1.7522,
      "step": 101504
    },
    {
      "epoch": 0.0003684942952048678,
      "grad_norm": 9603.856725295313,
      "learning_rate": 3.1398451052140893e-07,
      "loss": 1.7309,
      "step": 101536
    },
    {
      "epoch": 0.00036861042955570455,
      "grad_norm": 9064.1376865094,
      "learning_rate": 3.1393499493663904e-07,
      "loss": 1.7355,
      "step": 101568
    },
    {
      "epoch": 0.00036872656390654125,
      "grad_norm": 8845.818560201198,
      "learning_rate": 3.138855027704138e-07,
      "loss": 1.7362,
      "step": 101600
    },
    {
      "epoch": 0.00036884269825737795,
      "grad_norm": 9449.484007076788,
      "learning_rate": 3.1383603400427923e-07,
      "loss": 1.7639,
      "step": 101632
    },
    {
      "epoch": 0.00036895883260821465,
      "grad_norm": 8224.546552850194,
      "learning_rate": 3.137865886198017e-07,
      "loss": 1.7553,
      "step": 101664
    },
    {
      "epoch": 0.00036907496695905135,
      "grad_norm": 9508.613148088421,
      "learning_rate": 3.1373716659856795e-07,
      "loss": 1.7602,
      "step": 101696
    },
    {
      "epoch": 0.00036919110130988805,
      "grad_norm": 8255.729283352259,
      "learning_rate": 3.136877679221849e-07,
      "loss": 1.7576,
      "step": 101728
    },
    {
      "epoch": 0.00036930723566072475,
      "grad_norm": 9606.992037053014,
      "learning_rate": 3.136383925722799e-07,
      "loss": 1.7661,
      "step": 101760
    },
    {
      "epoch": 0.00036942337001156145,
      "grad_norm": 9287.282379684597,
      "learning_rate": 3.135890405305004e-07,
      "loss": 1.777,
      "step": 101792
    },
    {
      "epoch": 0.00036953950436239815,
      "grad_norm": 8690.407355239455,
      "learning_rate": 3.135397117785141e-07,
      "loss": 1.7341,
      "step": 101824
    },
    {
      "epoch": 0.0003696556387132349,
      "grad_norm": 9478.986443707998,
      "learning_rate": 3.1349040629800903e-07,
      "loss": 1.7292,
      "step": 101856
    },
    {
      "epoch": 0.0003697717730640716,
      "grad_norm": 8356.370503992746,
      "learning_rate": 3.1344112407069314e-07,
      "loss": 1.7474,
      "step": 101888
    },
    {
      "epoch": 0.0003698879074149083,
      "grad_norm": 10035.781783199553,
      "learning_rate": 3.1339186507829456e-07,
      "loss": 1.7384,
      "step": 101920
    },
    {
      "epoch": 0.000370004041765745,
      "grad_norm": 8790.801897438027,
      "learning_rate": 3.133426293025616e-07,
      "loss": 1.7786,
      "step": 101952
    },
    {
      "epoch": 0.0003701201761165817,
      "grad_norm": 9935.054906743093,
      "learning_rate": 3.1329341672526256e-07,
      "loss": 1.7933,
      "step": 101984
    },
    {
      "epoch": 0.0003702363104674184,
      "grad_norm": 10150.176254627306,
      "learning_rate": 3.132442273281859e-07,
      "loss": 1.777,
      "step": 102016
    },
    {
      "epoch": 0.0003703524448182551,
      "grad_norm": 8926.001120322582,
      "learning_rate": 3.131950610931398e-07,
      "loss": 1.7677,
      "step": 102048
    },
    {
      "epoch": 0.0003704685791690918,
      "grad_norm": 10803.025594711882,
      "learning_rate": 3.131474533734107e-07,
      "loss": 1.7655,
      "step": 102080
    },
    {
      "epoch": 0.0003705847135199285,
      "grad_norm": 9723.741152457731,
      "learning_rate": 3.1309833268552727e-07,
      "loss": 1.7709,
      "step": 102112
    },
    {
      "epoch": 0.00037070084787076525,
      "grad_norm": 9494.695361095057,
      "learning_rate": 3.130492351057858e-07,
      "loss": 1.7706,
      "step": 102144
    },
    {
      "epoch": 0.00037081698222160195,
      "grad_norm": 9524.188259374128,
      "learning_rate": 3.1300016061607365e-07,
      "loss": 1.7681,
      "step": 102176
    },
    {
      "epoch": 0.00037093311657243865,
      "grad_norm": 9378.645424580247,
      "learning_rate": 3.129511091982983e-07,
      "loss": 1.7837,
      "step": 102208
    },
    {
      "epoch": 0.00037104925092327535,
      "grad_norm": 10758.030302987623,
      "learning_rate": 3.1290208083438704e-07,
      "loss": 1.7628,
      "step": 102240
    },
    {
      "epoch": 0.00037116538527411205,
      "grad_norm": 9453.035808670144,
      "learning_rate": 3.128530755062867e-07,
      "loss": 1.7615,
      "step": 102272
    },
    {
      "epoch": 0.00037128151962494875,
      "grad_norm": 8385.829714464753,
      "learning_rate": 3.1280409319596427e-07,
      "loss": 1.7599,
      "step": 102304
    },
    {
      "epoch": 0.00037139765397578545,
      "grad_norm": 10350.117294021358,
      "learning_rate": 3.127551338854063e-07,
      "loss": 1.7462,
      "step": 102336
    },
    {
      "epoch": 0.00037151378832662215,
      "grad_norm": 9691.52557650239,
      "learning_rate": 3.127061975566191e-07,
      "loss": 1.7582,
      "step": 102368
    },
    {
      "epoch": 0.00037162992267745885,
      "grad_norm": 8938.69431181087,
      "learning_rate": 3.1265728419162854e-07,
      "loss": 1.7445,
      "step": 102400
    },
    {
      "epoch": 0.0003717460570282956,
      "grad_norm": 10307.458270592222,
      "learning_rate": 3.1260839377248053e-07,
      "loss": 1.7341,
      "step": 102432
    },
    {
      "epoch": 0.0003718621913791323,
      "grad_norm": 8638.035424794229,
      "learning_rate": 3.125595262812402e-07,
      "loss": 1.7464,
      "step": 102464
    },
    {
      "epoch": 0.000371978325729969,
      "grad_norm": 8305.631342649396,
      "learning_rate": 3.1251068169999265e-07,
      "loss": 1.7666,
      "step": 102496
    },
    {
      "epoch": 0.0003720944600808057,
      "grad_norm": 10064.891852374769,
      "learning_rate": 3.124618600108423e-07,
      "loss": 1.7489,
      "step": 102528
    },
    {
      "epoch": 0.0003722105944316424,
      "grad_norm": 8780.02300680357,
      "learning_rate": 3.124130611959133e-07,
      "loss": 1.7673,
      "step": 102560
    },
    {
      "epoch": 0.0003723267287824791,
      "grad_norm": 9646.651854400054,
      "learning_rate": 3.123642852373493e-07,
      "loss": 1.7435,
      "step": 102592
    },
    {
      "epoch": 0.0003724428631333158,
      "grad_norm": 10117.37979913772,
      "learning_rate": 3.1231553211731345e-07,
      "loss": 1.7413,
      "step": 102624
    },
    {
      "epoch": 0.0003725589974841525,
      "grad_norm": 9738.398020208457,
      "learning_rate": 3.122668018179883e-07,
      "loss": 1.7226,
      "step": 102656
    },
    {
      "epoch": 0.0003726751318349892,
      "grad_norm": 8736.279642960155,
      "learning_rate": 3.1221809432157605e-07,
      "loss": 1.7261,
      "step": 102688
    },
    {
      "epoch": 0.00037279126618582596,
      "grad_norm": 8299.177308625234,
      "learning_rate": 3.1216940961029814e-07,
      "loss": 1.7669,
      "step": 102720
    },
    {
      "epoch": 0.00037290740053666266,
      "grad_norm": 9842.339356067743,
      "learning_rate": 3.121207476663955e-07,
      "loss": 1.7638,
      "step": 102752
    },
    {
      "epoch": 0.00037302353488749936,
      "grad_norm": 8037.29942206958,
      "learning_rate": 3.120721084721284e-07,
      "loss": 1.7678,
      "step": 102784
    },
    {
      "epoch": 0.00037313966923833606,
      "grad_norm": 8667.194701862882,
      "learning_rate": 3.120234920097766e-07,
      "loss": 1.7695,
      "step": 102816
    },
    {
      "epoch": 0.00037325580358917276,
      "grad_norm": 7676.779923900385,
      "learning_rate": 3.119748982616388e-07,
      "loss": 1.7737,
      "step": 102848
    },
    {
      "epoch": 0.00037337193794000946,
      "grad_norm": 10961.057613205034,
      "learning_rate": 3.119263272100334e-07,
      "loss": 1.79,
      "step": 102880
    },
    {
      "epoch": 0.00037348807229084616,
      "grad_norm": 10087.939531936141,
      "learning_rate": 3.1187777883729784e-07,
      "loss": 1.8032,
      "step": 102912
    },
    {
      "epoch": 0.00037360420664168286,
      "grad_norm": 9397.136159490294,
      "learning_rate": 3.118292531257889e-07,
      "loss": 1.7849,
      "step": 102944
    },
    {
      "epoch": 0.00037372034099251956,
      "grad_norm": 10088.410974975197,
      "learning_rate": 3.117807500578825e-07,
      "loss": 1.7871,
      "step": 102976
    },
    {
      "epoch": 0.0003738364753433563,
      "grad_norm": 10162.250636547005,
      "learning_rate": 3.1173226961597377e-07,
      "loss": 1.7658,
      "step": 103008
    },
    {
      "epoch": 0.000373952609694193,
      "grad_norm": 11392.207512154962,
      "learning_rate": 3.1168381178247697e-07,
      "loss": 1.7444,
      "step": 103040
    },
    {
      "epoch": 0.0003740687440450297,
      "grad_norm": 9739.570832434047,
      "learning_rate": 3.1163688979938157e-07,
      "loss": 1.7574,
      "step": 103072
    },
    {
      "epoch": 0.0003741848783958664,
      "grad_norm": 9887.785899785655,
      "learning_rate": 3.1158847642487795e-07,
      "loss": 1.7334,
      "step": 103104
    },
    {
      "epoch": 0.0003743010127467031,
      "grad_norm": 10930.106312383243,
      "learning_rate": 3.1154008560669125e-07,
      "loss": 1.746,
      "step": 103136
    },
    {
      "epoch": 0.0003744171470975398,
      "grad_norm": 9251.974600051602,
      "learning_rate": 3.1149171732731166e-07,
      "loss": 1.7409,
      "step": 103168
    },
    {
      "epoch": 0.0003745332814483765,
      "grad_norm": 9315.367303547404,
      "learning_rate": 3.114433715692481e-07,
      "loss": 1.7449,
      "step": 103200
    },
    {
      "epoch": 0.0003746494157992132,
      "grad_norm": 9529.608176625103,
      "learning_rate": 3.1139504831502874e-07,
      "loss": 1.759,
      "step": 103232
    },
    {
      "epoch": 0.0003747655501500499,
      "grad_norm": 8883.079421011613,
      "learning_rate": 3.113467475472006e-07,
      "loss": 1.738,
      "step": 103264
    },
    {
      "epoch": 0.00037488168450088667,
      "grad_norm": 9915.67970438739,
      "learning_rate": 3.112984692483296e-07,
      "loss": 1.7479,
      "step": 103296
    },
    {
      "epoch": 0.00037499781885172337,
      "grad_norm": 11421.897215436671,
      "learning_rate": 3.112502134010007e-07,
      "loss": 1.7671,
      "step": 103328
    },
    {
      "epoch": 0.00037511395320256007,
      "grad_norm": 9459.084733736134,
      "learning_rate": 3.112019799878177e-07,
      "loss": 1.7401,
      "step": 103360
    },
    {
      "epoch": 0.00037523008755339677,
      "grad_norm": 9847.858447398601,
      "learning_rate": 3.1115376899140335e-07,
      "loss": 1.7328,
      "step": 103392
    },
    {
      "epoch": 0.00037534622190423347,
      "grad_norm": 9637.312177158112,
      "learning_rate": 3.111055803943989e-07,
      "loss": 1.7404,
      "step": 103424
    },
    {
      "epoch": 0.00037546235625507017,
      "grad_norm": 9140.10667333812,
      "learning_rate": 3.110574141794649e-07,
      "loss": 1.7461,
      "step": 103456
    },
    {
      "epoch": 0.00037557849060590687,
      "grad_norm": 9108.592426934032,
      "learning_rate": 3.110092703292804e-07,
      "loss": 1.7794,
      "step": 103488
    },
    {
      "epoch": 0.00037569462495674357,
      "grad_norm": 9100.772439743783,
      "learning_rate": 3.1096114882654307e-07,
      "loss": 1.7934,
      "step": 103520
    },
    {
      "epoch": 0.00037581075930758027,
      "grad_norm": 9971.044980341829,
      "learning_rate": 3.1091304965396975e-07,
      "loss": 1.7802,
      "step": 103552
    },
    {
      "epoch": 0.000375926893658417,
      "grad_norm": 10934.665518432652,
      "learning_rate": 3.1086497279429557e-07,
      "loss": 1.7636,
      "step": 103584
    },
    {
      "epoch": 0.0003760430280092537,
      "grad_norm": 9524.543453625482,
      "learning_rate": 3.1081691823027466e-07,
      "loss": 1.7573,
      "step": 103616
    },
    {
      "epoch": 0.0003761591623600904,
      "grad_norm": 9478.038193634799,
      "learning_rate": 3.1076888594467954e-07,
      "loss": 1.7746,
      "step": 103648
    },
    {
      "epoch": 0.0003762752967109271,
      "grad_norm": 9631.602255076774,
      "learning_rate": 3.1072087592030147e-07,
      "loss": 1.7659,
      "step": 103680
    },
    {
      "epoch": 0.0003763914310617638,
      "grad_norm": 10533.182140265115,
      "learning_rate": 3.1067288813995045e-07,
      "loss": 1.7781,
      "step": 103712
    },
    {
      "epoch": 0.0003765075654126005,
      "grad_norm": 9723.020621185578,
      "learning_rate": 3.106249225864547e-07,
      "loss": 1.771,
      "step": 103744
    },
    {
      "epoch": 0.0003766236997634372,
      "grad_norm": 8231.396114876261,
      "learning_rate": 3.1057697924266146e-07,
      "loss": 1.7568,
      "step": 103776
    },
    {
      "epoch": 0.0003767398341142739,
      "grad_norm": 10585.449163828618,
      "learning_rate": 3.1052905809143613e-07,
      "loss": 1.7553,
      "step": 103808
    },
    {
      "epoch": 0.0003768559684651106,
      "grad_norm": 10266.333230516142,
      "learning_rate": 3.104811591156628e-07,
      "loss": 1.7547,
      "step": 103840
    },
    {
      "epoch": 0.0003769721028159474,
      "grad_norm": 9559.591204648868,
      "learning_rate": 3.104332822982439e-07,
      "loss": 1.7371,
      "step": 103872
    },
    {
      "epoch": 0.0003770882371667841,
      "grad_norm": 9480.272147992377,
      "learning_rate": 3.1038542762210043e-07,
      "loss": 1.7577,
      "step": 103904
    },
    {
      "epoch": 0.0003772043715176208,
      "grad_norm": 10748.866917028976,
      "learning_rate": 3.103375950701718e-07,
      "loss": 1.7614,
      "step": 103936
    },
    {
      "epoch": 0.0003773205058684575,
      "grad_norm": 9237.340309851099,
      "learning_rate": 3.102897846254157e-07,
      "loss": 1.7292,
      "step": 103968
    },
    {
      "epoch": 0.0003774366402192942,
      "grad_norm": 9430.29310254989,
      "learning_rate": 3.102419962708084e-07,
      "loss": 1.7462,
      "step": 104000
    },
    {
      "epoch": 0.0003775527745701309,
      "grad_norm": 8669.276382720764,
      "learning_rate": 3.101942299893442e-07,
      "loss": 1.7561,
      "step": 104032
    },
    {
      "epoch": 0.0003776689089209676,
      "grad_norm": 10126.429281834739,
      "learning_rate": 3.101464857640361e-07,
      "loss": 1.7561,
      "step": 104064
    },
    {
      "epoch": 0.0003777850432718043,
      "grad_norm": 10341.619795757335,
      "learning_rate": 3.101002545627989e-07,
      "loss": 1.7728,
      "step": 104096
    },
    {
      "epoch": 0.000377901177622641,
      "grad_norm": 10387.804002771712,
      "learning_rate": 3.10052553710976e-07,
      "loss": 1.7608,
      "step": 104128
    },
    {
      "epoch": 0.00037801731197347773,
      "grad_norm": 9496.228409215944,
      "learning_rate": 3.1000487486498623e-07,
      "loss": 1.7573,
      "step": 104160
    },
    {
      "epoch": 0.00037813344632431443,
      "grad_norm": 9078.654525864502,
      "learning_rate": 3.0995721800791465e-07,
      "loss": 1.7358,
      "step": 104192
    },
    {
      "epoch": 0.00037824958067515113,
      "grad_norm": 9424.042550837723,
      "learning_rate": 3.0990958312286496e-07,
      "loss": 1.7609,
      "step": 104224
    },
    {
      "epoch": 0.00037836571502598783,
      "grad_norm": 9622.944040157357,
      "learning_rate": 3.0986197019295865e-07,
      "loss": 1.7554,
      "step": 104256
    },
    {
      "epoch": 0.00037848184937682453,
      "grad_norm": 8875.284446145937,
      "learning_rate": 3.098143792013357e-07,
      "loss": 1.7626,
      "step": 104288
    },
    {
      "epoch": 0.00037859798372766123,
      "grad_norm": 9239.5886272063,
      "learning_rate": 3.097668101311538e-07,
      "loss": 1.766,
      "step": 104320
    },
    {
      "epoch": 0.0003787141180784979,
      "grad_norm": 10026.496995461575,
      "learning_rate": 3.097192629655892e-07,
      "loss": 1.7527,
      "step": 104352
    },
    {
      "epoch": 0.0003788302524293346,
      "grad_norm": 9269.637533366664,
      "learning_rate": 3.096717376878358e-07,
      "loss": 1.7509,
      "step": 104384
    },
    {
      "epoch": 0.0003789463867801713,
      "grad_norm": 9578.375645170741,
      "learning_rate": 3.096242342811059e-07,
      "loss": 1.7649,
      "step": 104416
    },
    {
      "epoch": 0.0003790625211310081,
      "grad_norm": 9840.971090293884,
      "learning_rate": 3.0957675272862946e-07,
      "loss": 1.7788,
      "step": 104448
    },
    {
      "epoch": 0.0003791786554818448,
      "grad_norm": 8992.782328067326,
      "learning_rate": 3.095292930136548e-07,
      "loss": 1.7615,
      "step": 104480
    },
    {
      "epoch": 0.0003792947898326815,
      "grad_norm": 8325.557398757155,
      "learning_rate": 3.094818551194479e-07,
      "loss": 1.7602,
      "step": 104512
    },
    {
      "epoch": 0.0003794109241835182,
      "grad_norm": 10427.23721797869,
      "learning_rate": 3.09434439029293e-07,
      "loss": 1.7612,
      "step": 104544
    },
    {
      "epoch": 0.0003795270585343549,
      "grad_norm": 9107.600122974218,
      "learning_rate": 3.0938704472649186e-07,
      "loss": 1.7395,
      "step": 104576
    },
    {
      "epoch": 0.0003796431928851916,
      "grad_norm": 8507.18367028713,
      "learning_rate": 3.093396721943646e-07,
      "loss": 1.7445,
      "step": 104608
    },
    {
      "epoch": 0.0003797593272360283,
      "grad_norm": 8443.529119982946,
      "learning_rate": 3.0929232141624887e-07,
      "loss": 1.7547,
      "step": 104640
    },
    {
      "epoch": 0.000379875461586865,
      "grad_norm": 8586.57335611826,
      "learning_rate": 3.092449923755002e-07,
      "loss": 1.7665,
      "step": 104672
    },
    {
      "epoch": 0.0003799915959377017,
      "grad_norm": 8745.6460024403,
      "learning_rate": 3.091976850554922e-07,
      "loss": 1.7657,
      "step": 104704
    },
    {
      "epoch": 0.00038010773028853843,
      "grad_norm": 9887.10291238035,
      "learning_rate": 3.091503994396161e-07,
      "loss": 1.781,
      "step": 104736
    },
    {
      "epoch": 0.00038022386463937513,
      "grad_norm": 10123.092610462476,
      "learning_rate": 3.0910313551128065e-07,
      "loss": 1.7716,
      "step": 104768
    },
    {
      "epoch": 0.00038033999899021183,
      "grad_norm": 9890.955868873341,
      "learning_rate": 3.0905589325391286e-07,
      "loss": 1.7592,
      "step": 104800
    },
    {
      "epoch": 0.00038045613334104853,
      "grad_norm": 9284.69374831502,
      "learning_rate": 3.090086726509571e-07,
      "loss": 1.7514,
      "step": 104832
    },
    {
      "epoch": 0.00038057226769188523,
      "grad_norm": 12120.229700793629,
      "learning_rate": 3.0896147368587554e-07,
      "loss": 1.7437,
      "step": 104864
    },
    {
      "epoch": 0.00038068840204272193,
      "grad_norm": 10219.510849350863,
      "learning_rate": 3.089142963421481e-07,
      "loss": 1.7352,
      "step": 104896
    },
    {
      "epoch": 0.00038080453639355863,
      "grad_norm": 10385.975736540115,
      "learning_rate": 3.088671406032723e-07,
      "loss": 1.7323,
      "step": 104928
    },
    {
      "epoch": 0.00038092067074439533,
      "grad_norm": 9529.022510205335,
      "learning_rate": 3.088200064527632e-07,
      "loss": 1.7314,
      "step": 104960
    },
    {
      "epoch": 0.00038103680509523203,
      "grad_norm": 8173.158752893522,
      "learning_rate": 3.087728938741536e-07,
      "loss": 1.7456,
      "step": 104992
    },
    {
      "epoch": 0.00038115293944606873,
      "grad_norm": 10169.978662711148,
      "learning_rate": 3.087258028509939e-07,
      "loss": 1.7685,
      "step": 105024
    },
    {
      "epoch": 0.0003812690737969055,
      "grad_norm": 9831.148356117918,
      "learning_rate": 3.0867873336685176e-07,
      "loss": 1.784,
      "step": 105056
    },
    {
      "epoch": 0.0003813852081477422,
      "grad_norm": 9079.271887106366,
      "learning_rate": 3.0863315532849223e-07,
      "loss": 1.7651,
      "step": 105088
    },
    {
      "epoch": 0.0003815013424985789,
      "grad_norm": 8990.167851603217,
      "learning_rate": 3.085861282013383e-07,
      "loss": 1.7552,
      "step": 105120
    },
    {
      "epoch": 0.0003816174768494156,
      "grad_norm": 9327.798990115512,
      "learning_rate": 3.0853912256452245e-07,
      "loss": 1.7588,
      "step": 105152
    },
    {
      "epoch": 0.0003817336112002523,
      "grad_norm": 9842.341794512116,
      "learning_rate": 3.08492138401682e-07,
      "loss": 1.7622,
      "step": 105184
    },
    {
      "epoch": 0.000381849745551089,
      "grad_norm": 9215.328860111287,
      "learning_rate": 3.084451756964715e-07,
      "loss": 1.7631,
      "step": 105216
    },
    {
      "epoch": 0.0003819658799019257,
      "grad_norm": 9539.44673448099,
      "learning_rate": 3.0839823443256335e-07,
      "loss": 1.7646,
      "step": 105248
    },
    {
      "epoch": 0.0003820820142527624,
      "grad_norm": 8758.429996294997,
      "learning_rate": 3.08351314593647e-07,
      "loss": 1.7707,
      "step": 105280
    },
    {
      "epoch": 0.0003821981486035991,
      "grad_norm": 11252.483103742035,
      "learning_rate": 3.083044161634294e-07,
      "loss": 1.763,
      "step": 105312
    },
    {
      "epoch": 0.00038231428295443584,
      "grad_norm": 8015.991142709678,
      "learning_rate": 3.082575391256348e-07,
      "loss": 1.7638,
      "step": 105344
    },
    {
      "epoch": 0.00038243041730527254,
      "grad_norm": 9588.05047963349,
      "learning_rate": 3.082106834640047e-07,
      "loss": 1.7504,
      "step": 105376
    },
    {
      "epoch": 0.00038254655165610924,
      "grad_norm": 9699.910618144891,
      "learning_rate": 3.081638491622982e-07,
      "loss": 1.7458,
      "step": 105408
    },
    {
      "epoch": 0.00038266268600694594,
      "grad_norm": 10185.312366344,
      "learning_rate": 3.0811703620429114e-07,
      "loss": 1.7398,
      "step": 105440
    },
    {
      "epoch": 0.00038277882035778264,
      "grad_norm": 9390.398287612725,
      "learning_rate": 3.0807024457377717e-07,
      "loss": 1.756,
      "step": 105472
    },
    {
      "epoch": 0.00038289495470861934,
      "grad_norm": 9103.046523005361,
      "learning_rate": 3.080234742545668e-07,
      "loss": 1.7504,
      "step": 105504
    },
    {
      "epoch": 0.00038301108905945604,
      "grad_norm": 10199.933038995894,
      "learning_rate": 3.0797672523048774e-07,
      "loss": 1.7497,
      "step": 105536
    },
    {
      "epoch": 0.00038312722341029274,
      "grad_norm": 11408.577562518474,
      "learning_rate": 3.0792999748538516e-07,
      "loss": 1.7604,
      "step": 105568
    },
    {
      "epoch": 0.00038324335776112944,
      "grad_norm": 9723.689217575806,
      "learning_rate": 3.078832910031211e-07,
      "loss": 1.758,
      "step": 105600
    },
    {
      "epoch": 0.0003833594921119662,
      "grad_norm": 8809.236289259132,
      "learning_rate": 3.078366057675749e-07,
      "loss": 1.756,
      "step": 105632
    },
    {
      "epoch": 0.0003834756264628029,
      "grad_norm": 8911.981036784133,
      "learning_rate": 3.077899417626428e-07,
      "loss": 1.758,
      "step": 105664
    },
    {
      "epoch": 0.0003835917608136396,
      "grad_norm": 8878.929890476667,
      "learning_rate": 3.077432989722384e-07,
      "loss": 1.7357,
      "step": 105696
    },
    {
      "epoch": 0.0003837078951644763,
      "grad_norm": 9076.678467368996,
      "learning_rate": 3.0769667738029217e-07,
      "loss": 1.7335,
      "step": 105728
    },
    {
      "epoch": 0.000383824029515313,
      "grad_norm": 9102.648295963105,
      "learning_rate": 3.076500769707517e-07,
      "loss": 1.747,
      "step": 105760
    },
    {
      "epoch": 0.0003839401638661497,
      "grad_norm": 8750.37256349694,
      "learning_rate": 3.076034977275814e-07,
      "loss": 1.7696,
      "step": 105792
    },
    {
      "epoch": 0.0003840562982169864,
      "grad_norm": 10529.409575090145,
      "learning_rate": 3.0755693963476305e-07,
      "loss": 1.7601,
      "step": 105824
    },
    {
      "epoch": 0.0003841724325678231,
      "grad_norm": 9265.180192527288,
      "learning_rate": 3.0751040267629504e-07,
      "loss": 1.7699,
      "step": 105856
    },
    {
      "epoch": 0.0003842885669186598,
      "grad_norm": 9496.966568331174,
      "learning_rate": 3.074638868361929e-07,
      "loss": 1.7798,
      "step": 105888
    },
    {
      "epoch": 0.00038440470126949655,
      "grad_norm": 10449.738369930608,
      "learning_rate": 3.074173920984889e-07,
      "loss": 1.7897,
      "step": 105920
    },
    {
      "epoch": 0.00038452083562033325,
      "grad_norm": 9478.454409870841,
      "learning_rate": 3.0737091844723247e-07,
      "loss": 1.7709,
      "step": 105952
    },
    {
      "epoch": 0.00038463696997116995,
      "grad_norm": 10456.69718410168,
      "learning_rate": 3.0732446586648964e-07,
      "loss": 1.7686,
      "step": 105984
    },
    {
      "epoch": 0.00038475310432200665,
      "grad_norm": 9143.343152261103,
      "learning_rate": 3.0727803434034355e-07,
      "loss": 1.7508,
      "step": 106016
    },
    {
      "epoch": 0.00038486923867284335,
      "grad_norm": 9470.993189734643,
      "learning_rate": 3.0723162385289393e-07,
      "loss": 1.7534,
      "step": 106048
    },
    {
      "epoch": 0.00038498537302368005,
      "grad_norm": 19841.959983832243,
      "learning_rate": 3.071852343882575e-07,
      "loss": 1.7457,
      "step": 106080
    },
    {
      "epoch": 0.00038510150737451675,
      "grad_norm": 9402.918483109379,
      "learning_rate": 3.0714031462705167e-07,
      "loss": 1.7282,
      "step": 106112
    },
    {
      "epoch": 0.00038521764172535345,
      "grad_norm": 9284.975821185535,
      "learning_rate": 3.0709396650472657e-07,
      "loss": 1.7274,
      "step": 106144
    },
    {
      "epoch": 0.00038533377607619015,
      "grad_norm": 8615.610715439736,
      "learning_rate": 3.070476393581596e-07,
      "loss": 1.7425,
      "step": 106176
    },
    {
      "epoch": 0.0003854499104270269,
      "grad_norm": 10587.482136938885,
      "learning_rate": 3.0700133317153407e-07,
      "loss": 1.743,
      "step": 106208
    },
    {
      "epoch": 0.0003855660447778636,
      "grad_norm": 9090.237180624057,
      "learning_rate": 3.069550479290497e-07,
      "loss": 1.7489,
      "step": 106240
    },
    {
      "epoch": 0.0003856821791287003,
      "grad_norm": 9333.321166658736,
      "learning_rate": 3.0690878361492286e-07,
      "loss": 1.7718,
      "step": 106272
    },
    {
      "epoch": 0.000385798313479537,
      "grad_norm": 8852.12754087965,
      "learning_rate": 3.068625402133868e-07,
      "loss": 1.7602,
      "step": 106304
    },
    {
      "epoch": 0.0003859144478303737,
      "grad_norm": 8854.349100865631,
      "learning_rate": 3.0681631770869126e-07,
      "loss": 1.7569,
      "step": 106336
    },
    {
      "epoch": 0.0003860305821812104,
      "grad_norm": 8206.23104719822,
      "learning_rate": 3.0677011608510255e-07,
      "loss": 1.758,
      "step": 106368
    },
    {
      "epoch": 0.0003861467165320471,
      "grad_norm": 11384.881027046353,
      "learning_rate": 3.067239353269036e-07,
      "loss": 1.7559,
      "step": 106400
    },
    {
      "epoch": 0.0003862628508828838,
      "grad_norm": 8999.833998469083,
      "learning_rate": 3.0667777541839396e-07,
      "loss": 1.7548,
      "step": 106432
    },
    {
      "epoch": 0.0003863789852337205,
      "grad_norm": 9728.603496905402,
      "learning_rate": 3.066316363438896e-07,
      "loss": 1.7572,
      "step": 106464
    },
    {
      "epoch": 0.00038649511958455725,
      "grad_norm": 8665.703549048974,
      "learning_rate": 3.0658551808772317e-07,
      "loss": 1.7802,
      "step": 106496
    },
    {
      "epoch": 0.00038661125393539395,
      "grad_norm": 8521.0073348167,
      "learning_rate": 3.0653942063424345e-07,
      "loss": 1.761,
      "step": 106528
    },
    {
      "epoch": 0.00038672738828623065,
      "grad_norm": 9271.976272618476,
      "learning_rate": 3.064933439678162e-07,
      "loss": 1.7679,
      "step": 106560
    },
    {
      "epoch": 0.00038684352263706735,
      "grad_norm": 10216.968630665358,
      "learning_rate": 3.064472880728232e-07,
      "loss": 1.7733,
      "step": 106592
    },
    {
      "epoch": 0.00038695965698790405,
      "grad_norm": 9644.132827787058,
      "learning_rate": 3.0640125293366285e-07,
      "loss": 1.7439,
      "step": 106624
    },
    {
      "epoch": 0.00038707579133874075,
      "grad_norm": 9473.677955261093,
      "learning_rate": 3.063552385347499e-07,
      "loss": 1.7652,
      "step": 106656
    },
    {
      "epoch": 0.00038719192568957745,
      "grad_norm": 9792.235495534203,
      "learning_rate": 3.0630924486051555e-07,
      "loss": 1.7642,
      "step": 106688
    },
    {
      "epoch": 0.00038730806004041415,
      "grad_norm": 9389.01602938242,
      "learning_rate": 3.0626327189540726e-07,
      "loss": 1.758,
      "step": 106720
    },
    {
      "epoch": 0.00038742419439125085,
      "grad_norm": 9432.360256054684,
      "learning_rate": 3.0621731962388885e-07,
      "loss": 1.7588,
      "step": 106752
    },
    {
      "epoch": 0.0003875403287420876,
      "grad_norm": 9003.630823173504,
      "learning_rate": 3.0617138803044046e-07,
      "loss": 1.7697,
      "step": 106784
    },
    {
      "epoch": 0.0003876564630929243,
      "grad_norm": 8732.862531839144,
      "learning_rate": 3.061254770995586e-07,
      "loss": 1.7439,
      "step": 106816
    },
    {
      "epoch": 0.000387772597443761,
      "grad_norm": 9077.134129228234,
      "learning_rate": 3.0607958681575597e-07,
      "loss": 1.7396,
      "step": 106848
    },
    {
      "epoch": 0.0003878887317945977,
      "grad_norm": 7833.9989788102475,
      "learning_rate": 3.060337171635614e-07,
      "loss": 1.7447,
      "step": 106880
    },
    {
      "epoch": 0.0003880048661454344,
      "grad_norm": 9628.241376284665,
      "learning_rate": 3.0598786812752017e-07,
      "loss": 1.7424,
      "step": 106912
    },
    {
      "epoch": 0.0003881210004962711,
      "grad_norm": 10176.859830026155,
      "learning_rate": 3.059420396921936e-07,
      "loss": 1.7386,
      "step": 106944
    },
    {
      "epoch": 0.0003882371348471078,
      "grad_norm": 8219.27575884883,
      "learning_rate": 3.058962318421594e-07,
      "loss": 1.7404,
      "step": 106976
    },
    {
      "epoch": 0.0003883532691979445,
      "grad_norm": 9757.556456408542,
      "learning_rate": 3.058504445620111e-07,
      "loss": 1.7477,
      "step": 107008
    },
    {
      "epoch": 0.0003884694035487812,
      "grad_norm": 10168.32159208195,
      "learning_rate": 3.058046778363586e-07,
      "loss": 1.7682,
      "step": 107040
    },
    {
      "epoch": 0.00038858553789961796,
      "grad_norm": 9724.771976761202,
      "learning_rate": 3.05758931649828e-07,
      "loss": 1.7732,
      "step": 107072
    },
    {
      "epoch": 0.00038870167225045466,
      "grad_norm": 9557.367419954095,
      "learning_rate": 3.057146346035127e-07,
      "loss": 1.7721,
      "step": 107104
    },
    {
      "epoch": 0.00038881780660129136,
      "grad_norm": 8460.791806917365,
      "learning_rate": 3.056689288085119e-07,
      "loss": 1.7447,
      "step": 107136
    },
    {
      "epoch": 0.00038893394095212806,
      "grad_norm": 10286.626852374884,
      "learning_rate": 3.056232435070859e-07,
      "loss": 1.7528,
      "step": 107168
    },
    {
      "epoch": 0.00038905007530296476,
      "grad_norm": 9580.930017487864,
      "learning_rate": 3.055775786839246e-07,
      "loss": 1.757,
      "step": 107200
    },
    {
      "epoch": 0.00038916620965380146,
      "grad_norm": 9936.207123445041,
      "learning_rate": 3.055319343237336e-07,
      "loss": 1.7383,
      "step": 107232
    },
    {
      "epoch": 0.00038928234400463816,
      "grad_norm": 8674.472779368209,
      "learning_rate": 3.054863104112347e-07,
      "loss": 1.7647,
      "step": 107264
    },
    {
      "epoch": 0.00038939847835547486,
      "grad_norm": 8904.03144648535,
      "learning_rate": 3.0544070693116553e-07,
      "loss": 1.765,
      "step": 107296
    },
    {
      "epoch": 0.00038951461270631156,
      "grad_norm": 10619.925423466966,
      "learning_rate": 3.053951238682797e-07,
      "loss": 1.7754,
      "step": 107328
    },
    {
      "epoch": 0.0003896307470571483,
      "grad_norm": 8938.390235383551,
      "learning_rate": 3.0534956120734685e-07,
      "loss": 1.7778,
      "step": 107360
    },
    {
      "epoch": 0.000389746881407985,
      "grad_norm": 8768.141080069368,
      "learning_rate": 3.053040189331524e-07,
      "loss": 1.7536,
      "step": 107392
    },
    {
      "epoch": 0.0003898630157588217,
      "grad_norm": 9766.663606370397,
      "learning_rate": 3.0525849703049755e-07,
      "loss": 1.758,
      "step": 107424
    },
    {
      "epoch": 0.0003899791501096584,
      "grad_norm": 9165.145607135764,
      "learning_rate": 3.052129954841996e-07,
      "loss": 1.76,
      "step": 107456
    },
    {
      "epoch": 0.0003900952844604951,
      "grad_norm": 9951.813603559905,
      "learning_rate": 3.0516751427909165e-07,
      "loss": 1.7617,
      "step": 107488
    },
    {
      "epoch": 0.0003902114188113318,
      "grad_norm": 9863.511950618806,
      "learning_rate": 3.051220534000223e-07,
      "loss": 1.7745,
      "step": 107520
    },
    {
      "epoch": 0.0003903275531621685,
      "grad_norm": 11840.031756714168,
      "learning_rate": 3.050766128318564e-07,
      "loss": 1.7486,
      "step": 107552
    },
    {
      "epoch": 0.0003904436875130052,
      "grad_norm": 9303.418941442978,
      "learning_rate": 3.050311925594743e-07,
      "loss": 1.7484,
      "step": 107584
    },
    {
      "epoch": 0.0003905598218638419,
      "grad_norm": 8650.303000473452,
      "learning_rate": 3.0498579256777213e-07,
      "loss": 1.7604,
      "step": 107616
    },
    {
      "epoch": 0.00039067595621467867,
      "grad_norm": 10253.949483004097,
      "learning_rate": 3.0494041284166185e-07,
      "loss": 1.7605,
      "step": 107648
    },
    {
      "epoch": 0.00039079209056551537,
      "grad_norm": 8724.315560546856,
      "learning_rate": 3.0489505336607095e-07,
      "loss": 1.7474,
      "step": 107680
    },
    {
      "epoch": 0.00039090822491635207,
      "grad_norm": 8728.399395078115,
      "learning_rate": 3.048497141259428e-07,
      "loss": 1.7426,
      "step": 107712
    },
    {
      "epoch": 0.00039102435926718877,
      "grad_norm": 8981.081894738518,
      "learning_rate": 3.0480439510623634e-07,
      "loss": 1.7235,
      "step": 107744
    },
    {
      "epoch": 0.00039114049361802547,
      "grad_norm": 8540.828765406786,
      "learning_rate": 3.047590962919262e-07,
      "loss": 1.7455,
      "step": 107776
    },
    {
      "epoch": 0.00039125662796886217,
      "grad_norm": 9284.452164775259,
      "learning_rate": 3.047138176680026e-07,
      "loss": 1.7581,
      "step": 107808
    },
    {
      "epoch": 0.00039137276231969887,
      "grad_norm": 9267.370608754136,
      "learning_rate": 3.0466855921947137e-07,
      "loss": 1.7544,
      "step": 107840
    },
    {
      "epoch": 0.00039148889667053557,
      "grad_norm": 9270.605373976394,
      "learning_rate": 3.046233209313539e-07,
      "loss": 1.7602,
      "step": 107872
    },
    {
      "epoch": 0.00039160503102137227,
      "grad_norm": 10473.03575855635,
      "learning_rate": 3.045781027886873e-07,
      "loss": 1.7356,
      "step": 107904
    },
    {
      "epoch": 0.000391721165372209,
      "grad_norm": 9845.506487733375,
      "learning_rate": 3.0453290477652395e-07,
      "loss": 1.7362,
      "step": 107936
    },
    {
      "epoch": 0.0003918372997230457,
      "grad_norm": 7511.244237807741,
      "learning_rate": 3.0448772687993203e-07,
      "loss": 1.7396,
      "step": 107968
    },
    {
      "epoch": 0.0003919534340738824,
      "grad_norm": 10872.121044212117,
      "learning_rate": 3.04442569083995e-07,
      "loss": 1.745,
      "step": 108000
    },
    {
      "epoch": 0.0003920695684247191,
      "grad_norm": 9189.776167023874,
      "learning_rate": 3.0439743137381194e-07,
      "loss": 1.761,
      "step": 108032
    },
    {
      "epoch": 0.0003921857027755558,
      "grad_norm": 9338.191580814779,
      "learning_rate": 3.0435231373449734e-07,
      "loss": 1.7689,
      "step": 108064
    },
    {
      "epoch": 0.0003923018371263925,
      "grad_norm": 17060.49870314464,
      "learning_rate": 3.0430721615118116e-07,
      "loss": 1.7816,
      "step": 108096
    },
    {
      "epoch": 0.0003924179714772292,
      "grad_norm": 10343.285068100946,
      "learning_rate": 3.0426354697899184e-07,
      "loss": 1.7805,
      "step": 108128
    },
    {
      "epoch": 0.0003925341058280659,
      "grad_norm": 9776.52637699096,
      "learning_rate": 3.0421848883752635e-07,
      "loss": 1.7656,
      "step": 108160
    },
    {
      "epoch": 0.0003926502401789026,
      "grad_norm": 9319.524880593432,
      "learning_rate": 3.0417345070800453e-07,
      "loss": 1.7811,
      "step": 108192
    },
    {
      "epoch": 0.0003927663745297394,
      "grad_norm": 10223.87675982061,
      "learning_rate": 3.041284325756173e-07,
      "loss": 1.8054,
      "step": 108224
    },
    {
      "epoch": 0.0003928825088805761,
      "grad_norm": 7937.686186792723,
      "learning_rate": 3.0408343442557113e-07,
      "loss": 1.7713,
      "step": 108256
    },
    {
      "epoch": 0.0003929986432314128,
      "grad_norm": 9040.32632154393,
      "learning_rate": 3.040384562430877e-07,
      "loss": 1.7611,
      "step": 108288
    },
    {
      "epoch": 0.0003931147775822495,
      "grad_norm": 9928.473095093726,
      "learning_rate": 3.039934980134038e-07,
      "loss": 1.756,
      "step": 108320
    },
    {
      "epoch": 0.0003932309119330862,
      "grad_norm": 8771.845529875683,
      "learning_rate": 3.039485597217718e-07,
      "loss": 1.7262,
      "step": 108352
    },
    {
      "epoch": 0.0003933470462839229,
      "grad_norm": 9679.820142957202,
      "learning_rate": 3.039036413534592e-07,
      "loss": 1.7408,
      "step": 108384
    },
    {
      "epoch": 0.0003934631806347596,
      "grad_norm": 10261.566352170608,
      "learning_rate": 3.038587428937487e-07,
      "loss": 1.7462,
      "step": 108416
    },
    {
      "epoch": 0.0003935793149855963,
      "grad_norm": 9996.712659669676,
      "learning_rate": 3.038138643279382e-07,
      "loss": 1.752,
      "step": 108448
    },
    {
      "epoch": 0.000393695449336433,
      "grad_norm": 8867.935780101252,
      "learning_rate": 3.037690056413409e-07,
      "loss": 1.7435,
      "step": 108480
    },
    {
      "epoch": 0.00039381158368726973,
      "grad_norm": 9436.019499767897,
      "learning_rate": 3.037241668192851e-07,
      "loss": 1.7413,
      "step": 108512
    },
    {
      "epoch": 0.00039392771803810643,
      "grad_norm": 10285.63678145403,
      "learning_rate": 3.0367934784711425e-07,
      "loss": 1.7378,
      "step": 108544
    },
    {
      "epoch": 0.00039404385238894313,
      "grad_norm": 8884.266767719215,
      "learning_rate": 3.03634548710187e-07,
      "loss": 1.7423,
      "step": 108576
    },
    {
      "epoch": 0.00039415998673977983,
      "grad_norm": 11184.571694973394,
      "learning_rate": 3.035897693938771e-07,
      "loss": 1.7497,
      "step": 108608
    },
    {
      "epoch": 0.00039427612109061653,
      "grad_norm": 9467.148250661336,
      "learning_rate": 3.035450098835733e-07,
      "loss": 1.7404,
      "step": 108640
    },
    {
      "epoch": 0.00039439225544145323,
      "grad_norm": 11071.96920154676,
      "learning_rate": 3.035002701646795e-07,
      "loss": 1.7269,
      "step": 108672
    },
    {
      "epoch": 0.00039450838979228993,
      "grad_norm": 9984.683570349138,
      "learning_rate": 3.0345555022261465e-07,
      "loss": 1.7463,
      "step": 108704
    },
    {
      "epoch": 0.00039462452414312663,
      "grad_norm": 9838.643504060912,
      "learning_rate": 3.0341085004281275e-07,
      "loss": 1.7561,
      "step": 108736
    },
    {
      "epoch": 0.00039474065849396333,
      "grad_norm": 9998.11982324677,
      "learning_rate": 3.0336616961072285e-07,
      "loss": 1.7533,
      "step": 108768
    },
    {
      "epoch": 0.0003948567928448001,
      "grad_norm": 9987.423091068087,
      "learning_rate": 3.033215089118088e-07,
      "loss": 1.7779,
      "step": 108800
    },
    {
      "epoch": 0.0003949729271956368,
      "grad_norm": 9630.199686403184,
      "learning_rate": 3.0327686793154965e-07,
      "loss": 1.7922,
      "step": 108832
    },
    {
      "epoch": 0.0003950890615464735,
      "grad_norm": 9476.437938381701,
      "learning_rate": 3.032322466554394e-07,
      "loss": 1.7787,
      "step": 108864
    },
    {
      "epoch": 0.0003952051958973102,
      "grad_norm": 9249.769294420266,
      "learning_rate": 3.0318764506898674e-07,
      "loss": 1.7631,
      "step": 108896
    },
    {
      "epoch": 0.0003953213302481469,
      "grad_norm": 9217.861682624663,
      "learning_rate": 3.0314306315771557e-07,
      "loss": 1.7644,
      "step": 108928
    },
    {
      "epoch": 0.0003954374645989836,
      "grad_norm": 10369.212506261023,
      "learning_rate": 3.030985009071645e-07,
      "loss": 1.7639,
      "step": 108960
    },
    {
      "epoch": 0.0003955535989498203,
      "grad_norm": 9240.90309439505,
      "learning_rate": 3.0305395830288714e-07,
      "loss": 1.7721,
      "step": 108992
    },
    {
      "epoch": 0.000395669733300657,
      "grad_norm": 10911.49586445415,
      "learning_rate": 3.030094353304518e-07,
      "loss": 1.7941,
      "step": 109024
    },
    {
      "epoch": 0.0003957858676514937,
      "grad_norm": 9077.424304283677,
      "learning_rate": 3.029649319754418e-07,
      "loss": 1.7767,
      "step": 109056
    },
    {
      "epoch": 0.00039590200200233044,
      "grad_norm": 9273.08643332952,
      "learning_rate": 3.0292044822345523e-07,
      "loss": 1.7586,
      "step": 109088
    },
    {
      "epoch": 0.00039601813635316714,
      "grad_norm": 9142.175889797789,
      "learning_rate": 3.028773732688446e-07,
      "loss": 1.7223,
      "step": 109120
    },
    {
      "epoch": 0.00039613427070400384,
      "grad_norm": 8796.304792354571,
      "learning_rate": 3.0283292806827975e-07,
      "loss": 1.7404,
      "step": 109152
    },
    {
      "epoch": 0.00039625040505484054,
      "grad_norm": 10597.30088277199,
      "learning_rate": 3.0278850242806953e-07,
      "loss": 1.727,
      "step": 109184
    },
    {
      "epoch": 0.00039636653940567724,
      "grad_norm": 9772.623700931086,
      "learning_rate": 3.0274409633387075e-07,
      "loss": 1.7339,
      "step": 109216
    },
    {
      "epoch": 0.00039648267375651394,
      "grad_norm": 11069.157510849685,
      "learning_rate": 3.026997097713547e-07,
      "loss": 1.7313,
      "step": 109248
    },
    {
      "epoch": 0.00039659880810735064,
      "grad_norm": 7883.123746333048,
      "learning_rate": 3.0265534272620754e-07,
      "loss": 1.731,
      "step": 109280
    },
    {
      "epoch": 0.00039671494245818733,
      "grad_norm": 9295.278801628276,
      "learning_rate": 3.0261099518413e-07,
      "loss": 1.7582,
      "step": 109312
    },
    {
      "epoch": 0.00039683107680902403,
      "grad_norm": 10323.621845069685,
      "learning_rate": 3.0256666713083755e-07,
      "loss": 1.7842,
      "step": 109344
    },
    {
      "epoch": 0.0003969472111598608,
      "grad_norm": 9056.570211730266,
      "learning_rate": 3.025223585520603e-07,
      "loss": 1.7639,
      "step": 109376
    },
    {
      "epoch": 0.0003970633455106975,
      "grad_norm": 8889.423378375,
      "learning_rate": 3.0247806943354283e-07,
      "loss": 1.7578,
      "step": 109408
    },
    {
      "epoch": 0.0003971794798615342,
      "grad_norm": 9424.342523486717,
      "learning_rate": 3.024337997610446e-07,
      "loss": 1.7475,
      "step": 109440
    },
    {
      "epoch": 0.0003972956142123709,
      "grad_norm": 9801.672102248676,
      "learning_rate": 3.023895495203394e-07,
      "loss": 1.7337,
      "step": 109472
    },
    {
      "epoch": 0.0003974117485632076,
      "grad_norm": 9924.688811242397,
      "learning_rate": 3.023453186972157e-07,
      "loss": 1.7238,
      "step": 109504
    },
    {
      "epoch": 0.0003975278829140443,
      "grad_norm": 10897.374913253192,
      "learning_rate": 3.0230110727747655e-07,
      "loss": 1.7399,
      "step": 109536
    },
    {
      "epoch": 0.000397644017264881,
      "grad_norm": 10438.994587602774,
      "learning_rate": 3.0225691524693947e-07,
      "loss": 1.7495,
      "step": 109568
    },
    {
      "epoch": 0.0003977601516157177,
      "grad_norm": 10839.301268993311,
      "learning_rate": 3.022127425914365e-07,
      "loss": 1.7756,
      "step": 109600
    },
    {
      "epoch": 0.0003978762859665544,
      "grad_norm": 9690.367691682291,
      "learning_rate": 3.0216858929681424e-07,
      "loss": 1.7912,
      "step": 109632
    },
    {
      "epoch": 0.00039799242031739114,
      "grad_norm": 9434.83534567509,
      "learning_rate": 3.0212445534893355e-07,
      "loss": 1.7733,
      "step": 109664
    },
    {
      "epoch": 0.00039810855466822784,
      "grad_norm": 8780.389057439312,
      "learning_rate": 3.0208034073367e-07,
      "loss": 1.7636,
      "step": 109696
    },
    {
      "epoch": 0.00039822468901906454,
      "grad_norm": 9077.404254521221,
      "learning_rate": 3.020362454369134e-07,
      "loss": 1.7587,
      "step": 109728
    },
    {
      "epoch": 0.00039834082336990124,
      "grad_norm": 10626.218894790376,
      "learning_rate": 3.0199216944456815e-07,
      "loss": 1.766,
      "step": 109760
    },
    {
      "epoch": 0.00039845695772073794,
      "grad_norm": 9275.38160940023,
      "learning_rate": 3.0194811274255287e-07,
      "loss": 1.7697,
      "step": 109792
    },
    {
      "epoch": 0.00039857309207157464,
      "grad_norm": 9058.97941271532,
      "learning_rate": 3.0190407531680063e-07,
      "loss": 1.7566,
      "step": 109824
    },
    {
      "epoch": 0.00039868922642241134,
      "grad_norm": 8935.567357476524,
      "learning_rate": 3.0186005715325894e-07,
      "loss": 1.755,
      "step": 109856
    },
    {
      "epoch": 0.00039880536077324804,
      "grad_norm": 8656.909841277084,
      "learning_rate": 3.0181605823788945e-07,
      "loss": 1.743,
      "step": 109888
    },
    {
      "epoch": 0.00039892149512408474,
      "grad_norm": 9755.055714858834,
      "learning_rate": 3.017720785566683e-07,
      "loss": 1.7664,
      "step": 109920
    },
    {
      "epoch": 0.0003990376294749215,
      "grad_norm": 10163.980322688549,
      "learning_rate": 3.017281180955859e-07,
      "loss": 1.767,
      "step": 109952
    },
    {
      "epoch": 0.0003991537638257582,
      "grad_norm": 10030.11924156438,
      "learning_rate": 3.016841768406469e-07,
      "loss": 1.7602,
      "step": 109984
    },
    {
      "epoch": 0.0003992698981765949,
      "grad_norm": 8628.744056929721,
      "learning_rate": 3.0164025477787023e-07,
      "loss": 1.7498,
      "step": 110016
    },
    {
      "epoch": 0.0003993860325274316,
      "grad_norm": 8878.832580919632,
      "learning_rate": 3.0159635189328904e-07,
      "loss": 1.7435,
      "step": 110048
    },
    {
      "epoch": 0.0003995021668782683,
      "grad_norm": 8793.274702862409,
      "learning_rate": 3.0155246817295084e-07,
      "loss": 1.7396,
      "step": 110080
    },
    {
      "epoch": 0.000399618301229105,
      "grad_norm": 8117.675159797909,
      "learning_rate": 3.015099740809962e-07,
      "loss": 1.7456,
      "step": 110112
    },
    {
      "epoch": 0.0003997344355799417,
      "grad_norm": 11123.096691119788,
      "learning_rate": 3.014661280495415e-07,
      "loss": 1.7522,
      "step": 110144
    },
    {
      "epoch": 0.0003998505699307784,
      "grad_norm": 10340.85064199266,
      "learning_rate": 3.014223011409916e-07,
      "loss": 1.7352,
      "step": 110176
    },
    {
      "epoch": 0.0003999667042816151,
      "grad_norm": 9666.043451174839,
      "learning_rate": 3.0137849334145005e-07,
      "loss": 1.7569,
      "step": 110208
    },
    {
      "epoch": 0.00040008283863245185,
      "grad_norm": 8927.264306605915,
      "learning_rate": 3.0133470463703477e-07,
      "loss": 1.7501,
      "step": 110240
    },
    {
      "epoch": 0.00040019897298328855,
      "grad_norm": 9270.389635824376,
      "learning_rate": 3.012909350138777e-07,
      "loss": 1.7388,
      "step": 110272
    },
    {
      "epoch": 0.00040031510733412525,
      "grad_norm": 10414.134625594197,
      "learning_rate": 3.0124718445812473e-07,
      "loss": 1.7405,
      "step": 110304
    },
    {
      "epoch": 0.00040043124168496195,
      "grad_norm": 10149.550926026235,
      "learning_rate": 3.0120345295593613e-07,
      "loss": 1.7551,
      "step": 110336
    },
    {
      "epoch": 0.00040054737603579865,
      "grad_norm": 9148.08646657868,
      "learning_rate": 3.011597404934859e-07,
      "loss": 1.7652,
      "step": 110368
    },
    {
      "epoch": 0.00040066351038663535,
      "grad_norm": 9817.572612412907,
      "learning_rate": 3.011160470569624e-07,
      "loss": 1.7512,
      "step": 110400
    },
    {
      "epoch": 0.00040077964473747205,
      "grad_norm": 9783.436001732725,
      "learning_rate": 3.0107237263256766e-07,
      "loss": 1.7512,
      "step": 110432
    },
    {
      "epoch": 0.00040089577908830875,
      "grad_norm": 8407.882313638791,
      "learning_rate": 3.0102871720651806e-07,
      "loss": 1.761,
      "step": 110464
    },
    {
      "epoch": 0.00040101191343914545,
      "grad_norm": 8281.765874498024,
      "learning_rate": 3.009850807650438e-07,
      "loss": 1.7688,
      "step": 110496
    },
    {
      "epoch": 0.00040112804778998215,
      "grad_norm": 8648.519526485443,
      "learning_rate": 3.00941463294389e-07,
      "loss": 1.778,
      "step": 110528
    },
    {
      "epoch": 0.0004012441821408189,
      "grad_norm": 9037.213951213062,
      "learning_rate": 3.008978647808118e-07,
      "loss": 1.7956,
      "step": 110560
    },
    {
      "epoch": 0.0004013603164916556,
      "grad_norm": 8580.969176031342,
      "learning_rate": 3.0085428521058433e-07,
      "loss": 1.7871,
      "step": 110592
    },
    {
      "epoch": 0.0004014764508424923,
      "grad_norm": 8758.087348274164,
      "learning_rate": 3.008107245699925e-07,
      "loss": 1.7566,
      "step": 110624
    },
    {
      "epoch": 0.000401592585193329,
      "grad_norm": 9335.846185536691,
      "learning_rate": 3.007671828453362e-07,
      "loss": 1.739,
      "step": 110656
    },
    {
      "epoch": 0.0004017087195441657,
      "grad_norm": 8586.581275455324,
      "learning_rate": 3.007236600229292e-07,
      "loss": 1.7382,
      "step": 110688
    },
    {
      "epoch": 0.0004018248538950024,
      "grad_norm": 9659.25928837196,
      "learning_rate": 3.0068015608909913e-07,
      "loss": 1.741,
      "step": 110720
    },
    {
      "epoch": 0.0004019409882458391,
      "grad_norm": 10141.216790898417,
      "learning_rate": 3.0063667103018737e-07,
      "loss": 1.746,
      "step": 110752
    },
    {
      "epoch": 0.0004020571225966758,
      "grad_norm": 8878.95770910077,
      "learning_rate": 3.005932048325492e-07,
      "loss": 1.7619,
      "step": 110784
    },
    {
      "epoch": 0.0004021732569475125,
      "grad_norm": 10851.98599335624,
      "learning_rate": 3.0054975748255387e-07,
      "loss": 1.7524,
      "step": 110816
    },
    {
      "epoch": 0.00040228939129834926,
      "grad_norm": 9083.018441024988,
      "learning_rate": 3.00506328966584e-07,
      "loss": 1.7578,
      "step": 110848
    },
    {
      "epoch": 0.00040240552564918596,
      "grad_norm": 11813.377840397725,
      "learning_rate": 3.004629192710364e-07,
      "loss": 1.7631,
      "step": 110880
    },
    {
      "epoch": 0.00040252166000002266,
      "grad_norm": 9926.84501742623,
      "learning_rate": 3.0041952838232136e-07,
      "loss": 1.7415,
      "step": 110912
    },
    {
      "epoch": 0.00040263779435085936,
      "grad_norm": 9740.993481159916,
      "learning_rate": 3.0037615628686303e-07,
      "loss": 1.72,
      "step": 110944
    },
    {
      "epoch": 0.00040275392870169606,
      "grad_norm": 9238.884239993486,
      "learning_rate": 3.0033280297109934e-07,
      "loss": 1.7425,
      "step": 110976
    },
    {
      "epoch": 0.00040287006305253276,
      "grad_norm": 9864.950886851897,
      "learning_rate": 3.0028946842148166e-07,
      "loss": 1.7249,
      "step": 111008
    },
    {
      "epoch": 0.00040298619740336946,
      "grad_norm": 9447.14697673324,
      "learning_rate": 3.002461526244754e-07,
      "loss": 1.7373,
      "step": 111040
    },
    {
      "epoch": 0.00040310233175420616,
      "grad_norm": 9073.46835559589,
      "learning_rate": 3.002028555665592e-07,
      "loss": 1.7757,
      "step": 111072
    },
    {
      "epoch": 0.00040321846610504286,
      "grad_norm": 8872.049368663364,
      "learning_rate": 3.001595772342258e-07,
      "loss": 1.7614,
      "step": 111104
    },
    {
      "epoch": 0.0004033346004558796,
      "grad_norm": 9574.969660526345,
      "learning_rate": 3.001176691940082e-07,
      "loss": 1.7683,
      "step": 111136
    },
    {
      "epoch": 0.0004034507348067163,
      "grad_norm": 10083.11251548846,
      "learning_rate": 3.0007442768824474e-07,
      "loss": 1.7891,
      "step": 111168
    },
    {
      "epoch": 0.000403566869157553,
      "grad_norm": 9330.43482373678,
      "learning_rate": 3.000312048680438e-07,
      "loss": 1.7763,
      "step": 111200
    },
    {
      "epoch": 0.0004036830035083897,
      "grad_norm": 9858.226412494289,
      "learning_rate": 2.99988000719952e-07,
      "loss": 1.7539,
      "step": 111232
    },
    {
      "epoch": 0.0004037991378592264,
      "grad_norm": 9625.293553964992,
      "learning_rate": 2.9994481523052935e-07,
      "loss": 1.7466,
      "step": 111264
    },
    {
      "epoch": 0.0004039152722100631,
      "grad_norm": 10380.351053793895,
      "learning_rate": 2.9990164838634954e-07,
      "loss": 1.7577,
      "step": 111296
    },
    {
      "epoch": 0.0004040314065608998,
      "grad_norm": 8131.653583374048,
      "learning_rate": 2.9985850017399957e-07,
      "loss": 1.7595,
      "step": 111328
    },
    {
      "epoch": 0.0004041475409117365,
      "grad_norm": 10963.64784184534,
      "learning_rate": 2.998153705800801e-07,
      "loss": 1.7636,
      "step": 111360
    },
    {
      "epoch": 0.0004042636752625732,
      "grad_norm": 9087.51825307658,
      "learning_rate": 2.997722595912053e-07,
      "loss": 1.7462,
      "step": 111392
    },
    {
      "epoch": 0.00040437980961340996,
      "grad_norm": 9831.838993799685,
      "learning_rate": 2.9972916719400267e-07,
      "loss": 1.7358,
      "step": 111424
    },
    {
      "epoch": 0.00040449594396424666,
      "grad_norm": 9928.993101014825,
      "learning_rate": 2.996860933751133e-07,
      "loss": 1.7493,
      "step": 111456
    },
    {
      "epoch": 0.00040461207831508336,
      "grad_norm": 9712.353782683165,
      "learning_rate": 2.996430381211917e-07,
      "loss": 1.7596,
      "step": 111488
    },
    {
      "epoch": 0.00040472821266592006,
      "grad_norm": 9120.683307735228,
      "learning_rate": 2.996000014189056e-07,
      "loss": 1.7376,
      "step": 111520
    },
    {
      "epoch": 0.00040484434701675676,
      "grad_norm": 8941.461401806753,
      "learning_rate": 2.9955698325493646e-07,
      "loss": 1.7412,
      "step": 111552
    },
    {
      "epoch": 0.00040496048136759346,
      "grad_norm": 8904.443609793932,
      "learning_rate": 2.9951398361597883e-07,
      "loss": 1.7503,
      "step": 111584
    },
    {
      "epoch": 0.00040507661571843016,
      "grad_norm": 10351.769124164237,
      "learning_rate": 2.9947100248874087e-07,
      "loss": 1.7542,
      "step": 111616
    },
    {
      "epoch": 0.00040519275006926686,
      "grad_norm": 10558.418252749794,
      "learning_rate": 2.994280398599438e-07,
      "loss": 1.7646,
      "step": 111648
    },
    {
      "epoch": 0.00040530888442010356,
      "grad_norm": 8590.204304904511,
      "learning_rate": 2.9938509571632245e-07,
      "loss": 1.7539,
      "step": 111680
    },
    {
      "epoch": 0.0004054250187709403,
      "grad_norm": 10349.7201894544,
      "learning_rate": 2.9934217004462483e-07,
      "loss": 1.7309,
      "step": 111712
    },
    {
      "epoch": 0.000405541153121777,
      "grad_norm": 9260.997894395614,
      "learning_rate": 2.992992628316123e-07,
      "loss": 1.752,
      "step": 111744
    },
    {
      "epoch": 0.0004056572874726137,
      "grad_norm": 9717.70744568903,
      "learning_rate": 2.992563740640593e-07,
      "loss": 1.7593,
      "step": 111776
    },
    {
      "epoch": 0.0004057734218234504,
      "grad_norm": 9378.128811228815,
      "learning_rate": 2.992135037287539e-07,
      "loss": 1.7696,
      "step": 111808
    },
    {
      "epoch": 0.0004058895561742871,
      "grad_norm": 10558.404046066811,
      "learning_rate": 2.9917065181249706e-07,
      "loss": 1.744,
      "step": 111840
    },
    {
      "epoch": 0.0004060056905251238,
      "grad_norm": 10949.15941979109,
      "learning_rate": 2.991278183021033e-07,
      "loss": 1.7512,
      "step": 111872
    },
    {
      "epoch": 0.0004061218248759605,
      "grad_norm": 8993.385013441824,
      "learning_rate": 2.9908500318439996e-07,
      "loss": 1.7692,
      "step": 111904
    },
    {
      "epoch": 0.0004062379592267972,
      "grad_norm": 8723.67365276808,
      "learning_rate": 2.9904220644622783e-07,
      "loss": 1.7687,
      "step": 111936
    },
    {
      "epoch": 0.0004063540935776339,
      "grad_norm": 9266.536893575723,
      "learning_rate": 2.9899942807444097e-07,
      "loss": 1.7622,
      "step": 111968
    },
    {
      "epoch": 0.00040647022792847067,
      "grad_norm": 8802.054192062214,
      "learning_rate": 2.9895666805590635e-07,
      "loss": 1.7528,
      "step": 112000
    },
    {
      "epoch": 0.00040658636227930737,
      "grad_norm": 11375.24839289235,
      "learning_rate": 2.9891392637750424e-07,
      "loss": 1.7497,
      "step": 112032
    },
    {
      "epoch": 0.00040670249663014407,
      "grad_norm": 10512.796392967954,
      "learning_rate": 2.9887120302612795e-07,
      "loss": 1.759,
      "step": 112064
    },
    {
      "epoch": 0.00040681863098098077,
      "grad_norm": 9660.335604936301,
      "learning_rate": 2.98828497988684e-07,
      "loss": 1.7648,
      "step": 112096
    },
    {
      "epoch": 0.00040693476533181747,
      "grad_norm": 10614.10156348619,
      "learning_rate": 2.987871449357253e-07,
      "loss": 1.7403,
      "step": 112128
    },
    {
      "epoch": 0.00040705089968265417,
      "grad_norm": 9789.632985970415,
      "learning_rate": 2.9874447591562204e-07,
      "loss": 1.7283,
      "step": 112160
    },
    {
      "epoch": 0.00040716703403349087,
      "grad_norm": 11632.74963196578,
      "learning_rate": 2.9870182517065675e-07,
      "loss": 1.7355,
      "step": 112192
    },
    {
      "epoch": 0.00040728316838432757,
      "grad_norm": 8879.735469032847,
      "learning_rate": 2.9865919268778767e-07,
      "loss": 1.7397,
      "step": 112224
    },
    {
      "epoch": 0.00040739930273516427,
      "grad_norm": 9006.302570977725,
      "learning_rate": 2.986165784539862e-07,
      "loss": 1.7462,
      "step": 112256
    },
    {
      "epoch": 0.000407515437086001,
      "grad_norm": 8730.812505145212,
      "learning_rate": 2.985739824562367e-07,
      "loss": 1.7505,
      "step": 112288
    },
    {
      "epoch": 0.0004076315714368377,
      "grad_norm": 8551.842725401351,
      "learning_rate": 2.985314046815365e-07,
      "loss": 1.7494,
      "step": 112320
    },
    {
      "epoch": 0.0004077477057876744,
      "grad_norm": 10137.363562583716,
      "learning_rate": 2.9848884511689596e-07,
      "loss": 1.7762,
      "step": 112352
    },
    {
      "epoch": 0.0004078638401385111,
      "grad_norm": 11250.792683184594,
      "learning_rate": 2.984463037493383e-07,
      "loss": 1.7918,
      "step": 112384
    },
    {
      "epoch": 0.0004079799744893478,
      "grad_norm": 9081.114689287873,
      "learning_rate": 2.9840378056589977e-07,
      "loss": 1.7751,
      "step": 112416
    },
    {
      "epoch": 0.0004080961088401845,
      "grad_norm": 10758.343552796592,
      "learning_rate": 2.983612755536296e-07,
      "loss": 1.7365,
      "step": 112448
    },
    {
      "epoch": 0.0004082122431910212,
      "grad_norm": 8685.28519969264,
      "learning_rate": 2.983187886995898e-07,
      "loss": 1.7398,
      "step": 112480
    },
    {
      "epoch": 0.0004083283775418579,
      "grad_norm": 10175.620374208149,
      "learning_rate": 2.9827631999085526e-07,
      "loss": 1.7565,
      "step": 112512
    },
    {
      "epoch": 0.0004084445118926946,
      "grad_norm": 8490.861322622104,
      "learning_rate": 2.9823386941451394e-07,
      "loss": 1.7258,
      "step": 112544
    },
    {
      "epoch": 0.0004085606462435314,
      "grad_norm": 9569.478355688987,
      "learning_rate": 2.9819143695766643e-07,
      "loss": 1.7477,
      "step": 112576
    },
    {
      "epoch": 0.0004086767805943681,
      "grad_norm": 9963.356161454834,
      "learning_rate": 2.981490226074264e-07,
      "loss": 1.7502,
      "step": 112608
    },
    {
      "epoch": 0.0004087929149452048,
      "grad_norm": 7914.63366176856,
      "learning_rate": 2.9810662635092004e-07,
      "loss": 1.7458,
      "step": 112640
    },
    {
      "epoch": 0.0004089090492960415,
      "grad_norm": 8169.911627429026,
      "learning_rate": 2.9806424817528665e-07,
      "loss": 1.7658,
      "step": 112672
    },
    {
      "epoch": 0.0004090251836468782,
      "grad_norm": 8720.139792457458,
      "learning_rate": 2.980218880676782e-07,
      "loss": 1.7653,
      "step": 112704
    },
    {
      "epoch": 0.0004091413179977149,
      "grad_norm": 8109.958199645667,
      "learning_rate": 2.979795460152593e-07,
      "loss": 1.7659,
      "step": 112736
    },
    {
      "epoch": 0.0004092574523485516,
      "grad_norm": 8978.668720918486,
      "learning_rate": 2.979372220052076e-07,
      "loss": 1.7567,
      "step": 112768
    },
    {
      "epoch": 0.0004093735866993883,
      "grad_norm": 9688.026527626771,
      "learning_rate": 2.9789491602471337e-07,
      "loss": 1.7711,
      "step": 112800
    },
    {
      "epoch": 0.000409489721050225,
      "grad_norm": 9098.164650081906,
      "learning_rate": 2.978526280609795e-07,
      "loss": 1.7582,
      "step": 112832
    },
    {
      "epoch": 0.00040960585540106173,
      "grad_norm": 8656.810729131139,
      "learning_rate": 2.978103581012217e-07,
      "loss": 1.7561,
      "step": 112864
    },
    {
      "epoch": 0.00040972198975189843,
      "grad_norm": 9676.203387692924,
      "learning_rate": 2.977681061326684e-07,
      "loss": 1.7373,
      "step": 112896
    },
    {
      "epoch": 0.00040983812410273513,
      "grad_norm": 9275.447266843794,
      "learning_rate": 2.9772587214256064e-07,
      "loss": 1.7445,
      "step": 112928
    },
    {
      "epoch": 0.00040995425845357183,
      "grad_norm": 10770.94294850734,
      "learning_rate": 2.976836561181522e-07,
      "loss": 1.7431,
      "step": 112960
    },
    {
      "epoch": 0.00041007039280440853,
      "grad_norm": 10038.669134900303,
      "learning_rate": 2.9764145804670937e-07,
      "loss": 1.7552,
      "step": 112992
    },
    {
      "epoch": 0.00041018652715524523,
      "grad_norm": 8318.914111829741,
      "learning_rate": 2.9759927791551126e-07,
      "loss": 1.748,
      "step": 113024
    },
    {
      "epoch": 0.00041030266150608193,
      "grad_norm": 10102.639160140285,
      "learning_rate": 2.975571157118495e-07,
      "loss": 1.731,
      "step": 113056
    },
    {
      "epoch": 0.00041041879585691863,
      "grad_norm": 9347.52523398573,
      "learning_rate": 2.9751497142302827e-07,
      "loss": 1.7376,
      "step": 113088
    },
    {
      "epoch": 0.00041053493020775533,
      "grad_norm": 8813.69695417309,
      "learning_rate": 2.974741612150936e-07,
      "loss": 1.7621,
      "step": 113120
    },
    {
      "epoch": 0.0004106510645585921,
      "grad_norm": 11414.596620117592,
      "learning_rate": 2.9743205215906174e-07,
      "loss": 1.7643,
      "step": 113152
    },
    {
      "epoch": 0.0004107671989094288,
      "grad_norm": 9437.13250940136,
      "learning_rate": 2.973899609802539e-07,
      "loss": 1.7577,
      "step": 113184
    },
    {
      "epoch": 0.0004108833332602655,
      "grad_norm": 9793.319559781554,
      "learning_rate": 2.9734788766602413e-07,
      "loss": 1.7395,
      "step": 113216
    },
    {
      "epoch": 0.0004109994676111022,
      "grad_norm": 9553.7472229487,
      "learning_rate": 2.973058322037392e-07,
      "loss": 1.736,
      "step": 113248
    },
    {
      "epoch": 0.0004111156019619389,
      "grad_norm": 8912.412468013361,
      "learning_rate": 2.9726379458077804e-07,
      "loss": 1.7479,
      "step": 113280
    },
    {
      "epoch": 0.0004112317363127756,
      "grad_norm": 9671.275407101175,
      "learning_rate": 2.9722177478453233e-07,
      "loss": 1.7595,
      "step": 113312
    },
    {
      "epoch": 0.0004113478706636123,
      "grad_norm": 9805.556180044046,
      "learning_rate": 2.971797728024061e-07,
      "loss": 1.7728,
      "step": 113344
    },
    {
      "epoch": 0.000411464005014449,
      "grad_norm": 10453.072658314397,
      "learning_rate": 2.971377886218159e-07,
      "loss": 1.7752,
      "step": 113376
    },
    {
      "epoch": 0.0004115801393652857,
      "grad_norm": 9452.730187623045,
      "learning_rate": 2.970958222301906e-07,
      "loss": 1.7512,
      "step": 113408
    },
    {
      "epoch": 0.00041169627371612244,
      "grad_norm": 9119.691990412834,
      "learning_rate": 2.970538736149717e-07,
      "loss": 1.765,
      "step": 113440
    },
    {
      "epoch": 0.00041181240806695914,
      "grad_norm": 10753.962246539642,
      "learning_rate": 2.970119427636129e-07,
      "loss": 1.7575,
      "step": 113472
    },
    {
      "epoch": 0.00041192854241779584,
      "grad_norm": 8794.458937308196,
      "learning_rate": 2.969700296635805e-07,
      "loss": 1.7569,
      "step": 113504
    },
    {
      "epoch": 0.00041204467676863254,
      "grad_norm": 10321.344001630794,
      "learning_rate": 2.969281343023529e-07,
      "loss": 1.7809,
      "step": 113536
    },
    {
      "epoch": 0.00041216081111946924,
      "grad_norm": 10128.093403992678,
      "learning_rate": 2.9688625666742123e-07,
      "loss": 1.7651,
      "step": 113568
    },
    {
      "epoch": 0.00041227694547030594,
      "grad_norm": 9443.62568084949,
      "learning_rate": 2.968443967462886e-07,
      "loss": 1.7675,
      "step": 113600
    },
    {
      "epoch": 0.00041239307982114264,
      "grad_norm": 9941.808487393026,
      "learning_rate": 2.9680255452647064e-07,
      "loss": 1.7784,
      "step": 113632
    },
    {
      "epoch": 0.00041250921417197934,
      "grad_norm": 9624.234618919054,
      "learning_rate": 2.967607299954953e-07,
      "loss": 1.7348,
      "step": 113664
    },
    {
      "epoch": 0.00041262534852281604,
      "grad_norm": 8544.67190710094,
      "learning_rate": 2.967189231409028e-07,
      "loss": 1.7256,
      "step": 113696
    },
    {
      "epoch": 0.0004127414828736528,
      "grad_norm": 9428.471350118216,
      "learning_rate": 2.9667713395024567e-07,
      "loss": 1.7404,
      "step": 113728
    },
    {
      "epoch": 0.0004128576172244895,
      "grad_norm": 8720.019724748334,
      "learning_rate": 2.966353624110886e-07,
      "loss": 1.734,
      "step": 113760
    },
    {
      "epoch": 0.0004129737515753262,
      "grad_norm": 10195.212602001,
      "learning_rate": 2.965936085110086e-07,
      "loss": 1.7334,
      "step": 113792
    },
    {
      "epoch": 0.0004130898859261629,
      "grad_norm": 10534.149040145578,
      "learning_rate": 2.96551872237595e-07,
      "loss": 1.7225,
      "step": 113824
    },
    {
      "epoch": 0.0004132060202769996,
      "grad_norm": 10197.185101781766,
      "learning_rate": 2.9651015357844936e-07,
      "loss": 1.7338,
      "step": 113856
    },
    {
      "epoch": 0.0004133221546278363,
      "grad_norm": 8922.424334226658,
      "learning_rate": 2.964684525211852e-07,
      "loss": 1.7469,
      "step": 113888
    },
    {
      "epoch": 0.000413438288978673,
      "grad_norm": 9025.840348687761,
      "learning_rate": 2.9642676905342845e-07,
      "loss": 1.7716,
      "step": 113920
    },
    {
      "epoch": 0.0004135544233295097,
      "grad_norm": 7755.582247645884,
      "learning_rate": 2.9638510316281714e-07,
      "loss": 1.7612,
      "step": 113952
    },
    {
      "epoch": 0.0004136705576803464,
      "grad_norm": 9804.045083535671,
      "learning_rate": 2.9634345483700154e-07,
      "loss": 1.7356,
      "step": 113984
    },
    {
      "epoch": 0.00041378669203118314,
      "grad_norm": 9760.80068437011,
      "learning_rate": 2.9630182406364396e-07,
      "loss": 1.7403,
      "step": 114016
    },
    {
      "epoch": 0.00041390282638201984,
      "grad_norm": 8897.65339850907,
      "learning_rate": 2.9626021083041893e-07,
      "loss": 1.7494,
      "step": 114048
    },
    {
      "epoch": 0.00041401896073285654,
      "grad_norm": 8358.622613804262,
      "learning_rate": 2.96218615125013e-07,
      "loss": 1.752,
      "step": 114080
    },
    {
      "epoch": 0.00041413509508369324,
      "grad_norm": 8843.244088002999,
      "learning_rate": 2.9617703693512484e-07,
      "loss": 1.7797,
      "step": 114112
    },
    {
      "epoch": 0.00041425122943452994,
      "grad_norm": 10332.65590252574,
      "learning_rate": 2.96136774755104e-07,
      "loss": 1.7845,
      "step": 114144
    },
    {
      "epoch": 0.00041436736378536664,
      "grad_norm": 9262.978138806115,
      "learning_rate": 2.9609523101298943e-07,
      "loss": 1.7672,
      "step": 114176
    },
    {
      "epoch": 0.00041448349813620334,
      "grad_norm": 10002.383016061722,
      "learning_rate": 2.960537047499443e-07,
      "loss": 1.7782,
      "step": 114208
    },
    {
      "epoch": 0.00041459963248704004,
      "grad_norm": 10600.772990683274,
      "learning_rate": 2.9601219595371514e-07,
      "loss": 1.784,
      "step": 114240
    },
    {
      "epoch": 0.00041471576683787674,
      "grad_norm": 8557.31196112424,
      "learning_rate": 2.959707046120606e-07,
      "loss": 1.7478,
      "step": 114272
    },
    {
      "epoch": 0.0004148319011887135,
      "grad_norm": 8389.561967111274,
      "learning_rate": 2.959292307127512e-07,
      "loss": 1.7555,
      "step": 114304
    },
    {
      "epoch": 0.0004149480355395502,
      "grad_norm": 8349.110371770157,
      "learning_rate": 2.9588777424356963e-07,
      "loss": 1.7583,
      "step": 114336
    },
    {
      "epoch": 0.0004150641698903869,
      "grad_norm": 8966.028998391652,
      "learning_rate": 2.958463351923104e-07,
      "loss": 1.7449,
      "step": 114368
    },
    {
      "epoch": 0.0004151803042412236,
      "grad_norm": 9349.15172622629,
      "learning_rate": 2.9580491354678e-07,
      "loss": 1.7476,
      "step": 114400
    },
    {
      "epoch": 0.0004152964385920603,
      "grad_norm": 8869.947688684528,
      "learning_rate": 2.957635092947969e-07,
      "loss": 1.7206,
      "step": 114432
    },
    {
      "epoch": 0.000415412572942897,
      "grad_norm": 8930.874761186611,
      "learning_rate": 2.9572212242419164e-07,
      "loss": 1.729,
      "step": 114464
    },
    {
      "epoch": 0.0004155287072937337,
      "grad_norm": 10651.153364776981,
      "learning_rate": 2.9568075292280633e-07,
      "loss": 1.7491,
      "step": 114496
    },
    {
      "epoch": 0.0004156448416445704,
      "grad_norm": 9551.703722373302,
      "learning_rate": 2.9563940077849534e-07,
      "loss": 1.761,
      "step": 114528
    },
    {
      "epoch": 0.0004157609759954071,
      "grad_norm": 9118.89752108225,
      "learning_rate": 2.9559806597912477e-07,
      "loss": 1.7403,
      "step": 114560
    },
    {
      "epoch": 0.00041587711034624385,
      "grad_norm": 9006.379072635129,
      "learning_rate": 2.955567485125725e-07,
      "loss": 1.7275,
      "step": 114592
    },
    {
      "epoch": 0.00041599324469708055,
      "grad_norm": 9478.692736870416,
      "learning_rate": 2.9551544836672847e-07,
      "loss": 1.7427,
      "step": 114624
    },
    {
      "epoch": 0.00041610937904791725,
      "grad_norm": 9499.46219530348,
      "learning_rate": 2.9547416552949433e-07,
      "loss": 1.758,
      "step": 114656
    },
    {
      "epoch": 0.00041622551339875395,
      "grad_norm": 9803.966646210094,
      "learning_rate": 2.954328999887836e-07,
      "loss": 1.7571,
      "step": 114688
    },
    {
      "epoch": 0.00041634164774959065,
      "grad_norm": 9749.055133703983,
      "learning_rate": 2.9539165173252156e-07,
      "loss": 1.7449,
      "step": 114720
    },
    {
      "epoch": 0.00041645778210042735,
      "grad_norm": 11890.255506085645,
      "learning_rate": 2.953504207486454e-07,
      "loss": 1.7442,
      "step": 114752
    },
    {
      "epoch": 0.00041657391645126405,
      "grad_norm": 9035.23259246822,
      "learning_rate": 2.95309207025104e-07,
      "loss": 1.7356,
      "step": 114784
    },
    {
      "epoch": 0.00041669005080210075,
      "grad_norm": 9548.804741955928,
      "learning_rate": 2.9526801054985795e-07,
      "loss": 1.7422,
      "step": 114816
    },
    {
      "epoch": 0.00041680618515293745,
      "grad_norm": 8432.057518779151,
      "learning_rate": 2.9522683131087975e-07,
      "loss": 1.772,
      "step": 114848
    },
    {
      "epoch": 0.0004169223195037742,
      "grad_norm": 11411.226577366693,
      "learning_rate": 2.9518566929615363e-07,
      "loss": 1.7632,
      "step": 114880
    },
    {
      "epoch": 0.0004170384538546109,
      "grad_norm": 9476.933048196552,
      "learning_rate": 2.9514452449367535e-07,
      "loss": 1.7744,
      "step": 114912
    },
    {
      "epoch": 0.0004171545882054476,
      "grad_norm": 9571.482643770503,
      "learning_rate": 2.951033968914525e-07,
      "loss": 1.7603,
      "step": 114944
    },
    {
      "epoch": 0.0004172707225562843,
      "grad_norm": 9910.763038232728,
      "learning_rate": 2.950622864775045e-07,
      "loss": 1.767,
      "step": 114976
    },
    {
      "epoch": 0.000417386856907121,
      "grad_norm": 9457.110340902234,
      "learning_rate": 2.950211932398622e-07,
      "loss": 1.7572,
      "step": 115008
    },
    {
      "epoch": 0.0004175029912579577,
      "grad_norm": 9326.039888398505,
      "learning_rate": 2.949801171665683e-07,
      "loss": 1.7603,
      "step": 115040
    },
    {
      "epoch": 0.0004176191256087944,
      "grad_norm": 10200.706348091784,
      "learning_rate": 2.949390582456771e-07,
      "loss": 1.7689,
      "step": 115072
    },
    {
      "epoch": 0.0004177352599596311,
      "grad_norm": 9284.834301160145,
      "learning_rate": 2.9489801646525437e-07,
      "loss": 1.766,
      "step": 115104
    },
    {
      "epoch": 0.0004178513943104678,
      "grad_norm": 10871.324666295272,
      "learning_rate": 2.9485827357459736e-07,
      "loss": 1.7636,
      "step": 115136
    },
    {
      "epoch": 0.00041796752866130456,
      "grad_norm": 9443.501681050308,
      "learning_rate": 2.948172655046415e-07,
      "loss": 1.7467,
      "step": 115168
    },
    {
      "epoch": 0.00041808366301214126,
      "grad_norm": 10592.025490905882,
      "learning_rate": 2.9477627453979327e-07,
      "loss": 1.7297,
      "step": 115200
    },
    {
      "epoch": 0.00041819979736297796,
      "grad_norm": 8365.158336815868,
      "learning_rate": 2.947353006681647e-07,
      "loss": 1.7309,
      "step": 115232
    },
    {
      "epoch": 0.00041831593171381466,
      "grad_norm": 9434.658976348854,
      "learning_rate": 2.946943438778793e-07,
      "loss": 1.7515,
      "step": 115264
    },
    {
      "epoch": 0.00041843206606465136,
      "grad_norm": 10055.17657726606,
      "learning_rate": 2.946534041570722e-07,
      "loss": 1.7468,
      "step": 115296
    },
    {
      "epoch": 0.00041854820041548806,
      "grad_norm": 9582.510318282992,
      "learning_rate": 2.9461248149388995e-07,
      "loss": 1.746,
      "step": 115328
    },
    {
      "epoch": 0.00041866433476632476,
      "grad_norm": 9987.206316082591,
      "learning_rate": 2.945715758764907e-07,
      "loss": 1.7488,
      "step": 115360
    },
    {
      "epoch": 0.00041878046911716146,
      "grad_norm": 8397.29408797858,
      "learning_rate": 2.9453068729304405e-07,
      "loss": 1.7471,
      "step": 115392
    },
    {
      "epoch": 0.00041889660346799816,
      "grad_norm": 9379.08556310262,
      "learning_rate": 2.944898157317311e-07,
      "loss": 1.7518,
      "step": 115424
    },
    {
      "epoch": 0.0004190127378188349,
      "grad_norm": 11701.706713125226,
      "learning_rate": 2.944489611807446e-07,
      "loss": 1.7656,
      "step": 115456
    },
    {
      "epoch": 0.0004191288721696716,
      "grad_norm": 9615.613344971813,
      "learning_rate": 2.944081236282885e-07,
      "loss": 1.7355,
      "step": 115488
    },
    {
      "epoch": 0.0004192450065205083,
      "grad_norm": 9844.030678538136,
      "learning_rate": 2.943673030625783e-07,
      "loss": 1.7296,
      "step": 115520
    },
    {
      "epoch": 0.000419361140871345,
      "grad_norm": 8587.214216496523,
      "learning_rate": 2.94326499471841e-07,
      "loss": 1.7183,
      "step": 115552
    },
    {
      "epoch": 0.0004194772752221817,
      "grad_norm": 10670.458097007833,
      "learning_rate": 2.942857128443149e-07,
      "loss": 1.7322,
      "step": 115584
    },
    {
      "epoch": 0.0004195934095730184,
      "grad_norm": 8413.095268686788,
      "learning_rate": 2.9424494316824986e-07,
      "loss": 1.7507,
      "step": 115616
    },
    {
      "epoch": 0.0004197095439238551,
      "grad_norm": 10720.473683564545,
      "learning_rate": 2.9420419043190706e-07,
      "loss": 1.7591,
      "step": 115648
    },
    {
      "epoch": 0.0004198256782746918,
      "grad_norm": 8795.354000834759,
      "learning_rate": 2.9416345462355894e-07,
      "loss": 1.7672,
      "step": 115680
    },
    {
      "epoch": 0.0004199418126255285,
      "grad_norm": 10239.606730729458,
      "learning_rate": 2.9412273573148946e-07,
      "loss": 1.7571,
      "step": 115712
    },
    {
      "epoch": 0.00042005794697636526,
      "grad_norm": 9294.32213773549,
      "learning_rate": 2.940820337439939e-07,
      "loss": 1.7733,
      "step": 115744
    },
    {
      "epoch": 0.00042017408132720196,
      "grad_norm": 8892.393603524306,
      "learning_rate": 2.9404134864937884e-07,
      "loss": 1.7804,
      "step": 115776
    },
    {
      "epoch": 0.00042029021567803866,
      "grad_norm": 8062.536201469114,
      "learning_rate": 2.9400068043596224e-07,
      "loss": 1.7685,
      "step": 115808
    },
    {
      "epoch": 0.00042040635002887536,
      "grad_norm": 9171.868184835628,
      "learning_rate": 2.9396002909207324e-07,
      "loss": 1.7708,
      "step": 115840
    },
    {
      "epoch": 0.00042052248437971206,
      "grad_norm": 8939.489247155007,
      "learning_rate": 2.9391939460605234e-07,
      "loss": 1.7811,
      "step": 115872
    },
    {
      "epoch": 0.00042063861873054876,
      "grad_norm": 8137.758413715658,
      "learning_rate": 2.9387877696625145e-07,
      "loss": 1.7826,
      "step": 115904
    },
    {
      "epoch": 0.00042075475308138546,
      "grad_norm": 9824.25793635326,
      "learning_rate": 2.938381761610335e-07,
      "loss": 1.7496,
      "step": 115936
    },
    {
      "epoch": 0.00042087088743222216,
      "grad_norm": 9931.451152777221,
      "learning_rate": 2.9379759217877287e-07,
      "loss": 1.7285,
      "step": 115968
    },
    {
      "epoch": 0.00042098702178305886,
      "grad_norm": 9998.427076295551,
      "learning_rate": 2.9375702500785513e-07,
      "loss": 1.7255,
      "step": 116000
    },
    {
      "epoch": 0.00042110315613389556,
      "grad_norm": 9121.201455948663,
      "learning_rate": 2.9371647463667695e-07,
      "loss": 1.7356,
      "step": 116032
    },
    {
      "epoch": 0.0004212192904847323,
      "grad_norm": 10164.698126358697,
      "learning_rate": 2.9367594105364637e-07,
      "loss": 1.7498,
      "step": 116064
    },
    {
      "epoch": 0.000421335424835569,
      "grad_norm": 10318.26904088084,
      "learning_rate": 2.936354242471826e-07,
      "loss": 1.7517,
      "step": 116096
    },
    {
      "epoch": 0.0004214515591864057,
      "grad_norm": 11986.19822963061,
      "learning_rate": 2.935949242057159e-07,
      "loss": 1.7267,
      "step": 116128
    },
    {
      "epoch": 0.0004215676935372424,
      "grad_norm": 10560.118181156875,
      "learning_rate": 2.935557057669614e-07,
      "loss": 1.7397,
      "step": 116160
    },
    {
      "epoch": 0.0004216838278880791,
      "grad_norm": 9676.645389803225,
      "learning_rate": 2.9351523869781526e-07,
      "loss": 1.7538,
      "step": 116192
    },
    {
      "epoch": 0.0004217999622389158,
      "grad_norm": 9829.685752861074,
      "learning_rate": 2.9347478835938454e-07,
      "loss": 1.7583,
      "step": 116224
    },
    {
      "epoch": 0.0004219160965897525,
      "grad_norm": 9046.547186634247,
      "learning_rate": 2.9343435474014387e-07,
      "loss": 1.7254,
      "step": 116256
    },
    {
      "epoch": 0.0004220322309405892,
      "grad_norm": 8837.535176733387,
      "learning_rate": 2.93393937828579e-07,
      "loss": 1.7292,
      "step": 116288
    },
    {
      "epoch": 0.0004221483652914259,
      "grad_norm": 10340.111508102802,
      "learning_rate": 2.9335353761318667e-07,
      "loss": 1.72,
      "step": 116320
    },
    {
      "epoch": 0.00042226449964226267,
      "grad_norm": 9304.655501414332,
      "learning_rate": 2.9331315408247483e-07,
      "loss": 1.7428,
      "step": 116352
    },
    {
      "epoch": 0.00042238063399309937,
      "grad_norm": 9037.103739583828,
      "learning_rate": 2.9327278722496235e-07,
      "loss": 1.7708,
      "step": 116384
    },
    {
      "epoch": 0.00042249676834393607,
      "grad_norm": 9819.289689178133,
      "learning_rate": 2.932324370291793e-07,
      "loss": 1.7621,
      "step": 116416
    },
    {
      "epoch": 0.00042261290269477277,
      "grad_norm": 9544.9934520669,
      "learning_rate": 2.931921034836667e-07,
      "loss": 1.768,
      "step": 116448
    },
    {
      "epoch": 0.00042272903704560947,
      "grad_norm": 8464.446113007041,
      "learning_rate": 2.931517865769767e-07,
      "loss": 1.778,
      "step": 116480
    },
    {
      "epoch": 0.00042284517139644617,
      "grad_norm": 8679.148575753268,
      "learning_rate": 2.9311148629767236e-07,
      "loss": 1.7796,
      "step": 116512
    },
    {
      "epoch": 0.00042296130574728287,
      "grad_norm": 9730.4041026054,
      "learning_rate": 2.930712026343278e-07,
      "loss": 1.7587,
      "step": 116544
    },
    {
      "epoch": 0.00042307744009811957,
      "grad_norm": 9569.78432358849,
      "learning_rate": 2.9303093557552816e-07,
      "loss": 1.7559,
      "step": 116576
    },
    {
      "epoch": 0.00042319357444895627,
      "grad_norm": 8481.878683404992,
      "learning_rate": 2.929906851098694e-07,
      "loss": 1.7618,
      "step": 116608
    },
    {
      "epoch": 0.000423309708799793,
      "grad_norm": 9405.481274235784,
      "learning_rate": 2.9295045122595867e-07,
      "loss": 1.7595,
      "step": 116640
    },
    {
      "epoch": 0.0004234258431506297,
      "grad_norm": 9826.921898539746,
      "learning_rate": 2.9291023391241393e-07,
      "loss": 1.7786,
      "step": 116672
    },
    {
      "epoch": 0.0004235419775014664,
      "grad_norm": 10097.873637553601,
      "learning_rate": 2.928700331578641e-07,
      "loss": 1.7506,
      "step": 116704
    },
    {
      "epoch": 0.0004236581118523031,
      "grad_norm": 8192.61057294926,
      "learning_rate": 2.928298489509489e-07,
      "loss": 1.7329,
      "step": 116736
    },
    {
      "epoch": 0.0004237742462031398,
      "grad_norm": 8801.521686617605,
      "learning_rate": 2.927896812803192e-07,
      "loss": 1.7333,
      "step": 116768
    },
    {
      "epoch": 0.0004238903805539765,
      "grad_norm": 10211.421252695434,
      "learning_rate": 2.927495301346367e-07,
      "loss": 1.7641,
      "step": 116800
    },
    {
      "epoch": 0.0004240065149048132,
      "grad_norm": 9754.654478760383,
      "learning_rate": 2.9270939550257377e-07,
      "loss": 1.7263,
      "step": 116832
    },
    {
      "epoch": 0.0004241226492556499,
      "grad_norm": 8256.52323923333,
      "learning_rate": 2.926692773728139e-07,
      "loss": 1.7318,
      "step": 116864
    },
    {
      "epoch": 0.0004242387836064866,
      "grad_norm": 9921.677882293901,
      "learning_rate": 2.9262917573405137e-07,
      "loss": 1.7357,
      "step": 116896
    },
    {
      "epoch": 0.0004243549179573234,
      "grad_norm": 9762.04281900054,
      "learning_rate": 2.925890905749911e-07,
      "loss": 1.7419,
      "step": 116928
    },
    {
      "epoch": 0.0004244710523081601,
      "grad_norm": 10071.123373288603,
      "learning_rate": 2.925490218843492e-07,
      "loss": 1.7629,
      "step": 116960
    },
    {
      "epoch": 0.0004245871866589968,
      "grad_norm": 9713.39116889668,
      "learning_rate": 2.9250896965085226e-07,
      "loss": 1.7553,
      "step": 116992
    },
    {
      "epoch": 0.0004247033210098335,
      "grad_norm": 9770.17891340788,
      "learning_rate": 2.924689338632379e-07,
      "loss": 1.7407,
      "step": 117024
    },
    {
      "epoch": 0.0004248194553606702,
      "grad_norm": 8838.411395720386,
      "learning_rate": 2.9242891451025435e-07,
      "loss": 1.7534,
      "step": 117056
    },
    {
      "epoch": 0.0004249355897115069,
      "grad_norm": 9218.447374693853,
      "learning_rate": 2.923889115806608e-07,
      "loss": 1.7619,
      "step": 117088
    },
    {
      "epoch": 0.0004250517240623436,
      "grad_norm": 9079.779292471816,
      "learning_rate": 2.92348925063227e-07,
      "loss": 1.7437,
      "step": 117120
    },
    {
      "epoch": 0.0004251678584131803,
      "grad_norm": 9152.264091469387,
      "learning_rate": 2.92310203764729e-07,
      "loss": 1.7445,
      "step": 117152
    },
    {
      "epoch": 0.000425283992764017,
      "grad_norm": 9922.794364492293,
      "learning_rate": 2.9227024952595776e-07,
      "loss": 1.7524,
      "step": 117184
    },
    {
      "epoch": 0.00042540012711485373,
      "grad_norm": 10897.722147311335,
      "learning_rate": 2.922303116660701e-07,
      "loss": 1.75,
      "step": 117216
    },
    {
      "epoch": 0.00042551626146569043,
      "grad_norm": 9195.625699211556,
      "learning_rate": 2.921903901738784e-07,
      "loss": 1.7552,
      "step": 117248
    },
    {
      "epoch": 0.00042563239581652713,
      "grad_norm": 9436.21767447106,
      "learning_rate": 2.9215048503820584e-07,
      "loss": 1.7731,
      "step": 117280
    },
    {
      "epoch": 0.00042574853016736383,
      "grad_norm": 8565.951202289212,
      "learning_rate": 2.921105962478862e-07,
      "loss": 1.7662,
      "step": 117312
    },
    {
      "epoch": 0.00042586466451820053,
      "grad_norm": 10071.573660555732,
      "learning_rate": 2.9207072379176396e-07,
      "loss": 1.7506,
      "step": 117344
    },
    {
      "epoch": 0.00042598079886903723,
      "grad_norm": 9143.47417560743,
      "learning_rate": 2.9203086765869424e-07,
      "loss": 1.7566,
      "step": 117376
    },
    {
      "epoch": 0.00042609693321987393,
      "grad_norm": 9067.900969904777,
      "learning_rate": 2.919910278375428e-07,
      "loss": 1.7607,
      "step": 117408
    },
    {
      "epoch": 0.00042621306757071063,
      "grad_norm": 9114.586880380262,
      "learning_rate": 2.919512043171861e-07,
      "loss": 1.7533,
      "step": 117440
    },
    {
      "epoch": 0.00042632920192154733,
      "grad_norm": 9320.941261482125,
      "learning_rate": 2.9191139708651114e-07,
      "loss": 1.7287,
      "step": 117472
    },
    {
      "epoch": 0.0004264453362723841,
      "grad_norm": 9322.957685198406,
      "learning_rate": 2.9187160613441543e-07,
      "loss": 1.7392,
      "step": 117504
    },
    {
      "epoch": 0.0004265614706232208,
      "grad_norm": 8229.331443076042,
      "learning_rate": 2.918318314498074e-07,
      "loss": 1.7367,
      "step": 117536
    },
    {
      "epoch": 0.0004266776049740575,
      "grad_norm": 9342.184755184411,
      "learning_rate": 2.9179207302160574e-07,
      "loss": 1.7548,
      "step": 117568
    },
    {
      "epoch": 0.0004267937393248942,
      "grad_norm": 10656.157374964017,
      "learning_rate": 2.9175233083873974e-07,
      "loss": 1.761,
      "step": 117600
    },
    {
      "epoch": 0.0004269098736757309,
      "grad_norm": 10333.51072966008,
      "learning_rate": 2.9171260489014934e-07,
      "loss": 1.7654,
      "step": 117632
    },
    {
      "epoch": 0.0004270260080265676,
      "grad_norm": 10839.487995288338,
      "learning_rate": 2.9167289516478506e-07,
      "loss": 1.7829,
      "step": 117664
    },
    {
      "epoch": 0.0004271421423774043,
      "grad_norm": 10209.385290016242,
      "learning_rate": 2.916332016516079e-07,
      "loss": 1.7513,
      "step": 117696
    },
    {
      "epoch": 0.000427258276728241,
      "grad_norm": 9845.461187775816,
      "learning_rate": 2.9159352433958917e-07,
      "loss": 1.7495,
      "step": 117728
    },
    {
      "epoch": 0.0004273744110790777,
      "grad_norm": 8107.025101725047,
      "learning_rate": 2.9155386321771105e-07,
      "loss": 1.7221,
      "step": 117760
    },
    {
      "epoch": 0.00042749054542991444,
      "grad_norm": 9534.181034572397,
      "learning_rate": 2.915142182749659e-07,
      "loss": 1.714,
      "step": 117792
    },
    {
      "epoch": 0.00042760667978075114,
      "grad_norm": 10019.422737862695,
      "learning_rate": 2.914745895003568e-07,
      "loss": 1.7302,
      "step": 117824
    },
    {
      "epoch": 0.00042772281413158784,
      "grad_norm": 10519.457590579468,
      "learning_rate": 2.9143497688289697e-07,
      "loss": 1.7327,
      "step": 117856
    },
    {
      "epoch": 0.00042783894848242454,
      "grad_norm": 9519.364264487414,
      "learning_rate": 2.913953804116104e-07,
      "loss": 1.7567,
      "step": 117888
    },
    {
      "epoch": 0.00042795508283326124,
      "grad_norm": 10156.290267612481,
      "learning_rate": 2.913558000755314e-07,
      "loss": 1.7781,
      "step": 117920
    },
    {
      "epoch": 0.00042807121718409794,
      "grad_norm": 8463.45496827389,
      "learning_rate": 2.913162358637046e-07,
      "loss": 1.7655,
      "step": 117952
    },
    {
      "epoch": 0.00042818735153493464,
      "grad_norm": 8102.087138509435,
      "learning_rate": 2.9127668776518526e-07,
      "loss": 1.7447,
      "step": 117984
    },
    {
      "epoch": 0.00042830348588577134,
      "grad_norm": 10413.031547056793,
      "learning_rate": 2.912371557690388e-07,
      "loss": 1.7538,
      "step": 118016
    },
    {
      "epoch": 0.00042841962023660804,
      "grad_norm": 11053.085903945557,
      "learning_rate": 2.911976398643412e-07,
      "loss": 1.7521,
      "step": 118048
    },
    {
      "epoch": 0.0004285357545874448,
      "grad_norm": 9633.88561277328,
      "learning_rate": 2.911581400401787e-07,
      "loss": 1.7435,
      "step": 118080
    },
    {
      "epoch": 0.0004286518889382815,
      "grad_norm": 8006.279660366605,
      "learning_rate": 2.9111865628564796e-07,
      "loss": 1.7496,
      "step": 118112
    },
    {
      "epoch": 0.0004287680232891182,
      "grad_norm": 9387.87483938724,
      "learning_rate": 2.91079188589856e-07,
      "loss": 1.756,
      "step": 118144
    },
    {
      "epoch": 0.0004288841576399549,
      "grad_norm": 8640.76408658401,
      "learning_rate": 2.910409695631143e-07,
      "loss": 1.7654,
      "step": 118176
    },
    {
      "epoch": 0.0004290002919907916,
      "grad_norm": 10087.761892511144,
      "learning_rate": 2.9100153345117094e-07,
      "loss": 1.7634,
      "step": 118208
    },
    {
      "epoch": 0.0004291164263416283,
      "grad_norm": 9013.083323702273,
      "learning_rate": 2.909621133656885e-07,
      "loss": 1.7707,
      "step": 118240
    },
    {
      "epoch": 0.000429232560692465,
      "grad_norm": 9194.253313891237,
      "learning_rate": 2.9092270929581513e-07,
      "loss": 1.7488,
      "step": 118272
    },
    {
      "epoch": 0.0004293486950433017,
      "grad_norm": 9939.664984293988,
      "learning_rate": 2.9088332123070885e-07,
      "loss": 1.7379,
      "step": 118304
    },
    {
      "epoch": 0.0004294648293941384,
      "grad_norm": 9182.530588024196,
      "learning_rate": 2.908439491595382e-07,
      "loss": 1.7489,
      "step": 118336
    },
    {
      "epoch": 0.00042958096374497514,
      "grad_norm": 9912.860333929859,
      "learning_rate": 2.908045930714819e-07,
      "loss": 1.7345,
      "step": 118368
    },
    {
      "epoch": 0.00042969709809581184,
      "grad_norm": 9317.301648009472,
      "learning_rate": 2.907652529557291e-07,
      "loss": 1.735,
      "step": 118400
    },
    {
      "epoch": 0.00042981323244664854,
      "grad_norm": 9601.671208701118,
      "learning_rate": 2.90725928801479e-07,
      "loss": 1.7581,
      "step": 118432
    },
    {
      "epoch": 0.00042992936679748524,
      "grad_norm": 8395.363184520369,
      "learning_rate": 2.9068662059794087e-07,
      "loss": 1.7537,
      "step": 118464
    },
    {
      "epoch": 0.00043004550114832194,
      "grad_norm": 9268.45078748331,
      "learning_rate": 2.906473283343346e-07,
      "loss": 1.7646,
      "step": 118496
    },
    {
      "epoch": 0.00043016163549915864,
      "grad_norm": 9365.640181002045,
      "learning_rate": 2.9060805199988995e-07,
      "loss": 1.7604,
      "step": 118528
    },
    {
      "epoch": 0.00043027776984999534,
      "grad_norm": 10377.898438508637,
      "learning_rate": 2.9056879158384705e-07,
      "loss": 1.7243,
      "step": 118560
    },
    {
      "epoch": 0.00043039390420083204,
      "grad_norm": 9356.248179692542,
      "learning_rate": 2.9052954707545596e-07,
      "loss": 1.7235,
      "step": 118592
    },
    {
      "epoch": 0.00043051003855166874,
      "grad_norm": 9057.439483650995,
      "learning_rate": 2.904903184639772e-07,
      "loss": 1.7309,
      "step": 118624
    },
    {
      "epoch": 0.0004306261729025055,
      "grad_norm": 9488.48913157411,
      "learning_rate": 2.904511057386813e-07,
      "loss": 1.7421,
      "step": 118656
    },
    {
      "epoch": 0.0004307423072533422,
      "grad_norm": 8672.470697557876,
      "learning_rate": 2.904119088888489e-07,
      "loss": 1.7587,
      "step": 118688
    },
    {
      "epoch": 0.0004308584416041789,
      "grad_norm": 9911.472544480965,
      "learning_rate": 2.9037272790377083e-07,
      "loss": 1.7471,
      "step": 118720
    },
    {
      "epoch": 0.0004309745759550156,
      "grad_norm": 11338.529887070898,
      "learning_rate": 2.9033356277274793e-07,
      "loss": 1.7585,
      "step": 118752
    },
    {
      "epoch": 0.0004310907103058523,
      "grad_norm": 9752.785960944699,
      "learning_rate": 2.902944134850912e-07,
      "loss": 1.7725,
      "step": 118784
    },
    {
      "epoch": 0.000431206844656689,
      "grad_norm": 10297.574665910415,
      "learning_rate": 2.902552800301218e-07,
      "loss": 1.7978,
      "step": 118816
    },
    {
      "epoch": 0.0004313229790075257,
      "grad_norm": 10083.069175603228,
      "learning_rate": 2.902161623971708e-07,
      "loss": 1.781,
      "step": 118848
    },
    {
      "epoch": 0.0004314391133583624,
      "grad_norm": 9949.242584237254,
      "learning_rate": 2.901770605755796e-07,
      "loss": 1.7574,
      "step": 118880
    },
    {
      "epoch": 0.0004315552477091991,
      "grad_norm": 9599.293828193822,
      "learning_rate": 2.9013797455469926e-07,
      "loss": 1.7416,
      "step": 118912
    },
    {
      "epoch": 0.00043167138206003585,
      "grad_norm": 8767.400869128775,
      "learning_rate": 2.9009890432389114e-07,
      "loss": 1.7555,
      "step": 118944
    },
    {
      "epoch": 0.00043178751641087255,
      "grad_norm": 10013.36466928075,
      "learning_rate": 2.900598498725267e-07,
      "loss": 1.7081,
      "step": 118976
    },
    {
      "epoch": 0.00043190365076170925,
      "grad_norm": 8754.475769570672,
      "learning_rate": 2.900208111899872e-07,
      "loss": 1.7256,
      "step": 119008
    },
    {
      "epoch": 0.00043201978511254595,
      "grad_norm": 10433.385069094307,
      "learning_rate": 2.89981788265664e-07,
      "loss": 1.741,
      "step": 119040
    },
    {
      "epoch": 0.00043213591946338265,
      "grad_norm": 9172.52462520543,
      "learning_rate": 2.8994278108895846e-07,
      "loss": 1.7382,
      "step": 119072
    },
    {
      "epoch": 0.00043225205381421935,
      "grad_norm": 9694.94775643479,
      "learning_rate": 2.899037896492819e-07,
      "loss": 1.7473,
      "step": 119104
    },
    {
      "epoch": 0.00043236818816505605,
      "grad_norm": 10038.281028144213,
      "learning_rate": 2.898648139360556e-07,
      "loss": 1.7499,
      "step": 119136
    },
    {
      "epoch": 0.00043248432251589275,
      "grad_norm": 9929.325958996411,
      "learning_rate": 2.8982707120084587e-07,
      "loss": 1.7531,
      "step": 119168
    },
    {
      "epoch": 0.00043260045686672945,
      "grad_norm": 10488.929401993322,
      "learning_rate": 2.897881264181921e-07,
      "loss": 1.7497,
      "step": 119200
    },
    {
      "epoch": 0.0004327165912175662,
      "grad_norm": 8903.308149221839,
      "learning_rate": 2.897491973306416e-07,
      "loss": 1.7505,
      "step": 119232
    },
    {
      "epoch": 0.0004328327255684029,
      "grad_norm": 10064.18302695256,
      "learning_rate": 2.8971028392765517e-07,
      "loss": 1.7563,
      "step": 119264
    },
    {
      "epoch": 0.0004329488599192396,
      "grad_norm": 10425.480132828416,
      "learning_rate": 2.8967138619870326e-07,
      "loss": 1.7299,
      "step": 119296
    },
    {
      "epoch": 0.0004330649942700763,
      "grad_norm": 8654.89040947371,
      "learning_rate": 2.8963250413326656e-07,
      "loss": 1.7439,
      "step": 119328
    },
    {
      "epoch": 0.000433181128620913,
      "grad_norm": 10691.279624067458,
      "learning_rate": 2.895936377208354e-07,
      "loss": 1.7508,
      "step": 119360
    },
    {
      "epoch": 0.0004332972629717497,
      "grad_norm": 10913.656215952562,
      "learning_rate": 2.8955478695091e-07,
      "loss": 1.774,
      "step": 119392
    },
    {
      "epoch": 0.0004334133973225864,
      "grad_norm": 8295.532773728279,
      "learning_rate": 2.8951595181300053e-07,
      "loss": 1.78,
      "step": 119424
    },
    {
      "epoch": 0.0004335295316734231,
      "grad_norm": 11031.916605921204,
      "learning_rate": 2.894771322966269e-07,
      "loss": 1.7885,
      "step": 119456
    },
    {
      "epoch": 0.0004336456660242598,
      "grad_norm": 9332.86740503689,
      "learning_rate": 2.8943832839131895e-07,
      "loss": 1.7421,
      "step": 119488
    },
    {
      "epoch": 0.00043376180037509656,
      "grad_norm": 8842.256499333187,
      "learning_rate": 2.8939954008661633e-07,
      "loss": 1.7452,
      "step": 119520
    },
    {
      "epoch": 0.00043387793472593326,
      "grad_norm": 9087.031528502583,
      "learning_rate": 2.893607673720685e-07,
      "loss": 1.7614,
      "step": 119552
    },
    {
      "epoch": 0.00043399406907676996,
      "grad_norm": 9763.681785064484,
      "learning_rate": 2.893220102372346e-07,
      "loss": 1.7462,
      "step": 119584
    },
    {
      "epoch": 0.00043411020342760666,
      "grad_norm": 10562.000189358074,
      "learning_rate": 2.892832686716837e-07,
      "loss": 1.7618,
      "step": 119616
    },
    {
      "epoch": 0.00043422633777844336,
      "grad_norm": 9456.019247019329,
      "learning_rate": 2.8924454266499453e-07,
      "loss": 1.7591,
      "step": 119648
    },
    {
      "epoch": 0.00043434247212928006,
      "grad_norm": 8969.376789944774,
      "learning_rate": 2.892058322067557e-07,
      "loss": 1.7552,
      "step": 119680
    },
    {
      "epoch": 0.00043445860648011676,
      "grad_norm": 9236.957940794144,
      "learning_rate": 2.8916713728656557e-07,
      "loss": 1.7409,
      "step": 119712
    },
    {
      "epoch": 0.00043457474083095346,
      "grad_norm": 8674.016947181968,
      "learning_rate": 2.891284578940321e-07,
      "loss": 1.7435,
      "step": 119744
    },
    {
      "epoch": 0.00043469087518179016,
      "grad_norm": 11614.81398904003,
      "learning_rate": 2.890897940187731e-07,
      "loss": 1.7308,
      "step": 119776
    },
    {
      "epoch": 0.0004348070095326269,
      "grad_norm": 9881.492195007797,
      "learning_rate": 2.890511456504161e-07,
      "loss": 1.7264,
      "step": 119808
    },
    {
      "epoch": 0.0004349231438834636,
      "grad_norm": 10720.614348067933,
      "learning_rate": 2.890125127785982e-07,
      "loss": 1.7238,
      "step": 119840
    },
    {
      "epoch": 0.0004350392782343003,
      "grad_norm": 9671.666971106893,
      "learning_rate": 2.889738953929664e-07,
      "loss": 1.7296,
      "step": 119872
    },
    {
      "epoch": 0.000435155412585137,
      "grad_norm": 11027.010655658225,
      "learning_rate": 2.8893529348317715e-07,
      "loss": 1.7334,
      "step": 119904
    },
    {
      "epoch": 0.0004352715469359737,
      "grad_norm": 10007.733209873251,
      "learning_rate": 2.888967070388968e-07,
      "loss": 1.7558,
      "step": 119936
    },
    {
      "epoch": 0.0004353876812868104,
      "grad_norm": 10221.725294684846,
      "learning_rate": 2.888581360498011e-07,
      "loss": 1.7603,
      "step": 119968
    },
    {
      "epoch": 0.0004355038156376471,
      "grad_norm": 10536.822101563639,
      "learning_rate": 2.8881958050557573e-07,
      "loss": 1.7751,
      "step": 120000
    },
    {
      "epoch": 0.0004356199499884838,
      "grad_norm": 9371.304925142496,
      "learning_rate": 2.8878104039591584e-07,
      "loss": 1.7691,
      "step": 120032
    },
    {
      "epoch": 0.0004357360843393205,
      "grad_norm": 10732.011926940819,
      "learning_rate": 2.8874251571052617e-07,
      "loss": 1.7562,
      "step": 120064
    },
    {
      "epoch": 0.00043585221869015727,
      "grad_norm": 9568.430592317634,
      "learning_rate": 2.887040064391212e-07,
      "loss": 1.734,
      "step": 120096
    },
    {
      "epoch": 0.00043596835304099397,
      "grad_norm": 9833.534359527097,
      "learning_rate": 2.8866551257142487e-07,
      "loss": 1.7325,
      "step": 120128
    },
    {
      "epoch": 0.00043608448739183066,
      "grad_norm": 16796.03262678422,
      "learning_rate": 2.886270340971709e-07,
      "loss": 1.7496,
      "step": 120160
    },
    {
      "epoch": 0.00043620062174266736,
      "grad_norm": 10847.765299820974,
      "learning_rate": 2.8858977274494905e-07,
      "loss": 1.7695,
      "step": 120192
    },
    {
      "epoch": 0.00043631675609350406,
      "grad_norm": 9412.687182733738,
      "learning_rate": 2.8855132454656936e-07,
      "loss": 1.7738,
      "step": 120224
    },
    {
      "epoch": 0.00043643289044434076,
      "grad_norm": 7847.433465789945,
      "learning_rate": 2.8851289171120993e-07,
      "loss": 1.7612,
      "step": 120256
    },
    {
      "epoch": 0.00043654902479517746,
      "grad_norm": 8847.757795057458,
      "learning_rate": 2.884744742286423e-07,
      "loss": 1.7449,
      "step": 120288
    },
    {
      "epoch": 0.00043666515914601416,
      "grad_norm": 8662.040637170898,
      "learning_rate": 2.884360720886475e-07,
      "loss": 1.7511,
      "step": 120320
    },
    {
      "epoch": 0.00043678129349685086,
      "grad_norm": 9928.928743827302,
      "learning_rate": 2.883976852810163e-07,
      "loss": 1.7644,
      "step": 120352
    },
    {
      "epoch": 0.0004368974278476876,
      "grad_norm": 10107.477430100946,
      "learning_rate": 2.8835931379554856e-07,
      "loss": 1.7612,
      "step": 120384
    },
    {
      "epoch": 0.0004370135621985243,
      "grad_norm": 11921.86294167149,
      "learning_rate": 2.88320957622054e-07,
      "loss": 1.7503,
      "step": 120416
    },
    {
      "epoch": 0.000437129696549361,
      "grad_norm": 10413.47473228797,
      "learning_rate": 2.882826167503517e-07,
      "loss": 1.746,
      "step": 120448
    },
    {
      "epoch": 0.0004372458309001977,
      "grad_norm": 9554.654572510719,
      "learning_rate": 2.8824429117027024e-07,
      "loss": 1.741,
      "step": 120480
    },
    {
      "epoch": 0.0004373619652510344,
      "grad_norm": 9082.102179561734,
      "learning_rate": 2.8820598087164755e-07,
      "loss": 1.7396,
      "step": 120512
    },
    {
      "epoch": 0.0004374780996018711,
      "grad_norm": 9305.002847930784,
      "learning_rate": 2.8816768584433116e-07,
      "loss": 1.7387,
      "step": 120544
    },
    {
      "epoch": 0.0004375942339527078,
      "grad_norm": 10963.583082186226,
      "learning_rate": 2.88129406078178e-07,
      "loss": 1.7463,
      "step": 120576
    },
    {
      "epoch": 0.0004377103683035445,
      "grad_norm": 8450.803157096963,
      "learning_rate": 2.880911415630544e-07,
      "loss": 1.7551,
      "step": 120608
    },
    {
      "epoch": 0.0004378265026543812,
      "grad_norm": 9175.665970380569,
      "learning_rate": 2.8805289228883606e-07,
      "loss": 1.7453,
      "step": 120640
    },
    {
      "epoch": 0.00043794263700521797,
      "grad_norm": 8912.841410010615,
      "learning_rate": 2.8801465824540827e-07,
      "loss": 1.7409,
      "step": 120672
    },
    {
      "epoch": 0.00043805877135605467,
      "grad_norm": 8484.102545349155,
      "learning_rate": 2.8797643942266543e-07,
      "loss": 1.7397,
      "step": 120704
    },
    {
      "epoch": 0.00043817490570689137,
      "grad_norm": 11172.959858515558,
      "learning_rate": 2.879382358105116e-07,
      "loss": 1.739,
      "step": 120736
    },
    {
      "epoch": 0.00043829104005772807,
      "grad_norm": 10768.488844772974,
      "learning_rate": 2.879000473988601e-07,
      "loss": 1.7491,
      "step": 120768
    },
    {
      "epoch": 0.00043840717440856477,
      "grad_norm": 11254.55179027579,
      "learning_rate": 2.878618741776335e-07,
      "loss": 1.7475,
      "step": 120800
    },
    {
      "epoch": 0.00043852330875940147,
      "grad_norm": 9094.6439182631,
      "learning_rate": 2.87823716136764e-07,
      "loss": 1.729,
      "step": 120832
    },
    {
      "epoch": 0.00043863944311023817,
      "grad_norm": 11059.582451430977,
      "learning_rate": 2.877855732661928e-07,
      "loss": 1.737,
      "step": 120864
    },
    {
      "epoch": 0.00043875557746107487,
      "grad_norm": 9859.396228978729,
      "learning_rate": 2.8774744555587073e-07,
      "loss": 1.7312,
      "step": 120896
    },
    {
      "epoch": 0.00043887171181191157,
      "grad_norm": 9311.817438072978,
      "learning_rate": 2.877093329957578e-07,
      "loss": 1.7591,
      "step": 120928
    },
    {
      "epoch": 0.0004389878461627483,
      "grad_norm": 10711.165949606046,
      "learning_rate": 2.876712355758232e-07,
      "loss": 1.7767,
      "step": 120960
    },
    {
      "epoch": 0.000439103980513585,
      "grad_norm": 9355.489618400525,
      "learning_rate": 2.876331532860457e-07,
      "loss": 1.7739,
      "step": 120992
    },
    {
      "epoch": 0.0004392201148644217,
      "grad_norm": 8447.638841711925,
      "learning_rate": 2.875950861164131e-07,
      "loss": 1.7545,
      "step": 121024
    },
    {
      "epoch": 0.0004393362492152584,
      "grad_norm": 9480.435011116315,
      "learning_rate": 2.875570340569227e-07,
      "loss": 1.7597,
      "step": 121056
    },
    {
      "epoch": 0.0004394523835660951,
      "grad_norm": 8503.929797452469,
      "learning_rate": 2.875189970975808e-07,
      "loss": 1.7779,
      "step": 121088
    },
    {
      "epoch": 0.0004395685179169318,
      "grad_norm": 8560.94247148058,
      "learning_rate": 2.874809752284031e-07,
      "loss": 1.7632,
      "step": 121120
    },
    {
      "epoch": 0.0004396846522677685,
      "grad_norm": 8425.194834542404,
      "learning_rate": 2.874429684394146e-07,
      "loss": 1.7626,
      "step": 121152
    },
    {
      "epoch": 0.0004398007866186052,
      "grad_norm": 8717.147813361891,
      "learning_rate": 2.8740616373384606e-07,
      "loss": 1.7721,
      "step": 121184
    },
    {
      "epoch": 0.0004399169209694419,
      "grad_norm": 9477.320929461026,
      "learning_rate": 2.8736818660486485e-07,
      "loss": 1.7635,
      "step": 121216
    },
    {
      "epoch": 0.0004400330553202786,
      "grad_norm": 8848.259941932085,
      "learning_rate": 2.8733022452651365e-07,
      "loss": 1.7227,
      "step": 121248
    },
    {
      "epoch": 0.0004401491896711154,
      "grad_norm": 8956.414014548456,
      "learning_rate": 2.87292277488854e-07,
      "loss": 1.7417,
      "step": 121280
    },
    {
      "epoch": 0.0004402653240219521,
      "grad_norm": 9570.130406635011,
      "learning_rate": 2.8725434548195647e-07,
      "loss": 1.7327,
      "step": 121312
    },
    {
      "epoch": 0.0004403814583727888,
      "grad_norm": 8662.278222269244,
      "learning_rate": 2.8721642849590096e-07,
      "loss": 1.7282,
      "step": 121344
    },
    {
      "epoch": 0.0004404975927236255,
      "grad_norm": 10196.770861405095,
      "learning_rate": 2.8717852652077646e-07,
      "loss": 1.7509,
      "step": 121376
    },
    {
      "epoch": 0.0004406137270744622,
      "grad_norm": 8989.432907586552,
      "learning_rate": 2.871406395466812e-07,
      "loss": 1.742,
      "step": 121408
    },
    {
      "epoch": 0.0004407298614252989,
      "grad_norm": 10549.221772244624,
      "learning_rate": 2.8710276756372245e-07,
      "loss": 1.7293,
      "step": 121440
    },
    {
      "epoch": 0.0004408459957761356,
      "grad_norm": 9961.142103192786,
      "learning_rate": 2.870649105620166e-07,
      "loss": 1.736,
      "step": 121472
    },
    {
      "epoch": 0.0004409621301269723,
      "grad_norm": 9644.832606116086,
      "learning_rate": 2.8702706853168925e-07,
      "loss": 1.7398,
      "step": 121504
    },
    {
      "epoch": 0.000441078264477809,
      "grad_norm": 10064.91281631391,
      "learning_rate": 2.8698924146287513e-07,
      "loss": 1.736,
      "step": 121536
    },
    {
      "epoch": 0.00044119439882864573,
      "grad_norm": 8498.44432822855,
      "learning_rate": 2.8695142934571793e-07,
      "loss": 1.7356,
      "step": 121568
    },
    {
      "epoch": 0.00044131053317948243,
      "grad_norm": 10537.258466982767,
      "learning_rate": 2.869136321703705e-07,
      "loss": 1.745,
      "step": 121600
    },
    {
      "epoch": 0.00044142666753031913,
      "grad_norm": 9133.491665294276,
      "learning_rate": 2.8687584992699494e-07,
      "loss": 1.7255,
      "step": 121632
    },
    {
      "epoch": 0.00044154280188115583,
      "grad_norm": 9398.926960031129,
      "learning_rate": 2.86838082605762e-07,
      "loss": 1.7477,
      "step": 121664
    },
    {
      "epoch": 0.00044165893623199253,
      "grad_norm": 9493.747626727814,
      "learning_rate": 2.868003301968518e-07,
      "loss": 1.7675,
      "step": 121696
    },
    {
      "epoch": 0.00044177507058282923,
      "grad_norm": 9735.666284338222,
      "learning_rate": 2.867625926904536e-07,
      "loss": 1.7688,
      "step": 121728
    },
    {
      "epoch": 0.00044189120493366593,
      "grad_norm": 9966.144088864057,
      "learning_rate": 2.8672487007676543e-07,
      "loss": 1.7791,
      "step": 121760
    },
    {
      "epoch": 0.00044200733928450263,
      "grad_norm": 9968.747363636016,
      "learning_rate": 2.866871623459944e-07,
      "loss": 1.776,
      "step": 121792
    },
    {
      "epoch": 0.00044212347363533933,
      "grad_norm": 9642.235529170608,
      "learning_rate": 2.8664946948835666e-07,
      "loss": 1.7635,
      "step": 121824
    },
    {
      "epoch": 0.0004422396079861761,
      "grad_norm": 9882.699226425946,
      "learning_rate": 2.8661179149407753e-07,
      "loss": 1.7638,
      "step": 121856
    },
    {
      "epoch": 0.0004423557423370128,
      "grad_norm": 9463.123374446726,
      "learning_rate": 2.8657412835339104e-07,
      "loss": 1.7764,
      "step": 121888
    },
    {
      "epoch": 0.0004424718766878495,
      "grad_norm": 9250.27145547632,
      "learning_rate": 2.865364800565404e-07,
      "loss": 1.7651,
      "step": 121920
    },
    {
      "epoch": 0.0004425880110386862,
      "grad_norm": 10461.31817697942,
      "learning_rate": 2.8649884659377775e-07,
      "loss": 1.7684,
      "step": 121952
    },
    {
      "epoch": 0.0004427041453895229,
      "grad_norm": 10094.532183315876,
      "learning_rate": 2.86461227955364e-07,
      "loss": 1.7533,
      "step": 121984
    },
    {
      "epoch": 0.0004428202797403596,
      "grad_norm": 10025.357250492372,
      "learning_rate": 2.864236241315694e-07,
      "loss": 1.7267,
      "step": 122016
    },
    {
      "epoch": 0.0004429364140911963,
      "grad_norm": 9173.854805914469,
      "learning_rate": 2.863860351126728e-07,
      "loss": 1.7213,
      "step": 122048
    },
    {
      "epoch": 0.000443052548442033,
      "grad_norm": 8902.587264385562,
      "learning_rate": 2.86348460888962e-07,
      "loss": 1.7229,
      "step": 122080
    },
    {
      "epoch": 0.0004431686827928697,
      "grad_norm": 9200.403252031945,
      "learning_rate": 2.8631090145073404e-07,
      "loss": 1.7342,
      "step": 122112
    },
    {
      "epoch": 0.00044328481714370644,
      "grad_norm": 8655.206987703992,
      "learning_rate": 2.862733567882944e-07,
      "loss": 1.7294,
      "step": 122144
    },
    {
      "epoch": 0.00044340095149454314,
      "grad_norm": 8815.622836759749,
      "learning_rate": 2.8623699947780416e-07,
      "loss": 1.7344,
      "step": 122176
    },
    {
      "epoch": 0.00044351708584537984,
      "grad_norm": 8368.068116357563,
      "learning_rate": 2.861994838769022e-07,
      "loss": 1.7593,
      "step": 122208
    },
    {
      "epoch": 0.00044363322019621654,
      "grad_norm": 9545.801590228031,
      "learning_rate": 2.86161983023061e-07,
      "loss": 1.7579,
      "step": 122240
    },
    {
      "epoch": 0.00044374935454705324,
      "grad_norm": 8367.813812460217,
      "learning_rate": 2.861244969066217e-07,
      "loss": 1.7524,
      "step": 122272
    },
    {
      "epoch": 0.00044386548889788994,
      "grad_norm": 11367.316305971257,
      "learning_rate": 2.8608702551793407e-07,
      "loss": 1.7385,
      "step": 122304
    },
    {
      "epoch": 0.00044398162324872664,
      "grad_norm": 8346.32637751484,
      "learning_rate": 2.8604956884735674e-07,
      "loss": 1.7393,
      "step": 122336
    },
    {
      "epoch": 0.00044409775759956334,
      "grad_norm": 9145.436457600043,
      "learning_rate": 2.860121268852572e-07,
      "loss": 1.7427,
      "step": 122368
    },
    {
      "epoch": 0.00044421389195040004,
      "grad_norm": 9666.20835695155,
      "learning_rate": 2.859746996220117e-07,
      "loss": 1.7287,
      "step": 122400
    },
    {
      "epoch": 0.0004443300263012368,
      "grad_norm": 8796.42995765896,
      "learning_rate": 2.859372870480055e-07,
      "loss": 1.7324,
      "step": 122432
    },
    {
      "epoch": 0.0004444461606520735,
      "grad_norm": 10571.77203689145,
      "learning_rate": 2.858998891536325e-07,
      "loss": 1.7506,
      "step": 122464
    },
    {
      "epoch": 0.0004445622950029102,
      "grad_norm": 10048.432016986531,
      "learning_rate": 2.858625059292954e-07,
      "loss": 1.7723,
      "step": 122496
    },
    {
      "epoch": 0.0004446784293537469,
      "grad_norm": 9578.360402490605,
      "learning_rate": 2.8582513736540567e-07,
      "loss": 1.7736,
      "step": 122528
    },
    {
      "epoch": 0.0004447945637045836,
      "grad_norm": 9495.558330082546,
      "learning_rate": 2.8578778345238365e-07,
      "loss": 1.7628,
      "step": 122560
    },
    {
      "epoch": 0.0004449106980554203,
      "grad_norm": 10071.91223154769,
      "learning_rate": 2.8575044418065836e-07,
      "loss": 1.7531,
      "step": 122592
    },
    {
      "epoch": 0.000445026832406257,
      "grad_norm": 7736.661812435645,
      "learning_rate": 2.8571311954066756e-07,
      "loss": 1.7658,
      "step": 122624
    },
    {
      "epoch": 0.0004451429667570937,
      "grad_norm": 9967.98073834415,
      "learning_rate": 2.8567580952285776e-07,
      "loss": 1.7499,
      "step": 122656
    },
    {
      "epoch": 0.0004452591011079304,
      "grad_norm": 7907.828526213754,
      "learning_rate": 2.8563851411768434e-07,
      "loss": 1.7555,
      "step": 122688
    },
    {
      "epoch": 0.00044537523545876715,
      "grad_norm": 8517.25260867611,
      "learning_rate": 2.856012333156112e-07,
      "loss": 1.7548,
      "step": 122720
    },
    {
      "epoch": 0.00044549136980960385,
      "grad_norm": 9385.177249258533,
      "learning_rate": 2.85563967107111e-07,
      "loss": 1.7334,
      "step": 122752
    },
    {
      "epoch": 0.00044560750416044055,
      "grad_norm": 10446.713167307696,
      "learning_rate": 2.855267154826652e-07,
      "loss": 1.7458,
      "step": 122784
    },
    {
      "epoch": 0.00044572363851127725,
      "grad_norm": 9363.53800654432,
      "learning_rate": 2.854894784327638e-07,
      "loss": 1.7644,
      "step": 122816
    },
    {
      "epoch": 0.00044583977286211395,
      "grad_norm": 9896.319517881382,
      "learning_rate": 2.8545225594790566e-07,
      "loss": 1.7408,
      "step": 122848
    },
    {
      "epoch": 0.00044595590721295065,
      "grad_norm": 9533.51666490388,
      "learning_rate": 2.8541504801859815e-07,
      "loss": 1.7407,
      "step": 122880
    },
    {
      "epoch": 0.00044607204156378735,
      "grad_norm": 11442.38279380654,
      "learning_rate": 2.853778546353573e-07,
      "loss": 1.73,
      "step": 122912
    },
    {
      "epoch": 0.00044618817591462405,
      "grad_norm": 8692.790346028138,
      "learning_rate": 2.8534067578870806e-07,
      "loss": 1.7542,
      "step": 122944
    },
    {
      "epoch": 0.00044630431026546075,
      "grad_norm": 9309.884746869855,
      "learning_rate": 2.853035114691836e-07,
      "loss": 1.7606,
      "step": 122976
    },
    {
      "epoch": 0.0004464204446162975,
      "grad_norm": 11220.977319289083,
      "learning_rate": 2.8526636166732595e-07,
      "loss": 1.734,
      "step": 123008
    },
    {
      "epoch": 0.0004465365789671342,
      "grad_norm": 10100.596616042045,
      "learning_rate": 2.852292263736858e-07,
      "loss": 1.7446,
      "step": 123040
    },
    {
      "epoch": 0.0004466527133179709,
      "grad_norm": 9849.622530838427,
      "learning_rate": 2.851921055788224e-07,
      "loss": 1.7336,
      "step": 123072
    },
    {
      "epoch": 0.0004467688476688076,
      "grad_norm": 8678.748527293552,
      "learning_rate": 2.8515499927330356e-07,
      "loss": 1.7529,
      "step": 123104
    },
    {
      "epoch": 0.0004468849820196443,
      "grad_norm": 10294.946138761485,
      "learning_rate": 2.851179074477056e-07,
      "loss": 1.7452,
      "step": 123136
    },
    {
      "epoch": 0.000447001116370481,
      "grad_norm": 9230.132935120708,
      "learning_rate": 2.8508083009261363e-07,
      "loss": 1.7232,
      "step": 123168
    },
    {
      "epoch": 0.0004471172507213177,
      "grad_norm": 11158.928980865503,
      "learning_rate": 2.8504492519525824e-07,
      "loss": 1.7339,
      "step": 123200
    },
    {
      "epoch": 0.0004472333850721544,
      "grad_norm": 9382.271473369336,
      "learning_rate": 2.850078763014939e-07,
      "loss": 1.7532,
      "step": 123232
    },
    {
      "epoch": 0.0004473495194229911,
      "grad_norm": 9637.548235936358,
      "learning_rate": 2.849708418503352e-07,
      "loss": 1.7451,
      "step": 123264
    },
    {
      "epoch": 0.00044746565377382785,
      "grad_norm": 10208.090516840062,
      "learning_rate": 2.84933821832401e-07,
      "loss": 1.743,
      "step": 123296
    },
    {
      "epoch": 0.00044758178812466455,
      "grad_norm": 10619.112957304862,
      "learning_rate": 2.8489681623831887e-07,
      "loss": 1.7563,
      "step": 123328
    },
    {
      "epoch": 0.00044769792247550125,
      "grad_norm": 10197.847223801698,
      "learning_rate": 2.8485982505872477e-07,
      "loss": 1.752,
      "step": 123360
    },
    {
      "epoch": 0.00044781405682633795,
      "grad_norm": 9301.935390014274,
      "learning_rate": 2.8482284828426313e-07,
      "loss": 1.7656,
      "step": 123392
    },
    {
      "epoch": 0.00044793019117717465,
      "grad_norm": 10129.734053764689,
      "learning_rate": 2.84785885905587e-07,
      "loss": 1.7762,
      "step": 123424
    },
    {
      "epoch": 0.00044804632552801135,
      "grad_norm": 9818.4335817889,
      "learning_rate": 2.8474893791335786e-07,
      "loss": 1.7713,
      "step": 123456
    },
    {
      "epoch": 0.00044816245987884805,
      "grad_norm": 11143.692924699602,
      "learning_rate": 2.847120042982457e-07,
      "loss": 1.7657,
      "step": 123488
    },
    {
      "epoch": 0.00044827859422968475,
      "grad_norm": 8292.318614235708,
      "learning_rate": 2.8467508505092896e-07,
      "loss": 1.75,
      "step": 123520
    },
    {
      "epoch": 0.00044839472858052145,
      "grad_norm": 9604.524662886759,
      "learning_rate": 2.846381801620944e-07,
      "loss": 1.7586,
      "step": 123552
    },
    {
      "epoch": 0.0004485108629313582,
      "grad_norm": 9241.72061901895,
      "learning_rate": 2.8460128962243757e-07,
      "loss": 1.7356,
      "step": 123584
    },
    {
      "epoch": 0.0004486269972821949,
      "grad_norm": 9412.648298964537,
      "learning_rate": 2.845644134226621e-07,
      "loss": 1.7314,
      "step": 123616
    },
    {
      "epoch": 0.0004487431316330316,
      "grad_norm": 8797.089405024823,
      "learning_rate": 2.8452755155348026e-07,
      "loss": 1.745,
      "step": 123648
    },
    {
      "epoch": 0.0004488592659838683,
      "grad_norm": 9519.251441158596,
      "learning_rate": 2.8449070400561264e-07,
      "loss": 1.7448,
      "step": 123680
    },
    {
      "epoch": 0.000448975400334705,
      "grad_norm": 9350.496671300407,
      "learning_rate": 2.844538707697884e-07,
      "loss": 1.7461,
      "step": 123712
    },
    {
      "epoch": 0.0004490915346855417,
      "grad_norm": 8289.63678335788,
      "learning_rate": 2.8441705183674483e-07,
      "loss": 1.7687,
      "step": 123744
    },
    {
      "epoch": 0.0004492076690363784,
      "grad_norm": 9220.147721159352,
      "learning_rate": 2.8438024719722787e-07,
      "loss": 1.731,
      "step": 123776
    },
    {
      "epoch": 0.0004493238033872151,
      "grad_norm": 10134.314974382827,
      "learning_rate": 2.843434568419916e-07,
      "loss": 1.731,
      "step": 123808
    },
    {
      "epoch": 0.0004494399377380518,
      "grad_norm": 8048.200419969672,
      "learning_rate": 2.8430668076179873e-07,
      "loss": 1.724,
      "step": 123840
    },
    {
      "epoch": 0.00044955607208888856,
      "grad_norm": 8319.170271126803,
      "learning_rate": 2.842699189474202e-07,
      "loss": 1.7253,
      "step": 123872
    },
    {
      "epoch": 0.00044967220643972526,
      "grad_norm": 9753.963399562252,
      "learning_rate": 2.842331713896352e-07,
      "loss": 1.7216,
      "step": 123904
    },
    {
      "epoch": 0.00044978834079056196,
      "grad_norm": 8874.943605454628,
      "learning_rate": 2.841964380792314e-07,
      "loss": 1.7385,
      "step": 123936
    },
    {
      "epoch": 0.00044990447514139866,
      "grad_norm": 9632.507669345507,
      "learning_rate": 2.841597190070049e-07,
      "loss": 1.7579,
      "step": 123968
    },
    {
      "epoch": 0.00045002060949223536,
      "grad_norm": 9324.731202560211,
      "learning_rate": 2.8412301416375985e-07,
      "loss": 1.7496,
      "step": 124000
    },
    {
      "epoch": 0.00045013674384307206,
      "grad_norm": 8896.375666528476,
      "learning_rate": 2.840863235403089e-07,
      "loss": 1.7717,
      "step": 124032
    },
    {
      "epoch": 0.00045025287819390876,
      "grad_norm": 9909.766092093194,
      "learning_rate": 2.8404964712747295e-07,
      "loss": 1.7646,
      "step": 124064
    },
    {
      "epoch": 0.00045036901254474546,
      "grad_norm": 9796.441292632748,
      "learning_rate": 2.840129849160812e-07,
      "loss": 1.7603,
      "step": 124096
    },
    {
      "epoch": 0.00045048514689558216,
      "grad_norm": 8996.895464547757,
      "learning_rate": 2.8397633689697113e-07,
      "loss": 1.7551,
      "step": 124128
    },
    {
      "epoch": 0.0004506012812464189,
      "grad_norm": 9821.638458017074,
      "learning_rate": 2.8393970306098846e-07,
      "loss": 1.7739,
      "step": 124160
    },
    {
      "epoch": 0.0004507174155972556,
      "grad_norm": 9946.821803973367,
      "learning_rate": 2.83904227548968e-07,
      "loss": 1.742,
      "step": 124192
    },
    {
      "epoch": 0.0004508335499480923,
      "grad_norm": 9943.127475799552,
      "learning_rate": 2.838676216092974e-07,
      "loss": 1.7519,
      "step": 124224
    },
    {
      "epoch": 0.000450949684298929,
      "grad_norm": 8873.99166102831,
      "learning_rate": 2.8383102982562625e-07,
      "loss": 1.7526,
      "step": 124256
    },
    {
      "epoch": 0.0004510658186497657,
      "grad_norm": 7978.2340151188855,
      "learning_rate": 2.8379445218883295e-07,
      "loss": 1.7309,
      "step": 124288
    },
    {
      "epoch": 0.0004511819530006024,
      "grad_norm": 9979.995490980946,
      "learning_rate": 2.8375788868980437e-07,
      "loss": 1.7492,
      "step": 124320
    },
    {
      "epoch": 0.0004512980873514391,
      "grad_norm": 9298.350283786904,
      "learning_rate": 2.8372133931943533e-07,
      "loss": 1.7552,
      "step": 124352
    },
    {
      "epoch": 0.0004514142217022758,
      "grad_norm": 9597.359220118835,
      "learning_rate": 2.8368480406862906e-07,
      "loss": 1.7439,
      "step": 124384
    },
    {
      "epoch": 0.0004515303560531125,
      "grad_norm": 8797.457246272927,
      "learning_rate": 2.8364828292829686e-07,
      "loss": 1.7311,
      "step": 124416
    },
    {
      "epoch": 0.00045164649040394927,
      "grad_norm": 8790.705546200486,
      "learning_rate": 2.836117758893583e-07,
      "loss": 1.7299,
      "step": 124448
    },
    {
      "epoch": 0.00045176262475478597,
      "grad_norm": 8923.61429018534,
      "learning_rate": 2.835752829427411e-07,
      "loss": 1.7523,
      "step": 124480
    },
    {
      "epoch": 0.00045187875910562267,
      "grad_norm": 11518.378010813849,
      "learning_rate": 2.8353880407938103e-07,
      "loss": 1.7543,
      "step": 124512
    },
    {
      "epoch": 0.00045199489345645937,
      "grad_norm": 9540.310686764871,
      "learning_rate": 2.835023392902223e-07,
      "loss": 1.7608,
      "step": 124544
    },
    {
      "epoch": 0.00045211102780729607,
      "grad_norm": 9087.670328527549,
      "learning_rate": 2.83465888566217e-07,
      "loss": 1.7258,
      "step": 124576
    },
    {
      "epoch": 0.00045222716215813277,
      "grad_norm": 9728.598049051056,
      "learning_rate": 2.834294518983255e-07,
      "loss": 1.7279,
      "step": 124608
    },
    {
      "epoch": 0.00045234329650896947,
      "grad_norm": 8755.593869064509,
      "learning_rate": 2.8339302927751624e-07,
      "loss": 1.7453,
      "step": 124640
    },
    {
      "epoch": 0.00045245943085980617,
      "grad_norm": 10318.14169315386,
      "learning_rate": 2.8335662069476584e-07,
      "loss": 1.7445,
      "step": 124672
    },
    {
      "epoch": 0.00045257556521064287,
      "grad_norm": 9345.060299430925,
      "learning_rate": 2.83320226141059e-07,
      "loss": 1.7612,
      "step": 124704
    },
    {
      "epoch": 0.0004526916995614796,
      "grad_norm": 8761.712275577189,
      "learning_rate": 2.8328384560738854e-07,
      "loss": 1.7605,
      "step": 124736
    },
    {
      "epoch": 0.0004528078339123163,
      "grad_norm": 10104.09441761111,
      "learning_rate": 2.832474790847553e-07,
      "loss": 1.7651,
      "step": 124768
    },
    {
      "epoch": 0.000452923968263153,
      "grad_norm": 9394.506266962622,
      "learning_rate": 2.832111265641683e-07,
      "loss": 1.7507,
      "step": 124800
    },
    {
      "epoch": 0.0004530401026139897,
      "grad_norm": 11422.304671124826,
      "learning_rate": 2.8317478803664457e-07,
      "loss": 1.7637,
      "step": 124832
    },
    {
      "epoch": 0.0004531562369648264,
      "grad_norm": 9342.579729389523,
      "learning_rate": 2.831384634932093e-07,
      "loss": 1.7438,
      "step": 124864
    },
    {
      "epoch": 0.0004532723713156631,
      "grad_norm": 9574.328592648155,
      "learning_rate": 2.831021529248956e-07,
      "loss": 1.7496,
      "step": 124896
    },
    {
      "epoch": 0.0004533885056664998,
      "grad_norm": 9623.13015603551,
      "learning_rate": 2.830658563227448e-07,
      "loss": 1.7578,
      "step": 124928
    },
    {
      "epoch": 0.0004535046400173365,
      "grad_norm": 9054.125247642645,
      "learning_rate": 2.8302957367780603e-07,
      "loss": 1.7616,
      "step": 124960
    },
    {
      "epoch": 0.0004536207743681732,
      "grad_norm": 9877.498671222385,
      "learning_rate": 2.8299330498113673e-07,
      "loss": 1.7516,
      "step": 124992
    },
    {
      "epoch": 0.00045373690871901,
      "grad_norm": 10323.507252867119,
      "learning_rate": 2.8295705022380207e-07,
      "loss": 1.7225,
      "step": 125024
    },
    {
      "epoch": 0.0004538530430698467,
      "grad_norm": 8865.42926202674,
      "learning_rate": 2.8292080939687556e-07,
      "loss": 1.7238,
      "step": 125056
    },
    {
      "epoch": 0.00045396917742068337,
      "grad_norm": 9440.086440282208,
      "learning_rate": 2.828845824914383e-07,
      "loss": 1.7392,
      "step": 125088
    },
    {
      "epoch": 0.00045408531177152007,
      "grad_norm": 10336.733913572507,
      "learning_rate": 2.8284836949857983e-07,
      "loss": 1.7318,
      "step": 125120
    },
    {
      "epoch": 0.00045420144612235677,
      "grad_norm": 7836.176235894647,
      "learning_rate": 2.828121704093973e-07,
      "loss": 1.7387,
      "step": 125152
    },
    {
      "epoch": 0.00045431758047319347,
      "grad_norm": 8542.79123003717,
      "learning_rate": 2.82775985214996e-07,
      "loss": 1.7349,
      "step": 125184
    },
    {
      "epoch": 0.00045443371482403017,
      "grad_norm": 9462.987794560448,
      "learning_rate": 2.8274094404978144e-07,
      "loss": 1.7457,
      "step": 125216
    },
    {
      "epoch": 0.00045454984917486687,
      "grad_norm": 9099.418882544094,
      "learning_rate": 2.82704786184768e-07,
      "loss": 1.7677,
      "step": 125248
    },
    {
      "epoch": 0.00045466598352570357,
      "grad_norm": 9524.55069806445,
      "learning_rate": 2.826686421881765e-07,
      "loss": 1.7847,
      "step": 125280
    },
    {
      "epoch": 0.0004547821178765403,
      "grad_norm": 9097.897889073058,
      "learning_rate": 2.8263251205114373e-07,
      "loss": 1.7697,
      "step": 125312
    },
    {
      "epoch": 0.000454898252227377,
      "grad_norm": 9319.522841862668,
      "learning_rate": 2.825963957648146e-07,
      "loss": 1.7392,
      "step": 125344
    },
    {
      "epoch": 0.0004550143865782137,
      "grad_norm": 10407.548030155805,
      "learning_rate": 2.825602933203416e-07,
      "loss": 1.7407,
      "step": 125376
    },
    {
      "epoch": 0.0004551305209290504,
      "grad_norm": 9734.282510796571,
      "learning_rate": 2.825242047088854e-07,
      "loss": 1.7399,
      "step": 125408
    },
    {
      "epoch": 0.0004552466552798871,
      "grad_norm": 9849.931979460569,
      "learning_rate": 2.824881299216145e-07,
      "loss": 1.7155,
      "step": 125440
    },
    {
      "epoch": 0.0004553627896307238,
      "grad_norm": 9312.475503323485,
      "learning_rate": 2.8245206894970523e-07,
      "loss": 1.7492,
      "step": 125472
    },
    {
      "epoch": 0.0004554789239815605,
      "grad_norm": 9810.870297787042,
      "learning_rate": 2.824160217843419e-07,
      "loss": 1.7593,
      "step": 125504
    },
    {
      "epoch": 0.0004555950583323972,
      "grad_norm": 10250.611298844571,
      "learning_rate": 2.823799884167166e-07,
      "loss": 1.7454,
      "step": 125536
    },
    {
      "epoch": 0.0004557111926832339,
      "grad_norm": 9519.881721954323,
      "learning_rate": 2.823439688380295e-07,
      "loss": 1.7681,
      "step": 125568
    },
    {
      "epoch": 0.0004558273270340707,
      "grad_norm": 9027.053229044348,
      "learning_rate": 2.823079630394882e-07,
      "loss": 1.7484,
      "step": 125600
    },
    {
      "epoch": 0.0004559434613849074,
      "grad_norm": 10603.5296010338,
      "learning_rate": 2.822719710123086e-07,
      "loss": 1.7488,
      "step": 125632
    },
    {
      "epoch": 0.0004560595957357441,
      "grad_norm": 9952.516264744309,
      "learning_rate": 2.8223599274771414e-07,
      "loss": 1.7567,
      "step": 125664
    },
    {
      "epoch": 0.0004561757300865808,
      "grad_norm": 11094.035694912829,
      "learning_rate": 2.8220002823693626e-07,
      "loss": 1.7692,
      "step": 125696
    },
    {
      "epoch": 0.0004562918644374175,
      "grad_norm": 8531.061129777467,
      "learning_rate": 2.8216407747121406e-07,
      "loss": 1.7402,
      "step": 125728
    },
    {
      "epoch": 0.0004564079987882542,
      "grad_norm": 10049.567950912118,
      "learning_rate": 2.821281404417946e-07,
      "loss": 1.7477,
      "step": 125760
    },
    {
      "epoch": 0.0004565241331390909,
      "grad_norm": 9407.551222289465,
      "learning_rate": 2.820922171399327e-07,
      "loss": 1.7179,
      "step": 125792
    },
    {
      "epoch": 0.0004566402674899276,
      "grad_norm": 10212.349191052957,
      "learning_rate": 2.820563075568909e-07,
      "loss": 1.7338,
      "step": 125824
    },
    {
      "epoch": 0.0004567564018407643,
      "grad_norm": 10122.447727699067,
      "learning_rate": 2.8202041168393955e-07,
      "loss": 1.7405,
      "step": 125856
    },
    {
      "epoch": 0.00045687253619160103,
      "grad_norm": 10150.171230082771,
      "learning_rate": 2.819845295123569e-07,
      "loss": 1.7649,
      "step": 125888
    },
    {
      "epoch": 0.00045698867054243773,
      "grad_norm": 10989.859416753246,
      "learning_rate": 2.819486610334288e-07,
      "loss": 1.7504,
      "step": 125920
    },
    {
      "epoch": 0.00045710480489327443,
      "grad_norm": 10466.59046681392,
      "learning_rate": 2.8191280623844896e-07,
      "loss": 1.7291,
      "step": 125952
    },
    {
      "epoch": 0.00045722093924411113,
      "grad_norm": 9451.2956783713,
      "learning_rate": 2.818769651187188e-07,
      "loss": 1.739,
      "step": 125984
    },
    {
      "epoch": 0.00045733707359494783,
      "grad_norm": 8708.783267483466,
      "learning_rate": 2.8184113766554744e-07,
      "loss": 1.7484,
      "step": 126016
    },
    {
      "epoch": 0.00045745320794578453,
      "grad_norm": 7454.846879715236,
      "learning_rate": 2.818053238702518e-07,
      "loss": 1.7473,
      "step": 126048
    },
    {
      "epoch": 0.00045756934229662123,
      "grad_norm": 11026.318515261564,
      "learning_rate": 2.8176952372415656e-07,
      "loss": 1.757,
      "step": 126080
    },
    {
      "epoch": 0.00045768547664745793,
      "grad_norm": 9319.323795211754,
      "learning_rate": 2.8173373721859387e-07,
      "loss": 1.7344,
      "step": 126112
    },
    {
      "epoch": 0.00045780161099829463,
      "grad_norm": 9012.647113917197,
      "learning_rate": 2.81697964344904e-07,
      "loss": 1.7332,
      "step": 126144
    },
    {
      "epoch": 0.0004579177453491314,
      "grad_norm": 9383.406311143091,
      "learning_rate": 2.816622050944345e-07,
      "loss": 1.7442,
      "step": 126176
    },
    {
      "epoch": 0.0004580338796999681,
      "grad_norm": 10365.438919794955,
      "learning_rate": 2.8162757630366826e-07,
      "loss": 1.7494,
      "step": 126208
    },
    {
      "epoch": 0.0004581500140508048,
      "grad_norm": 9168.278682500875,
      "learning_rate": 2.815918438486586e-07,
      "loss": 1.7569,
      "step": 126240
    },
    {
      "epoch": 0.0004582661484016415,
      "grad_norm": 10401.56545910278,
      "learning_rate": 2.815561249912282e-07,
      "loss": 1.758,
      "step": 126272
    },
    {
      "epoch": 0.0004583822827524782,
      "grad_norm": 9629.020199376466,
      "learning_rate": 2.8152041972275514e-07,
      "loss": 1.7537,
      "step": 126304
    },
    {
      "epoch": 0.0004584984171033149,
      "grad_norm": 9247.243913729106,
      "learning_rate": 2.814847280346254e-07,
      "loss": 1.746,
      "step": 126336
    },
    {
      "epoch": 0.0004586145514541516,
      "grad_norm": 9918.634785090135,
      "learning_rate": 2.814490499182324e-07,
      "loss": 1.7415,
      "step": 126368
    },
    {
      "epoch": 0.0004587306858049883,
      "grad_norm": 11755.459497612163,
      "learning_rate": 2.8141338536497715e-07,
      "loss": 1.7534,
      "step": 126400
    },
    {
      "epoch": 0.000458846820155825,
      "grad_norm": 9648.298088264064,
      "learning_rate": 2.813777343662685e-07,
      "loss": 1.7553,
      "step": 126432
    },
    {
      "epoch": 0.00045896295450666174,
      "grad_norm": 9617.696813686736,
      "learning_rate": 2.8134209691352274e-07,
      "loss": 1.7753,
      "step": 126464
    },
    {
      "epoch": 0.00045907908885749844,
      "grad_norm": 9120.11776239759,
      "learning_rate": 2.813064729981639e-07,
      "loss": 1.789,
      "step": 126496
    },
    {
      "epoch": 0.00045919522320833514,
      "grad_norm": 9495.91280499142,
      "learning_rate": 2.8127086261162347e-07,
      "loss": 1.7558,
      "step": 126528
    },
    {
      "epoch": 0.00045931135755917184,
      "grad_norm": 9749.729739844075,
      "learning_rate": 2.8123526574534063e-07,
      "loss": 1.7331,
      "step": 126560
    },
    {
      "epoch": 0.00045942749191000854,
      "grad_norm": 8291.099203362604,
      "learning_rate": 2.8119968239076215e-07,
      "loss": 1.7267,
      "step": 126592
    },
    {
      "epoch": 0.00045954362626084524,
      "grad_norm": 8336.701745894476,
      "learning_rate": 2.8116411253934225e-07,
      "loss": 1.73,
      "step": 126624
    },
    {
      "epoch": 0.00045965976061168194,
      "grad_norm": 7995.072982781333,
      "learning_rate": 2.8112855618254283e-07,
      "loss": 1.7301,
      "step": 126656
    },
    {
      "epoch": 0.00045977589496251864,
      "grad_norm": 9879.46233354832,
      "learning_rate": 2.8109301331183334e-07,
      "loss": 1.7328,
      "step": 126688
    },
    {
      "epoch": 0.00045989202931335534,
      "grad_norm": 8975.247517478278,
      "learning_rate": 2.8105748391869075e-07,
      "loss": 1.7212,
      "step": 126720
    },
    {
      "epoch": 0.00046000816366419204,
      "grad_norm": 9048.272321277693,
      "learning_rate": 2.810219679945996e-07,
      "loss": 1.7272,
      "step": 126752
    },
    {
      "epoch": 0.0004601242980150288,
      "grad_norm": 8409.249907096351,
      "learning_rate": 2.809864655310519e-07,
      "loss": 1.7472,
      "step": 126784
    },
    {
      "epoch": 0.0004602404323658655,
      "grad_norm": 9436.580418774589,
      "learning_rate": 2.8095097651954717e-07,
      "loss": 1.7512,
      "step": 126816
    },
    {
      "epoch": 0.0004603565667167022,
      "grad_norm": 8590.15704163783,
      "learning_rate": 2.8091550095159253e-07,
      "loss": 1.7332,
      "step": 126848
    },
    {
      "epoch": 0.0004604727010675389,
      "grad_norm": 9168.697508370531,
      "learning_rate": 2.8088003881870265e-07,
      "loss": 1.7334,
      "step": 126880
    },
    {
      "epoch": 0.0004605888354183756,
      "grad_norm": 9576.649622910927,
      "learning_rate": 2.8084459011239947e-07,
      "loss": 1.743,
      "step": 126912
    },
    {
      "epoch": 0.0004607049697692123,
      "grad_norm": 10228.861324702764,
      "learning_rate": 2.8080915482421265e-07,
      "loss": 1.7318,
      "step": 126944
    },
    {
      "epoch": 0.000460821104120049,
      "grad_norm": 10497.087596090641,
      "learning_rate": 2.8077373294567923e-07,
      "loss": 1.7365,
      "step": 126976
    },
    {
      "epoch": 0.0004609372384708857,
      "grad_norm": 9283.504402971972,
      "learning_rate": 2.8073832446834365e-07,
      "loss": 1.7504,
      "step": 127008
    },
    {
      "epoch": 0.0004610533728217224,
      "grad_norm": 10240.945464164919,
      "learning_rate": 2.8070292938375797e-07,
      "loss": 1.7535,
      "step": 127040
    },
    {
      "epoch": 0.00046116950717255915,
      "grad_norm": 8463.587655362235,
      "learning_rate": 2.806675476834816e-07,
      "loss": 1.7606,
      "step": 127072
    },
    {
      "epoch": 0.00046128564152339585,
      "grad_norm": 9934.705632277184,
      "learning_rate": 2.8063217935908147e-07,
      "loss": 1.7772,
      "step": 127104
    },
    {
      "epoch": 0.00046140177587423255,
      "grad_norm": 10873.182606762382,
      "learning_rate": 2.805968244021318e-07,
      "loss": 1.7397,
      "step": 127136
    },
    {
      "epoch": 0.00046151791022506925,
      "grad_norm": 9667.86512111128,
      "learning_rate": 2.8056148280421437e-07,
      "loss": 1.7241,
      "step": 127168
    },
    {
      "epoch": 0.00046163404457590595,
      "grad_norm": 9202.01499672762,
      "learning_rate": 2.8052725836264534e-07,
      "loss": 1.7215,
      "step": 127200
    },
    {
      "epoch": 0.00046175017892674265,
      "grad_norm": 12004.380033970934,
      "learning_rate": 2.8049194304075017e-07,
      "loss": 1.7322,
      "step": 127232
    },
    {
      "epoch": 0.00046186631327757935,
      "grad_norm": 8402.245890236729,
      "learning_rate": 2.8045664105293924e-07,
      "loss": 1.7336,
      "step": 127264
    },
    {
      "epoch": 0.00046198244762841605,
      "grad_norm": 10187.81350437865,
      "learning_rate": 2.8042135239082375e-07,
      "loss": 1.7178,
      "step": 127296
    },
    {
      "epoch": 0.00046209858197925275,
      "grad_norm": 9455.643394291052,
      "learning_rate": 2.8038607704602215e-07,
      "loss": 1.7279,
      "step": 127328
    },
    {
      "epoch": 0.0004622147163300895,
      "grad_norm": 9202.540953454105,
      "learning_rate": 2.8035081501016036e-07,
      "loss": 1.7238,
      "step": 127360
    },
    {
      "epoch": 0.0004623308506809262,
      "grad_norm": 9169.993020717082,
      "learning_rate": 2.8031556627487174e-07,
      "loss": 1.7522,
      "step": 127392
    },
    {
      "epoch": 0.0004624469850317629,
      "grad_norm": 9459.667647438784,
      "learning_rate": 2.802803308317969e-07,
      "loss": 1.7442,
      "step": 127424
    },
    {
      "epoch": 0.0004625631193825996,
      "grad_norm": 10309.104325788929,
      "learning_rate": 2.802451086725838e-07,
      "loss": 1.7253,
      "step": 127456
    },
    {
      "epoch": 0.0004626792537334363,
      "grad_norm": 9244.926392351645,
      "learning_rate": 2.802098997888877e-07,
      "loss": 1.7185,
      "step": 127488
    },
    {
      "epoch": 0.000462795388084273,
      "grad_norm": 8152.171857854813,
      "learning_rate": 2.8017470417237143e-07,
      "loss": 1.7274,
      "step": 127520
    },
    {
      "epoch": 0.0004629115224351097,
      "grad_norm": 9451.665673308593,
      "learning_rate": 2.80139521814705e-07,
      "loss": 1.7307,
      "step": 127552
    },
    {
      "epoch": 0.0004630276567859464,
      "grad_norm": 9622.775898876582,
      "learning_rate": 2.801043527075656e-07,
      "loss": 1.7268,
      "step": 127584
    },
    {
      "epoch": 0.0004631437911367831,
      "grad_norm": 7877.617660181281,
      "learning_rate": 2.8006919684263805e-07,
      "loss": 1.7357,
      "step": 127616
    },
    {
      "epoch": 0.00046325992548761985,
      "grad_norm": 9185.407339906053,
      "learning_rate": 2.800340542116141e-07,
      "loss": 1.7505,
      "step": 127648
    },
    {
      "epoch": 0.00046337605983845655,
      "grad_norm": 8470.188545717268,
      "learning_rate": 2.7999892480619313e-07,
      "loss": 1.7591,
      "step": 127680
    },
    {
      "epoch": 0.00046349219418929325,
      "grad_norm": 9125.444865868185,
      "learning_rate": 2.799638086180816e-07,
      "loss": 1.7524,
      "step": 127712
    },
    {
      "epoch": 0.00046360832854012995,
      "grad_norm": 9144.381116292125,
      "learning_rate": 2.799287056389933e-07,
      "loss": 1.7421,
      "step": 127744
    },
    {
      "epoch": 0.00046372446289096665,
      "grad_norm": 9038.622793324213,
      "learning_rate": 2.798936158606494e-07,
      "loss": 1.734,
      "step": 127776
    },
    {
      "epoch": 0.00046384059724180335,
      "grad_norm": 9730.718061890397,
      "learning_rate": 2.7985853927477814e-07,
      "loss": 1.7287,
      "step": 127808
    },
    {
      "epoch": 0.00046395673159264005,
      "grad_norm": 8274.493579669997,
      "learning_rate": 2.798234758731152e-07,
      "loss": 1.75,
      "step": 127840
    },
    {
      "epoch": 0.00046407286594347675,
      "grad_norm": 9075.65766212014,
      "learning_rate": 2.7978842564740327e-07,
      "loss": 1.7407,
      "step": 127872
    },
    {
      "epoch": 0.00046418900029431345,
      "grad_norm": 10048.985819474521,
      "learning_rate": 2.797533885893926e-07,
      "loss": 1.727,
      "step": 127904
    },
    {
      "epoch": 0.0004643051346451502,
      "grad_norm": 8643.284676556708,
      "learning_rate": 2.7971836469084035e-07,
      "loss": 1.7347,
      "step": 127936
    },
    {
      "epoch": 0.0004644212689959869,
      "grad_norm": 10803.005692861594,
      "learning_rate": 2.7968335394351123e-07,
      "loss": 1.7394,
      "step": 127968
    },
    {
      "epoch": 0.0004645374033468236,
      "grad_norm": 10892.155525881917,
      "learning_rate": 2.7964835633917683e-07,
      "loss": 1.7395,
      "step": 128000
    },
    {
      "epoch": 0.0004646535376976603,
      "grad_norm": 9318.968827075236,
      "learning_rate": 2.796133718696161e-07,
      "loss": 1.7488,
      "step": 128032
    },
    {
      "epoch": 0.000464769672048497,
      "grad_norm": 9074.402239266232,
      "learning_rate": 2.795784005266153e-07,
      "loss": 1.724,
      "step": 128064
    },
    {
      "epoch": 0.0004648858063993337,
      "grad_norm": 10303.056051482978,
      "learning_rate": 2.7954344230196764e-07,
      "loss": 1.7225,
      "step": 128096
    },
    {
      "epoch": 0.0004650019407501704,
      "grad_norm": 9612.987464883121,
      "learning_rate": 2.795084971874737e-07,
      "loss": 1.7266,
      "step": 128128
    },
    {
      "epoch": 0.0004651180751010071,
      "grad_norm": 9551.720682683304,
      "learning_rate": 2.794735651749412e-07,
      "loss": 1.7348,
      "step": 128160
    },
    {
      "epoch": 0.0004652342094518438,
      "grad_norm": 10634.228321791854,
      "learning_rate": 2.7943864625618493e-07,
      "loss": 1.7293,
      "step": 128192
    },
    {
      "epoch": 0.00046535034380268056,
      "grad_norm": 10880.078216630614,
      "learning_rate": 2.794048310323213e-07,
      "loss": 1.7501,
      "step": 128224
    },
    {
      "epoch": 0.00046546647815351726,
      "grad_norm": 10408.515263955758,
      "learning_rate": 2.7936993786804474e-07,
      "loss": 1.7606,
      "step": 128256
    },
    {
      "epoch": 0.00046558261250435396,
      "grad_norm": 8973.239325906781,
      "learning_rate": 2.7933505777328695e-07,
      "loss": 1.736,
      "step": 128288
    },
    {
      "epoch": 0.00046569874685519066,
      "grad_norm": 8554.732725222922,
      "learning_rate": 2.793001907398911e-07,
      "loss": 1.7331,
      "step": 128320
    },
    {
      "epoch": 0.00046581488120602736,
      "grad_norm": 9704.205583147957,
      "learning_rate": 2.792653367597075e-07,
      "loss": 1.741,
      "step": 128352
    },
    {
      "epoch": 0.00046593101555686406,
      "grad_norm": 9219.878957990717,
      "learning_rate": 2.7923049582459366e-07,
      "loss": 1.7214,
      "step": 128384
    },
    {
      "epoch": 0.00046604714990770076,
      "grad_norm": 10559.566089570159,
      "learning_rate": 2.7919566792641414e-07,
      "loss": 1.7219,
      "step": 128416
    },
    {
      "epoch": 0.00046616328425853746,
      "grad_norm": 9872.58041243524,
      "learning_rate": 2.7916085305704057e-07,
      "loss": 1.7381,
      "step": 128448
    },
    {
      "epoch": 0.00046627941860937416,
      "grad_norm": 8230.592688257633,
      "learning_rate": 2.7912605120835174e-07,
      "loss": 1.7191,
      "step": 128480
    },
    {
      "epoch": 0.0004663955529602109,
      "grad_norm": 8414.45030884371,
      "learning_rate": 2.7909126237223357e-07,
      "loss": 1.725,
      "step": 128512
    },
    {
      "epoch": 0.0004665116873110476,
      "grad_norm": 9251.620506700434,
      "learning_rate": 2.7905648654057883e-07,
      "loss": 1.7324,
      "step": 128544
    },
    {
      "epoch": 0.0004666278216618843,
      "grad_norm": 8447.347867822184,
      "learning_rate": 2.7902172370528757e-07,
      "loss": 1.7296,
      "step": 128576
    },
    {
      "epoch": 0.000466743956012721,
      "grad_norm": 9638.683312569201,
      "learning_rate": 2.7898697385826684e-07,
      "loss": 1.7366,
      "step": 128608
    },
    {
      "epoch": 0.0004668600903635577,
      "grad_norm": 9585.31418368746,
      "learning_rate": 2.789522369914308e-07,
      "loss": 1.7515,
      "step": 128640
    },
    {
      "epoch": 0.0004669762247143944,
      "grad_norm": 9779.992638034038,
      "learning_rate": 2.7891751309670055e-07,
      "loss": 1.7444,
      "step": 128672
    },
    {
      "epoch": 0.0004670923590652311,
      "grad_norm": 8170.091798749877,
      "learning_rate": 2.788828021660043e-07,
      "loss": 1.7371,
      "step": 128704
    },
    {
      "epoch": 0.0004672084934160678,
      "grad_norm": 8698.448942196534,
      "learning_rate": 2.788481041912772e-07,
      "loss": 1.7311,
      "step": 128736
    },
    {
      "epoch": 0.0004673246277669045,
      "grad_norm": 10812.45337562202,
      "learning_rate": 2.788134191644616e-07,
      "loss": 1.7494,
      "step": 128768
    },
    {
      "epoch": 0.00046744076211774127,
      "grad_norm": 10011.51866601666,
      "learning_rate": 2.787787470775067e-07,
      "loss": 1.749,
      "step": 128800
    },
    {
      "epoch": 0.00046755689646857797,
      "grad_norm": 10487.017783907873,
      "learning_rate": 2.7874408792236877e-07,
      "loss": 1.7678,
      "step": 128832
    },
    {
      "epoch": 0.00046767303081941467,
      "grad_norm": 9250.793911875888,
      "learning_rate": 2.7870944169101104e-07,
      "loss": 1.7415,
      "step": 128864
    },
    {
      "epoch": 0.00046778916517025137,
      "grad_norm": 8982.990593338056,
      "learning_rate": 2.786748083754038e-07,
      "loss": 1.721,
      "step": 128896
    },
    {
      "epoch": 0.00046790529952108807,
      "grad_norm": 9838.965596037015,
      "learning_rate": 2.786401879675243e-07,
      "loss": 1.7284,
      "step": 128928
    },
    {
      "epoch": 0.00046802143387192477,
      "grad_norm": 9398.541801790318,
      "learning_rate": 2.786055804593567e-07,
      "loss": 1.741,
      "step": 128960
    },
    {
      "epoch": 0.00046813756822276147,
      "grad_norm": 9834.348580358539,
      "learning_rate": 2.785709858428922e-07,
      "loss": 1.726,
      "step": 128992
    },
    {
      "epoch": 0.00046825370257359817,
      "grad_norm": 9171.082815022444,
      "learning_rate": 2.7853640411012893e-07,
      "loss": 1.7263,
      "step": 129024
    },
    {
      "epoch": 0.00046836983692443487,
      "grad_norm": 9297.135472821723,
      "learning_rate": 2.7850183525307195e-07,
      "loss": 1.7339,
      "step": 129056
    },
    {
      "epoch": 0.0004684859712752716,
      "grad_norm": 9771.074045364716,
      "learning_rate": 2.7846727926373335e-07,
      "loss": 1.7306,
      "step": 129088
    },
    {
      "epoch": 0.0004686021056261083,
      "grad_norm": 8820.383211629753,
      "learning_rate": 2.7843273613413207e-07,
      "loss": 1.7375,
      "step": 129120
    },
    {
      "epoch": 0.000468718239976945,
      "grad_norm": 9009.885459871286,
      "learning_rate": 2.7839820585629404e-07,
      "loss": 1.7222,
      "step": 129152
    },
    {
      "epoch": 0.0004688343743277817,
      "grad_norm": 8391.594127458739,
      "learning_rate": 2.78363688422252e-07,
      "loss": 1.7232,
      "step": 129184
    },
    {
      "epoch": 0.0004689505086786184,
      "grad_norm": 9441.65949396609,
      "learning_rate": 2.783302618985263e-07,
      "loss": 1.726,
      "step": 129216
    },
    {
      "epoch": 0.0004690666430294551,
      "grad_norm": 11227.595379243054,
      "learning_rate": 2.7829576972745143e-07,
      "loss": 1.739,
      "step": 129248
    },
    {
      "epoch": 0.0004691827773802918,
      "grad_norm": 9622.180314253106,
      "learning_rate": 2.7826129037656075e-07,
      "loss": 1.7288,
      "step": 129280
    },
    {
      "epoch": 0.0004692989117311285,
      "grad_norm": 9758.267879085919,
      "learning_rate": 2.782268238379144e-07,
      "loss": 1.7218,
      "step": 129312
    },
    {
      "epoch": 0.0004694150460819652,
      "grad_norm": 9465.097146886554,
      "learning_rate": 2.7819237010357953e-07,
      "loss": 1.7296,
      "step": 129344
    },
    {
      "epoch": 0.000469531180432802,
      "grad_norm": 10898.686985137247,
      "learning_rate": 2.7815792916563004e-07,
      "loss": 1.7527,
      "step": 129376
    },
    {
      "epoch": 0.0004696473147836387,
      "grad_norm": 10048.094545733535,
      "learning_rate": 2.7812350101614683e-07,
      "loss": 1.7516,
      "step": 129408
    },
    {
      "epoch": 0.0004697634491344754,
      "grad_norm": 9612.465032446153,
      "learning_rate": 2.7808908564721754e-07,
      "loss": 1.7489,
      "step": 129440
    },
    {
      "epoch": 0.0004698795834853121,
      "grad_norm": 8693.422571116625,
      "learning_rate": 2.780546830509367e-07,
      "loss": 1.738,
      "step": 129472
    },
    {
      "epoch": 0.0004699957178361488,
      "grad_norm": 9471.051156022757,
      "learning_rate": 2.7802029321940583e-07,
      "loss": 1.7297,
      "step": 129504
    },
    {
      "epoch": 0.0004701118521869855,
      "grad_norm": 9785.694354515677,
      "learning_rate": 2.7798591614473297e-07,
      "loss": 1.7461,
      "step": 129536
    },
    {
      "epoch": 0.0004702279865378222,
      "grad_norm": 10239.731344132031,
      "learning_rate": 2.779515518190333e-07,
      "loss": 1.7451,
      "step": 129568
    },
    {
      "epoch": 0.0004703441208886589,
      "grad_norm": 9100.443176021705,
      "learning_rate": 2.7791720023442865e-07,
      "loss": 1.7377,
      "step": 129600
    },
    {
      "epoch": 0.0004704602552394956,
      "grad_norm": 10390.581215697224,
      "learning_rate": 2.7788286138304773e-07,
      "loss": 1.7419,
      "step": 129632
    },
    {
      "epoch": 0.00047057638959033233,
      "grad_norm": 8910.63443308051,
      "learning_rate": 2.77848535257026e-07,
      "loss": 1.735,
      "step": 129664
    },
    {
      "epoch": 0.00047069252394116903,
      "grad_norm": 9319.497947851054,
      "learning_rate": 2.7781422184850576e-07,
      "loss": 1.7478,
      "step": 129696
    },
    {
      "epoch": 0.00047080865829200573,
      "grad_norm": 8945.83478497116,
      "learning_rate": 2.777799211496361e-07,
      "loss": 1.7156,
      "step": 129728
    },
    {
      "epoch": 0.00047092479264284243,
      "grad_norm": 9534.804140620823,
      "learning_rate": 2.777456331525729e-07,
      "loss": 1.726,
      "step": 129760
    },
    {
      "epoch": 0.00047104092699367913,
      "grad_norm": 10300.179804255846,
      "learning_rate": 2.777113578494789e-07,
      "loss": 1.7242,
      "step": 129792
    },
    {
      "epoch": 0.00047115706134451583,
      "grad_norm": 10888.639584447637,
      "learning_rate": 2.7767709523252323e-07,
      "loss": 1.7254,
      "step": 129824
    },
    {
      "epoch": 0.0004712731956953525,
      "grad_norm": 10137.574857923368,
      "learning_rate": 2.7764284529388236e-07,
      "loss": 1.7524,
      "step": 129856
    },
    {
      "epoch": 0.0004713893300461892,
      "grad_norm": 11446.533449040367,
      "learning_rate": 2.776086080257391e-07,
      "loss": 1.7467,
      "step": 129888
    },
    {
      "epoch": 0.0004715054643970259,
      "grad_norm": 10531.114850764852,
      "learning_rate": 2.7757438342028307e-07,
      "loss": 1.7648,
      "step": 129920
    },
    {
      "epoch": 0.0004716215987478627,
      "grad_norm": 10618.324161561466,
      "learning_rate": 2.7754017146971076e-07,
      "loss": 1.7682,
      "step": 129952
    },
    {
      "epoch": 0.0004717377330986994,
      "grad_norm": 9097.231886678497,
      "learning_rate": 2.7750597216622535e-07,
      "loss": 1.7933,
      "step": 129984
    },
    {
      "epoch": 0.0004718538674495361,
      "grad_norm": 9502.640264684336,
      "learning_rate": 2.7747178550203656e-07,
      "loss": 1.7713,
      "step": 130016
    },
    {
      "epoch": 0.0004719700018003728,
      "grad_norm": 11098.88462864625,
      "learning_rate": 2.774376114693611e-07,
      "loss": 1.7697,
      "step": 130048
    },
    {
      "epoch": 0.0004720861361512095,
      "grad_norm": 8067.071091790377,
      "learning_rate": 2.774034500604222e-07,
      "loss": 1.7454,
      "step": 130080
    },
    {
      "epoch": 0.0004722022705020462,
      "grad_norm": 10338.490798951267,
      "learning_rate": 2.773693012674499e-07,
      "loss": 1.7574,
      "step": 130112
    },
    {
      "epoch": 0.0004723184048528829,
      "grad_norm": 10866.104177671039,
      "learning_rate": 2.7733516508268085e-07,
      "loss": 1.7599,
      "step": 130144
    },
    {
      "epoch": 0.0004724345392037196,
      "grad_norm": 10004.142541967303,
      "learning_rate": 2.773010414983585e-07,
      "loss": 1.7745,
      "step": 130176
    },
    {
      "epoch": 0.0004725506735545563,
      "grad_norm": 8942.78390659195,
      "learning_rate": 2.7726799628468584e-07,
      "loss": 1.7839,
      "step": 130208
    },
    {
      "epoch": 0.00047266680790539303,
      "grad_norm": 10142.964655365808,
      "learning_rate": 2.772338974848509e-07,
      "loss": 1.7688,
      "step": 130240
    },
    {
      "epoch": 0.00047278294225622973,
      "grad_norm": 10949.472316052495,
      "learning_rate": 2.7719981126247436e-07,
      "loss": 1.7638,
      "step": 130272
    },
    {
      "epoch": 0.00047289907660706643,
      "grad_norm": 10865.743784941738,
      "learning_rate": 2.7716573760982617e-07,
      "loss": 1.7377,
      "step": 130304
    },
    {
      "epoch": 0.00047301521095790313,
      "grad_norm": 8625.290024109334,
      "learning_rate": 2.7713167651918274e-07,
      "loss": 1.7378,
      "step": 130336
    },
    {
      "epoch": 0.00047313134530873983,
      "grad_norm": 9940.946635004135,
      "learning_rate": 2.7709762798282724e-07,
      "loss": 1.7468,
      "step": 130368
    },
    {
      "epoch": 0.00047324747965957653,
      "grad_norm": 9072.200394612104,
      "learning_rate": 2.7706359199304943e-07,
      "loss": 1.7559,
      "step": 130400
    },
    {
      "epoch": 0.00047336361401041323,
      "grad_norm": 10963.540486539921,
      "learning_rate": 2.7702956854214567e-07,
      "loss": 1.768,
      "step": 130432
    },
    {
      "epoch": 0.00047347974836124993,
      "grad_norm": 9433.280871467783,
      "learning_rate": 2.76995557622419e-07,
      "loss": 1.786,
      "step": 130464
    },
    {
      "epoch": 0.00047359588271208663,
      "grad_norm": 10493.829615540744,
      "learning_rate": 2.7696155922617903e-07,
      "loss": 1.8026,
      "step": 130496
    },
    {
      "epoch": 0.0004737120170629234,
      "grad_norm": 10375.420184262419,
      "learning_rate": 2.7692757334574203e-07,
      "loss": 1.7652,
      "step": 130528
    },
    {
      "epoch": 0.0004738281514137601,
      "grad_norm": 10763.636931818166,
      "learning_rate": 2.7689359997343077e-07,
      "loss": 1.7582,
      "step": 130560
    },
    {
      "epoch": 0.0004739442857645968,
      "grad_norm": 8931.563133069149,
      "learning_rate": 2.768596391015748e-07,
      "loss": 1.7702,
      "step": 130592
    },
    {
      "epoch": 0.0004740604201154335,
      "grad_norm": 10148.063460582023,
      "learning_rate": 2.768256907225099e-07,
      "loss": 1.7732,
      "step": 130624
    },
    {
      "epoch": 0.0004741765544662702,
      "grad_norm": 9572.05620543465,
      "learning_rate": 2.7679175482857884e-07,
      "loss": 1.747,
      "step": 130656
    },
    {
      "epoch": 0.0004742926888171069,
      "grad_norm": 9972.9857114106,
      "learning_rate": 2.7675783141213073e-07,
      "loss": 1.7456,
      "step": 130688
    },
    {
      "epoch": 0.0004744088231679436,
      "grad_norm": 9375.58552838168,
      "learning_rate": 2.767239204655212e-07,
      "loss": 1.7574,
      "step": 130720
    },
    {
      "epoch": 0.0004745249575187803,
      "grad_norm": 10865.022135274277,
      "learning_rate": 2.766900219811126e-07,
      "loss": 1.7527,
      "step": 130752
    },
    {
      "epoch": 0.000474641091869617,
      "grad_norm": 10666.340422094168,
      "learning_rate": 2.766561359512737e-07,
      "loss": 1.7747,
      "step": 130784
    },
    {
      "epoch": 0.00047475722622045374,
      "grad_norm": 10520.9660202854,
      "learning_rate": 2.766222623683799e-07,
      "loss": 1.7829,
      "step": 130816
    },
    {
      "epoch": 0.00047487336057129044,
      "grad_norm": 8748.145060525689,
      "learning_rate": 2.765884012248131e-07,
      "loss": 1.7669,
      "step": 130848
    },
    {
      "epoch": 0.00047498949492212714,
      "grad_norm": 9390.31330680718,
      "learning_rate": 2.765545525129617e-07,
      "loss": 1.7497,
      "step": 130880
    },
    {
      "epoch": 0.00047510562927296384,
      "grad_norm": 10718.649728393963,
      "learning_rate": 2.765207162252206e-07,
      "loss": 1.7655,
      "step": 130912
    },
    {
      "epoch": 0.00047522176362380054,
      "grad_norm": 9601.664230746668,
      "learning_rate": 2.7648689235399123e-07,
      "loss": 1.75,
      "step": 130944
    },
    {
      "epoch": 0.00047533789797463724,
      "grad_norm": 8858.633528936616,
      "learning_rate": 2.7645308089168166e-07,
      "loss": 1.7286,
      "step": 130976
    },
    {
      "epoch": 0.00047545403232547394,
      "grad_norm": 9200.778445327329,
      "learning_rate": 2.7641928183070614e-07,
      "loss": 1.7298,
      "step": 131008
    },
    {
      "epoch": 0.00047557016667631064,
      "grad_norm": 10420.03838764522,
      "learning_rate": 2.7638549516348585e-07,
      "loss": 1.7324,
      "step": 131040
    },
    {
      "epoch": 0.00047568630102714734,
      "grad_norm": 9952.423825380429,
      "learning_rate": 2.76351720882448e-07,
      "loss": 1.7657,
      "step": 131072
    },
    {
      "epoch": 0.0004758024353779841,
      "grad_norm": 9034.400256796242,
      "learning_rate": 2.763179589800266e-07,
      "loss": 1.7872,
      "step": 131104
    },
    {
      "epoch": 0.0004759185697288208,
      "grad_norm": 9950.714848693031,
      "learning_rate": 2.76284209448662e-07,
      "loss": 1.7744,
      "step": 131136
    },
    {
      "epoch": 0.0004760347040796575,
      "grad_norm": 9630.094599743037,
      "learning_rate": 2.7625047228080097e-07,
      "loss": 1.7995,
      "step": 131168
    },
    {
      "epoch": 0.0004761508384304942,
      "grad_norm": 10070.73324043488,
      "learning_rate": 2.7621674746889687e-07,
      "loss": 1.7862,
      "step": 131200
    },
    {
      "epoch": 0.0004762669727813309,
      "grad_norm": 7878.087204391685,
      "learning_rate": 2.7618408833305376e-07,
      "loss": 1.7411,
      "step": 131232
    },
    {
      "epoch": 0.0004763831071321676,
      "grad_norm": 8827.537482220056,
      "learning_rate": 2.761503878249105e-07,
      "loss": 1.7459,
      "step": 131264
    },
    {
      "epoch": 0.0004764992414830043,
      "grad_norm": 9307.423273924958,
      "learning_rate": 2.761166996503577e-07,
      "loss": 1.7614,
      "step": 131296
    },
    {
      "epoch": 0.000476615375833841,
      "grad_norm": 10108.991047577398,
      "learning_rate": 2.7608302380187437e-07,
      "loss": 1.7582,
      "step": 131328
    },
    {
      "epoch": 0.0004767315101846777,
      "grad_norm": 9274.59012571445,
      "learning_rate": 2.7604936027194564e-07,
      "loss": 1.7664,
      "step": 131360
    },
    {
      "epoch": 0.00047684764453551445,
      "grad_norm": 8633.417052361134,
      "learning_rate": 2.760157090530632e-07,
      "loss": 1.8058,
      "step": 131392
    },
    {
      "epoch": 0.00047696377888635115,
      "grad_norm": 10426.893305294727,
      "learning_rate": 2.759820701377251e-07,
      "loss": 1.7702,
      "step": 131424
    },
    {
      "epoch": 0.00047707991323718785,
      "grad_norm": 10795.911633576852,
      "learning_rate": 2.759484435184358e-07,
      "loss": 1.7411,
      "step": 131456
    },
    {
      "epoch": 0.00047719604758802455,
      "grad_norm": 8697.051914298316,
      "learning_rate": 2.7591482918770614e-07,
      "loss": 1.749,
      "step": 131488
    },
    {
      "epoch": 0.00047731218193886125,
      "grad_norm": 10470.62672431789,
      "learning_rate": 2.7588122713805346e-07,
      "loss": 1.7654,
      "step": 131520
    },
    {
      "epoch": 0.00047742831628969795,
      "grad_norm": 8505.696914421535,
      "learning_rate": 2.7584763736200126e-07,
      "loss": 1.7533,
      "step": 131552
    },
    {
      "epoch": 0.00047754445064053465,
      "grad_norm": 9718.429605651316,
      "learning_rate": 2.758140598520796e-07,
      "loss": 1.7566,
      "step": 131584
    },
    {
      "epoch": 0.00047766058499137135,
      "grad_norm": 9264.593677004945,
      "learning_rate": 2.757804946008248e-07,
      "loss": 1.755,
      "step": 131616
    },
    {
      "epoch": 0.00047777671934220805,
      "grad_norm": 10140.4098536499,
      "learning_rate": 2.7574694160077965e-07,
      "loss": 1.743,
      "step": 131648
    },
    {
      "epoch": 0.0004778928536930448,
      "grad_norm": 9550.134658736493,
      "learning_rate": 2.757134008444931e-07,
      "loss": 1.7621,
      "step": 131680
    },
    {
      "epoch": 0.0004780089880438815,
      "grad_norm": 11871.821090296131,
      "learning_rate": 2.756798723245206e-07,
      "loss": 1.757,
      "step": 131712
    },
    {
      "epoch": 0.0004781251223947182,
      "grad_norm": 8278.364331194902,
      "learning_rate": 2.7564635603342384e-07,
      "loss": 1.7657,
      "step": 131744
    },
    {
      "epoch": 0.0004782412567455549,
      "grad_norm": 9467.983840290392,
      "learning_rate": 2.7561285196377096e-07,
      "loss": 1.7604,
      "step": 131776
    },
    {
      "epoch": 0.0004783573910963916,
      "grad_norm": 12209.857329223794,
      "learning_rate": 2.7557936010813635e-07,
      "loss": 1.7646,
      "step": 131808
    },
    {
      "epoch": 0.0004784735254472283,
      "grad_norm": 10139.613602105359,
      "learning_rate": 2.755458804591007e-07,
      "loss": 1.7652,
      "step": 131840
    },
    {
      "epoch": 0.000478589659798065,
      "grad_norm": 10011.529153930484,
      "learning_rate": 2.755124130092509e-07,
      "loss": 1.7576,
      "step": 131872
    },
    {
      "epoch": 0.0004787057941489017,
      "grad_norm": 10092.61829259385,
      "learning_rate": 2.7547895775118043e-07,
      "loss": 1.7249,
      "step": 131904
    },
    {
      "epoch": 0.0004788219284997384,
      "grad_norm": 10694.239944942323,
      "learning_rate": 2.7544551467748885e-07,
      "loss": 1.7417,
      "step": 131936
    },
    {
      "epoch": 0.0004789380628505751,
      "grad_norm": 9052.537323866718,
      "learning_rate": 2.75412083780782e-07,
      "loss": 1.7703,
      "step": 131968
    },
    {
      "epoch": 0.00047905419720141185,
      "grad_norm": 10869.996872124664,
      "learning_rate": 2.753786650536721e-07,
      "loss": 1.7742,
      "step": 132000
    },
    {
      "epoch": 0.00047917033155224855,
      "grad_norm": 11320.917188991358,
      "learning_rate": 2.753452584887775e-07,
      "loss": 1.771,
      "step": 132032
    },
    {
      "epoch": 0.00047928646590308525,
      "grad_norm": 10366.16438225827,
      "learning_rate": 2.7531186407872306e-07,
      "loss": 1.7564,
      "step": 132064
    },
    {
      "epoch": 0.00047940260025392195,
      "grad_norm": 9370.64330769238,
      "learning_rate": 2.752784818161397e-07,
      "loss": 1.761,
      "step": 132096
    },
    {
      "epoch": 0.00047951873460475865,
      "grad_norm": 10176.123820001405,
      "learning_rate": 2.7524511169366464e-07,
      "loss": 1.7622,
      "step": 132128
    },
    {
      "epoch": 0.00047963486895559535,
      "grad_norm": 9575.133837184732,
      "learning_rate": 2.7521175370394134e-07,
      "loss": 1.7616,
      "step": 132160
    },
    {
      "epoch": 0.00047975100330643205,
      "grad_norm": 8857.932490146897,
      "learning_rate": 2.751784078396195e-07,
      "loss": 1.766,
      "step": 132192
    },
    {
      "epoch": 0.00047986713765726875,
      "grad_norm": 9367.726084808415,
      "learning_rate": 2.7514507409335515e-07,
      "loss": 1.7662,
      "step": 132224
    },
    {
      "epoch": 0.00047998327200810545,
      "grad_norm": 9863.351965736598,
      "learning_rate": 2.751127935756774e-07,
      "loss": 1.7852,
      "step": 132256
    },
    {
      "epoch": 0.0004800994063589422,
      "grad_norm": 9771.870854652143,
      "learning_rate": 2.7507948366540063e-07,
      "loss": 1.7765,
      "step": 132288
    },
    {
      "epoch": 0.0004802155407097789,
      "grad_norm": 9227.181259734742,
      "learning_rate": 2.7504618585141523e-07,
      "loss": 1.7827,
      "step": 132320
    },
    {
      "epoch": 0.0004803316750606156,
      "grad_norm": 10683.657987786768,
      "learning_rate": 2.7501290012640194e-07,
      "loss": 1.7473,
      "step": 132352
    },
    {
      "epoch": 0.0004804478094114523,
      "grad_norm": 10250.593934011824,
      "learning_rate": 2.7497962648304744e-07,
      "loss": 1.7446,
      "step": 132384
    },
    {
      "epoch": 0.000480563943762289,
      "grad_norm": 11124.236603021352,
      "learning_rate": 2.749463649140448e-07,
      "loss": 1.7438,
      "step": 132416
    },
    {
      "epoch": 0.0004806800781131257,
      "grad_norm": 10380.735041412048,
      "learning_rate": 2.749131154120932e-07,
      "loss": 1.7488,
      "step": 132448
    },
    {
      "epoch": 0.0004807962124639624,
      "grad_norm": 9843.028802152312,
      "learning_rate": 2.7487987796989805e-07,
      "loss": 1.7598,
      "step": 132480
    },
    {
      "epoch": 0.0004809123468147991,
      "grad_norm": 10977.393497547586,
      "learning_rate": 2.748466525801708e-07,
      "loss": 1.7646,
      "step": 132512
    },
    {
      "epoch": 0.0004810284811656358,
      "grad_norm": 8128.472427215337,
      "learning_rate": 2.7481343923562926e-07,
      "loss": 1.7905,
      "step": 132544
    },
    {
      "epoch": 0.00048114461551647256,
      "grad_norm": 8256.803376610103,
      "learning_rate": 2.747802379289972e-07,
      "loss": 1.7412,
      "step": 132576
    },
    {
      "epoch": 0.00048126074986730926,
      "grad_norm": 9973.933125903743,
      "learning_rate": 2.747470486530047e-07,
      "loss": 1.7401,
      "step": 132608
    },
    {
      "epoch": 0.00048137688421814596,
      "grad_norm": 9303.445598271643,
      "learning_rate": 2.7471387140038783e-07,
      "loss": 1.7617,
      "step": 132640
    },
    {
      "epoch": 0.00048149301856898266,
      "grad_norm": 10154.669861694176,
      "learning_rate": 2.7468070616388906e-07,
      "loss": 1.7455,
      "step": 132672
    },
    {
      "epoch": 0.00048160915291981936,
      "grad_norm": 10716.541233065826,
      "learning_rate": 2.746475529362567e-07,
      "loss": 1.742,
      "step": 132704
    },
    {
      "epoch": 0.00048172528727065606,
      "grad_norm": 8723.078355718239,
      "learning_rate": 2.7461441171024534e-07,
      "loss": 1.7601,
      "step": 132736
    },
    {
      "epoch": 0.00048184142162149276,
      "grad_norm": 9735.193783382025,
      "learning_rate": 2.745812824786156e-07,
      "loss": 1.7443,
      "step": 132768
    },
    {
      "epoch": 0.00048195755597232946,
      "grad_norm": 10137.040791079022,
      "learning_rate": 2.745481652341344e-07,
      "loss": 1.7515,
      "step": 132800
    },
    {
      "epoch": 0.00048207369032316616,
      "grad_norm": 10174.479544428796,
      "learning_rate": 2.7451505996957455e-07,
      "loss": 1.7635,
      "step": 132832
    },
    {
      "epoch": 0.0004821898246740029,
      "grad_norm": 8639.943171109402,
      "learning_rate": 2.7448196667771497e-07,
      "loss": 1.769,
      "step": 132864
    },
    {
      "epoch": 0.0004823059590248396,
      "grad_norm": 9750.956773568429,
      "learning_rate": 2.744488853513409e-07,
      "loss": 1.7788,
      "step": 132896
    },
    {
      "epoch": 0.0004824220933756763,
      "grad_norm": 11487.960045195143,
      "learning_rate": 2.744158159832434e-07,
      "loss": 1.8022,
      "step": 132928
    },
    {
      "epoch": 0.000482538227726513,
      "grad_norm": 9266.992931906228,
      "learning_rate": 2.7438275856621977e-07,
      "loss": 1.7963,
      "step": 132960
    },
    {
      "epoch": 0.0004826543620773497,
      "grad_norm": 10391.506243081414,
      "learning_rate": 2.7434971309307334e-07,
      "loss": 1.7696,
      "step": 132992
    },
    {
      "epoch": 0.0004827704964281864,
      "grad_norm": 10526.563731816761,
      "learning_rate": 2.7431667955661347e-07,
      "loss": 1.7264,
      "step": 133024
    },
    {
      "epoch": 0.0004828866307790231,
      "grad_norm": 11232.004540597372,
      "learning_rate": 2.7428365794965556e-07,
      "loss": 1.7471,
      "step": 133056
    },
    {
      "epoch": 0.0004830027651298598,
      "grad_norm": 9312.580308378554,
      "learning_rate": 2.742506482650212e-07,
      "loss": 1.7366,
      "step": 133088
    },
    {
      "epoch": 0.0004831188994806965,
      "grad_norm": 9738.381487701126,
      "learning_rate": 2.7421765049553786e-07,
      "loss": 1.7705,
      "step": 133120
    },
    {
      "epoch": 0.00048323503383153327,
      "grad_norm": 9897.678111557276,
      "learning_rate": 2.741846646340392e-07,
      "loss": 1.766,
      "step": 133152
    },
    {
      "epoch": 0.00048335116818236997,
      "grad_norm": 9113.325188974659,
      "learning_rate": 2.7415169067336477e-07,
      "loss": 1.7549,
      "step": 133184
    },
    {
      "epoch": 0.00048346730253320667,
      "grad_norm": 8702.75232325958,
      "learning_rate": 2.7411872860636025e-07,
      "loss": 1.7812,
      "step": 133216
    },
    {
      "epoch": 0.00048358343688404337,
      "grad_norm": 10606.876260237977,
      "learning_rate": 2.740868079391654e-07,
      "loss": 1.7616,
      "step": 133248
    },
    {
      "epoch": 0.00048369957123488007,
      "grad_norm": 9403.686511150827,
      "learning_rate": 2.7405386926693915e-07,
      "loss": 1.7306,
      "step": 133280
    },
    {
      "epoch": 0.00048381570558571677,
      "grad_norm": 10505.433260936932,
      "learning_rate": 2.740209424671787e-07,
      "loss": 1.7374,
      "step": 133312
    },
    {
      "epoch": 0.00048393183993655347,
      "grad_norm": 9337.188549022667,
      "learning_rate": 2.7398802753275357e-07,
      "loss": 1.7499,
      "step": 133344
    },
    {
      "epoch": 0.00048404797428739017,
      "grad_norm": 9615.520162736906,
      "learning_rate": 2.739551244565393e-07,
      "loss": 1.7604,
      "step": 133376
    },
    {
      "epoch": 0.00048416410863822687,
      "grad_norm": 9303.334455989423,
      "learning_rate": 2.739222332314174e-07,
      "loss": 1.7652,
      "step": 133408
    },
    {
      "epoch": 0.0004842802429890636,
      "grad_norm": 10882.86102088968,
      "learning_rate": 2.7388935385027524e-07,
      "loss": 1.753,
      "step": 133440
    },
    {
      "epoch": 0.0004843963773399003,
      "grad_norm": 10346.599248062139,
      "learning_rate": 2.7385648630600627e-07,
      "loss": 1.7531,
      "step": 133472
    },
    {
      "epoch": 0.000484512511690737,
      "grad_norm": 9366.985107279716,
      "learning_rate": 2.738236305915099e-07,
      "loss": 1.7569,
      "step": 133504
    },
    {
      "epoch": 0.0004846286460415737,
      "grad_norm": 10309.809697564742,
      "learning_rate": 2.737907866996915e-07,
      "loss": 1.7878,
      "step": 133536
    },
    {
      "epoch": 0.0004847447803924104,
      "grad_norm": 9211.80774875377,
      "learning_rate": 2.737579546234624e-07,
      "loss": 1.7844,
      "step": 133568
    },
    {
      "epoch": 0.0004848609147432471,
      "grad_norm": 9910.126538041783,
      "learning_rate": 2.7372513435573984e-07,
      "loss": 1.7461,
      "step": 133600
    },
    {
      "epoch": 0.0004849770490940838,
      "grad_norm": 11079.205386669208,
      "learning_rate": 2.736923258894471e-07,
      "loss": 1.7513,
      "step": 133632
    },
    {
      "epoch": 0.0004850931834449205,
      "grad_norm": 10681.518244144883,
      "learning_rate": 2.736595292175132e-07,
      "loss": 1.7686,
      "step": 133664
    },
    {
      "epoch": 0.0004852093177957572,
      "grad_norm": 9671.074914403258,
      "learning_rate": 2.7362674433287335e-07,
      "loss": 1.7406,
      "step": 133696
    },
    {
      "epoch": 0.000485325452146594,
      "grad_norm": 9379.644342937529,
      "learning_rate": 2.735939712284685e-07,
      "loss": 1.7468,
      "step": 133728
    },
    {
      "epoch": 0.0004854415864974307,
      "grad_norm": 9756.121155459276,
      "learning_rate": 2.735612098972455e-07,
      "loss": 1.7386,
      "step": 133760
    },
    {
      "epoch": 0.0004855577208482674,
      "grad_norm": 8809.325059276676,
      "learning_rate": 2.735284603321573e-07,
      "loss": 1.7499,
      "step": 133792
    },
    {
      "epoch": 0.0004856738551991041,
      "grad_norm": 10697.003505655219,
      "learning_rate": 2.7349572252616257e-07,
      "loss": 1.7595,
      "step": 133824
    },
    {
      "epoch": 0.0004857899895499408,
      "grad_norm": 9101.811907526984,
      "learning_rate": 2.7346299647222593e-07,
      "loss": 1.7799,
      "step": 133856
    },
    {
      "epoch": 0.0004859061239007775,
      "grad_norm": 9246.168990452206,
      "learning_rate": 2.7343028216331796e-07,
      "loss": 1.7803,
      "step": 133888
    },
    {
      "epoch": 0.0004860222582516142,
      "grad_norm": 9477.366089795201,
      "learning_rate": 2.7339757959241495e-07,
      "loss": 1.7723,
      "step": 133920
    },
    {
      "epoch": 0.0004861383926024509,
      "grad_norm": 9359.575204035704,
      "learning_rate": 2.733648887524993e-07,
      "loss": 1.7342,
      "step": 133952
    },
    {
      "epoch": 0.0004862545269532876,
      "grad_norm": 11532.525655726937,
      "learning_rate": 2.733322096365591e-07,
      "loss": 1.7627,
      "step": 133984
    },
    {
      "epoch": 0.00048637066130412433,
      "grad_norm": 9419.32874466116,
      "learning_rate": 2.732995422375884e-07,
      "loss": 1.7475,
      "step": 134016
    },
    {
      "epoch": 0.00048648679565496103,
      "grad_norm": 10237.6653588599,
      "learning_rate": 2.732668865485871e-07,
      "loss": 1.7588,
      "step": 134048
    },
    {
      "epoch": 0.00048660293000579773,
      "grad_norm": 10007.458818301477,
      "learning_rate": 2.732342425625609e-07,
      "loss": 1.7543,
      "step": 134080
    },
    {
      "epoch": 0.00048671906435663443,
      "grad_norm": 10497.169713784759,
      "learning_rate": 2.732016102725214e-07,
      "loss": 1.7725,
      "step": 134112
    },
    {
      "epoch": 0.00048683519870747113,
      "grad_norm": 10164.53540502467,
      "learning_rate": 2.7316898967148605e-07,
      "loss": 1.7914,
      "step": 134144
    },
    {
      "epoch": 0.00048695133305830783,
      "grad_norm": 10107.200304733256,
      "learning_rate": 2.7313638075247804e-07,
      "loss": 1.7579,
      "step": 134176
    },
    {
      "epoch": 0.00048706746740914453,
      "grad_norm": 9411.145413816535,
      "learning_rate": 2.7310378350852654e-07,
      "loss": 1.7498,
      "step": 134208
    },
    {
      "epoch": 0.00048718360175998123,
      "grad_norm": 9065.277160682954,
      "learning_rate": 2.730722160553646e-07,
      "loss": 1.7612,
      "step": 134240
    },
    {
      "epoch": 0.00048729973611081793,
      "grad_norm": 10900.5509952479,
      "learning_rate": 2.730396417763315e-07,
      "loss": 1.7774,
      "step": 134272
    },
    {
      "epoch": 0.0004874158704616547,
      "grad_norm": 9203.57962968757,
      "learning_rate": 2.730070791516943e-07,
      "loss": 1.7578,
      "step": 134304
    },
    {
      "epoch": 0.0004875320048124914,
      "grad_norm": 10050.259300137484,
      "learning_rate": 2.7297452817450504e-07,
      "loss": 1.7599,
      "step": 134336
    },
    {
      "epoch": 0.0004876481391633281,
      "grad_norm": 9611.435064546813,
      "learning_rate": 2.729419888378218e-07,
      "loss": 1.7434,
      "step": 134368
    },
    {
      "epoch": 0.0004877642735141648,
      "grad_norm": 9219.404319152078,
      "learning_rate": 2.729094611347083e-07,
      "loss": 1.7183,
      "step": 134400
    },
    {
      "epoch": 0.0004878804078650015,
      "grad_norm": 10415.40762524444,
      "learning_rate": 2.7287694505823407e-07,
      "loss": 1.7258,
      "step": 134432
    },
    {
      "epoch": 0.0004879965422158382,
      "grad_norm": 11630.950348101396,
      "learning_rate": 2.7284444060147435e-07,
      "loss": 1.7508,
      "step": 134464
    },
    {
      "epoch": 0.0004881126765666749,
      "grad_norm": 9764.879517945934,
      "learning_rate": 2.728119477575103e-07,
      "loss": 1.757,
      "step": 134496
    },
    {
      "epoch": 0.0004882288109175116,
      "grad_norm": 9856.48618930702,
      "learning_rate": 2.727794665194287e-07,
      "loss": 1.7578,
      "step": 134528
    },
    {
      "epoch": 0.0004883449452683483,
      "grad_norm": 9295.65586712417,
      "learning_rate": 2.7274699688032225e-07,
      "loss": 1.7697,
      "step": 134560
    },
    {
      "epoch": 0.000488461079619185,
      "grad_norm": 8287.17068727319,
      "learning_rate": 2.727145388332893e-07,
      "loss": 1.785,
      "step": 134592
    },
    {
      "epoch": 0.0004885772139700217,
      "grad_norm": 9097.98263352926,
      "learning_rate": 2.726820923714339e-07,
      "loss": 1.7434,
      "step": 134624
    },
    {
      "epoch": 0.0004886933483208584,
      "grad_norm": 9592.031693025207,
      "learning_rate": 2.726496574878659e-07,
      "loss": 1.7425,
      "step": 134656
    },
    {
      "epoch": 0.0004888094826716951,
      "grad_norm": 9547.509308715022,
      "learning_rate": 2.72617234175701e-07,
      "loss": 1.7749,
      "step": 134688
    },
    {
      "epoch": 0.0004889256170225318,
      "grad_norm": 9780.879306074685,
      "learning_rate": 2.725848224280604e-07,
      "loss": 1.7772,
      "step": 134720
    },
    {
      "epoch": 0.0004890417513733685,
      "grad_norm": 9160.667006282894,
      "learning_rate": 2.7255242223807125e-07,
      "loss": 1.7801,
      "step": 134752
    },
    {
      "epoch": 0.0004891578857242053,
      "grad_norm": 11049.431840597053,
      "learning_rate": 2.7252003359886624e-07,
      "loss": 1.7761,
      "step": 134784
    },
    {
      "epoch": 0.0004892740200750419,
      "grad_norm": 10199.830488787546,
      "learning_rate": 2.724876565035838e-07,
      "loss": 1.7619,
      "step": 134816
    },
    {
      "epoch": 0.0004893901544258787,
      "grad_norm": 10753.370634363906,
      "learning_rate": 2.724552909453682e-07,
      "loss": 1.7673,
      "step": 134848
    },
    {
      "epoch": 0.0004895062887767153,
      "grad_norm": 9368.767048016509,
      "learning_rate": 2.724229369173692e-07,
      "loss": 1.7509,
      "step": 134880
    },
    {
      "epoch": 0.0004896224231275521,
      "grad_norm": 9710.789051359317,
      "learning_rate": 2.723905944127425e-07,
      "loss": 1.7631,
      "step": 134912
    },
    {
      "epoch": 0.0004897385574783887,
      "grad_norm": 9713.101152567084,
      "learning_rate": 2.7235826342464926e-07,
      "loss": 1.7482,
      "step": 134944
    },
    {
      "epoch": 0.0004898546918292255,
      "grad_norm": 9473.073630031597,
      "learning_rate": 2.7232594394625646e-07,
      "loss": 1.7567,
      "step": 134976
    },
    {
      "epoch": 0.0004899708261800621,
      "grad_norm": 10382.05461361093,
      "learning_rate": 2.722936359707367e-07,
      "loss": 1.7556,
      "step": 135008
    },
    {
      "epoch": 0.0004900869605308989,
      "grad_norm": 8617.070964080543,
      "learning_rate": 2.722613394912683e-07,
      "loss": 1.751,
      "step": 135040
    },
    {
      "epoch": 0.0004902030948817356,
      "grad_norm": 9780.466246554915,
      "learning_rate": 2.722290545010351e-07,
      "loss": 1.7389,
      "step": 135072
    },
    {
      "epoch": 0.0004903192292325723,
      "grad_norm": 8577.527382643555,
      "learning_rate": 2.7219678099322674e-07,
      "loss": 1.7478,
      "step": 135104
    },
    {
      "epoch": 0.000490435363583409,
      "grad_norm": 10999.198879918484,
      "learning_rate": 2.7216451896103845e-07,
      "loss": 1.7448,
      "step": 135136
    },
    {
      "epoch": 0.0004905514979342457,
      "grad_norm": 10305.72578715347,
      "learning_rate": 2.7213226839767123e-07,
      "loss": 1.739,
      "step": 135168
    },
    {
      "epoch": 0.0004906676322850824,
      "grad_norm": 9087.171837265982,
      "learning_rate": 2.7210002929633147e-07,
      "loss": 1.7588,
      "step": 135200
    },
    {
      "epoch": 0.0004907837666359191,
      "grad_norm": 8777.159677253228,
      "learning_rate": 2.720678016502314e-07,
      "loss": 1.7622,
      "step": 135232
    },
    {
      "epoch": 0.0004908999009867558,
      "grad_norm": 8890.90883993307,
      "learning_rate": 2.720365920355404e-07,
      "loss": 1.7783,
      "step": 135264
    },
    {
      "epoch": 0.0004910160353375925,
      "grad_norm": 10006.803485629165,
      "learning_rate": 2.720043869221287e-07,
      "loss": 1.7575,
      "step": 135296
    },
    {
      "epoch": 0.0004911321696884292,
      "grad_norm": 8929.58498475713,
      "learning_rate": 2.719721932438384e-07,
      "loss": 1.7407,
      "step": 135328
    },
    {
      "epoch": 0.000491248304039266,
      "grad_norm": 7991.640257168738,
      "learning_rate": 2.71940010993904e-07,
      "loss": 1.7479,
      "step": 135360
    },
    {
      "epoch": 0.0004913644383901026,
      "grad_norm": 8772.01117190351,
      "learning_rate": 2.719078401655655e-07,
      "loss": 1.7658,
      "step": 135392
    },
    {
      "epoch": 0.0004914805727409394,
      "grad_norm": 9172.5854588551,
      "learning_rate": 2.7187568075206856e-07,
      "loss": 1.7642,
      "step": 135424
    },
    {
      "epoch": 0.000491596707091776,
      "grad_norm": 10021.861104605272,
      "learning_rate": 2.7184353274666445e-07,
      "loss": 1.7542,
      "step": 135456
    },
    {
      "epoch": 0.0004917128414426128,
      "grad_norm": 10310.5109475719,
      "learning_rate": 2.7181139614261e-07,
      "loss": 1.7693,
      "step": 135488
    },
    {
      "epoch": 0.0004918289757934494,
      "grad_norm": 10968.811239145289,
      "learning_rate": 2.7177927093316764e-07,
      "loss": 1.7649,
      "step": 135520
    },
    {
      "epoch": 0.0004919451101442862,
      "grad_norm": 9047.164859777897,
      "learning_rate": 2.717471571116053e-07,
      "loss": 1.7445,
      "step": 135552
    },
    {
      "epoch": 0.0004920612444951228,
      "grad_norm": 9350.265771623821,
      "learning_rate": 2.7171505467119664e-07,
      "loss": 1.7605,
      "step": 135584
    },
    {
      "epoch": 0.0004921773788459596,
      "grad_norm": 9048.848545533294,
      "learning_rate": 2.716829636052206e-07,
      "loss": 1.7638,
      "step": 135616
    },
    {
      "epoch": 0.0004922935131967963,
      "grad_norm": 10622.26670725227,
      "learning_rate": 2.71650883906962e-07,
      "loss": 1.7585,
      "step": 135648
    },
    {
      "epoch": 0.000492409647547633,
      "grad_norm": 10801.830585599831,
      "learning_rate": 2.7161881556971095e-07,
      "loss": 1.7906,
      "step": 135680
    },
    {
      "epoch": 0.0004925257818984697,
      "grad_norm": 9872.345719230056,
      "learning_rate": 2.715867585867633e-07,
      "loss": 1.7885,
      "step": 135712
    },
    {
      "epoch": 0.0004926419162493064,
      "grad_norm": 9813.743220606499,
      "learning_rate": 2.7155471295142024e-07,
      "loss": 1.7285,
      "step": 135744
    },
    {
      "epoch": 0.0004927580506001431,
      "grad_norm": 7756.78735044348,
      "learning_rate": 2.7152267865698873e-07,
      "loss": 1.7313,
      "step": 135776
    },
    {
      "epoch": 0.0004928741849509798,
      "grad_norm": 9145.004647347097,
      "learning_rate": 2.71490655696781e-07,
      "loss": 1.7409,
      "step": 135808
    },
    {
      "epoch": 0.0004929903193018165,
      "grad_norm": 8821.35114367408,
      "learning_rate": 2.7145864406411497e-07,
      "loss": 1.7447,
      "step": 135840
    },
    {
      "epoch": 0.0004931064536526532,
      "grad_norm": 10133.17827732247,
      "learning_rate": 2.7142664375231406e-07,
      "loss": 1.7665,
      "step": 135872
    },
    {
      "epoch": 0.00049322258800349,
      "grad_norm": 10468.54096806236,
      "learning_rate": 2.7139465475470714e-07,
      "loss": 1.769,
      "step": 135904
    },
    {
      "epoch": 0.0004933387223543267,
      "grad_norm": 11286.994994240053,
      "learning_rate": 2.7136267706462855e-07,
      "loss": 1.7778,
      "step": 135936
    },
    {
      "epoch": 0.0004934548567051633,
      "grad_norm": 9117.85259806277,
      "learning_rate": 2.713307106754183e-07,
      "loss": 1.7671,
      "step": 135968
    },
    {
      "epoch": 0.0004935709910560001,
      "grad_norm": 9752.414060118654,
      "learning_rate": 2.712987555804217e-07,
      "loss": 1.7539,
      "step": 136000
    },
    {
      "epoch": 0.0004936871254068367,
      "grad_norm": 9953.945951229593,
      "learning_rate": 2.712668117729896e-07,
      "loss": 1.7377,
      "step": 136032
    },
    {
      "epoch": 0.0004938032597576735,
      "grad_norm": 8458.209030285312,
      "learning_rate": 2.7123487924647837e-07,
      "loss": 1.7471,
      "step": 136064
    },
    {
      "epoch": 0.0004939193941085101,
      "grad_norm": 9333.562878129658,
      "learning_rate": 2.7120295799424984e-07,
      "loss": 1.7584,
      "step": 136096
    },
    {
      "epoch": 0.0004940355284593469,
      "grad_norm": 9394.081328155511,
      "learning_rate": 2.711710480096713e-07,
      "loss": 1.7411,
      "step": 136128
    },
    {
      "epoch": 0.0004941516628101835,
      "grad_norm": 10890.830087738952,
      "learning_rate": 2.711391492861155e-07,
      "loss": 1.7453,
      "step": 136160
    },
    {
      "epoch": 0.0004942677971610203,
      "grad_norm": 10206.423271646145,
      "learning_rate": 2.711072618169607e-07,
      "loss": 1.7448,
      "step": 136192
    },
    {
      "epoch": 0.0004943839315118571,
      "grad_norm": 11081.905070880186,
      "learning_rate": 2.710753855955904e-07,
      "loss": 1.7427,
      "step": 136224
    },
    {
      "epoch": 0.0004945000658626937,
      "grad_norm": 10030.897866093543,
      "learning_rate": 2.7104451622593613e-07,
      "loss": 1.7654,
      "step": 136256
    },
    {
      "epoch": 0.0004946162002135305,
      "grad_norm": 9708.251953879237,
      "learning_rate": 2.710126621293275e-07,
      "loss": 1.7785,
      "step": 136288
    },
    {
      "epoch": 0.0004947323345643671,
      "grad_norm": 10015.852534856931,
      "learning_rate": 2.709808192608932e-07,
      "loss": 1.7772,
      "step": 136320
    },
    {
      "epoch": 0.0004948484689152039,
      "grad_norm": 9659.197585721084,
      "learning_rate": 2.709489876140386e-07,
      "loss": 1.7616,
      "step": 136352
    },
    {
      "epoch": 0.0004949646032660405,
      "grad_norm": 10667.986032986732,
      "learning_rate": 2.7091716718217417e-07,
      "loss": 1.7676,
      "step": 136384
    },
    {
      "epoch": 0.0004950807376168773,
      "grad_norm": 10107.745742746005,
      "learning_rate": 2.708853579587161e-07,
      "loss": 1.7648,
      "step": 136416
    },
    {
      "epoch": 0.0004951968719677139,
      "grad_norm": 8550.1154378172,
      "learning_rate": 2.7085355993708585e-07,
      "loss": 1.7472,
      "step": 136448
    },
    {
      "epoch": 0.0004953130063185507,
      "grad_norm": 9369.971237949452,
      "learning_rate": 2.7082177311071024e-07,
      "loss": 1.7493,
      "step": 136480
    },
    {
      "epoch": 0.0004954291406693874,
      "grad_norm": 11311.646918110555,
      "learning_rate": 2.707899974730216e-07,
      "loss": 1.7644,
      "step": 136512
    },
    {
      "epoch": 0.0004955452750202241,
      "grad_norm": 9197.946727395196,
      "learning_rate": 2.7075823301745763e-07,
      "loss": 1.756,
      "step": 136544
    },
    {
      "epoch": 0.0004956614093710608,
      "grad_norm": 10885.789544171796,
      "learning_rate": 2.707264797374613e-07,
      "loss": 1.7708,
      "step": 136576
    },
    {
      "epoch": 0.0004957775437218975,
      "grad_norm": 9271.021195100355,
      "learning_rate": 2.706947376264811e-07,
      "loss": 1.7766,
      "step": 136608
    },
    {
      "epoch": 0.0004958936780727342,
      "grad_norm": 9286.195238094017,
      "learning_rate": 2.7066300667797096e-07,
      "loss": 1.7761,
      "step": 136640
    },
    {
      "epoch": 0.0004960098124235709,
      "grad_norm": 9000.990501050426,
      "learning_rate": 2.7063128688538987e-07,
      "loss": 1.7359,
      "step": 136672
    },
    {
      "epoch": 0.0004961259467744076,
      "grad_norm": 9711.218873035454,
      "learning_rate": 2.7059957824220246e-07,
      "loss": 1.7204,
      "step": 136704
    },
    {
      "epoch": 0.0004962420811252443,
      "grad_norm": 10207.891457103176,
      "learning_rate": 2.7056788074187863e-07,
      "loss": 1.7537,
      "step": 136736
    },
    {
      "epoch": 0.000496358215476081,
      "grad_norm": 10947.547761941942,
      "learning_rate": 2.705361943778937e-07,
      "loss": 1.745,
      "step": 136768
    },
    {
      "epoch": 0.0004964743498269178,
      "grad_norm": 10212.977528615247,
      "learning_rate": 2.7050451914372823e-07,
      "loss": 1.7482,
      "step": 136800
    },
    {
      "epoch": 0.0004965904841777544,
      "grad_norm": 9463.813713297615,
      "learning_rate": 2.704728550328682e-07,
      "loss": 1.7574,
      "step": 136832
    },
    {
      "epoch": 0.0004967066185285912,
      "grad_norm": 9735.861954649932,
      "learning_rate": 2.704412020388049e-07,
      "loss": 1.7656,
      "step": 136864
    },
    {
      "epoch": 0.0004968227528794278,
      "grad_norm": 10181.66528618968,
      "learning_rate": 2.704095601550349e-07,
      "loss": 1.7513,
      "step": 136896
    },
    {
      "epoch": 0.0004969388872302646,
      "grad_norm": 10193.85736608081,
      "learning_rate": 2.703779293750602e-07,
      "loss": 1.7687,
      "step": 136928
    },
    {
      "epoch": 0.0004970550215811012,
      "grad_norm": 9284.964189483986,
      "learning_rate": 2.70346309692388e-07,
      "loss": 1.7537,
      "step": 136960
    },
    {
      "epoch": 0.000497171155931938,
      "grad_norm": 9235.50832385527,
      "learning_rate": 2.7031470110053095e-07,
      "loss": 1.7564,
      "step": 136992
    },
    {
      "epoch": 0.0004972872902827746,
      "grad_norm": 10862.558630451667,
      "learning_rate": 2.702831035930069e-07,
      "loss": 1.7657,
      "step": 137024
    },
    {
      "epoch": 0.0004974034246336114,
      "grad_norm": 8960.900512783299,
      "learning_rate": 2.7025151716333905e-07,
      "loss": 1.7746,
      "step": 137056
    },
    {
      "epoch": 0.0004975195589844481,
      "grad_norm": 10849.339979925046,
      "learning_rate": 2.702199418050559e-07,
      "loss": 1.7744,
      "step": 137088
    },
    {
      "epoch": 0.0004976356933352848,
      "grad_norm": 10406.036997820063,
      "learning_rate": 2.7018837751169123e-07,
      "loss": 1.7282,
      "step": 137120
    },
    {
      "epoch": 0.0004977518276861215,
      "grad_norm": 8958.475428330425,
      "learning_rate": 2.70156824276784e-07,
      "loss": 1.7317,
      "step": 137152
    },
    {
      "epoch": 0.0004978679620369582,
      "grad_norm": 10066.79412722839,
      "learning_rate": 2.7012528209387876e-07,
      "loss": 1.7463,
      "step": 137184
    },
    {
      "epoch": 0.0004979840963877949,
      "grad_norm": 9244.710703964727,
      "learning_rate": 2.700937509565249e-07,
      "loss": 1.7633,
      "step": 137216
    },
    {
      "epoch": 0.0004981002307386316,
      "grad_norm": 18907.80283375094,
      "learning_rate": 2.7006223085827743e-07,
      "loss": 1.7766,
      "step": 137248
    },
    {
      "epoch": 0.0004982163650894683,
      "grad_norm": 9944.913272623346,
      "learning_rate": 2.700317062840615e-07,
      "loss": 1.7696,
      "step": 137280
    },
    {
      "epoch": 0.000498332499440305,
      "grad_norm": 8958.575109915639,
      "learning_rate": 2.7000020790024013e-07,
      "loss": 1.7701,
      "step": 137312
    },
    {
      "epoch": 0.0004984486337911417,
      "grad_norm": 9226.270319040083,
      "learning_rate": 2.699687205364222e-07,
      "loss": 1.7568,
      "step": 137344
    },
    {
      "epoch": 0.0004985647681419785,
      "grad_norm": 9092.221950656505,
      "learning_rate": 2.6993724418618346e-07,
      "loss": 1.7299,
      "step": 137376
    },
    {
      "epoch": 0.0004986809024928151,
      "grad_norm": 9520.260500637574,
      "learning_rate": 2.699057788431049e-07,
      "loss": 1.7549,
      "step": 137408
    },
    {
      "epoch": 0.0004987970368436519,
      "grad_norm": 9292.27539411096,
      "learning_rate": 2.698743245007727e-07,
      "loss": 1.765,
      "step": 137440
    },
    {
      "epoch": 0.0004989131711944885,
      "grad_norm": 9957.68527319477,
      "learning_rate": 2.698428811527784e-07,
      "loss": 1.7448,
      "step": 137472
    },
    {
      "epoch": 0.0004990293055453253,
      "grad_norm": 8967.836862922964,
      "learning_rate": 2.698114487927186e-07,
      "loss": 1.7554,
      "step": 137504
    },
    {
      "epoch": 0.0004991454398961619,
      "grad_norm": 9574.249108937995,
      "learning_rate": 2.6978002741419526e-07,
      "loss": 1.7701,
      "step": 137536
    },
    {
      "epoch": 0.0004992615742469987,
      "grad_norm": 11017.126304077665,
      "learning_rate": 2.697486170108155e-07,
      "loss": 1.7569,
      "step": 137568
    },
    {
      "epoch": 0.0004993777085978353,
      "grad_norm": 10702.581557736432,
      "learning_rate": 2.697172175761916e-07,
      "loss": 1.7509,
      "step": 137600
    },
    {
      "epoch": 0.0004994938429486721,
      "grad_norm": 9375.609633511838,
      "learning_rate": 2.696858291039411e-07,
      "loss": 1.7673,
      "step": 137632
    },
    {
      "epoch": 0.0004996099772995088,
      "grad_norm": 9381.832550200414,
      "learning_rate": 2.6965445158768674e-07,
      "loss": 1.7776,
      "step": 137664
    },
    {
      "epoch": 0.0004997261116503455,
      "grad_norm": 9417.680393812481,
      "learning_rate": 2.6962308502105647e-07,
      "loss": 1.7724,
      "step": 137696
    },
    {
      "epoch": 0.0004998422460011822,
      "grad_norm": 10740.141246743453,
      "learning_rate": 2.695917293976833e-07,
      "loss": 1.7586,
      "step": 137728
    },
    {
      "epoch": 0.0004999583803520189,
      "grad_norm": 9875.696532397094,
      "learning_rate": 2.695603847112056e-07,
      "loss": 1.7512,
      "step": 137760
    },
    {
      "epoch": 0.0005000745147028556,
      "grad_norm": 9152.348114008775,
      "learning_rate": 2.6952905095526687e-07,
      "loss": 1.7252,
      "step": 137792
    },
    {
      "epoch": 0.0005001906490536923,
      "grad_norm": 10009.0357177902,
      "learning_rate": 2.694977281235156e-07,
      "loss": 1.7449,
      "step": 137824
    },
    {
      "epoch": 0.000500306783404529,
      "grad_norm": 8939.38689172809,
      "learning_rate": 2.6946641620960573e-07,
      "loss": 1.7466,
      "step": 137856
    },
    {
      "epoch": 0.0005004229177553657,
      "grad_norm": 9009.030802478144,
      "learning_rate": 2.694351152071962e-07,
      "loss": 1.734,
      "step": 137888
    },
    {
      "epoch": 0.0005005390521062024,
      "grad_norm": 10833.74616649292,
      "learning_rate": 2.6940382510995104e-07,
      "loss": 1.7432,
      "step": 137920
    },
    {
      "epoch": 0.0005006551864570392,
      "grad_norm": 9157.996287398242,
      "learning_rate": 2.693725459115396e-07,
      "loss": 1.7589,
      "step": 137952
    },
    {
      "epoch": 0.0005007713208078758,
      "grad_norm": 10253.218031427987,
      "learning_rate": 2.6934127760563626e-07,
      "loss": 1.7738,
      "step": 137984
    },
    {
      "epoch": 0.0005008874551587126,
      "grad_norm": 9667.564946769171,
      "learning_rate": 2.693100201859206e-07,
      "loss": 1.7607,
      "step": 138016
    },
    {
      "epoch": 0.0005010035895095492,
      "grad_norm": 9330.060021243164,
      "learning_rate": 2.692787736460773e-07,
      "loss": 1.7349,
      "step": 138048
    },
    {
      "epoch": 0.000501119723860386,
      "grad_norm": 10105.155120036505,
      "learning_rate": 2.6924753797979606e-07,
      "loss": 1.7418,
      "step": 138080
    },
    {
      "epoch": 0.0005012358582112226,
      "grad_norm": 8957.562726545653,
      "learning_rate": 2.692163131807719e-07,
      "loss": 1.7632,
      "step": 138112
    },
    {
      "epoch": 0.0005013519925620594,
      "grad_norm": 9662.713076563952,
      "learning_rate": 2.6918509924270496e-07,
      "loss": 1.7773,
      "step": 138144
    },
    {
      "epoch": 0.000501468126912896,
      "grad_norm": 9691.018315945956,
      "learning_rate": 2.691538961593002e-07,
      "loss": 1.7686,
      "step": 138176
    },
    {
      "epoch": 0.0005015842612637328,
      "grad_norm": 10561.561816322432,
      "learning_rate": 2.691227039242681e-07,
      "loss": 1.773,
      "step": 138208
    },
    {
      "epoch": 0.0005017003956145695,
      "grad_norm": 10370.203276696171,
      "learning_rate": 2.690915225313238e-07,
      "loss": 1.7766,
      "step": 138240
    },
    {
      "epoch": 0.0005018165299654062,
      "grad_norm": 10717.213257185844,
      "learning_rate": 2.6906132589014226e-07,
      "loss": 1.7869,
      "step": 138272
    },
    {
      "epoch": 0.0005019326643162429,
      "grad_norm": 9885.369188856832,
      "learning_rate": 2.6903016582421233e-07,
      "loss": 1.7538,
      "step": 138304
    },
    {
      "epoch": 0.0005020487986670796,
      "grad_norm": 11582.319629504273,
      "learning_rate": 2.6899901658174283e-07,
      "loss": 1.7511,
      "step": 138336
    },
    {
      "epoch": 0.0005021649330179163,
      "grad_norm": 8779.11259752374,
      "learning_rate": 2.6896787815646936e-07,
      "loss": 1.7514,
      "step": 138368
    },
    {
      "epoch": 0.000502281067368753,
      "grad_norm": 8596.104001232186,
      "learning_rate": 2.689367505421325e-07,
      "loss": 1.7552,
      "step": 138400
    },
    {
      "epoch": 0.0005023972017195897,
      "grad_norm": 9739.708414526585,
      "learning_rate": 2.6890563373247797e-07,
      "loss": 1.7731,
      "step": 138432
    },
    {
      "epoch": 0.0005025133360704264,
      "grad_norm": 9027.57043727713,
      "learning_rate": 2.6887452772125656e-07,
      "loss": 1.757,
      "step": 138464
    },
    {
      "epoch": 0.0005026294704212631,
      "grad_norm": 8013.333139212422,
      "learning_rate": 2.6884343250222407e-07,
      "loss": 1.7197,
      "step": 138496
    },
    {
      "epoch": 0.0005027456047720999,
      "grad_norm": 8955.27872263058,
      "learning_rate": 2.6881234806914137e-07,
      "loss": 1.7251,
      "step": 138528
    },
    {
      "epoch": 0.0005028617391229365,
      "grad_norm": 11338.056623601771,
      "learning_rate": 2.6878127441577445e-07,
      "loss": 1.7628,
      "step": 138560
    },
    {
      "epoch": 0.0005029778734737733,
      "grad_norm": 9512.590919407814,
      "learning_rate": 2.687502115358943e-07,
      "loss": 1.7398,
      "step": 138592
    },
    {
      "epoch": 0.0005030940078246099,
      "grad_norm": 9144.63208663968,
      "learning_rate": 2.687191594232769e-07,
      "loss": 1.7471,
      "step": 138624
    },
    {
      "epoch": 0.0005032101421754467,
      "grad_norm": 9696.763480667145,
      "learning_rate": 2.686881180717032e-07,
      "loss": 1.7665,
      "step": 138656
    },
    {
      "epoch": 0.0005033262765262833,
      "grad_norm": 10690.033956915198,
      "learning_rate": 2.6865708747495937e-07,
      "loss": 1.7628,
      "step": 138688
    },
    {
      "epoch": 0.0005034424108771201,
      "grad_norm": 9688.801783502437,
      "learning_rate": 2.6862606762683653e-07,
      "loss": 1.7213,
      "step": 138720
    },
    {
      "epoch": 0.0005035585452279567,
      "grad_norm": 9573.903070326125,
      "learning_rate": 2.6859505852113075e-07,
      "loss": 1.7502,
      "step": 138752
    },
    {
      "epoch": 0.0005036746795787935,
      "grad_norm": 9596.258229122433,
      "learning_rate": 2.6856406015164317e-07,
      "loss": 1.7666,
      "step": 138784
    },
    {
      "epoch": 0.0005037908139296302,
      "grad_norm": 11125.553469378501,
      "learning_rate": 2.6853307251218e-07,
      "loss": 1.7717,
      "step": 138816
    },
    {
      "epoch": 0.0005039069482804669,
      "grad_norm": 9954.782468743353,
      "learning_rate": 2.6850209559655215e-07,
      "loss": 1.7727,
      "step": 138848
    },
    {
      "epoch": 0.0005040230826313036,
      "grad_norm": 9751.248535444063,
      "learning_rate": 2.68471129398576e-07,
      "loss": 1.7678,
      "step": 138880
    },
    {
      "epoch": 0.0005041392169821403,
      "grad_norm": 9567.03830869303,
      "learning_rate": 2.684401739120725e-07,
      "loss": 1.7567,
      "step": 138912
    },
    {
      "epoch": 0.000504255351332977,
      "grad_norm": 9300.968121652713,
      "learning_rate": 2.684092291308678e-07,
      "loss": 1.7461,
      "step": 138944
    },
    {
      "epoch": 0.0005043714856838137,
      "grad_norm": 9471.808697392487,
      "learning_rate": 2.683782950487931e-07,
      "loss": 1.7611,
      "step": 138976
    },
    {
      "epoch": 0.0005044876200346504,
      "grad_norm": 9138.592889498908,
      "learning_rate": 2.6834737165968433e-07,
      "loss": 1.768,
      "step": 139008
    },
    {
      "epoch": 0.0005046037543854871,
      "grad_norm": 10759.994423790378,
      "learning_rate": 2.6831645895738257e-07,
      "loss": 1.7789,
      "step": 139040
    },
    {
      "epoch": 0.0005047198887363238,
      "grad_norm": 8683.80515672709,
      "learning_rate": 2.6828555693573376e-07,
      "loss": 1.7814,
      "step": 139072
    },
    {
      "epoch": 0.0005048360230871606,
      "grad_norm": 8556.14539380906,
      "learning_rate": 2.6825466558858897e-07,
      "loss": 1.7693,
      "step": 139104
    },
    {
      "epoch": 0.0005049521574379972,
      "grad_norm": 10767.16155725361,
      "learning_rate": 2.6822378490980404e-07,
      "loss": 1.7709,
      "step": 139136
    },
    {
      "epoch": 0.000505068291788834,
      "grad_norm": 9035.778439072086,
      "learning_rate": 2.6819291489323987e-07,
      "loss": 1.7194,
      "step": 139168
    },
    {
      "epoch": 0.0005051844261396706,
      "grad_norm": 11070.077867838148,
      "learning_rate": 2.681620555327622e-07,
      "loss": 1.7266,
      "step": 139200
    },
    {
      "epoch": 0.0005053005604905074,
      "grad_norm": 10077.220350870572,
      "learning_rate": 2.6813120682224187e-07,
      "loss": 1.7314,
      "step": 139232
    },
    {
      "epoch": 0.000505416694841344,
      "grad_norm": 7858.02265204167,
      "learning_rate": 2.6810133228408663e-07,
      "loss": 1.7343,
      "step": 139264
    },
    {
      "epoch": 0.0005055328291921808,
      "grad_norm": 9641.522079008066,
      "learning_rate": 2.680705045227769e-07,
      "loss": 1.7552,
      "step": 139296
    },
    {
      "epoch": 0.0005056489635430174,
      "grad_norm": 9787.847771599229,
      "learning_rate": 2.680396873932573e-07,
      "loss": 1.7626,
      "step": 139328
    },
    {
      "epoch": 0.0005057650978938542,
      "grad_norm": 9088.714320518608,
      "learning_rate": 2.6800888088941815e-07,
      "loss": 1.7929,
      "step": 139360
    },
    {
      "epoch": 0.0005058812322446909,
      "grad_norm": 10727.488242827396,
      "learning_rate": 2.679780850051547e-07,
      "loss": 1.7686,
      "step": 139392
    },
    {
      "epoch": 0.0005059973665955276,
      "grad_norm": 10405.156606221744,
      "learning_rate": 2.6794729973436693e-07,
      "loss": 1.7603,
      "step": 139424
    },
    {
      "epoch": 0.0005061135009463643,
      "grad_norm": 8807.858990696888,
      "learning_rate": 2.6791652507096005e-07,
      "loss": 1.7467,
      "step": 139456
    },
    {
      "epoch": 0.000506229635297201,
      "grad_norm": 9586.9484195963,
      "learning_rate": 2.678857610088439e-07,
      "loss": 1.7524,
      "step": 139488
    },
    {
      "epoch": 0.0005063457696480377,
      "grad_norm": 10321.0587635184,
      "learning_rate": 2.678550075419333e-07,
      "loss": 1.7358,
      "step": 139520
    },
    {
      "epoch": 0.0005064619039988744,
      "grad_norm": 10355.7237313478,
      "learning_rate": 2.67824264664148e-07,
      "loss": 1.7417,
      "step": 139552
    },
    {
      "epoch": 0.0005065780383497111,
      "grad_norm": 9016.81373878822,
      "learning_rate": 2.677935323694125e-07,
      "loss": 1.7527,
      "step": 139584
    },
    {
      "epoch": 0.0005066941727005478,
      "grad_norm": 8755.03489427655,
      "learning_rate": 2.677628106516564e-07,
      "loss": 1.7452,
      "step": 139616
    },
    {
      "epoch": 0.0005068103070513845,
      "grad_norm": 10594.784377230148,
      "learning_rate": 2.67732099504814e-07,
      "loss": 1.7557,
      "step": 139648
    },
    {
      "epoch": 0.0005069264414022213,
      "grad_norm": 9762.670228989607,
      "learning_rate": 2.677013989228245e-07,
      "loss": 1.7743,
      "step": 139680
    },
    {
      "epoch": 0.0005070425757530579,
      "grad_norm": 10318.929401832344,
      "learning_rate": 2.67670708899632e-07,
      "loss": 1.7653,
      "step": 139712
    },
    {
      "epoch": 0.0005071587101038947,
      "grad_norm": 10246.031719646391,
      "learning_rate": 2.676400294291854e-07,
      "loss": 1.7639,
      "step": 139744
    },
    {
      "epoch": 0.0005072748444547313,
      "grad_norm": 10680.982539073828,
      "learning_rate": 2.6760936050543854e-07,
      "loss": 1.7605,
      "step": 139776
    },
    {
      "epoch": 0.0005073909788055681,
      "grad_norm": 9459.753802293166,
      "learning_rate": 2.6757870212235007e-07,
      "loss": 1.7679,
      "step": 139808
    },
    {
      "epoch": 0.0005075071131564047,
      "grad_norm": 10689.443203460132,
      "learning_rate": 2.6754805427388346e-07,
      "loss": 1.7247,
      "step": 139840
    },
    {
      "epoch": 0.0005076232475072415,
      "grad_norm": 9158.831038948147,
      "learning_rate": 2.67517416954007e-07,
      "loss": 1.7389,
      "step": 139872
    },
    {
      "epoch": 0.0005077393818580781,
      "grad_norm": 9599.708849751642,
      "learning_rate": 2.6748679015669387e-07,
      "loss": 1.7439,
      "step": 139904
    },
    {
      "epoch": 0.0005078555162089149,
      "grad_norm": 8865.878636660893,
      "learning_rate": 2.674561738759221e-07,
      "loss": 1.7615,
      "step": 139936
    },
    {
      "epoch": 0.0005079716505597517,
      "grad_norm": 10265.980518196984,
      "learning_rate": 2.674255681056745e-07,
      "loss": 1.7843,
      "step": 139968
    },
    {
      "epoch": 0.0005080877849105883,
      "grad_norm": 10134.173079240358,
      "learning_rate": 2.673949728399386e-07,
      "loss": 1.8125,
      "step": 140000
    },
    {
      "epoch": 0.000508203919261425,
      "grad_norm": 9903.913569897508,
      "learning_rate": 2.6736438807270694e-07,
      "loss": 1.7825,
      "step": 140032
    },
    {
      "epoch": 0.0005083200536122617,
      "grad_norm": 10382.731625155298,
      "learning_rate": 2.673338137979768e-07,
      "loss": 1.7429,
      "step": 140064
    },
    {
      "epoch": 0.0005084361879630985,
      "grad_norm": 8867.460177525467,
      "learning_rate": 2.673032500097501e-07,
      "loss": 1.7205,
      "step": 140096
    },
    {
      "epoch": 0.0005085523223139351,
      "grad_norm": 7993.368376347984,
      "learning_rate": 2.672726967020338e-07,
      "loss": 1.736,
      "step": 140128
    },
    {
      "epoch": 0.0005086684566647719,
      "grad_norm": 9292.34717388454,
      "learning_rate": 2.672421538688396e-07,
      "loss": 1.7401,
      "step": 140160
    },
    {
      "epoch": 0.0005087845910156085,
      "grad_norm": 9135.626196380848,
      "learning_rate": 2.6721162150418373e-07,
      "loss": 1.7507,
      "step": 140192
    },
    {
      "epoch": 0.0005089007253664452,
      "grad_norm": 10124.195869302412,
      "learning_rate": 2.6718109960208765e-07,
      "loss": 1.7504,
      "step": 140224
    },
    {
      "epoch": 0.000509016859717282,
      "grad_norm": 8961.580664146253,
      "learning_rate": 2.6715058815657724e-07,
      "loss": 1.7558,
      "step": 140256
    },
    {
      "epoch": 0.0005091329940681186,
      "grad_norm": 11105.385180172725,
      "learning_rate": 2.6712104015964494e-07,
      "loss": 1.7765,
      "step": 140288
    },
    {
      "epoch": 0.0005092491284189554,
      "grad_norm": 9502.038518128624,
      "learning_rate": 2.670905492830979e-07,
      "loss": 1.7449,
      "step": 140320
    },
    {
      "epoch": 0.000509365262769792,
      "grad_norm": 9961.80646268537,
      "learning_rate": 2.6706006884542935e-07,
      "loss": 1.7441,
      "step": 140352
    },
    {
      "epoch": 0.0005094813971206288,
      "grad_norm": 10018.678655391637,
      "learning_rate": 2.670295988406842e-07,
      "loss": 1.7574,
      "step": 140384
    },
    {
      "epoch": 0.0005095975314714654,
      "grad_norm": 12079.41786676825,
      "learning_rate": 2.669991392629122e-07,
      "loss": 1.7586,
      "step": 140416
    },
    {
      "epoch": 0.0005097136658223022,
      "grad_norm": 9450.130157833806,
      "learning_rate": 2.669686901061677e-07,
      "loss": 1.7523,
      "step": 140448
    },
    {
      "epoch": 0.0005098298001731388,
      "grad_norm": 9876.363095795943,
      "learning_rate": 2.6693825136450987e-07,
      "loss": 1.7489,
      "step": 140480
    },
    {
      "epoch": 0.0005099459345239756,
      "grad_norm": 8498.357135352691,
      "learning_rate": 2.669078230320027e-07,
      "loss": 1.7361,
      "step": 140512
    },
    {
      "epoch": 0.0005100620688748124,
      "grad_norm": 9273.444236096962,
      "learning_rate": 2.668774051027148e-07,
      "loss": 1.7409,
      "step": 140544
    },
    {
      "epoch": 0.000510178203225649,
      "grad_norm": 9354.112036960003,
      "learning_rate": 2.6684699757071947e-07,
      "loss": 1.7636,
      "step": 140576
    },
    {
      "epoch": 0.0005102943375764858,
      "grad_norm": 10358.376224099993,
      "learning_rate": 2.6681660043009496e-07,
      "loss": 1.7891,
      "step": 140608
    },
    {
      "epoch": 0.0005104104719273224,
      "grad_norm": 9918.419833824337,
      "learning_rate": 2.6678621367492395e-07,
      "loss": 1.7619,
      "step": 140640
    },
    {
      "epoch": 0.0005105266062781592,
      "grad_norm": 10635.652495263279,
      "learning_rate": 2.6675583729929414e-07,
      "loss": 1.7526,
      "step": 140672
    },
    {
      "epoch": 0.0005106427406289958,
      "grad_norm": 10708.564796460822,
      "learning_rate": 2.667254712972976e-07,
      "loss": 1.776,
      "step": 140704
    },
    {
      "epoch": 0.0005107588749798326,
      "grad_norm": 8291.986251797574,
      "learning_rate": 2.6669511566303144e-07,
      "loss": 1.7608,
      "step": 140736
    },
    {
      "epoch": 0.0005108750093306692,
      "grad_norm": 8212.180465625437,
      "learning_rate": 2.666647703905973e-07,
      "loss": 1.7286,
      "step": 140768
    },
    {
      "epoch": 0.000510991143681506,
      "grad_norm": 10015.429296839951,
      "learning_rate": 2.666344354741015e-07,
      "loss": 1.7608,
      "step": 140800
    },
    {
      "epoch": 0.0005111072780323427,
      "grad_norm": 9099.418662749835,
      "learning_rate": 2.666041109076551e-07,
      "loss": 1.7524,
      "step": 140832
    },
    {
      "epoch": 0.0005112234123831794,
      "grad_norm": 10098.823594854997,
      "learning_rate": 2.665737966853738e-07,
      "loss": 1.7685,
      "step": 140864
    },
    {
      "epoch": 0.0005113395467340161,
      "grad_norm": 10384.512699207411,
      "learning_rate": 2.6654349280137805e-07,
      "loss": 1.7488,
      "step": 140896
    },
    {
      "epoch": 0.0005114556810848528,
      "grad_norm": 9199.093324888057,
      "learning_rate": 2.6651319924979296e-07,
      "loss": 1.7602,
      "step": 140928
    },
    {
      "epoch": 0.0005115718154356895,
      "grad_norm": 11065.369763365343,
      "learning_rate": 2.664829160247483e-07,
      "loss": 1.7444,
      "step": 140960
    },
    {
      "epoch": 0.0005116879497865262,
      "grad_norm": 10585.42318473853,
      "learning_rate": 2.664526431203786e-07,
      "loss": 1.7303,
      "step": 140992
    },
    {
      "epoch": 0.0005118040841373629,
      "grad_norm": 10369.763353133958,
      "learning_rate": 2.6642238053082277e-07,
      "loss": 1.7643,
      "step": 141024
    },
    {
      "epoch": 0.0005119202184881996,
      "grad_norm": 9975.62850150305,
      "learning_rate": 2.6639212825022463e-07,
      "loss": 1.7475,
      "step": 141056
    },
    {
      "epoch": 0.0005120363528390363,
      "grad_norm": 10386.704193342564,
      "learning_rate": 2.663618862727327e-07,
      "loss": 1.7545,
      "step": 141088
    },
    {
      "epoch": 0.0005121524871898731,
      "grad_norm": 9659.343766529899,
      "learning_rate": 2.6633165459249996e-07,
      "loss": 1.7735,
      "step": 141120
    },
    {
      "epoch": 0.0005122686215407097,
      "grad_norm": 10013.724681655673,
      "learning_rate": 2.6630143320368417e-07,
      "loss": 1.7793,
      "step": 141152
    },
    {
      "epoch": 0.0005123847558915465,
      "grad_norm": 10009.7155803749,
      "learning_rate": 2.6627122210044757e-07,
      "loss": 1.7646,
      "step": 141184
    },
    {
      "epoch": 0.0005125008902423831,
      "grad_norm": 9373.044862796722,
      "learning_rate": 2.6624102127695725e-07,
      "loss": 1.7566,
      "step": 141216
    },
    {
      "epoch": 0.0005126170245932199,
      "grad_norm": 9784.453484993426,
      "learning_rate": 2.6621083072738477e-07,
      "loss": 1.7172,
      "step": 141248
    },
    {
      "epoch": 0.0005127331589440565,
      "grad_norm": 10873.579263517602,
      "learning_rate": 2.661815934243352e-07,
      "loss": 1.7347,
      "step": 141280
    },
    {
      "epoch": 0.0005128492932948933,
      "grad_norm": 9842.980239744466,
      "learning_rate": 2.661514230845237e-07,
      "loss": 1.7386,
      "step": 141312
    },
    {
      "epoch": 0.0005129654276457299,
      "grad_norm": 11327.392639085132,
      "learning_rate": 2.661212630013543e-07,
      "loss": 1.7516,
      "step": 141344
    },
    {
      "epoch": 0.0005130815619965667,
      "grad_norm": 8949.68289941046,
      "learning_rate": 2.6609111316901683e-07,
      "loss": 1.7732,
      "step": 141376
    },
    {
      "epoch": 0.0005131976963474034,
      "grad_norm": 11036.377032341728,
      "learning_rate": 2.6606097358170594e-07,
      "loss": 1.7795,
      "step": 141408
    },
    {
      "epoch": 0.0005133138306982401,
      "grad_norm": 9893.34119496543,
      "learning_rate": 2.660308442336208e-07,
      "loss": 1.7294,
      "step": 141440
    },
    {
      "epoch": 0.0005134299650490768,
      "grad_norm": 8115.685183642844,
      "learning_rate": 2.66000725118965e-07,
      "loss": 1.733,
      "step": 141472
    },
    {
      "epoch": 0.0005135460993999135,
      "grad_norm": 8314.968189957193,
      "learning_rate": 2.6597061623194706e-07,
      "loss": 1.765,
      "step": 141504
    },
    {
      "epoch": 0.0005136622337507502,
      "grad_norm": 10088.394322190226,
      "learning_rate": 2.659405175667798e-07,
      "loss": 1.7727,
      "step": 141536
    },
    {
      "epoch": 0.0005137783681015869,
      "grad_norm": 9932.074304997925,
      "learning_rate": 2.6591042911768077e-07,
      "loss": 1.7511,
      "step": 141568
    },
    {
      "epoch": 0.0005138945024524236,
      "grad_norm": 10197.058399362044,
      "learning_rate": 2.6588035087887206e-07,
      "loss": 1.7529,
      "step": 141600
    },
    {
      "epoch": 0.0005140106368032603,
      "grad_norm": 9635.004618576993,
      "learning_rate": 2.6585028284458034e-07,
      "loss": 1.7667,
      "step": 141632
    },
    {
      "epoch": 0.000514126771154097,
      "grad_norm": 10019.56925221838,
      "learning_rate": 2.658202250090368e-07,
      "loss": 1.7484,
      "step": 141664
    },
    {
      "epoch": 0.0005142429055049338,
      "grad_norm": 9916.679988786569,
      "learning_rate": 2.657901773664773e-07,
      "loss": 1.77,
      "step": 141696
    },
    {
      "epoch": 0.0005143590398557704,
      "grad_norm": 10970.388689558817,
      "learning_rate": 2.657601399111421e-07,
      "loss": 1.7813,
      "step": 141728
    },
    {
      "epoch": 0.0005144751742066072,
      "grad_norm": 11228.030103272791,
      "learning_rate": 2.657301126372762e-07,
      "loss": 1.7682,
      "step": 141760
    },
    {
      "epoch": 0.0005145913085574438,
      "grad_norm": 9670.415192741208,
      "learning_rate": 2.6570009553912904e-07,
      "loss": 1.7809,
      "step": 141792
    },
    {
      "epoch": 0.0005147074429082806,
      "grad_norm": 8498.026594451208,
      "learning_rate": 2.656700886109546e-07,
      "loss": 1.7722,
      "step": 141824
    },
    {
      "epoch": 0.0005148235772591172,
      "grad_norm": 9943.052046529778,
      "learning_rate": 2.656400918470115e-07,
      "loss": 1.7497,
      "step": 141856
    },
    {
      "epoch": 0.000514939711609954,
      "grad_norm": 10171.482881074913,
      "learning_rate": 2.6561010524156275e-07,
      "loss": 1.7213,
      "step": 141888
    },
    {
      "epoch": 0.0005150558459607906,
      "grad_norm": 9387.17657232461,
      "learning_rate": 2.6558012878887605e-07,
      "loss": 1.7108,
      "step": 141920
    },
    {
      "epoch": 0.0005151719803116274,
      "grad_norm": 9513.041784834124,
      "learning_rate": 2.6555016248322345e-07,
      "loss": 1.7451,
      "step": 141952
    },
    {
      "epoch": 0.0005152881146624641,
      "grad_norm": 11579.026556667015,
      "learning_rate": 2.6552020631888166e-07,
      "loss": 1.7539,
      "step": 141984
    },
    {
      "epoch": 0.0005154042490133008,
      "grad_norm": 9380.601260047248,
      "learning_rate": 2.6549026029013194e-07,
      "loss": 1.7531,
      "step": 142016
    },
    {
      "epoch": 0.0005155203833641375,
      "grad_norm": 9092.012648473385,
      "learning_rate": 2.6546032439125993e-07,
      "loss": 1.7475,
      "step": 142048
    },
    {
      "epoch": 0.0005156365177149742,
      "grad_norm": 9816.469833906689,
      "learning_rate": 2.654303986165558e-07,
      "loss": 1.7658,
      "step": 142080
    },
    {
      "epoch": 0.0005157526520658109,
      "grad_norm": 10016.669506377855,
      "learning_rate": 2.654004829603143e-07,
      "loss": 1.7491,
      "step": 142112
    },
    {
      "epoch": 0.0005158687864166476,
      "grad_norm": 9615.712142114073,
      "learning_rate": 2.653705774168347e-07,
      "loss": 1.74,
      "step": 142144
    },
    {
      "epoch": 0.0005159849207674843,
      "grad_norm": 7315.1148999861925,
      "learning_rate": 2.653406819804206e-07,
      "loss": 1.7275,
      "step": 142176
    },
    {
      "epoch": 0.000516101055118321,
      "grad_norm": 9629.519614186369,
      "learning_rate": 2.653107966453804e-07,
      "loss": 1.7443,
      "step": 142208
    },
    {
      "epoch": 0.0005162171894691577,
      "grad_norm": 9484.07960742633,
      "learning_rate": 2.652809214060266e-07,
      "loss": 1.7555,
      "step": 142240
    },
    {
      "epoch": 0.0005163333238199945,
      "grad_norm": 9532.645907616627,
      "learning_rate": 2.6525198938992043e-07,
      "loss": 1.7623,
      "step": 142272
    },
    {
      "epoch": 0.0005164494581708311,
      "grad_norm": 10186.165323614181,
      "learning_rate": 2.652221340098463e-07,
      "loss": 1.7604,
      "step": 142304
    },
    {
      "epoch": 0.0005165655925216679,
      "grad_norm": 10470.724807767607,
      "learning_rate": 2.6519228870860076e-07,
      "loss": 1.7713,
      "step": 142336
    },
    {
      "epoch": 0.0005166817268725045,
      "grad_norm": 9319.736798858647,
      "learning_rate": 2.6516245348051436e-07,
      "loss": 1.7655,
      "step": 142368
    },
    {
      "epoch": 0.0005167978612233413,
      "grad_norm": 10071.52471078734,
      "learning_rate": 2.6513262831992196e-07,
      "loss": 1.7741,
      "step": 142400
    },
    {
      "epoch": 0.0005169139955741779,
      "grad_norm": 10548.130071249596,
      "learning_rate": 2.65102813221163e-07,
      "loss": 1.7762,
      "step": 142432
    },
    {
      "epoch": 0.0005170301299250147,
      "grad_norm": 9276.843967643306,
      "learning_rate": 2.650730081785812e-07,
      "loss": 1.7692,
      "step": 142464
    },
    {
      "epoch": 0.0005171462642758513,
      "grad_norm": 9434.131862551,
      "learning_rate": 2.6504321318652487e-07,
      "loss": 1.7604,
      "step": 142496
    },
    {
      "epoch": 0.0005172623986266881,
      "grad_norm": 9359.119509868437,
      "learning_rate": 2.650134282393468e-07,
      "loss": 1.7605,
      "step": 142528
    },
    {
      "epoch": 0.0005173785329775248,
      "grad_norm": 10320.413751395823,
      "learning_rate": 2.6498365333140407e-07,
      "loss": 1.7636,
      "step": 142560
    },
    {
      "epoch": 0.0005174946673283615,
      "grad_norm": 8237.376281316763,
      "learning_rate": 2.6495388845705836e-07,
      "loss": 1.7341,
      "step": 142592
    },
    {
      "epoch": 0.0005176108016791982,
      "grad_norm": 9217.345170926388,
      "learning_rate": 2.649241336106757e-07,
      "loss": 1.7186,
      "step": 142624
    },
    {
      "epoch": 0.0005177269360300349,
      "grad_norm": 10176.151237083694,
      "learning_rate": 2.648943887866265e-07,
      "loss": 1.737,
      "step": 142656
    },
    {
      "epoch": 0.0005178430703808716,
      "grad_norm": 9708.348160217576,
      "learning_rate": 2.648646539792857e-07,
      "loss": 1.7444,
      "step": 142688
    },
    {
      "epoch": 0.0005179592047317083,
      "grad_norm": 9496.660886859128,
      "learning_rate": 2.6483492918303255e-07,
      "loss": 1.7545,
      "step": 142720
    },
    {
      "epoch": 0.000518075339082545,
      "grad_norm": 8856.47401622113,
      "learning_rate": 2.648052143922508e-07,
      "loss": 1.7855,
      "step": 142752
    },
    {
      "epoch": 0.0005181914734333817,
      "grad_norm": 9822.235997979278,
      "learning_rate": 2.6477550960132863e-07,
      "loss": 1.7468,
      "step": 142784
    },
    {
      "epoch": 0.0005183076077842184,
      "grad_norm": 10627.337578151924,
      "learning_rate": 2.647458148046585e-07,
      "loss": 1.7227,
      "step": 142816
    },
    {
      "epoch": 0.0005184237421350552,
      "grad_norm": 8566.269549809882,
      "learning_rate": 2.647161299966374e-07,
      "loss": 1.7559,
      "step": 142848
    },
    {
      "epoch": 0.0005185398764858918,
      "grad_norm": 9459.616376999651,
      "learning_rate": 2.646864551716666e-07,
      "loss": 1.7613,
      "step": 142880
    },
    {
      "epoch": 0.0005186560108367286,
      "grad_norm": 8950.92531529562,
      "learning_rate": 2.6465679032415193e-07,
      "loss": 1.7554,
      "step": 142912
    },
    {
      "epoch": 0.0005187721451875652,
      "grad_norm": 9356.421003781306,
      "learning_rate": 2.646271354485034e-07,
      "loss": 1.761,
      "step": 142944
    },
    {
      "epoch": 0.000518888279538402,
      "grad_norm": 9796.04757032141,
      "learning_rate": 2.645974905391356e-07,
      "loss": 1.767,
      "step": 142976
    },
    {
      "epoch": 0.0005190044138892386,
      "grad_norm": 10184.222994416412,
      "learning_rate": 2.645678555904673e-07,
      "loss": 1.7456,
      "step": 143008
    },
    {
      "epoch": 0.0005191205482400754,
      "grad_norm": 9574.314179093979,
      "learning_rate": 2.6453823059692185e-07,
      "loss": 1.7444,
      "step": 143040
    },
    {
      "epoch": 0.000519236682590912,
      "grad_norm": 11354.057336476684,
      "learning_rate": 2.6450861555292684e-07,
      "loss": 1.7598,
      "step": 143072
    },
    {
      "epoch": 0.0005193528169417488,
      "grad_norm": 9730.050770679461,
      "learning_rate": 2.6447901045291424e-07,
      "loss": 1.7546,
      "step": 143104
    },
    {
      "epoch": 0.0005194689512925855,
      "grad_norm": 10535.016279057189,
      "learning_rate": 2.644494152913204e-07,
      "loss": 1.7619,
      "step": 143136
    },
    {
      "epoch": 0.0005195850856434222,
      "grad_norm": 9039.043754734237,
      "learning_rate": 2.6441983006258605e-07,
      "loss": 1.7763,
      "step": 143168
    },
    {
      "epoch": 0.0005197012199942589,
      "grad_norm": 9343.848885764366,
      "learning_rate": 2.643902547611562e-07,
      "loss": 1.7537,
      "step": 143200
    },
    {
      "epoch": 0.0005198173543450956,
      "grad_norm": 10090.218233517053,
      "learning_rate": 2.643606893814803e-07,
      "loss": 1.7446,
      "step": 143232
    },
    {
      "epoch": 0.0005199334886959323,
      "grad_norm": 8585.253752801951,
      "learning_rate": 2.6433113391801216e-07,
      "loss": 1.7229,
      "step": 143264
    },
    {
      "epoch": 0.000520049623046769,
      "grad_norm": 8697.330739945446,
      "learning_rate": 2.643025115137749e-07,
      "loss": 1.728,
      "step": 143296
    },
    {
      "epoch": 0.0005201657573976057,
      "grad_norm": 9557.150202858591,
      "learning_rate": 2.642729755566492e-07,
      "loss": 1.7588,
      "step": 143328
    },
    {
      "epoch": 0.0005202818917484424,
      "grad_norm": 10235.054469811092,
      "learning_rate": 2.642434494992914e-07,
      "loss": 1.7676,
      "step": 143360
    },
    {
      "epoch": 0.0005203980260992791,
      "grad_norm": 12614.542401530069,
      "learning_rate": 2.642139333361723e-07,
      "loss": 1.7661,
      "step": 143392
    },
    {
      "epoch": 0.0005205141604501159,
      "grad_norm": 11965.25486565163,
      "learning_rate": 2.641844270617673e-07,
      "loss": 1.7818,
      "step": 143424
    },
    {
      "epoch": 0.0005206302948009525,
      "grad_norm": 11183.553639161391,
      "learning_rate": 2.641549306705558e-07,
      "loss": 1.7803,
      "step": 143456
    },
    {
      "epoch": 0.0005207464291517893,
      "grad_norm": 9601.920432913408,
      "learning_rate": 2.641254441570219e-07,
      "loss": 1.7376,
      "step": 143488
    },
    {
      "epoch": 0.0005208625635026259,
      "grad_norm": 9961.637014065509,
      "learning_rate": 2.640959675156538e-07,
      "loss": 1.7391,
      "step": 143520
    },
    {
      "epoch": 0.0005209786978534627,
      "grad_norm": 9596.827809229464,
      "learning_rate": 2.6406650074094384e-07,
      "loss": 1.7589,
      "step": 143552
    },
    {
      "epoch": 0.0005210948322042993,
      "grad_norm": 9404.499242383934,
      "learning_rate": 2.64037043827389e-07,
      "loss": 1.7525,
      "step": 143584
    },
    {
      "epoch": 0.0005212109665551361,
      "grad_norm": 10088.788430728438,
      "learning_rate": 2.640075967694903e-07,
      "loss": 1.7404,
      "step": 143616
    },
    {
      "epoch": 0.0005213271009059727,
      "grad_norm": 10623.158287439757,
      "learning_rate": 2.639781595617532e-07,
      "loss": 1.7608,
      "step": 143648
    },
    {
      "epoch": 0.0005214432352568095,
      "grad_norm": 9633.845960985675,
      "learning_rate": 2.6394873219868744e-07,
      "loss": 1.7627,
      "step": 143680
    },
    {
      "epoch": 0.0005215593696076462,
      "grad_norm": 9094.653594282741,
      "learning_rate": 2.6391931467480685e-07,
      "loss": 1.754,
      "step": 143712
    },
    {
      "epoch": 0.0005216755039584829,
      "grad_norm": 10868.092748960142,
      "learning_rate": 2.6388990698462974e-07,
      "loss": 1.7526,
      "step": 143744
    },
    {
      "epoch": 0.0005217916383093196,
      "grad_norm": 10329.412761623966,
      "learning_rate": 2.638605091226787e-07,
      "loss": 1.7507,
      "step": 143776
    },
    {
      "epoch": 0.0005219077726601563,
      "grad_norm": 10663.896942487769,
      "learning_rate": 2.6383112108348054e-07,
      "loss": 1.7449,
      "step": 143808
    },
    {
      "epoch": 0.000522023907010993,
      "grad_norm": 10086.541032484824,
      "learning_rate": 2.638017428615662e-07,
      "loss": 1.751,
      "step": 143840
    },
    {
      "epoch": 0.0005221400413618297,
      "grad_norm": 10071.210652151012,
      "learning_rate": 2.637723744514712e-07,
      "loss": 1.7488,
      "step": 143872
    },
    {
      "epoch": 0.0005222561757126664,
      "grad_norm": 8953.044398415546,
      "learning_rate": 2.637430158477349e-07,
      "loss": 1.7479,
      "step": 143904
    },
    {
      "epoch": 0.0005223723100635031,
      "grad_norm": 10266.88930494529,
      "learning_rate": 2.637136670449013e-07,
      "loss": 1.7169,
      "step": 143936
    },
    {
      "epoch": 0.0005224884444143398,
      "grad_norm": 9021.713141083572,
      "learning_rate": 2.636843280375184e-07,
      "loss": 1.7471,
      "step": 143968
    },
    {
      "epoch": 0.0005226045787651766,
      "grad_norm": 9148.767895186762,
      "learning_rate": 2.636549988201386e-07,
      "loss": 1.7438,
      "step": 144000
    },
    {
      "epoch": 0.0005227207131160132,
      "grad_norm": 8943.400471856328,
      "learning_rate": 2.6362567938731845e-07,
      "loss": 1.7474,
      "step": 144032
    },
    {
      "epoch": 0.00052283684746685,
      "grad_norm": 10323.384135059587,
      "learning_rate": 2.6359636973361874e-07,
      "loss": 1.7609,
      "step": 144064
    },
    {
      "epoch": 0.0005229529818176866,
      "grad_norm": 11417.823084984282,
      "learning_rate": 2.635670698536045e-07,
      "loss": 1.7871,
      "step": 144096
    },
    {
      "epoch": 0.0005230691161685234,
      "grad_norm": 9440.717451549961,
      "learning_rate": 2.635377797418451e-07,
      "loss": 1.7917,
      "step": 144128
    },
    {
      "epoch": 0.00052318525051936,
      "grad_norm": 8646.257109293016,
      "learning_rate": 2.6350849939291393e-07,
      "loss": 1.7599,
      "step": 144160
    },
    {
      "epoch": 0.0005233013848701968,
      "grad_norm": 10894.898806322157,
      "learning_rate": 2.634792288013887e-07,
      "loss": 1.7304,
      "step": 144192
    },
    {
      "epoch": 0.0005234175192210334,
      "grad_norm": 9672.309341620541,
      "learning_rate": 2.6344996796185137e-07,
      "loss": 1.7478,
      "step": 144224
    },
    {
      "epoch": 0.0005235336535718702,
      "grad_norm": 9521.273864352395,
      "learning_rate": 2.6342071686888805e-07,
      "loss": 1.7706,
      "step": 144256
    },
    {
      "epoch": 0.000523649787922707,
      "grad_norm": 9276.15469901187,
      "learning_rate": 2.633923891619373e-07,
      "loss": 1.7854,
      "step": 144288
    },
    {
      "epoch": 0.0005237659222735436,
      "grad_norm": 10576.010779117049,
      "learning_rate": 2.6336315724173665e-07,
      "loss": 1.7534,
      "step": 144320
    },
    {
      "epoch": 0.0005238820566243804,
      "grad_norm": 10554.01459161394,
      "learning_rate": 2.633339350520624e-07,
      "loss": 1.7474,
      "step": 144352
    },
    {
      "epoch": 0.000523998190975217,
      "grad_norm": 8770.80737446673,
      "learning_rate": 2.6330472258751736e-07,
      "loss": 1.7455,
      "step": 144384
    },
    {
      "epoch": 0.0005241143253260538,
      "grad_norm": 10822.69448889693,
      "learning_rate": 2.6327551984270854e-07,
      "loss": 1.7338,
      "step": 144416
    },
    {
      "epoch": 0.0005242304596768904,
      "grad_norm": 11066.213805995256,
      "learning_rate": 2.632463268122471e-07,
      "loss": 1.7479,
      "step": 144448
    },
    {
      "epoch": 0.0005243465940277272,
      "grad_norm": 9618.101163951229,
      "learning_rate": 2.6321714349074845e-07,
      "loss": 1.7482,
      "step": 144480
    },
    {
      "epoch": 0.0005244627283785638,
      "grad_norm": 10617.599257836018,
      "learning_rate": 2.6318796987283204e-07,
      "loss": 1.7533,
      "step": 144512
    },
    {
      "epoch": 0.0005245788627294006,
      "grad_norm": 9197.46986948041,
      "learning_rate": 2.6315880595312175e-07,
      "loss": 1.7588,
      "step": 144544
    },
    {
      "epoch": 0.0005246949970802373,
      "grad_norm": 8526.320542883665,
      "learning_rate": 2.6312965172624537e-07,
      "loss": 1.7872,
      "step": 144576
    },
    {
      "epoch": 0.000524811131431074,
      "grad_norm": 9009.980466127548,
      "learning_rate": 2.6310050718683493e-07,
      "loss": 1.7342,
      "step": 144608
    },
    {
      "epoch": 0.0005249272657819107,
      "grad_norm": 10344.67843869494,
      "learning_rate": 2.630713723295267e-07,
      "loss": 1.7308,
      "step": 144640
    },
    {
      "epoch": 0.0005250434001327474,
      "grad_norm": 9431.889100281025,
      "learning_rate": 2.63042247148961e-07,
      "loss": 1.732,
      "step": 144672
    },
    {
      "epoch": 0.0005251595344835841,
      "grad_norm": 9694.29770535236,
      "learning_rate": 2.6301313163978246e-07,
      "loss": 1.7631,
      "step": 144704
    },
    {
      "epoch": 0.0005252756688344208,
      "grad_norm": 9813.344180247628,
      "learning_rate": 2.629840257966396e-07,
      "loss": 1.7575,
      "step": 144736
    },
    {
      "epoch": 0.0005253918031852575,
      "grad_norm": 10341.893830435507,
      "learning_rate": 2.6295492961418533e-07,
      "loss": 1.7456,
      "step": 144768
    },
    {
      "epoch": 0.0005255079375360942,
      "grad_norm": 9597.07913898807,
      "learning_rate": 2.629258430870766e-07,
      "loss": 1.7628,
      "step": 144800
    },
    {
      "epoch": 0.0005256240718869309,
      "grad_norm": 9142.402200734772,
      "learning_rate": 2.6289676620997446e-07,
      "loss": 1.7557,
      "step": 144832
    },
    {
      "epoch": 0.0005257402062377677,
      "grad_norm": 9057.49137454737,
      "learning_rate": 2.628676989775442e-07,
      "loss": 1.7378,
      "step": 144864
    },
    {
      "epoch": 0.0005258563405886043,
      "grad_norm": 9969.817450685845,
      "learning_rate": 2.6283864138445516e-07,
      "loss": 1.7685,
      "step": 144896
    },
    {
      "epoch": 0.0005259724749394411,
      "grad_norm": 9604.013744263384,
      "learning_rate": 2.6280959342538075e-07,
      "loss": 1.7628,
      "step": 144928
    },
    {
      "epoch": 0.0005260886092902777,
      "grad_norm": 10028.035600255915,
      "learning_rate": 2.6278055509499864e-07,
      "loss": 1.7516,
      "step": 144960
    },
    {
      "epoch": 0.0005262047436411145,
      "grad_norm": 8943.070166335496,
      "learning_rate": 2.627515263879906e-07,
      "loss": 1.7514,
      "step": 144992
    },
    {
      "epoch": 0.0005263208779919511,
      "grad_norm": 10550.705474042958,
      "learning_rate": 2.627225072990423e-07,
      "loss": 1.7622,
      "step": 145024
    },
    {
      "epoch": 0.0005264370123427879,
      "grad_norm": 10728.794340465289,
      "learning_rate": 2.626934978228437e-07,
      "loss": 1.7509,
      "step": 145056
    },
    {
      "epoch": 0.0005265531466936245,
      "grad_norm": 9677.674204063702,
      "learning_rate": 2.6266449795408897e-07,
      "loss": 1.7493,
      "step": 145088
    },
    {
      "epoch": 0.0005266692810444613,
      "grad_norm": 11256.727588424621,
      "learning_rate": 2.626355076874761e-07,
      "loss": 1.7574,
      "step": 145120
    },
    {
      "epoch": 0.000526785415395298,
      "grad_norm": 10337.341050773162,
      "learning_rate": 2.6260652701770743e-07,
      "loss": 1.7827,
      "step": 145152
    },
    {
      "epoch": 0.0005269015497461347,
      "grad_norm": 9855.385634261096,
      "learning_rate": 2.6257755593948917e-07,
      "loss": 1.7635,
      "step": 145184
    },
    {
      "epoch": 0.0005270176840969714,
      "grad_norm": 9568.001881270719,
      "learning_rate": 2.6254859444753183e-07,
      "loss": 1.7715,
      "step": 145216
    },
    {
      "epoch": 0.0005271338184478081,
      "grad_norm": 10211.467083627113,
      "learning_rate": 2.625196425365498e-07,
      "loss": 1.7488,
      "step": 145248
    },
    {
      "epoch": 0.0005272499527986448,
      "grad_norm": 10564.017606952386,
      "learning_rate": 2.624916045043474e-07,
      "loss": 1.749,
      "step": 145280
    },
    {
      "epoch": 0.0005273660871494815,
      "grad_norm": 9232.494570808043,
      "learning_rate": 2.6246267144048025e-07,
      "loss": 1.7551,
      "step": 145312
    },
    {
      "epoch": 0.0005274822215003182,
      "grad_norm": 8108.664748280076,
      "learning_rate": 2.6243374794192117e-07,
      "loss": 1.7226,
      "step": 145344
    },
    {
      "epoch": 0.0005275983558511549,
      "grad_norm": 11183.849963228226,
      "learning_rate": 2.624048340034007e-07,
      "loss": 1.7386,
      "step": 145376
    },
    {
      "epoch": 0.0005277144902019916,
      "grad_norm": 10015.116973855074,
      "learning_rate": 2.623759296196537e-07,
      "loss": 1.748,
      "step": 145408
    },
    {
      "epoch": 0.0005278306245528283,
      "grad_norm": 9857.637749481364,
      "learning_rate": 2.623470347854188e-07,
      "loss": 1.7604,
      "step": 145440
    },
    {
      "epoch": 0.000527946758903665,
      "grad_norm": 8638.137183444125,
      "learning_rate": 2.62318149495439e-07,
      "loss": 1.7679,
      "step": 145472
    },
    {
      "epoch": 0.0005280628932545018,
      "grad_norm": 9524.997847768786,
      "learning_rate": 2.622892737444611e-07,
      "loss": 1.794,
      "step": 145504
    },
    {
      "epoch": 0.0005281790276053384,
      "grad_norm": 9944.593405464097,
      "learning_rate": 2.6226040752723594e-07,
      "loss": 1.7111,
      "step": 145536
    },
    {
      "epoch": 0.0005282951619561752,
      "grad_norm": 8868.383956505266,
      "learning_rate": 2.622315508385187e-07,
      "loss": 1.7242,
      "step": 145568
    },
    {
      "epoch": 0.0005284112963070118,
      "grad_norm": 9519.768064401569,
      "learning_rate": 2.622027036730682e-07,
      "loss": 1.7332,
      "step": 145600
    },
    {
      "epoch": 0.0005285274306578486,
      "grad_norm": 10356.276357842136,
      "learning_rate": 2.621738660256476e-07,
      "loss": 1.7459,
      "step": 145632
    },
    {
      "epoch": 0.0005286435650086852,
      "grad_norm": 9914.961522870373,
      "learning_rate": 2.621450378910239e-07,
      "loss": 1.7429,
      "step": 145664
    },
    {
      "epoch": 0.000528759699359522,
      "grad_norm": 9372.417510973355,
      "learning_rate": 2.6211621926396825e-07,
      "loss": 1.7542,
      "step": 145696
    },
    {
      "epoch": 0.0005288758337103586,
      "grad_norm": 8458.53462486263,
      "learning_rate": 2.6208741013925583e-07,
      "loss": 1.7529,
      "step": 145728
    },
    {
      "epoch": 0.0005289919680611954,
      "grad_norm": 8967.94959843107,
      "learning_rate": 2.620586105116657e-07,
      "loss": 1.7533,
      "step": 145760
    },
    {
      "epoch": 0.0005291081024120321,
      "grad_norm": 10673.073409285631,
      "learning_rate": 2.6202982037598104e-07,
      "loss": 1.7757,
      "step": 145792
    },
    {
      "epoch": 0.0005292242367628688,
      "grad_norm": 10727.577825399356,
      "learning_rate": 2.620010397269891e-07,
      "loss": 1.7784,
      "step": 145824
    },
    {
      "epoch": 0.0005293403711137055,
      "grad_norm": 9377.919598716977,
      "learning_rate": 2.6197226855948096e-07,
      "loss": 1.7653,
      "step": 145856
    },
    {
      "epoch": 0.0005294565054645422,
      "grad_norm": 8698.962007044289,
      "learning_rate": 2.6194350686825185e-07,
      "loss": 1.778,
      "step": 145888
    },
    {
      "epoch": 0.0005295726398153789,
      "grad_norm": 9715.923116204656,
      "learning_rate": 2.6191475464810094e-07,
      "loss": 1.7834,
      "step": 145920
    },
    {
      "epoch": 0.0005296887741662156,
      "grad_norm": 9634.670207121777,
      "learning_rate": 2.618860118938314e-07,
      "loss": 1.7475,
      "step": 145952
    },
    {
      "epoch": 0.0005298049085170523,
      "grad_norm": 9339.191185536358,
      "learning_rate": 2.6185727860025044e-07,
      "loss": 1.7278,
      "step": 145984
    },
    {
      "epoch": 0.000529921042867889,
      "grad_norm": 9784.643580631846,
      "learning_rate": 2.618285547621692e-07,
      "loss": 1.7449,
      "step": 146016
    },
    {
      "epoch": 0.0005300371772187257,
      "grad_norm": 9571.604149775521,
      "learning_rate": 2.617998403744028e-07,
      "loss": 1.7255,
      "step": 146048
    },
    {
      "epoch": 0.0005301533115695625,
      "grad_norm": 11661.315020185331,
      "learning_rate": 2.6177113543177037e-07,
      "loss": 1.7527,
      "step": 146080
    },
    {
      "epoch": 0.0005302694459203991,
      "grad_norm": 10903.73284705747,
      "learning_rate": 2.61742439929095e-07,
      "loss": 1.7671,
      "step": 146112
    },
    {
      "epoch": 0.0005303855802712359,
      "grad_norm": 10161.280824777947,
      "learning_rate": 2.6171375386120374e-07,
      "loss": 1.7551,
      "step": 146144
    },
    {
      "epoch": 0.0005305017146220725,
      "grad_norm": 8121.031215307573,
      "learning_rate": 2.616850772229277e-07,
      "loss": 1.7628,
      "step": 146176
    },
    {
      "epoch": 0.0005306178489729093,
      "grad_norm": 9648.778368270256,
      "learning_rate": 2.6165641000910177e-07,
      "loss": 1.7172,
      "step": 146208
    },
    {
      "epoch": 0.0005307339833237459,
      "grad_norm": 9562.665318832402,
      "learning_rate": 2.61627752214565e-07,
      "loss": 1.7278,
      "step": 146240
    },
    {
      "epoch": 0.0005308501176745827,
      "grad_norm": 10837.457635441995,
      "learning_rate": 2.615991038341602e-07,
      "loss": 1.7511,
      "step": 146272
    },
    {
      "epoch": 0.0005309662520254193,
      "grad_norm": 10088.607436113272,
      "learning_rate": 2.615713596882215e-07,
      "loss": 1.7558,
      "step": 146304
    },
    {
      "epoch": 0.0005310823863762561,
      "grad_norm": 10973.772368698013,
      "learning_rate": 2.6154272982683364e-07,
      "loss": 1.7416,
      "step": 146336
    },
    {
      "epoch": 0.0005311985207270928,
      "grad_norm": 9580.92918249582,
      "learning_rate": 2.615141093642909e-07,
      "loss": 1.7503,
      "step": 146368
    },
    {
      "epoch": 0.0005313146550779295,
      "grad_norm": 8194.445435781485,
      "learning_rate": 2.61485498295452e-07,
      "loss": 1.762,
      "step": 146400
    },
    {
      "epoch": 0.0005314307894287662,
      "grad_norm": 9097.338511894564,
      "learning_rate": 2.614568966151793e-07,
      "loss": 1.7663,
      "step": 146432
    },
    {
      "epoch": 0.0005315469237796029,
      "grad_norm": 9946.470630329131,
      "learning_rate": 2.614283043183394e-07,
      "loss": 1.7563,
      "step": 146464
    },
    {
      "epoch": 0.0005316630581304396,
      "grad_norm": 10165.075405524545,
      "learning_rate": 2.613997213998026e-07,
      "loss": 1.7641,
      "step": 146496
    },
    {
      "epoch": 0.0005317791924812763,
      "grad_norm": 9336.347572793122,
      "learning_rate": 2.613711478544432e-07,
      "loss": 1.7625,
      "step": 146528
    },
    {
      "epoch": 0.000531895326832113,
      "grad_norm": 10401.528926076204,
      "learning_rate": 2.6134258367713946e-07,
      "loss": 1.7559,
      "step": 146560
    },
    {
      "epoch": 0.0005320114611829497,
      "grad_norm": 10911.409258203086,
      "learning_rate": 2.6131402886277347e-07,
      "loss": 1.7643,
      "step": 146592
    },
    {
      "epoch": 0.0005321275955337864,
      "grad_norm": 8333.42330618096,
      "learning_rate": 2.612854834062313e-07,
      "loss": 1.7549,
      "step": 146624
    },
    {
      "epoch": 0.0005322437298846232,
      "grad_norm": 10311.29245051269,
      "learning_rate": 2.6125694730240287e-07,
      "loss": 1.7265,
      "step": 146656
    },
    {
      "epoch": 0.0005323598642354598,
      "grad_norm": 8605.059906822264,
      "learning_rate": 2.6122842054618204e-07,
      "loss": 1.7481,
      "step": 146688
    },
    {
      "epoch": 0.0005324759985862966,
      "grad_norm": 9938.222577503484,
      "learning_rate": 2.611999031324667e-07,
      "loss": 1.746,
      "step": 146720
    },
    {
      "epoch": 0.0005325921329371332,
      "grad_norm": 9722.852770663556,
      "learning_rate": 2.611713950561584e-07,
      "loss": 1.7546,
      "step": 146752
    },
    {
      "epoch": 0.00053270826728797,
      "grad_norm": 7625.40136648557,
      "learning_rate": 2.6114289631216276e-07,
      "loss": 1.7478,
      "step": 146784
    },
    {
      "epoch": 0.0005328244016388066,
      "grad_norm": 10139.880275427318,
      "learning_rate": 2.611144068953892e-07,
      "loss": 1.7549,
      "step": 146816
    },
    {
      "epoch": 0.0005329405359896434,
      "grad_norm": 10111.662276797026,
      "learning_rate": 2.610859268007511e-07,
      "loss": 1.7906,
      "step": 146848
    },
    {
      "epoch": 0.00053305667034048,
      "grad_norm": 9653.72156217487,
      "learning_rate": 2.610574560231656e-07,
      "loss": 1.7514,
      "step": 146880
    },
    {
      "epoch": 0.0005331728046913168,
      "grad_norm": 10310.189232017034,
      "learning_rate": 2.6102899455755383e-07,
      "loss": 1.7183,
      "step": 146912
    },
    {
      "epoch": 0.0005332889390421535,
      "grad_norm": 8996.654600461217,
      "learning_rate": 2.610005423988408e-07,
      "loss": 1.7406,
      "step": 146944
    },
    {
      "epoch": 0.0005334050733929902,
      "grad_norm": 9318.683598019627,
      "learning_rate": 2.609720995419553e-07,
      "loss": 1.7485,
      "step": 146976
    },
    {
      "epoch": 0.0005335212077438269,
      "grad_norm": 10218.59618538672,
      "learning_rate": 2.6094366598183015e-07,
      "loss": 1.7599,
      "step": 147008
    },
    {
      "epoch": 0.0005336373420946636,
      "grad_norm": 8853.02626224502,
      "learning_rate": 2.6091524171340183e-07,
      "loss": 1.7794,
      "step": 147040
    },
    {
      "epoch": 0.0005337534764455003,
      "grad_norm": 9626.554523815881,
      "learning_rate": 2.6088682673161076e-07,
      "loss": 1.7616,
      "step": 147072
    },
    {
      "epoch": 0.000533869610796337,
      "grad_norm": 10694.35252832073,
      "learning_rate": 2.608584210314013e-07,
      "loss": 1.7583,
      "step": 147104
    },
    {
      "epoch": 0.0005339857451471737,
      "grad_norm": 10179.602349797364,
      "learning_rate": 2.6083002460772165e-07,
      "loss": 1.7468,
      "step": 147136
    },
    {
      "epoch": 0.0005341018794980104,
      "grad_norm": 10146.785106623674,
      "learning_rate": 2.6080163745552364e-07,
      "loss": 1.7572,
      "step": 147168
    },
    {
      "epoch": 0.0005342180138488471,
      "grad_norm": 9963.808006982068,
      "learning_rate": 2.6077325956976326e-07,
      "loss": 1.7488,
      "step": 147200
    },
    {
      "epoch": 0.0005343341481996839,
      "grad_norm": 9034.387859727964,
      "learning_rate": 2.6074489094540004e-07,
      "loss": 1.7495,
      "step": 147232
    },
    {
      "epoch": 0.0005344502825505205,
      "grad_norm": 8818.617351943558,
      "learning_rate": 2.607165315773977e-07,
      "loss": 1.7582,
      "step": 147264
    },
    {
      "epoch": 0.0005345664169013573,
      "grad_norm": 9835.049262713432,
      "learning_rate": 2.606890672618847e-07,
      "loss": 1.752,
      "step": 147296
    },
    {
      "epoch": 0.0005346825512521939,
      "grad_norm": 9945.770759473597,
      "learning_rate": 2.6066072610263895e-07,
      "loss": 1.7357,
      "step": 147328
    },
    {
      "epoch": 0.0005347986856030307,
      "grad_norm": 9413.93148477298,
      "learning_rate": 2.606323941848244e-07,
      "loss": 1.7256,
      "step": 147360
    },
    {
      "epoch": 0.0005349148199538673,
      "grad_norm": 10493.5706982895,
      "learning_rate": 2.6060407150342e-07,
      "loss": 1.7199,
      "step": 147392
    },
    {
      "epoch": 0.0005350309543047041,
      "grad_norm": 8766.919071144663,
      "learning_rate": 2.60575758053408e-07,
      "loss": 1.7497,
      "step": 147424
    },
    {
      "epoch": 0.0005351470886555407,
      "grad_norm": 9861.89880296893,
      "learning_rate": 2.6054745382977493e-07,
      "loss": 1.7519,
      "step": 147456
    },
    {
      "epoch": 0.0005352632230063775,
      "grad_norm": 10697.856233844237,
      "learning_rate": 2.6051915882751087e-07,
      "loss": 1.7518,
      "step": 147488
    },
    {
      "epoch": 0.0005353793573572142,
      "grad_norm": 10373.767782247682,
      "learning_rate": 2.604908730416098e-07,
      "loss": 1.7703,
      "step": 147520
    },
    {
      "epoch": 0.0005354954917080509,
      "grad_norm": 9094.98026385984,
      "learning_rate": 2.604625964670695e-07,
      "loss": 1.7711,
      "step": 147552
    },
    {
      "epoch": 0.0005356116260588876,
      "grad_norm": 8929.904814722271,
      "learning_rate": 2.604343290988915e-07,
      "loss": 1.7302,
      "step": 147584
    },
    {
      "epoch": 0.0005357277604097243,
      "grad_norm": 9142.4795323807,
      "learning_rate": 2.604060709320812e-07,
      "loss": 1.7597,
      "step": 147616
    },
    {
      "epoch": 0.000535843894760561,
      "grad_norm": 9331.594933343387,
      "learning_rate": 2.6037782196164777e-07,
      "loss": 1.7878,
      "step": 147648
    },
    {
      "epoch": 0.0005359600291113977,
      "grad_norm": 9862.912957133913,
      "learning_rate": 2.6034958218260404e-07,
      "loss": 1.7786,
      "step": 147680
    },
    {
      "epoch": 0.0005360761634622344,
      "grad_norm": 9810.0540263548,
      "learning_rate": 2.6032135158996694e-07,
      "loss": 1.7608,
      "step": 147712
    },
    {
      "epoch": 0.0005361922978130711,
      "grad_norm": 9988.787514007894,
      "learning_rate": 2.6029313017875686e-07,
      "loss": 1.7551,
      "step": 147744
    },
    {
      "epoch": 0.0005363084321639078,
      "grad_norm": 9561.636993737004,
      "learning_rate": 2.60264917943998e-07,
      "loss": 1.749,
      "step": 147776
    },
    {
      "epoch": 0.0005364245665147446,
      "grad_norm": 9123.561037226638,
      "learning_rate": 2.6023671488071865e-07,
      "loss": 1.7371,
      "step": 147808
    },
    {
      "epoch": 0.0005365407008655812,
      "grad_norm": 10130.184401085698,
      "learning_rate": 2.602085209839505e-07,
      "loss": 1.7493,
      "step": 147840
    },
    {
      "epoch": 0.000536656835216418,
      "grad_norm": 9718.870304721635,
      "learning_rate": 2.601803362487291e-07,
      "loss": 1.7551,
      "step": 147872
    },
    {
      "epoch": 0.0005367729695672546,
      "grad_norm": 10677.680459725325,
      "learning_rate": 2.6015216067009397e-07,
      "loss": 1.7419,
      "step": 147904
    },
    {
      "epoch": 0.0005368891039180914,
      "grad_norm": 9766.325921245922,
      "learning_rate": 2.6012399424308814e-07,
      "loss": 1.765,
      "step": 147936
    },
    {
      "epoch": 0.000537005238268928,
      "grad_norm": 9735.201590105877,
      "learning_rate": 2.6009583696275855e-07,
      "loss": 1.7655,
      "step": 147968
    },
    {
      "epoch": 0.0005371213726197648,
      "grad_norm": 9710.183314438507,
      "learning_rate": 2.6006768882415575e-07,
      "loss": 1.7474,
      "step": 148000
    },
    {
      "epoch": 0.0005372375069706014,
      "grad_norm": 7641.604150962022,
      "learning_rate": 2.600395498223342e-07,
      "loss": 1.7228,
      "step": 148032
    },
    {
      "epoch": 0.0005373536413214382,
      "grad_norm": 9400.780818634163,
      "learning_rate": 2.60011419952352e-07,
      "loss": 1.7227,
      "step": 148064
    },
    {
      "epoch": 0.000537469775672275,
      "grad_norm": 9738.812350589777,
      "learning_rate": 2.5998329920927096e-07,
      "loss": 1.7326,
      "step": 148096
    },
    {
      "epoch": 0.0005375859100231116,
      "grad_norm": 8174.3175861964155,
      "learning_rate": 2.5995518758815677e-07,
      "loss": 1.7346,
      "step": 148128
    },
    {
      "epoch": 0.0005377020443739483,
      "grad_norm": 11425.36546461425,
      "learning_rate": 2.5992708508407875e-07,
      "loss": 1.7579,
      "step": 148160
    },
    {
      "epoch": 0.000537818178724785,
      "grad_norm": 9971.511018897787,
      "learning_rate": 2.5989899169210997e-07,
      "loss": 1.7602,
      "step": 148192
    },
    {
      "epoch": 0.0005379343130756217,
      "grad_norm": 9779.50162329349,
      "learning_rate": 2.598709074073272e-07,
      "loss": 1.7866,
      "step": 148224
    },
    {
      "epoch": 0.0005380504474264584,
      "grad_norm": 8789.45072231479,
      "learning_rate": 2.5984283222481105e-07,
      "loss": 1.7681,
      "step": 148256
    },
    {
      "epoch": 0.0005381665817772951,
      "grad_norm": 18500.76495715785,
      "learning_rate": 2.5981476613964564e-07,
      "loss": 1.7475,
      "step": 148288
    },
    {
      "epoch": 0.0005382827161281318,
      "grad_norm": 10221.009930530348,
      "learning_rate": 2.597875857903607e-07,
      "loss": 1.7427,
      "step": 148320
    },
    {
      "epoch": 0.0005383988504789685,
      "grad_norm": 10204.436780146174,
      "learning_rate": 2.597595376012534e-07,
      "loss": 1.7516,
      "step": 148352
    },
    {
      "epoch": 0.0005385149848298053,
      "grad_norm": 10392.013953031434,
      "learning_rate": 2.597314984949251e-07,
      "loss": 1.7565,
      "step": 148384
    },
    {
      "epoch": 0.0005386311191806419,
      "grad_norm": 10519.914828552557,
      "learning_rate": 2.5970346846647477e-07,
      "loss": 1.7507,
      "step": 148416
    },
    {
      "epoch": 0.0005387472535314787,
      "grad_norm": 10122.215864127775,
      "learning_rate": 2.5967544751100503e-07,
      "loss": 1.765,
      "step": 148448
    },
    {
      "epoch": 0.0005388633878823153,
      "grad_norm": 8703.413812981662,
      "learning_rate": 2.596474356236224e-07,
      "loss": 1.7427,
      "step": 148480
    },
    {
      "epoch": 0.0005389795222331521,
      "grad_norm": 9530.975186202091,
      "learning_rate": 2.5961943279943673e-07,
      "loss": 1.7525,
      "step": 148512
    },
    {
      "epoch": 0.0005390956565839887,
      "grad_norm": 9946.232553082598,
      "learning_rate": 2.595914390335619e-07,
      "loss": 1.7642,
      "step": 148544
    },
    {
      "epoch": 0.0005392117909348255,
      "grad_norm": 9536.772200278247,
      "learning_rate": 2.5956345432111534e-07,
      "loss": 1.7879,
      "step": 148576
    },
    {
      "epoch": 0.0005393279252856621,
      "grad_norm": 9334.019070046943,
      "learning_rate": 2.5953547865721817e-07,
      "loss": 1.7657,
      "step": 148608
    },
    {
      "epoch": 0.0005394440596364989,
      "grad_norm": 9123.994958350208,
      "learning_rate": 2.595075120369952e-07,
      "loss": 1.7485,
      "step": 148640
    },
    {
      "epoch": 0.0005395601939873357,
      "grad_norm": 8959.53960870758,
      "learning_rate": 2.594795544555749e-07,
      "loss": 1.7527,
      "step": 148672
    },
    {
      "epoch": 0.0005396763283381723,
      "grad_norm": 10666.681208323422,
      "learning_rate": 2.594516059080893e-07,
      "loss": 1.7137,
      "step": 148704
    },
    {
      "epoch": 0.000539792462689009,
      "grad_norm": 8814.070455810981,
      "learning_rate": 2.594236663896745e-07,
      "loss": 1.7248,
      "step": 148736
    },
    {
      "epoch": 0.0005399085970398457,
      "grad_norm": 9269.597833778982,
      "learning_rate": 2.593957358954697e-07,
      "loss": 1.7302,
      "step": 148768
    },
    {
      "epoch": 0.0005400247313906825,
      "grad_norm": 10415.029236636832,
      "learning_rate": 2.5936781442061823e-07,
      "loss": 1.7514,
      "step": 148800
    },
    {
      "epoch": 0.0005401408657415191,
      "grad_norm": 12488.516965596837,
      "learning_rate": 2.593399019602668e-07,
      "loss": 1.7687,
      "step": 148832
    },
    {
      "epoch": 0.0005402570000923559,
      "grad_norm": 10075.65948213813,
      "learning_rate": 2.593119985095659e-07,
      "loss": 1.8003,
      "step": 148864
    },
    {
      "epoch": 0.0005403731344431925,
      "grad_norm": 9407.779121556798,
      "learning_rate": 2.592841040636697e-07,
      "loss": 1.7747,
      "step": 148896
    },
    {
      "epoch": 0.0005404892687940293,
      "grad_norm": 9967.80677982875,
      "learning_rate": 2.5925621861773594e-07,
      "loss": 1.7311,
      "step": 148928
    },
    {
      "epoch": 0.000540605403144866,
      "grad_norm": 10099.818810255954,
      "learning_rate": 2.59228342166926e-07,
      "loss": 1.7194,
      "step": 148960
    },
    {
      "epoch": 0.0005407215374957027,
      "grad_norm": 9284.43557789056,
      "learning_rate": 2.592004747064049e-07,
      "loss": 1.7432,
      "step": 148992
    },
    {
      "epoch": 0.0005408376718465394,
      "grad_norm": 10081.175427498521,
      "learning_rate": 2.591726162313414e-07,
      "loss": 1.7458,
      "step": 149024
    },
    {
      "epoch": 0.000540953806197376,
      "grad_norm": 11057.302202617057,
      "learning_rate": 2.5914476673690777e-07,
      "loss": 1.74,
      "step": 149056
    },
    {
      "epoch": 0.0005410699405482128,
      "grad_norm": 8961.524870243902,
      "learning_rate": 2.5911692621828e-07,
      "loss": 1.7402,
      "step": 149088
    },
    {
      "epoch": 0.0005411860748990495,
      "grad_norm": 9485.029572963913,
      "learning_rate": 2.5908909467063756e-07,
      "loss": 1.7485,
      "step": 149120
    },
    {
      "epoch": 0.0005413022092498862,
      "grad_norm": 8758.11634999216,
      "learning_rate": 2.590612720891638e-07,
      "loss": 1.7566,
      "step": 149152
    },
    {
      "epoch": 0.0005414183436007229,
      "grad_norm": 9334.019284316912,
      "learning_rate": 2.5903345846904556e-07,
      "loss": 1.7644,
      "step": 149184
    },
    {
      "epoch": 0.0005415344779515596,
      "grad_norm": 9569.115319610271,
      "learning_rate": 2.590056538054731e-07,
      "loss": 1.76,
      "step": 149216
    },
    {
      "epoch": 0.0005416506123023964,
      "grad_norm": 9820.338588867493,
      "learning_rate": 2.5897785809364057e-07,
      "loss": 1.7573,
      "step": 149248
    },
    {
      "epoch": 0.000541766746653233,
      "grad_norm": 10640.23401998283,
      "learning_rate": 2.589500713287457e-07,
      "loss": 1.7593,
      "step": 149280
    },
    {
      "epoch": 0.0005418828810040698,
      "grad_norm": 9051.505620613623,
      "learning_rate": 2.589231614276437e-07,
      "loss": 1.7676,
      "step": 149312
    },
    {
      "epoch": 0.0005419990153549064,
      "grad_norm": 9941.303737438064,
      "learning_rate": 2.5889539226301197e-07,
      "loss": 1.7628,
      "step": 149344
    },
    {
      "epoch": 0.0005421151497057432,
      "grad_norm": 9658.933067373435,
      "learning_rate": 2.588676320310821e-07,
      "loss": 1.7448,
      "step": 149376
    },
    {
      "epoch": 0.0005422312840565798,
      "grad_norm": 9046.683591239389,
      "learning_rate": 2.58839880727066e-07,
      "loss": 1.7536,
      "step": 149408
    },
    {
      "epoch": 0.0005423474184074166,
      "grad_norm": 8999.286305035528,
      "learning_rate": 2.5881213834617935e-07,
      "loss": 1.763,
      "step": 149440
    },
    {
      "epoch": 0.0005424635527582532,
      "grad_norm": 12394.283521043079,
      "learning_rate": 2.587844048836411e-07,
      "loss": 1.7644,
      "step": 149472
    },
    {
      "epoch": 0.00054257968710909,
      "grad_norm": 10932.651645415215,
      "learning_rate": 2.5875668033467415e-07,
      "loss": 1.748,
      "step": 149504
    },
    {
      "epoch": 0.0005426958214599267,
      "grad_norm": 8083.922191609714,
      "learning_rate": 2.5872896469450475e-07,
      "loss": 1.736,
      "step": 149536
    },
    {
      "epoch": 0.0005428119558107634,
      "grad_norm": 9078.769520149743,
      "learning_rate": 2.587012579583627e-07,
      "loss": 1.7554,
      "step": 149568
    },
    {
      "epoch": 0.0005429280901616001,
      "grad_norm": 9060.356615498089,
      "learning_rate": 2.586735601214815e-07,
      "loss": 1.7654,
      "step": 149600
    },
    {
      "epoch": 0.0005430442245124368,
      "grad_norm": 8451.18630725888,
      "learning_rate": 2.5864587117909824e-07,
      "loss": 1.7229,
      "step": 149632
    },
    {
      "epoch": 0.0005431603588632735,
      "grad_norm": 10374.075380485723,
      "learning_rate": 2.586181911264534e-07,
      "loss": 1.7235,
      "step": 149664
    },
    {
      "epoch": 0.0005432764932141102,
      "grad_norm": 9402.940816574355,
      "learning_rate": 2.5859051995879123e-07,
      "loss": 1.7485,
      "step": 149696
    },
    {
      "epoch": 0.0005433926275649469,
      "grad_norm": 9716.385130283794,
      "learning_rate": 2.585628576713594e-07,
      "loss": 1.7562,
      "step": 149728
    },
    {
      "epoch": 0.0005435087619157836,
      "grad_norm": 9308.576260631913,
      "learning_rate": 2.5853520425940924e-07,
      "loss": 1.7419,
      "step": 149760
    },
    {
      "epoch": 0.0005436248962666203,
      "grad_norm": 10437.74726653218,
      "learning_rate": 2.585075597181955e-07,
      "loss": 1.7631,
      "step": 149792
    },
    {
      "epoch": 0.0005437410306174571,
      "grad_norm": 8621.502305282997,
      "learning_rate": 2.584799240429766e-07,
      "loss": 1.7476,
      "step": 149824
    },
    {
      "epoch": 0.0005438571649682937,
      "grad_norm": 8218.600975835243,
      "learning_rate": 2.5845229722901453e-07,
      "loss": 1.7355,
      "step": 149856
    },
    {
      "epoch": 0.0005439732993191305,
      "grad_norm": 9073.805596330572,
      "learning_rate": 2.5842467927157463e-07,
      "loss": 1.7398,
      "step": 149888
    },
    {
      "epoch": 0.0005440894336699671,
      "grad_norm": 9965.774430519687,
      "learning_rate": 2.583970701659261e-07,
      "loss": 1.7632,
      "step": 149920
    },
    {
      "epoch": 0.0005442055680208039,
      "grad_norm": 9516.823944993414,
      "learning_rate": 2.583694699073413e-07,
      "loss": 1.7463,
      "step": 149952
    },
    {
      "epoch": 0.0005443217023716405,
      "grad_norm": 10856.822002777793,
      "learning_rate": 2.583418784910964e-07,
      "loss": 1.7741,
      "step": 149984
    },
    {
      "epoch": 0.0005444378367224773,
      "grad_norm": 9277.75145172579,
      "learning_rate": 2.5831429591247106e-07,
      "loss": 1.7743,
      "step": 150016
    },
    {
      "epoch": 0.0005445539710733139,
      "grad_norm": 9642.557129724459,
      "learning_rate": 2.582867221667484e-07,
      "loss": 1.76,
      "step": 150048
    },
    {
      "epoch": 0.0005446701054241507,
      "grad_norm": 9115.000493691703,
      "learning_rate": 2.5825915724921504e-07,
      "loss": 1.7377,
      "step": 150080
    },
    {
      "epoch": 0.0005447862397749874,
      "grad_norm": 8888.425282354574,
      "learning_rate": 2.5823160115516123e-07,
      "loss": 1.7461,
      "step": 150112
    },
    {
      "epoch": 0.0005449023741258241,
      "grad_norm": 9940.17062227807,
      "learning_rate": 2.582040538798806e-07,
      "loss": 1.7361,
      "step": 150144
    },
    {
      "epoch": 0.0005450185084766608,
      "grad_norm": 9226.331448631141,
      "learning_rate": 2.581765154186704e-07,
      "loss": 1.7406,
      "step": 150176
    },
    {
      "epoch": 0.0005451346428274975,
      "grad_norm": 9730.60131749318,
      "learning_rate": 2.581489857668314e-07,
      "loss": 1.7691,
      "step": 150208
    },
    {
      "epoch": 0.0005452507771783342,
      "grad_norm": 8119.994458126188,
      "learning_rate": 2.581214649196678e-07,
      "loss": 1.7622,
      "step": 150240
    },
    {
      "epoch": 0.0005453669115291709,
      "grad_norm": 9561.754964440366,
      "learning_rate": 2.580939528724874e-07,
      "loss": 1.7711,
      "step": 150272
    },
    {
      "epoch": 0.0005454830458800076,
      "grad_norm": 9957.047353507967,
      "learning_rate": 2.5806730896413743e-07,
      "loss": 1.74,
      "step": 150304
    },
    {
      "epoch": 0.0005455991802308443,
      "grad_norm": 9015.936113349517,
      "learning_rate": 2.5803981422822484e-07,
      "loss": 1.7242,
      "step": 150336
    },
    {
      "epoch": 0.000545715314581681,
      "grad_norm": 11856.455456838692,
      "learning_rate": 2.5801232827838584e-07,
      "loss": 1.7479,
      "step": 150368
    },
    {
      "epoch": 0.0005458314489325178,
      "grad_norm": 9015.334048164827,
      "learning_rate": 2.579848511099421e-07,
      "loss": 1.758,
      "step": 150400
    },
    {
      "epoch": 0.0005459475832833544,
      "grad_norm": 9140.371436653983,
      "learning_rate": 2.5795738271821866e-07,
      "loss": 1.7422,
      "step": 150432
    },
    {
      "epoch": 0.0005460637176341912,
      "grad_norm": 11099.875584888328,
      "learning_rate": 2.579299230985441e-07,
      "loss": 1.7448,
      "step": 150464
    },
    {
      "epoch": 0.0005461798519850278,
      "grad_norm": 9605.752443197774,
      "learning_rate": 2.579024722462505e-07,
      "loss": 1.7379,
      "step": 150496
    },
    {
      "epoch": 0.0005462959863358646,
      "grad_norm": 10864.951357461294,
      "learning_rate": 2.5787503015667336e-07,
      "loss": 1.7537,
      "step": 150528
    },
    {
      "epoch": 0.0005464121206867012,
      "grad_norm": 8813.414434826038,
      "learning_rate": 2.5784759682515177e-07,
      "loss": 1.763,
      "step": 150560
    },
    {
      "epoch": 0.000546528255037538,
      "grad_norm": 9812.84331883476,
      "learning_rate": 2.578201722470281e-07,
      "loss": 1.7797,
      "step": 150592
    },
    {
      "epoch": 0.0005466443893883746,
      "grad_norm": 8938.639941288608,
      "learning_rate": 2.5779275641764847e-07,
      "loss": 1.7638,
      "step": 150624
    },
    {
      "epoch": 0.0005467605237392114,
      "grad_norm": 9363.868644956528,
      "learning_rate": 2.577653493323621e-07,
      "loss": 1.7505,
      "step": 150656
    },
    {
      "epoch": 0.0005468766580900481,
      "grad_norm": 9640.276759512664,
      "learning_rate": 2.5773795098652197e-07,
      "loss": 1.7568,
      "step": 150688
    },
    {
      "epoch": 0.0005469927924408848,
      "grad_norm": 11207.960831480452,
      "learning_rate": 2.5771056137548443e-07,
      "loss": 1.7693,
      "step": 150720
    },
    {
      "epoch": 0.0005471089267917215,
      "grad_norm": 10225.79053178775,
      "learning_rate": 2.576831804946092e-07,
      "loss": 1.7051,
      "step": 150752
    },
    {
      "epoch": 0.0005472250611425582,
      "grad_norm": 8842.185024076345,
      "learning_rate": 2.5765580833925954e-07,
      "loss": 1.7166,
      "step": 150784
    },
    {
      "epoch": 0.0005473411954933949,
      "grad_norm": 10358.357591819275,
      "learning_rate": 2.576284449048021e-07,
      "loss": 1.74,
      "step": 150816
    },
    {
      "epoch": 0.0005474573298442316,
      "grad_norm": 10846.566645717898,
      "learning_rate": 2.5760109018660704e-07,
      "loss": 1.7394,
      "step": 150848
    },
    {
      "epoch": 0.0005475734641950683,
      "grad_norm": 10675.751964147537,
      "learning_rate": 2.5757374418004794e-07,
      "loss": 1.752,
      "step": 150880
    },
    {
      "epoch": 0.000547689598545905,
      "grad_norm": 9190.103590275792,
      "learning_rate": 2.575464068805017e-07,
      "loss": 1.7601,
      "step": 150912
    },
    {
      "epoch": 0.0005478057328967417,
      "grad_norm": 10183.460315629458,
      "learning_rate": 2.5751907828334886e-07,
      "loss": 1.767,
      "step": 150944
    },
    {
      "epoch": 0.0005479218672475785,
      "grad_norm": 8980.220153203372,
      "learning_rate": 2.574917583839733e-07,
      "loss": 1.749,
      "step": 150976
    },
    {
      "epoch": 0.0005480380015984151,
      "grad_norm": 9605.208066460611,
      "learning_rate": 2.5746444717776227e-07,
      "loss": 1.7418,
      "step": 151008
    },
    {
      "epoch": 0.0005481541359492519,
      "grad_norm": 10252.620250453052,
      "learning_rate": 2.5743714466010646e-07,
      "loss": 1.7597,
      "step": 151040
    },
    {
      "epoch": 0.0005482702703000885,
      "grad_norm": 10071.948967305187,
      "learning_rate": 2.5740985082640003e-07,
      "loss": 1.7511,
      "step": 151072
    },
    {
      "epoch": 0.0005483864046509253,
      "grad_norm": 8269.769767049142,
      "learning_rate": 2.573825656720406e-07,
      "loss": 1.757,
      "step": 151104
    },
    {
      "epoch": 0.0005485025390017619,
      "grad_norm": 11299.347591785996,
      "learning_rate": 2.573552891924289e-07,
      "loss": 1.7728,
      "step": 151136
    },
    {
      "epoch": 0.0005486186733525987,
      "grad_norm": 9686.828583184488,
      "learning_rate": 2.573280213829697e-07,
      "loss": 1.7878,
      "step": 151168
    },
    {
      "epoch": 0.0005487348077034353,
      "grad_norm": 10727.253143279504,
      "learning_rate": 2.573007622390705e-07,
      "loss": 1.7565,
      "step": 151200
    },
    {
      "epoch": 0.0005488509420542721,
      "grad_norm": 10170.790726388976,
      "learning_rate": 2.5727351175614256e-07,
      "loss": 1.746,
      "step": 151232
    },
    {
      "epoch": 0.0005489670764051088,
      "grad_norm": 10623.171654454238,
      "learning_rate": 2.572462699296005e-07,
      "loss": 1.7379,
      "step": 151264
    },
    {
      "epoch": 0.0005490832107559455,
      "grad_norm": 9600.625604615565,
      "learning_rate": 2.5721903675486234e-07,
      "loss": 1.7552,
      "step": 151296
    },
    {
      "epoch": 0.0005491993451067822,
      "grad_norm": 9450.71320060026,
      "learning_rate": 2.57192662862989e-07,
      "loss": 1.7672,
      "step": 151328
    },
    {
      "epoch": 0.0005493154794576189,
      "grad_norm": 11632.632032347623,
      "learning_rate": 2.5716544670811247e-07,
      "loss": 1.7518,
      "step": 151360
    },
    {
      "epoch": 0.0005494316138084556,
      "grad_norm": 12153.322837808597,
      "learning_rate": 2.57138239191457e-07,
      "loss": 1.759,
      "step": 151392
    },
    {
      "epoch": 0.0005495477481592923,
      "grad_norm": 7858.580915152556,
      "learning_rate": 2.5711104030845395e-07,
      "loss": 1.7173,
      "step": 151424
    },
    {
      "epoch": 0.000549663882510129,
      "grad_norm": 10708.185653975186,
      "learning_rate": 2.570838500545382e-07,
      "loss": 1.7391,
      "step": 151456
    },
    {
      "epoch": 0.0005497800168609657,
      "grad_norm": 9146.373926316373,
      "learning_rate": 2.570566684251479e-07,
      "loss": 1.7153,
      "step": 151488
    },
    {
      "epoch": 0.0005498961512118024,
      "grad_norm": 9485.272900660264,
      "learning_rate": 2.5702949541572476e-07,
      "loss": 1.7357,
      "step": 151520
    },
    {
      "epoch": 0.0005500122855626392,
      "grad_norm": 9158.883556416687,
      "learning_rate": 2.570023310217136e-07,
      "loss": 1.7415,
      "step": 151552
    },
    {
      "epoch": 0.0005501284199134758,
      "grad_norm": 9245.702461143772,
      "learning_rate": 2.569751752385628e-07,
      "loss": 1.7439,
      "step": 151584
    },
    {
      "epoch": 0.0005502445542643126,
      "grad_norm": 9432.234941942444,
      "learning_rate": 2.569480280617241e-07,
      "loss": 1.7715,
      "step": 151616
    },
    {
      "epoch": 0.0005503606886151492,
      "grad_norm": 10081.307653275939,
      "learning_rate": 2.569208894866524e-07,
      "loss": 1.7722,
      "step": 151648
    },
    {
      "epoch": 0.000550476822965986,
      "grad_norm": 9766.950803602935,
      "learning_rate": 2.568937595088063e-07,
      "loss": 1.7631,
      "step": 151680
    },
    {
      "epoch": 0.0005505929573168226,
      "grad_norm": 10244.960126813574,
      "learning_rate": 2.568666381236474e-07,
      "loss": 1.7721,
      "step": 151712
    },
    {
      "epoch": 0.0005507090916676594,
      "grad_norm": 10561.824274243536,
      "learning_rate": 2.56839525326641e-07,
      "loss": 1.7892,
      "step": 151744
    },
    {
      "epoch": 0.000550825226018496,
      "grad_norm": 10728.245429705643,
      "learning_rate": 2.5681242111325543e-07,
      "loss": 1.7849,
      "step": 151776
    },
    {
      "epoch": 0.0005509413603693328,
      "grad_norm": 10351.664407234231,
      "learning_rate": 2.567853254789625e-07,
      "loss": 1.7862,
      "step": 151808
    },
    {
      "epoch": 0.0005510574947201695,
      "grad_norm": 9482.651106098969,
      "learning_rate": 2.5675823841923747e-07,
      "loss": 1.7654,
      "step": 151840
    },
    {
      "epoch": 0.0005511736290710062,
      "grad_norm": 9826.80599177576,
      "learning_rate": 2.567311599295588e-07,
      "loss": 1.7592,
      "step": 151872
    },
    {
      "epoch": 0.0005512897634218429,
      "grad_norm": 9614.908319895723,
      "learning_rate": 2.5670409000540833e-07,
      "loss": 1.7685,
      "step": 151904
    },
    {
      "epoch": 0.0005514058977726796,
      "grad_norm": 9816.441717852756,
      "learning_rate": 2.566770286422712e-07,
      "loss": 1.7842,
      "step": 151936
    },
    {
      "epoch": 0.0005515220321235163,
      "grad_norm": 10671.523602560226,
      "learning_rate": 2.56649975835636e-07,
      "loss": 1.7838,
      "step": 151968
    },
    {
      "epoch": 0.000551638166474353,
      "grad_norm": 9991.697253219796,
      "learning_rate": 2.566229315809944e-07,
      "loss": 1.7714,
      "step": 152000
    },
    {
      "epoch": 0.0005517543008251897,
      "grad_norm": 12902.39264632727,
      "learning_rate": 2.565958958738418e-07,
      "loss": 1.7711,
      "step": 152032
    },
    {
      "epoch": 0.0005518704351760264,
      "grad_norm": 8062.358587907139,
      "learning_rate": 2.565688687096765e-07,
      "loss": 1.7528,
      "step": 152064
    },
    {
      "epoch": 0.0005519865695268631,
      "grad_norm": 9434.738894108305,
      "learning_rate": 2.565418500840003e-07,
      "loss": 1.7556,
      "step": 152096
    },
    {
      "epoch": 0.0005521027038776999,
      "grad_norm": 11118.564835445266,
      "learning_rate": 2.5651483999231836e-07,
      "loss": 1.748,
      "step": 152128
    },
    {
      "epoch": 0.0005522188382285365,
      "grad_norm": 9985.243912894666,
      "learning_rate": 2.564878384301391e-07,
      "loss": 1.7501,
      "step": 152160
    },
    {
      "epoch": 0.0005523349725793733,
      "grad_norm": 9838.058751603387,
      "learning_rate": 2.564608453929742e-07,
      "loss": 1.753,
      "step": 152192
    },
    {
      "epoch": 0.0005524511069302099,
      "grad_norm": 10201.969025634218,
      "learning_rate": 2.564338608763388e-07,
      "loss": 1.7663,
      "step": 152224
    },
    {
      "epoch": 0.0005525672412810467,
      "grad_norm": 10122.404457439943,
      "learning_rate": 2.564068848757512e-07,
      "loss": 1.7842,
      "step": 152256
    },
    {
      "epoch": 0.0005526833756318833,
      "grad_norm": 10138.370480506224,
      "learning_rate": 2.5637991738673305e-07,
      "loss": 1.7668,
      "step": 152288
    },
    {
      "epoch": 0.0005527995099827201,
      "grad_norm": 9104.164761250755,
      "learning_rate": 2.563538007442693e-07,
      "loss": 1.7765,
      "step": 152320
    },
    {
      "epoch": 0.0005529156443335567,
      "grad_norm": 9166.127644758173,
      "learning_rate": 2.563268499993288e-07,
      "loss": 1.773,
      "step": 152352
    },
    {
      "epoch": 0.0005530317786843935,
      "grad_norm": 10321.458230308352,
      "learning_rate": 2.5629990775268197e-07,
      "loss": 1.7746,
      "step": 152384
    },
    {
      "epoch": 0.0005531479130352302,
      "grad_norm": 9269.285733000142,
      "learning_rate": 2.5627297399986357e-07,
      "loss": 1.7514,
      "step": 152416
    },
    {
      "epoch": 0.0005532640473860669,
      "grad_norm": 8601.60357142783,
      "learning_rate": 2.5624604873641155e-07,
      "loss": 1.7495,
      "step": 152448
    },
    {
      "epoch": 0.0005533801817369036,
      "grad_norm": 10351.111534516474,
      "learning_rate": 2.5621913195786723e-07,
      "loss": 1.7427,
      "step": 152480
    },
    {
      "epoch": 0.0005534963160877403,
      "grad_norm": 11730.448755269339,
      "learning_rate": 2.561922236597751e-07,
      "loss": 1.7559,
      "step": 152512
    },
    {
      "epoch": 0.000553612450438577,
      "grad_norm": 9937.978466468923,
      "learning_rate": 2.561653238376831e-07,
      "loss": 1.7835,
      "step": 152544
    },
    {
      "epoch": 0.0005537285847894137,
      "grad_norm": 10617.490287257155,
      "learning_rate": 2.5613843248714214e-07,
      "loss": 1.7792,
      "step": 152576
    },
    {
      "epoch": 0.0005538447191402504,
      "grad_norm": 9759.827764873722,
      "learning_rate": 2.5611154960370673e-07,
      "loss": 1.7491,
      "step": 152608
    },
    {
      "epoch": 0.0005539608534910871,
      "grad_norm": 8932.756461473693,
      "learning_rate": 2.560846751829344e-07,
      "loss": 1.7548,
      "step": 152640
    },
    {
      "epoch": 0.0005540769878419238,
      "grad_norm": 10264.278834871935,
      "learning_rate": 2.5605780922038614e-07,
      "loss": 1.7664,
      "step": 152672
    },
    {
      "epoch": 0.0005541931221927606,
      "grad_norm": 10234.532622450328,
      "learning_rate": 2.5603095171162605e-07,
      "loss": 1.7604,
      "step": 152704
    },
    {
      "epoch": 0.0005543092565435972,
      "grad_norm": 8643.86348804746,
      "learning_rate": 2.560041026522215e-07,
      "loss": 1.7575,
      "step": 152736
    },
    {
      "epoch": 0.000554425390894434,
      "grad_norm": 10483.994467758937,
      "learning_rate": 2.5597726203774317e-07,
      "loss": 1.7547,
      "step": 152768
    },
    {
      "epoch": 0.0005545415252452706,
      "grad_norm": 9185.380340519385,
      "learning_rate": 2.55950429863765e-07,
      "loss": 1.7575,
      "step": 152800
    },
    {
      "epoch": 0.0005546576595961074,
      "grad_norm": 11045.793045318203,
      "learning_rate": 2.559236061258641e-07,
      "loss": 1.7789,
      "step": 152832
    },
    {
      "epoch": 0.000554773793946944,
      "grad_norm": 11451.834787491478,
      "learning_rate": 2.558967908196209e-07,
      "loss": 1.7985,
      "step": 152864
    },
    {
      "epoch": 0.0005548899282977808,
      "grad_norm": 9987.898277415525,
      "learning_rate": 2.5586998394061904e-07,
      "loss": 1.7769,
      "step": 152896
    },
    {
      "epoch": 0.0005550060626486174,
      "grad_norm": 10520.479836965613,
      "learning_rate": 2.558431854844454e-07,
      "loss": 1.7694,
      "step": 152928
    },
    {
      "epoch": 0.0005551221969994542,
      "grad_norm": 9355.93918321405,
      "learning_rate": 2.5581639544669015e-07,
      "loss": 1.7707,
      "step": 152960
    },
    {
      "epoch": 0.000555238331350291,
      "grad_norm": 9467.16050355121,
      "learning_rate": 2.557896138229465e-07,
      "loss": 1.7643,
      "step": 152992
    },
    {
      "epoch": 0.0005553544657011276,
      "grad_norm": 11240.113878426677,
      "learning_rate": 2.557628406088111e-07,
      "loss": 1.7427,
      "step": 153024
    },
    {
      "epoch": 0.0005554706000519644,
      "grad_norm": 9296.465027094977,
      "learning_rate": 2.5573607579988375e-07,
      "loss": 1.7506,
      "step": 153056
    },
    {
      "epoch": 0.000555586734402801,
      "grad_norm": 9940.67070171827,
      "learning_rate": 2.557093193917675e-07,
      "loss": 1.7447,
      "step": 153088
    },
    {
      "epoch": 0.0005557028687536378,
      "grad_norm": 8288.118845673003,
      "learning_rate": 2.5568257138006853e-07,
      "loss": 1.7635,
      "step": 153120
    },
    {
      "epoch": 0.0005558190031044744,
      "grad_norm": 8831.605176863377,
      "learning_rate": 2.5565583176039637e-07,
      "loss": 1.7945,
      "step": 153152
    },
    {
      "epoch": 0.0005559351374553112,
      "grad_norm": 8813.98831403809,
      "learning_rate": 2.556291005283636e-07,
      "loss": 1.7704,
      "step": 153184
    },
    {
      "epoch": 0.0005560512718061478,
      "grad_norm": 9836.394766376552,
      "learning_rate": 2.556023776795862e-07,
      "loss": 1.7405,
      "step": 153216
    },
    {
      "epoch": 0.0005561674061569846,
      "grad_norm": 10975.239405133721,
      "learning_rate": 2.5557566320968317e-07,
      "loss": 1.7453,
      "step": 153248
    },
    {
      "epoch": 0.0005562835405078213,
      "grad_norm": 8562.227280328407,
      "learning_rate": 2.555489571142769e-07,
      "loss": 1.7634,
      "step": 153280
    },
    {
      "epoch": 0.000556399674858658,
      "grad_norm": 11376.29570642395,
      "learning_rate": 2.5552309356625513e-07,
      "loss": 1.7548,
      "step": 153312
    },
    {
      "epoch": 0.0005565158092094947,
      "grad_norm": 10297.691197545206,
      "learning_rate": 2.5549640394535834e-07,
      "loss": 1.7415,
      "step": 153344
    },
    {
      "epoch": 0.0005566319435603314,
      "grad_norm": 9381.415458234434,
      "learning_rate": 2.554697226859808e-07,
      "loss": 1.7468,
      "step": 153376
    },
    {
      "epoch": 0.0005567480779111681,
      "grad_norm": 8283.08227654416,
      "learning_rate": 2.554430497837575e-07,
      "loss": 1.7561,
      "step": 153408
    },
    {
      "epoch": 0.0005568642122620048,
      "grad_norm": 9329.61767705408,
      "learning_rate": 2.554163852343266e-07,
      "loss": 1.7775,
      "step": 153440
    },
    {
      "epoch": 0.0005569803466128415,
      "grad_norm": 8824.221212095717,
      "learning_rate": 2.553897290333294e-07,
      "loss": 1.7857,
      "step": 153472
    },
    {
      "epoch": 0.0005570964809636782,
      "grad_norm": 9036.980690474003,
      "learning_rate": 2.553630811764105e-07,
      "loss": 1.7772,
      "step": 153504
    },
    {
      "epoch": 0.0005572126153145149,
      "grad_norm": 9666.684333317191,
      "learning_rate": 2.553364416592176e-07,
      "loss": 1.7774,
      "step": 153536
    },
    {
      "epoch": 0.0005573287496653517,
      "grad_norm": 9977.896171037259,
      "learning_rate": 2.5530981047740167e-07,
      "loss": 1.7771,
      "step": 153568
    },
    {
      "epoch": 0.0005574448840161883,
      "grad_norm": 10554.81994161909,
      "learning_rate": 2.552831876266167e-07,
      "loss": 1.7653,
      "step": 153600
    },
    {
      "epoch": 0.0005575610183670251,
      "grad_norm": 9598.952755379098,
      "learning_rate": 2.5525657310252e-07,
      "loss": 1.7556,
      "step": 153632
    },
    {
      "epoch": 0.0005576771527178617,
      "grad_norm": 9349.142420564573,
      "learning_rate": 2.5522996690077187e-07,
      "loss": 1.7542,
      "step": 153664
    },
    {
      "epoch": 0.0005577932870686985,
      "grad_norm": 10026.343102048722,
      "learning_rate": 2.5520336901703606e-07,
      "loss": 1.756,
      "step": 153696
    },
    {
      "epoch": 0.0005579094214195351,
      "grad_norm": 9531.117563014319,
      "learning_rate": 2.551767794469792e-07,
      "loss": 1.7916,
      "step": 153728
    },
    {
      "epoch": 0.0005580255557703719,
      "grad_norm": 9859.723119844693,
      "learning_rate": 2.5515019818627116e-07,
      "loss": 1.7853,
      "step": 153760
    },
    {
      "epoch": 0.0005581416901212085,
      "grad_norm": 9092.911140003514,
      "learning_rate": 2.551236252305851e-07,
      "loss": 1.7634,
      "step": 153792
    },
    {
      "epoch": 0.0005582578244720453,
      "grad_norm": 9489.72307288258,
      "learning_rate": 2.5509706057559714e-07,
      "loss": 1.7419,
      "step": 153824
    },
    {
      "epoch": 0.000558373958822882,
      "grad_norm": 10387.222150315261,
      "learning_rate": 2.5507050421698676e-07,
      "loss": 1.7472,
      "step": 153856
    },
    {
      "epoch": 0.0005584900931737187,
      "grad_norm": 9826.88801198019,
      "learning_rate": 2.550439561504364e-07,
      "loss": 1.76,
      "step": 153888
    },
    {
      "epoch": 0.0005586062275245554,
      "grad_norm": 9316.328032009178,
      "learning_rate": 2.5501741637163164e-07,
      "loss": 1.7546,
      "step": 153920
    },
    {
      "epoch": 0.0005587223618753921,
      "grad_norm": 10845.976396802642,
      "learning_rate": 2.549908848762614e-07,
      "loss": 1.7413,
      "step": 153952
    },
    {
      "epoch": 0.0005588384962262288,
      "grad_norm": 9992.071056592822,
      "learning_rate": 2.549643616600176e-07,
      "loss": 1.7559,
      "step": 153984
    },
    {
      "epoch": 0.0005589546305770655,
      "grad_norm": 9726.776033198255,
      "learning_rate": 2.549378467185953e-07,
      "loss": 1.7704,
      "step": 154016
    },
    {
      "epoch": 0.0005590707649279022,
      "grad_norm": 10764.170567210462,
      "learning_rate": 2.5491134004769265e-07,
      "loss": 1.7748,
      "step": 154048
    },
    {
      "epoch": 0.0005591868992787389,
      "grad_norm": 9758.462481354323,
      "learning_rate": 2.548848416430111e-07,
      "loss": 1.7792,
      "step": 154080
    },
    {
      "epoch": 0.0005593030336295756,
      "grad_norm": 10770.932828682946,
      "learning_rate": 2.5485835150025495e-07,
      "loss": 1.773,
      "step": 154112
    },
    {
      "epoch": 0.0005594191679804124,
      "grad_norm": 9808.013356434625,
      "learning_rate": 2.5483186961513197e-07,
      "loss": 1.7667,
      "step": 154144
    },
    {
      "epoch": 0.000559535302331249,
      "grad_norm": 11233.638235229047,
      "learning_rate": 2.5480539598335275e-07,
      "loss": 1.7519,
      "step": 154176
    },
    {
      "epoch": 0.0005596514366820858,
      "grad_norm": 10904.887894884569,
      "learning_rate": 2.5477893060063116e-07,
      "loss": 1.7542,
      "step": 154208
    },
    {
      "epoch": 0.0005597675710329224,
      "grad_norm": 9608.018422130548,
      "learning_rate": 2.547524734626841e-07,
      "loss": 1.7489,
      "step": 154240
    },
    {
      "epoch": 0.0005598837053837592,
      "grad_norm": 10720.60091599347,
      "learning_rate": 2.5472602456523174e-07,
      "loss": 1.7438,
      "step": 154272
    },
    {
      "epoch": 0.0005599998397345958,
      "grad_norm": 8587.501965065278,
      "learning_rate": 2.546995839039972e-07,
      "loss": 1.7687,
      "step": 154304
    },
    {
      "epoch": 0.0005601159740854326,
      "grad_norm": 10158.511702016196,
      "learning_rate": 2.546739773635598e-07,
      "loss": 1.7694,
      "step": 154336
    },
    {
      "epoch": 0.0005602321084362692,
      "grad_norm": 9701.534105490739,
      "learning_rate": 2.546475529048925e-07,
      "loss": 1.7698,
      "step": 154368
    },
    {
      "epoch": 0.000560348242787106,
      "grad_norm": 10260.656801589263,
      "learning_rate": 2.546211366697645e-07,
      "loss": 1.7705,
      "step": 154400
    },
    {
      "epoch": 0.0005604643771379427,
      "grad_norm": 8839.991628955313,
      "learning_rate": 2.545947286539112e-07,
      "loss": 1.7524,
      "step": 154432
    },
    {
      "epoch": 0.0005605805114887794,
      "grad_norm": 8625.443061083877,
      "learning_rate": 2.545683288530712e-07,
      "loss": 1.7588,
      "step": 154464
    },
    {
      "epoch": 0.0005606966458396161,
      "grad_norm": 9040.160175572111,
      "learning_rate": 2.545419372629862e-07,
      "loss": 1.7698,
      "step": 154496
    },
    {
      "epoch": 0.0005608127801904528,
      "grad_norm": 9129.363395111402,
      "learning_rate": 2.545155538794009e-07,
      "loss": 1.7577,
      "step": 154528
    },
    {
      "epoch": 0.0005609289145412895,
      "grad_norm": 8845.628637920541,
      "learning_rate": 2.544891786980631e-07,
      "loss": 1.7579,
      "step": 154560
    },
    {
      "epoch": 0.0005610450488921262,
      "grad_norm": 10055.387809527785,
      "learning_rate": 2.5446281171472386e-07,
      "loss": 1.7789,
      "step": 154592
    },
    {
      "epoch": 0.0005611611832429629,
      "grad_norm": 10345.700169635693,
      "learning_rate": 2.544364529251371e-07,
      "loss": 1.7665,
      "step": 154624
    },
    {
      "epoch": 0.0005612773175937996,
      "grad_norm": 10126.860717912536,
      "learning_rate": 2.5441010232505997e-07,
      "loss": 1.7626,
      "step": 154656
    },
    {
      "epoch": 0.0005613934519446363,
      "grad_norm": 10290.162292209001,
      "learning_rate": 2.543837599102526e-07,
      "loss": 1.7837,
      "step": 154688
    },
    {
      "epoch": 0.0005615095862954731,
      "grad_norm": 10619.178781807941,
      "learning_rate": 2.543574256764782e-07,
      "loss": 1.7786,
      "step": 154720
    },
    {
      "epoch": 0.0005616257206463097,
      "grad_norm": 10043.363579996494,
      "learning_rate": 2.5433109961950305e-07,
      "loss": 1.7607,
      "step": 154752
    },
    {
      "epoch": 0.0005617418549971465,
      "grad_norm": 9945.13448878395,
      "learning_rate": 2.5430478173509667e-07,
      "loss": 1.7492,
      "step": 154784
    },
    {
      "epoch": 0.0005618579893479831,
      "grad_norm": 9433.820647012535,
      "learning_rate": 2.5427847201903134e-07,
      "loss": 1.7591,
      "step": 154816
    },
    {
      "epoch": 0.0005619741236988199,
      "grad_norm": 10939.14749877704,
      "learning_rate": 2.5425217046708263e-07,
      "loss": 1.7453,
      "step": 154848
    },
    {
      "epoch": 0.0005620902580496565,
      "grad_norm": 10721.565184244322,
      "learning_rate": 2.5422587707502906e-07,
      "loss": 1.7715,
      "step": 154880
    },
    {
      "epoch": 0.0005622063924004933,
      "grad_norm": 9486.524758835556,
      "learning_rate": 2.541995918386523e-07,
      "loss": 1.7701,
      "step": 154912
    },
    {
      "epoch": 0.0005623225267513299,
      "grad_norm": 11473.752481206835,
      "learning_rate": 2.5417331475373694e-07,
      "loss": 1.7483,
      "step": 154944
    },
    {
      "epoch": 0.0005624386611021667,
      "grad_norm": 10398.84935942434,
      "learning_rate": 2.541470458160707e-07,
      "loss": 1.7562,
      "step": 154976
    },
    {
      "epoch": 0.0005625547954530034,
      "grad_norm": 9597.88080776168,
      "learning_rate": 2.541207850214444e-07,
      "loss": 1.759,
      "step": 155008
    },
    {
      "epoch": 0.0005626709298038401,
      "grad_norm": 9887.797934828563,
      "learning_rate": 2.540945323656518e-07,
      "loss": 1.7528,
      "step": 155040
    },
    {
      "epoch": 0.0005627870641546768,
      "grad_norm": 9279.440069314527,
      "learning_rate": 2.540682878444897e-07,
      "loss": 1.7466,
      "step": 155072
    },
    {
      "epoch": 0.0005629031985055135,
      "grad_norm": 10958.883154774487,
      "learning_rate": 2.540420514537581e-07,
      "loss": 1.7495,
      "step": 155104
    },
    {
      "epoch": 0.0005630193328563502,
      "grad_norm": 10489.025788890025,
      "learning_rate": 2.540158231892598e-07,
      "loss": 1.7541,
      "step": 155136
    },
    {
      "epoch": 0.0005631354672071869,
      "grad_norm": 8960.358140163818,
      "learning_rate": 2.5398960304680085e-07,
      "loss": 1.7547,
      "step": 155168
    },
    {
      "epoch": 0.0005632516015580236,
      "grad_norm": 9641.976560850997,
      "learning_rate": 2.539633910221901e-07,
      "loss": 1.7675,
      "step": 155200
    },
    {
      "epoch": 0.0005633677359088603,
      "grad_norm": 10997.543543901065,
      "learning_rate": 2.539371871112397e-07,
      "loss": 1.7707,
      "step": 155232
    },
    {
      "epoch": 0.000563483870259697,
      "grad_norm": 10762.741100667618,
      "learning_rate": 2.539109913097646e-07,
      "loss": 1.7779,
      "step": 155264
    },
    {
      "epoch": 0.0005636000046105338,
      "grad_norm": 9808.320549411097,
      "learning_rate": 2.538848036135829e-07,
      "loss": 1.7872,
      "step": 155296
    },
    {
      "epoch": 0.0005637161389613704,
      "grad_norm": 10034.093481725193,
      "learning_rate": 2.5385944200827865e-07,
      "loss": 1.7898,
      "step": 155328
    },
    {
      "epoch": 0.0005638322733122072,
      "grad_norm": 9953.256552505818,
      "learning_rate": 2.538332702571837e-07,
      "loss": 1.7601,
      "step": 155360
    },
    {
      "epoch": 0.0005639484076630438,
      "grad_norm": 9944.571785652714,
      "learning_rate": 2.538071065989848e-07,
      "loss": 1.7568,
      "step": 155392
    },
    {
      "epoch": 0.0005640645420138806,
      "grad_norm": 9493.738357464883,
      "learning_rate": 2.5378095102951184e-07,
      "loss": 1.7677,
      "step": 155424
    },
    {
      "epoch": 0.0005641806763647172,
      "grad_norm": 10034.81041176165,
      "learning_rate": 2.53754803544598e-07,
      "loss": 1.761,
      "step": 155456
    },
    {
      "epoch": 0.000564296810715554,
      "grad_norm": 10735.613443115395,
      "learning_rate": 2.5372866414007914e-07,
      "loss": 1.7646,
      "step": 155488
    },
    {
      "epoch": 0.0005644129450663906,
      "grad_norm": 8803.173064299031,
      "learning_rate": 2.5370253281179434e-07,
      "loss": 1.7594,
      "step": 155520
    },
    {
      "epoch": 0.0005645290794172274,
      "grad_norm": 10223.614624974867,
      "learning_rate": 2.536764095555857e-07,
      "loss": 1.7454,
      "step": 155552
    },
    {
      "epoch": 0.0005646452137680641,
      "grad_norm": 10759.49757191292,
      "learning_rate": 2.536502943672982e-07,
      "loss": 1.7489,
      "step": 155584
    },
    {
      "epoch": 0.0005647613481189008,
      "grad_norm": 9928.999546782143,
      "learning_rate": 2.536241872427799e-07,
      "loss": 1.7666,
      "step": 155616
    },
    {
      "epoch": 0.0005648774824697375,
      "grad_norm": 8860.812942388526,
      "learning_rate": 2.535980881778817e-07,
      "loss": 1.7514,
      "step": 155648
    },
    {
      "epoch": 0.0005649936168205742,
      "grad_norm": 10446.1682927282,
      "learning_rate": 2.535719971684577e-07,
      "loss": 1.7544,
      "step": 155680
    },
    {
      "epoch": 0.0005651097511714109,
      "grad_norm": 10273.307354498842,
      "learning_rate": 2.5354591421036486e-07,
      "loss": 1.7507,
      "step": 155712
    },
    {
      "epoch": 0.0005652258855222476,
      "grad_norm": 9294.986820862094,
      "learning_rate": 2.535198392994631e-07,
      "loss": 1.7706,
      "step": 155744
    },
    {
      "epoch": 0.0005653420198730843,
      "grad_norm": 8883.33450906809,
      "learning_rate": 2.5349377243161543e-07,
      "loss": 1.7438,
      "step": 155776
    },
    {
      "epoch": 0.000565458154223921,
      "grad_norm": 10061.950009814202,
      "learning_rate": 2.5346771360268773e-07,
      "loss": 1.7597,
      "step": 155808
    },
    {
      "epoch": 0.0005655742885747577,
      "grad_norm": 10840.315678060302,
      "learning_rate": 2.534416628085489e-07,
      "loss": 1.7627,
      "step": 155840
    },
    {
      "epoch": 0.0005656904229255945,
      "grad_norm": 10030.244064827137,
      "learning_rate": 2.5341562004507083e-07,
      "loss": 1.7693,
      "step": 155872
    },
    {
      "epoch": 0.0005658065572764311,
      "grad_norm": 10427.078977355068,
      "learning_rate": 2.533895853081284e-07,
      "loss": 1.7697,
      "step": 155904
    },
    {
      "epoch": 0.0005659226916272679,
      "grad_norm": 10350.168501043836,
      "learning_rate": 2.533635585935993e-07,
      "loss": 1.7734,
      "step": 155936
    },
    {
      "epoch": 0.0005660388259781045,
      "grad_norm": 10134.57705086897,
      "learning_rate": 2.5333753989736437e-07,
      "loss": 1.7524,
      "step": 155968
    },
    {
      "epoch": 0.0005661549603289413,
      "grad_norm": 10190.69722835489,
      "learning_rate": 2.5331152921530733e-07,
      "loss": 1.7602,
      "step": 156000
    },
    {
      "epoch": 0.0005662710946797779,
      "grad_norm": 9839.501511763692,
      "learning_rate": 2.5328552654331487e-07,
      "loss": 1.7555,
      "step": 156032
    },
    {
      "epoch": 0.0005663872290306147,
      "grad_norm": 11994.789702199869,
      "learning_rate": 2.532595318772766e-07,
      "loss": 1.7595,
      "step": 156064
    },
    {
      "epoch": 0.0005665033633814513,
      "grad_norm": 9770.056294617754,
      "learning_rate": 2.5323354521308516e-07,
      "loss": 1.7717,
      "step": 156096
    },
    {
      "epoch": 0.0005666194977322881,
      "grad_norm": 9598.408618099149,
      "learning_rate": 2.5320756654663604e-07,
      "loss": 1.76,
      "step": 156128
    },
    {
      "epoch": 0.0005667356320831248,
      "grad_norm": 9341.898201115231,
      "learning_rate": 2.5318159587382773e-07,
      "loss": 1.7537,
      "step": 156160
    },
    {
      "epoch": 0.0005668517664339615,
      "grad_norm": 9016.13442668198,
      "learning_rate": 2.531556331905617e-07,
      "loss": 1.7549,
      "step": 156192
    },
    {
      "epoch": 0.0005669679007847982,
      "grad_norm": 8659.307362601237,
      "learning_rate": 2.5312967849274235e-07,
      "loss": 1.7776,
      "step": 156224
    },
    {
      "epoch": 0.0005670840351356349,
      "grad_norm": 10100.898177884974,
      "learning_rate": 2.531037317762769e-07,
      "loss": 1.7733,
      "step": 156256
    },
    {
      "epoch": 0.0005672001694864716,
      "grad_norm": 8941.070964934794,
      "learning_rate": 2.5307779303707567e-07,
      "loss": 1.7666,
      "step": 156288
    },
    {
      "epoch": 0.0005673163038373083,
      "grad_norm": 11445.31834419646,
      "learning_rate": 2.530526724868429e-07,
      "loss": 1.7732,
      "step": 156320
    },
    {
      "epoch": 0.000567432438188145,
      "grad_norm": 10627.904779400313,
      "learning_rate": 2.5302674944094015e-07,
      "loss": 1.7538,
      "step": 156352
    },
    {
      "epoch": 0.0005675485725389817,
      "grad_norm": 10274.41034804431,
      "learning_rate": 2.530008343601774e-07,
      "loss": 1.7522,
      "step": 156384
    },
    {
      "epoch": 0.0005676647068898184,
      "grad_norm": 9528.173172229815,
      "learning_rate": 2.5297492724047657e-07,
      "loss": 1.756,
      "step": 156416
    },
    {
      "epoch": 0.0005677808412406551,
      "grad_norm": 9665.28230317149,
      "learning_rate": 2.5294902807776246e-07,
      "loss": 1.7638,
      "step": 156448
    },
    {
      "epoch": 0.0005678969755914918,
      "grad_norm": 9742.681355766492,
      "learning_rate": 2.5292313686796267e-07,
      "loss": 1.7703,
      "step": 156480
    },
    {
      "epoch": 0.0005680131099423286,
      "grad_norm": 9564.659638481653,
      "learning_rate": 2.5289725360700804e-07,
      "loss": 1.7693,
      "step": 156512
    },
    {
      "epoch": 0.0005681292442931652,
      "grad_norm": 10567.019447318151,
      "learning_rate": 2.528713782908319e-07,
      "loss": 1.7715,
      "step": 156544
    },
    {
      "epoch": 0.000568245378644002,
      "grad_norm": 10294.241885636842,
      "learning_rate": 2.5284551091537083e-07,
      "loss": 1.7565,
      "step": 156576
    },
    {
      "epoch": 0.0005683615129948386,
      "grad_norm": 11051.907165733886,
      "learning_rate": 2.528196514765642e-07,
      "loss": 1.7678,
      "step": 156608
    },
    {
      "epoch": 0.0005684776473456754,
      "grad_norm": 9455.536684926985,
      "learning_rate": 2.5279379997035427e-07,
      "loss": 1.7438,
      "step": 156640
    },
    {
      "epoch": 0.000568593781696512,
      "grad_norm": 9653.507963429667,
      "learning_rate": 2.527679563926862e-07,
      "loss": 1.764,
      "step": 156672
    },
    {
      "epoch": 0.0005687099160473488,
      "grad_norm": 10962.703316244584,
      "learning_rate": 2.5274212073950814e-07,
      "loss": 1.7491,
      "step": 156704
    },
    {
      "epoch": 0.0005688260503981854,
      "grad_norm": 11042.891288064011,
      "learning_rate": 2.5271629300677097e-07,
      "loss": 1.7526,
      "step": 156736
    },
    {
      "epoch": 0.0005689421847490222,
      "grad_norm": 10323.061367637025,
      "learning_rate": 2.526904731904286e-07,
      "loss": 1.7317,
      "step": 156768
    },
    {
      "epoch": 0.000569058319099859,
      "grad_norm": 11586.14621002169,
      "learning_rate": 2.5266466128643786e-07,
      "loss": 1.7478,
      "step": 156800
    },
    {
      "epoch": 0.0005691744534506956,
      "grad_norm": 10156.927094352897,
      "learning_rate": 2.5263885729075835e-07,
      "loss": 1.7667,
      "step": 156832
    },
    {
      "epoch": 0.0005692905878015323,
      "grad_norm": 10070.111915962007,
      "learning_rate": 2.526130611993526e-07,
      "loss": 1.7735,
      "step": 156864
    },
    {
      "epoch": 0.000569406722152369,
      "grad_norm": 9536.681603157358,
      "learning_rate": 2.5258727300818607e-07,
      "loss": 1.7356,
      "step": 156896
    },
    {
      "epoch": 0.0005695228565032057,
      "grad_norm": 8897.209225369492,
      "learning_rate": 2.525614927132271e-07,
      "loss": 1.7376,
      "step": 156928
    },
    {
      "epoch": 0.0005696389908540424,
      "grad_norm": 7439.037975437415,
      "learning_rate": 2.5253572031044684e-07,
      "loss": 1.7496,
      "step": 156960
    },
    {
      "epoch": 0.0005697551252048791,
      "grad_norm": 9930.054078402594,
      "learning_rate": 2.5250995579581935e-07,
      "loss": 1.7565,
      "step": 156992
    },
    {
      "epoch": 0.0005698712595557158,
      "grad_norm": 9879.727627824564,
      "learning_rate": 2.5248419916532165e-07,
      "loss": 1.7582,
      "step": 157024
    },
    {
      "epoch": 0.0005699873939065525,
      "grad_norm": 9120.974399700945,
      "learning_rate": 2.5245845041493345e-07,
      "loss": 1.7516,
      "step": 157056
    },
    {
      "epoch": 0.0005701035282573893,
      "grad_norm": 9441.054602108812,
      "learning_rate": 2.5243270954063754e-07,
      "loss": 1.766,
      "step": 157088
    },
    {
      "epoch": 0.000570219662608226,
      "grad_norm": 10882.580208755642,
      "learning_rate": 2.524069765384194e-07,
      "loss": 1.777,
      "step": 157120
    },
    {
      "epoch": 0.0005703357969590627,
      "grad_norm": 9459.691538311385,
      "learning_rate": 2.523812514042675e-07,
      "loss": 1.7876,
      "step": 157152
    },
    {
      "epoch": 0.0005704519313098993,
      "grad_norm": 11154.206919364551,
      "learning_rate": 2.523555341341731e-07,
      "loss": 1.7817,
      "step": 157184
    },
    {
      "epoch": 0.0005705680656607361,
      "grad_norm": 8425.28907515938,
      "learning_rate": 2.523298247241304e-07,
      "loss": 1.7411,
      "step": 157216
    },
    {
      "epoch": 0.0005706842000115727,
      "grad_norm": 10538.973764081586,
      "learning_rate": 2.523041231701363e-07,
      "loss": 1.7244,
      "step": 157248
    },
    {
      "epoch": 0.0005708003343624095,
      "grad_norm": 10757.647605308513,
      "learning_rate": 2.5227842946819063e-07,
      "loss": 1.7321,
      "step": 157280
    },
    {
      "epoch": 0.0005709164687132461,
      "grad_norm": 8690.849210520224,
      "learning_rate": 2.522527436142962e-07,
      "loss": 1.7262,
      "step": 157312
    },
    {
      "epoch": 0.0005710326030640829,
      "grad_norm": 10047.246587996136,
      "learning_rate": 2.522278679235723e-07,
      "loss": 1.7489,
      "step": 157344
    },
    {
      "epoch": 0.0005711487374149197,
      "grad_norm": 10284.523518374588,
      "learning_rate": 2.52202197508858e-07,
      "loss": 1.7193,
      "step": 157376
    },
    {
      "epoch": 0.0005712648717657563,
      "grad_norm": 10384.687188355747,
      "learning_rate": 2.521765349303447e-07,
      "loss": 1.7274,
      "step": 157408
    },
    {
      "epoch": 0.0005713810061165931,
      "grad_norm": 9603.146880059681,
      "learning_rate": 2.5215088018404634e-07,
      "loss": 1.7767,
      "step": 157440
    },
    {
      "epoch": 0.0005714971404674297,
      "grad_norm": 7419.068809493548,
      "learning_rate": 2.5212523326597977e-07,
      "loss": 1.776,
      "step": 157472
    },
    {
      "epoch": 0.0005716132748182665,
      "grad_norm": 10671.39934591523,
      "learning_rate": 2.520995941721647e-07,
      "loss": 1.7449,
      "step": 157504
    },
    {
      "epoch": 0.0005717294091691031,
      "grad_norm": 8744.424280648784,
      "learning_rate": 2.5207396289862343e-07,
      "loss": 1.7469,
      "step": 157536
    },
    {
      "epoch": 0.0005718455435199399,
      "grad_norm": 9485.864430825479,
      "learning_rate": 2.520483394413815e-07,
      "loss": 1.7509,
      "step": 157568
    },
    {
      "epoch": 0.0005719616778707765,
      "grad_norm": 10419.694621244906,
      "learning_rate": 2.5202272379646697e-07,
      "loss": 1.7264,
      "step": 157600
    },
    {
      "epoch": 0.0005720778122216133,
      "grad_norm": 8897.237548812553,
      "learning_rate": 2.519971159599108e-07,
      "loss": 1.7394,
      "step": 157632
    },
    {
      "epoch": 0.00057219394657245,
      "grad_norm": 9860.135090352464,
      "learning_rate": 2.519715159277468e-07,
      "loss": 1.7601,
      "step": 157664
    },
    {
      "epoch": 0.0005723100809232867,
      "grad_norm": 9775.710306673373,
      "learning_rate": 2.5194592369601164e-07,
      "loss": 1.7587,
      "step": 157696
    },
    {
      "epoch": 0.0005724262152741234,
      "grad_norm": 9317.962867494161,
      "learning_rate": 2.5192033926074475e-07,
      "loss": 1.7335,
      "step": 157728
    },
    {
      "epoch": 0.00057254234962496,
      "grad_norm": 9345.965332698383,
      "learning_rate": 2.518947626179883e-07,
      "loss": 1.7544,
      "step": 157760
    },
    {
      "epoch": 0.0005726584839757968,
      "grad_norm": 9551.47046270887,
      "learning_rate": 2.5186919376378737e-07,
      "loss": 1.7611,
      "step": 157792
    },
    {
      "epoch": 0.0005727746183266335,
      "grad_norm": 9639.327673650274,
      "learning_rate": 2.5184363269418993e-07,
      "loss": 1.7581,
      "step": 157824
    },
    {
      "epoch": 0.0005728907526774702,
      "grad_norm": 9110.343791537178,
      "learning_rate": 2.5181807940524656e-07,
      "loss": 1.7648,
      "step": 157856
    },
    {
      "epoch": 0.0005730068870283069,
      "grad_norm": 8915.59240880829,
      "learning_rate": 2.517925338930108e-07,
      "loss": 1.7629,
      "step": 157888
    },
    {
      "epoch": 0.0005731230213791436,
      "grad_norm": 10204.33398120622,
      "learning_rate": 2.517669961535389e-07,
      "loss": 1.7329,
      "step": 157920
    },
    {
      "epoch": 0.0005732391557299804,
      "grad_norm": 9702.180991921352,
      "learning_rate": 2.5174146618288995e-07,
      "loss": 1.7249,
      "step": 157952
    },
    {
      "epoch": 0.000573355290080817,
      "grad_norm": 8202.02706652447,
      "learning_rate": 2.517159439771259e-07,
      "loss": 1.7382,
      "step": 157984
    },
    {
      "epoch": 0.0005734714244316538,
      "grad_norm": 9021.621472883908,
      "learning_rate": 2.516904295323113e-07,
      "loss": 1.7631,
      "step": 158016
    },
    {
      "epoch": 0.0005735875587824904,
      "grad_norm": 8068.7406700178435,
      "learning_rate": 2.5166492284451374e-07,
      "loss": 1.7612,
      "step": 158048
    },
    {
      "epoch": 0.0005737036931333272,
      "grad_norm": 8376.01910217497,
      "learning_rate": 2.516394239098034e-07,
      "loss": 1.7321,
      "step": 158080
    },
    {
      "epoch": 0.0005738198274841638,
      "grad_norm": 9856.306610490565,
      "learning_rate": 2.516139327242534e-07,
      "loss": 1.7351,
      "step": 158112
    },
    {
      "epoch": 0.0005739359618350006,
      "grad_norm": 9226.439616666876,
      "learning_rate": 2.5158844928393945e-07,
      "loss": 1.7473,
      "step": 158144
    },
    {
      "epoch": 0.0005740520961858372,
      "grad_norm": 9751.698313627221,
      "learning_rate": 2.5156297358494025e-07,
      "loss": 1.7559,
      "step": 158176
    },
    {
      "epoch": 0.000574168230536674,
      "grad_norm": 10229.823458887255,
      "learning_rate": 2.5153750562333715e-07,
      "loss": 1.7589,
      "step": 158208
    },
    {
      "epoch": 0.0005742843648875107,
      "grad_norm": 9259.433675986886,
      "learning_rate": 2.515120453952144e-07,
      "loss": 1.7614,
      "step": 158240
    },
    {
      "epoch": 0.0005744004992383474,
      "grad_norm": 8155.7864121125685,
      "learning_rate": 2.514865928966588e-07,
      "loss": 1.7382,
      "step": 158272
    },
    {
      "epoch": 0.0005745166335891841,
      "grad_norm": 10582.334997532445,
      "learning_rate": 2.514611481237602e-07,
      "loss": 1.7343,
      "step": 158304
    },
    {
      "epoch": 0.0005746327679400208,
      "grad_norm": 20955.428127337316,
      "learning_rate": 2.51435711072611e-07,
      "loss": 1.7474,
      "step": 158336
    },
    {
      "epoch": 0.0005747489022908575,
      "grad_norm": 11054.795430038495,
      "learning_rate": 2.5141107628918804e-07,
      "loss": 1.762,
      "step": 158368
    },
    {
      "epoch": 0.0005748650366416942,
      "grad_norm": 8924.975966354195,
      "learning_rate": 2.513856544288245e-07,
      "loss": 1.7341,
      "step": 158400
    },
    {
      "epoch": 0.0005749811709925309,
      "grad_norm": 9860.925717193086,
      "learning_rate": 2.5136024027862616e-07,
      "loss": 1.7198,
      "step": 158432
    },
    {
      "epoch": 0.0005750973053433676,
      "grad_norm": 8458.775916171322,
      "learning_rate": 2.513348338346965e-07,
      "loss": 1.7456,
      "step": 158464
    },
    {
      "epoch": 0.0005752134396942043,
      "grad_norm": 8616.05025519234,
      "learning_rate": 2.5130943509314176e-07,
      "loss": 1.7437,
      "step": 158496
    },
    {
      "epoch": 0.0005753295740450411,
      "grad_norm": 9046.70138779876,
      "learning_rate": 2.5128404405007083e-07,
      "loss": 1.7389,
      "step": 158528
    },
    {
      "epoch": 0.0005754457083958777,
      "grad_norm": 9777.212077069822,
      "learning_rate": 2.512586607015955e-07,
      "loss": 1.747,
      "step": 158560
    },
    {
      "epoch": 0.0005755618427467145,
      "grad_norm": 10406.427629114614,
      "learning_rate": 2.5123328504383016e-07,
      "loss": 1.7467,
      "step": 158592
    },
    {
      "epoch": 0.0005756779770975511,
      "grad_norm": 9094.35396276173,
      "learning_rate": 2.51207917072892e-07,
      "loss": 1.7326,
      "step": 158624
    },
    {
      "epoch": 0.0005757941114483879,
      "grad_norm": 8491.159520348207,
      "learning_rate": 2.51182556784901e-07,
      "loss": 1.7472,
      "step": 158656
    },
    {
      "epoch": 0.0005759102457992245,
      "grad_norm": 8833.79193778074,
      "learning_rate": 2.511572041759798e-07,
      "loss": 1.7666,
      "step": 158688
    },
    {
      "epoch": 0.0005760263801500613,
      "grad_norm": 11784.00746775052,
      "learning_rate": 2.511318592422538e-07,
      "loss": 1.7537,
      "step": 158720
    },
    {
      "epoch": 0.0005761425145008979,
      "grad_norm": 9109.30249799621,
      "learning_rate": 2.511065219798514e-07,
      "loss": 1.7318,
      "step": 158752
    },
    {
      "epoch": 0.0005762586488517347,
      "grad_norm": 9254.571302875136,
      "learning_rate": 2.5108119238490313e-07,
      "loss": 1.7504,
      "step": 158784
    },
    {
      "epoch": 0.0005763747832025714,
      "grad_norm": 8534.560328452779,
      "learning_rate": 2.5105587045354287e-07,
      "loss": 1.7779,
      "step": 158816
    },
    {
      "epoch": 0.0005764909175534081,
      "grad_norm": 9349.629404420262,
      "learning_rate": 2.5103055618190694e-07,
      "loss": 1.7722,
      "step": 158848
    },
    {
      "epoch": 0.0005766070519042448,
      "grad_norm": 9183.458063278778,
      "learning_rate": 2.5100524956613437e-07,
      "loss": 1.7714,
      "step": 158880
    },
    {
      "epoch": 0.0005767231862550815,
      "grad_norm": 9750.799556959419,
      "learning_rate": 2.509799506023669e-07,
      "loss": 1.7535,
      "step": 158912
    },
    {
      "epoch": 0.0005768393206059182,
      "grad_norm": 9284.65745194727,
      "learning_rate": 2.509546592867492e-07,
      "loss": 1.7297,
      "step": 158944
    },
    {
      "epoch": 0.0005769554549567549,
      "grad_norm": 9589.75755689371,
      "learning_rate": 2.5092937561542846e-07,
      "loss": 1.7386,
      "step": 158976
    },
    {
      "epoch": 0.0005770715893075916,
      "grad_norm": 10146.571243528526,
      "learning_rate": 2.5090409958455464e-07,
      "loss": 1.742,
      "step": 159008
    },
    {
      "epoch": 0.0005771877236584283,
      "grad_norm": 8398.333406099093,
      "learning_rate": 2.5087883119028037e-07,
      "loss": 1.7461,
      "step": 159040
    },
    {
      "epoch": 0.000577303858009265,
      "grad_norm": 10608.080222170267,
      "learning_rate": 2.508535704287611e-07,
      "loss": 1.7292,
      "step": 159072
    },
    {
      "epoch": 0.0005774199923601018,
      "grad_norm": 11528.29250149388,
      "learning_rate": 2.508283172961549e-07,
      "loss": 1.7255,
      "step": 159104
    },
    {
      "epoch": 0.0005775361267109384,
      "grad_norm": 9530.368513336722,
      "learning_rate": 2.508030717886226e-07,
      "loss": 1.7387,
      "step": 159136
    },
    {
      "epoch": 0.0005776522610617752,
      "grad_norm": 7705.919283252323,
      "learning_rate": 2.507778339023277e-07,
      "loss": 1.7571,
      "step": 159168
    },
    {
      "epoch": 0.0005777683954126118,
      "grad_norm": 9870.808274908393,
      "learning_rate": 2.507526036334364e-07,
      "loss": 1.7492,
      "step": 159200
    },
    {
      "epoch": 0.0005778845297634486,
      "grad_norm": 10010.40998161414,
      "learning_rate": 2.507273809781176e-07,
      "loss": 1.7404,
      "step": 159232
    },
    {
      "epoch": 0.0005780006641142852,
      "grad_norm": 10707.922300801403,
      "learning_rate": 2.5070216593254294e-07,
      "loss": 1.7547,
      "step": 159264
    },
    {
      "epoch": 0.000578116798465122,
      "grad_norm": 9682.480260759636,
      "learning_rate": 2.5067695849288667e-07,
      "loss": 1.732,
      "step": 159296
    },
    {
      "epoch": 0.0005782329328159586,
      "grad_norm": 10895.657667162639,
      "learning_rate": 2.5065175865532584e-07,
      "loss": 1.7289,
      "step": 159328
    },
    {
      "epoch": 0.0005783490671667954,
      "grad_norm": 9662.722804675708,
      "learning_rate": 2.506273535585428e-07,
      "loss": 1.7389,
      "step": 159360
    },
    {
      "epoch": 0.0005784652015176321,
      "grad_norm": 10719.645143380447,
      "learning_rate": 2.5060216867644545e-07,
      "loss": 1.7572,
      "step": 159392
    },
    {
      "epoch": 0.0005785813358684688,
      "grad_norm": 10278.879316345729,
      "learning_rate": 2.505769913851099e-07,
      "loss": 1.75,
      "step": 159424
    },
    {
      "epoch": 0.0005786974702193055,
      "grad_norm": 9733.342694059427,
      "learning_rate": 2.5055182168072366e-07,
      "loss": 1.7451,
      "step": 159456
    },
    {
      "epoch": 0.0005788136045701422,
      "grad_norm": 9362.437609938985,
      "learning_rate": 2.5052665955947713e-07,
      "loss": 1.7506,
      "step": 159488
    },
    {
      "epoch": 0.0005789297389209789,
      "grad_norm": 9660.873666496214,
      "learning_rate": 2.505015050175632e-07,
      "loss": 1.7536,
      "step": 159520
    },
    {
      "epoch": 0.0005790458732718156,
      "grad_norm": 9861.976069733691,
      "learning_rate": 2.504763580511778e-07,
      "loss": 1.7562,
      "step": 159552
    },
    {
      "epoch": 0.0005791620076226523,
      "grad_norm": 10018.230682111487,
      "learning_rate": 2.504512186565191e-07,
      "loss": 1.768,
      "step": 159584
    },
    {
      "epoch": 0.000579278141973489,
      "grad_norm": 10990.828540196595,
      "learning_rate": 2.5042608682978814e-07,
      "loss": 1.7648,
      "step": 159616
    },
    {
      "epoch": 0.0005793942763243257,
      "grad_norm": 9303.485583371428,
      "learning_rate": 2.5040096256718873e-07,
      "loss": 1.7301,
      "step": 159648
    },
    {
      "epoch": 0.0005795104106751625,
      "grad_norm": 10136.546749263282,
      "learning_rate": 2.503758458649271e-07,
      "loss": 1.7358,
      "step": 159680
    },
    {
      "epoch": 0.0005796265450259991,
      "grad_norm": 10271.569695036878,
      "learning_rate": 2.503507367192124e-07,
      "loss": 1.7563,
      "step": 159712
    },
    {
      "epoch": 0.0005797426793768359,
      "grad_norm": 9812.459019022703,
      "learning_rate": 2.503256351262562e-07,
      "loss": 1.7555,
      "step": 159744
    },
    {
      "epoch": 0.0005798588137276725,
      "grad_norm": 9857.40361352826,
      "learning_rate": 2.503005410822729e-07,
      "loss": 1.7242,
      "step": 159776
    },
    {
      "epoch": 0.0005799749480785093,
      "grad_norm": 10328.851242998904,
      "learning_rate": 2.5027545458347956e-07,
      "loss": 1.7306,
      "step": 159808
    },
    {
      "epoch": 0.0005800910824293459,
      "grad_norm": 8717.288913417979,
      "learning_rate": 2.502503756260957e-07,
      "loss": 1.7406,
      "step": 159840
    },
    {
      "epoch": 0.0005802072167801827,
      "grad_norm": 9611.391262455192,
      "learning_rate": 2.502253042063438e-07,
      "loss": 1.7495,
      "step": 159872
    },
    {
      "epoch": 0.0005803233511310193,
      "grad_norm": 9957.516959563765,
      "learning_rate": 2.5020024032044863e-07,
      "loss": 1.7696,
      "step": 159904
    },
    {
      "epoch": 0.0005804394854818561,
      "grad_norm": 9157.106748313028,
      "learning_rate": 2.5017518396463794e-07,
      "loss": 1.7604,
      "step": 159936
    },
    {
      "epoch": 0.0005805556198326928,
      "grad_norm": 9638.245690995846,
      "learning_rate": 2.501501351351419e-07,
      "loss": 1.7232,
      "step": 159968
    },
    {
      "epoch": 0.0005806717541835295,
      "grad_norm": 10145.840921283952,
      "learning_rate": 2.501250938281934e-07,
      "loss": 1.7425,
      "step": 160000
    },
    {
      "epoch": 0.0005807878885343662,
      "grad_norm": 9897.459472005936,
      "learning_rate": 2.50100060040028e-07,
      "loss": 1.7722,
      "step": 160032
    },
    {
      "epoch": 0.0005809040228852029,
      "grad_norm": 9166.148809614646,
      "learning_rate": 2.500750337668839e-07,
      "loss": 1.7453,
      "step": 160064
    },
    {
      "epoch": 0.0005810201572360396,
      "grad_norm": 10226.204770099219,
      "learning_rate": 2.500500150050018e-07,
      "loss": 1.728,
      "step": 160096
    },
    {
      "epoch": 0.0005811362915868763,
      "grad_norm": 8659.31810248359,
      "learning_rate": 2.5002500375062514e-07,
      "loss": 1.718,
      "step": 160128
    },
    {
      "epoch": 0.000581252425937713,
      "grad_norm": 9903.516547166466,
      "learning_rate": 2.5e-07,
      "loss": 1.7386,
      "step": 160160
    },
    {
      "epoch": 0.0005813685602885497,
      "grad_norm": 9511.613217535709,
      "learning_rate": 2.4997500374937513e-07,
      "loss": 1.7555,
      "step": 160192
    },
    {
      "epoch": 0.0005814846946393864,
      "grad_norm": 9291.132008533728,
      "learning_rate": 2.4995001499500175e-07,
      "loss": 1.7658,
      "step": 160224
    },
    {
      "epoch": 0.0005816008289902232,
      "grad_norm": 9356.964571911129,
      "learning_rate": 2.4992503373313385e-07,
      "loss": 1.7427,
      "step": 160256
    },
    {
      "epoch": 0.0005817169633410598,
      "grad_norm": 9376.149636177955,
      "learning_rate": 2.4990005996002797e-07,
      "loss": 1.7425,
      "step": 160288
    },
    {
      "epoch": 0.0005818330976918966,
      "grad_norm": 9043.075472426402,
      "learning_rate": 2.498750936719433e-07,
      "loss": 1.7278,
      "step": 160320
    },
    {
      "epoch": 0.0005819492320427332,
      "grad_norm": 9942.066284228848,
      "learning_rate": 2.498501348651416e-07,
      "loss": 1.7318,
      "step": 160352
    },
    {
      "epoch": 0.00058206536639357,
      "grad_norm": 8882.28292726594,
      "learning_rate": 2.49825963151778e-07,
      "loss": 1.7501,
      "step": 160384
    },
    {
      "epoch": 0.0005821815007444066,
      "grad_norm": 8943.004528680503,
      "learning_rate": 2.498010190628379e-07,
      "loss": 1.7544,
      "step": 160416
    },
    {
      "epoch": 0.0005822976350952434,
      "grad_norm": 10867.642798693745,
      "learning_rate": 2.497760824440983e-07,
      "loss": 1.7285,
      "step": 160448
    },
    {
      "epoch": 0.00058241376944608,
      "grad_norm": 9794.821897308802,
      "learning_rate": 2.497511532918314e-07,
      "loss": 1.7366,
      "step": 160480
    },
    {
      "epoch": 0.0005825299037969168,
      "grad_norm": 8319.184094609278,
      "learning_rate": 2.49726231602312e-07,
      "loss": 1.7771,
      "step": 160512
    },
    {
      "epoch": 0.0005826460381477535,
      "grad_norm": 8854.077591708805,
      "learning_rate": 2.497013173718173e-07,
      "loss": 1.7688,
      "step": 160544
    },
    {
      "epoch": 0.0005827621724985902,
      "grad_norm": 9617.161535505162,
      "learning_rate": 2.496764105966274e-07,
      "loss": 1.77,
      "step": 160576
    },
    {
      "epoch": 0.0005828783068494269,
      "grad_norm": 9631.84509842221,
      "learning_rate": 2.496515112730248e-07,
      "loss": 1.7838,
      "step": 160608
    },
    {
      "epoch": 0.0005829944412002636,
      "grad_norm": 8829.51833340868,
      "learning_rate": 2.496266193972946e-07,
      "loss": 1.743,
      "step": 160640
    },
    {
      "epoch": 0.0005831105755511003,
      "grad_norm": 9821.067355435456,
      "learning_rate": 2.496017349657246e-07,
      "loss": 1.7175,
      "step": 160672
    },
    {
      "epoch": 0.000583226709901937,
      "grad_norm": 8402.449999851235,
      "learning_rate": 2.495768579746051e-07,
      "loss": 1.7213,
      "step": 160704
    },
    {
      "epoch": 0.0005833428442527737,
      "grad_norm": 10326.12202135923,
      "learning_rate": 2.49551988420229e-07,
      "loss": 1.7396,
      "step": 160736
    },
    {
      "epoch": 0.0005834589786036104,
      "grad_norm": 9560.923386368077,
      "learning_rate": 2.495271262988918e-07,
      "loss": 1.7405,
      "step": 160768
    },
    {
      "epoch": 0.0005835751129544471,
      "grad_norm": 9207.942006767853,
      "learning_rate": 2.495022716068916e-07,
      "loss": 1.7333,
      "step": 160800
    },
    {
      "epoch": 0.0005836912473052839,
      "grad_norm": 9741.325782458976,
      "learning_rate": 2.4947742434052906e-07,
      "loss": 1.7547,
      "step": 160832
    },
    {
      "epoch": 0.0005838073816561205,
      "grad_norm": 9947.865097597574,
      "learning_rate": 2.4945258449610736e-07,
      "loss": 1.751,
      "step": 160864
    },
    {
      "epoch": 0.0005839235160069573,
      "grad_norm": 9243.84671010938,
      "learning_rate": 2.494277520699324e-07,
      "loss": 1.755,
      "step": 160896
    },
    {
      "epoch": 0.0005840396503577939,
      "grad_norm": 7971.118491152919,
      "learning_rate": 2.494029270583125e-07,
      "loss": 1.7391,
      "step": 160928
    },
    {
      "epoch": 0.0005841557847086307,
      "grad_norm": 9671.764575298552,
      "learning_rate": 2.4937810945755867e-07,
      "loss": 1.7475,
      "step": 160960
    },
    {
      "epoch": 0.0005842719190594673,
      "grad_norm": 10650.368068757061,
      "learning_rate": 2.493532992639844e-07,
      "loss": 1.7129,
      "step": 160992
    },
    {
      "epoch": 0.0005843880534103041,
      "grad_norm": 10259.210690886506,
      "learning_rate": 2.493284964739058e-07,
      "loss": 1.7219,
      "step": 161024
    },
    {
      "epoch": 0.0005845041877611407,
      "grad_norm": 10064.626769036196,
      "learning_rate": 2.493037010836415e-07,
      "loss": 1.7275,
      "step": 161056
    },
    {
      "epoch": 0.0005846203221119775,
      "grad_norm": 8991.415350210444,
      "learning_rate": 2.492789130895128e-07,
      "loss": 1.7494,
      "step": 161088
    },
    {
      "epoch": 0.0005847364564628142,
      "grad_norm": 11776.593565203819,
      "learning_rate": 2.492541324878435e-07,
      "loss": 1.7514,
      "step": 161120
    },
    {
      "epoch": 0.0005848525908136509,
      "grad_norm": 9940.990695096742,
      "learning_rate": 2.492293592749597e-07,
      "loss": 1.7491,
      "step": 161152
    },
    {
      "epoch": 0.0005849687251644876,
      "grad_norm": 9695.905940137827,
      "learning_rate": 2.4920459344719064e-07,
      "loss": 1.7704,
      "step": 161184
    },
    {
      "epoch": 0.0005850848595153243,
      "grad_norm": 8089.4821836752935,
      "learning_rate": 2.4917983500086757e-07,
      "loss": 1.7712,
      "step": 161216
    },
    {
      "epoch": 0.000585200993866161,
      "grad_norm": 8631.334890965592,
      "learning_rate": 2.491550839323245e-07,
      "loss": 1.7636,
      "step": 161248
    },
    {
      "epoch": 0.0005853171282169977,
      "grad_norm": 10119.065371861177,
      "learning_rate": 2.4913034023789796e-07,
      "loss": 1.7457,
      "step": 161280
    },
    {
      "epoch": 0.0005854332625678344,
      "grad_norm": 10400.99091433119,
      "learning_rate": 2.491056039139272e-07,
      "loss": 1.7378,
      "step": 161312
    },
    {
      "epoch": 0.0005855493969186711,
      "grad_norm": 10956.73363735744,
      "learning_rate": 2.490808749567536e-07,
      "loss": 1.7254,
      "step": 161344
    },
    {
      "epoch": 0.0005856655312695078,
      "grad_norm": 8768.905518934504,
      "learning_rate": 2.490569258011176e-07,
      "loss": 1.738,
      "step": 161376
    },
    {
      "epoch": 0.0005857816656203446,
      "grad_norm": 10386.191987441787,
      "learning_rate": 2.49032211336645e-07,
      "loss": 1.7599,
      "step": 161408
    },
    {
      "epoch": 0.0005858977999711812,
      "grad_norm": 8860.688009404235,
      "learning_rate": 2.4900750422812395e-07,
      "loss": 1.7715,
      "step": 161440
    },
    {
      "epoch": 0.000586013934322018,
      "grad_norm": 10909.891841810348,
      "learning_rate": 2.4898280447190615e-07,
      "loss": 1.7477,
      "step": 161472
    },
    {
      "epoch": 0.0005861300686728546,
      "grad_norm": 9239.395759463927,
      "learning_rate": 2.4895811206434577e-07,
      "loss": 1.7201,
      "step": 161504
    },
    {
      "epoch": 0.0005862462030236914,
      "grad_norm": 11304.355797656053,
      "learning_rate": 2.489334270017996e-07,
      "loss": 1.7316,
      "step": 161536
    },
    {
      "epoch": 0.000586362337374528,
      "grad_norm": 8704.668862168164,
      "learning_rate": 2.4890874928062703e-07,
      "loss": 1.738,
      "step": 161568
    },
    {
      "epoch": 0.0005864784717253648,
      "grad_norm": 10042.428391579399,
      "learning_rate": 2.4888407889718975e-07,
      "loss": 1.7353,
      "step": 161600
    },
    {
      "epoch": 0.0005865946060762014,
      "grad_norm": 9758.453975912373,
      "learning_rate": 2.488594158478523e-07,
      "loss": 1.7422,
      "step": 161632
    },
    {
      "epoch": 0.0005867107404270382,
      "grad_norm": 9384.13522920466,
      "learning_rate": 2.4883476012898135e-07,
      "loss": 1.7116,
      "step": 161664
    },
    {
      "epoch": 0.000586826874777875,
      "grad_norm": 9446.749070447462,
      "learning_rate": 2.4881011173694645e-07,
      "loss": 1.7345,
      "step": 161696
    },
    {
      "epoch": 0.0005869430091287116,
      "grad_norm": 9160.532080616278,
      "learning_rate": 2.487854706681195e-07,
      "loss": 1.7549,
      "step": 161728
    },
    {
      "epoch": 0.0005870591434795484,
      "grad_norm": 8771.290783003376,
      "learning_rate": 2.487608369188748e-07,
      "loss": 1.77,
      "step": 161760
    },
    {
      "epoch": 0.000587175277830385,
      "grad_norm": 10354.274093339427,
      "learning_rate": 2.487362104855894e-07,
      "loss": 1.7405,
      "step": 161792
    },
    {
      "epoch": 0.0005872914121812218,
      "grad_norm": 8917.455915226046,
      "learning_rate": 2.4871159136464266e-07,
      "loss": 1.7194,
      "step": 161824
    },
    {
      "epoch": 0.0005874075465320584,
      "grad_norm": 8719.223474599099,
      "learning_rate": 2.4868697955241666e-07,
      "loss": 1.7216,
      "step": 161856
    },
    {
      "epoch": 0.0005875236808828952,
      "grad_norm": 9271.012673920794,
      "learning_rate": 2.4866237504529577e-07,
      "loss": 1.74,
      "step": 161888
    },
    {
      "epoch": 0.0005876398152337318,
      "grad_norm": 8711.445689436398,
      "learning_rate": 2.48637777839667e-07,
      "loss": 1.726,
      "step": 161920
    },
    {
      "epoch": 0.0005877559495845686,
      "grad_norm": 8783.281049812764,
      "learning_rate": 2.4861318793191975e-07,
      "loss": 1.7398,
      "step": 161952
    },
    {
      "epoch": 0.0005878720839354053,
      "grad_norm": 10321.480320186634,
      "learning_rate": 2.4858860531844606e-07,
      "loss": 1.7553,
      "step": 161984
    },
    {
      "epoch": 0.000587988218286242,
      "grad_norm": 9570.709064640927,
      "learning_rate": 2.485640299956403e-07,
      "loss": 1.7149,
      "step": 162016
    },
    {
      "epoch": 0.0005881043526370787,
      "grad_norm": 9868.388115594156,
      "learning_rate": 2.4853946195989954e-07,
      "loss": 1.7263,
      "step": 162048
    },
    {
      "epoch": 0.0005882204869879154,
      "grad_norm": 9145.359697682754,
      "learning_rate": 2.485149012076232e-07,
      "loss": 1.735,
      "step": 162080
    },
    {
      "epoch": 0.0005883366213387521,
      "grad_norm": 9338.720362019627,
      "learning_rate": 2.484903477352132e-07,
      "loss": 1.7512,
      "step": 162112
    },
    {
      "epoch": 0.0005884527556895888,
      "grad_norm": 9571.296568386124,
      "learning_rate": 2.484658015390739e-07,
      "loss": 1.7367,
      "step": 162144
    },
    {
      "epoch": 0.0005885688900404255,
      "grad_norm": 10606.55797136847,
      "learning_rate": 2.484412626156123e-07,
      "loss": 1.7323,
      "step": 162176
    },
    {
      "epoch": 0.0005886850243912622,
      "grad_norm": 10144.238759019821,
      "learning_rate": 2.4841673096123776e-07,
      "loss": 1.7688,
      "step": 162208
    },
    {
      "epoch": 0.0005888011587420989,
      "grad_norm": 10150.754356204272,
      "learning_rate": 2.4839220657236216e-07,
      "loss": 1.771,
      "step": 162240
    },
    {
      "epoch": 0.0005889172930929357,
      "grad_norm": 9919.197346559851,
      "learning_rate": 2.483676894453999e-07,
      "loss": 1.7778,
      "step": 162272
    },
    {
      "epoch": 0.0005890334274437723,
      "grad_norm": 10462.807271473559,
      "learning_rate": 2.4834317957676777e-07,
      "loss": 1.7872,
      "step": 162304
    },
    {
      "epoch": 0.0005891495617946091,
      "grad_norm": 10128.729930252854,
      "learning_rate": 2.483186769628851e-07,
      "loss": 1.7803,
      "step": 162336
    },
    {
      "epoch": 0.0005892656961454457,
      "grad_norm": 19056.634750133613,
      "learning_rate": 2.4829418160017365e-07,
      "loss": 1.7305,
      "step": 162368
    },
    {
      "epoch": 0.0005893818304962825,
      "grad_norm": 10345.788708455242,
      "learning_rate": 2.4827045862898576e-07,
      "loss": 1.7364,
      "step": 162400
    },
    {
      "epoch": 0.0005894979648471191,
      "grad_norm": 9891.913667233453,
      "learning_rate": 2.4824597753157033e-07,
      "loss": 1.7308,
      "step": 162432
    },
    {
      "epoch": 0.0005896140991979559,
      "grad_norm": 9210.326595729384,
      "learning_rate": 2.482215036747179e-07,
      "loss": 1.7261,
      "step": 162464
    },
    {
      "epoch": 0.0005897302335487925,
      "grad_norm": 10968.945984004115,
      "learning_rate": 2.481970370548599e-07,
      "loss": 1.714,
      "step": 162496
    },
    {
      "epoch": 0.0005898463678996293,
      "grad_norm": 9674.250978757993,
      "learning_rate": 2.481725776684305e-07,
      "loss": 1.7108,
      "step": 162528
    },
    {
      "epoch": 0.000589962502250466,
      "grad_norm": 9486.547527947141,
      "learning_rate": 2.481481255118661e-07,
      "loss": 1.755,
      "step": 162560
    },
    {
      "epoch": 0.0005900786366013027,
      "grad_norm": 9981.806249371904,
      "learning_rate": 2.4812368058160573e-07,
      "loss": 1.7616,
      "step": 162592
    },
    {
      "epoch": 0.0005901947709521394,
      "grad_norm": 9462.408361511354,
      "learning_rate": 2.4809924287409073e-07,
      "loss": 1.7567,
      "step": 162624
    },
    {
      "epoch": 0.0005903109053029761,
      "grad_norm": 10136.673024222493,
      "learning_rate": 2.48074812385765e-07,
      "loss": 1.7302,
      "step": 162656
    },
    {
      "epoch": 0.0005904270396538128,
      "grad_norm": 8475.327486298096,
      "learning_rate": 2.480503891130748e-07,
      "loss": 1.7169,
      "step": 162688
    },
    {
      "epoch": 0.0005905431740046495,
      "grad_norm": 8774.799143000368,
      "learning_rate": 2.4802597305246896e-07,
      "loss": 1.71,
      "step": 162720
    },
    {
      "epoch": 0.0005906593083554862,
      "grad_norm": 9930.51579727861,
      "learning_rate": 2.4800156420039863e-07,
      "loss": 1.7202,
      "step": 162752
    },
    {
      "epoch": 0.0005907754427063229,
      "grad_norm": 10117.42477115595,
      "learning_rate": 2.4797716255331745e-07,
      "loss": 1.7271,
      "step": 162784
    },
    {
      "epoch": 0.0005908915770571596,
      "grad_norm": 10828.059844681318,
      "learning_rate": 2.4795276810768153e-07,
      "loss": 1.7221,
      "step": 162816
    },
    {
      "epoch": 0.0005910077114079964,
      "grad_norm": 9564.421362528943,
      "learning_rate": 2.479283808599494e-07,
      "loss": 1.7227,
      "step": 162848
    },
    {
      "epoch": 0.000591123845758833,
      "grad_norm": 9322.169275442278,
      "learning_rate": 2.479040008065821e-07,
      "loss": 1.7521,
      "step": 162880
    },
    {
      "epoch": 0.0005912399801096698,
      "grad_norm": 8643.773481529928,
      "learning_rate": 2.4787962794404283e-07,
      "loss": 1.7548,
      "step": 162912
    },
    {
      "epoch": 0.0005913561144605064,
      "grad_norm": 8425.943982723835,
      "learning_rate": 2.4785526226879766e-07,
      "loss": 1.7643,
      "step": 162944
    },
    {
      "epoch": 0.0005914722488113432,
      "grad_norm": 10271.010661079074,
      "learning_rate": 2.478309037773148e-07,
      "loss": 1.7689,
      "step": 162976
    },
    {
      "epoch": 0.0005915883831621798,
      "grad_norm": 9437.91809669908,
      "learning_rate": 2.4780655246606477e-07,
      "loss": 1.7524,
      "step": 163008
    },
    {
      "epoch": 0.0005917045175130166,
      "grad_norm": 8564.58662166482,
      "learning_rate": 2.4778220833152094e-07,
      "loss": 1.7267,
      "step": 163040
    },
    {
      "epoch": 0.0005918206518638532,
      "grad_norm": 9285.879818304778,
      "learning_rate": 2.4775787137015874e-07,
      "loss": 1.7398,
      "step": 163072
    },
    {
      "epoch": 0.00059193678621469,
      "grad_norm": 8832.53530986432,
      "learning_rate": 2.4773354157845615e-07,
      "loss": 1.7595,
      "step": 163104
    },
    {
      "epoch": 0.0005920529205655267,
      "grad_norm": 10236.909299197683,
      "learning_rate": 2.4770921895289354e-07,
      "loss": 1.7778,
      "step": 163136
    },
    {
      "epoch": 0.0005921690549163634,
      "grad_norm": 10337.944089614724,
      "learning_rate": 2.4768490348995377e-07,
      "loss": 1.7568,
      "step": 163168
    },
    {
      "epoch": 0.0005922851892672001,
      "grad_norm": 10143.892743912465,
      "learning_rate": 2.4766059518612207e-07,
      "loss": 1.7533,
      "step": 163200
    },
    {
      "epoch": 0.0005924013236180368,
      "grad_norm": 9388.095547021238,
      "learning_rate": 2.47636294037886e-07,
      "loss": 1.7438,
      "step": 163232
    },
    {
      "epoch": 0.0005925174579688735,
      "grad_norm": 8850.21389572026,
      "learning_rate": 2.4761200004173573e-07,
      "loss": 1.7291,
      "step": 163264
    },
    {
      "epoch": 0.0005926335923197102,
      "grad_norm": 9560.00606694368,
      "learning_rate": 2.4758771319416364e-07,
      "loss": 1.7275,
      "step": 163296
    },
    {
      "epoch": 0.0005927497266705469,
      "grad_norm": 11262.290530793458,
      "learning_rate": 2.475634334916646e-07,
      "loss": 1.7348,
      "step": 163328
    },
    {
      "epoch": 0.0005928658610213836,
      "grad_norm": 10175.322501031602,
      "learning_rate": 2.47539160930736e-07,
      "loss": 1.7257,
      "step": 163360
    },
    {
      "epoch": 0.0005929819953722203,
      "grad_norm": 9085.585726853278,
      "learning_rate": 2.4751565369432947e-07,
      "loss": 1.7063,
      "step": 163392
    },
    {
      "epoch": 0.0005930981297230571,
      "grad_norm": 8716.853560775242,
      "learning_rate": 2.4749139518314047e-07,
      "loss": 1.7265,
      "step": 163424
    },
    {
      "epoch": 0.0005932142640738937,
      "grad_norm": 9827.442190112339,
      "learning_rate": 2.4746714380313736e-07,
      "loss": 1.7623,
      "step": 163456
    },
    {
      "epoch": 0.0005933303984247305,
      "grad_norm": 9815.974123845273,
      "learning_rate": 2.474428995508268e-07,
      "loss": 1.7656,
      "step": 163488
    },
    {
      "epoch": 0.0005934465327755671,
      "grad_norm": 10435.126448682833,
      "learning_rate": 2.4741866242271803e-07,
      "loss": 1.7211,
      "step": 163520
    },
    {
      "epoch": 0.0005935626671264039,
      "grad_norm": 8235.578425344513,
      "learning_rate": 2.473944324153227e-07,
      "loss": 1.7385,
      "step": 163552
    },
    {
      "epoch": 0.0005936788014772405,
      "grad_norm": 9088.783416937604,
      "learning_rate": 2.4737020952515464e-07,
      "loss": 1.744,
      "step": 163584
    },
    {
      "epoch": 0.0005937949358280773,
      "grad_norm": 9998.054410734121,
      "learning_rate": 2.473459937487304e-07,
      "loss": 1.7263,
      "step": 163616
    },
    {
      "epoch": 0.0005939110701789139,
      "grad_norm": 9564.459524719627,
      "learning_rate": 2.4732178508256865e-07,
      "loss": 1.7304,
      "step": 163648
    },
    {
      "epoch": 0.0005940272045297507,
      "grad_norm": 8799.39020614497,
      "learning_rate": 2.4729758352319056e-07,
      "loss": 1.7351,
      "step": 163680
    },
    {
      "epoch": 0.0005941433388805874,
      "grad_norm": 9915.790235780505,
      "learning_rate": 2.4727338906711966e-07,
      "loss": 1.7126,
      "step": 163712
    },
    {
      "epoch": 0.0005942594732314241,
      "grad_norm": 9461.027639744005,
      "learning_rate": 2.472492017108818e-07,
      "loss": 1.7418,
      "step": 163744
    },
    {
      "epoch": 0.0005943756075822608,
      "grad_norm": 9341.185577858947,
      "learning_rate": 2.472250214510053e-07,
      "loss": 1.7275,
      "step": 163776
    },
    {
      "epoch": 0.0005944917419330975,
      "grad_norm": 9504.89210880376,
      "learning_rate": 2.4720084828402084e-07,
      "loss": 1.7402,
      "step": 163808
    },
    {
      "epoch": 0.0005946078762839342,
      "grad_norm": 10349.261712798647,
      "learning_rate": 2.471766822064614e-07,
      "loss": 1.7282,
      "step": 163840
    },
    {
      "epoch": 0.0005947240106347709,
      "grad_norm": 8163.7253750968375,
      "learning_rate": 2.4715252321486243e-07,
      "loss": 1.7233,
      "step": 163872
    },
    {
      "epoch": 0.0005948401449856076,
      "grad_norm": 9414.794315331588,
      "learning_rate": 2.471283713057617e-07,
      "loss": 1.7533,
      "step": 163904
    },
    {
      "epoch": 0.0005949562793364443,
      "grad_norm": 9613.486360316949,
      "learning_rate": 2.471042264756994e-07,
      "loss": 1.7587,
      "step": 163936
    },
    {
      "epoch": 0.000595072413687281,
      "grad_norm": 11579.548523150634,
      "learning_rate": 2.470800887212179e-07,
      "loss": 1.7637,
      "step": 163968
    },
    {
      "epoch": 0.0005951885480381178,
      "grad_norm": 8485.471937376258,
      "learning_rate": 2.4705595803886223e-07,
      "loss": 1.771,
      "step": 164000
    },
    {
      "epoch": 0.0005953046823889544,
      "grad_norm": 8847.772827101744,
      "learning_rate": 2.4703183442517956e-07,
      "loss": 1.77,
      "step": 164032
    },
    {
      "epoch": 0.0005954208167397912,
      "grad_norm": 8967.194879113535,
      "learning_rate": 2.4700771787671946e-07,
      "loss": 1.7537,
      "step": 164064
    },
    {
      "epoch": 0.0005955369510906278,
      "grad_norm": 9271.856556267467,
      "learning_rate": 2.469836083900339e-07,
      "loss": 1.7451,
      "step": 164096
    },
    {
      "epoch": 0.0005956530854414646,
      "grad_norm": 8307.851587504438,
      "learning_rate": 2.4695950596167726e-07,
      "loss": 1.7497,
      "step": 164128
    },
    {
      "epoch": 0.0005957692197923012,
      "grad_norm": 9561.504693300109,
      "learning_rate": 2.469354105882061e-07,
      "loss": 1.7464,
      "step": 164160
    },
    {
      "epoch": 0.000595885354143138,
      "grad_norm": 10250.538522438712,
      "learning_rate": 2.469113222661794e-07,
      "loss": 1.7029,
      "step": 164192
    },
    {
      "epoch": 0.0005960014884939746,
      "grad_norm": 9631.826410395903,
      "learning_rate": 2.4688724099215865e-07,
      "loss": 1.7228,
      "step": 164224
    },
    {
      "epoch": 0.0005961176228448114,
      "grad_norm": 8464.790842070464,
      "learning_rate": 2.468631667627075e-07,
      "loss": 1.7354,
      "step": 164256
    },
    {
      "epoch": 0.0005962337571956481,
      "grad_norm": 10518.292827260515,
      "learning_rate": 2.4683909957439196e-07,
      "loss": 1.734,
      "step": 164288
    },
    {
      "epoch": 0.0005963498915464848,
      "grad_norm": 10252.941626674756,
      "learning_rate": 2.468150394237805e-07,
      "loss": 1.7661,
      "step": 164320
    },
    {
      "epoch": 0.0005964660258973215,
      "grad_norm": 10020.58920423345,
      "learning_rate": 2.467909863074438e-07,
      "loss": 1.7626,
      "step": 164352
    },
    {
      "epoch": 0.0005965821602481582,
      "grad_norm": 8580.080885399624,
      "learning_rate": 2.467676915557366e-07,
      "loss": 1.7162,
      "step": 164384
    },
    {
      "epoch": 0.0005966982945989949,
      "grad_norm": 9716.115890622137,
      "learning_rate": 2.4674365227811587e-07,
      "loss": 1.7091,
      "step": 164416
    },
    {
      "epoch": 0.0005968144289498316,
      "grad_norm": 8361.579994235539,
      "learning_rate": 2.4671962002460305e-07,
      "loss": 1.7168,
      "step": 164448
    },
    {
      "epoch": 0.0005969305633006683,
      "grad_norm": 9473.108465546036,
      "learning_rate": 2.4669559479177825e-07,
      "loss": 1.7247,
      "step": 164480
    },
    {
      "epoch": 0.000597046697651505,
      "grad_norm": 8681.663435079709,
      "learning_rate": 2.466715765762236e-07,
      "loss": 1.7216,
      "step": 164512
    },
    {
      "epoch": 0.0005971628320023417,
      "grad_norm": 10495.894625995443,
      "learning_rate": 2.4664756537452404e-07,
      "loss": 1.7066,
      "step": 164544
    },
    {
      "epoch": 0.0005972789663531785,
      "grad_norm": 9628.2320287787,
      "learning_rate": 2.4662356118326635e-07,
      "loss": 1.7376,
      "step": 164576
    },
    {
      "epoch": 0.0005973951007040151,
      "grad_norm": 9976.867444243208,
      "learning_rate": 2.465995639990399e-07,
      "loss": 1.7533,
      "step": 164608
    },
    {
      "epoch": 0.0005975112350548519,
      "grad_norm": 8780.429488356478,
      "learning_rate": 2.465755738184364e-07,
      "loss": 1.7498,
      "step": 164640
    },
    {
      "epoch": 0.0005976273694056885,
      "grad_norm": 8786.641906894807,
      "learning_rate": 2.4655159063804974e-07,
      "loss": 1.7529,
      "step": 164672
    },
    {
      "epoch": 0.0005977435037565253,
      "grad_norm": 10099.959801900204,
      "learning_rate": 2.465276144544762e-07,
      "loss": 1.761,
      "step": 164704
    },
    {
      "epoch": 0.0005978596381073619,
      "grad_norm": 8060.5635038749,
      "learning_rate": 2.465036452643144e-07,
      "loss": 1.7389,
      "step": 164736
    },
    {
      "epoch": 0.0005979757724581987,
      "grad_norm": 9247.581089128118,
      "learning_rate": 2.4647968306416527e-07,
      "loss": 1.7364,
      "step": 164768
    },
    {
      "epoch": 0.0005980919068090353,
      "grad_norm": 9858.985647621159,
      "learning_rate": 2.46455727850632e-07,
      "loss": 1.7454,
      "step": 164800
    },
    {
      "epoch": 0.0005982080411598721,
      "grad_norm": 10218.819109858046,
      "learning_rate": 2.4643177962032014e-07,
      "loss": 1.7651,
      "step": 164832
    },
    {
      "epoch": 0.0005983241755107088,
      "grad_norm": 10322.226600884134,
      "learning_rate": 2.464078383698376e-07,
      "loss": 1.7446,
      "step": 164864
    },
    {
      "epoch": 0.0005984403098615455,
      "grad_norm": 9053.620601726141,
      "learning_rate": 2.463839040957944e-07,
      "loss": 1.7639,
      "step": 164896
    },
    {
      "epoch": 0.0005985564442123822,
      "grad_norm": 10910.696586377975,
      "learning_rate": 2.4635997679480305e-07,
      "loss": 1.7737,
      "step": 164928
    },
    {
      "epoch": 0.0005986725785632189,
      "grad_norm": 9453.383838605096,
      "learning_rate": 2.4633605646347833e-07,
      "loss": 1.7534,
      "step": 164960
    },
    {
      "epoch": 0.0005987887129140556,
      "grad_norm": 7901.978992632163,
      "learning_rate": 2.463121430984373e-07,
      "loss": 1.7318,
      "step": 164992
    },
    {
      "epoch": 0.0005989048472648923,
      "grad_norm": 9942.863269702546,
      "learning_rate": 2.462882366962992e-07,
      "loss": 1.7247,
      "step": 165024
    },
    {
      "epoch": 0.000599020981615729,
      "grad_norm": 9406.944562396442,
      "learning_rate": 2.462643372536859e-07,
      "loss": 1.7192,
      "step": 165056
    },
    {
      "epoch": 0.0005991371159665657,
      "grad_norm": 9374.966986608539,
      "learning_rate": 2.4624044476722114e-07,
      "loss": 1.7087,
      "step": 165088
    },
    {
      "epoch": 0.0005992532503174024,
      "grad_norm": 9528.900251340656,
      "learning_rate": 2.462165592335313e-07,
      "loss": 1.7162,
      "step": 165120
    },
    {
      "epoch": 0.0005993693846682392,
      "grad_norm": 8014.839237314744,
      "learning_rate": 2.461926806492448e-07,
      "loss": 1.7327,
      "step": 165152
    },
    {
      "epoch": 0.0005994855190190758,
      "grad_norm": 10191.174024615613,
      "learning_rate": 2.4616880901099254e-07,
      "loss": 1.7527,
      "step": 165184
    },
    {
      "epoch": 0.0005996016533699126,
      "grad_norm": 7541.623963046686,
      "learning_rate": 2.4614494431540753e-07,
      "loss": 1.7305,
      "step": 165216
    },
    {
      "epoch": 0.0005997177877207492,
      "grad_norm": 8717.607928784135,
      "learning_rate": 2.4612108655912524e-07,
      "loss": 1.74,
      "step": 165248
    },
    {
      "epoch": 0.000599833922071586,
      "grad_norm": 8509.245324939222,
      "learning_rate": 2.4609723573878334e-07,
      "loss": 1.7469,
      "step": 165280
    },
    {
      "epoch": 0.0005999500564224226,
      "grad_norm": 7840.3376202814125,
      "learning_rate": 2.4607339185102173e-07,
      "loss": 1.7534,
      "step": 165312
    },
    {
      "epoch": 0.0006000661907732594,
      "grad_norm": 10105.317906924056,
      "learning_rate": 2.460495548924827e-07,
      "loss": 1.7429,
      "step": 165344
    },
    {
      "epoch": 0.000600182325124096,
      "grad_norm": 9316.591436786312,
      "learning_rate": 2.4602572485981074e-07,
      "loss": 1.725,
      "step": 165376
    },
    {
      "epoch": 0.0006002984594749328,
      "grad_norm": 9727.968544357038,
      "learning_rate": 2.460026461170941e-07,
      "loss": 1.7074,
      "step": 165408
    },
    {
      "epoch": 0.0006004145938257696,
      "grad_norm": 9331.094683904992,
      "learning_rate": 2.459788297099257e-07,
      "loss": 1.7236,
      "step": 165440
    },
    {
      "epoch": 0.0006005307281766062,
      "grad_norm": 8995.27831698386,
      "learning_rate": 2.459550202186761e-07,
      "loss": 1.7307,
      "step": 165472
    },
    {
      "epoch": 0.000600646862527443,
      "grad_norm": 10183.70266651575,
      "learning_rate": 2.459312176399988e-07,
      "loss": 1.7607,
      "step": 165504
    },
    {
      "epoch": 0.0006007629968782796,
      "grad_norm": 10169.234976142503,
      "learning_rate": 2.459074219705497e-07,
      "loss": 1.7291,
      "step": 165536
    },
    {
      "epoch": 0.0006008791312291164,
      "grad_norm": 8977.793381449586,
      "learning_rate": 2.458836332069867e-07,
      "loss": 1.7157,
      "step": 165568
    },
    {
      "epoch": 0.000600995265579953,
      "grad_norm": 8848.398499163563,
      "learning_rate": 2.4585985134597023e-07,
      "loss": 1.7434,
      "step": 165600
    },
    {
      "epoch": 0.0006011113999307898,
      "grad_norm": 9444.74954670583,
      "learning_rate": 2.458360763841628e-07,
      "loss": 1.7512,
      "step": 165632
    },
    {
      "epoch": 0.0006012275342816264,
      "grad_norm": 10557.774765546004,
      "learning_rate": 2.458123083182293e-07,
      "loss": 1.7534,
      "step": 165664
    },
    {
      "epoch": 0.0006013436686324632,
      "grad_norm": 9781.741971653106,
      "learning_rate": 2.4578854714483676e-07,
      "loss": 1.7579,
      "step": 165696
    },
    {
      "epoch": 0.0006014598029832999,
      "grad_norm": 10478.898987966246,
      "learning_rate": 2.457647928606545e-07,
      "loss": 1.7608,
      "step": 165728
    },
    {
      "epoch": 0.0006015759373341365,
      "grad_norm": 11883.794175262377,
      "learning_rate": 2.4574104546235427e-07,
      "loss": 1.7616,
      "step": 165760
    },
    {
      "epoch": 0.0006016920716849733,
      "grad_norm": 9333.91225585499,
      "learning_rate": 2.4571730494660974e-07,
      "loss": 1.7421,
      "step": 165792
    },
    {
      "epoch": 0.00060180820603581,
      "grad_norm": 8851.221836560193,
      "learning_rate": 2.4569357131009713e-07,
      "loss": 1.7604,
      "step": 165824
    },
    {
      "epoch": 0.0006019243403866467,
      "grad_norm": 9930.24209171156,
      "learning_rate": 2.456698445494947e-07,
      "loss": 1.7551,
      "step": 165856
    },
    {
      "epoch": 0.0006020404747374833,
      "grad_norm": 9677.812976080908,
      "learning_rate": 2.4564612466148313e-07,
      "loss": 1.7213,
      "step": 165888
    },
    {
      "epoch": 0.0006021566090883201,
      "grad_norm": 8697.215301462877,
      "learning_rate": 2.456224116427452e-07,
      "loss": 1.7369,
      "step": 165920
    },
    {
      "epoch": 0.0006022727434391567,
      "grad_norm": 9905.920048132833,
      "learning_rate": 2.45598705489966e-07,
      "loss": 1.7324,
      "step": 165952
    },
    {
      "epoch": 0.0006023888777899935,
      "grad_norm": 9436.528493042344,
      "learning_rate": 2.4557500619983275e-07,
      "loss": 1.7413,
      "step": 165984
    },
    {
      "epoch": 0.0006025050121408303,
      "grad_norm": 8727.4454452606,
      "learning_rate": 2.455513137690351e-07,
      "loss": 1.7352,
      "step": 166016
    },
    {
      "epoch": 0.0006026211464916669,
      "grad_norm": 9878.05426184732,
      "learning_rate": 2.455276281942648e-07,
      "loss": 1.7506,
      "step": 166048
    },
    {
      "epoch": 0.0006027372808425037,
      "grad_norm": 9167.552999574096,
      "learning_rate": 2.4550394947221594e-07,
      "loss": 1.7312,
      "step": 166080
    },
    {
      "epoch": 0.0006028534151933403,
      "grad_norm": 9000.154665337703,
      "learning_rate": 2.454802775995847e-07,
      "loss": 1.7333,
      "step": 166112
    },
    {
      "epoch": 0.0006029695495441771,
      "grad_norm": 9597.798080809995,
      "learning_rate": 2.4545661257306953e-07,
      "loss": 1.7111,
      "step": 166144
    },
    {
      "epoch": 0.0006030856838950137,
      "grad_norm": 10240.214743842045,
      "learning_rate": 2.454329543893712e-07,
      "loss": 1.7304,
      "step": 166176
    },
    {
      "epoch": 0.0006032018182458505,
      "grad_norm": 9451.150406167495,
      "learning_rate": 2.4540930304519263e-07,
      "loss": 1.7207,
      "step": 166208
    },
    {
      "epoch": 0.0006033179525966871,
      "grad_norm": 10393.402618969401,
      "learning_rate": 2.4538565853723893e-07,
      "loss": 1.6999,
      "step": 166240
    },
    {
      "epoch": 0.0006034340869475239,
      "grad_norm": 10514.760292084646,
      "learning_rate": 2.4536202086221754e-07,
      "loss": 1.7318,
      "step": 166272
    },
    {
      "epoch": 0.0006035502212983606,
      "grad_norm": 9938.920665746356,
      "learning_rate": 2.4533839001683804e-07,
      "loss": 1.738,
      "step": 166304
    },
    {
      "epoch": 0.0006036663556491973,
      "grad_norm": 10606.269560971945,
      "learning_rate": 2.453147659978122e-07,
      "loss": 1.7473,
      "step": 166336
    },
    {
      "epoch": 0.000603782490000034,
      "grad_norm": 10524.285628963136,
      "learning_rate": 2.452911488018541e-07,
      "loss": 1.7484,
      "step": 166368
    },
    {
      "epoch": 0.0006038986243508707,
      "grad_norm": 8444.146493281603,
      "learning_rate": 2.452682761467389e-07,
      "loss": 1.7544,
      "step": 166400
    },
    {
      "epoch": 0.0006040147587017074,
      "grad_norm": 10757.994980478472,
      "learning_rate": 2.452446723741011e-07,
      "loss": 1.7221,
      "step": 166432
    },
    {
      "epoch": 0.0006041308930525441,
      "grad_norm": 9817.34404001408,
      "learning_rate": 2.452210754147889e-07,
      "loss": 1.7472,
      "step": 166464
    },
    {
      "epoch": 0.0006042470274033808,
      "grad_norm": 8670.562034839495,
      "learning_rate": 2.4519748526552506e-07,
      "loss": 1.7579,
      "step": 166496
    },
    {
      "epoch": 0.0006043631617542175,
      "grad_norm": 10228.301129708687,
      "learning_rate": 2.4517390192303455e-07,
      "loss": 1.7546,
      "step": 166528
    },
    {
      "epoch": 0.0006044792961050542,
      "grad_norm": 10784.500359311969,
      "learning_rate": 2.4515032538404464e-07,
      "loss": 1.7363,
      "step": 166560
    },
    {
      "epoch": 0.000604595430455891,
      "grad_norm": 10096.382520487226,
      "learning_rate": 2.4512675564528474e-07,
      "loss": 1.7402,
      "step": 166592
    },
    {
      "epoch": 0.0006047115648067276,
      "grad_norm": 10578.873474997232,
      "learning_rate": 2.451031927034864e-07,
      "loss": 1.7794,
      "step": 166624
    },
    {
      "epoch": 0.0006048276991575644,
      "grad_norm": 9752.966112932003,
      "learning_rate": 2.4507963655538354e-07,
      "loss": 1.7594,
      "step": 166656
    },
    {
      "epoch": 0.000604943833508401,
      "grad_norm": 10196.295797984678,
      "learning_rate": 2.4505608719771215e-07,
      "loss": 1.7683,
      "step": 166688
    },
    {
      "epoch": 0.0006050599678592378,
      "grad_norm": 8257.431683035591,
      "learning_rate": 2.450325446272104e-07,
      "loss": 1.7461,
      "step": 166720
    },
    {
      "epoch": 0.0006051761022100744,
      "grad_norm": 9074.517948629558,
      "learning_rate": 2.4500900884061873e-07,
      "loss": 1.7171,
      "step": 166752
    },
    {
      "epoch": 0.0006052922365609112,
      "grad_norm": 8564.90630421606,
      "learning_rate": 2.4498547983467975e-07,
      "loss": 1.7072,
      "step": 166784
    },
    {
      "epoch": 0.0006054083709117478,
      "grad_norm": 9670.493886043256,
      "learning_rate": 2.4496195760613826e-07,
      "loss": 1.7151,
      "step": 166816
    },
    {
      "epoch": 0.0006055245052625846,
      "grad_norm": 8896.805494108547,
      "learning_rate": 2.4493844215174117e-07,
      "loss": 1.7366,
      "step": 166848
    },
    {
      "epoch": 0.0006056406396134213,
      "grad_norm": 11317.449712722384,
      "learning_rate": 2.4491493346823763e-07,
      "loss": 1.7359,
      "step": 166880
    },
    {
      "epoch": 0.000605756773964258,
      "grad_norm": 8084.206578261096,
      "learning_rate": 2.4489143155237906e-07,
      "loss": 1.725,
      "step": 166912
    },
    {
      "epoch": 0.0006058729083150947,
      "grad_norm": 9954.794221881233,
      "learning_rate": 2.4486793640091893e-07,
      "loss": 1.7231,
      "step": 166944
    },
    {
      "epoch": 0.0006059890426659314,
      "grad_norm": 8416.446280943044,
      "learning_rate": 2.448444480106129e-07,
      "loss": 1.7573,
      "step": 166976
    },
    {
      "epoch": 0.0006061051770167681,
      "grad_norm": 8918.035097486441,
      "learning_rate": 2.44820966378219e-07,
      "loss": 1.7484,
      "step": 167008
    },
    {
      "epoch": 0.0006062213113676048,
      "grad_norm": 9189.055446562503,
      "learning_rate": 2.4479749150049713e-07,
      "loss": 1.7462,
      "step": 167040
    },
    {
      "epoch": 0.0006063374457184415,
      "grad_norm": 9243.203016270929,
      "learning_rate": 2.447740233742096e-07,
      "loss": 1.7505,
      "step": 167072
    },
    {
      "epoch": 0.0006064535800692782,
      "grad_norm": 8089.767116549153,
      "learning_rate": 2.4475056199612085e-07,
      "loss": 1.7241,
      "step": 167104
    },
    {
      "epoch": 0.0006065697144201149,
      "grad_norm": 10045.676482945288,
      "learning_rate": 2.447271073629974e-07,
      "loss": 1.7076,
      "step": 167136
    },
    {
      "epoch": 0.0006066858487709516,
      "grad_norm": 9770.12998889984,
      "learning_rate": 2.4470365947160793e-07,
      "loss": 1.7275,
      "step": 167168
    },
    {
      "epoch": 0.0006068019831217883,
      "grad_norm": 10334.328715499618,
      "learning_rate": 2.4468021831872345e-07,
      "loss": 1.7415,
      "step": 167200
    },
    {
      "epoch": 0.0006069181174726251,
      "grad_norm": 10057.487857313077,
      "learning_rate": 2.44656783901117e-07,
      "loss": 1.7383,
      "step": 167232
    },
    {
      "epoch": 0.0006070342518234617,
      "grad_norm": 9144.887533480114,
      "learning_rate": 2.446333562155639e-07,
      "loss": 1.7272,
      "step": 167264
    },
    {
      "epoch": 0.0006071503861742985,
      "grad_norm": 9674.354758845677,
      "learning_rate": 2.446099352588414e-07,
      "loss": 1.7401,
      "step": 167296
    },
    {
      "epoch": 0.0006072665205251351,
      "grad_norm": 9096.167874440313,
      "learning_rate": 2.4458652102772913e-07,
      "loss": 1.744,
      "step": 167328
    },
    {
      "epoch": 0.0006073826548759719,
      "grad_norm": 8436.492991759076,
      "learning_rate": 2.4456311351900883e-07,
      "loss": 1.7466,
      "step": 167360
    },
    {
      "epoch": 0.0006074987892268085,
      "grad_norm": 9167.428428954327,
      "learning_rate": 2.445397127294643e-07,
      "loss": 1.7479,
      "step": 167392
    },
    {
      "epoch": 0.0006076149235776453,
      "grad_norm": 9172.662972114478,
      "learning_rate": 2.445170496190554e-07,
      "loss": 1.7455,
      "step": 167424
    },
    {
      "epoch": 0.0006077310579284819,
      "grad_norm": 9295.77043606392,
      "learning_rate": 2.4449366204849797e-07,
      "loss": 1.7391,
      "step": 167456
    },
    {
      "epoch": 0.0006078471922793187,
      "grad_norm": 10015.949480703266,
      "learning_rate": 2.4447028118758114e-07,
      "loss": 1.7555,
      "step": 167488
    },
    {
      "epoch": 0.0006079633266301554,
      "grad_norm": 9032.883260620609,
      "learning_rate": 2.444469070330973e-07,
      "loss": 1.7439,
      "step": 167520
    },
    {
      "epoch": 0.0006080794609809921,
      "grad_norm": 8302.03023362358,
      "learning_rate": 2.4442353958184105e-07,
      "loss": 1.761,
      "step": 167552
    },
    {
      "epoch": 0.0006081955953318288,
      "grad_norm": 10150.759577489756,
      "learning_rate": 2.444001788306091e-07,
      "loss": 1.7407,
      "step": 167584
    },
    {
      "epoch": 0.0006083117296826655,
      "grad_norm": 9394.485297236884,
      "learning_rate": 2.4437682477620023e-07,
      "loss": 1.7271,
      "step": 167616
    },
    {
      "epoch": 0.0006084278640335022,
      "grad_norm": 8420.726215713226,
      "learning_rate": 2.443534774154155e-07,
      "loss": 1.7589,
      "step": 167648
    },
    {
      "epoch": 0.0006085439983843389,
      "grad_norm": 9944.29766247974,
      "learning_rate": 2.44330136745058e-07,
      "loss": 1.7457,
      "step": 167680
    },
    {
      "epoch": 0.0006086601327351756,
      "grad_norm": 9992.46155859506,
      "learning_rate": 2.4430680276193296e-07,
      "loss": 1.7407,
      "step": 167712
    },
    {
      "epoch": 0.0006087762670860123,
      "grad_norm": 9625.086389222697,
      "learning_rate": 2.442834754628478e-07,
      "loss": 1.7416,
      "step": 167744
    },
    {
      "epoch": 0.000608892401436849,
      "grad_norm": 9499.52546183229,
      "learning_rate": 2.4426015484461204e-07,
      "loss": 1.7368,
      "step": 167776
    },
    {
      "epoch": 0.0006090085357876858,
      "grad_norm": 9586.501551661064,
      "learning_rate": 2.4423684090403736e-07,
      "loss": 1.7088,
      "step": 167808
    },
    {
      "epoch": 0.0006091246701385224,
      "grad_norm": 9682.771090963579,
      "learning_rate": 2.4421353363793756e-07,
      "loss": 1.7344,
      "step": 167840
    },
    {
      "epoch": 0.0006092408044893592,
      "grad_norm": 10127.696085487558,
      "learning_rate": 2.441902330431285e-07,
      "loss": 1.7529,
      "step": 167872
    },
    {
      "epoch": 0.0006093569388401958,
      "grad_norm": 8615.926647784323,
      "learning_rate": 2.4416693911642823e-07,
      "loss": 1.7206,
      "step": 167904
    },
    {
      "epoch": 0.0006094730731910326,
      "grad_norm": 8863.602089444223,
      "learning_rate": 2.4414365185465696e-07,
      "loss": 1.7026,
      "step": 167936
    },
    {
      "epoch": 0.0006095892075418692,
      "grad_norm": 8685.254285281462,
      "learning_rate": 2.44120371254637e-07,
      "loss": 1.7151,
      "step": 167968
    },
    {
      "epoch": 0.000609705341892706,
      "grad_norm": 8400.128570444622,
      "learning_rate": 2.440970973131927e-07,
      "loss": 1.7318,
      "step": 168000
    },
    {
      "epoch": 0.0006098214762435426,
      "grad_norm": 9296.504611949591,
      "learning_rate": 2.440738300271506e-07,
      "loss": 1.7464,
      "step": 168032
    },
    {
      "epoch": 0.0006099376105943794,
      "grad_norm": 9504.607619465414,
      "learning_rate": 2.4405056939333935e-07,
      "loss": 1.7442,
      "step": 168064
    },
    {
      "epoch": 0.0006100537449452161,
      "grad_norm": 9781.016920545633,
      "learning_rate": 2.440273154085897e-07,
      "loss": 1.7491,
      "step": 168096
    },
    {
      "epoch": 0.0006101698792960528,
      "grad_norm": 10537.294244729052,
      "learning_rate": 2.440040680697346e-07,
      "loss": 1.7274,
      "step": 168128
    },
    {
      "epoch": 0.0006102860136468895,
      "grad_norm": 9177.246645917281,
      "learning_rate": 2.4398082737360895e-07,
      "loss": 1.7348,
      "step": 168160
    },
    {
      "epoch": 0.0006104021479977262,
      "grad_norm": 8917.905583711907,
      "learning_rate": 2.4395759331704986e-07,
      "loss": 1.7358,
      "step": 168192
    },
    {
      "epoch": 0.0006105182823485629,
      "grad_norm": 9401.201838063047,
      "learning_rate": 2.4393436589689655e-07,
      "loss": 1.76,
      "step": 168224
    },
    {
      "epoch": 0.0006106344166993996,
      "grad_norm": 9306.143024905645,
      "learning_rate": 2.439111451099904e-07,
      "loss": 1.7561,
      "step": 168256
    },
    {
      "epoch": 0.0006107505510502363,
      "grad_norm": 9092.866654691468,
      "learning_rate": 2.438879309531747e-07,
      "loss": 1.732,
      "step": 168288
    },
    {
      "epoch": 0.000610866685401073,
      "grad_norm": 9494.140508755914,
      "learning_rate": 2.438647234232951e-07,
      "loss": 1.7595,
      "step": 168320
    },
    {
      "epoch": 0.0006109828197519097,
      "grad_norm": 10294.429950220652,
      "learning_rate": 2.43841522517199e-07,
      "loss": 1.7689,
      "step": 168352
    },
    {
      "epoch": 0.0006110989541027465,
      "grad_norm": 9146.73887240693,
      "learning_rate": 2.438183282317364e-07,
      "loss": 1.7533,
      "step": 168384
    },
    {
      "epoch": 0.0006112150884535831,
      "grad_norm": 10013.972837989926,
      "learning_rate": 2.4379586507824746e-07,
      "loss": 1.7563,
      "step": 168416
    },
    {
      "epoch": 0.0006113312228044199,
      "grad_norm": 9052.359581899074,
      "learning_rate": 2.437726838179586e-07,
      "loss": 1.7623,
      "step": 168448
    },
    {
      "epoch": 0.0006114473571552565,
      "grad_norm": 10413.826386108038,
      "learning_rate": 2.43749509168963e-07,
      "loss": 1.7157,
      "step": 168480
    },
    {
      "epoch": 0.0006115634915060933,
      "grad_norm": 10650.316427224123,
      "learning_rate": 2.437263411281188e-07,
      "loss": 1.7163,
      "step": 168512
    },
    {
      "epoch": 0.0006116796258569299,
      "grad_norm": 8666.721525467401,
      "learning_rate": 2.4370317969228605e-07,
      "loss": 1.7166,
      "step": 168544
    },
    {
      "epoch": 0.0006117957602077667,
      "grad_norm": 10067.111800312938,
      "learning_rate": 2.4368002485832697e-07,
      "loss": 1.7355,
      "step": 168576
    },
    {
      "epoch": 0.0006119118945586033,
      "grad_norm": 9641.736150714767,
      "learning_rate": 2.4365687662310586e-07,
      "loss": 1.7233,
      "step": 168608
    },
    {
      "epoch": 0.0006120280289094401,
      "grad_norm": 9681.181539460977,
      "learning_rate": 2.4363373498348907e-07,
      "loss": 1.7201,
      "step": 168640
    },
    {
      "epoch": 0.0006121441632602768,
      "grad_norm": 8333.296826586702,
      "learning_rate": 2.436105999363451e-07,
      "loss": 1.7308,
      "step": 168672
    },
    {
      "epoch": 0.0006122602976111135,
      "grad_norm": 9111.131214069963,
      "learning_rate": 2.4358747147854446e-07,
      "loss": 1.7411,
      "step": 168704
    },
    {
      "epoch": 0.0006123764319619502,
      "grad_norm": 9908.242427393468,
      "learning_rate": 2.435643496069598e-07,
      "loss": 1.7485,
      "step": 168736
    },
    {
      "epoch": 0.0006124925663127869,
      "grad_norm": 10354.29746530396,
      "learning_rate": 2.4354123431846586e-07,
      "loss": 1.7395,
      "step": 168768
    },
    {
      "epoch": 0.0006126087006636236,
      "grad_norm": 8551.985617387345,
      "learning_rate": 2.4351812560993935e-07,
      "loss": 1.7253,
      "step": 168800
    },
    {
      "epoch": 0.0006127248350144603,
      "grad_norm": 8866.411111605417,
      "learning_rate": 2.4349502347825913e-07,
      "loss": 1.727,
      "step": 168832
    },
    {
      "epoch": 0.000612840969365297,
      "grad_norm": 9783.176375799427,
      "learning_rate": 2.4347192792030617e-07,
      "loss": 1.7385,
      "step": 168864
    },
    {
      "epoch": 0.0006129571037161337,
      "grad_norm": 10180.67777704412,
      "learning_rate": 2.434488389329634e-07,
      "loss": 1.7391,
      "step": 168896
    },
    {
      "epoch": 0.0006130732380669704,
      "grad_norm": 10648.855713174069,
      "learning_rate": 2.434257565131159e-07,
      "loss": 1.7377,
      "step": 168928
    },
    {
      "epoch": 0.0006131893724178072,
      "grad_norm": 9190.382037760999,
      "learning_rate": 2.4340268065765084e-07,
      "loss": 1.7159,
      "step": 168960
    },
    {
      "epoch": 0.0006133055067686438,
      "grad_norm": 9440.942325848622,
      "learning_rate": 2.433796113634574e-07,
      "loss": 1.7351,
      "step": 168992
    },
    {
      "epoch": 0.0006134216411194806,
      "grad_norm": 8443.594495237203,
      "learning_rate": 2.433565486274268e-07,
      "loss": 1.7523,
      "step": 169024
    },
    {
      "epoch": 0.0006135377754703172,
      "grad_norm": 8495.787309013804,
      "learning_rate": 2.4333349244645234e-07,
      "loss": 1.7441,
      "step": 169056
    },
    {
      "epoch": 0.000613653909821154,
      "grad_norm": 9667.736860299829,
      "learning_rate": 2.433104428174295e-07,
      "loss": 1.7419,
      "step": 169088
    },
    {
      "epoch": 0.0006137700441719906,
      "grad_norm": 10527.527345013168,
      "learning_rate": 2.4328739973725567e-07,
      "loss": 1.746,
      "step": 169120
    },
    {
      "epoch": 0.0006138861785228274,
      "grad_norm": 9331.868087366001,
      "learning_rate": 2.4326436320283037e-07,
      "loss": 1.7214,
      "step": 169152
    },
    {
      "epoch": 0.000614002312873664,
      "grad_norm": 9739.43098953938,
      "learning_rate": 2.4324133321105506e-07,
      "loss": 1.7416,
      "step": 169184
    },
    {
      "epoch": 0.0006141184472245008,
      "grad_norm": 9448.71769077688,
      "learning_rate": 2.432183097588334e-07,
      "loss": 1.7481,
      "step": 169216
    },
    {
      "epoch": 0.0006142345815753375,
      "grad_norm": 10050.43302549696,
      "learning_rate": 2.431952928430711e-07,
      "loss": 1.7522,
      "step": 169248
    },
    {
      "epoch": 0.0006143507159261742,
      "grad_norm": 9910.307563340302,
      "learning_rate": 2.4317228246067575e-07,
      "loss": 1.7418,
      "step": 169280
    },
    {
      "epoch": 0.0006144668502770109,
      "grad_norm": 9238.654447483139,
      "learning_rate": 2.4314927860855715e-07,
      "loss": 1.7328,
      "step": 169312
    },
    {
      "epoch": 0.0006145829846278476,
      "grad_norm": 9760.443739912647,
      "learning_rate": 2.4312628128362717e-07,
      "loss": 1.7566,
      "step": 169344
    },
    {
      "epoch": 0.0006146991189786843,
      "grad_norm": 9930.735420904133,
      "learning_rate": 2.4310329048279954e-07,
      "loss": 1.7446,
      "step": 169376
    },
    {
      "epoch": 0.000614815253329521,
      "grad_norm": 10401.462397182428,
      "learning_rate": 2.43081024363058e-07,
      "loss": 1.7454,
      "step": 169408
    },
    {
      "epoch": 0.0006149313876803577,
      "grad_norm": 8846.262939795539,
      "learning_rate": 2.4305804639754593e-07,
      "loss": 1.756,
      "step": 169440
    },
    {
      "epoch": 0.0006150475220311944,
      "grad_norm": 8696.468018684367,
      "learning_rate": 2.4303507494698634e-07,
      "loss": 1.7398,
      "step": 169472
    },
    {
      "epoch": 0.0006151636563820311,
      "grad_norm": 9250.519336772395,
      "learning_rate": 2.4301211000830096e-07,
      "loss": 1.7116,
      "step": 169504
    },
    {
      "epoch": 0.0006152797907328679,
      "grad_norm": 9510.3295421347,
      "learning_rate": 2.4298915157841394e-07,
      "loss": 1.7104,
      "step": 169536
    },
    {
      "epoch": 0.0006153959250837045,
      "grad_norm": 8738.223503664804,
      "learning_rate": 2.429661996542512e-07,
      "loss": 1.7261,
      "step": 169568
    },
    {
      "epoch": 0.0006155120594345413,
      "grad_norm": 8753.96927113638,
      "learning_rate": 2.429432542327408e-07,
      "loss": 1.7544,
      "step": 169600
    },
    {
      "epoch": 0.0006156281937853779,
      "grad_norm": 8765.704649370751,
      "learning_rate": 2.429203153108128e-07,
      "loss": 1.7202,
      "step": 169632
    },
    {
      "epoch": 0.0006157443281362147,
      "grad_norm": 10539.730357082197,
      "learning_rate": 2.4289738288539927e-07,
      "loss": 1.7037,
      "step": 169664
    },
    {
      "epoch": 0.0006158604624870513,
      "grad_norm": 8743.916170686909,
      "learning_rate": 2.428744569534344e-07,
      "loss": 1.731,
      "step": 169696
    },
    {
      "epoch": 0.0006159765968378881,
      "grad_norm": 8162.088458231753,
      "learning_rate": 2.428515375118543e-07,
      "loss": 1.7335,
      "step": 169728
    },
    {
      "epoch": 0.0006160927311887247,
      "grad_norm": 9565.429838747446,
      "learning_rate": 2.428286245575972e-07,
      "loss": 1.7347,
      "step": 169760
    },
    {
      "epoch": 0.0006162088655395615,
      "grad_norm": 10066.39369387071,
      "learning_rate": 2.4280571808760313e-07,
      "loss": 1.7452,
      "step": 169792
    },
    {
      "epoch": 0.0006163249998903983,
      "grad_norm": 9164.143822529195,
      "learning_rate": 2.4278281809881445e-07,
      "loss": 1.7191,
      "step": 169824
    },
    {
      "epoch": 0.0006164411342412349,
      "grad_norm": 10523.114938077983,
      "learning_rate": 2.4275992458817536e-07,
      "loss": 1.7261,
      "step": 169856
    },
    {
      "epoch": 0.0006165572685920717,
      "grad_norm": 8799.361567750242,
      "learning_rate": 2.4273703755263215e-07,
      "loss": 1.7305,
      "step": 169888
    },
    {
      "epoch": 0.0006166734029429083,
      "grad_norm": 9865.729775338466,
      "learning_rate": 2.42714156989133e-07,
      "loss": 1.7455,
      "step": 169920
    },
    {
      "epoch": 0.000616789537293745,
      "grad_norm": 9065.435786546612,
      "learning_rate": 2.4269128289462826e-07,
      "loss": 1.7378,
      "step": 169952
    },
    {
      "epoch": 0.0006169056716445817,
      "grad_norm": 8482.778613166796,
      "learning_rate": 2.426684152660702e-07,
      "loss": 1.7379,
      "step": 169984
    },
    {
      "epoch": 0.0006170218059954185,
      "grad_norm": 9722.722663945528,
      "learning_rate": 2.426455541004131e-07,
      "loss": 1.7667,
      "step": 170016
    },
    {
      "epoch": 0.0006171379403462551,
      "grad_norm": 9560.855610247443,
      "learning_rate": 2.4262269939461326e-07,
      "loss": 1.7743,
      "step": 170048
    },
    {
      "epoch": 0.0006172540746970919,
      "grad_norm": 10387.782824067895,
      "learning_rate": 2.42599851145629e-07,
      "loss": 1.7467,
      "step": 170080
    },
    {
      "epoch": 0.0006173702090479286,
      "grad_norm": 8226.797311226283,
      "learning_rate": 2.425770093504207e-07,
      "loss": 1.755,
      "step": 170112
    },
    {
      "epoch": 0.0006174863433987653,
      "grad_norm": 11335.915137297032,
      "learning_rate": 2.425541740059506e-07,
      "loss": 1.7526,
      "step": 170144
    },
    {
      "epoch": 0.000617602477749602,
      "grad_norm": 10099.9463364911,
      "learning_rate": 2.42531345109183e-07,
      "loss": 1.7306,
      "step": 170176
    },
    {
      "epoch": 0.0006177186121004387,
      "grad_norm": 9679.324149960057,
      "learning_rate": 2.4250852265708433e-07,
      "loss": 1.7465,
      "step": 170208
    },
    {
      "epoch": 0.0006178347464512754,
      "grad_norm": 9960.627490273893,
      "learning_rate": 2.4248570664662287e-07,
      "loss": 1.732,
      "step": 170240
    },
    {
      "epoch": 0.000617950880802112,
      "grad_norm": 9424.919309999423,
      "learning_rate": 2.424628970747689e-07,
      "loss": 1.7307,
      "step": 170272
    },
    {
      "epoch": 0.0006180670151529488,
      "grad_norm": 11151.842897028277,
      "learning_rate": 2.424400939384948e-07,
      "loss": 1.7115,
      "step": 170304
    },
    {
      "epoch": 0.0006181831495037855,
      "grad_norm": 8781.704731998225,
      "learning_rate": 2.4241729723477473e-07,
      "loss": 1.7241,
      "step": 170336
    },
    {
      "epoch": 0.0006182992838546222,
      "grad_norm": 9699.450706096712,
      "learning_rate": 2.4239450696058515e-07,
      "loss": 1.7385,
      "step": 170368
    },
    {
      "epoch": 0.000618415418205459,
      "grad_norm": 9301.036716409628,
      "learning_rate": 2.4237172311290427e-07,
      "loss": 1.7294,
      "step": 170400
    },
    {
      "epoch": 0.0006185315525562956,
      "grad_norm": 10140.555606079975,
      "learning_rate": 2.4234965738601777e-07,
      "loss": 1.73,
      "step": 170432
    },
    {
      "epoch": 0.0006186476869071324,
      "grad_norm": 8095.2912239152965,
      "learning_rate": 2.423268861817031e-07,
      "loss": 1.741,
      "step": 170464
    },
    {
      "epoch": 0.000618763821257969,
      "grad_norm": 9360.509174184917,
      "learning_rate": 2.42304121394938e-07,
      "loss": 1.7358,
      "step": 170496
    },
    {
      "epoch": 0.0006188799556088058,
      "grad_norm": 8932.173643632326,
      "learning_rate": 2.422813630227088e-07,
      "loss": 1.7176,
      "step": 170528
    },
    {
      "epoch": 0.0006189960899596424,
      "grad_norm": 10535.418928547644,
      "learning_rate": 2.4225861106200365e-07,
      "loss": 1.7244,
      "step": 170560
    },
    {
      "epoch": 0.0006191122243104792,
      "grad_norm": 9811.281669588332,
      "learning_rate": 2.4223586550981257e-07,
      "loss": 1.7534,
      "step": 170592
    },
    {
      "epoch": 0.0006192283586613158,
      "grad_norm": 9466.477803280373,
      "learning_rate": 2.4221312636312774e-07,
      "loss": 1.7536,
      "step": 170624
    },
    {
      "epoch": 0.0006193444930121526,
      "grad_norm": 10025.37819735495,
      "learning_rate": 2.421903936189432e-07,
      "loss": 1.7122,
      "step": 170656
    },
    {
      "epoch": 0.0006194606273629893,
      "grad_norm": 9160.026200835891,
      "learning_rate": 2.4216766727425504e-07,
      "loss": 1.725,
      "step": 170688
    },
    {
      "epoch": 0.000619576761713826,
      "grad_norm": 9279.69848648112,
      "learning_rate": 2.421449473260613e-07,
      "loss": 1.7468,
      "step": 170720
    },
    {
      "epoch": 0.0006196928960646627,
      "grad_norm": 9808.217065297851,
      "learning_rate": 2.421222337713619e-07,
      "loss": 1.7446,
      "step": 170752
    },
    {
      "epoch": 0.0006198090304154994,
      "grad_norm": 9474.230311745647,
      "learning_rate": 2.4209952660715883e-07,
      "loss": 1.7522,
      "step": 170784
    },
    {
      "epoch": 0.0006199251647663361,
      "grad_norm": 8908.99096418893,
      "learning_rate": 2.4207682583045607e-07,
      "loss": 1.7462,
      "step": 170816
    },
    {
      "epoch": 0.0006200412991171728,
      "grad_norm": 8289.18126234431,
      "learning_rate": 2.4205413143825954e-07,
      "loss": 1.722,
      "step": 170848
    },
    {
      "epoch": 0.0006201574334680095,
      "grad_norm": 10360.760203768832,
      "learning_rate": 2.42031443427577e-07,
      "loss": 1.7266,
      "step": 170880
    },
    {
      "epoch": 0.0006202735678188462,
      "grad_norm": 9359.34271196434,
      "learning_rate": 2.4200876179541835e-07,
      "loss": 1.745,
      "step": 170912
    },
    {
      "epoch": 0.0006203897021696829,
      "grad_norm": 11265.591684416757,
      "learning_rate": 2.4198608653879533e-07,
      "loss": 1.7437,
      "step": 170944
    },
    {
      "epoch": 0.0006205058365205197,
      "grad_norm": 10547.910693592357,
      "learning_rate": 2.419634176547217e-07,
      "loss": 1.7418,
      "step": 170976
    },
    {
      "epoch": 0.0006206219708713563,
      "grad_norm": 11219.13739999649,
      "learning_rate": 2.419407551402133e-07,
      "loss": 1.7254,
      "step": 171008
    },
    {
      "epoch": 0.0006207381052221931,
      "grad_norm": 10324.595585300181,
      "learning_rate": 2.419180989922876e-07,
      "loss": 1.7494,
      "step": 171040
    },
    {
      "epoch": 0.0006208542395730297,
      "grad_norm": 9288.310072343624,
      "learning_rate": 2.418954492079643e-07,
      "loss": 1.7603,
      "step": 171072
    },
    {
      "epoch": 0.0006209703739238665,
      "grad_norm": 10253.731808468563,
      "learning_rate": 2.41872805784265e-07,
      "loss": 1.7557,
      "step": 171104
    },
    {
      "epoch": 0.0006210865082747031,
      "grad_norm": 10922.473346270981,
      "learning_rate": 2.4185016871821314e-07,
      "loss": 1.739,
      "step": 171136
    },
    {
      "epoch": 0.0006212026426255399,
      "grad_norm": 8998.81770012039,
      "learning_rate": 2.418275380068343e-07,
      "loss": 1.7477,
      "step": 171168
    },
    {
      "epoch": 0.0006213187769763765,
      "grad_norm": 9750.902932549376,
      "learning_rate": 2.4180491364715577e-07,
      "loss": 1.7453,
      "step": 171200
    },
    {
      "epoch": 0.0006214349113272133,
      "grad_norm": 8554.424586142542,
      "learning_rate": 2.4178229563620703e-07,
      "loss": 1.7195,
      "step": 171232
    },
    {
      "epoch": 0.00062155104567805,
      "grad_norm": 9591.527094263978,
      "learning_rate": 2.4175968397101933e-07,
      "loss": 1.7179,
      "step": 171264
    },
    {
      "epoch": 0.0006216671800288867,
      "grad_norm": 9171.206463710214,
      "learning_rate": 2.4173707864862594e-07,
      "loss": 1.7314,
      "step": 171296
    },
    {
      "epoch": 0.0006217833143797234,
      "grad_norm": 8622.775191317469,
      "learning_rate": 2.41714479666062e-07,
      "loss": 1.7154,
      "step": 171328
    },
    {
      "epoch": 0.0006218994487305601,
      "grad_norm": 9470.69617293259,
      "learning_rate": 2.4169188702036474e-07,
      "loss": 1.725,
      "step": 171360
    },
    {
      "epoch": 0.0006220155830813968,
      "grad_norm": 9724.454534831248,
      "learning_rate": 2.4166930070857315e-07,
      "loss": 1.7489,
      "step": 171392
    },
    {
      "epoch": 0.0006221317174322335,
      "grad_norm": 9838.093819434738,
      "learning_rate": 2.4164742625632935e-07,
      "loss": 1.7293,
      "step": 171424
    },
    {
      "epoch": 0.0006222478517830702,
      "grad_norm": 9298.335980163332,
      "learning_rate": 2.416248524057692e-07,
      "loss": 1.7371,
      "step": 171456
    },
    {
      "epoch": 0.0006223639861339069,
      "grad_norm": 10202.993482307043,
      "learning_rate": 2.416022848803359e-07,
      "loss": 1.7358,
      "step": 171488
    },
    {
      "epoch": 0.0006224801204847436,
      "grad_norm": 9395.468801502137,
      "learning_rate": 2.415797236770763e-07,
      "loss": 1.7272,
      "step": 171520
    },
    {
      "epoch": 0.0006225962548355804,
      "grad_norm": 8646.255837066123,
      "learning_rate": 2.415571687930389e-07,
      "loss": 1.7196,
      "step": 171552
    },
    {
      "epoch": 0.000622712389186417,
      "grad_norm": 9296.196103783526,
      "learning_rate": 2.415346202252744e-07,
      "loss": 1.722,
      "step": 171584
    },
    {
      "epoch": 0.0006228285235372538,
      "grad_norm": 11095.858146173283,
      "learning_rate": 2.415120779708353e-07,
      "loss": 1.7474,
      "step": 171616
    },
    {
      "epoch": 0.0006229446578880904,
      "grad_norm": 9801.941134285596,
      "learning_rate": 2.414895420267761e-07,
      "loss": 1.7366,
      "step": 171648
    },
    {
      "epoch": 0.0006230607922389272,
      "grad_norm": 8722.197773497228,
      "learning_rate": 2.4146701239015313e-07,
      "loss": 1.7175,
      "step": 171680
    },
    {
      "epoch": 0.0006231769265897638,
      "grad_norm": 9982.038368990574,
      "learning_rate": 2.414444890580247e-07,
      "loss": 1.738,
      "step": 171712
    },
    {
      "epoch": 0.0006232930609406006,
      "grad_norm": 7916.715985811288,
      "learning_rate": 2.4142197202745113e-07,
      "loss": 1.7675,
      "step": 171744
    },
    {
      "epoch": 0.0006234091952914372,
      "grad_norm": 9154.282713571829,
      "learning_rate": 2.4139946129549447e-07,
      "loss": 1.7856,
      "step": 171776
    },
    {
      "epoch": 0.000623525329642274,
      "grad_norm": 8967.996208741393,
      "learning_rate": 2.413769568592188e-07,
      "loss": 1.7548,
      "step": 171808
    },
    {
      "epoch": 0.0006236414639931107,
      "grad_norm": 10554.47080625078,
      "learning_rate": 2.413544587156901e-07,
      "loss": 1.7433,
      "step": 171840
    },
    {
      "epoch": 0.0006237575983439474,
      "grad_norm": 10342.350409843983,
      "learning_rate": 2.4133196686197624e-07,
      "loss": 1.7226,
      "step": 171872
    },
    {
      "epoch": 0.0006238737326947841,
      "grad_norm": 9762.114934787442,
      "learning_rate": 2.413094812951471e-07,
      "loss": 1.7319,
      "step": 171904
    },
    {
      "epoch": 0.0006239898670456208,
      "grad_norm": 9699.117485627236,
      "learning_rate": 2.412870020122743e-07,
      "loss": 1.7493,
      "step": 171936
    },
    {
      "epoch": 0.0006241060013964575,
      "grad_norm": 10081.023360750634,
      "learning_rate": 2.412645290104315e-07,
      "loss": 1.7569,
      "step": 171968
    },
    {
      "epoch": 0.0006242221357472942,
      "grad_norm": 11595.308534058075,
      "learning_rate": 2.4124206228669426e-07,
      "loss": 1.7311,
      "step": 172000
    },
    {
      "epoch": 0.0006243382700981309,
      "grad_norm": 10198.154048650176,
      "learning_rate": 2.4121960183813997e-07,
      "loss": 1.7104,
      "step": 172032
    },
    {
      "epoch": 0.0006244544044489676,
      "grad_norm": 9003.141229593148,
      "learning_rate": 2.41197147661848e-07,
      "loss": 1.7386,
      "step": 172064
    },
    {
      "epoch": 0.0006245705387998043,
      "grad_norm": 8940.560496971093,
      "learning_rate": 2.4117469975489957e-07,
      "loss": 1.7317,
      "step": 172096
    },
    {
      "epoch": 0.0006246866731506411,
      "grad_norm": 10993.292318500404,
      "learning_rate": 2.411522581143778e-07,
      "loss": 1.7375,
      "step": 172128
    },
    {
      "epoch": 0.0006248028075014777,
      "grad_norm": 9561.496535584793,
      "learning_rate": 2.4112982273736784e-07,
      "loss": 1.7274,
      "step": 172160
    },
    {
      "epoch": 0.0006249189418523145,
      "grad_norm": 8650.125085800782,
      "learning_rate": 2.411073936209565e-07,
      "loss": 1.7238,
      "step": 172192
    },
    {
      "epoch": 0.0006250350762031511,
      "grad_norm": 9932.45760121834,
      "learning_rate": 2.410849707622327e-07,
      "loss": 1.7137,
      "step": 172224
    },
    {
      "epoch": 0.0006251512105539879,
      "grad_norm": 8340.95114480357,
      "learning_rate": 2.4106255415828713e-07,
      "loss": 1.7339,
      "step": 172256
    },
    {
      "epoch": 0.0006252673449048245,
      "grad_norm": 8748.26154158642,
      "learning_rate": 2.410401438062125e-07,
      "loss": 1.731,
      "step": 172288
    },
    {
      "epoch": 0.0006253834792556613,
      "grad_norm": 10833.529803346644,
      "learning_rate": 2.4101773970310323e-07,
      "loss": 1.7479,
      "step": 172320
    },
    {
      "epoch": 0.0006254996136064979,
      "grad_norm": 9846.807604498019,
      "learning_rate": 2.409953418460558e-07,
      "loss": 1.7345,
      "step": 172352
    },
    {
      "epoch": 0.0006256157479573347,
      "grad_norm": 9128.36699525167,
      "learning_rate": 2.409729502321684e-07,
      "loss": 1.7373,
      "step": 172384
    },
    {
      "epoch": 0.0006257318823081714,
      "grad_norm": 9994.398231009209,
      "learning_rate": 2.409505648585413e-07,
      "loss": 1.7477,
      "step": 172416
    },
    {
      "epoch": 0.0006258480166590081,
      "grad_norm": 8351.919659575276,
      "learning_rate": 2.4092888497590044e-07,
      "loss": 1.7394,
      "step": 172448
    },
    {
      "epoch": 0.0006259641510098448,
      "grad_norm": 9834.164428155551,
      "learning_rate": 2.4090651187931877e-07,
      "loss": 1.7461,
      "step": 172480
    },
    {
      "epoch": 0.0006260802853606815,
      "grad_norm": 10531.266970312736,
      "learning_rate": 2.4088414501439964e-07,
      "loss": 1.7447,
      "step": 172512
    },
    {
      "epoch": 0.0006261964197115182,
      "grad_norm": 9038.356155850466,
      "learning_rate": 2.408617843782508e-07,
      "loss": 1.7325,
      "step": 172544
    },
    {
      "epoch": 0.0006263125540623549,
      "grad_norm": 8718.923672105406,
      "learning_rate": 2.408394299679816e-07,
      "loss": 1.7259,
      "step": 172576
    },
    {
      "epoch": 0.0006264286884131916,
      "grad_norm": 9093.658559677728,
      "learning_rate": 2.408170817807036e-07,
      "loss": 1.7327,
      "step": 172608
    },
    {
      "epoch": 0.0006265448227640283,
      "grad_norm": 8611.602638301421,
      "learning_rate": 2.4079473981353005e-07,
      "loss": 1.753,
      "step": 172640
    },
    {
      "epoch": 0.000626660957114865,
      "grad_norm": 10294.785281879365,
      "learning_rate": 2.4077240406357617e-07,
      "loss": 1.7264,
      "step": 172672
    },
    {
      "epoch": 0.0006267770914657018,
      "grad_norm": 10407.691963158786,
      "learning_rate": 2.4075007452795885e-07,
      "loss": 1.719,
      "step": 172704
    },
    {
      "epoch": 0.0006268932258165384,
      "grad_norm": 9289.84197928038,
      "learning_rate": 2.4072775120379714e-07,
      "loss": 1.7414,
      "step": 172736
    },
    {
      "epoch": 0.0006270093601673752,
      "grad_norm": 10559.95681809353,
      "learning_rate": 2.407054340882117e-07,
      "loss": 1.7547,
      "step": 172768
    },
    {
      "epoch": 0.0006271254945182118,
      "grad_norm": 9945.539904902096,
      "learning_rate": 2.4068312317832527e-07,
      "loss": 1.76,
      "step": 172800
    },
    {
      "epoch": 0.0006272416288690486,
      "grad_norm": 7842.3521981609565,
      "learning_rate": 2.406608184712624e-07,
      "loss": 1.7489,
      "step": 172832
    },
    {
      "epoch": 0.0006273577632198852,
      "grad_norm": 10400.205767195186,
      "learning_rate": 2.406385199641493e-07,
      "loss": 1.761,
      "step": 172864
    },
    {
      "epoch": 0.000627473897570722,
      "grad_norm": 10158.771579280638,
      "learning_rate": 2.4061622765411433e-07,
      "loss": 1.7185,
      "step": 172896
    },
    {
      "epoch": 0.0006275900319215586,
      "grad_norm": 9560.833541067432,
      "learning_rate": 2.405939415382876e-07,
      "loss": 1.7402,
      "step": 172928
    },
    {
      "epoch": 0.0006277061662723954,
      "grad_norm": 9579.70531905862,
      "learning_rate": 2.4057166161380103e-07,
      "loss": 1.7342,
      "step": 172960
    },
    {
      "epoch": 0.0006278223006232321,
      "grad_norm": 9751.750612069609,
      "learning_rate": 2.4054938787778847e-07,
      "loss": 1.7443,
      "step": 172992
    },
    {
      "epoch": 0.0006279384349740688,
      "grad_norm": 8537.571551676741,
      "learning_rate": 2.405271203273855e-07,
      "loss": 1.7071,
      "step": 173024
    },
    {
      "epoch": 0.0006280545693249055,
      "grad_norm": 9815.614295600659,
      "learning_rate": 2.4050485895972984e-07,
      "loss": 1.7033,
      "step": 173056
    },
    {
      "epoch": 0.0006281707036757422,
      "grad_norm": 10341.124697053025,
      "learning_rate": 2.4048260377196075e-07,
      "loss": 1.7342,
      "step": 173088
    },
    {
      "epoch": 0.0006282868380265789,
      "grad_norm": 9261.341047602124,
      "learning_rate": 2.4046035476121944e-07,
      "loss": 1.7435,
      "step": 173120
    },
    {
      "epoch": 0.0006284029723774156,
      "grad_norm": 8868.121672597867,
      "learning_rate": 2.404381119246491e-07,
      "loss": 1.7454,
      "step": 173152
    },
    {
      "epoch": 0.0006285191067282523,
      "grad_norm": 8856.55644141672,
      "learning_rate": 2.404158752593946e-07,
      "loss": 1.7363,
      "step": 173184
    },
    {
      "epoch": 0.000628635241079089,
      "grad_norm": 9945.117596087037,
      "learning_rate": 2.403936447626028e-07,
      "loss": 1.7312,
      "step": 173216
    },
    {
      "epoch": 0.0006287513754299257,
      "grad_norm": 8341.38957248731,
      "learning_rate": 2.4037142043142225e-07,
      "loss": 1.7069,
      "step": 173248
    },
    {
      "epoch": 0.0006288675097807625,
      "grad_norm": 8406.160598037608,
      "learning_rate": 2.403492022630035e-07,
      "loss": 1.7228,
      "step": 173280
    },
    {
      "epoch": 0.0006289836441315991,
      "grad_norm": 9730.802741809126,
      "learning_rate": 2.4032699025449885e-07,
      "loss": 1.7395,
      "step": 173312
    },
    {
      "epoch": 0.0006290997784824359,
      "grad_norm": 9752.666097021882,
      "learning_rate": 2.4030478440306247e-07,
      "loss": 1.7435,
      "step": 173344
    },
    {
      "epoch": 0.0006292159128332725,
      "grad_norm": 8651.074384144435,
      "learning_rate": 2.402825847058503e-07,
      "loss": 1.7148,
      "step": 173376
    },
    {
      "epoch": 0.0006293320471841093,
      "grad_norm": 10346.524150650786,
      "learning_rate": 2.4026039116002035e-07,
      "loss": 1.7252,
      "step": 173408
    },
    {
      "epoch": 0.0006294481815349459,
      "grad_norm": 9241.556146017834,
      "learning_rate": 2.4023889702585685e-07,
      "loss": 1.7468,
      "step": 173440
    },
    {
      "epoch": 0.0006295643158857827,
      "grad_norm": 9875.712227480102,
      "learning_rate": 2.402167155822617e-07,
      "loss": 1.7567,
      "step": 173472
    },
    {
      "epoch": 0.0006296804502366193,
      "grad_norm": 9378.167198338917,
      "learning_rate": 2.401945402816218e-07,
      "loss": 1.7643,
      "step": 173504
    },
    {
      "epoch": 0.0006297965845874561,
      "grad_norm": 10027.711503628332,
      "learning_rate": 2.4017237112110235e-07,
      "loss": 1.7561,
      "step": 173536
    },
    {
      "epoch": 0.0006299127189382928,
      "grad_norm": 8737.546108605093,
      "learning_rate": 2.4015020809787027e-07,
      "loss": 1.7428,
      "step": 173568
    },
    {
      "epoch": 0.0006300288532891295,
      "grad_norm": 9586.89397041607,
      "learning_rate": 2.4012805120909436e-07,
      "loss": 1.725,
      "step": 173600
    },
    {
      "epoch": 0.0006301449876399662,
      "grad_norm": 9598.95879770301,
      "learning_rate": 2.401059004519452e-07,
      "loss": 1.7374,
      "step": 173632
    },
    {
      "epoch": 0.0006302611219908029,
      "grad_norm": 9936.724611258984,
      "learning_rate": 2.4008375582359526e-07,
      "loss": 1.7538,
      "step": 173664
    },
    {
      "epoch": 0.0006303772563416396,
      "grad_norm": 8671.99331180554,
      "learning_rate": 2.400616173212188e-07,
      "loss": 1.7388,
      "step": 173696
    },
    {
      "epoch": 0.0006304933906924763,
      "grad_norm": 10545.235891150087,
      "learning_rate": 2.40039484941992e-07,
      "loss": 1.7355,
      "step": 173728
    },
    {
      "epoch": 0.000630609525043313,
      "grad_norm": 10846.736836486814,
      "learning_rate": 2.4001735868309263e-07,
      "loss": 1.7407,
      "step": 173760
    },
    {
      "epoch": 0.0006307256593941497,
      "grad_norm": 8061.3264417216105,
      "learning_rate": 2.3999523854170056e-07,
      "loss": 1.7419,
      "step": 173792
    },
    {
      "epoch": 0.0006308417937449864,
      "grad_norm": 10073.848916873829,
      "learning_rate": 2.399731245149972e-07,
      "loss": 1.7283,
      "step": 173824
    },
    {
      "epoch": 0.0006309579280958232,
      "grad_norm": 8921.69983803535,
      "learning_rate": 2.3995101660016605e-07,
      "loss": 1.7308,
      "step": 173856
    },
    {
      "epoch": 0.0006310740624466598,
      "grad_norm": 8078.472627916741,
      "learning_rate": 2.3992891479439227e-07,
      "loss": 1.7282,
      "step": 173888
    },
    {
      "epoch": 0.0006311901967974966,
      "grad_norm": 9377.623153016973,
      "learning_rate": 2.399068190948628e-07,
      "loss": 1.7057,
      "step": 173920
    },
    {
      "epoch": 0.0006313063311483332,
      "grad_norm": 9000.332882732728,
      "learning_rate": 2.3988472949876646e-07,
      "loss": 1.7106,
      "step": 173952
    },
    {
      "epoch": 0.00063142246549917,
      "grad_norm": 9806.961608979613,
      "learning_rate": 2.39862646003294e-07,
      "loss": 1.7346,
      "step": 173984
    },
    {
      "epoch": 0.0006315385998500066,
      "grad_norm": 10067.372249003212,
      "learning_rate": 2.3984056860563767e-07,
      "loss": 1.7474,
      "step": 174016
    },
    {
      "epoch": 0.0006316547342008434,
      "grad_norm": 9548.100229888667,
      "learning_rate": 2.398184973029919e-07,
      "loss": 1.7276,
      "step": 174048
    },
    {
      "epoch": 0.00063177086855168,
      "grad_norm": 8474.99132742919,
      "learning_rate": 2.397964320925526e-07,
      "loss": 1.7225,
      "step": 174080
    },
    {
      "epoch": 0.0006318870029025168,
      "grad_norm": 9730.407596807032,
      "learning_rate": 2.3977437297151774e-07,
      "loss": 1.7596,
      "step": 174112
    },
    {
      "epoch": 0.0006320031372533536,
      "grad_norm": 10147.431793315982,
      "learning_rate": 2.397523199370869e-07,
      "loss": 1.7575,
      "step": 174144
    },
    {
      "epoch": 0.0006321192716041902,
      "grad_norm": 10682.018910299681,
      "learning_rate": 2.3973027298646153e-07,
      "loss": 1.7486,
      "step": 174176
    },
    {
      "epoch": 0.000632235405955027,
      "grad_norm": 8826.142758872644,
      "learning_rate": 2.3970823211684503e-07,
      "loss": 1.7371,
      "step": 174208
    },
    {
      "epoch": 0.0006323515403058636,
      "grad_norm": 8169.159809919255,
      "learning_rate": 2.396861973254423e-07,
      "loss": 1.7341,
      "step": 174240
    },
    {
      "epoch": 0.0006324676746567004,
      "grad_norm": 8249.27899879741,
      "learning_rate": 2.396641686094603e-07,
      "loss": 1.7188,
      "step": 174272
    },
    {
      "epoch": 0.000632583809007537,
      "grad_norm": 9810.549831686296,
      "learning_rate": 2.396421459661077e-07,
      "loss": 1.7298,
      "step": 174304
    },
    {
      "epoch": 0.0006326999433583738,
      "grad_norm": 10952.07596759628,
      "learning_rate": 2.396201293925949e-07,
      "loss": 1.7566,
      "step": 174336
    },
    {
      "epoch": 0.0006328160777092104,
      "grad_norm": 9348.21137972393,
      "learning_rate": 2.395981188861341e-07,
      "loss": 1.7449,
      "step": 174368
    },
    {
      "epoch": 0.0006329322120600472,
      "grad_norm": 9547.236877756832,
      "learning_rate": 2.395761144439395e-07,
      "loss": 1.7063,
      "step": 174400
    },
    {
      "epoch": 0.0006330483464108839,
      "grad_norm": 17633.002466965176,
      "learning_rate": 2.395541160632269e-07,
      "loss": 1.72,
      "step": 174432
    },
    {
      "epoch": 0.0006331644807617206,
      "grad_norm": 9407.47011688052,
      "learning_rate": 2.3953281090959544e-07,
      "loss": 1.7417,
      "step": 174464
    },
    {
      "epoch": 0.0006332806151125573,
      "grad_norm": 9188.302237083844,
      "learning_rate": 2.3951082445429593e-07,
      "loss": 1.7553,
      "step": 174496
    },
    {
      "epoch": 0.000633396749463394,
      "grad_norm": 10258.316236108145,
      "learning_rate": 2.3948884405222336e-07,
      "loss": 1.7577,
      "step": 174528
    },
    {
      "epoch": 0.0006335128838142307,
      "grad_norm": 10980.619290367917,
      "learning_rate": 2.3946686970060073e-07,
      "loss": 1.7531,
      "step": 174560
    },
    {
      "epoch": 0.0006336290181650674,
      "grad_norm": 9231.923743185924,
      "learning_rate": 2.3944490139665274e-07,
      "loss": 1.7335,
      "step": 174592
    },
    {
      "epoch": 0.0006337451525159041,
      "grad_norm": 9711.986408557212,
      "learning_rate": 2.3942293913760577e-07,
      "loss": 1.7362,
      "step": 174624
    },
    {
      "epoch": 0.0006338612868667408,
      "grad_norm": 9210.087947462825,
      "learning_rate": 2.3940098292068823e-07,
      "loss": 1.7308,
      "step": 174656
    },
    {
      "epoch": 0.0006339774212175775,
      "grad_norm": 9325.938451437474,
      "learning_rate": 2.393790327431301e-07,
      "loss": 1.7423,
      "step": 174688
    },
    {
      "epoch": 0.0006340935555684143,
      "grad_norm": 9191.38139780958,
      "learning_rate": 2.393570886021632e-07,
      "loss": 1.7393,
      "step": 174720
    },
    {
      "epoch": 0.0006342096899192509,
      "grad_norm": 9061.224751654712,
      "learning_rate": 2.393351504950212e-07,
      "loss": 1.7165,
      "step": 174752
    },
    {
      "epoch": 0.0006343258242700877,
      "grad_norm": 9752.756123271001,
      "learning_rate": 2.393132184189394e-07,
      "loss": 1.7189,
      "step": 174784
    },
    {
      "epoch": 0.0006344419586209243,
      "grad_norm": 8954.989335560373,
      "learning_rate": 2.3929129237115496e-07,
      "loss": 1.7259,
      "step": 174816
    },
    {
      "epoch": 0.0006345580929717611,
      "grad_norm": 8091.656196354365,
      "learning_rate": 2.3926937234890684e-07,
      "loss": 1.7401,
      "step": 174848
    },
    {
      "epoch": 0.0006346742273225977,
      "grad_norm": 8221.101750982043,
      "learning_rate": 2.392474583494357e-07,
      "loss": 1.7542,
      "step": 174880
    },
    {
      "epoch": 0.0006347903616734345,
      "grad_norm": 8626.762776383735,
      "learning_rate": 2.39225550369984e-07,
      "loss": 1.7454,
      "step": 174912
    },
    {
      "epoch": 0.0006349064960242711,
      "grad_norm": 10195.565408548953,
      "learning_rate": 2.3920364840779594e-07,
      "loss": 1.7153,
      "step": 174944
    },
    {
      "epoch": 0.0006350226303751079,
      "grad_norm": 8778.260875594891,
      "learning_rate": 2.391817524601176e-07,
      "loss": 1.7182,
      "step": 174976
    },
    {
      "epoch": 0.0006351387647259446,
      "grad_norm": 8716.642013986808,
      "learning_rate": 2.3915986252419667e-07,
      "loss": 1.7215,
      "step": 175008
    },
    {
      "epoch": 0.0006352548990767813,
      "grad_norm": 8468.755752765574,
      "learning_rate": 2.3913797859728267e-07,
      "loss": 1.7421,
      "step": 175040
    },
    {
      "epoch": 0.000635371033427618,
      "grad_norm": 9674.756017595482,
      "learning_rate": 2.3911610067662694e-07,
      "loss": 1.7186,
      "step": 175072
    },
    {
      "epoch": 0.0006354871677784547,
      "grad_norm": 10135.655479543491,
      "learning_rate": 2.390942287594825e-07,
      "loss": 1.7197,
      "step": 175104
    },
    {
      "epoch": 0.0006356033021292914,
      "grad_norm": 9539.92285084109,
      "learning_rate": 2.3907236284310407e-07,
      "loss": 1.746,
      "step": 175136
    },
    {
      "epoch": 0.0006357194364801281,
      "grad_norm": 9041.660798769217,
      "learning_rate": 2.3905050292474825e-07,
      "loss": 1.7399,
      "step": 175168
    },
    {
      "epoch": 0.0006358355708309648,
      "grad_norm": 9809.130644455705,
      "learning_rate": 2.390286490016735e-07,
      "loss": 1.7558,
      "step": 175200
    },
    {
      "epoch": 0.0006359517051818015,
      "grad_norm": 11236.881061931732,
      "learning_rate": 2.390068010711396e-07,
      "loss": 1.7475,
      "step": 175232
    },
    {
      "epoch": 0.0006360678395326382,
      "grad_norm": 10178.7661334761,
      "learning_rate": 2.3898495913040864e-07,
      "loss": 1.731,
      "step": 175264
    },
    {
      "epoch": 0.000636183973883475,
      "grad_norm": 11043.569893834148,
      "learning_rate": 2.3896312317674403e-07,
      "loss": 1.7319,
      "step": 175296
    },
    {
      "epoch": 0.0006363001082343116,
      "grad_norm": 11009.400710302083,
      "learning_rate": 2.389412932074112e-07,
      "loss": 1.7406,
      "step": 175328
    },
    {
      "epoch": 0.0006364162425851484,
      "grad_norm": 10380.563953851448,
      "learning_rate": 2.389194692196771e-07,
      "loss": 1.7506,
      "step": 175360
    },
    {
      "epoch": 0.000636532376935985,
      "grad_norm": 9811.439140105798,
      "learning_rate": 2.388976512108106e-07,
      "loss": 1.7475,
      "step": 175392
    },
    {
      "epoch": 0.0006366485112868218,
      "grad_norm": 9901.756813818445,
      "learning_rate": 2.3887583917808235e-07,
      "loss": 1.7273,
      "step": 175424
    },
    {
      "epoch": 0.0006367646456376584,
      "grad_norm": 9823.313901123185,
      "learning_rate": 2.3885471446772746e-07,
      "loss": 1.7346,
      "step": 175456
    },
    {
      "epoch": 0.0006368807799884952,
      "grad_norm": 9869.106342521596,
      "learning_rate": 2.388329141925515e-07,
      "loss": 1.7695,
      "step": 175488
    },
    {
      "epoch": 0.0006369969143393318,
      "grad_norm": 9662.011798792217,
      "learning_rate": 2.38811119885421e-07,
      "loss": 1.741,
      "step": 175520
    },
    {
      "epoch": 0.0006371130486901686,
      "grad_norm": 9696.899091977806,
      "learning_rate": 2.387893315436134e-07,
      "loss": 1.7232,
      "step": 175552
    },
    {
      "epoch": 0.0006372291830410053,
      "grad_norm": 9321.023978083094,
      "learning_rate": 2.387675491644079e-07,
      "loss": 1.7335,
      "step": 175584
    },
    {
      "epoch": 0.000637345317391842,
      "grad_norm": 9155.614670790816,
      "learning_rate": 2.3874577274508544e-07,
      "loss": 1.7071,
      "step": 175616
    },
    {
      "epoch": 0.0006374614517426787,
      "grad_norm": 11698.855670534618,
      "learning_rate": 2.387240022829287e-07,
      "loss": 1.7069,
      "step": 175648
    },
    {
      "epoch": 0.0006375775860935154,
      "grad_norm": 8366.327629252874,
      "learning_rate": 2.387022377752222e-07,
      "loss": 1.709,
      "step": 175680
    },
    {
      "epoch": 0.0006376937204443521,
      "grad_norm": 9569.92309268993,
      "learning_rate": 2.38680479219252e-07,
      "loss": 1.7349,
      "step": 175712
    },
    {
      "epoch": 0.0006378098547951888,
      "grad_norm": 9366.438917753107,
      "learning_rate": 2.38658726612306e-07,
      "loss": 1.7417,
      "step": 175744
    },
    {
      "epoch": 0.0006379259891460255,
      "grad_norm": 9406.810298927048,
      "learning_rate": 2.3863697995167383e-07,
      "loss": 1.7176,
      "step": 175776
    },
    {
      "epoch": 0.0006380421234968622,
      "grad_norm": 10357.190642254298,
      "learning_rate": 2.3861523923464686e-07,
      "loss": 1.7422,
      "step": 175808
    },
    {
      "epoch": 0.0006381582578476989,
      "grad_norm": 8586.344158022086,
      "learning_rate": 2.385935044585181e-07,
      "loss": 1.7454,
      "step": 175840
    },
    {
      "epoch": 0.0006382743921985357,
      "grad_norm": 10019.938223362458,
      "learning_rate": 2.3857177562058237e-07,
      "loss": 1.7551,
      "step": 175872
    },
    {
      "epoch": 0.0006383905265493723,
      "grad_norm": 8565.679541052186,
      "learning_rate": 2.385500527181362e-07,
      "loss": 1.7593,
      "step": 175904
    },
    {
      "epoch": 0.0006385066609002091,
      "grad_norm": 9199.793693338997,
      "learning_rate": 2.385283357484778e-07,
      "loss": 1.7471,
      "step": 175936
    },
    {
      "epoch": 0.0006386227952510457,
      "grad_norm": 9980.4711311641,
      "learning_rate": 2.385066247089072e-07,
      "loss": 1.7097,
      "step": 175968
    },
    {
      "epoch": 0.0006387389296018825,
      "grad_norm": 8108.043167127319,
      "learning_rate": 2.3848491959672603e-07,
      "loss": 1.727,
      "step": 176000
    },
    {
      "epoch": 0.0006388550639527191,
      "grad_norm": 10366.771725083947,
      "learning_rate": 2.3846322040923773e-07,
      "loss": 1.7303,
      "step": 176032
    },
    {
      "epoch": 0.0006389711983035559,
      "grad_norm": 9401.597311095598,
      "learning_rate": 2.384415271437474e-07,
      "loss": 1.7512,
      "step": 176064
    },
    {
      "epoch": 0.0006390873326543925,
      "grad_norm": 9294.051215697062,
      "learning_rate": 2.3841983979756187e-07,
      "loss": 1.7278,
      "step": 176096
    },
    {
      "epoch": 0.0006392034670052293,
      "grad_norm": 9280.625625462973,
      "learning_rate": 2.3839815836798966e-07,
      "loss": 1.7108,
      "step": 176128
    },
    {
      "epoch": 0.000639319601356066,
      "grad_norm": 9352.403113638762,
      "learning_rate": 2.3837648285234108e-07,
      "loss": 1.7386,
      "step": 176160
    },
    {
      "epoch": 0.0006394357357069027,
      "grad_norm": 10568.167674672843,
      "learning_rate": 2.3835481324792813e-07,
      "loss": 1.7342,
      "step": 176192
    },
    {
      "epoch": 0.0006395518700577394,
      "grad_norm": 10163.986324272579,
      "learning_rate": 2.3833314955206445e-07,
      "loss": 1.7437,
      "step": 176224
    },
    {
      "epoch": 0.0006396680044085761,
      "grad_norm": 9347.193375553969,
      "learning_rate": 2.3831149176206543e-07,
      "loss": 1.7513,
      "step": 176256
    },
    {
      "epoch": 0.0006397841387594128,
      "grad_norm": 9183.342202052583,
      "learning_rate": 2.382898398752482e-07,
      "loss": 1.7376,
      "step": 176288
    },
    {
      "epoch": 0.0006399002731102495,
      "grad_norm": 9651.424972510536,
      "learning_rate": 2.3826819388893152e-07,
      "loss": 1.7309,
      "step": 176320
    },
    {
      "epoch": 0.0006400164074610862,
      "grad_norm": 8673.190647045642,
      "learning_rate": 2.38246553800436e-07,
      "loss": 1.7556,
      "step": 176352
    },
    {
      "epoch": 0.0006401325418119229,
      "grad_norm": 10171.13680961966,
      "learning_rate": 2.3822491960708376e-07,
      "loss": 1.7402,
      "step": 176384
    },
    {
      "epoch": 0.0006402486761627596,
      "grad_norm": 9388.068171887122,
      "learning_rate": 2.3820329130619873e-07,
      "loss": 1.7286,
      "step": 176416
    },
    {
      "epoch": 0.0006403648105135964,
      "grad_norm": 9006.401057026053,
      "learning_rate": 2.3818234450632761e-07,
      "loss": 1.7171,
      "step": 176448
    },
    {
      "epoch": 0.000640480944864433,
      "grad_norm": 9330.54189208751,
      "learning_rate": 2.3816072779842353e-07,
      "loss": 1.7356,
      "step": 176480
    },
    {
      "epoch": 0.0006405970792152698,
      "grad_norm": 9222.25221949606,
      "learning_rate": 2.381391169750521e-07,
      "loss": 1.7452,
      "step": 176512
    },
    {
      "epoch": 0.0006407132135661064,
      "grad_norm": 9126.841512812633,
      "learning_rate": 2.381175120335439e-07,
      "loss": 1.7218,
      "step": 176544
    },
    {
      "epoch": 0.0006408293479169432,
      "grad_norm": 9662.312973610407,
      "learning_rate": 2.3809591297123134e-07,
      "loss": 1.7329,
      "step": 176576
    },
    {
      "epoch": 0.0006409454822677798,
      "grad_norm": 9277.178450369487,
      "learning_rate": 2.3807431978544845e-07,
      "loss": 1.755,
      "step": 176608
    },
    {
      "epoch": 0.0006410616166186166,
      "grad_norm": 8685.418585192081,
      "learning_rate": 2.3805273247353096e-07,
      "loss": 1.7211,
      "step": 176640
    },
    {
      "epoch": 0.0006411777509694532,
      "grad_norm": 9758.395462369825,
      "learning_rate": 2.3803115103281635e-07,
      "loss": 1.7244,
      "step": 176672
    },
    {
      "epoch": 0.00064129388532029,
      "grad_norm": 9480.25685306047,
      "learning_rate": 2.380095754606437e-07,
      "loss": 1.7274,
      "step": 176704
    },
    {
      "epoch": 0.0006414100196711267,
      "grad_norm": 10288.843861192569,
      "learning_rate": 2.3798800575435377e-07,
      "loss": 1.7352,
      "step": 176736
    },
    {
      "epoch": 0.0006415261540219634,
      "grad_norm": 10224.361495956606,
      "learning_rate": 2.3796644191128912e-07,
      "loss": 1.7171,
      "step": 176768
    },
    {
      "epoch": 0.0006416422883728001,
      "grad_norm": 8711.758490683727,
      "learning_rate": 2.379448839287939e-07,
      "loss": 1.717,
      "step": 176800
    },
    {
      "epoch": 0.0006417584227236368,
      "grad_norm": 8954.849970825866,
      "learning_rate": 2.3792333180421396e-07,
      "loss": 1.7352,
      "step": 176832
    },
    {
      "epoch": 0.0006418745570744735,
      "grad_norm": 9368.423880247947,
      "learning_rate": 2.3790178553489688e-07,
      "loss": 1.7447,
      "step": 176864
    },
    {
      "epoch": 0.0006419906914253102,
      "grad_norm": 8749.62742063912,
      "learning_rate": 2.3788024511819188e-07,
      "loss": 1.7427,
      "step": 176896
    },
    {
      "epoch": 0.0006421068257761469,
      "grad_norm": 10053.005421265821,
      "learning_rate": 2.378587105514498e-07,
      "loss": 1.7499,
      "step": 176928
    },
    {
      "epoch": 0.0006422229601269836,
      "grad_norm": 9009.866258718828,
      "learning_rate": 2.3783718183202335e-07,
      "loss": 1.7254,
      "step": 176960
    },
    {
      "epoch": 0.0006423390944778203,
      "grad_norm": 8880.197745545986,
      "learning_rate": 2.3781565895726668e-07,
      "loss": 1.7182,
      "step": 176992
    },
    {
      "epoch": 0.0006424552288286571,
      "grad_norm": 9225.215878232877,
      "learning_rate": 2.3779414192453575e-07,
      "loss": 1.7245,
      "step": 177024
    },
    {
      "epoch": 0.0006425713631794937,
      "grad_norm": 10005.721163414459,
      "learning_rate": 2.3777263073118817e-07,
      "loss": 1.7472,
      "step": 177056
    },
    {
      "epoch": 0.0006426874975303305,
      "grad_norm": 10051.897333339612,
      "learning_rate": 2.3775112537458327e-07,
      "loss": 1.7654,
      "step": 177088
    },
    {
      "epoch": 0.0006428036318811671,
      "grad_norm": 9411.057963906078,
      "learning_rate": 2.3772962585208194e-07,
      "loss": 1.7267,
      "step": 177120
    },
    {
      "epoch": 0.0006429197662320039,
      "grad_norm": 9081.571890372283,
      "learning_rate": 2.3770813216104685e-07,
      "loss": 1.7389,
      "step": 177152
    },
    {
      "epoch": 0.0006430359005828405,
      "grad_norm": 9640.773205505873,
      "learning_rate": 2.3768664429884228e-07,
      "loss": 1.7599,
      "step": 177184
    },
    {
      "epoch": 0.0006431520349336773,
      "grad_norm": 9494.867455630963,
      "learning_rate": 2.376651622628342e-07,
      "loss": 1.7674,
      "step": 177216
    },
    {
      "epoch": 0.0006432681692845139,
      "grad_norm": 9372.85506129269,
      "learning_rate": 2.376436860503902e-07,
      "loss": 1.7403,
      "step": 177248
    },
    {
      "epoch": 0.0006433843036353507,
      "grad_norm": 9714.790064638557,
      "learning_rate": 2.3762221565887963e-07,
      "loss": 1.7365,
      "step": 177280
    },
    {
      "epoch": 0.0006435004379861874,
      "grad_norm": 9324.108965472251,
      "learning_rate": 2.3760075108567345e-07,
      "loss": 1.7042,
      "step": 177312
    },
    {
      "epoch": 0.0006436165723370241,
      "grad_norm": 10990.961377422815,
      "learning_rate": 2.3757929232814418e-07,
      "loss": 1.7066,
      "step": 177344
    },
    {
      "epoch": 0.0006437327066878608,
      "grad_norm": 9133.882526067433,
      "learning_rate": 2.375578393836662e-07,
      "loss": 1.7107,
      "step": 177376
    },
    {
      "epoch": 0.0006438488410386975,
      "grad_norm": 10613.35931738863,
      "learning_rate": 2.3753639224961538e-07,
      "loss": 1.72,
      "step": 177408
    },
    {
      "epoch": 0.0006439649753895342,
      "grad_norm": 9151.277724995565,
      "learning_rate": 2.3751495092336938e-07,
      "loss": 1.722,
      "step": 177440
    },
    {
      "epoch": 0.0006440811097403709,
      "grad_norm": 9196.212698714618,
      "learning_rate": 2.3749418517449517e-07,
      "loss": 1.7194,
      "step": 177472
    },
    {
      "epoch": 0.0006441972440912076,
      "grad_norm": 9357.786917856165,
      "learning_rate": 2.3747275527470767e-07,
      "loss": 1.7515,
      "step": 177504
    },
    {
      "epoch": 0.0006443133784420443,
      "grad_norm": 9453.821872660814,
      "learning_rate": 2.374513311749494e-07,
      "loss": 1.7375,
      "step": 177536
    },
    {
      "epoch": 0.000644429512792881,
      "grad_norm": 7643.672154141621,
      "learning_rate": 2.374299128726046e-07,
      "loss": 1.7421,
      "step": 177568
    },
    {
      "epoch": 0.0006445456471437178,
      "grad_norm": 10461.196489885848,
      "learning_rate": 2.3740850036505904e-07,
      "loss": 1.7471,
      "step": 177600
    },
    {
      "epoch": 0.0006446617814945544,
      "grad_norm": 10092.862626628781,
      "learning_rate": 2.373870936497002e-07,
      "loss": 1.7508,
      "step": 177632
    },
    {
      "epoch": 0.0006447779158453912,
      "grad_norm": 9202.268198656242,
      "learning_rate": 2.3736569272391722e-07,
      "loss": 1.7309,
      "step": 177664
    },
    {
      "epoch": 0.0006448940501962278,
      "grad_norm": 9598.357984572152,
      "learning_rate": 2.3734429758510085e-07,
      "loss": 1.7331,
      "step": 177696
    },
    {
      "epoch": 0.0006450101845470646,
      "grad_norm": 8569.398812052103,
      "learning_rate": 2.3732290823064347e-07,
      "loss": 1.7295,
      "step": 177728
    },
    {
      "epoch": 0.0006451263188979012,
      "grad_norm": 10668.999203299249,
      "learning_rate": 2.3730152465793913e-07,
      "loss": 1.7428,
      "step": 177760
    },
    {
      "epoch": 0.000645242453248738,
      "grad_norm": 8714.608998687205,
      "learning_rate": 2.3728014686438354e-07,
      "loss": 1.7271,
      "step": 177792
    },
    {
      "epoch": 0.0006453585875995746,
      "grad_norm": 8660.642123999813,
      "learning_rate": 2.3725877484737408e-07,
      "loss": 1.7064,
      "step": 177824
    },
    {
      "epoch": 0.0006454747219504114,
      "grad_norm": 9943.23991463547,
      "learning_rate": 2.3723740860430967e-07,
      "loss": 1.7454,
      "step": 177856
    },
    {
      "epoch": 0.000645590856301248,
      "grad_norm": 9948.972409249109,
      "learning_rate": 2.3721604813259092e-07,
      "loss": 1.7287,
      "step": 177888
    },
    {
      "epoch": 0.0006457069906520848,
      "grad_norm": 9867.285543653838,
      "learning_rate": 2.3719469342962014e-07,
      "loss": 1.7321,
      "step": 177920
    },
    {
      "epoch": 0.0006458231250029215,
      "grad_norm": 11207.017801360003,
      "learning_rate": 2.3717334449280118e-07,
      "loss": 1.7427,
      "step": 177952
    },
    {
      "epoch": 0.0006459392593537582,
      "grad_norm": 10674.683508188895,
      "learning_rate": 2.3715200131953954e-07,
      "loss": 1.7272,
      "step": 177984
    },
    {
      "epoch": 0.000646055393704595,
      "grad_norm": 10252.922900324571,
      "learning_rate": 2.3713066390724243e-07,
      "loss": 1.7328,
      "step": 178016
    },
    {
      "epoch": 0.0006461715280554316,
      "grad_norm": 10362.574969571993,
      "learning_rate": 2.3710933225331867e-07,
      "loss": 1.746,
      "step": 178048
    },
    {
      "epoch": 0.0006462876624062683,
      "grad_norm": 10220.910526954045,
      "learning_rate": 2.370880063551786e-07,
      "loss": 1.7676,
      "step": 178080
    },
    {
      "epoch": 0.000646403796757105,
      "grad_norm": 8308.711693156767,
      "learning_rate": 2.3706668621023436e-07,
      "loss": 1.7439,
      "step": 178112
    },
    {
      "epoch": 0.0006465199311079417,
      "grad_norm": 9976.032878855201,
      "learning_rate": 2.3704537181589957e-07,
      "loss": 1.7037,
      "step": 178144
    },
    {
      "epoch": 0.0006466360654587784,
      "grad_norm": 9389.527783653446,
      "learning_rate": 2.3702406316958958e-07,
      "loss": 1.7163,
      "step": 178176
    },
    {
      "epoch": 0.0006467521998096151,
      "grad_norm": 8623.075669388505,
      "learning_rate": 2.370027602687213e-07,
      "loss": 1.7462,
      "step": 178208
    },
    {
      "epoch": 0.0006468683341604519,
      "grad_norm": 8614.959431129088,
      "learning_rate": 2.3698146311071333e-07,
      "loss": 1.742,
      "step": 178240
    },
    {
      "epoch": 0.0006469844685112885,
      "grad_norm": 10052.522767942382,
      "learning_rate": 2.369601716929858e-07,
      "loss": 1.7455,
      "step": 178272
    },
    {
      "epoch": 0.0006471006028621253,
      "grad_norm": 9834.375425007935,
      "learning_rate": 2.3693888601296057e-07,
      "loss": 1.7293,
      "step": 178304
    },
    {
      "epoch": 0.000647216737212962,
      "grad_norm": 10092.64643193251,
      "learning_rate": 2.36917606068061e-07,
      "loss": 1.7172,
      "step": 178336
    },
    {
      "epoch": 0.0006473328715637987,
      "grad_norm": 8749.568789374709,
      "learning_rate": 2.3689633185571224e-07,
      "loss": 1.7337,
      "step": 178368
    },
    {
      "epoch": 0.0006474490059146353,
      "grad_norm": 8917.401639491181,
      "learning_rate": 2.3687506337334086e-07,
      "loss": 1.7311,
      "step": 178400
    },
    {
      "epoch": 0.0006475651402654721,
      "grad_norm": 9417.877043155746,
      "learning_rate": 2.368538006183752e-07,
      "loss": 1.7384,
      "step": 178432
    },
    {
      "epoch": 0.0006476812746163087,
      "grad_norm": 10038.964289208325,
      "learning_rate": 2.3683320778380702e-07,
      "loss": 1.7267,
      "step": 178464
    },
    {
      "epoch": 0.0006477974089671455,
      "grad_norm": 9788.318548147072,
      "learning_rate": 2.368119562971621e-07,
      "loss": 1.709,
      "step": 178496
    },
    {
      "epoch": 0.0006479135433179823,
      "grad_norm": 8079.845295548671,
      "learning_rate": 2.367907105302976e-07,
      "loss": 1.7335,
      "step": 178528
    },
    {
      "epoch": 0.0006480296776688189,
      "grad_norm": 8411.750471810252,
      "learning_rate": 2.3676947048064824e-07,
      "loss": 1.7394,
      "step": 178560
    },
    {
      "epoch": 0.0006481458120196557,
      "grad_norm": 9633.411649047288,
      "learning_rate": 2.367482361456503e-07,
      "loss": 1.7316,
      "step": 178592
    },
    {
      "epoch": 0.0006482619463704923,
      "grad_norm": 9023.57501215566,
      "learning_rate": 2.367270075227417e-07,
      "loss": 1.7507,
      "step": 178624
    },
    {
      "epoch": 0.000648378080721329,
      "grad_norm": 8977.860101382734,
      "learning_rate": 2.3670578460936192e-07,
      "loss": 1.7427,
      "step": 178656
    },
    {
      "epoch": 0.0006484942150721657,
      "grad_norm": 7781.4479372415,
      "learning_rate": 2.3668456740295212e-07,
      "loss": 1.6992,
      "step": 178688
    },
    {
      "epoch": 0.0006486103494230025,
      "grad_norm": 8654.806063685079,
      "learning_rate": 2.3666335590095498e-07,
      "loss": 1.7147,
      "step": 178720
    },
    {
      "epoch": 0.0006487264837738391,
      "grad_norm": 9595.03996865047,
      "learning_rate": 2.366421501008149e-07,
      "loss": 1.7314,
      "step": 178752
    },
    {
      "epoch": 0.0006488426181246759,
      "grad_norm": 10069.832371991104,
      "learning_rate": 2.3662094999997775e-07,
      "loss": 1.7484,
      "step": 178784
    },
    {
      "epoch": 0.0006489587524755126,
      "grad_norm": 9365.279280405897,
      "learning_rate": 2.365997555958911e-07,
      "loss": 1.7333,
      "step": 178816
    },
    {
      "epoch": 0.0006490748868263493,
      "grad_norm": 9516.535819298953,
      "learning_rate": 2.365785668860041e-07,
      "loss": 1.7361,
      "step": 178848
    },
    {
      "epoch": 0.000649191021177186,
      "grad_norm": 10091.001932414838,
      "learning_rate": 2.3655738386776742e-07,
      "loss": 1.761,
      "step": 178880
    },
    {
      "epoch": 0.0006493071555280227,
      "grad_norm": 9177.317690916012,
      "learning_rate": 2.3653620653863347e-07,
      "loss": 1.7607,
      "step": 178912
    },
    {
      "epoch": 0.0006494232898788594,
      "grad_norm": 8659.335309364109,
      "learning_rate": 2.365150348960561e-07,
      "loss": 1.768,
      "step": 178944
    },
    {
      "epoch": 0.000649539424229696,
      "grad_norm": 7814.612722329879,
      "learning_rate": 2.3649386893749093e-07,
      "loss": 1.7445,
      "step": 178976
    },
    {
      "epoch": 0.0006496555585805328,
      "grad_norm": 9095.365413220075,
      "learning_rate": 2.3647270866039498e-07,
      "loss": 1.7227,
      "step": 179008
    },
    {
      "epoch": 0.0006497716929313695,
      "grad_norm": 9700.02443295892,
      "learning_rate": 2.36451554062227e-07,
      "loss": 1.7089,
      "step": 179040
    },
    {
      "epoch": 0.0006498878272822062,
      "grad_norm": 9237.280768711104,
      "learning_rate": 2.3643040514044732e-07,
      "loss": 1.7118,
      "step": 179072
    },
    {
      "epoch": 0.000650003961633043,
      "grad_norm": 9548.605447917513,
      "learning_rate": 2.3640926189251775e-07,
      "loss": 1.7203,
      "step": 179104
    },
    {
      "epoch": 0.0006501200959838796,
      "grad_norm": 10304.54579299835,
      "learning_rate": 2.3638812431590185e-07,
      "loss": 1.7152,
      "step": 179136
    },
    {
      "epoch": 0.0006502362303347164,
      "grad_norm": 8624.658370045738,
      "learning_rate": 2.3636699240806465e-07,
      "loss": 1.7048,
      "step": 179168
    },
    {
      "epoch": 0.000650352364685553,
      "grad_norm": 9576.241956007587,
      "learning_rate": 2.363458661664728e-07,
      "loss": 1.7363,
      "step": 179200
    },
    {
      "epoch": 0.0006504684990363898,
      "grad_norm": 9441.55400344668,
      "learning_rate": 2.363247455885945e-07,
      "loss": 1.749,
      "step": 179232
    },
    {
      "epoch": 0.0006505846333872264,
      "grad_norm": 7994.836208453554,
      "learning_rate": 2.3630363067189965e-07,
      "loss": 1.7513,
      "step": 179264
    },
    {
      "epoch": 0.0006507007677380632,
      "grad_norm": 9704.206613628958,
      "learning_rate": 2.3628252141385964e-07,
      "loss": 1.7354,
      "step": 179296
    },
    {
      "epoch": 0.0006508169020888998,
      "grad_norm": 9042.18756717643,
      "learning_rate": 2.362614178119474e-07,
      "loss": 1.7395,
      "step": 179328
    },
    {
      "epoch": 0.0006509330364397366,
      "grad_norm": 10158.392786262992,
      "learning_rate": 2.362403198636375e-07,
      "loss": 1.7224,
      "step": 179360
    },
    {
      "epoch": 0.0006510491707905733,
      "grad_norm": 9270.574739464646,
      "learning_rate": 2.3621922756640612e-07,
      "loss": 1.7309,
      "step": 179392
    },
    {
      "epoch": 0.00065116530514141,
      "grad_norm": 8478.669353147343,
      "learning_rate": 2.361981409177309e-07,
      "loss": 1.7471,
      "step": 179424
    },
    {
      "epoch": 0.0006512814394922467,
      "grad_norm": 9833.823366320956,
      "learning_rate": 2.3617771861098633e-07,
      "loss": 1.7542,
      "step": 179456
    },
    {
      "epoch": 0.0006513975738430834,
      "grad_norm": 10107.788284288506,
      "learning_rate": 2.3615664307554127e-07,
      "loss": 1.7244,
      "step": 179488
    },
    {
      "epoch": 0.0006515137081939201,
      "grad_norm": 10323.340932082017,
      "learning_rate": 2.3613557318117368e-07,
      "loss": 1.7208,
      "step": 179520
    },
    {
      "epoch": 0.0006516298425447568,
      "grad_norm": 9072.773556085262,
      "learning_rate": 2.3611450892536754e-07,
      "loss": 1.728,
      "step": 179552
    },
    {
      "epoch": 0.0006517459768955935,
      "grad_norm": 8337.372367838681,
      "learning_rate": 2.3609345030560836e-07,
      "loss": 1.7283,
      "step": 179584
    },
    {
      "epoch": 0.0006518621112464302,
      "grad_norm": 10082.665421405196,
      "learning_rate": 2.3607239731938323e-07,
      "loss": 1.737,
      "step": 179616
    },
    {
      "epoch": 0.0006519782455972669,
      "grad_norm": 8831.977128593575,
      "learning_rate": 2.3605134996418086e-07,
      "loss": 1.7345,
      "step": 179648
    },
    {
      "epoch": 0.0006520943799481037,
      "grad_norm": 7916.921371341262,
      "learning_rate": 2.3603030823749148e-07,
      "loss": 1.7246,
      "step": 179680
    },
    {
      "epoch": 0.0006522105142989403,
      "grad_norm": 8618.894592695748,
      "learning_rate": 2.3600927213680692e-07,
      "loss": 1.709,
      "step": 179712
    },
    {
      "epoch": 0.0006523266486497771,
      "grad_norm": 9609.315688434843,
      "learning_rate": 2.3598824165962048e-07,
      "loss": 1.7363,
      "step": 179744
    },
    {
      "epoch": 0.0006524427830006137,
      "grad_norm": 9683.844381236204,
      "learning_rate": 2.3596721680342716e-07,
      "loss": 1.7602,
      "step": 179776
    },
    {
      "epoch": 0.0006525589173514505,
      "grad_norm": 8829.694785212001,
      "learning_rate": 2.3594619756572342e-07,
      "loss": 1.762,
      "step": 179808
    },
    {
      "epoch": 0.0006526750517022871,
      "grad_norm": 9661.218970709648,
      "learning_rate": 2.3592518394400733e-07,
      "loss": 1.7225,
      "step": 179840
    },
    {
      "epoch": 0.0006527911860531239,
      "grad_norm": 9244.062851365736,
      "learning_rate": 2.3590417593577851e-07,
      "loss": 1.7279,
      "step": 179872
    },
    {
      "epoch": 0.0006529073204039605,
      "grad_norm": 9311.326865705016,
      "learning_rate": 2.3588317353853814e-07,
      "loss": 1.7281,
      "step": 179904
    },
    {
      "epoch": 0.0006530234547547973,
      "grad_norm": 9205.145408954711,
      "learning_rate": 2.3586217674978893e-07,
      "loss": 1.7271,
      "step": 179936
    },
    {
      "epoch": 0.000653139589105634,
      "grad_norm": 9990.207905744504,
      "learning_rate": 2.3584118556703518e-07,
      "loss": 1.7303,
      "step": 179968
    },
    {
      "epoch": 0.0006532557234564707,
      "grad_norm": 8957.382430152238,
      "learning_rate": 2.358201999877827e-07,
      "loss": 1.7504,
      "step": 180000
    },
    {
      "epoch": 0.0006533718578073074,
      "grad_norm": 10612.701258397883,
      "learning_rate": 2.3579922000953898e-07,
      "loss": 1.722,
      "step": 180032
    },
    {
      "epoch": 0.0006534879921581441,
      "grad_norm": 10201.547921761678,
      "learning_rate": 2.3577824562981283e-07,
      "loss": 1.7213,
      "step": 180064
    },
    {
      "epoch": 0.0006536041265089808,
      "grad_norm": 9146.823055028452,
      "learning_rate": 2.3575727684611488e-07,
      "loss": 1.7304,
      "step": 180096
    },
    {
      "epoch": 0.0006537202608598175,
      "grad_norm": 10230.4766262379,
      "learning_rate": 2.3573631365595704e-07,
      "loss": 1.7532,
      "step": 180128
    },
    {
      "epoch": 0.0006538363952106542,
      "grad_norm": 9760.498552840423,
      "learning_rate": 2.3571535605685305e-07,
      "loss": 1.7231,
      "step": 180160
    },
    {
      "epoch": 0.0006539525295614909,
      "grad_norm": 10155.028311137297,
      "learning_rate": 2.3569440404631793e-07,
      "loss": 1.7146,
      "step": 180192
    },
    {
      "epoch": 0.0006540686639123276,
      "grad_norm": 9473.4293685022,
      "learning_rate": 2.3567345762186847e-07,
      "loss": 1.7452,
      "step": 180224
    },
    {
      "epoch": 0.0006541847982631644,
      "grad_norm": 9766.8764710116,
      "learning_rate": 2.3565251678102284e-07,
      "loss": 1.7308,
      "step": 180256
    },
    {
      "epoch": 0.000654300932614001,
      "grad_norm": 8023.7278119338025,
      "learning_rate": 2.356315815213008e-07,
      "loss": 1.7342,
      "step": 180288
    },
    {
      "epoch": 0.0006544170669648378,
      "grad_norm": 8974.186759812836,
      "learning_rate": 2.356106518402237e-07,
      "loss": 1.7392,
      "step": 180320
    },
    {
      "epoch": 0.0006545332013156744,
      "grad_norm": 11108.898325216593,
      "learning_rate": 2.3558972773531443e-07,
      "loss": 1.7431,
      "step": 180352
    },
    {
      "epoch": 0.0006546493356665112,
      "grad_norm": 10432.360423221582,
      "learning_rate": 2.3556880920409733e-07,
      "loss": 1.721,
      "step": 180384
    },
    {
      "epoch": 0.0006547654700173478,
      "grad_norm": 11005.687620498777,
      "learning_rate": 2.3554789624409838e-07,
      "loss": 1.7053,
      "step": 180416
    },
    {
      "epoch": 0.0006548816043681846,
      "grad_norm": 12044.53220345232,
      "learning_rate": 2.3552698885284505e-07,
      "loss": 1.7241,
      "step": 180448
    },
    {
      "epoch": 0.0006549977387190212,
      "grad_norm": 8183.716759517035,
      "learning_rate": 2.355067401256663e-07,
      "loss": 1.7342,
      "step": 180480
    },
    {
      "epoch": 0.000655113873069858,
      "grad_norm": 8554.884803432482,
      "learning_rate": 2.3548584369066117e-07,
      "loss": 1.7056,
      "step": 180512
    },
    {
      "epoch": 0.0006552300074206947,
      "grad_norm": 8827.497720192285,
      "learning_rate": 2.354649528170704e-07,
      "loss": 1.7243,
      "step": 180544
    },
    {
      "epoch": 0.0006553461417715314,
      "grad_norm": 10381.614806955611,
      "learning_rate": 2.3544406750242752e-07,
      "loss": 1.7595,
      "step": 180576
    },
    {
      "epoch": 0.0006554622761223681,
      "grad_norm": 10573.171898725566,
      "learning_rate": 2.3542318774426765e-07,
      "loss": 1.7654,
      "step": 180608
    },
    {
      "epoch": 0.0006555784104732048,
      "grad_norm": 10794.368994989933,
      "learning_rate": 2.3540231354012742e-07,
      "loss": 1.7743,
      "step": 180640
    },
    {
      "epoch": 0.0006556945448240415,
      "grad_norm": 9622.324251447775,
      "learning_rate": 2.3538144488754492e-07,
      "loss": 1.7546,
      "step": 180672
    },
    {
      "epoch": 0.0006558106791748782,
      "grad_norm": 10439.564262937414,
      "learning_rate": 2.3536058178405991e-07,
      "loss": 1.735,
      "step": 180704
    },
    {
      "epoch": 0.0006559268135257149,
      "grad_norm": 11556.610229647791,
      "learning_rate": 2.3533972422721356e-07,
      "loss": 1.7252,
      "step": 180736
    },
    {
      "epoch": 0.0006560429478765516,
      "grad_norm": 8155.516537902427,
      "learning_rate": 2.3531887221454858e-07,
      "loss": 1.7168,
      "step": 180768
    },
    {
      "epoch": 0.0006561590822273883,
      "grad_norm": 9587.79995619433,
      "learning_rate": 2.3529802574360928e-07,
      "loss": 1.7388,
      "step": 180800
    },
    {
      "epoch": 0.0006562752165782251,
      "grad_norm": 10249.561161337591,
      "learning_rate": 2.3527718481194138e-07,
      "loss": 1.7166,
      "step": 180832
    },
    {
      "epoch": 0.0006563913509290617,
      "grad_norm": 10126.611871697265,
      "learning_rate": 2.3525634941709223e-07,
      "loss": 1.7023,
      "step": 180864
    },
    {
      "epoch": 0.0006565074852798985,
      "grad_norm": 9122.201269430532,
      "learning_rate": 2.352355195566106e-07,
      "loss": 1.7219,
      "step": 180896
    },
    {
      "epoch": 0.0006566236196307351,
      "grad_norm": 10123.34608713937,
      "learning_rate": 2.352146952280469e-07,
      "loss": 1.7331,
      "step": 180928
    },
    {
      "epoch": 0.0006567397539815719,
      "grad_norm": 9486.48153953825,
      "learning_rate": 2.351938764289529e-07,
      "loss": 1.7429,
      "step": 180960
    },
    {
      "epoch": 0.0006568558883324085,
      "grad_norm": 7984.555591891135,
      "learning_rate": 2.3517306315688202e-07,
      "loss": 1.7519,
      "step": 180992
    },
    {
      "epoch": 0.0006569720226832453,
      "grad_norm": 8816.13486738945,
      "learning_rate": 2.3515225540938917e-07,
      "loss": 1.7441,
      "step": 181024
    },
    {
      "epoch": 0.0006570881570340819,
      "grad_norm": 8922.79855202391,
      "learning_rate": 2.3513145318403068e-07,
      "loss": 1.7082,
      "step": 181056
    },
    {
      "epoch": 0.0006572042913849187,
      "grad_norm": 8826.28053032533,
      "learning_rate": 2.351106564783645e-07,
      "loss": 1.7238,
      "step": 181088
    },
    {
      "epoch": 0.0006573204257357554,
      "grad_norm": 10239.584171244456,
      "learning_rate": 2.350898652899501e-07,
      "loss": 1.7307,
      "step": 181120
    },
    {
      "epoch": 0.0006574365600865921,
      "grad_norm": 8920.577896078257,
      "learning_rate": 2.3506907961634836e-07,
      "loss": 1.7477,
      "step": 181152
    },
    {
      "epoch": 0.0006575526944374288,
      "grad_norm": 10738.750020370155,
      "learning_rate": 2.3504829945512176e-07,
      "loss": 1.7453,
      "step": 181184
    },
    {
      "epoch": 0.0006576688287882655,
      "grad_norm": 8880.298418409147,
      "learning_rate": 2.350275248038342e-07,
      "loss": 1.742,
      "step": 181216
    },
    {
      "epoch": 0.0006577849631391022,
      "grad_norm": 9222.651787853643,
      "learning_rate": 2.3500675566005126e-07,
      "loss": 1.7345,
      "step": 181248
    },
    {
      "epoch": 0.0006579010974899389,
      "grad_norm": 8656.99555273075,
      "learning_rate": 2.3498599202133979e-07,
      "loss": 1.7297,
      "step": 181280
    },
    {
      "epoch": 0.0006580172318407756,
      "grad_norm": 9500.805334286142,
      "learning_rate": 2.3496523388526826e-07,
      "loss": 1.7265,
      "step": 181312
    },
    {
      "epoch": 0.0006581333661916123,
      "grad_norm": 8922.688159966143,
      "learning_rate": 2.3494448124940675e-07,
      "loss": 1.7221,
      "step": 181344
    },
    {
      "epoch": 0.000658249500542449,
      "grad_norm": 9976.522039267993,
      "learning_rate": 2.3492373411132663e-07,
      "loss": 1.7324,
      "step": 181376
    },
    {
      "epoch": 0.0006583656348932858,
      "grad_norm": 8942.229140432491,
      "learning_rate": 2.3490299246860096e-07,
      "loss": 1.7126,
      "step": 181408
    },
    {
      "epoch": 0.0006584817692441224,
      "grad_norm": 9147.943266111788,
      "learning_rate": 2.3488225631880414e-07,
      "loss": 1.7148,
      "step": 181440
    },
    {
      "epoch": 0.0006585979035949592,
      "grad_norm": 8986.04495871237,
      "learning_rate": 2.3486217340953092e-07,
      "loss": 1.7425,
      "step": 181472
    },
    {
      "epoch": 0.0006587140379457958,
      "grad_norm": 10271.111332275588,
      "learning_rate": 2.348414480668554e-07,
      "loss": 1.7711,
      "step": 181504
    },
    {
      "epoch": 0.0006588301722966326,
      "grad_norm": 9515.508394195236,
      "learning_rate": 2.3482072820991684e-07,
      "loss": 1.7225,
      "step": 181536
    },
    {
      "epoch": 0.0006589463066474692,
      "grad_norm": 10540.601405991974,
      "learning_rate": 2.3480001383629562e-07,
      "loss": 1.7354,
      "step": 181568
    },
    {
      "epoch": 0.000659062440998306,
      "grad_norm": 9709.818329917403,
      "learning_rate": 2.347793049435737e-07,
      "loss": 1.7517,
      "step": 181600
    },
    {
      "epoch": 0.0006591785753491426,
      "grad_norm": 10563.160322554988,
      "learning_rate": 2.3475860152933442e-07,
      "loss": 1.7388,
      "step": 181632
    },
    {
      "epoch": 0.0006592947096999794,
      "grad_norm": 9870.81394820103,
      "learning_rate": 2.3473790359116276e-07,
      "loss": 1.7265,
      "step": 181664
    },
    {
      "epoch": 0.0006594108440508161,
      "grad_norm": 9067.947066453355,
      "learning_rate": 2.34717211126645e-07,
      "loss": 1.7301,
      "step": 181696
    },
    {
      "epoch": 0.0006595269784016528,
      "grad_norm": 9192.572871617609,
      "learning_rate": 2.3469652413336912e-07,
      "loss": 1.7197,
      "step": 181728
    },
    {
      "epoch": 0.0006596431127524895,
      "grad_norm": 8937.762359785585,
      "learning_rate": 2.3467584260892443e-07,
      "loss": 1.7341,
      "step": 181760
    },
    {
      "epoch": 0.0006597592471033262,
      "grad_norm": 8983.868431805979,
      "learning_rate": 2.3465516655090176e-07,
      "loss": 1.7384,
      "step": 181792
    },
    {
      "epoch": 0.0006598753814541629,
      "grad_norm": 8978.941362989292,
      "learning_rate": 2.346344959568935e-07,
      "loss": 1.7419,
      "step": 181824
    },
    {
      "epoch": 0.0006599915158049996,
      "grad_norm": 8445.698313342717,
      "learning_rate": 2.346138308244934e-07,
      "loss": 1.7386,
      "step": 181856
    },
    {
      "epoch": 0.0006601076501558363,
      "grad_norm": 9040.864560427835,
      "learning_rate": 2.3459317115129687e-07,
      "loss": 1.7313,
      "step": 181888
    },
    {
      "epoch": 0.000660223784506673,
      "grad_norm": 9042.35190644558,
      "learning_rate": 2.345725169349006e-07,
      "loss": 1.7263,
      "step": 181920
    },
    {
      "epoch": 0.0006603399188575097,
      "grad_norm": 8558.212196481225,
      "learning_rate": 2.3455186817290287e-07,
      "loss": 1.7425,
      "step": 181952
    },
    {
      "epoch": 0.0006604560532083465,
      "grad_norm": 8416.863786470587,
      "learning_rate": 2.3453122486290345e-07,
      "loss": 1.746,
      "step": 181984
    },
    {
      "epoch": 0.0006605721875591831,
      "grad_norm": 10080.812665653499,
      "learning_rate": 2.3451058700250355e-07,
      "loss": 1.7294,
      "step": 182016
    },
    {
      "epoch": 0.0006606883219100199,
      "grad_norm": 10225.90944610796,
      "learning_rate": 2.344899545893059e-07,
      "loss": 1.739,
      "step": 182048
    },
    {
      "epoch": 0.0006608044562608565,
      "grad_norm": 9958.822219519736,
      "learning_rate": 2.3446932762091464e-07,
      "loss": 1.7209,
      "step": 182080
    },
    {
      "epoch": 0.0006609205906116933,
      "grad_norm": 10149.347171123865,
      "learning_rate": 2.344487060949354e-07,
      "loss": 1.7107,
      "step": 182112
    },
    {
      "epoch": 0.0006610367249625299,
      "grad_norm": 10318.748179890814,
      "learning_rate": 2.3442809000897537e-07,
      "loss": 1.723,
      "step": 182144
    },
    {
      "epoch": 0.0006611528593133667,
      "grad_norm": 9297.764785151321,
      "learning_rate": 2.3440747936064313e-07,
      "loss": 1.7256,
      "step": 182176
    },
    {
      "epoch": 0.0006612689936642033,
      "grad_norm": 9175.376395549121,
      "learning_rate": 2.3438687414754876e-07,
      "loss": 1.7037,
      "step": 182208
    },
    {
      "epoch": 0.0006613851280150401,
      "grad_norm": 8921.057672720202,
      "learning_rate": 2.3436627436730375e-07,
      "loss": 1.7147,
      "step": 182240
    },
    {
      "epoch": 0.0006615012623658768,
      "grad_norm": 9713.977764026435,
      "learning_rate": 2.3434568001752114e-07,
      "loss": 1.7358,
      "step": 182272
    },
    {
      "epoch": 0.0006616173967167135,
      "grad_norm": 9608.663382593855,
      "learning_rate": 2.343250910958154e-07,
      "loss": 1.746,
      "step": 182304
    },
    {
      "epoch": 0.0006617335310675502,
      "grad_norm": 8911.907315496499,
      "learning_rate": 2.3430450759980252e-07,
      "loss": 1.7674,
      "step": 182336
    },
    {
      "epoch": 0.0006618496654183869,
      "grad_norm": 9275.225819353403,
      "learning_rate": 2.3428392952709986e-07,
      "loss": 1.778,
      "step": 182368
    },
    {
      "epoch": 0.0006619657997692236,
      "grad_norm": 9436.942725268602,
      "learning_rate": 2.342633568753263e-07,
      "loss": 1.7512,
      "step": 182400
    },
    {
      "epoch": 0.0006620819341200603,
      "grad_norm": 9775.66908196058,
      "learning_rate": 2.3424278964210218e-07,
      "loss": 1.7263,
      "step": 182432
    },
    {
      "epoch": 0.000662198068470897,
      "grad_norm": 10920.406494265679,
      "learning_rate": 2.3422287029987277e-07,
      "loss": 1.7333,
      "step": 182464
    },
    {
      "epoch": 0.0006623142028217337,
      "grad_norm": 9157.059134896968,
      "learning_rate": 2.3420231372746932e-07,
      "loss": 1.7387,
      "step": 182496
    },
    {
      "epoch": 0.0006624303371725704,
      "grad_norm": 9190.643394235247,
      "learning_rate": 2.3418176256655925e-07,
      "loss": 1.7196,
      "step": 182528
    },
    {
      "epoch": 0.0006625464715234072,
      "grad_norm": 9942.588998847332,
      "learning_rate": 2.3416121681476882e-07,
      "loss": 1.7074,
      "step": 182560
    },
    {
      "epoch": 0.0006626626058742438,
      "grad_norm": 9530.760410376497,
      "learning_rate": 2.341406764697255e-07,
      "loss": 1.7152,
      "step": 182592
    },
    {
      "epoch": 0.0006627787402250806,
      "grad_norm": 9298.41298286971,
      "learning_rate": 2.341201415290584e-07,
      "loss": 1.7357,
      "step": 182624
    },
    {
      "epoch": 0.0006628948745759172,
      "grad_norm": 10663.527934037591,
      "learning_rate": 2.3409961199039804e-07,
      "loss": 1.732,
      "step": 182656
    },
    {
      "epoch": 0.000663011008926754,
      "grad_norm": 9098.441404988,
      "learning_rate": 2.3407908785137633e-07,
      "loss": 1.7356,
      "step": 182688
    },
    {
      "epoch": 0.0006631271432775906,
      "grad_norm": 9333.40741637265,
      "learning_rate": 2.340585691096267e-07,
      "loss": 1.747,
      "step": 182720
    },
    {
      "epoch": 0.0006632432776284274,
      "grad_norm": 8815.340719450383,
      "learning_rate": 2.3403805576278401e-07,
      "loss": 1.7254,
      "step": 182752
    },
    {
      "epoch": 0.000663359411979264,
      "grad_norm": 8172.093489430967,
      "learning_rate": 2.340175478084846e-07,
      "loss": 1.7216,
      "step": 182784
    },
    {
      "epoch": 0.0006634755463301008,
      "grad_norm": 9515.339930869523,
      "learning_rate": 2.339970452443662e-07,
      "loss": 1.7213,
      "step": 182816
    },
    {
      "epoch": 0.0006635916806809376,
      "grad_norm": 10253.962551131148,
      "learning_rate": 2.3397654806806805e-07,
      "loss": 1.7411,
      "step": 182848
    },
    {
      "epoch": 0.0006637078150317742,
      "grad_norm": 10322.16237035632,
      "learning_rate": 2.3395605627723079e-07,
      "loss": 1.728,
      "step": 182880
    },
    {
      "epoch": 0.000663823949382611,
      "grad_norm": 9341.200244079986,
      "learning_rate": 2.3393556986949656e-07,
      "loss": 1.7276,
      "step": 182912
    },
    {
      "epoch": 0.0006639400837334476,
      "grad_norm": 9571.341494273413,
      "learning_rate": 2.3391508884250883e-07,
      "loss": 1.7675,
      "step": 182944
    },
    {
      "epoch": 0.0006640562180842844,
      "grad_norm": 9674.757774745578,
      "learning_rate": 2.3389461319391267e-07,
      "loss": 1.7421,
      "step": 182976
    },
    {
      "epoch": 0.000664172352435121,
      "grad_norm": 9614.681689998895,
      "learning_rate": 2.3387414292135448e-07,
      "loss": 1.7355,
      "step": 183008
    },
    {
      "epoch": 0.0006642884867859578,
      "grad_norm": 7904.2021735276985,
      "learning_rate": 2.3385367802248215e-07,
      "loss": 1.7233,
      "step": 183040
    },
    {
      "epoch": 0.0006644046211367944,
      "grad_norm": 9006.14368084365,
      "learning_rate": 2.33833218494945e-07,
      "loss": 1.716,
      "step": 183072
    },
    {
      "epoch": 0.0006645207554876312,
      "grad_norm": 11061.212953379028,
      "learning_rate": 2.3381276433639384e-07,
      "loss": 1.7081,
      "step": 183104
    },
    {
      "epoch": 0.0006646368898384679,
      "grad_norm": 11461.633914935515,
      "learning_rate": 2.3379231554448076e-07,
      "loss": 1.7154,
      "step": 183136
    },
    {
      "epoch": 0.0006647530241893046,
      "grad_norm": 9303.409697524881,
      "learning_rate": 2.3377187211685944e-07,
      "loss": 1.7358,
      "step": 183168
    },
    {
      "epoch": 0.0006648691585401413,
      "grad_norm": 9339.989079222738,
      "learning_rate": 2.3375143405118498e-07,
      "loss": 1.7462,
      "step": 183200
    },
    {
      "epoch": 0.000664985292890978,
      "grad_norm": 10527.352563679056,
      "learning_rate": 2.3373100134511388e-07,
      "loss": 1.7282,
      "step": 183232
    },
    {
      "epoch": 0.0006651014272418147,
      "grad_norm": 9308.54897392714,
      "learning_rate": 2.3371057399630404e-07,
      "loss": 1.7284,
      "step": 183264
    },
    {
      "epoch": 0.0006652175615926514,
      "grad_norm": 10234.352153409614,
      "learning_rate": 2.3369015200241486e-07,
      "loss": 1.7508,
      "step": 183296
    },
    {
      "epoch": 0.0006653336959434881,
      "grad_norm": 9694.528972570044,
      "learning_rate": 2.336697353611071e-07,
      "loss": 1.7515,
      "step": 183328
    },
    {
      "epoch": 0.0006654498302943248,
      "grad_norm": 9730.005960943703,
      "learning_rate": 2.3364932407004307e-07,
      "loss": 1.7484,
      "step": 183360
    },
    {
      "epoch": 0.0006655659646451615,
      "grad_norm": 10144.22554954295,
      "learning_rate": 2.3362891812688638e-07,
      "loss": 1.7276,
      "step": 183392
    },
    {
      "epoch": 0.0006656820989959983,
      "grad_norm": 9013.668620489661,
      "learning_rate": 2.336085175293021e-07,
      "loss": 1.7043,
      "step": 183424
    },
    {
      "epoch": 0.0006657982333468349,
      "grad_norm": 8686.308306754947,
      "learning_rate": 2.335881222749568e-07,
      "loss": 1.7151,
      "step": 183456
    },
    {
      "epoch": 0.0006659143676976717,
      "grad_norm": 10920.292303780152,
      "learning_rate": 2.335683694654927e-07,
      "loss": 1.7269,
      "step": 183488
    },
    {
      "epoch": 0.0006660305020485083,
      "grad_norm": 9354.963922966244,
      "learning_rate": 2.3354798472383533e-07,
      "loss": 1.7576,
      "step": 183520
    },
    {
      "epoch": 0.0006661466363993451,
      "grad_norm": 9940.023943633134,
      "learning_rate": 2.3352760531849778e-07,
      "loss": 1.7434,
      "step": 183552
    },
    {
      "epoch": 0.0006662627707501817,
      "grad_norm": 9655.489008848801,
      "learning_rate": 2.3350723124715222e-07,
      "loss": 1.7211,
      "step": 183584
    },
    {
      "epoch": 0.0006663789051010185,
      "grad_norm": 9007.51075492003,
      "learning_rate": 2.3348686250747222e-07,
      "loss": 1.7372,
      "step": 183616
    },
    {
      "epoch": 0.0006664950394518551,
      "grad_norm": 7825.079168928581,
      "learning_rate": 2.3346649909713283e-07,
      "loss": 1.7551,
      "step": 183648
    },
    {
      "epoch": 0.0006666111738026919,
      "grad_norm": 9443.488126746386,
      "learning_rate": 2.3344614101381047e-07,
      "loss": 1.7277,
      "step": 183680
    },
    {
      "epoch": 0.0006667273081535286,
      "grad_norm": 10169.8289071154,
      "learning_rate": 2.3342578825518296e-07,
      "loss": 1.7373,
      "step": 183712
    },
    {
      "epoch": 0.0006668434425043653,
      "grad_norm": 9664.472153201126,
      "learning_rate": 2.3340544081892958e-07,
      "loss": 1.7502,
      "step": 183744
    },
    {
      "epoch": 0.000666959576855202,
      "grad_norm": 9090.013421332225,
      "learning_rate": 2.3338509870273104e-07,
      "loss": 1.713,
      "step": 183776
    },
    {
      "epoch": 0.0006670757112060387,
      "grad_norm": 8411.235462166067,
      "learning_rate": 2.333647619042694e-07,
      "loss": 1.7188,
      "step": 183808
    },
    {
      "epoch": 0.0006671918455568754,
      "grad_norm": 8408.34169143952,
      "learning_rate": 2.333444304212282e-07,
      "loss": 1.7087,
      "step": 183840
    },
    {
      "epoch": 0.0006673079799077121,
      "grad_norm": 9973.276191904042,
      "learning_rate": 2.3332410425129232e-07,
      "loss": 1.7292,
      "step": 183872
    },
    {
      "epoch": 0.0006674241142585488,
      "grad_norm": 9352.822889374096,
      "learning_rate": 2.3330378339214808e-07,
      "loss": 1.7142,
      "step": 183904
    },
    {
      "epoch": 0.0006675402486093855,
      "grad_norm": 10587.422915894122,
      "learning_rate": 2.332834678414833e-07,
      "loss": 1.6935,
      "step": 183936
    },
    {
      "epoch": 0.0006676563829602222,
      "grad_norm": 9567.58611144943,
      "learning_rate": 2.332631575969871e-07,
      "loss": 1.7298,
      "step": 183968
    },
    {
      "epoch": 0.000667772517311059,
      "grad_norm": 9914.210003827839,
      "learning_rate": 2.3324285265635e-07,
      "loss": 1.7277,
      "step": 184000
    },
    {
      "epoch": 0.0006678886516618956,
      "grad_norm": 10208.705500698901,
      "learning_rate": 2.33222553017264e-07,
      "loss": 1.7409,
      "step": 184032
    },
    {
      "epoch": 0.0006680047860127324,
      "grad_norm": 9129.31454162907,
      "learning_rate": 2.332022586774225e-07,
      "loss": 1.7525,
      "step": 184064
    },
    {
      "epoch": 0.000668120920363569,
      "grad_norm": 8615.56684147944,
      "learning_rate": 2.331819696345202e-07,
      "loss": 1.7619,
      "step": 184096
    },
    {
      "epoch": 0.0006682370547144058,
      "grad_norm": 9531.327294768551,
      "learning_rate": 2.3316168588625335e-07,
      "loss": 1.7444,
      "step": 184128
    },
    {
      "epoch": 0.0006683531890652424,
      "grad_norm": 9849.00482282347,
      "learning_rate": 2.3314140743031955e-07,
      "loss": 1.7458,
      "step": 184160
    },
    {
      "epoch": 0.0006684693234160792,
      "grad_norm": 9396.298207272905,
      "learning_rate": 2.331211342644177e-07,
      "loss": 1.7408,
      "step": 184192
    },
    {
      "epoch": 0.0006685854577669158,
      "grad_norm": 11499.842607618592,
      "learning_rate": 2.3310086638624828e-07,
      "loss": 1.7495,
      "step": 184224
    },
    {
      "epoch": 0.0006687015921177526,
      "grad_norm": 9930.806613765066,
      "learning_rate": 2.33080603793513e-07,
      "loss": 1.7037,
      "step": 184256
    },
    {
      "epoch": 0.0006688177264685893,
      "grad_norm": 8644.70635707194,
      "learning_rate": 2.3306034648391507e-07,
      "loss": 1.7057,
      "step": 184288
    },
    {
      "epoch": 0.000668933860819426,
      "grad_norm": 9700.863466722949,
      "learning_rate": 2.3304009445515908e-07,
      "loss": 1.7386,
      "step": 184320
    },
    {
      "epoch": 0.0006690499951702627,
      "grad_norm": 10090.279084346479,
      "learning_rate": 2.33019847704951e-07,
      "loss": 1.7383,
      "step": 184352
    },
    {
      "epoch": 0.0006691661295210994,
      "grad_norm": 9611.543268383075,
      "learning_rate": 2.3299960623099823e-07,
      "loss": 1.7392,
      "step": 184384
    },
    {
      "epoch": 0.0006692822638719361,
      "grad_norm": 8749.473698457527,
      "learning_rate": 2.3297937003100948e-07,
      "loss": 1.7299,
      "step": 184416
    },
    {
      "epoch": 0.0006693983982227728,
      "grad_norm": 9244.462234224336,
      "learning_rate": 2.3295913910269493e-07,
      "loss": 1.7193,
      "step": 184448
    },
    {
      "epoch": 0.0006695145325736095,
      "grad_norm": 8851.06456873974,
      "learning_rate": 2.3293954541586918e-07,
      "loss": 1.728,
      "step": 184480
    },
    {
      "epoch": 0.0006696306669244462,
      "grad_norm": 8806.713916098332,
      "learning_rate": 2.3291932485947687e-07,
      "loss": 1.7367,
      "step": 184512
    },
    {
      "epoch": 0.0006697468012752829,
      "grad_norm": 8841.825603346857,
      "learning_rate": 2.32899109567969e-07,
      "loss": 1.7331,
      "step": 184544
    },
    {
      "epoch": 0.0006698629356261197,
      "grad_norm": 8550.744996782443,
      "learning_rate": 2.328788995390612e-07,
      "loss": 1.7284,
      "step": 184576
    },
    {
      "epoch": 0.0006699790699769563,
      "grad_norm": 10237.150287067197,
      "learning_rate": 2.328586947704706e-07,
      "loss": 1.7144,
      "step": 184608
    },
    {
      "epoch": 0.0006700952043277931,
      "grad_norm": 10122.586230800902,
      "learning_rate": 2.328384952599156e-07,
      "loss": 1.7421,
      "step": 184640
    },
    {
      "epoch": 0.0006702113386786297,
      "grad_norm": 10188.075186216483,
      "learning_rate": 2.3281830100511605e-07,
      "loss": 1.7509,
      "step": 184672
    },
    {
      "epoch": 0.0006703274730294665,
      "grad_norm": 9489.595354913718,
      "learning_rate": 2.3279811200379317e-07,
      "loss": 1.7439,
      "step": 184704
    },
    {
      "epoch": 0.0006704436073803031,
      "grad_norm": 10651.03440985898,
      "learning_rate": 2.3277792825366958e-07,
      "loss": 1.7439,
      "step": 184736
    },
    {
      "epoch": 0.0006705597417311399,
      "grad_norm": 7194.052404590892,
      "learning_rate": 2.3275774975246922e-07,
      "loss": 1.732,
      "step": 184768
    },
    {
      "epoch": 0.0006706758760819765,
      "grad_norm": 8380.658923974892,
      "learning_rate": 2.327375764979175e-07,
      "loss": 1.6948,
      "step": 184800
    },
    {
      "epoch": 0.0006707920104328133,
      "grad_norm": 10900.885835564008,
      "learning_rate": 2.327174084877411e-07,
      "loss": 1.7129,
      "step": 184832
    },
    {
      "epoch": 0.00067090814478365,
      "grad_norm": 11141.733078834728,
      "learning_rate": 2.326972457196682e-07,
      "loss": 1.715,
      "step": 184864
    },
    {
      "epoch": 0.0006710242791344867,
      "grad_norm": 7895.111905476704,
      "learning_rate": 2.3267708819142827e-07,
      "loss": 1.7389,
      "step": 184896
    },
    {
      "epoch": 0.0006711404134853234,
      "grad_norm": 8612.941077239528,
      "learning_rate": 2.3265693590075217e-07,
      "loss": 1.74,
      "step": 184928
    },
    {
      "epoch": 0.0006712565478361601,
      "grad_norm": 10222.056348895754,
      "learning_rate": 2.326367888453722e-07,
      "loss": 1.719,
      "step": 184960
    },
    {
      "epoch": 0.0006713726821869968,
      "grad_norm": 9029.184237792471,
      "learning_rate": 2.32616647023022e-07,
      "loss": 1.7397,
      "step": 184992
    },
    {
      "epoch": 0.0006714888165378335,
      "grad_norm": 8685.708376407763,
      "learning_rate": 2.3259651043143647e-07,
      "loss": 1.7488,
      "step": 185024
    },
    {
      "epoch": 0.0006716049508886702,
      "grad_norm": 10449.876267210057,
      "learning_rate": 2.3257637906835204e-07,
      "loss": 1.7458,
      "step": 185056
    },
    {
      "epoch": 0.0006717210852395069,
      "grad_norm": 9077.087858999714,
      "learning_rate": 2.3255625293150648e-07,
      "loss": 1.7504,
      "step": 185088
    },
    {
      "epoch": 0.0006718372195903436,
      "grad_norm": 9467.668984496659,
      "learning_rate": 2.325361320186389e-07,
      "loss": 1.7313,
      "step": 185120
    },
    {
      "epoch": 0.0006719533539411804,
      "grad_norm": 8373.55253163196,
      "learning_rate": 2.3251601632748975e-07,
      "loss": 1.6988,
      "step": 185152
    },
    {
      "epoch": 0.000672069488292017,
      "grad_norm": 9115.662455356714,
      "learning_rate": 2.3249590585580084e-07,
      "loss": 1.7166,
      "step": 185184
    },
    {
      "epoch": 0.0006721856226428538,
      "grad_norm": 8959.837944963067,
      "learning_rate": 2.3247580060131548e-07,
      "loss": 1.741,
      "step": 185216
    },
    {
      "epoch": 0.0006723017569936904,
      "grad_norm": 8541.66107967297,
      "learning_rate": 2.324557005617782e-07,
      "loss": 1.7453,
      "step": 185248
    },
    {
      "epoch": 0.0006724178913445272,
      "grad_norm": 10407.25112601786,
      "learning_rate": 2.3243560573493491e-07,
      "loss": 1.7275,
      "step": 185280
    },
    {
      "epoch": 0.0006725340256953638,
      "grad_norm": 8339.887049594856,
      "learning_rate": 2.3241551611853302e-07,
      "loss": 1.7358,
      "step": 185312
    },
    {
      "epoch": 0.0006726501600462006,
      "grad_norm": 10103.660326832054,
      "learning_rate": 2.3239543171032115e-07,
      "loss": 1.7555,
      "step": 185344
    },
    {
      "epoch": 0.0006727662943970372,
      "grad_norm": 9513.229945712445,
      "learning_rate": 2.323753525080493e-07,
      "loss": 1.7503,
      "step": 185376
    },
    {
      "epoch": 0.000672882428747874,
      "grad_norm": 9725.112338682777,
      "learning_rate": 2.323552785094689e-07,
      "loss": 1.7444,
      "step": 185408
    },
    {
      "epoch": 0.0006729985630987107,
      "grad_norm": 9281.442237066392,
      "learning_rate": 2.3233520971233275e-07,
      "loss": 1.7339,
      "step": 185440
    },
    {
      "epoch": 0.0006731146974495474,
      "grad_norm": 8434.781087852843,
      "learning_rate": 2.3231514611439487e-07,
      "loss": 1.7186,
      "step": 185472
    },
    {
      "epoch": 0.0006732308318003841,
      "grad_norm": 9049.307708327748,
      "learning_rate": 2.3229571445979902e-07,
      "loss": 1.7341,
      "step": 185504
    },
    {
      "epoch": 0.0006733469661512208,
      "grad_norm": 9109.31391488953,
      "learning_rate": 2.322756610912248e-07,
      "loss": 1.7128,
      "step": 185536
    },
    {
      "epoch": 0.0006734631005020575,
      "grad_norm": 10554.415947839085,
      "learning_rate": 2.3225561291518943e-07,
      "loss": 1.7255,
      "step": 185568
    },
    {
      "epoch": 0.0006735792348528942,
      "grad_norm": 9719.221162212536,
      "learning_rate": 2.3223556992945242e-07,
      "loss": 1.7095,
      "step": 185600
    },
    {
      "epoch": 0.0006736953692037309,
      "grad_norm": 9176.941538442969,
      "learning_rate": 2.3221553213177455e-07,
      "loss": 1.7043,
      "step": 185632
    },
    {
      "epoch": 0.0006738115035545676,
      "grad_norm": 11869.042926874938,
      "learning_rate": 2.3219549951991804e-07,
      "loss": 1.7233,
      "step": 185664
    },
    {
      "epoch": 0.0006739276379054043,
      "grad_norm": 10182.966365455599,
      "learning_rate": 2.321754720916465e-07,
      "loss": 1.7194,
      "step": 185696
    },
    {
      "epoch": 0.0006740437722562411,
      "grad_norm": 8828.523998947956,
      "learning_rate": 2.321554498447248e-07,
      "loss": 1.7282,
      "step": 185728
    },
    {
      "epoch": 0.0006741599066070777,
      "grad_norm": 9184.076436964144,
      "learning_rate": 2.3213543277691917e-07,
      "loss": 1.7329,
      "step": 185760
    },
    {
      "epoch": 0.0006742760409579145,
      "grad_norm": 10524.675766977338,
      "learning_rate": 2.3211542088599723e-07,
      "loss": 1.755,
      "step": 185792
    },
    {
      "epoch": 0.0006743921753087511,
      "grad_norm": 9395.360663646712,
      "learning_rate": 2.3209541416972791e-07,
      "loss": 1.7188,
      "step": 185824
    },
    {
      "epoch": 0.0006745083096595879,
      "grad_norm": 9387.464300864212,
      "learning_rate": 2.320754126258815e-07,
      "loss": 1.7363,
      "step": 185856
    },
    {
      "epoch": 0.0006746244440104245,
      "grad_norm": 7949.671125273045,
      "learning_rate": 2.3205541625222965e-07,
      "loss": 1.7593,
      "step": 185888
    },
    {
      "epoch": 0.0006747405783612613,
      "grad_norm": 8956.141356633447,
      "learning_rate": 2.3203542504654534e-07,
      "loss": 1.7695,
      "step": 185920
    },
    {
      "epoch": 0.0006748567127120979,
      "grad_norm": 10503.657839057782,
      "learning_rate": 2.320154390066029e-07,
      "loss": 1.7192,
      "step": 185952
    },
    {
      "epoch": 0.0006749728470629347,
      "grad_norm": 9174.736944457864,
      "learning_rate": 2.3199545813017793e-07,
      "loss": 1.7224,
      "step": 185984
    },
    {
      "epoch": 0.0006750889814137714,
      "grad_norm": 9003.946912326837,
      "learning_rate": 2.3197548241504753e-07,
      "loss": 1.7315,
      "step": 186016
    },
    {
      "epoch": 0.0006752051157646081,
      "grad_norm": 9479.749469263415,
      "learning_rate": 2.3195551185899e-07,
      "loss": 1.7278,
      "step": 186048
    },
    {
      "epoch": 0.0006753212501154448,
      "grad_norm": 10660.186302311982,
      "learning_rate": 2.31935546459785e-07,
      "loss": 1.7431,
      "step": 186080
    },
    {
      "epoch": 0.0006754373844662815,
      "grad_norm": 9936.750072332503,
      "learning_rate": 2.3191558621521358e-07,
      "loss": 1.733,
      "step": 186112
    },
    {
      "epoch": 0.0006755535188171182,
      "grad_norm": 10150.68549409349,
      "learning_rate": 2.3189563112305812e-07,
      "loss": 1.7256,
      "step": 186144
    },
    {
      "epoch": 0.0006756696531679549,
      "grad_norm": 10979.764114041795,
      "learning_rate": 2.3187568118110225e-07,
      "loss": 1.7068,
      "step": 186176
    },
    {
      "epoch": 0.0006757857875187916,
      "grad_norm": 9323.447002048117,
      "learning_rate": 2.3185573638713104e-07,
      "loss": 1.7229,
      "step": 186208
    },
    {
      "epoch": 0.0006759019218696283,
      "grad_norm": 9775.702532299149,
      "learning_rate": 2.3183579673893086e-07,
      "loss": 1.7459,
      "step": 186240
    },
    {
      "epoch": 0.000676018056220465,
      "grad_norm": 8691.996663598073,
      "learning_rate": 2.3181586223428936e-07,
      "loss": 1.746,
      "step": 186272
    },
    {
      "epoch": 0.0006761341905713018,
      "grad_norm": 8961.726396180593,
      "learning_rate": 2.317959328709956e-07,
      "loss": 1.7047,
      "step": 186304
    },
    {
      "epoch": 0.0006762503249221384,
      "grad_norm": 9903.754035717971,
      "learning_rate": 2.317760086468399e-07,
      "loss": 1.7254,
      "step": 186336
    },
    {
      "epoch": 0.0006763664592729752,
      "grad_norm": 9672.138129700175,
      "learning_rate": 2.31756089559614e-07,
      "loss": 1.7558,
      "step": 186368
    },
    {
      "epoch": 0.0006764825936238118,
      "grad_norm": 9522.03360632591,
      "learning_rate": 2.3173617560711087e-07,
      "loss": 1.7343,
      "step": 186400
    },
    {
      "epoch": 0.0006765987279746486,
      "grad_norm": 8998.094353806255,
      "learning_rate": 2.3171626678712484e-07,
      "loss": 1.7381,
      "step": 186432
    },
    {
      "epoch": 0.0006767148623254852,
      "grad_norm": 9450.058412517883,
      "learning_rate": 2.3169636309745162e-07,
      "loss": 1.7467,
      "step": 186464
    },
    {
      "epoch": 0.000676830996676322,
      "grad_norm": 9710.994387806018,
      "learning_rate": 2.3167708628833611e-07,
      "loss": 1.7204,
      "step": 186496
    },
    {
      "epoch": 0.0006769471310271586,
      "grad_norm": 9069.90319683733,
      "learning_rate": 2.3165719269252946e-07,
      "loss": 1.7144,
      "step": 186528
    },
    {
      "epoch": 0.0006770632653779954,
      "grad_norm": 8495.001118304812,
      "learning_rate": 2.3163730422049924e-07,
      "loss": 1.711,
      "step": 186560
    },
    {
      "epoch": 0.0006771793997288321,
      "grad_norm": 9464.847595180812,
      "learning_rate": 2.3161742087004643e-07,
      "loss": 1.7274,
      "step": 186592
    },
    {
      "epoch": 0.0006772955340796688,
      "grad_norm": 9107.469132530727,
      "learning_rate": 2.3159754263897324e-07,
      "loss": 1.7152,
      "step": 186624
    },
    {
      "epoch": 0.0006774116684305055,
      "grad_norm": 8943.683133921953,
      "learning_rate": 2.315776695250832e-07,
      "loss": 1.7304,
      "step": 186656
    },
    {
      "epoch": 0.0006775278027813422,
      "grad_norm": 10039.315713732685,
      "learning_rate": 2.3155780152618127e-07,
      "loss": 1.7367,
      "step": 186688
    },
    {
      "epoch": 0.000677643937132179,
      "grad_norm": 9833.896481049615,
      "learning_rate": 2.3153793864007358e-07,
      "loss": 1.7355,
      "step": 186720
    },
    {
      "epoch": 0.0006777600714830156,
      "grad_norm": 10170.688078984627,
      "learning_rate": 2.315180808645677e-07,
      "loss": 1.7404,
      "step": 186752
    },
    {
      "epoch": 0.0006778762058338523,
      "grad_norm": 8553.36191213724,
      "learning_rate": 2.314982281974724e-07,
      "loss": 1.7477,
      "step": 186784
    },
    {
      "epoch": 0.000677992340184689,
      "grad_norm": 10448.441414871406,
      "learning_rate": 2.3147838063659786e-07,
      "loss": 1.7321,
      "step": 186816
    },
    {
      "epoch": 0.0006781084745355257,
      "grad_norm": 10005.293998678899,
      "learning_rate": 2.3145853817975553e-07,
      "loss": 1.7277,
      "step": 186848
    },
    {
      "epoch": 0.0006782246088863625,
      "grad_norm": 9418.50221638239,
      "learning_rate": 2.314387008247582e-07,
      "loss": 1.7234,
      "step": 186880
    },
    {
      "epoch": 0.0006783407432371991,
      "grad_norm": 8298.541920120666,
      "learning_rate": 2.3141886856941995e-07,
      "loss": 1.7263,
      "step": 186912
    },
    {
      "epoch": 0.0006784568775880359,
      "grad_norm": 10740.476525741304,
      "learning_rate": 2.3139904141155614e-07,
      "loss": 1.7371,
      "step": 186944
    },
    {
      "epoch": 0.0006785730119388725,
      "grad_norm": 8881.93357327108,
      "learning_rate": 2.3137921934898353e-07,
      "loss": 1.7167,
      "step": 186976
    },
    {
      "epoch": 0.0006786891462897093,
      "grad_norm": 8396.697207831185,
      "learning_rate": 2.313594023795201e-07,
      "loss": 1.7289,
      "step": 187008
    },
    {
      "epoch": 0.000678805280640546,
      "grad_norm": 9641.479761945258,
      "learning_rate": 2.3133959050098515e-07,
      "loss": 1.7562,
      "step": 187040
    },
    {
      "epoch": 0.0006789214149913827,
      "grad_norm": 8292.540623958377,
      "learning_rate": 2.3131978371119937e-07,
      "loss": 1.7533,
      "step": 187072
    },
    {
      "epoch": 0.0006790375493422193,
      "grad_norm": 10621.75823486865,
      "learning_rate": 2.3129998200798466e-07,
      "loss": 1.7541,
      "step": 187104
    },
    {
      "epoch": 0.0006791536836930561,
      "grad_norm": 9428.915738302045,
      "learning_rate": 2.3128018538916426e-07,
      "loss": 1.7466,
      "step": 187136
    },
    {
      "epoch": 0.0006792698180438929,
      "grad_norm": 9845.399737948683,
      "learning_rate": 2.312603938525627e-07,
      "loss": 1.7193,
      "step": 187168
    },
    {
      "epoch": 0.0006793859523947295,
      "grad_norm": 8289.57562243086,
      "learning_rate": 2.3124060739600585e-07,
      "loss": 1.7145,
      "step": 187200
    },
    {
      "epoch": 0.0006795020867455663,
      "grad_norm": 9177.734143022448,
      "learning_rate": 2.3122082601732088e-07,
      "loss": 1.7343,
      "step": 187232
    },
    {
      "epoch": 0.0006796182210964029,
      "grad_norm": 9968.131921277927,
      "learning_rate": 2.3120104971433618e-07,
      "loss": 1.7321,
      "step": 187264
    },
    {
      "epoch": 0.0006797343554472397,
      "grad_norm": 8494.57332654207,
      "learning_rate": 2.3118127848488154e-07,
      "loss": 1.7116,
      "step": 187296
    },
    {
      "epoch": 0.0006798504897980763,
      "grad_norm": 9866.301029261169,
      "learning_rate": 2.3116151232678798e-07,
      "loss": 1.6979,
      "step": 187328
    },
    {
      "epoch": 0.0006799666241489131,
      "grad_norm": 10346.157547611578,
      "learning_rate": 2.311417512378879e-07,
      "loss": 1.708,
      "step": 187360
    },
    {
      "epoch": 0.0006800827584997497,
      "grad_norm": 8817.693235761833,
      "learning_rate": 2.311219952160149e-07,
      "loss": 1.7388,
      "step": 187392
    },
    {
      "epoch": 0.0006801988928505865,
      "grad_norm": 10324.741933820911,
      "learning_rate": 2.3110224425900392e-07,
      "loss": 1.7261,
      "step": 187424
    },
    {
      "epoch": 0.0006803150272014232,
      "grad_norm": 9663.445762252717,
      "learning_rate": 2.3108249836469118e-07,
      "loss": 1.7294,
      "step": 187456
    },
    {
      "epoch": 0.0006804311615522599,
      "grad_norm": 8652.531074778062,
      "learning_rate": 2.3106275753091427e-07,
      "loss": 1.7355,
      "step": 187488
    },
    {
      "epoch": 0.0006805472959030966,
      "grad_norm": 8772.195506257258,
      "learning_rate": 2.3104363842194755e-07,
      "loss": 1.7215,
      "step": 187520
    },
    {
      "epoch": 0.0006806634302539333,
      "grad_norm": 8484.515307311314,
      "learning_rate": 2.3102390754478594e-07,
      "loss": 1.7129,
      "step": 187552
    },
    {
      "epoch": 0.00068077956460477,
      "grad_norm": 9619.839291796927,
      "learning_rate": 2.310041817217479e-07,
      "loss": 1.7375,
      "step": 187584
    },
    {
      "epoch": 0.0006808956989556067,
      "grad_norm": 10168.634126567835,
      "learning_rate": 2.3098446095067606e-07,
      "loss": 1.7522,
      "step": 187616
    },
    {
      "epoch": 0.0006810118333064434,
      "grad_norm": 9777.713025038114,
      "learning_rate": 2.3096474522941438e-07,
      "loss": 1.7457,
      "step": 187648
    },
    {
      "epoch": 0.0006811279676572801,
      "grad_norm": 9728.888528501084,
      "learning_rate": 2.3094503455580812e-07,
      "loss": 1.741,
      "step": 187680
    },
    {
      "epoch": 0.0006812441020081168,
      "grad_norm": 9313.830683451359,
      "learning_rate": 2.3092532892770377e-07,
      "loss": 1.7437,
      "step": 187712
    },
    {
      "epoch": 0.0006813602363589536,
      "grad_norm": 8397.08080227885,
      "learning_rate": 2.3090562834294907e-07,
      "loss": 1.7438,
      "step": 187744
    },
    {
      "epoch": 0.0006814763707097902,
      "grad_norm": 7997.634025135184,
      "learning_rate": 2.3088593279939323e-07,
      "loss": 1.7392,
      "step": 187776
    },
    {
      "epoch": 0.000681592505060627,
      "grad_norm": 10419.479257621277,
      "learning_rate": 2.3086624229488656e-07,
      "loss": 1.7309,
      "step": 187808
    },
    {
      "epoch": 0.0006817086394114636,
      "grad_norm": 8544.343274939274,
      "learning_rate": 2.3084655682728073e-07,
      "loss": 1.7335,
      "step": 187840
    },
    {
      "epoch": 0.0006818247737623004,
      "grad_norm": 8722.491387212716,
      "learning_rate": 2.308268763944286e-07,
      "loss": 1.7149,
      "step": 187872
    },
    {
      "epoch": 0.000681940908113137,
      "grad_norm": 9011.717261432474,
      "learning_rate": 2.308072009941845e-07,
      "loss": 1.7189,
      "step": 187904
    },
    {
      "epoch": 0.0006820570424639738,
      "grad_norm": 10659.083075011658,
      "learning_rate": 2.3078753062440392e-07,
      "loss": 1.7294,
      "step": 187936
    },
    {
      "epoch": 0.0006821731768148104,
      "grad_norm": 9475.037308633671,
      "learning_rate": 2.3076786528294355e-07,
      "loss": 1.737,
      "step": 187968
    },
    {
      "epoch": 0.0006822893111656472,
      "grad_norm": 9499.025002598952,
      "learning_rate": 2.3074820496766152e-07,
      "loss": 1.7192,
      "step": 188000
    },
    {
      "epoch": 0.0006824054455164839,
      "grad_norm": 9040.899180944338,
      "learning_rate": 2.3072854967641714e-07,
      "loss": 1.7361,
      "step": 188032
    },
    {
      "epoch": 0.0006825215798673206,
      "grad_norm": 10528.983236761278,
      "learning_rate": 2.30708899407071e-07,
      "loss": 1.7401,
      "step": 188064
    },
    {
      "epoch": 0.0006826377142181573,
      "grad_norm": 8914.38993986689,
      "learning_rate": 2.3068925415748505e-07,
      "loss": 1.7474,
      "step": 188096
    },
    {
      "epoch": 0.000682753848568994,
      "grad_norm": 8878.430604560695,
      "learning_rate": 2.3066961392552237e-07,
      "loss": 1.7233,
      "step": 188128
    },
    {
      "epoch": 0.0006828699829198307,
      "grad_norm": 9203.642974387914,
      "learning_rate": 2.3064997870904743e-07,
      "loss": 1.7346,
      "step": 188160
    },
    {
      "epoch": 0.0006829861172706674,
      "grad_norm": 9166.914857246138,
      "learning_rate": 2.3063034850592594e-07,
      "loss": 1.7131,
      "step": 188192
    },
    {
      "epoch": 0.0006831022516215041,
      "grad_norm": 10587.45805186495,
      "learning_rate": 2.3061072331402487e-07,
      "loss": 1.7243,
      "step": 188224
    },
    {
      "epoch": 0.0006832183859723408,
      "grad_norm": 11077.138439145734,
      "learning_rate": 2.3059110313121248e-07,
      "loss": 1.7399,
      "step": 188256
    },
    {
      "epoch": 0.0006833345203231775,
      "grad_norm": 10185.448443735799,
      "learning_rate": 2.3057148795535822e-07,
      "loss": 1.7305,
      "step": 188288
    },
    {
      "epoch": 0.0006834506546740143,
      "grad_norm": 10631.110760405048,
      "learning_rate": 2.3055187778433294e-07,
      "loss": 1.7161,
      "step": 188320
    },
    {
      "epoch": 0.0006835667890248509,
      "grad_norm": 8950.959948519489,
      "learning_rate": 2.3053227261600873e-07,
      "loss": 1.7064,
      "step": 188352
    },
    {
      "epoch": 0.0006836829233756877,
      "grad_norm": 9808.60458984865,
      "learning_rate": 2.305126724482588e-07,
      "loss": 1.7394,
      "step": 188384
    },
    {
      "epoch": 0.0006837990577265243,
      "grad_norm": 9387.539826812987,
      "learning_rate": 2.3049307727895777e-07,
      "loss": 1.7298,
      "step": 188416
    },
    {
      "epoch": 0.0006839151920773611,
      "grad_norm": 9375.668936134638,
      "learning_rate": 2.3047348710598155e-07,
      "loss": 1.7461,
      "step": 188448
    },
    {
      "epoch": 0.0006840313264281977,
      "grad_norm": 9837.01255463263,
      "learning_rate": 2.3045390192720718e-07,
      "loss": 1.7352,
      "step": 188480
    },
    {
      "epoch": 0.0006841474607790345,
      "grad_norm": 10089.543498097424,
      "learning_rate": 2.3043493354580459e-07,
      "loss": 1.7421,
      "step": 188512
    },
    {
      "epoch": 0.0006842635951298711,
      "grad_norm": 9880.074797287722,
      "learning_rate": 2.3041535819316617e-07,
      "loss": 1.7164,
      "step": 188544
    },
    {
      "epoch": 0.0006843797294807079,
      "grad_norm": 9158.403900243753,
      "learning_rate": 2.3039578782843477e-07,
      "loss": 1.7297,
      "step": 188576
    },
    {
      "epoch": 0.0006844958638315445,
      "grad_norm": 8125.352423126026,
      "learning_rate": 2.3037622244949247e-07,
      "loss": 1.7482,
      "step": 188608
    },
    {
      "epoch": 0.0006846119981823813,
      "grad_norm": 10175.649168480604,
      "learning_rate": 2.3035666205422262e-07,
      "loss": 1.7477,
      "step": 188640
    },
    {
      "epoch": 0.000684728132533218,
      "grad_norm": 10014.927458549064,
      "learning_rate": 2.3033710664050987e-07,
      "loss": 1.7092,
      "step": 188672
    },
    {
      "epoch": 0.0006848442668840547,
      "grad_norm": 9920.53849344883,
      "learning_rate": 2.3031755620624012e-07,
      "loss": 1.7105,
      "step": 188704
    },
    {
      "epoch": 0.0006849604012348914,
      "grad_norm": 9713.223975591214,
      "learning_rate": 2.3029801074930054e-07,
      "loss": 1.7454,
      "step": 188736
    },
    {
      "epoch": 0.0006850765355857281,
      "grad_norm": 9076.787978134114,
      "learning_rate": 2.3027847026757948e-07,
      "loss": 1.7457,
      "step": 188768
    },
    {
      "epoch": 0.0006851926699365648,
      "grad_norm": 10547.58076527504,
      "learning_rate": 2.3025893475896664e-07,
      "loss": 1.7501,
      "step": 188800
    },
    {
      "epoch": 0.0006853088042874015,
      "grad_norm": 9639.584845832314,
      "learning_rate": 2.3023940422135288e-07,
      "loss": 1.7567,
      "step": 188832
    },
    {
      "epoch": 0.0006854249386382382,
      "grad_norm": 9092.092608415292,
      "learning_rate": 2.3021987865263038e-07,
      "loss": 1.748,
      "step": 188864
    },
    {
      "epoch": 0.0006855410729890749,
      "grad_norm": 10198.160226236887,
      "learning_rate": 2.3020035805069258e-07,
      "loss": 1.7249,
      "step": 188896
    },
    {
      "epoch": 0.0006856572073399116,
      "grad_norm": 10418.22643255559,
      "learning_rate": 2.301808424134341e-07,
      "loss": 1.7264,
      "step": 188928
    },
    {
      "epoch": 0.0006857733416907484,
      "grad_norm": 9064.333511075152,
      "learning_rate": 2.3016133173875088e-07,
      "loss": 1.7409,
      "step": 188960
    },
    {
      "epoch": 0.000685889476041585,
      "grad_norm": 8837.38479415715,
      "learning_rate": 2.3014182602454005e-07,
      "loss": 1.7136,
      "step": 188992
    },
    {
      "epoch": 0.0006860056103924218,
      "grad_norm": 9797.043227423263,
      "learning_rate": 2.3012232526870002e-07,
      "loss": 1.7048,
      "step": 189024
    },
    {
      "epoch": 0.0006861217447432584,
      "grad_norm": 11116.37926664973,
      "learning_rate": 2.3010282946913046e-07,
      "loss": 1.7126,
      "step": 189056
    },
    {
      "epoch": 0.0006862378790940952,
      "grad_norm": 8834.251750997364,
      "learning_rate": 2.300833386237323e-07,
      "loss": 1.7205,
      "step": 189088
    },
    {
      "epoch": 0.0006863540134449318,
      "grad_norm": 10768.120355939564,
      "learning_rate": 2.3006385273040758e-07,
      "loss": 1.7233,
      "step": 189120
    },
    {
      "epoch": 0.0006864701477957686,
      "grad_norm": 11226.745209543147,
      "learning_rate": 2.3004437178705977e-07,
      "loss": 1.7245,
      "step": 189152
    },
    {
      "epoch": 0.0006865862821466052,
      "grad_norm": 9279.994612067401,
      "learning_rate": 2.300248957915935e-07,
      "loss": 1.7319,
      "step": 189184
    },
    {
      "epoch": 0.000686702416497442,
      "grad_norm": 9181.61336585243,
      "learning_rate": 2.3000542474191457e-07,
      "loss": 1.7082,
      "step": 189216
    },
    {
      "epoch": 0.0006868185508482787,
      "grad_norm": 9136.690867048092,
      "learning_rate": 2.2998595863593014e-07,
      "loss": 1.7219,
      "step": 189248
    },
    {
      "epoch": 0.0006869346851991154,
      "grad_norm": 9010.883641463804,
      "learning_rate": 2.2996649747154857e-07,
      "loss": 1.7152,
      "step": 189280
    },
    {
      "epoch": 0.0006870508195499521,
      "grad_norm": 9816.228399950767,
      "learning_rate": 2.2994704124667942e-07,
      "loss": 1.7405,
      "step": 189312
    },
    {
      "epoch": 0.0006871669539007888,
      "grad_norm": 9044.037704476912,
      "learning_rate": 2.299275899592335e-07,
      "loss": 1.7277,
      "step": 189344
    },
    {
      "epoch": 0.0006872830882516255,
      "grad_norm": 9930.028197341637,
      "learning_rate": 2.2990814360712295e-07,
      "loss": 1.7283,
      "step": 189376
    },
    {
      "epoch": 0.0006873992226024622,
      "grad_norm": 9663.366494136502,
      "learning_rate": 2.2988870218826097e-07,
      "loss": 1.762,
      "step": 189408
    },
    {
      "epoch": 0.0006875153569532989,
      "grad_norm": 9600.093020382667,
      "learning_rate": 2.298692657005622e-07,
      "loss": 1.768,
      "step": 189440
    },
    {
      "epoch": 0.0006876314913041356,
      "grad_norm": 8885.607463758457,
      "learning_rate": 2.298498341419423e-07,
      "loss": 1.7532,
      "step": 189472
    },
    {
      "epoch": 0.0006877476256549723,
      "grad_norm": 18955.654776345764,
      "learning_rate": 2.2983040751031833e-07,
      "loss": 1.7543,
      "step": 189504
    },
    {
      "epoch": 0.0006878637600058091,
      "grad_norm": 10136.311360647916,
      "learning_rate": 2.2981159265741684e-07,
      "loss": 1.7286,
      "step": 189536
    },
    {
      "epoch": 0.0006879798943566457,
      "grad_norm": 9938.777188366787,
      "learning_rate": 2.2979217571973355e-07,
      "loss": 1.7042,
      "step": 189568
    },
    {
      "epoch": 0.0006880960287074825,
      "grad_norm": 9647.03021660034,
      "learning_rate": 2.2977276370286954e-07,
      "loss": 1.7241,
      "step": 189600
    },
    {
      "epoch": 0.0006882121630583191,
      "grad_norm": 9509.50692728072,
      "learning_rate": 2.297533566047467e-07,
      "loss": 1.7398,
      "step": 189632
    },
    {
      "epoch": 0.0006883282974091559,
      "grad_norm": 10514.692006901583,
      "learning_rate": 2.2973395442328812e-07,
      "loss": 1.7314,
      "step": 189664
    },
    {
      "epoch": 0.0006884444317599925,
      "grad_norm": 8886.840946028009,
      "learning_rate": 2.2971455715641816e-07,
      "loss": 1.7055,
      "step": 189696
    },
    {
      "epoch": 0.0006885605661108293,
      "grad_norm": 8775.646756792345,
      "learning_rate": 2.2969516480206239e-07,
      "loss": 1.723,
      "step": 189728
    },
    {
      "epoch": 0.0006886767004616659,
      "grad_norm": 9223.266558004272,
      "learning_rate": 2.2967577735814756e-07,
      "loss": 1.7482,
      "step": 189760
    },
    {
      "epoch": 0.0006887928348125027,
      "grad_norm": 10128.053317395204,
      "learning_rate": 2.2965639482260177e-07,
      "loss": 1.7553,
      "step": 189792
    },
    {
      "epoch": 0.0006889089691633394,
      "grad_norm": 10515.765497575534,
      "learning_rate": 2.2963701719335419e-07,
      "loss": 1.7336,
      "step": 189824
    },
    {
      "epoch": 0.0006890251035141761,
      "grad_norm": 10864.053755389836,
      "learning_rate": 2.296176444683353e-07,
      "loss": 1.7242,
      "step": 189856
    },
    {
      "epoch": 0.0006891412378650128,
      "grad_norm": 10136.028216219605,
      "learning_rate": 2.295982766454768e-07,
      "loss": 1.7057,
      "step": 189888
    },
    {
      "epoch": 0.0006892573722158495,
      "grad_norm": 7634.404102482395,
      "learning_rate": 2.295789137227116e-07,
      "loss": 1.7039,
      "step": 189920
    },
    {
      "epoch": 0.0006893735065666862,
      "grad_norm": 10375.799728213724,
      "learning_rate": 2.295595556979738e-07,
      "loss": 1.7178,
      "step": 189952
    },
    {
      "epoch": 0.0006894896409175229,
      "grad_norm": 10345.334600678703,
      "learning_rate": 2.295402025691988e-07,
      "loss": 1.745,
      "step": 189984
    },
    {
      "epoch": 0.0006896057752683596,
      "grad_norm": 9273.925598148822,
      "learning_rate": 2.2952085433432312e-07,
      "loss": 1.7311,
      "step": 190016
    },
    {
      "epoch": 0.0006897219096191963,
      "grad_norm": 9735.86113294556,
      "learning_rate": 2.2950151099128457e-07,
      "loss": 1.7139,
      "step": 190048
    },
    {
      "epoch": 0.000689838043970033,
      "grad_norm": 10245.652346239354,
      "learning_rate": 2.2948217253802212e-07,
      "loss": 1.726,
      "step": 190080
    },
    {
      "epoch": 0.0006899541783208698,
      "grad_norm": 8778.786020857326,
      "learning_rate": 2.2946283897247603e-07,
      "loss": 1.7379,
      "step": 190112
    },
    {
      "epoch": 0.0006900703126717064,
      "grad_norm": 8752.126827234624,
      "learning_rate": 2.2944351029258767e-07,
      "loss": 1.7247,
      "step": 190144
    },
    {
      "epoch": 0.0006901864470225432,
      "grad_norm": 8871.350291810148,
      "learning_rate": 2.294241864962997e-07,
      "loss": 1.7333,
      "step": 190176
    },
    {
      "epoch": 0.0006903025813733798,
      "grad_norm": 10910.847446463542,
      "learning_rate": 2.2940486758155602e-07,
      "loss": 1.7406,
      "step": 190208
    },
    {
      "epoch": 0.0006904187157242166,
      "grad_norm": 9943.746678189264,
      "learning_rate": 2.2938555354630166e-07,
      "loss": 1.7002,
      "step": 190240
    },
    {
      "epoch": 0.0006905348500750532,
      "grad_norm": 8429.792049629694,
      "learning_rate": 2.293662443884829e-07,
      "loss": 1.7267,
      "step": 190272
    },
    {
      "epoch": 0.00069065098442589,
      "grad_norm": 9362.399158335431,
      "learning_rate": 2.2934694010604724e-07,
      "loss": 1.7344,
      "step": 190304
    },
    {
      "epoch": 0.0006907671187767266,
      "grad_norm": 9864.683877347516,
      "learning_rate": 2.2932764069694336e-07,
      "loss": 1.7494,
      "step": 190336
    },
    {
      "epoch": 0.0006908832531275634,
      "grad_norm": 9318.641853832563,
      "learning_rate": 2.293083461591212e-07,
      "loss": 1.7329,
      "step": 190368
    },
    {
      "epoch": 0.0006909993874784001,
      "grad_norm": 11222.435564528761,
      "learning_rate": 2.2928905649053182e-07,
      "loss": 1.7258,
      "step": 190400
    },
    {
      "epoch": 0.0006911155218292368,
      "grad_norm": 10235.260817390048,
      "learning_rate": 2.2926977168912756e-07,
      "loss": 1.7342,
      "step": 190432
    },
    {
      "epoch": 0.0006912316561800735,
      "grad_norm": 9299.848600918189,
      "learning_rate": 2.2925049175286194e-07,
      "loss": 1.734,
      "step": 190464
    },
    {
      "epoch": 0.0006913477905309102,
      "grad_norm": 8646.493277624171,
      "learning_rate": 2.2923121667968972e-07,
      "loss": 1.7411,
      "step": 190496
    },
    {
      "epoch": 0.0006914639248817469,
      "grad_norm": 9397.126901345964,
      "learning_rate": 2.292125485881356e-07,
      "loss": 1.7469,
      "step": 190528
    },
    {
      "epoch": 0.0006915800592325836,
      "grad_norm": 9398.087465011165,
      "learning_rate": 2.2919328308320606e-07,
      "loss": 1.7299,
      "step": 190560
    },
    {
      "epoch": 0.0006916961935834203,
      "grad_norm": 8864.181575306318,
      "learning_rate": 2.2917402243530515e-07,
      "loss": 1.7332,
      "step": 190592
    },
    {
      "epoch": 0.000691812327934257,
      "grad_norm": 9014.459051989754,
      "learning_rate": 2.2915476664239224e-07,
      "loss": 1.7532,
      "step": 190624
    },
    {
      "epoch": 0.0006919284622850937,
      "grad_norm": 9447.883466681837,
      "learning_rate": 2.291355157024281e-07,
      "loss": 1.7625,
      "step": 190656
    },
    {
      "epoch": 0.0006920445966359305,
      "grad_norm": 10275.50388058902,
      "learning_rate": 2.2911626961337464e-07,
      "loss": 1.7244,
      "step": 190688
    },
    {
      "epoch": 0.0006921607309867671,
      "grad_norm": 10791.553363626574,
      "learning_rate": 2.2909702837319496e-07,
      "loss": 1.6983,
      "step": 190720
    },
    {
      "epoch": 0.0006922768653376039,
      "grad_norm": 10568.441890837079,
      "learning_rate": 2.290777919798533e-07,
      "loss": 1.713,
      "step": 190752
    },
    {
      "epoch": 0.0006923929996884405,
      "grad_norm": 8476.053916770468,
      "learning_rate": 2.290585604313152e-07,
      "loss": 1.7248,
      "step": 190784
    },
    {
      "epoch": 0.0006925091340392773,
      "grad_norm": 8346.839761250962,
      "learning_rate": 2.290393337255473e-07,
      "loss": 1.7248,
      "step": 190816
    },
    {
      "epoch": 0.0006926252683901139,
      "grad_norm": 10370.68792318041,
      "learning_rate": 2.2902011186051753e-07,
      "loss": 1.7228,
      "step": 190848
    },
    {
      "epoch": 0.0006927414027409507,
      "grad_norm": 10041.30609034502,
      "learning_rate": 2.2900089483419493e-07,
      "loss": 1.7265,
      "step": 190880
    },
    {
      "epoch": 0.0006928575370917873,
      "grad_norm": 8296.00361619979,
      "learning_rate": 2.2898168264454976e-07,
      "loss": 1.7056,
      "step": 190912
    },
    {
      "epoch": 0.0006929736714426241,
      "grad_norm": 9276.225094293475,
      "learning_rate": 2.2896247528955357e-07,
      "loss": 1.7171,
      "step": 190944
    },
    {
      "epoch": 0.0006930898057934609,
      "grad_norm": 8711.614775688833,
      "learning_rate": 2.2894327276717886e-07,
      "loss": 1.7177,
      "step": 190976
    },
    {
      "epoch": 0.0006932059401442975,
      "grad_norm": 9403.58548639826,
      "learning_rate": 2.2892407507539956e-07,
      "loss": 1.7268,
      "step": 191008
    },
    {
      "epoch": 0.0006933220744951343,
      "grad_norm": 9493.174179377518,
      "learning_rate": 2.2890488221219072e-07,
      "loss": 1.7139,
      "step": 191040
    },
    {
      "epoch": 0.0006934382088459709,
      "grad_norm": 9559.987238485206,
      "learning_rate": 2.2888569417552847e-07,
      "loss": 1.7059,
      "step": 191072
    },
    {
      "epoch": 0.0006935543431968077,
      "grad_norm": 9642.745459670705,
      "learning_rate": 2.288665109633903e-07,
      "loss": 1.7434,
      "step": 191104
    },
    {
      "epoch": 0.0006936704775476443,
      "grad_norm": 9087.180750925998,
      "learning_rate": 2.2884733257375476e-07,
      "loss": 1.7465,
      "step": 191136
    },
    {
      "epoch": 0.000693786611898481,
      "grad_norm": 9319.719738275395,
      "learning_rate": 2.288281590046016e-07,
      "loss": 1.7621,
      "step": 191168
    },
    {
      "epoch": 0.0006939027462493177,
      "grad_norm": 9552.342853980903,
      "learning_rate": 2.2880899025391185e-07,
      "loss": 1.7674,
      "step": 191200
    },
    {
      "epoch": 0.0006940188806001544,
      "grad_norm": 9395.727539685258,
      "learning_rate": 2.287898263196676e-07,
      "loss": 1.7532,
      "step": 191232
    },
    {
      "epoch": 0.0006941350149509912,
      "grad_norm": 8412.705391251971,
      "learning_rate": 2.2877066719985214e-07,
      "loss": 1.7276,
      "step": 191264
    },
    {
      "epoch": 0.0006942511493018278,
      "grad_norm": 7946.89952119693,
      "learning_rate": 2.287515128924501e-07,
      "loss": 1.7149,
      "step": 191296
    },
    {
      "epoch": 0.0006943672836526646,
      "grad_norm": 9527.92737167953,
      "learning_rate": 2.2873236339544708e-07,
      "loss": 1.7254,
      "step": 191328
    },
    {
      "epoch": 0.0006944834180035012,
      "grad_norm": 10339.56807608519,
      "learning_rate": 2.2871321870682995e-07,
      "loss": 1.744,
      "step": 191360
    },
    {
      "epoch": 0.000694599552354338,
      "grad_norm": 9254.369346422262,
      "learning_rate": 2.2869407882458677e-07,
      "loss": 1.7204,
      "step": 191392
    },
    {
      "epoch": 0.0006947156867051746,
      "grad_norm": 8853.956290834058,
      "learning_rate": 2.2867494374670682e-07,
      "loss": 1.7113,
      "step": 191424
    },
    {
      "epoch": 0.0006948318210560114,
      "grad_norm": 9523.237159705728,
      "learning_rate": 2.2865581347118044e-07,
      "loss": 1.7284,
      "step": 191456
    },
    {
      "epoch": 0.000694947955406848,
      "grad_norm": 9114.243468330214,
      "learning_rate": 2.2863668799599922e-07,
      "loss": 1.7399,
      "step": 191488
    },
    {
      "epoch": 0.0006950640897576848,
      "grad_norm": 9911.374072246492,
      "learning_rate": 2.2861756731915596e-07,
      "loss": 1.7558,
      "step": 191520
    },
    {
      "epoch": 0.0006951802241085216,
      "grad_norm": 10188.063407733582,
      "learning_rate": 2.2859904873732972e-07,
      "loss": 1.7402,
      "step": 191552
    },
    {
      "epoch": 0.0006952963584593582,
      "grad_norm": 8614.676662533539,
      "learning_rate": 2.285799375013529e-07,
      "loss": 1.6955,
      "step": 191584
    },
    {
      "epoch": 0.000695412492810195,
      "grad_norm": 8630.314478627068,
      "learning_rate": 2.285608310577619e-07,
      "loss": 1.7031,
      "step": 191616
    },
    {
      "epoch": 0.0006955286271610316,
      "grad_norm": 9374.108810975047,
      "learning_rate": 2.285417294045542e-07,
      "loss": 1.7033,
      "step": 191648
    },
    {
      "epoch": 0.0006956447615118684,
      "grad_norm": 9708.772321977687,
      "learning_rate": 2.2852263253972834e-07,
      "loss": 1.7262,
      "step": 191680
    },
    {
      "epoch": 0.000695760895862705,
      "grad_norm": 8345.947399786317,
      "learning_rate": 2.285035404612841e-07,
      "loss": 1.7196,
      "step": 191712
    },
    {
      "epoch": 0.0006958770302135418,
      "grad_norm": 9980.846857857303,
      "learning_rate": 2.2848445316722235e-07,
      "loss": 1.7119,
      "step": 191744
    },
    {
      "epoch": 0.0006959931645643784,
      "grad_norm": 8884.967416935191,
      "learning_rate": 2.284653706555453e-07,
      "loss": 1.7452,
      "step": 191776
    },
    {
      "epoch": 0.0006961092989152152,
      "grad_norm": 9790.462910404185,
      "learning_rate": 2.2844629292425607e-07,
      "loss": 1.7541,
      "step": 191808
    },
    {
      "epoch": 0.0006962254332660519,
      "grad_norm": 9607.533710583586,
      "learning_rate": 2.284272199713592e-07,
      "loss": 1.7232,
      "step": 191840
    },
    {
      "epoch": 0.0006963415676168886,
      "grad_norm": 8787.903618042246,
      "learning_rate": 2.2840815179486023e-07,
      "loss": 1.7241,
      "step": 191872
    },
    {
      "epoch": 0.0006964577019677253,
      "grad_norm": 8340.681866610188,
      "learning_rate": 2.2838908839276594e-07,
      "loss": 1.7244,
      "step": 191904
    },
    {
      "epoch": 0.000696573836318562,
      "grad_norm": 9440.894449150463,
      "learning_rate": 2.2837002976308425e-07,
      "loss": 1.7066,
      "step": 191936
    },
    {
      "epoch": 0.0006966899706693987,
      "grad_norm": 8200.157559461892,
      "learning_rate": 2.2835097590382428e-07,
      "loss": 1.7216,
      "step": 191968
    },
    {
      "epoch": 0.0006968061050202354,
      "grad_norm": 8395.452102180085,
      "learning_rate": 2.2833192681299623e-07,
      "loss": 1.7256,
      "step": 192000
    },
    {
      "epoch": 0.0006969222393710721,
      "grad_norm": 9867.00603019984,
      "learning_rate": 2.2831288248861155e-07,
      "loss": 1.7477,
      "step": 192032
    },
    {
      "epoch": 0.0006970383737219088,
      "grad_norm": 9566.582252821538,
      "learning_rate": 2.2829384292868282e-07,
      "loss": 1.733,
      "step": 192064
    },
    {
      "epoch": 0.0006971545080727455,
      "grad_norm": 8051.318401355147,
      "learning_rate": 2.2827480813122374e-07,
      "loss": 1.734,
      "step": 192096
    },
    {
      "epoch": 0.0006972706424235823,
      "grad_norm": 8921.54728732634,
      "learning_rate": 2.2825577809424922e-07,
      "loss": 1.7563,
      "step": 192128
    },
    {
      "epoch": 0.0006973867767744189,
      "grad_norm": 10027.129998159991,
      "learning_rate": 2.282367528157753e-07,
      "loss": 1.7394,
      "step": 192160
    },
    {
      "epoch": 0.0006975029111252557,
      "grad_norm": 8711.602148858728,
      "learning_rate": 2.282177322938192e-07,
      "loss": 1.7291,
      "step": 192192
    },
    {
      "epoch": 0.0006976190454760923,
      "grad_norm": 8064.272936849297,
      "learning_rate": 2.281987165263993e-07,
      "loss": 1.7327,
      "step": 192224
    },
    {
      "epoch": 0.0006977351798269291,
      "grad_norm": 8984.159949600185,
      "learning_rate": 2.2817970551153515e-07,
      "loss": 1.7287,
      "step": 192256
    },
    {
      "epoch": 0.0006978513141777657,
      "grad_norm": 9705.274030134336,
      "learning_rate": 2.2816069924724732e-07,
      "loss": 1.7201,
      "step": 192288
    },
    {
      "epoch": 0.0006979674485286025,
      "grad_norm": 10637.069991308697,
      "learning_rate": 2.281416977315577e-07,
      "loss": 1.7305,
      "step": 192320
    },
    {
      "epoch": 0.0006980835828794391,
      "grad_norm": 10494.444244456206,
      "learning_rate": 2.2812270096248932e-07,
      "loss": 1.7523,
      "step": 192352
    },
    {
      "epoch": 0.0006981997172302759,
      "grad_norm": 12103.877230044925,
      "learning_rate": 2.2810370893806624e-07,
      "loss": 1.7748,
      "step": 192384
    },
    {
      "epoch": 0.0006983158515811126,
      "grad_norm": 9573.708999128812,
      "learning_rate": 2.2808472165631378e-07,
      "loss": 1.7175,
      "step": 192416
    },
    {
      "epoch": 0.0006984319859319493,
      "grad_norm": 8820.263034626576,
      "learning_rate": 2.2806573911525837e-07,
      "loss": 1.7015,
      "step": 192448
    },
    {
      "epoch": 0.000698548120282786,
      "grad_norm": 11527.921928951462,
      "learning_rate": 2.280467613129276e-07,
      "loss": 1.7235,
      "step": 192480
    },
    {
      "epoch": 0.0006986642546336227,
      "grad_norm": 8675.177462161797,
      "learning_rate": 2.2802778824735017e-07,
      "loss": 1.7184,
      "step": 192512
    },
    {
      "epoch": 0.0006987803889844594,
      "grad_norm": 9413.976630521238,
      "learning_rate": 2.2800941260524382e-07,
      "loss": 1.7249,
      "step": 192544
    },
    {
      "epoch": 0.0006988965233352961,
      "grad_norm": 9078.383556558954,
      "learning_rate": 2.279904488593933e-07,
      "loss": 1.729,
      "step": 192576
    },
    {
      "epoch": 0.0006990126576861328,
      "grad_norm": 7809.525465737339,
      "learning_rate": 2.2797148984445074e-07,
      "loss": 1.7027,
      "step": 192608
    },
    {
      "epoch": 0.0006991287920369695,
      "grad_norm": 9706.973472715375,
      "learning_rate": 2.279525355584494e-07,
      "loss": 1.7062,
      "step": 192640
    },
    {
      "epoch": 0.0006992449263878062,
      "grad_norm": 8872.552056764727,
      "learning_rate": 2.279335859994237e-07,
      "loss": 1.731,
      "step": 192672
    },
    {
      "epoch": 0.000699361060738643,
      "grad_norm": 9148.8565405738,
      "learning_rate": 2.279146411654092e-07,
      "loss": 1.7152,
      "step": 192704
    },
    {
      "epoch": 0.0006994771950894796,
      "grad_norm": 8797.920208776617,
      "learning_rate": 2.2789570105444264e-07,
      "loss": 1.7125,
      "step": 192736
    },
    {
      "epoch": 0.0006995933294403164,
      "grad_norm": 9809.39447672485,
      "learning_rate": 2.2787676566456192e-07,
      "loss": 1.6926,
      "step": 192768
    },
    {
      "epoch": 0.000699709463791153,
      "grad_norm": 7898.9991771109835,
      "learning_rate": 2.2785783499380598e-07,
      "loss": 1.7231,
      "step": 192800
    },
    {
      "epoch": 0.0006998255981419898,
      "grad_norm": 9206.729386704053,
      "learning_rate": 2.2783890904021497e-07,
      "loss": 1.7274,
      "step": 192832
    },
    {
      "epoch": 0.0006999417324928264,
      "grad_norm": 9043.700127713213,
      "learning_rate": 2.2781998780183018e-07,
      "loss": 1.7415,
      "step": 192864
    },
    {
      "epoch": 0.0007000578668436632,
      "grad_norm": 9840.508116962254,
      "learning_rate": 2.2780107127669401e-07,
      "loss": 1.7532,
      "step": 192896
    },
    {
      "epoch": 0.0007001740011944998,
      "grad_norm": 9284.71259652123,
      "learning_rate": 2.2778215946285005e-07,
      "loss": 1.77,
      "step": 192928
    },
    {
      "epoch": 0.0007002901355453366,
      "grad_norm": 8800.334652727703,
      "learning_rate": 2.2776325235834294e-07,
      "loss": 1.7493,
      "step": 192960
    },
    {
      "epoch": 0.0007004062698961733,
      "grad_norm": 9144.527215772285,
      "learning_rate": 2.2774434996121857e-07,
      "loss": 1.7478,
      "step": 192992
    },
    {
      "epoch": 0.00070052240424701,
      "grad_norm": 8679.399287969185,
      "learning_rate": 2.2772545226952383e-07,
      "loss": 1.7387,
      "step": 193024
    },
    {
      "epoch": 0.0007006385385978467,
      "grad_norm": 9319.522305354498,
      "learning_rate": 2.2770655928130687e-07,
      "loss": 1.7349,
      "step": 193056
    },
    {
      "epoch": 0.0007007546729486834,
      "grad_norm": 10134.417990195589,
      "learning_rate": 2.2768767099461689e-07,
      "loss": 1.7039,
      "step": 193088
    },
    {
      "epoch": 0.0007008708072995201,
      "grad_norm": 9252.396770567073,
      "learning_rate": 2.2766878740750426e-07,
      "loss": 1.7143,
      "step": 193120
    },
    {
      "epoch": 0.0007009869416503568,
      "grad_norm": 9521.451149903569,
      "learning_rate": 2.276499085180205e-07,
      "loss": 1.7496,
      "step": 193152
    },
    {
      "epoch": 0.0007011030760011935,
      "grad_norm": 9221.374734821267,
      "learning_rate": 2.2763103432421822e-07,
      "loss": 1.7267,
      "step": 193184
    },
    {
      "epoch": 0.0007012192103520302,
      "grad_norm": 9495.371504053963,
      "learning_rate": 2.2761216482415117e-07,
      "loss": 1.7358,
      "step": 193216
    },
    {
      "epoch": 0.0007013353447028669,
      "grad_norm": 11190.574069278127,
      "learning_rate": 2.275933000158742e-07,
      "loss": 1.7483,
      "step": 193248
    },
    {
      "epoch": 0.0007014514790537037,
      "grad_norm": 11108.021786078743,
      "learning_rate": 2.2757443989744337e-07,
      "loss": 1.7226,
      "step": 193280
    },
    {
      "epoch": 0.0007015676134045403,
      "grad_norm": 11171.129217764872,
      "learning_rate": 2.2755558446691582e-07,
      "loss": 1.7033,
      "step": 193312
    },
    {
      "epoch": 0.0007016837477553771,
      "grad_norm": 9170.476105415684,
      "learning_rate": 2.2753673372234977e-07,
      "loss": 1.7029,
      "step": 193344
    },
    {
      "epoch": 0.0007017998821062137,
      "grad_norm": 8787.38800782121,
      "learning_rate": 2.2751788766180468e-07,
      "loss": 1.7159,
      "step": 193376
    },
    {
      "epoch": 0.0007019160164570505,
      "grad_norm": 8983.982190543345,
      "learning_rate": 2.27499046283341e-07,
      "loss": 1.7229,
      "step": 193408
    },
    {
      "epoch": 0.0007020321508078871,
      "grad_norm": 10085.406883214975,
      "learning_rate": 2.2748020958502042e-07,
      "loss": 1.6971,
      "step": 193440
    },
    {
      "epoch": 0.0007021482851587239,
      "grad_norm": 10981.767617282749,
      "learning_rate": 2.2746137756490567e-07,
      "loss": 1.7246,
      "step": 193472
    },
    {
      "epoch": 0.0007022644195095605,
      "grad_norm": 9204.161667419798,
      "learning_rate": 2.2744255022106066e-07,
      "loss": 1.7442,
      "step": 193504
    },
    {
      "epoch": 0.0007023805538603973,
      "grad_norm": 9729.897635638312,
      "learning_rate": 2.274243156892377e-07,
      "loss": 1.7687,
      "step": 193536
    },
    {
      "epoch": 0.000702496688211234,
      "grad_norm": 9141.871143261646,
      "learning_rate": 2.2740549754614503e-07,
      "loss": 1.7415,
      "step": 193568
    },
    {
      "epoch": 0.0007026128225620707,
      "grad_norm": 9121.763426004864,
      "learning_rate": 2.273866840735809e-07,
      "loss": 1.7197,
      "step": 193600
    },
    {
      "epoch": 0.0007027289569129074,
      "grad_norm": 9181.534947926735,
      "learning_rate": 2.2736787526961352e-07,
      "loss": 1.6998,
      "step": 193632
    },
    {
      "epoch": 0.0007028450912637441,
      "grad_norm": 9722.274219543491,
      "learning_rate": 2.273490711323125e-07,
      "loss": 1.7078,
      "step": 193664
    },
    {
      "epoch": 0.0007029612256145808,
      "grad_norm": 9037.209082454605,
      "learning_rate": 2.273302716597483e-07,
      "loss": 1.7117,
      "step": 193696
    },
    {
      "epoch": 0.0007030773599654175,
      "grad_norm": 10928.58947897669,
      "learning_rate": 2.2731147684999266e-07,
      "loss": 1.7385,
      "step": 193728
    },
    {
      "epoch": 0.0007031934943162542,
      "grad_norm": 10615.642797306247,
      "learning_rate": 2.2729268670111834e-07,
      "loss": 1.7347,
      "step": 193760
    },
    {
      "epoch": 0.0007033096286670909,
      "grad_norm": 8837.832426562522,
      "learning_rate": 2.2727390121119929e-07,
      "loss": 1.7275,
      "step": 193792
    },
    {
      "epoch": 0.0007034257630179276,
      "grad_norm": 8122.935676219527,
      "learning_rate": 2.2725512037831055e-07,
      "loss": 1.7512,
      "step": 193824
    },
    {
      "epoch": 0.0007035418973687644,
      "grad_norm": 8580.409896968793,
      "learning_rate": 2.2723634420052824e-07,
      "loss": 1.7555,
      "step": 193856
    },
    {
      "epoch": 0.000703658031719601,
      "grad_norm": 9290.134014103349,
      "learning_rate": 2.2721757267592956e-07,
      "loss": 1.7512,
      "step": 193888
    },
    {
      "epoch": 0.0007037741660704378,
      "grad_norm": 10816.37499349944,
      "learning_rate": 2.27198805802593e-07,
      "loss": 1.7381,
      "step": 193920
    },
    {
      "epoch": 0.0007038903004212744,
      "grad_norm": 7866.197810886782,
      "learning_rate": 2.2718004357859796e-07,
      "loss": 1.7254,
      "step": 193952
    },
    {
      "epoch": 0.0007040064347721112,
      "grad_norm": 9244.996917252054,
      "learning_rate": 2.2716128600202505e-07,
      "loss": 1.7097,
      "step": 193984
    },
    {
      "epoch": 0.0007041225691229478,
      "grad_norm": 7867.797023309638,
      "learning_rate": 2.2714253307095593e-07,
      "loss": 1.722,
      "step": 194016
    },
    {
      "epoch": 0.0007042387034737846,
      "grad_norm": 9289.676420629516,
      "learning_rate": 2.271237847834735e-07,
      "loss": 1.7333,
      "step": 194048
    },
    {
      "epoch": 0.0007043548378246212,
      "grad_norm": 9896.98984540249,
      "learning_rate": 2.2710504113766155e-07,
      "loss": 1.7465,
      "step": 194080
    },
    {
      "epoch": 0.000704470972175458,
      "grad_norm": 11459.13295149332,
      "learning_rate": 2.270863021316052e-07,
      "loss": 1.7464,
      "step": 194112
    },
    {
      "epoch": 0.0007045871065262947,
      "grad_norm": 9453.07759409601,
      "learning_rate": 2.2706756776339056e-07,
      "loss": 1.7336,
      "step": 194144
    },
    {
      "epoch": 0.0007047032408771314,
      "grad_norm": 9639.931120085868,
      "learning_rate": 2.270488380311048e-07,
      "loss": 1.7376,
      "step": 194176
    },
    {
      "epoch": 0.0007048193752279681,
      "grad_norm": 8324.893873197423,
      "learning_rate": 2.2703011293283632e-07,
      "loss": 1.7141,
      "step": 194208
    },
    {
      "epoch": 0.0007049355095788048,
      "grad_norm": 9639.385250108016,
      "learning_rate": 2.2701139246667454e-07,
      "loss": 1.717,
      "step": 194240
    },
    {
      "epoch": 0.0007050516439296415,
      "grad_norm": 9979.373327018086,
      "learning_rate": 2.2699267663071e-07,
      "loss": 1.7273,
      "step": 194272
    },
    {
      "epoch": 0.0007051677782804782,
      "grad_norm": 9898.23398389834,
      "learning_rate": 2.2697396542303434e-07,
      "loss": 1.7077,
      "step": 194304
    },
    {
      "epoch": 0.0007052839126313149,
      "grad_norm": 11689.422226953735,
      "learning_rate": 2.269552588417403e-07,
      "loss": 1.7122,
      "step": 194336
    },
    {
      "epoch": 0.0007054000469821516,
      "grad_norm": 9560.393506545637,
      "learning_rate": 2.2693655688492176e-07,
      "loss": 1.7107,
      "step": 194368
    },
    {
      "epoch": 0.0007055161813329883,
      "grad_norm": 9668.312055369333,
      "learning_rate": 2.269178595506736e-07,
      "loss": 1.7426,
      "step": 194400
    },
    {
      "epoch": 0.0007056323156838251,
      "grad_norm": 9960.241161739006,
      "learning_rate": 2.2689916683709192e-07,
      "loss": 1.7138,
      "step": 194432
    },
    {
      "epoch": 0.0007057484500346617,
      "grad_norm": 8599.918139145278,
      "learning_rate": 2.2688047874227384e-07,
      "loss": 1.6922,
      "step": 194464
    },
    {
      "epoch": 0.0007058645843854985,
      "grad_norm": 8804.493171103037,
      "learning_rate": 2.268617952643176e-07,
      "loss": 1.7129,
      "step": 194496
    },
    {
      "epoch": 0.0007059807187363351,
      "grad_norm": 9793.148012768928,
      "learning_rate": 2.268431164013225e-07,
      "loss": 1.7252,
      "step": 194528
    },
    {
      "epoch": 0.0007060968530871719,
      "grad_norm": 8233.448244812134,
      "learning_rate": 2.268250256518917e-07,
      "loss": 1.7232,
      "step": 194560
    },
    {
      "epoch": 0.0007062129874380085,
      "grad_norm": 10206.226334938883,
      "learning_rate": 2.2680635586905121e-07,
      "loss": 1.7288,
      "step": 194592
    },
    {
      "epoch": 0.0007063291217888453,
      "grad_norm": 9710.505136191423,
      "learning_rate": 2.2678769069553575e-07,
      "loss": 1.7467,
      "step": 194624
    },
    {
      "epoch": 0.0007064452561396819,
      "grad_norm": 8455.12802978169,
      "learning_rate": 2.2676903012944901e-07,
      "loss": 1.7372,
      "step": 194656
    },
    {
      "epoch": 0.0007065613904905187,
      "grad_norm": 9329.042180202638,
      "learning_rate": 2.2675037416889567e-07,
      "loss": 1.7553,
      "step": 194688
    },
    {
      "epoch": 0.0007066775248413554,
      "grad_norm": 9234.687650375621,
      "learning_rate": 2.2673172281198168e-07,
      "loss": 1.7657,
      "step": 194720
    },
    {
      "epoch": 0.0007067936591921921,
      "grad_norm": 11312.628518606984,
      "learning_rate": 2.2671307605681395e-07,
      "loss": 1.7707,
      "step": 194752
    },
    {
      "epoch": 0.0007069097935430288,
      "grad_norm": 9553.63386361441,
      "learning_rate": 2.2669443390150054e-07,
      "loss": 1.7278,
      "step": 194784
    },
    {
      "epoch": 0.0007070259278938655,
      "grad_norm": 10302.083672733394,
      "learning_rate": 2.2667579634415056e-07,
      "loss": 1.7044,
      "step": 194816
    },
    {
      "epoch": 0.0007071420622447022,
      "grad_norm": 8371.77125822248,
      "learning_rate": 2.2665716338287422e-07,
      "loss": 1.7259,
      "step": 194848
    },
    {
      "epoch": 0.0007072581965955389,
      "grad_norm": 8517.81004718936,
      "learning_rate": 2.2663853501578285e-07,
      "loss": 1.7343,
      "step": 194880
    },
    {
      "epoch": 0.0007073743309463756,
      "grad_norm": 9776.091448017454,
      "learning_rate": 2.2661991124098876e-07,
      "loss": 1.7467,
      "step": 194912
    },
    {
      "epoch": 0.0007074904652972123,
      "grad_norm": 9569.607724457675,
      "learning_rate": 2.2660129205660555e-07,
      "loss": 1.737,
      "step": 194944
    },
    {
      "epoch": 0.000707606599648049,
      "grad_norm": 9162.774688924748,
      "learning_rate": 2.265826774607477e-07,
      "loss": 1.7312,
      "step": 194976
    },
    {
      "epoch": 0.0007077227339988858,
      "grad_norm": 9008.886501671559,
      "learning_rate": 2.2656406745153087e-07,
      "loss": 1.7044,
      "step": 195008
    },
    {
      "epoch": 0.0007078388683497224,
      "grad_norm": 8650.512470368447,
      "learning_rate": 2.2654546202707176e-07,
      "loss": 1.7193,
      "step": 195040
    },
    {
      "epoch": 0.0007079550027005592,
      "grad_norm": 9362.343510040635,
      "learning_rate": 2.265268611854882e-07,
      "loss": 1.7144,
      "step": 195072
    },
    {
      "epoch": 0.0007080711370513958,
      "grad_norm": 9714.537868576148,
      "learning_rate": 2.265082649248991e-07,
      "loss": 1.719,
      "step": 195104
    },
    {
      "epoch": 0.0007081872714022326,
      "grad_norm": 8203.42233461133,
      "learning_rate": 2.2648967324342445e-07,
      "loss": 1.6984,
      "step": 195136
    },
    {
      "epoch": 0.0007083034057530692,
      "grad_norm": 9117.393267815094,
      "learning_rate": 2.2647108613918526e-07,
      "loss": 1.699,
      "step": 195168
    },
    {
      "epoch": 0.000708419540103906,
      "grad_norm": 9967.799957864323,
      "learning_rate": 2.2645250361030364e-07,
      "loss": 1.7309,
      "step": 195200
    },
    {
      "epoch": 0.0007085356744547426,
      "grad_norm": 9316.293039616132,
      "learning_rate": 2.2643392565490284e-07,
      "loss": 1.7371,
      "step": 195232
    },
    {
      "epoch": 0.0007086518088055794,
      "grad_norm": 9599.794164459987,
      "learning_rate": 2.2641535227110715e-07,
      "loss": 1.7478,
      "step": 195264
    },
    {
      "epoch": 0.0007087679431564162,
      "grad_norm": 9775.850448937934,
      "learning_rate": 2.2639678345704192e-07,
      "loss": 1.7462,
      "step": 195296
    },
    {
      "epoch": 0.0007088840775072528,
      "grad_norm": 7991.826199311394,
      "learning_rate": 2.2637821921083356e-07,
      "loss": 1.7221,
      "step": 195328
    },
    {
      "epoch": 0.0007090002118580896,
      "grad_norm": 10209.725657430761,
      "learning_rate": 2.2635965953060963e-07,
      "loss": 1.6999,
      "step": 195360
    },
    {
      "epoch": 0.0007091163462089262,
      "grad_norm": 8262.16206570653,
      "learning_rate": 2.2634110441449873e-07,
      "loss": 1.7073,
      "step": 195392
    },
    {
      "epoch": 0.000709232480559763,
      "grad_norm": 9351.461169250504,
      "learning_rate": 2.2632255386063047e-07,
      "loss": 1.7216,
      "step": 195424
    },
    {
      "epoch": 0.0007093486149105996,
      "grad_norm": 9412.7029061795,
      "learning_rate": 2.2630400786713564e-07,
      "loss": 1.7171,
      "step": 195456
    },
    {
      "epoch": 0.0007094647492614364,
      "grad_norm": 8598.771540167816,
      "learning_rate": 2.26285466432146e-07,
      "loss": 1.7183,
      "step": 195488
    },
    {
      "epoch": 0.000709580883612273,
      "grad_norm": 9572.10102328637,
      "learning_rate": 2.2626692955379446e-07,
      "loss": 1.7478,
      "step": 195520
    },
    {
      "epoch": 0.0007096970179631098,
      "grad_norm": 7500.607842035204,
      "learning_rate": 2.2624897629640102e-07,
      "loss": 1.752,
      "step": 195552
    },
    {
      "epoch": 0.0007098131523139465,
      "grad_norm": 8975.668888723558,
      "learning_rate": 2.2623044838347846e-07,
      "loss": 1.7543,
      "step": 195584
    },
    {
      "epoch": 0.0007099292866647832,
      "grad_norm": 8643.3618459486,
      "learning_rate": 2.262119250216573e-07,
      "loss": 1.757,
      "step": 195616
    },
    {
      "epoch": 0.0007100454210156199,
      "grad_norm": 9154.144198121418,
      "learning_rate": 2.261934062090746e-07,
      "loss": 1.7542,
      "step": 195648
    },
    {
      "epoch": 0.0007101615553664566,
      "grad_norm": 8934.525393102871,
      "learning_rate": 2.2617489194386859e-07,
      "loss": 1.7073,
      "step": 195680
    },
    {
      "epoch": 0.0007102776897172933,
      "grad_norm": 8299.726381032087,
      "learning_rate": 2.2615638222417853e-07,
      "loss": 1.7079,
      "step": 195712
    },
    {
      "epoch": 0.00071039382406813,
      "grad_norm": 8525.68120445516,
      "learning_rate": 2.2613787704814478e-07,
      "loss": 1.7245,
      "step": 195744
    },
    {
      "epoch": 0.0007105099584189667,
      "grad_norm": 10699.625787848845,
      "learning_rate": 2.2611937641390866e-07,
      "loss": 1.7378,
      "step": 195776
    },
    {
      "epoch": 0.0007106260927698034,
      "grad_norm": 9576.447775662957,
      "learning_rate": 2.2610088031961264e-07,
      "loss": 1.7299,
      "step": 195808
    },
    {
      "epoch": 0.0007107422271206401,
      "grad_norm": 9581.427242326688,
      "learning_rate": 2.260823887634003e-07,
      "loss": 1.7151,
      "step": 195840
    },
    {
      "epoch": 0.0007108583614714769,
      "grad_norm": 8988.796693662618,
      "learning_rate": 2.2606390174341614e-07,
      "loss": 1.7516,
      "step": 195872
    },
    {
      "epoch": 0.0007109744958223135,
      "grad_norm": 11564.650102791697,
      "learning_rate": 2.2604541925780583e-07,
      "loss": 1.757,
      "step": 195904
    },
    {
      "epoch": 0.0007110906301731503,
      "grad_norm": 8276.391484215812,
      "learning_rate": 2.2602694130471604e-07,
      "loss": 1.7418,
      "step": 195936
    },
    {
      "epoch": 0.0007112067645239869,
      "grad_norm": 8575.398649625567,
      "learning_rate": 2.2600846788229458e-07,
      "loss": 1.713,
      "step": 195968
    },
    {
      "epoch": 0.0007113228988748237,
      "grad_norm": 10512.023021283774,
      "learning_rate": 2.2598999898869025e-07,
      "loss": 1.7136,
      "step": 196000
    },
    {
      "epoch": 0.0007114390332256603,
      "grad_norm": 7764.435330402334,
      "learning_rate": 2.259715346220529e-07,
      "loss": 1.6943,
      "step": 196032
    },
    {
      "epoch": 0.0007115551675764971,
      "grad_norm": 9211.345830007687,
      "learning_rate": 2.2595307478053357e-07,
      "loss": 1.7049,
      "step": 196064
    },
    {
      "epoch": 0.0007116713019273337,
      "grad_norm": 10718.68947213231,
      "learning_rate": 2.259346194622841e-07,
      "loss": 1.7466,
      "step": 196096
    },
    {
      "epoch": 0.0007117874362781705,
      "grad_norm": 10854.537300133987,
      "learning_rate": 2.2591616866545767e-07,
      "loss": 1.725,
      "step": 196128
    },
    {
      "epoch": 0.0007119035706290072,
      "grad_norm": 11004.84202521781,
      "learning_rate": 2.2589772238820833e-07,
      "loss": 1.703,
      "step": 196160
    },
    {
      "epoch": 0.0007120197049798439,
      "grad_norm": 9539.5440142598,
      "learning_rate": 2.2587928062869124e-07,
      "loss": 1.706,
      "step": 196192
    },
    {
      "epoch": 0.0007121358393306806,
      "grad_norm": 8850.633762618358,
      "learning_rate": 2.2586084338506266e-07,
      "loss": 1.7182,
      "step": 196224
    },
    {
      "epoch": 0.0007122519736815173,
      "grad_norm": 8869.72412197809,
      "learning_rate": 2.2584241065547978e-07,
      "loss": 1.7199,
      "step": 196256
    },
    {
      "epoch": 0.000712368108032354,
      "grad_norm": 9808.62661130497,
      "learning_rate": 2.2582398243810102e-07,
      "loss": 1.7204,
      "step": 196288
    },
    {
      "epoch": 0.0007124842423831907,
      "grad_norm": 9494.196332497027,
      "learning_rate": 2.2580555873108564e-07,
      "loss": 1.7293,
      "step": 196320
    },
    {
      "epoch": 0.0007126003767340274,
      "grad_norm": 10152.267332965577,
      "learning_rate": 2.2578713953259416e-07,
      "loss": 1.718,
      "step": 196352
    },
    {
      "epoch": 0.0007127165110848641,
      "grad_norm": 11199.245956759769,
      "learning_rate": 2.2576872484078803e-07,
      "loss": 1.7294,
      "step": 196384
    },
    {
      "epoch": 0.0007128326454357008,
      "grad_norm": 10046.304196071309,
      "learning_rate": 2.2575031465382973e-07,
      "loss": 1.751,
      "step": 196416
    },
    {
      "epoch": 0.0007129487797865376,
      "grad_norm": 10117.945245947914,
      "learning_rate": 2.2573190896988288e-07,
      "loss": 1.7689,
      "step": 196448
    },
    {
      "epoch": 0.0007130649141373742,
      "grad_norm": 9477.602228411994,
      "learning_rate": 2.257135077871121e-07,
      "loss": 1.7671,
      "step": 196480
    },
    {
      "epoch": 0.000713181048488211,
      "grad_norm": 8744.905488340055,
      "learning_rate": 2.2569511110368307e-07,
      "loss": 1.7467,
      "step": 196512
    },
    {
      "epoch": 0.0007132971828390476,
      "grad_norm": 19378.6965505939,
      "learning_rate": 2.2567671891776245e-07,
      "loss": 1.7332,
      "step": 196544
    },
    {
      "epoch": 0.0007134133171898844,
      "grad_norm": 8560.355483272877,
      "learning_rate": 2.2565890577480655e-07,
      "loss": 1.7263,
      "step": 196576
    },
    {
      "epoch": 0.000713529451540721,
      "grad_norm": 8551.028826989183,
      "learning_rate": 2.256405224380022e-07,
      "loss": 1.7361,
      "step": 196608
    },
    {
      "epoch": 0.0007136455858915578,
      "grad_norm": 9590.143898816117,
      "learning_rate": 2.2562214359326984e-07,
      "loss": 1.7308,
      "step": 196640
    },
    {
      "epoch": 0.0007137617202423944,
      "grad_norm": 9281.717513477772,
      "learning_rate": 2.256037692387804e-07,
      "loss": 1.7536,
      "step": 196672
    },
    {
      "epoch": 0.0007138778545932312,
      "grad_norm": 11391.412203936789,
      "learning_rate": 2.2558539937270576e-07,
      "loss": 1.7017,
      "step": 196704
    },
    {
      "epoch": 0.0007139939889440679,
      "grad_norm": 8229.438741493856,
      "learning_rate": 2.2556703399321885e-07,
      "loss": 1.7104,
      "step": 196736
    },
    {
      "epoch": 0.0007141101232949046,
      "grad_norm": 8911.135168989413,
      "learning_rate": 2.2554867309849363e-07,
      "loss": 1.7249,
      "step": 196768
    },
    {
      "epoch": 0.0007142262576457413,
      "grad_norm": 9548.024402985155,
      "learning_rate": 2.2553031668670516e-07,
      "loss": 1.739,
      "step": 196800
    },
    {
      "epoch": 0.000714342391996578,
      "grad_norm": 8749.163045686142,
      "learning_rate": 2.2551196475602952e-07,
      "loss": 1.6895,
      "step": 196832
    },
    {
      "epoch": 0.0007144585263474147,
      "grad_norm": 8703.068998922161,
      "learning_rate": 2.254936173046438e-07,
      "loss": 1.6981,
      "step": 196864
    },
    {
      "epoch": 0.0007145746606982514,
      "grad_norm": 10910.9959215463,
      "learning_rate": 2.2547527433072612e-07,
      "loss": 1.7214,
      "step": 196896
    },
    {
      "epoch": 0.0007146907950490881,
      "grad_norm": 9310.663778700206,
      "learning_rate": 2.2545693583245565e-07,
      "loss": 1.7219,
      "step": 196928
    },
    {
      "epoch": 0.0007148069293999248,
      "grad_norm": 9681.071221719216,
      "learning_rate": 2.2543860180801265e-07,
      "loss": 1.7417,
      "step": 196960
    },
    {
      "epoch": 0.0007149230637507615,
      "grad_norm": 10663.21208642124,
      "learning_rate": 2.2542027225557835e-07,
      "loss": 1.7334,
      "step": 196992
    },
    {
      "epoch": 0.0007150391981015983,
      "grad_norm": 10054.431659721,
      "learning_rate": 2.2540194717333504e-07,
      "loss": 1.7186,
      "step": 197024
    },
    {
      "epoch": 0.0007151553324524349,
      "grad_norm": 9926.49787185793,
      "learning_rate": 2.25383626559466e-07,
      "loss": 1.7222,
      "step": 197056
    },
    {
      "epoch": 0.0007152714668032717,
      "grad_norm": 8733.201360326006,
      "learning_rate": 2.2536531041215564e-07,
      "loss": 1.7236,
      "step": 197088
    },
    {
      "epoch": 0.0007153876011541083,
      "grad_norm": 9818.827017520984,
      "learning_rate": 2.2534699872958932e-07,
      "loss": 1.7186,
      "step": 197120
    },
    {
      "epoch": 0.0007155037355049451,
      "grad_norm": 9077.290344590725,
      "learning_rate": 2.253286915099535e-07,
      "loss": 1.7132,
      "step": 197152
    },
    {
      "epoch": 0.0007156198698557817,
      "grad_norm": 10013.223257273354,
      "learning_rate": 2.2531038875143551e-07,
      "loss": 1.7032,
      "step": 197184
    },
    {
      "epoch": 0.0007157360042066185,
      "grad_norm": 8049.03957003567,
      "learning_rate": 2.2529209045222397e-07,
      "loss": 1.7171,
      "step": 197216
    },
    {
      "epoch": 0.0007158521385574551,
      "grad_norm": 8184.317931263423,
      "learning_rate": 2.252737966105083e-07,
      "loss": 1.7564,
      "step": 197248
    },
    {
      "epoch": 0.0007159682729082919,
      "grad_norm": 9775.482085298914,
      "learning_rate": 2.252555072244791e-07,
      "loss": 1.7623,
      "step": 197280
    },
    {
      "epoch": 0.0007160844072591286,
      "grad_norm": 9564.840824603409,
      "learning_rate": 2.2523722229232788e-07,
      "loss": 1.754,
      "step": 197312
    },
    {
      "epoch": 0.0007162005416099653,
      "grad_norm": 10090.995292834103,
      "learning_rate": 2.252189418122473e-07,
      "loss": 1.7584,
      "step": 197344
    },
    {
      "epoch": 0.000716316675960802,
      "grad_norm": 8168.682023435605,
      "learning_rate": 2.252006657824309e-07,
      "loss": 1.7217,
      "step": 197376
    },
    {
      "epoch": 0.0007164328103116387,
      "grad_norm": 9954.123165804209,
      "learning_rate": 2.2518239420107335e-07,
      "loss": 1.7305,
      "step": 197408
    },
    {
      "epoch": 0.0007165489446624754,
      "grad_norm": 10715.151702145891,
      "learning_rate": 2.2516412706637035e-07,
      "loss": 1.7174,
      "step": 197440
    },
    {
      "epoch": 0.0007166650790133121,
      "grad_norm": 9288.254626139402,
      "learning_rate": 2.251458643765186e-07,
      "loss": 1.7229,
      "step": 197472
    },
    {
      "epoch": 0.0007167812133641488,
      "grad_norm": 10095.404697187725,
      "learning_rate": 2.251276061297158e-07,
      "loss": 1.7203,
      "step": 197504
    },
    {
      "epoch": 0.0007168973477149855,
      "grad_norm": 9294.129760230378,
      "learning_rate": 2.251093523241607e-07,
      "loss": 1.717,
      "step": 197536
    },
    {
      "epoch": 0.0007170134820658222,
      "grad_norm": 18183.739989342128,
      "learning_rate": 2.25091102958053e-07,
      "loss": 1.729,
      "step": 197568
    },
    {
      "epoch": 0.000717129616416659,
      "grad_norm": 9502.809479306632,
      "learning_rate": 2.2507342811645441e-07,
      "loss": 1.7272,
      "step": 197600
    },
    {
      "epoch": 0.0007172457507674956,
      "grad_norm": 12140.03887967415,
      "learning_rate": 2.2505518748525196e-07,
      "loss": 1.7456,
      "step": 197632
    },
    {
      "epoch": 0.0007173618851183324,
      "grad_norm": 10115.092683707846,
      "learning_rate": 2.250369512881585e-07,
      "loss": 1.7559,
      "step": 197664
    },
    {
      "epoch": 0.000717478019469169,
      "grad_norm": 10314.929180561541,
      "learning_rate": 2.2501871952337792e-07,
      "loss": 1.7364,
      "step": 197696
    },
    {
      "epoch": 0.0007175941538200058,
      "grad_norm": 9271.192372073832,
      "learning_rate": 2.25000492189115e-07,
      "loss": 1.6879,
      "step": 197728
    },
    {
      "epoch": 0.0007177102881708424,
      "grad_norm": 8964.087683640762,
      "learning_rate": 2.249822692835756e-07,
      "loss": 1.7048,
      "step": 197760
    },
    {
      "epoch": 0.0007178264225216792,
      "grad_norm": 9139.033865786909,
      "learning_rate": 2.249640508049666e-07,
      "loss": 1.7185,
      "step": 197792
    },
    {
      "epoch": 0.0007179425568725158,
      "grad_norm": 10336.411176032037,
      "learning_rate": 2.2494583675149588e-07,
      "loss": 1.7438,
      "step": 197824
    },
    {
      "epoch": 0.0007180586912233526,
      "grad_norm": 8311.31902889066,
      "learning_rate": 2.2492762712137231e-07,
      "loss": 1.7077,
      "step": 197856
    },
    {
      "epoch": 0.0007181748255741893,
      "grad_norm": 8908.70967087827,
      "learning_rate": 2.2490942191280586e-07,
      "loss": 1.6994,
      "step": 197888
    },
    {
      "epoch": 0.000718290959925026,
      "grad_norm": 8894.84446182169,
      "learning_rate": 2.248912211240074e-07,
      "loss": 1.7274,
      "step": 197920
    },
    {
      "epoch": 0.0007184070942758627,
      "grad_norm": 11180.71303629603,
      "learning_rate": 2.2487302475318894e-07,
      "loss": 1.7193,
      "step": 197952
    },
    {
      "epoch": 0.0007185232286266994,
      "grad_norm": 12116.985103564335,
      "learning_rate": 2.2485483279856335e-07,
      "loss": 1.7137,
      "step": 197984
    },
    {
      "epoch": 0.0007186393629775361,
      "grad_norm": 8664.511988565773,
      "learning_rate": 2.2483664525834465e-07,
      "loss": 1.7246,
      "step": 198016
    },
    {
      "epoch": 0.0007187554973283728,
      "grad_norm": 10778.80382973918,
      "learning_rate": 2.248184621307478e-07,
      "loss": 1.7118,
      "step": 198048
    },
    {
      "epoch": 0.0007188716316792095,
      "grad_norm": 10801.4310163052,
      "learning_rate": 2.2480028341398878e-07,
      "loss": 1.7129,
      "step": 198080
    },
    {
      "epoch": 0.0007189877660300462,
      "grad_norm": 9640.742917431207,
      "learning_rate": 2.2478210910628461e-07,
      "loss": 1.7278,
      "step": 198112
    },
    {
      "epoch": 0.0007191039003808829,
      "grad_norm": 9351.511535575411,
      "learning_rate": 2.2476393920585328e-07,
      "loss": 1.7464,
      "step": 198144
    },
    {
      "epoch": 0.0007192200347317197,
      "grad_norm": 9902.208642520112,
      "learning_rate": 2.2474577371091382e-07,
      "loss": 1.7604,
      "step": 198176
    },
    {
      "epoch": 0.0007193361690825563,
      "grad_norm": 10795.04645659295,
      "learning_rate": 2.2472761261968623e-07,
      "loss": 1.7429,
      "step": 198208
    },
    {
      "epoch": 0.0007194523034333931,
      "grad_norm": 9694.884217978057,
      "learning_rate": 2.2470945593039156e-07,
      "loss": 1.7624,
      "step": 198240
    },
    {
      "epoch": 0.0007195684377842297,
      "grad_norm": 8210.241409361846,
      "learning_rate": 2.2469130364125182e-07,
      "loss": 1.7731,
      "step": 198272
    },
    {
      "epoch": 0.0007196845721350665,
      "grad_norm": 8709.709065175484,
      "learning_rate": 2.2467315575049006e-07,
      "loss": 1.7346,
      "step": 198304
    },
    {
      "epoch": 0.0007198007064859031,
      "grad_norm": 7864.987603296016,
      "learning_rate": 2.2465501225633031e-07,
      "loss": 1.7254,
      "step": 198336
    },
    {
      "epoch": 0.0007199168408367399,
      "grad_norm": 9640.149376436031,
      "learning_rate": 2.2463687315699765e-07,
      "loss": 1.7375,
      "step": 198368
    },
    {
      "epoch": 0.0007200329751875765,
      "grad_norm": 8444.784070655685,
      "learning_rate": 2.246187384507181e-07,
      "loss": 1.7131,
      "step": 198400
    },
    {
      "epoch": 0.0007201491095384133,
      "grad_norm": 9045.021503567585,
      "learning_rate": 2.2460060813571873e-07,
      "loss": 1.7137,
      "step": 198432
    },
    {
      "epoch": 0.00072026524388925,
      "grad_norm": 10428.387603076517,
      "learning_rate": 2.2458248221022756e-07,
      "loss": 1.7094,
      "step": 198464
    },
    {
      "epoch": 0.0007203813782400867,
      "grad_norm": 10263.416585133822,
      "learning_rate": 2.2456436067247373e-07,
      "loss": 1.7212,
      "step": 198496
    },
    {
      "epoch": 0.0007204975125909234,
      "grad_norm": 10908.980887323985,
      "learning_rate": 2.245462435206872e-07,
      "loss": 1.7141,
      "step": 198528
    },
    {
      "epoch": 0.0007206136469417601,
      "grad_norm": 10441.387743015772,
      "learning_rate": 2.2452813075309906e-07,
      "loss": 1.718,
      "step": 198560
    },
    {
      "epoch": 0.0007207297812925968,
      "grad_norm": 19294.25406694957,
      "learning_rate": 2.245105881886595e-07,
      "loss": 1.7073,
      "step": 198592
    },
    {
      "epoch": 0.0007208459156434335,
      "grad_norm": 8881.936050208873,
      "learning_rate": 2.244924840472963e-07,
      "loss": 1.7211,
      "step": 198624
    },
    {
      "epoch": 0.0007209620499942702,
      "grad_norm": 10168.710832745712,
      "learning_rate": 2.2447438428488587e-07,
      "loss": 1.7292,
      "step": 198656
    },
    {
      "epoch": 0.0007210781843451069,
      "grad_norm": 9614.783616909952,
      "learning_rate": 2.2445628889966312e-07,
      "loss": 1.7329,
      "step": 198688
    },
    {
      "epoch": 0.0007211943186959436,
      "grad_norm": 10161.76539780367,
      "learning_rate": 2.2443819788986414e-07,
      "loss": 1.7134,
      "step": 198720
    },
    {
      "epoch": 0.0007213104530467804,
      "grad_norm": 9580.588917180405,
      "learning_rate": 2.2442011125372589e-07,
      "loss": 1.7037,
      "step": 198752
    },
    {
      "epoch": 0.000721426587397617,
      "grad_norm": 9636.169259617642,
      "learning_rate": 2.244020289894864e-07,
      "loss": 1.7123,
      "step": 198784
    },
    {
      "epoch": 0.0007215427217484538,
      "grad_norm": 11198.136184204941,
      "learning_rate": 2.2438395109538463e-07,
      "loss": 1.7374,
      "step": 198816
    },
    {
      "epoch": 0.0007216588560992904,
      "grad_norm": 10659.390976974248,
      "learning_rate": 2.2436587756966056e-07,
      "loss": 1.7369,
      "step": 198848
    },
    {
      "epoch": 0.0007217749904501272,
      "grad_norm": 8151.381478007271,
      "learning_rate": 2.2434780841055524e-07,
      "loss": 1.6927,
      "step": 198880
    },
    {
      "epoch": 0.0007218911248009638,
      "grad_norm": 9002.016662948365,
      "learning_rate": 2.2432974361631056e-07,
      "loss": 1.7069,
      "step": 198912
    },
    {
      "epoch": 0.0007220072591518006,
      "grad_norm": 9423.447988926346,
      "learning_rate": 2.2431168318516952e-07,
      "loss": 1.7359,
      "step": 198944
    },
    {
      "epoch": 0.0007221233935026372,
      "grad_norm": 8789.873036625728,
      "learning_rate": 2.2429362711537606e-07,
      "loss": 1.7366,
      "step": 198976
    },
    {
      "epoch": 0.000722239527853474,
      "grad_norm": 9369.03516910893,
      "learning_rate": 2.2427557540517512e-07,
      "loss": 1.7487,
      "step": 199008
    },
    {
      "epoch": 0.0007223556622043107,
      "grad_norm": 9376.02741036949,
      "learning_rate": 2.2425752805281264e-07,
      "loss": 1.7613,
      "step": 199040
    },
    {
      "epoch": 0.0007224717965551474,
      "grad_norm": 10206.27297302987,
      "learning_rate": 2.242394850565355e-07,
      "loss": 1.7339,
      "step": 199072
    },
    {
      "epoch": 0.0007225879309059841,
      "grad_norm": 9499.12332797085,
      "learning_rate": 2.2422144641459161e-07,
      "loss": 1.7312,
      "step": 199104
    },
    {
      "epoch": 0.0007227040652568208,
      "grad_norm": 9880.66758878164,
      "learning_rate": 2.242034121252299e-07,
      "loss": 1.7373,
      "step": 199136
    },
    {
      "epoch": 0.0007228201996076575,
      "grad_norm": 9625.417601330344,
      "learning_rate": 2.2418538218670022e-07,
      "loss": 1.7474,
      "step": 199168
    },
    {
      "epoch": 0.0007229363339584942,
      "grad_norm": 9621.025309186127,
      "learning_rate": 2.2416735659725343e-07,
      "loss": 1.7283,
      "step": 199200
    },
    {
      "epoch": 0.0007230524683093309,
      "grad_norm": 11408.458791615984,
      "learning_rate": 2.2414933535514138e-07,
      "loss": 1.7009,
      "step": 199232
    },
    {
      "epoch": 0.0007231686026601676,
      "grad_norm": 10404.782169752523,
      "learning_rate": 2.241313184586169e-07,
      "loss": 1.7335,
      "step": 199264
    },
    {
      "epoch": 0.0007232847370110043,
      "grad_norm": 9908.94908655807,
      "learning_rate": 2.2411330590593378e-07,
      "loss": 1.7241,
      "step": 199296
    },
    {
      "epoch": 0.000723400871361841,
      "grad_norm": 9964.482124024309,
      "learning_rate": 2.2409529769534686e-07,
      "loss": 1.7239,
      "step": 199328
    },
    {
      "epoch": 0.0007235170057126777,
      "grad_norm": 8352.695732516539,
      "learning_rate": 2.2407729382511185e-07,
      "loss": 1.7262,
      "step": 199360
    },
    {
      "epoch": 0.0007236331400635145,
      "grad_norm": 10019.379621513499,
      "learning_rate": 2.2405929429348555e-07,
      "loss": 1.747,
      "step": 199392
    },
    {
      "epoch": 0.0007237492744143511,
      "grad_norm": 10727.300033093135,
      "learning_rate": 2.2404129909872565e-07,
      "loss": 1.7336,
      "step": 199424
    },
    {
      "epoch": 0.0007238654087651879,
      "grad_norm": 9676.028317445129,
      "learning_rate": 2.2402330823909096e-07,
      "loss": 1.7143,
      "step": 199456
    },
    {
      "epoch": 0.0007239815431160245,
      "grad_norm": 8459.953191359868,
      "learning_rate": 2.2400532171284106e-07,
      "loss": 1.7111,
      "step": 199488
    },
    {
      "epoch": 0.0007240976774668613,
      "grad_norm": 9431.485779027607,
      "learning_rate": 2.2398733951823672e-07,
      "loss": 1.7289,
      "step": 199520
    },
    {
      "epoch": 0.0007242138118176979,
      "grad_norm": 9619.857379400175,
      "learning_rate": 2.2396936165353952e-07,
      "loss": 1.7109,
      "step": 199552
    },
    {
      "epoch": 0.0007243299461685347,
      "grad_norm": 9679.890701862289,
      "learning_rate": 2.2395138811701207e-07,
      "loss": 1.6912,
      "step": 199584
    },
    {
      "epoch": 0.0007244460805193713,
      "grad_norm": 16573.515499132947,
      "learning_rate": 2.2393341890691805e-07,
      "loss": 1.731,
      "step": 199616
    },
    {
      "epoch": 0.0007245622148702081,
      "grad_norm": 9112.615102153717,
      "learning_rate": 2.2391601535874612e-07,
      "loss": 1.7123,
      "step": 199648
    },
    {
      "epoch": 0.0007246783492210449,
      "grad_norm": 10006.703353252758,
      "learning_rate": 2.238980546612472e-07,
      "loss": 1.7262,
      "step": 199680
    },
    {
      "epoch": 0.0007247944835718815,
      "grad_norm": 9342.306888558092,
      "learning_rate": 2.238800982850325e-07,
      "loss": 1.7124,
      "step": 199712
    },
    {
      "epoch": 0.0007249106179227183,
      "grad_norm": 8791.148502897673,
      "learning_rate": 2.2386214622836945e-07,
      "loss": 1.7029,
      "step": 199744
    },
    {
      "epoch": 0.0007250267522735549,
      "grad_norm": 9124.565085525994,
      "learning_rate": 2.238441984895265e-07,
      "loss": 1.6991,
      "step": 199776
    },
    {
      "epoch": 0.0007251428866243917,
      "grad_norm": 9429.15754455296,
      "learning_rate": 2.2382625506677304e-07,
      "loss": 1.7121,
      "step": 199808
    },
    {
      "epoch": 0.0007252590209752283,
      "grad_norm": 9694.475952830044,
      "learning_rate": 2.238083159583795e-07,
      "loss": 1.7327,
      "step": 199840
    },
    {
      "epoch": 0.000725375155326065,
      "grad_norm": 10172.14470994195,
      "learning_rate": 2.237903811626172e-07,
      "loss": 1.7431,
      "step": 199872
    },
    {
      "epoch": 0.0007254912896769017,
      "grad_norm": 11559.147200377716,
      "learning_rate": 2.2377245067775849e-07,
      "loss": 1.7233,
      "step": 199904
    },
    {
      "epoch": 0.0007256074240277385,
      "grad_norm": 8418.204084007468,
      "learning_rate": 2.237545245020766e-07,
      "loss": 1.7462,
      "step": 199936
    },
    {
      "epoch": 0.0007257235583785752,
      "grad_norm": 10565.635996001376,
      "learning_rate": 2.2373660263384587e-07,
      "loss": 1.7694,
      "step": 199968
    },
    {
      "epoch": 0.0007258396927294119,
      "grad_norm": 8689.166588344362,
      "learning_rate": 2.2371868507134144e-07,
      "loss": 1.7734,
      "step": 200000
    },
    {
      "epoch": 0.0007259558270802486,
      "grad_norm": 10656.963545025385,
      "learning_rate": 2.2370077181283957e-07,
      "loss": 1.7716,
      "step": 200032
    },
    {
      "epoch": 0.0007260719614310853,
      "grad_norm": 9495.799808336315,
      "learning_rate": 2.2368286285661737e-07,
      "loss": 1.7365,
      "step": 200064
    },
    {
      "epoch": 0.000726188095781922,
      "grad_norm": 9203.177712073151,
      "learning_rate": 2.2366495820095303e-07,
      "loss": 1.7092,
      "step": 200096
    },
    {
      "epoch": 0.0007263042301327587,
      "grad_norm": 8700.88443780286,
      "learning_rate": 2.2364705784412557e-07,
      "loss": 1.7182,
      "step": 200128
    },
    {
      "epoch": 0.0007264203644835954,
      "grad_norm": 8751.381605209546,
      "learning_rate": 2.2362916178441506e-07,
      "loss": 1.7053,
      "step": 200160
    },
    {
      "epoch": 0.000726536498834432,
      "grad_norm": 8637.572112578859,
      "learning_rate": 2.2361127002010253e-07,
      "loss": 1.7293,
      "step": 200192
    },
    {
      "epoch": 0.0007266526331852688,
      "grad_norm": 10498.456648479338,
      "learning_rate": 2.2359338254946995e-07,
      "loss": 1.71,
      "step": 200224
    },
    {
      "epoch": 0.0007267687675361056,
      "grad_norm": 8536.952149332923,
      "learning_rate": 2.235754993708003e-07,
      "loss": 1.6958,
      "step": 200256
    },
    {
      "epoch": 0.0007268849018869422,
      "grad_norm": 9573.93806121598,
      "learning_rate": 2.2355762048237737e-07,
      "loss": 1.7327,
      "step": 200288
    },
    {
      "epoch": 0.000727001036237779,
      "grad_norm": 8071.166458449485,
      "learning_rate": 2.2353974588248612e-07,
      "loss": 1.7287,
      "step": 200320
    },
    {
      "epoch": 0.0007271171705886156,
      "grad_norm": 9848.644170646028,
      "learning_rate": 2.235218755694124e-07,
      "loss": 1.7176,
      "step": 200352
    },
    {
      "epoch": 0.0007272333049394524,
      "grad_norm": 9319.738086448568,
      "learning_rate": 2.235040095414429e-07,
      "loss": 1.7285,
      "step": 200384
    },
    {
      "epoch": 0.000727349439290289,
      "grad_norm": 10681.104250029583,
      "learning_rate": 2.2348614779686538e-07,
      "loss": 1.7346,
      "step": 200416
    },
    {
      "epoch": 0.0007274655736411258,
      "grad_norm": 9802.61903778781,
      "learning_rate": 2.2346829033396857e-07,
      "loss": 1.7171,
      "step": 200448
    },
    {
      "epoch": 0.0007275817079919624,
      "grad_norm": 8282.45845145027,
      "learning_rate": 2.234504371510421e-07,
      "loss": 1.734,
      "step": 200480
    },
    {
      "epoch": 0.0007276978423427992,
      "grad_norm": 10204.53938205934,
      "learning_rate": 2.2343258824637658e-07,
      "loss": 1.7271,
      "step": 200512
    },
    {
      "epoch": 0.0007278139766936359,
      "grad_norm": 9894.364254463244,
      "learning_rate": 2.234147436182636e-07,
      "loss": 1.7407,
      "step": 200544
    },
    {
      "epoch": 0.0007279301110444726,
      "grad_norm": 9810.54259457651,
      "learning_rate": 2.2339690326499568e-07,
      "loss": 1.7397,
      "step": 200576
    },
    {
      "epoch": 0.0007280462453953093,
      "grad_norm": 9482.893545748577,
      "learning_rate": 2.2337906718486625e-07,
      "loss": 1.748,
      "step": 200608
    },
    {
      "epoch": 0.000728162379746146,
      "grad_norm": 10882.980290343265,
      "learning_rate": 2.2336179255555297e-07,
      "loss": 1.7357,
      "step": 200640
    },
    {
      "epoch": 0.0007282785140969827,
      "grad_norm": 10417.008399727822,
      "learning_rate": 2.2334396488318163e-07,
      "loss": 1.7343,
      "step": 200672
    },
    {
      "epoch": 0.0007283946484478194,
      "grad_norm": 10654.996762083038,
      "learning_rate": 2.2332614147888816e-07,
      "loss": 1.7448,
      "step": 200704
    },
    {
      "epoch": 0.0007285107827986561,
      "grad_norm": 8726.83344633092,
      "learning_rate": 2.233083223409698e-07,
      "loss": 1.7362,
      "step": 200736
    },
    {
      "epoch": 0.0007286269171494928,
      "grad_norm": 10547.06859748243,
      "learning_rate": 2.2329050746772484e-07,
      "loss": 1.7547,
      "step": 200768
    },
    {
      "epoch": 0.0007287430515003295,
      "grad_norm": 9635.353340692805,
      "learning_rate": 2.2327269685745235e-07,
      "loss": 1.7652,
      "step": 200800
    },
    {
      "epoch": 0.0007288591858511663,
      "grad_norm": 9740.617331565798,
      "learning_rate": 2.2325489050845249e-07,
      "loss": 1.7621,
      "step": 200832
    },
    {
      "epoch": 0.0007289753202020029,
      "grad_norm": 9583.444996450911,
      "learning_rate": 2.232370884190263e-07,
      "loss": 1.7515,
      "step": 200864
    },
    {
      "epoch": 0.0007290914545528397,
      "grad_norm": 9060.083884821377,
      "learning_rate": 2.2321929058747577e-07,
      "loss": 1.7592,
      "step": 200896
    },
    {
      "epoch": 0.0007292075889036763,
      "grad_norm": 9300.16311684908,
      "learning_rate": 2.2320149701210388e-07,
      "loss": 1.7429,
      "step": 200928
    },
    {
      "epoch": 0.0007293237232545131,
      "grad_norm": 10226.61058220171,
      "learning_rate": 2.2318370769121448e-07,
      "loss": 1.7478,
      "step": 200960
    },
    {
      "epoch": 0.0007294398576053497,
      "grad_norm": 9886.221017153117,
      "learning_rate": 2.2316592262311247e-07,
      "loss": 1.74,
      "step": 200992
    },
    {
      "epoch": 0.0007295559919561865,
      "grad_norm": 9812.607808324961,
      "learning_rate": 2.231481418061036e-07,
      "loss": 1.7216,
      "step": 201024
    },
    {
      "epoch": 0.0007296721263070231,
      "grad_norm": 10517.791973603586,
      "learning_rate": 2.231303652384946e-07,
      "loss": 1.7343,
      "step": 201056
    },
    {
      "epoch": 0.0007297882606578599,
      "grad_norm": 10345.823311849086,
      "learning_rate": 2.2311259291859317e-07,
      "loss": 1.7208,
      "step": 201088
    },
    {
      "epoch": 0.0007299043950086966,
      "grad_norm": 9969.120522894686,
      "learning_rate": 2.2309482484470794e-07,
      "loss": 1.7375,
      "step": 201120
    },
    {
      "epoch": 0.0007300205293595333,
      "grad_norm": 9032.416730864448,
      "learning_rate": 2.2307706101514843e-07,
      "loss": 1.7458,
      "step": 201152
    },
    {
      "epoch": 0.00073013666371037,
      "grad_norm": 10225.83923206306,
      "learning_rate": 2.230593014282252e-07,
      "loss": 1.7573,
      "step": 201184
    },
    {
      "epoch": 0.0007302527980612067,
      "grad_norm": 10211.222062025681,
      "learning_rate": 2.2304154608224964e-07,
      "loss": 1.7441,
      "step": 201216
    },
    {
      "epoch": 0.0007303689324120434,
      "grad_norm": 8969.037518039491,
      "learning_rate": 2.2302379497553422e-07,
      "loss": 1.7351,
      "step": 201248
    },
    {
      "epoch": 0.0007304850667628801,
      "grad_norm": 9139.127091796021,
      "learning_rate": 2.2300604810639218e-07,
      "loss": 1.7331,
      "step": 201280
    },
    {
      "epoch": 0.0007306012011137168,
      "grad_norm": 9764.128737373345,
      "learning_rate": 2.2298830547313781e-07,
      "loss": 1.7205,
      "step": 201312
    },
    {
      "epoch": 0.0007307173354645535,
      "grad_norm": 10496.358606678794,
      "learning_rate": 2.2297056707408635e-07,
      "loss": 1.7233,
      "step": 201344
    },
    {
      "epoch": 0.0007308334698153902,
      "grad_norm": 10756.375039947241,
      "learning_rate": 2.2295283290755395e-07,
      "loss": 1.7248,
      "step": 201376
    },
    {
      "epoch": 0.000730949604166227,
      "grad_norm": 10127.826025362008,
      "learning_rate": 2.2293510297185763e-07,
      "loss": 1.7288,
      "step": 201408
    },
    {
      "epoch": 0.0007310657385170636,
      "grad_norm": 10815.480202006753,
      "learning_rate": 2.2291737726531547e-07,
      "loss": 1.7225,
      "step": 201440
    },
    {
      "epoch": 0.0007311818728679004,
      "grad_norm": 8937.493720277515,
      "learning_rate": 2.228996557862464e-07,
      "loss": 1.7223,
      "step": 201472
    },
    {
      "epoch": 0.000731298007218737,
      "grad_norm": 9602.037804549616,
      "learning_rate": 2.2288193853297027e-07,
      "loss": 1.7311,
      "step": 201504
    },
    {
      "epoch": 0.0007314141415695738,
      "grad_norm": 10776.919225827016,
      "learning_rate": 2.2286422550380797e-07,
      "loss": 1.7318,
      "step": 201536
    },
    {
      "epoch": 0.0007315302759204104,
      "grad_norm": 10011.853374875203,
      "learning_rate": 2.2284651669708125e-07,
      "loss": 1.7407,
      "step": 201568
    },
    {
      "epoch": 0.0007316464102712472,
      "grad_norm": 9871.214818855884,
      "learning_rate": 2.2282881211111275e-07,
      "loss": 1.7349,
      "step": 201600
    },
    {
      "epoch": 0.0007317625446220838,
      "grad_norm": 9906.481716532868,
      "learning_rate": 2.2281111174422615e-07,
      "loss": 1.7548,
      "step": 201632
    },
    {
      "epoch": 0.0007318786789729206,
      "grad_norm": 10813.383929187015,
      "learning_rate": 2.2279396853559616e-07,
      "loss": 1.7516,
      "step": 201664
    },
    {
      "epoch": 0.0007319948133237573,
      "grad_norm": 10361.235061516556,
      "learning_rate": 2.227762764701316e-07,
      "loss": 1.7655,
      "step": 201696
    },
    {
      "epoch": 0.000732110947674594,
      "grad_norm": 9942.734633892227,
      "learning_rate": 2.227585886187777e-07,
      "loss": 1.7644,
      "step": 201728
    },
    {
      "epoch": 0.0007322270820254307,
      "grad_norm": 9586.873212888548,
      "learning_rate": 2.227409049798617e-07,
      "loss": 1.78,
      "step": 201760
    },
    {
      "epoch": 0.0007323432163762674,
      "grad_norm": 10155.935210506219,
      "learning_rate": 2.2272322555171192e-07,
      "loss": 1.7758,
      "step": 201792
    },
    {
      "epoch": 0.0007324593507271041,
      "grad_norm": 10778.751690246881,
      "learning_rate": 2.227055503326575e-07,
      "loss": 1.7487,
      "step": 201824
    },
    {
      "epoch": 0.0007325754850779408,
      "grad_norm": 9899.411901724265,
      "learning_rate": 2.2268787932102856e-07,
      "loss": 1.7347,
      "step": 201856
    },
    {
      "epoch": 0.0007326916194287775,
      "grad_norm": 9310.768281941077,
      "learning_rate": 2.2267021251515612e-07,
      "loss": 1.7246,
      "step": 201888
    },
    {
      "epoch": 0.0007328077537796142,
      "grad_norm": 8630.56371275944,
      "learning_rate": 2.2265254991337212e-07,
      "loss": 1.7272,
      "step": 201920
    },
    {
      "epoch": 0.0007329238881304509,
      "grad_norm": 9291.717602252018,
      "learning_rate": 2.2263489151400949e-07,
      "loss": 1.7233,
      "step": 201952
    },
    {
      "epoch": 0.0007330400224812877,
      "grad_norm": 12314.167775371587,
      "learning_rate": 2.22617237315402e-07,
      "loss": 1.7278,
      "step": 201984
    },
    {
      "epoch": 0.0007331561568321243,
      "grad_norm": 9507.277738658948,
      "learning_rate": 2.2259958731588443e-07,
      "loss": 1.727,
      "step": 202016
    },
    {
      "epoch": 0.0007332722911829611,
      "grad_norm": 9391.981473576276,
      "learning_rate": 2.2258194151379242e-07,
      "loss": 1.7377,
      "step": 202048
    },
    {
      "epoch": 0.0007333884255337977,
      "grad_norm": 10356.673597251194,
      "learning_rate": 2.2256429990746256e-07,
      "loss": 1.7375,
      "step": 202080
    },
    {
      "epoch": 0.0007335045598846345,
      "grad_norm": 9001.572529286203,
      "learning_rate": 2.2254666249523227e-07,
      "loss": 1.7347,
      "step": 202112
    },
    {
      "epoch": 0.0007336206942354711,
      "grad_norm": 9145.527868854811,
      "learning_rate": 2.225290292754401e-07,
      "loss": 1.7299,
      "step": 202144
    },
    {
      "epoch": 0.0007337368285863079,
      "grad_norm": 9431.704617936251,
      "learning_rate": 2.2251140024642536e-07,
      "loss": 1.713,
      "step": 202176
    },
    {
      "epoch": 0.0007338529629371445,
      "grad_norm": 8683.602132755737,
      "learning_rate": 2.224937754065283e-07,
      "loss": 1.7284,
      "step": 202208
    },
    {
      "epoch": 0.0007339690972879813,
      "grad_norm": 10721.871851500558,
      "learning_rate": 2.224761547540901e-07,
      "loss": 1.7207,
      "step": 202240
    },
    {
      "epoch": 0.000734085231638818,
      "grad_norm": 9573.305489745953,
      "learning_rate": 2.2245853828745292e-07,
      "loss": 1.7251,
      "step": 202272
    },
    {
      "epoch": 0.0007342013659896547,
      "grad_norm": 9229.499661411772,
      "learning_rate": 2.2244092600495972e-07,
      "loss": 1.7353,
      "step": 202304
    },
    {
      "epoch": 0.0007343175003404914,
      "grad_norm": 8843.856398653248,
      "learning_rate": 2.2242331790495453e-07,
      "loss": 1.7413,
      "step": 202336
    },
    {
      "epoch": 0.0007344336346913281,
      "grad_norm": 10304.703198054758,
      "learning_rate": 2.2240571398578214e-07,
      "loss": 1.7524,
      "step": 202368
    },
    {
      "epoch": 0.0007345497690421648,
      "grad_norm": 10559.466842601476,
      "learning_rate": 2.2238811424578832e-07,
      "loss": 1.744,
      "step": 202400
    },
    {
      "epoch": 0.0007346659033930015,
      "grad_norm": 9712.462200698648,
      "learning_rate": 2.2237051868331985e-07,
      "loss": 1.7374,
      "step": 202432
    },
    {
      "epoch": 0.0007347820377438382,
      "grad_norm": 8212.113856979822,
      "learning_rate": 2.223529272967243e-07,
      "loss": 1.7387,
      "step": 202464
    },
    {
      "epoch": 0.0007348981720946749,
      "grad_norm": 10258.453879605835,
      "learning_rate": 2.223353400843502e-07,
      "loss": 1.7433,
      "step": 202496
    },
    {
      "epoch": 0.0007350143064455116,
      "grad_norm": 9898.931659527709,
      "learning_rate": 2.2231775704454693e-07,
      "loss": 1.7422,
      "step": 202528
    },
    {
      "epoch": 0.0007351304407963484,
      "grad_norm": 9303.116789549618,
      "learning_rate": 2.2230017817566493e-07,
      "loss": 1.7705,
      "step": 202560
    },
    {
      "epoch": 0.000735246575147185,
      "grad_norm": 9075.27575338623,
      "learning_rate": 2.2228260347605543e-07,
      "loss": 1.7545,
      "step": 202592
    },
    {
      "epoch": 0.0007353627094980218,
      "grad_norm": 9164.664096408553,
      "learning_rate": 2.2226503294407063e-07,
      "loss": 1.7533,
      "step": 202624
    },
    {
      "epoch": 0.0007354788438488584,
      "grad_norm": 17858.853490635953,
      "learning_rate": 2.2224801546395843e-07,
      "loss": 1.7567,
      "step": 202656
    },
    {
      "epoch": 0.0007355949781996952,
      "grad_norm": 9715.180080677866,
      "learning_rate": 2.2223045313217272e-07,
      "loss": 1.755,
      "step": 202688
    },
    {
      "epoch": 0.0007357111125505318,
      "grad_norm": 10291.580247950264,
      "learning_rate": 2.222128949631251e-07,
      "loss": 1.7343,
      "step": 202720
    },
    {
      "epoch": 0.0007358272469013686,
      "grad_norm": 11965.309523785834,
      "learning_rate": 2.2219534095517137e-07,
      "loss": 1.7185,
      "step": 202752
    },
    {
      "epoch": 0.0007359433812522052,
      "grad_norm": 9422.343869759796,
      "learning_rate": 2.2217779110666824e-07,
      "loss": 1.7228,
      "step": 202784
    },
    {
      "epoch": 0.000736059515603042,
      "grad_norm": 9211.924771729304,
      "learning_rate": 2.221602454159733e-07,
      "loss": 1.7238,
      "step": 202816
    },
    {
      "epoch": 0.0007361756499538787,
      "grad_norm": 9536.64049862424,
      "learning_rate": 2.2214270388144508e-07,
      "loss": 1.7268,
      "step": 202848
    },
    {
      "epoch": 0.0007362917843047154,
      "grad_norm": 10501.656821663904,
      "learning_rate": 2.2212516650144298e-07,
      "loss": 1.7337,
      "step": 202880
    },
    {
      "epoch": 0.0007364079186555521,
      "grad_norm": 8676.641055154927,
      "learning_rate": 2.2210763327432738e-07,
      "loss": 1.7493,
      "step": 202912
    },
    {
      "epoch": 0.0007365240530063888,
      "grad_norm": 8787.763310422057,
      "learning_rate": 2.2209010419845948e-07,
      "loss": 1.7701,
      "step": 202944
    },
    {
      "epoch": 0.0007366401873572255,
      "grad_norm": 10163.340002184323,
      "learning_rate": 2.2207257927220146e-07,
      "loss": 1.7536,
      "step": 202976
    },
    {
      "epoch": 0.0007367563217080622,
      "grad_norm": 9855.18726356836,
      "learning_rate": 2.2205505849391638e-07,
      "loss": 1.7223,
      "step": 203008
    },
    {
      "epoch": 0.0007368724560588989,
      "grad_norm": 9858.490148090628,
      "learning_rate": 2.2203754186196815e-07,
      "loss": 1.7141,
      "step": 203040
    },
    {
      "epoch": 0.0007369885904097356,
      "grad_norm": 9973.151558058265,
      "learning_rate": 2.2202002937472163e-07,
      "loss": 1.7198,
      "step": 203072
    },
    {
      "epoch": 0.0007371047247605723,
      "grad_norm": 9019.641123681142,
      "learning_rate": 2.220025210305426e-07,
      "loss": 1.7203,
      "step": 203104
    },
    {
      "epoch": 0.0007372208591114091,
      "grad_norm": 9705.249713428295,
      "learning_rate": 2.2198501682779767e-07,
      "loss": 1.7232,
      "step": 203136
    },
    {
      "epoch": 0.0007373369934622457,
      "grad_norm": 9008.58246340677,
      "learning_rate": 2.2196751676485446e-07,
      "loss": 1.727,
      "step": 203168
    },
    {
      "epoch": 0.0007374531278130825,
      "grad_norm": 8979.099509416299,
      "learning_rate": 2.2195002084008142e-07,
      "loss": 1.7266,
      "step": 203200
    },
    {
      "epoch": 0.0007375692621639191,
      "grad_norm": 10198.529501844861,
      "learning_rate": 2.219325290518479e-07,
      "loss": 1.728,
      "step": 203232
    },
    {
      "epoch": 0.0007376853965147559,
      "grad_norm": 9080.002533039295,
      "learning_rate": 2.219150413985242e-07,
      "loss": 1.7294,
      "step": 203264
    },
    {
      "epoch": 0.0007378015308655925,
      "grad_norm": 9198.512923293634,
      "learning_rate": 2.2189755787848143e-07,
      "loss": 1.7392,
      "step": 203296
    },
    {
      "epoch": 0.0007379176652164293,
      "grad_norm": 8970.52127805291,
      "learning_rate": 2.2188007849009164e-07,
      "loss": 1.7343,
      "step": 203328
    },
    {
      "epoch": 0.0007380337995672659,
      "grad_norm": 12232.893034764917,
      "learning_rate": 2.2186260323172785e-07,
      "loss": 1.7423,
      "step": 203360
    },
    {
      "epoch": 0.0007381499339181027,
      "grad_norm": 9132.31208402341,
      "learning_rate": 2.218451321017639e-07,
      "loss": 1.7477,
      "step": 203392
    },
    {
      "epoch": 0.0007382660682689394,
      "grad_norm": 9159.582850763456,
      "learning_rate": 2.218276650985745e-07,
      "loss": 1.7497,
      "step": 203424
    },
    {
      "epoch": 0.0007383822026197761,
      "grad_norm": 10032.352665252552,
      "learning_rate": 2.218102022205353e-07,
      "loss": 1.7678,
      "step": 203456
    },
    {
      "epoch": 0.0007384983369706128,
      "grad_norm": 9624.65417560548,
      "learning_rate": 2.2179274346602284e-07,
      "loss": 1.762,
      "step": 203488
    },
    {
      "epoch": 0.0007386144713214495,
      "grad_norm": 10018.452275676118,
      "learning_rate": 2.217752888334146e-07,
      "loss": 1.781,
      "step": 203520
    },
    {
      "epoch": 0.0007387306056722862,
      "grad_norm": 10298.336758914034,
      "learning_rate": 2.2175783832108884e-07,
      "loss": 1.7841,
      "step": 203552
    },
    {
      "epoch": 0.0007388467400231229,
      "grad_norm": 9476.74226725619,
      "learning_rate": 2.2174039192742486e-07,
      "loss": 1.7292,
      "step": 203584
    },
    {
      "epoch": 0.0007389628743739596,
      "grad_norm": 10128.148695590917,
      "learning_rate": 2.217229496508027e-07,
      "loss": 1.7128,
      "step": 203616
    },
    {
      "epoch": 0.0007390790087247963,
      "grad_norm": 11290.337993169203,
      "learning_rate": 2.2170551148960343e-07,
      "loss": 1.7236,
      "step": 203648
    },
    {
      "epoch": 0.000739195143075633,
      "grad_norm": 11443.273133155566,
      "learning_rate": 2.2168862219393653e-07,
      "loss": 1.7216,
      "step": 203680
    },
    {
      "epoch": 0.0007393112774264698,
      "grad_norm": 8795.791038900366,
      "learning_rate": 2.2167119213024816e-07,
      "loss": 1.7282,
      "step": 203712
    },
    {
      "epoch": 0.0007394274117773064,
      "grad_norm": 10058.07536261287,
      "learning_rate": 2.2165376617718148e-07,
      "loss": 1.7341,
      "step": 203744
    },
    {
      "epoch": 0.0007395435461281432,
      "grad_norm": 9065.491933701116,
      "learning_rate": 2.2163634433312108e-07,
      "loss": 1.7256,
      "step": 203776
    },
    {
      "epoch": 0.0007396596804789798,
      "grad_norm": 8790.55959538413,
      "learning_rate": 2.2161892659645238e-07,
      "loss": 1.749,
      "step": 203808
    },
    {
      "epoch": 0.0007397758148298166,
      "grad_norm": 11897.890065049349,
      "learning_rate": 2.2160151296556166e-07,
      "loss": 1.7379,
      "step": 203840
    },
    {
      "epoch": 0.0007398919491806532,
      "grad_norm": 8017.493748048701,
      "learning_rate": 2.215841034388361e-07,
      "loss": 1.7158,
      "step": 203872
    },
    {
      "epoch": 0.00074000808353149,
      "grad_norm": 9411.367594563502,
      "learning_rate": 2.2156669801466387e-07,
      "loss": 1.7249,
      "step": 203904
    },
    {
      "epoch": 0.0007401242178823266,
      "grad_norm": 9359.611316716095,
      "learning_rate": 2.2154929669143393e-07,
      "loss": 1.7151,
      "step": 203936
    },
    {
      "epoch": 0.0007402403522331634,
      "grad_norm": 10849.245319375907,
      "learning_rate": 2.215318994675361e-07,
      "loss": 1.7234,
      "step": 203968
    },
    {
      "epoch": 0.0007403564865840002,
      "grad_norm": 9700.56225174603,
      "learning_rate": 2.2151450634136112e-07,
      "loss": 1.7164,
      "step": 204000
    },
    {
      "epoch": 0.0007404726209348368,
      "grad_norm": 11519.527247244134,
      "learning_rate": 2.2149711731130066e-07,
      "loss": 1.7245,
      "step": 204032
    },
    {
      "epoch": 0.0007405887552856736,
      "grad_norm": 10930.868035064734,
      "learning_rate": 2.2147973237574723e-07,
      "loss": 1.7343,
      "step": 204064
    },
    {
      "epoch": 0.0007407048896365102,
      "grad_norm": 8970.908315215354,
      "learning_rate": 2.2146235153309422e-07,
      "loss": 1.753,
      "step": 204096
    },
    {
      "epoch": 0.000740821023987347,
      "grad_norm": 9134.252897747028,
      "learning_rate": 2.2144497478173589e-07,
      "loss": 1.7468,
      "step": 204128
    },
    {
      "epoch": 0.0007409371583381836,
      "grad_norm": 11244.485403965804,
      "learning_rate": 2.2142760212006742e-07,
      "loss": 1.7466,
      "step": 204160
    },
    {
      "epoch": 0.0007410532926890204,
      "grad_norm": 9289.375759436152,
      "learning_rate": 2.2141023354648487e-07,
      "loss": 1.7353,
      "step": 204192
    },
    {
      "epoch": 0.000741169427039857,
      "grad_norm": 10205.43815815862,
      "learning_rate": 2.213928690593851e-07,
      "loss": 1.7356,
      "step": 204224
    },
    {
      "epoch": 0.0007412855613906938,
      "grad_norm": 8722.242372234332,
      "learning_rate": 2.21375508657166e-07,
      "loss": 1.7459,
      "step": 204256
    },
    {
      "epoch": 0.0007414016957415305,
      "grad_norm": 9970.189366305938,
      "learning_rate": 2.2135815233822622e-07,
      "loss": 1.7415,
      "step": 204288
    },
    {
      "epoch": 0.0007415178300923672,
      "grad_norm": 11082.354984388472,
      "learning_rate": 2.2134080010096528e-07,
      "loss": 1.7729,
      "step": 204320
    },
    {
      "epoch": 0.0007416339644432039,
      "grad_norm": 11103.568435417508,
      "learning_rate": 2.213234519437837e-07,
      "loss": 1.7526,
      "step": 204352
    },
    {
      "epoch": 0.0007417500987940406,
      "grad_norm": 10070.766008601331,
      "learning_rate": 2.2130610786508272e-07,
      "loss": 1.7621,
      "step": 204384
    },
    {
      "epoch": 0.0007418662331448773,
      "grad_norm": 10281.9784088472,
      "learning_rate": 2.2128876786326453e-07,
      "loss": 1.77,
      "step": 204416
    },
    {
      "epoch": 0.000741982367495714,
      "grad_norm": 8540.735214254099,
      "learning_rate": 2.2127143193673225e-07,
      "loss": 1.7236,
      "step": 204448
    },
    {
      "epoch": 0.0007420985018465507,
      "grad_norm": 9648.406396913431,
      "learning_rate": 2.2125410008388985e-07,
      "loss": 1.7222,
      "step": 204480
    },
    {
      "epoch": 0.0007422146361973874,
      "grad_norm": 8697.407889710588,
      "learning_rate": 2.2123677230314206e-07,
      "loss": 1.7188,
      "step": 204512
    },
    {
      "epoch": 0.0007423307705482241,
      "grad_norm": 9772.868770222998,
      "learning_rate": 2.2121944859289464e-07,
      "loss": 1.7252,
      "step": 204544
    },
    {
      "epoch": 0.0007424469048990609,
      "grad_norm": 11044.052698171989,
      "learning_rate": 2.2120212895155413e-07,
      "loss": 1.7233,
      "step": 204576
    },
    {
      "epoch": 0.0007425630392498975,
      "grad_norm": 8809.218580555258,
      "learning_rate": 2.2118481337752797e-07,
      "loss": 1.7224,
      "step": 204608
    },
    {
      "epoch": 0.0007426791736007343,
      "grad_norm": 9518.311404865886,
      "learning_rate": 2.2116750186922452e-07,
      "loss": 1.7307,
      "step": 204640
    },
    {
      "epoch": 0.0007427953079515709,
      "grad_norm": 9767.30740787859,
      "learning_rate": 2.2115073522118146e-07,
      "loss": 1.7561,
      "step": 204672
    },
    {
      "epoch": 0.0007429114423024077,
      "grad_norm": 10420.786726538452,
      "learning_rate": 2.2113343171262142e-07,
      "loss": 1.777,
      "step": 204704
    },
    {
      "epoch": 0.0007430275766532443,
      "grad_norm": 10597.83808142019,
      "learning_rate": 2.2111613226506386e-07,
      "loss": 1.7319,
      "step": 204736
    },
    {
      "epoch": 0.0007431437110040811,
      "grad_norm": 9514.507238948321,
      "learning_rate": 2.2109883687692056e-07,
      "loss": 1.7248,
      "step": 204768
    },
    {
      "epoch": 0.0007432598453549177,
      "grad_norm": 8689.3471561447,
      "learning_rate": 2.2108154554660416e-07,
      "loss": 1.7156,
      "step": 204800
    },
    {
      "epoch": 0.0007433759797057545,
      "grad_norm": 10005.521675554954,
      "learning_rate": 2.2106425827252818e-07,
      "loss": 1.724,
      "step": 204832
    },
    {
      "epoch": 0.0007434921140565912,
      "grad_norm": 9357.993588371388,
      "learning_rate": 2.2104697505310698e-07,
      "loss": 1.7114,
      "step": 204864
    },
    {
      "epoch": 0.0007436082484074279,
      "grad_norm": 11252.257551264991,
      "learning_rate": 2.2102969588675582e-07,
      "loss": 1.7259,
      "step": 204896
    },
    {
      "epoch": 0.0007437243827582646,
      "grad_norm": 10614.835844232355,
      "learning_rate": 2.2101242077189084e-07,
      "loss": 1.7321,
      "step": 204928
    },
    {
      "epoch": 0.0007438405171091013,
      "grad_norm": 9721.146845923067,
      "learning_rate": 2.2099514970692897e-07,
      "loss": 1.7288,
      "step": 204960
    },
    {
      "epoch": 0.000743956651459938,
      "grad_norm": 8453.58894198198,
      "learning_rate": 2.2097788269028805e-07,
      "loss": 1.7298,
      "step": 204992
    },
    {
      "epoch": 0.0007440727858107747,
      "grad_norm": 9983.685892494816,
      "learning_rate": 2.2096061972038681e-07,
      "loss": 1.7277,
      "step": 205024
    },
    {
      "epoch": 0.0007441889201616114,
      "grad_norm": 11081.601328327959,
      "learning_rate": 2.2094336079564487e-07,
      "loss": 1.7373,
      "step": 205056
    },
    {
      "epoch": 0.0007443050545124481,
      "grad_norm": 10248.880524232878,
      "learning_rate": 2.209261059144826e-07,
      "loss": 1.7335,
      "step": 205088
    },
    {
      "epoch": 0.0007444211888632848,
      "grad_norm": 10287.501737545419,
      "learning_rate": 2.209088550753213e-07,
      "loss": 1.7444,
      "step": 205120
    },
    {
      "epoch": 0.0007445373232141216,
      "grad_norm": 9122.418538962133,
      "learning_rate": 2.208916082765832e-07,
      "loss": 1.7382,
      "step": 205152
    },
    {
      "epoch": 0.0007446534575649582,
      "grad_norm": 12858.027220378715,
      "learning_rate": 2.2087436551669126e-07,
      "loss": 1.7515,
      "step": 205184
    },
    {
      "epoch": 0.000744769591915795,
      "grad_norm": 10215.125256207091,
      "learning_rate": 2.208571267940694e-07,
      "loss": 1.7634,
      "step": 205216
    },
    {
      "epoch": 0.0007448857262666316,
      "grad_norm": 10875.76213421386,
      "learning_rate": 2.2083989210714238e-07,
      "loss": 1.7697,
      "step": 205248
    },
    {
      "epoch": 0.0007450018606174684,
      "grad_norm": 10889.087565080923,
      "learning_rate": 2.2082266145433575e-07,
      "loss": 1.7862,
      "step": 205280
    },
    {
      "epoch": 0.000745117994968305,
      "grad_norm": 10450.50084924163,
      "learning_rate": 2.2080543483407605e-07,
      "loss": 1.754,
      "step": 205312
    },
    {
      "epoch": 0.0007452341293191418,
      "grad_norm": 10020.529327335957,
      "learning_rate": 2.2078821224479057e-07,
      "loss": 1.7398,
      "step": 205344
    },
    {
      "epoch": 0.0007453502636699784,
      "grad_norm": 9449.561471306486,
      "learning_rate": 2.2077099368490752e-07,
      "loss": 1.7093,
      "step": 205376
    },
    {
      "epoch": 0.0007454663980208152,
      "grad_norm": 10660.096059604717,
      "learning_rate": 2.2075377915285592e-07,
      "loss": 1.7241,
      "step": 205408
    },
    {
      "epoch": 0.0007455825323716519,
      "grad_norm": 9461.487832259787,
      "learning_rate": 2.2073656864706567e-07,
      "loss": 1.7293,
      "step": 205440
    },
    {
      "epoch": 0.0007456986667224886,
      "grad_norm": 9534.663496946287,
      "learning_rate": 2.2071936216596755e-07,
      "loss": 1.716,
      "step": 205472
    },
    {
      "epoch": 0.0007458148010733253,
      "grad_norm": 9817.358198619422,
      "learning_rate": 2.207021597079932e-07,
      "loss": 1.7398,
      "step": 205504
    },
    {
      "epoch": 0.000745930935424162,
      "grad_norm": 9980.068737238236,
      "learning_rate": 2.20684961271575e-07,
      "loss": 1.7394,
      "step": 205536
    },
    {
      "epoch": 0.0007460470697749987,
      "grad_norm": 9212.730539856248,
      "learning_rate": 2.2066776685514638e-07,
      "loss": 1.7511,
      "step": 205568
    },
    {
      "epoch": 0.0007461632041258354,
      "grad_norm": 9813.437114487462,
      "learning_rate": 2.2065057645714144e-07,
      "loss": 1.7128,
      "step": 205600
    },
    {
      "epoch": 0.0007462793384766721,
      "grad_norm": 9715.883799222796,
      "learning_rate": 2.2063339007599527e-07,
      "loss": 1.7247,
      "step": 205632
    },
    {
      "epoch": 0.0007463954728275088,
      "grad_norm": 9226.019943615991,
      "learning_rate": 2.206162077101437e-07,
      "loss": 1.7144,
      "step": 205664
    },
    {
      "epoch": 0.0007465116071783455,
      "grad_norm": 8854.652788223828,
      "learning_rate": 2.2059956612078805e-07,
      "loss": 1.7185,
      "step": 205696
    },
    {
      "epoch": 0.0007466277415291823,
      "grad_norm": 10858.58296464138,
      "learning_rate": 2.205823916554802e-07,
      "loss": 1.712,
      "step": 205728
    },
    {
      "epoch": 0.0007467438758800189,
      "grad_norm": 9625.97839183114,
      "learning_rate": 2.2056522120082852e-07,
      "loss": 1.721,
      "step": 205760
    },
    {
      "epoch": 0.0007468600102308557,
      "grad_norm": 10063.94773436349,
      "learning_rate": 2.205480547552723e-07,
      "loss": 1.7313,
      "step": 205792
    },
    {
      "epoch": 0.0007469761445816923,
      "grad_norm": 10545.025936430882,
      "learning_rate": 2.2053089231725162e-07,
      "loss": 1.7443,
      "step": 205824
    },
    {
      "epoch": 0.0007470922789325291,
      "grad_norm": 10157.665676719233,
      "learning_rate": 2.205137338852075e-07,
      "loss": 1.7468,
      "step": 205856
    },
    {
      "epoch": 0.0007472084132833657,
      "grad_norm": 9870.486816768462,
      "learning_rate": 2.2049657945758176e-07,
      "loss": 1.7426,
      "step": 205888
    },
    {
      "epoch": 0.0007473245476342025,
      "grad_norm": 8870.816986050382,
      "learning_rate": 2.2047942903281701e-07,
      "loss": 1.7562,
      "step": 205920
    },
    {
      "epoch": 0.0007474406819850391,
      "grad_norm": 9807.841964469044,
      "learning_rate": 2.2046228260935677e-07,
      "loss": 1.7308,
      "step": 205952
    },
    {
      "epoch": 0.0007475568163358759,
      "grad_norm": 9630.974197868043,
      "learning_rate": 2.2044514018564545e-07,
      "loss": 1.7354,
      "step": 205984
    },
    {
      "epoch": 0.0007476729506867126,
      "grad_norm": 9478.569406825061,
      "learning_rate": 2.2042800176012818e-07,
      "loss": 1.7468,
      "step": 206016
    },
    {
      "epoch": 0.0007477890850375493,
      "grad_norm": 11588.638315177499,
      "learning_rate": 2.204108673312511e-07,
      "loss": 1.7435,
      "step": 206048
    },
    {
      "epoch": 0.000747905219388386,
      "grad_norm": 8988.395407412827,
      "learning_rate": 2.2039373689746103e-07,
      "loss": 1.7644,
      "step": 206080
    },
    {
      "epoch": 0.0007480213537392227,
      "grad_norm": 9121.249585446065,
      "learning_rate": 2.2037661045720573e-07,
      "loss": 1.7661,
      "step": 206112
    },
    {
      "epoch": 0.0007481374880900594,
      "grad_norm": 9983.333711741785,
      "learning_rate": 2.203594880089338e-07,
      "loss": 1.7602,
      "step": 206144
    },
    {
      "epoch": 0.0007482536224408961,
      "grad_norm": 9031.558558742781,
      "learning_rate": 2.2034236955109466e-07,
      "loss": 1.7337,
      "step": 206176
    },
    {
      "epoch": 0.0007483697567917328,
      "grad_norm": 9346.417923461371,
      "learning_rate": 2.2032525508213862e-07,
      "loss": 1.7246,
      "step": 206208
    },
    {
      "epoch": 0.0007484858911425695,
      "grad_norm": 9884.264666630492,
      "learning_rate": 2.2030814460051675e-07,
      "loss": 1.7106,
      "step": 206240
    },
    {
      "epoch": 0.0007486020254934062,
      "grad_norm": 10145.049334527654,
      "learning_rate": 2.20291038104681e-07,
      "loss": 1.722,
      "step": 206272
    },
    {
      "epoch": 0.000748718159844243,
      "grad_norm": 11986.788060193607,
      "learning_rate": 2.202739355930842e-07,
      "loss": 1.728,
      "step": 206304
    },
    {
      "epoch": 0.0007488342941950796,
      "grad_norm": 9027.29117731338,
      "learning_rate": 2.2025683706417995e-07,
      "loss": 1.7195,
      "step": 206336
    },
    {
      "epoch": 0.0007489504285459164,
      "grad_norm": 10208.391450174704,
      "learning_rate": 2.202397425164228e-07,
      "loss": 1.7293,
      "step": 206368
    },
    {
      "epoch": 0.000749066562896753,
      "grad_norm": 9193.766910249575,
      "learning_rate": 2.2022265194826797e-07,
      "loss": 1.7401,
      "step": 206400
    },
    {
      "epoch": 0.0007491826972475898,
      "grad_norm": 9976.965670984338,
      "learning_rate": 2.2020556535817166e-07,
      "loss": 1.7597,
      "step": 206432
    },
    {
      "epoch": 0.0007492988315984264,
      "grad_norm": 8571.251833892176,
      "learning_rate": 2.2018848274459092e-07,
      "loss": 1.7418,
      "step": 206464
    },
    {
      "epoch": 0.0007494149659492632,
      "grad_norm": 9358.546575189974,
      "learning_rate": 2.2017140410598352e-07,
      "loss": 1.7338,
      "step": 206496
    },
    {
      "epoch": 0.0007495311003000998,
      "grad_norm": 9582.76139742611,
      "learning_rate": 2.2015432944080812e-07,
      "loss": 1.7278,
      "step": 206528
    },
    {
      "epoch": 0.0007496472346509366,
      "grad_norm": 10034.277054177845,
      "learning_rate": 2.201372587475243e-07,
      "loss": 1.7137,
      "step": 206560
    },
    {
      "epoch": 0.0007497633690017733,
      "grad_norm": 11041.325463910573,
      "learning_rate": 2.2012019202459233e-07,
      "loss": 1.716,
      "step": 206592
    },
    {
      "epoch": 0.00074987950335261,
      "grad_norm": 9684.295121484063,
      "learning_rate": 2.201031292704734e-07,
      "loss": 1.7149,
      "step": 206624
    },
    {
      "epoch": 0.0007499956377034467,
      "grad_norm": 8953.767698572485,
      "learning_rate": 2.200860704836296e-07,
      "loss": 1.73,
      "step": 206656
    },
    {
      "epoch": 0.0007501117720542834,
      "grad_norm": 11409.625410152605,
      "learning_rate": 2.2006954856567021e-07,
      "loss": 1.7403,
      "step": 206688
    },
    {
      "epoch": 0.0007502279064051201,
      "grad_norm": 9291.244373064352,
      "learning_rate": 2.2005249758490785e-07,
      "loss": 1.7323,
      "step": 206720
    },
    {
      "epoch": 0.0007503440407559568,
      "grad_norm": 9932.114981211202,
      "learning_rate": 2.2003545056685956e-07,
      "loss": 1.7254,
      "step": 206752
    },
    {
      "epoch": 0.0007504601751067935,
      "grad_norm": 7934.305640697237,
      "learning_rate": 2.2001840750999065e-07,
      "loss": 1.7268,
      "step": 206784
    },
    {
      "epoch": 0.0007505763094576302,
      "grad_norm": 9727.523014621966,
      "learning_rate": 2.2000136841276733e-07,
      "loss": 1.7351,
      "step": 206816
    },
    {
      "epoch": 0.0007506924438084669,
      "grad_norm": 9663.471425942129,
      "learning_rate": 2.1998433327365653e-07,
      "loss": 1.7358,
      "step": 206848
    },
    {
      "epoch": 0.0007508085781593037,
      "grad_norm": 10997.918166634992,
      "learning_rate": 2.199673020911261e-07,
      "loss": 1.7412,
      "step": 206880
    },
    {
      "epoch": 0.0007509247125101403,
      "grad_norm": 10712.342040842423,
      "learning_rate": 2.1995027486364466e-07,
      "loss": 1.7409,
      "step": 206912
    },
    {
      "epoch": 0.0007510408468609771,
      "grad_norm": 10821.731654407256,
      "learning_rate": 2.199332515896817e-07,
      "loss": 1.7611,
      "step": 206944
    },
    {
      "epoch": 0.0007511569812118137,
      "grad_norm": 8453.463432227054,
      "learning_rate": 2.199162322677075e-07,
      "loss": 1.7681,
      "step": 206976
    },
    {
      "epoch": 0.0007512731155626505,
      "grad_norm": 10573.48400481128,
      "learning_rate": 2.1989921689619322e-07,
      "loss": 1.764,
      "step": 207008
    },
    {
      "epoch": 0.0007513892499134871,
      "grad_norm": 9270.20409699808,
      "learning_rate": 2.1988220547361086e-07,
      "loss": 1.7606,
      "step": 207040
    },
    {
      "epoch": 0.0007515053842643239,
      "grad_norm": 11004.748793134717,
      "learning_rate": 2.1986519799843312e-07,
      "loss": 1.7509,
      "step": 207072
    },
    {
      "epoch": 0.0007516215186151605,
      "grad_norm": 10026.494901011021,
      "learning_rate": 2.1984819446913366e-07,
      "loss": 1.7402,
      "step": 207104
    },
    {
      "epoch": 0.0007517376529659973,
      "grad_norm": 9585.854369851442,
      "learning_rate": 2.1983119488418693e-07,
      "loss": 1.7092,
      "step": 207136
    },
    {
      "epoch": 0.000751853787316834,
      "grad_norm": 10060.569367585515,
      "learning_rate": 2.1981419924206817e-07,
      "loss": 1.7268,
      "step": 207168
    },
    {
      "epoch": 0.0007519699216676707,
      "grad_norm": 10260.326700451598,
      "learning_rate": 2.1979720754125347e-07,
      "loss": 1.7221,
      "step": 207200
    },
    {
      "epoch": 0.0007520860560185074,
      "grad_norm": 12403.75878514251,
      "learning_rate": 2.1978021978021978e-07,
      "loss": 1.7261,
      "step": 207232
    },
    {
      "epoch": 0.0007522021903693441,
      "grad_norm": 10273.294700338349,
      "learning_rate": 2.1976323595744482e-07,
      "loss": 1.7415,
      "step": 207264
    },
    {
      "epoch": 0.0007523183247201808,
      "grad_norm": 11089.010956798626,
      "learning_rate": 2.1974625607140717e-07,
      "loss": 1.7381,
      "step": 207296
    },
    {
      "epoch": 0.0007524344590710175,
      "grad_norm": 9256.374452235605,
      "learning_rate": 2.1972928012058616e-07,
      "loss": 1.7365,
      "step": 207328
    },
    {
      "epoch": 0.0007525505934218542,
      "grad_norm": 9532.298568551028,
      "learning_rate": 2.1971230810346204e-07,
      "loss": 1.7149,
      "step": 207360
    },
    {
      "epoch": 0.0007526667277726909,
      "grad_norm": 9431.242442011551,
      "learning_rate": 2.1969534001851585e-07,
      "loss": 1.7222,
      "step": 207392
    },
    {
      "epoch": 0.0007527828621235276,
      "grad_norm": 10014.399432816726,
      "learning_rate": 2.1967837586422944e-07,
      "loss": 1.7137,
      "step": 207424
    },
    {
      "epoch": 0.0007528989964743644,
      "grad_norm": 9973.84559736113,
      "learning_rate": 2.1966141563908543e-07,
      "loss": 1.7188,
      "step": 207456
    },
    {
      "epoch": 0.000753015130825201,
      "grad_norm": 10012.192966578301,
      "learning_rate": 2.196444593415674e-07,
      "loss": 1.7145,
      "step": 207488
    },
    {
      "epoch": 0.0007531312651760378,
      "grad_norm": 10385.845752754081,
      "learning_rate": 2.1962750697015956e-07,
      "loss": 1.7328,
      "step": 207520
    },
    {
      "epoch": 0.0007532473995268744,
      "grad_norm": 9556.678921047833,
      "learning_rate": 2.196105585233471e-07,
      "loss": 1.7301,
      "step": 207552
    },
    {
      "epoch": 0.0007533635338777112,
      "grad_norm": 10119.201944817585,
      "learning_rate": 2.1959361399961598e-07,
      "loss": 1.742,
      "step": 207584
    },
    {
      "epoch": 0.0007534796682285478,
      "grad_norm": 10061.465201450532,
      "learning_rate": 2.195766733974529e-07,
      "loss": 1.7433,
      "step": 207616
    },
    {
      "epoch": 0.0007535958025793846,
      "grad_norm": 9830.842385065484,
      "learning_rate": 2.1955973671534549e-07,
      "loss": 1.7491,
      "step": 207648
    },
    {
      "epoch": 0.0007537119369302212,
      "grad_norm": 9699.175738174868,
      "learning_rate": 2.1954333304134462e-07,
      "loss": 1.7554,
      "step": 207680
    },
    {
      "epoch": 0.000753828071281058,
      "grad_norm": 9274.891266209,
      "learning_rate": 2.1952640407243012e-07,
      "loss": 1.7355,
      "step": 207712
    },
    {
      "epoch": 0.0007539442056318947,
      "grad_norm": 9324.465239358233,
      "learning_rate": 2.195094790190861e-07,
      "loss": 1.7426,
      "step": 207744
    },
    {
      "epoch": 0.0007540603399827314,
      "grad_norm": 9876.320772433426,
      "learning_rate": 2.1949255787980338e-07,
      "loss": 1.7392,
      "step": 207776
    },
    {
      "epoch": 0.0007541764743335681,
      "grad_norm": 11405.288773196406,
      "learning_rate": 2.1947564065307356e-07,
      "loss": 1.7481,
      "step": 207808
    },
    {
      "epoch": 0.0007542926086844048,
      "grad_norm": 9451.784170197709,
      "learning_rate": 2.1945872733738913e-07,
      "loss": 1.7729,
      "step": 207840
    },
    {
      "epoch": 0.0007544087430352415,
      "grad_norm": 9562.137522541705,
      "learning_rate": 2.1944181793124333e-07,
      "loss": 1.7584,
      "step": 207872
    },
    {
      "epoch": 0.0007545248773860782,
      "grad_norm": 9365.238704912972,
      "learning_rate": 2.1942491243313026e-07,
      "loss": 1.7455,
      "step": 207904
    },
    {
      "epoch": 0.000754641011736915,
      "grad_norm": 10018.606489926631,
      "learning_rate": 2.1940801084154477e-07,
      "loss": 1.7342,
      "step": 207936
    },
    {
      "epoch": 0.0007547571460877516,
      "grad_norm": 9531.685685124117,
      "learning_rate": 2.193911131549826e-07,
      "loss": 1.7216,
      "step": 207968
    },
    {
      "epoch": 0.0007548732804385883,
      "grad_norm": 10434.99880210822,
      "learning_rate": 2.1937421937194018e-07,
      "loss": 1.7156,
      "step": 208000
    },
    {
      "epoch": 0.0007549894147894251,
      "grad_norm": 9505.18279676935,
      "learning_rate": 2.1935732949091488e-07,
      "loss": 1.7301,
      "step": 208032
    },
    {
      "epoch": 0.0007551055491402617,
      "grad_norm": 11039.872281869932,
      "learning_rate": 2.1934044351040482e-07,
      "loss": 1.7201,
      "step": 208064
    },
    {
      "epoch": 0.0007552216834910985,
      "grad_norm": 10056.81937791467,
      "learning_rate": 2.1932356142890893e-07,
      "loss": 1.7231,
      "step": 208096
    },
    {
      "epoch": 0.0007553378178419351,
      "grad_norm": 9262.885295630082,
      "learning_rate": 2.1930668324492697e-07,
      "loss": 1.739,
      "step": 208128
    },
    {
      "epoch": 0.0007554539521927719,
      "grad_norm": 8714.212873231867,
      "learning_rate": 2.1928980895695947e-07,
      "loss": 1.7309,
      "step": 208160
    },
    {
      "epoch": 0.0007555700865436085,
      "grad_norm": 12311.449955224609,
      "learning_rate": 2.192729385635078e-07,
      "loss": 1.7461,
      "step": 208192
    },
    {
      "epoch": 0.0007556862208944453,
      "grad_norm": 9685.991843894975,
      "learning_rate": 2.1925607206307416e-07,
      "loss": 1.7419,
      "step": 208224
    },
    {
      "epoch": 0.000755802355245282,
      "grad_norm": 9312.401838408821,
      "learning_rate": 2.1923920945416148e-07,
      "loss": 1.7371,
      "step": 208256
    },
    {
      "epoch": 0.0007559184895961187,
      "grad_norm": 9831.381083042199,
      "learning_rate": 2.1922235073527352e-07,
      "loss": 1.7246,
      "step": 208288
    },
    {
      "epoch": 0.0007560346239469555,
      "grad_norm": 9114.983817868246,
      "learning_rate": 2.192054959049149e-07,
      "loss": 1.716,
      "step": 208320
    },
    {
      "epoch": 0.0007561507582977921,
      "grad_norm": 11046.551860196014,
      "learning_rate": 2.1918864496159103e-07,
      "loss": 1.7176,
      "step": 208352
    },
    {
      "epoch": 0.0007562668926486289,
      "grad_norm": 9602.929448871317,
      "learning_rate": 2.1917179790380803e-07,
      "loss": 1.7272,
      "step": 208384
    },
    {
      "epoch": 0.0007563830269994655,
      "grad_norm": 10569.013009737475,
      "learning_rate": 2.19154954730073e-07,
      "loss": 1.7305,
      "step": 208416
    },
    {
      "epoch": 0.0007564991613503023,
      "grad_norm": 9831.444858208788,
      "learning_rate": 2.1913811543889365e-07,
      "loss": 1.7469,
      "step": 208448
    },
    {
      "epoch": 0.0007566152957011389,
      "grad_norm": 9077.4929358276,
      "learning_rate": 2.1912128002877864e-07,
      "loss": 1.7354,
      "step": 208480
    },
    {
      "epoch": 0.0007567314300519757,
      "grad_norm": 10615.92935168655,
      "learning_rate": 2.191044484982373e-07,
      "loss": 1.7239,
      "step": 208512
    },
    {
      "epoch": 0.0007568475644028123,
      "grad_norm": 9803.813135714083,
      "learning_rate": 2.1908762084577993e-07,
      "loss": 1.7316,
      "step": 208544
    },
    {
      "epoch": 0.0007569636987536491,
      "grad_norm": 9785.954015833102,
      "learning_rate": 2.1907079706991748e-07,
      "loss": 1.7271,
      "step": 208576
    },
    {
      "epoch": 0.0007570798331044858,
      "grad_norm": 8791.235180564789,
      "learning_rate": 2.190539771691618e-07,
      "loss": 1.7411,
      "step": 208608
    },
    {
      "epoch": 0.0007571959674553225,
      "grad_norm": 10021.34681567303,
      "learning_rate": 2.190371611420254e-07,
      "loss": 1.7377,
      "step": 208640
    },
    {
      "epoch": 0.0007573121018061592,
      "grad_norm": 9374.424142314023,
      "learning_rate": 2.1902034898702177e-07,
      "loss": 1.7457,
      "step": 208672
    },
    {
      "epoch": 0.0007574282361569959,
      "grad_norm": 10648.396311182261,
      "learning_rate": 2.190040659029771e-07,
      "loss": 1.7736,
      "step": 208704
    },
    {
      "epoch": 0.0007575443705078326,
      "grad_norm": 9875.514872653475,
      "learning_rate": 2.1898726136689359e-07,
      "loss": 1.7443,
      "step": 208736
    },
    {
      "epoch": 0.0007576605048586693,
      "grad_norm": 10353.751011107039,
      "learning_rate": 2.1897046069853413e-07,
      "loss": 1.7553,
      "step": 208768
    },
    {
      "epoch": 0.000757776639209506,
      "grad_norm": 10097.165542863997,
      "learning_rate": 2.189536638964154e-07,
      "loss": 1.7475,
      "step": 208800
    },
    {
      "epoch": 0.0007578927735603427,
      "grad_norm": 10636.149021144824,
      "learning_rate": 2.1893687095905475e-07,
      "loss": 1.7479,
      "step": 208832
    },
    {
      "epoch": 0.0007580089079111794,
      "grad_norm": 8658.740670559431,
      "learning_rate": 2.1892008188497032e-07,
      "loss": 1.7385,
      "step": 208864
    },
    {
      "epoch": 0.0007581250422620162,
      "grad_norm": 10990.828176256782,
      "learning_rate": 2.189032966726811e-07,
      "loss": 1.7231,
      "step": 208896
    },
    {
      "epoch": 0.0007582411766128528,
      "grad_norm": 11172.383273053247,
      "learning_rate": 2.1888651532070677e-07,
      "loss": 1.7191,
      "step": 208928
    },
    {
      "epoch": 0.0007583573109636896,
      "grad_norm": 8712.052915358125,
      "learning_rate": 2.1886973782756797e-07,
      "loss": 1.7316,
      "step": 208960
    },
    {
      "epoch": 0.0007584734453145262,
      "grad_norm": 11022.167300490408,
      "learning_rate": 2.1885296419178598e-07,
      "loss": 1.731,
      "step": 208992
    },
    {
      "epoch": 0.000758589579665363,
      "grad_norm": 9439.716309296587,
      "learning_rate": 2.1883619441188296e-07,
      "loss": 1.7257,
      "step": 209024
    },
    {
      "epoch": 0.0007587057140161996,
      "grad_norm": 11325.02362028442,
      "learning_rate": 2.1881942848638184e-07,
      "loss": 1.7441,
      "step": 209056
    },
    {
      "epoch": 0.0007588218483670364,
      "grad_norm": 11790.764691062239,
      "learning_rate": 2.1880266641380634e-07,
      "loss": 1.7248,
      "step": 209088
    },
    {
      "epoch": 0.000758937982717873,
      "grad_norm": 8960.060937292781,
      "learning_rate": 2.1878590819268096e-07,
      "loss": 1.7186,
      "step": 209120
    },
    {
      "epoch": 0.0007590541170687098,
      "grad_norm": 9914.213231517668,
      "learning_rate": 2.18769153821531e-07,
      "loss": 1.7055,
      "step": 209152
    },
    {
      "epoch": 0.0007591702514195465,
      "grad_norm": 10148.204767346784,
      "learning_rate": 2.1875240329888257e-07,
      "loss": 1.716,
      "step": 209184
    },
    {
      "epoch": 0.0007592863857703832,
      "grad_norm": 10286.407341730153,
      "learning_rate": 2.1873565662326254e-07,
      "loss": 1.7227,
      "step": 209216
    },
    {
      "epoch": 0.0007594025201212199,
      "grad_norm": 10189.2368703451,
      "learning_rate": 2.1871891379319857e-07,
      "loss": 1.7232,
      "step": 209248
    },
    {
      "epoch": 0.0007595186544720566,
      "grad_norm": 10817.177820485342,
      "learning_rate": 2.1870217480721916e-07,
      "loss": 1.7352,
      "step": 209280
    },
    {
      "epoch": 0.0007596347888228933,
      "grad_norm": 8749.252539503017,
      "learning_rate": 2.1868543966385353e-07,
      "loss": 1.7328,
      "step": 209312
    },
    {
      "epoch": 0.00075975092317373,
      "grad_norm": 8294.79535612543,
      "learning_rate": 2.186687083616317e-07,
      "loss": 1.7426,
      "step": 209344
    },
    {
      "epoch": 0.0007598670575245667,
      "grad_norm": 10063.892884962557,
      "learning_rate": 2.1865198089908452e-07,
      "loss": 1.7333,
      "step": 209376
    },
    {
      "epoch": 0.0007599831918754034,
      "grad_norm": 10823.0609348742,
      "learning_rate": 2.186352572747436e-07,
      "loss": 1.7502,
      "step": 209408
    },
    {
      "epoch": 0.0007600993262262401,
      "grad_norm": 9102.619073651274,
      "learning_rate": 2.1861853748714135e-07,
      "loss": 1.7522,
      "step": 209440
    },
    {
      "epoch": 0.0007602154605770769,
      "grad_norm": 9306.736270035806,
      "learning_rate": 2.1860182153481089e-07,
      "loss": 1.7467,
      "step": 209472
    },
    {
      "epoch": 0.0007603315949279135,
      "grad_norm": 9160.037882017738,
      "learning_rate": 2.1858510941628625e-07,
      "loss": 1.734,
      "step": 209504
    },
    {
      "epoch": 0.0007604477292787503,
      "grad_norm": 9957.290796195519,
      "learning_rate": 2.1856840113010217e-07,
      "loss": 1.747,
      "step": 209536
    },
    {
      "epoch": 0.0007605638636295869,
      "grad_norm": 10294.586732841683,
      "learning_rate": 2.1855169667479414e-07,
      "loss": 1.7667,
      "step": 209568
    },
    {
      "epoch": 0.0007606799979804237,
      "grad_norm": 9512.404427903599,
      "learning_rate": 2.1853499604889853e-07,
      "loss": 1.7376,
      "step": 209600
    },
    {
      "epoch": 0.0007607961323312603,
      "grad_norm": 10894.326596903546,
      "learning_rate": 2.185182992509524e-07,
      "loss": 1.7601,
      "step": 209632
    },
    {
      "epoch": 0.0007609122666820971,
      "grad_norm": 8448.3945220379,
      "learning_rate": 2.1850160627949367e-07,
      "loss": 1.7287,
      "step": 209664
    },
    {
      "epoch": 0.0007610284010329337,
      "grad_norm": 9470.043505707881,
      "learning_rate": 2.1848543861100318e-07,
      "loss": 1.7313,
      "step": 209696
    },
    {
      "epoch": 0.0007611445353837705,
      "grad_norm": 7894.24157218412,
      "learning_rate": 2.1846875316867168e-07,
      "loss": 1.7283,
      "step": 209728
    },
    {
      "epoch": 0.0007612606697346072,
      "grad_norm": 10382.646483435714,
      "learning_rate": 2.184520715484915e-07,
      "loss": 1.7178,
      "step": 209760
    },
    {
      "epoch": 0.0007613768040854439,
      "grad_norm": 11282.497418568286,
      "learning_rate": 2.1843539374900363e-07,
      "loss": 1.7291,
      "step": 209792
    },
    {
      "epoch": 0.0007614929384362806,
      "grad_norm": 8995.282430251982,
      "learning_rate": 2.1841871976874985e-07,
      "loss": 1.7356,
      "step": 209824
    },
    {
      "epoch": 0.0007616090727871173,
      "grad_norm": 10165.269991495554,
      "learning_rate": 2.1840204960627266e-07,
      "loss": 1.734,
      "step": 209856
    },
    {
      "epoch": 0.000761725207137954,
      "grad_norm": 9606.019050574489,
      "learning_rate": 2.1838538326011547e-07,
      "loss": 1.7238,
      "step": 209888
    },
    {
      "epoch": 0.0007618413414887907,
      "grad_norm": 9069.385646227644,
      "learning_rate": 2.1836872072882223e-07,
      "loss": 1.7387,
      "step": 209920
    },
    {
      "epoch": 0.0007619574758396274,
      "grad_norm": 9768.802690196993,
      "learning_rate": 2.1835206201093796e-07,
      "loss": 1.7411,
      "step": 209952
    },
    {
      "epoch": 0.0007620736101904641,
      "grad_norm": 10600.519987245909,
      "learning_rate": 2.1833540710500823e-07,
      "loss": 1.7393,
      "step": 209984
    },
    {
      "epoch": 0.0007621897445413008,
      "grad_norm": 11166.956434051312,
      "learning_rate": 2.1831875600957947e-07,
      "loss": 1.7316,
      "step": 210016
    },
    {
      "epoch": 0.0007623058788921375,
      "grad_norm": 9186.869651845507,
      "learning_rate": 2.1830210872319891e-07,
      "loss": 1.7428,
      "step": 210048
    },
    {
      "epoch": 0.0007624220132429742,
      "grad_norm": 10111.747128958477,
      "learning_rate": 2.1828546524441452e-07,
      "loss": 1.7202,
      "step": 210080
    },
    {
      "epoch": 0.000762538147593811,
      "grad_norm": 10946.160057298632,
      "learning_rate": 2.182688255717751e-07,
      "loss": 1.7294,
      "step": 210112
    },
    {
      "epoch": 0.0007626542819446476,
      "grad_norm": 9370.695385082156,
      "learning_rate": 2.182521897038301e-07,
      "loss": 1.7325,
      "step": 210144
    },
    {
      "epoch": 0.0007627704162954844,
      "grad_norm": 10000.491987897396,
      "learning_rate": 2.1823555763912988e-07,
      "loss": 1.724,
      "step": 210176
    },
    {
      "epoch": 0.000762886550646321,
      "grad_norm": 10166.988934783001,
      "learning_rate": 2.1821892937622547e-07,
      "loss": 1.7501,
      "step": 210208
    },
    {
      "epoch": 0.0007630026849971578,
      "grad_norm": 10520.09885885109,
      "learning_rate": 2.1820230491366877e-07,
      "loss": 1.7364,
      "step": 210240
    },
    {
      "epoch": 0.0007631188193479944,
      "grad_norm": 9943.849154125379,
      "learning_rate": 2.1818568425001235e-07,
      "loss": 1.7305,
      "step": 210272
    },
    {
      "epoch": 0.0007632349536988312,
      "grad_norm": 9293.834945812197,
      "learning_rate": 2.1816906738380964e-07,
      "loss": 1.7304,
      "step": 210304
    },
    {
      "epoch": 0.0007633510880496678,
      "grad_norm": 9550.53579648807,
      "learning_rate": 2.1815245431361478e-07,
      "loss": 1.731,
      "step": 210336
    },
    {
      "epoch": 0.0007634672224005046,
      "grad_norm": 10209.427897781541,
      "learning_rate": 2.1813584503798274e-07,
      "loss": 1.7393,
      "step": 210368
    },
    {
      "epoch": 0.0007635833567513413,
      "grad_norm": 9810.141894998258,
      "learning_rate": 2.181192395554692e-07,
      "loss": 1.7538,
      "step": 210400
    },
    {
      "epoch": 0.000763699491102178,
      "grad_norm": 9481.070403704425,
      "learning_rate": 2.1810263786463062e-07,
      "loss": 1.7569,
      "step": 210432
    },
    {
      "epoch": 0.0007638156254530147,
      "grad_norm": 10078.078586714832,
      "learning_rate": 2.1808603996402423e-07,
      "loss": 1.7427,
      "step": 210464
    },
    {
      "epoch": 0.0007639317598038514,
      "grad_norm": 9569.532590466475,
      "learning_rate": 2.180694458522081e-07,
      "loss": 1.7483,
      "step": 210496
    },
    {
      "epoch": 0.0007640478941546881,
      "grad_norm": 9148.230320668583,
      "learning_rate": 2.1805285552774097e-07,
      "loss": 1.747,
      "step": 210528
    },
    {
      "epoch": 0.0007641640285055248,
      "grad_norm": 9813.394112130623,
      "learning_rate": 2.1803626898918236e-07,
      "loss": 1.7495,
      "step": 210560
    },
    {
      "epoch": 0.0007642801628563615,
      "grad_norm": 8498.577528033735,
      "learning_rate": 2.1801968623509263e-07,
      "loss": 1.7518,
      "step": 210592
    },
    {
      "epoch": 0.0007643962972071982,
      "grad_norm": 10119.4926750307,
      "learning_rate": 2.1800310726403284e-07,
      "loss": 1.7382,
      "step": 210624
    },
    {
      "epoch": 0.0007645124315580349,
      "grad_norm": 9511.781746865305,
      "learning_rate": 2.1798653207456487e-07,
      "loss": 1.7274,
      "step": 210656
    },
    {
      "epoch": 0.0007646285659088717,
      "grad_norm": 9938.002817467903,
      "learning_rate": 2.1797047846458746e-07,
      "loss": 1.7271,
      "step": 210688
    },
    {
      "epoch": 0.0007647447002597083,
      "grad_norm": 11141.376126852552,
      "learning_rate": 2.1795391071592843e-07,
      "loss": 1.7372,
      "step": 210720
    },
    {
      "epoch": 0.0007648608346105451,
      "grad_norm": 12016.968003618882,
      "learning_rate": 2.179373467445962e-07,
      "loss": 1.7143,
      "step": 210752
    },
    {
      "epoch": 0.0007649769689613817,
      "grad_norm": 9434.309937668997,
      "learning_rate": 2.179207865491556e-07,
      "loss": 1.7341,
      "step": 210784
    },
    {
      "epoch": 0.0007650931033122185,
      "grad_norm": 8902.6024285037,
      "learning_rate": 2.1790423012817231e-07,
      "loss": 1.7436,
      "step": 210816
    },
    {
      "epoch": 0.0007652092376630551,
      "grad_norm": 9776.519216981062,
      "learning_rate": 2.1788767748021272e-07,
      "loss": 1.7287,
      "step": 210848
    },
    {
      "epoch": 0.0007653253720138919,
      "grad_norm": 10046.232527669266,
      "learning_rate": 2.17871128603844e-07,
      "loss": 1.7112,
      "step": 210880
    },
    {
      "epoch": 0.0007654415063647285,
      "grad_norm": 9092.970141818349,
      "learning_rate": 2.178545834976341e-07,
      "loss": 1.7159,
      "step": 210912
    },
    {
      "epoch": 0.0007655576407155653,
      "grad_norm": 9754.007176540315,
      "learning_rate": 2.1783804216015166e-07,
      "loss": 1.7251,
      "step": 210944
    },
    {
      "epoch": 0.000765673775066402,
      "grad_norm": 8558.5905381669,
      "learning_rate": 2.1782150458996615e-07,
      "loss": 1.7325,
      "step": 210976
    },
    {
      "epoch": 0.0007657899094172387,
      "grad_norm": 10629.582494152815,
      "learning_rate": 2.1780497078564782e-07,
      "loss": 1.7306,
      "step": 211008
    },
    {
      "epoch": 0.0007659060437680754,
      "grad_norm": 8721.64250585863,
      "learning_rate": 2.177884407457676e-07,
      "loss": 1.732,
      "step": 211040
    },
    {
      "epoch": 0.0007660221781189121,
      "grad_norm": 10394.013276882035,
      "learning_rate": 2.1777191446889722e-07,
      "loss": 1.735,
      "step": 211072
    },
    {
      "epoch": 0.0007661383124697488,
      "grad_norm": 9592.815228075646,
      "learning_rate": 2.1775539195360917e-07,
      "loss": 1.7447,
      "step": 211104
    },
    {
      "epoch": 0.0007662544468205855,
      "grad_norm": 10643.960822926774,
      "learning_rate": 2.1773887319847674e-07,
      "loss": 1.7395,
      "step": 211136
    },
    {
      "epoch": 0.0007663705811714222,
      "grad_norm": 10759.21930253306,
      "learning_rate": 2.177223582020739e-07,
      "loss": 1.7493,
      "step": 211168
    },
    {
      "epoch": 0.0007664867155222589,
      "grad_norm": 8864.671905942148,
      "learning_rate": 2.1770584696297537e-07,
      "loss": 1.7541,
      "step": 211200
    },
    {
      "epoch": 0.0007666028498730956,
      "grad_norm": 11940.67820519421,
      "learning_rate": 2.1768933947975673e-07,
      "loss": 1.7512,
      "step": 211232
    },
    {
      "epoch": 0.0007667189842239324,
      "grad_norm": 10075.20242972815,
      "learning_rate": 2.1767283575099424e-07,
      "loss": 1.7473,
      "step": 211264
    },
    {
      "epoch": 0.000766835118574769,
      "grad_norm": 10612.86144260821,
      "learning_rate": 2.176563357752649e-07,
      "loss": 1.7512,
      "step": 211296
    },
    {
      "epoch": 0.0007669512529256058,
      "grad_norm": 9553.632816892221,
      "learning_rate": 2.1763983955114656e-07,
      "loss": 1.7486,
      "step": 211328
    },
    {
      "epoch": 0.0007670673872764424,
      "grad_norm": 8789.394177075006,
      "learning_rate": 2.1762334707721767e-07,
      "loss": 1.7437,
      "step": 211360
    },
    {
      "epoch": 0.0007671835216272792,
      "grad_norm": 9002.117084330774,
      "learning_rate": 2.1760685835205756e-07,
      "loss": 1.7501,
      "step": 211392
    },
    {
      "epoch": 0.0007672996559781158,
      "grad_norm": 10710.82088357377,
      "learning_rate": 2.175903733742463e-07,
      "loss": 1.7405,
      "step": 211424
    },
    {
      "epoch": 0.0007674157903289526,
      "grad_norm": 9677.783423904464,
      "learning_rate": 2.1757389214236466e-07,
      "loss": 1.7383,
      "step": 211456
    },
    {
      "epoch": 0.0007675319246797892,
      "grad_norm": 11163.169084090772,
      "learning_rate": 2.175574146549942e-07,
      "loss": 1.7129,
      "step": 211488
    },
    {
      "epoch": 0.000767648059030626,
      "grad_norm": 9142.426483160802,
      "learning_rate": 2.1754094091071722e-07,
      "loss": 1.721,
      "step": 211520
    },
    {
      "epoch": 0.0007677641933814627,
      "grad_norm": 11158.279616500029,
      "learning_rate": 2.1752447090811678e-07,
      "loss": 1.7306,
      "step": 211552
    },
    {
      "epoch": 0.0007678803277322994,
      "grad_norm": 11634.828404407175,
      "learning_rate": 2.1750800464577668e-07,
      "loss": 1.7362,
      "step": 211584
    },
    {
      "epoch": 0.0007679964620831361,
      "grad_norm": 8525.087213630133,
      "learning_rate": 2.1749154212228147e-07,
      "loss": 1.7176,
      "step": 211616
    },
    {
      "epoch": 0.0007681125964339728,
      "grad_norm": 10112.076740215138,
      "learning_rate": 2.1747508333621646e-07,
      "loss": 1.7296,
      "step": 211648
    },
    {
      "epoch": 0.0007682287307848095,
      "grad_norm": 10107.357122413356,
      "learning_rate": 2.174586282861677e-07,
      "loss": 1.736,
      "step": 211680
    },
    {
      "epoch": 0.0007683448651356462,
      "grad_norm": 10849.271311936116,
      "learning_rate": 2.174426910178141e-07,
      "loss": 1.7432,
      "step": 211712
    },
    {
      "epoch": 0.0007684609994864829,
      "grad_norm": 10484.946161044414,
      "learning_rate": 2.1742624331891816e-07,
      "loss": 1.7263,
      "step": 211744
    },
    {
      "epoch": 0.0007685771338373196,
      "grad_norm": 9790.559126015225,
      "learning_rate": 2.1740979935184526e-07,
      "loss": 1.7376,
      "step": 211776
    },
    {
      "epoch": 0.0007686932681881563,
      "grad_norm": 9268.54789058135,
      "learning_rate": 2.1739335911518436e-07,
      "loss": 1.7459,
      "step": 211808
    },
    {
      "epoch": 0.0007688094025389931,
      "grad_norm": 10499.985523799545,
      "learning_rate": 2.1737692260752529e-07,
      "loss": 1.7287,
      "step": 211840
    },
    {
      "epoch": 0.0007689255368898297,
      "grad_norm": 9220.524822373181,
      "learning_rate": 2.1736048982745853e-07,
      "loss": 1.7281,
      "step": 211872
    },
    {
      "epoch": 0.0007690416712406665,
      "grad_norm": 9152.89932207276,
      "learning_rate": 2.1734406077357533e-07,
      "loss": 1.731,
      "step": 211904
    },
    {
      "epoch": 0.0007691578055915031,
      "grad_norm": 10118.123739112902,
      "learning_rate": 2.1732763544446775e-07,
      "loss": 1.7312,
      "step": 211936
    },
    {
      "epoch": 0.0007692739399423399,
      "grad_norm": 9434.865340851453,
      "learning_rate": 2.173112138387285e-07,
      "loss": 1.7423,
      "step": 211968
    },
    {
      "epoch": 0.0007693900742931765,
      "grad_norm": 9174.491702541345,
      "learning_rate": 2.1729479595495107e-07,
      "loss": 1.7363,
      "step": 212000
    },
    {
      "epoch": 0.0007695062086440133,
      "grad_norm": 10724.77086002307,
      "learning_rate": 2.1727838179172974e-07,
      "loss": 1.7249,
      "step": 212032
    },
    {
      "epoch": 0.0007696223429948499,
      "grad_norm": 9785.031732191777,
      "learning_rate": 2.1726197134765944e-07,
      "loss": 1.7292,
      "step": 212064
    },
    {
      "epoch": 0.0007697384773456867,
      "grad_norm": 8866.412916168523,
      "learning_rate": 2.1724556462133595e-07,
      "loss": 1.7334,
      "step": 212096
    },
    {
      "epoch": 0.0007698546116965234,
      "grad_norm": 11107.087106888106,
      "learning_rate": 2.172291616113557e-07,
      "loss": 1.7506,
      "step": 212128
    },
    {
      "epoch": 0.0007699707460473601,
      "grad_norm": 9124.4678748955,
      "learning_rate": 2.1721276231631592e-07,
      "loss": 1.7482,
      "step": 212160
    },
    {
      "epoch": 0.0007700868803981968,
      "grad_norm": 10265.391955497851,
      "learning_rate": 2.1719636673481451e-07,
      "loss": 1.7397,
      "step": 212192
    },
    {
      "epoch": 0.0007702030147490335,
      "grad_norm": 9295.97450512855,
      "learning_rate": 2.1717997486545022e-07,
      "loss": 1.7452,
      "step": 212224
    },
    {
      "epoch": 0.0007703191490998702,
      "grad_norm": 9390.640020786655,
      "learning_rate": 2.171635867068225e-07,
      "loss": 1.7466,
      "step": 212256
    },
    {
      "epoch": 0.0007704352834507069,
      "grad_norm": 8659.5570325508,
      "learning_rate": 2.1714720225753144e-07,
      "loss": 1.7524,
      "step": 212288
    },
    {
      "epoch": 0.0007705514178015436,
      "grad_norm": 9042.797244215973,
      "learning_rate": 2.1713082151617799e-07,
      "loss": 1.7497,
      "step": 212320
    },
    {
      "epoch": 0.0007706675521523803,
      "grad_norm": 12107.56490794082,
      "learning_rate": 2.1711444448136384e-07,
      "loss": 1.7377,
      "step": 212352
    },
    {
      "epoch": 0.000770783686503217,
      "grad_norm": 9609.532142617558,
      "learning_rate": 2.1709807115169131e-07,
      "loss": 1.7357,
      "step": 212384
    },
    {
      "epoch": 0.0007708998208540538,
      "grad_norm": 10165.808280702522,
      "learning_rate": 2.1708170152576358e-07,
      "loss": 1.7433,
      "step": 212416
    },
    {
      "epoch": 0.0007710159552048904,
      "grad_norm": 9867.407156897905,
      "learning_rate": 2.1706533560218446e-07,
      "loss": 1.7264,
      "step": 212448
    },
    {
      "epoch": 0.0007711320895557272,
      "grad_norm": 10566.258940609017,
      "learning_rate": 2.1704897337955857e-07,
      "loss": 1.7193,
      "step": 212480
    },
    {
      "epoch": 0.0007712482239065638,
      "grad_norm": 8935.863248729805,
      "learning_rate": 2.1703261485649125e-07,
      "loss": 1.7154,
      "step": 212512
    },
    {
      "epoch": 0.0007713643582574006,
      "grad_norm": 10217.115248444641,
      "learning_rate": 2.170162600315886e-07,
      "loss": 1.7336,
      "step": 212544
    },
    {
      "epoch": 0.0007714804926082372,
      "grad_norm": 10501.142414042388,
      "learning_rate": 2.1699990890345737e-07,
      "loss": 1.744,
      "step": 212576
    },
    {
      "epoch": 0.000771596626959074,
      "grad_norm": 10937.138748319872,
      "learning_rate": 2.1698356147070513e-07,
      "loss": 1.7186,
      "step": 212608
    },
    {
      "epoch": 0.0007717127613099106,
      "grad_norm": 10600.34169260595,
      "learning_rate": 2.1696721773194015e-07,
      "loss": 1.714,
      "step": 212640
    },
    {
      "epoch": 0.0007718288956607474,
      "grad_norm": 9436.566112734017,
      "learning_rate": 2.1695087768577145e-07,
      "loss": 1.7253,
      "step": 212672
    },
    {
      "epoch": 0.0007719450300115842,
      "grad_norm": 9128.079425596603,
      "learning_rate": 2.169350517860424e-07,
      "loss": 1.7238,
      "step": 212704
    },
    {
      "epoch": 0.0007720611643624208,
      "grad_norm": 8369.259226478769,
      "learning_rate": 2.1691871900561048e-07,
      "loss": 1.7327,
      "step": 212736
    },
    {
      "epoch": 0.0007721772987132576,
      "grad_norm": 10469.510781311608,
      "learning_rate": 2.1690238991364957e-07,
      "loss": 1.7306,
      "step": 212768
    },
    {
      "epoch": 0.0007722934330640942,
      "grad_norm": 8982.552643875793,
      "learning_rate": 2.1688606450877174e-07,
      "loss": 1.7271,
      "step": 212800
    },
    {
      "epoch": 0.000772409567414931,
      "grad_norm": 10360.52431105685,
      "learning_rate": 2.1686974278958948e-07,
      "loss": 1.7367,
      "step": 212832
    },
    {
      "epoch": 0.0007725257017657676,
      "grad_norm": 8711.238717886223,
      "learning_rate": 2.1685342475471627e-07,
      "loss": 1.7372,
      "step": 212864
    },
    {
      "epoch": 0.0007726418361166044,
      "grad_norm": 9467.034805048517,
      "learning_rate": 2.1683711040276618e-07,
      "loss": 1.7355,
      "step": 212896
    },
    {
      "epoch": 0.000772757970467441,
      "grad_norm": 9896.667722016335,
      "learning_rate": 2.1682079973235405e-07,
      "loss": 1.7496,
      "step": 212928
    },
    {
      "epoch": 0.0007728741048182778,
      "grad_norm": 9887.052037892792,
      "learning_rate": 2.1680449274209546e-07,
      "loss": 1.754,
      "step": 212960
    },
    {
      "epoch": 0.0007729902391691145,
      "grad_norm": 10191.195611899519,
      "learning_rate": 2.167881894306067e-07,
      "loss": 1.7691,
      "step": 212992
    },
    {
      "epoch": 0.0007731063735199512,
      "grad_norm": 11274.292350298532,
      "learning_rate": 2.1677188979650478e-07,
      "loss": 1.7404,
      "step": 213024
    },
    {
      "epoch": 0.0007732225078707879,
      "grad_norm": 9290.784143440209,
      "learning_rate": 2.1675559383840744e-07,
      "loss": 1.7367,
      "step": 213056
    },
    {
      "epoch": 0.0007733386422216246,
      "grad_norm": 10854.052515074725,
      "learning_rate": 2.1673930155493318e-07,
      "loss": 1.7418,
      "step": 213088
    },
    {
      "epoch": 0.0007734547765724613,
      "grad_norm": 10014.925062126025,
      "learning_rate": 2.167230129447012e-07,
      "loss": 1.7424,
      "step": 213120
    },
    {
      "epoch": 0.000773570910923298,
      "grad_norm": 10712.155151975721,
      "learning_rate": 2.1670672800633147e-07,
      "loss": 1.7569,
      "step": 213152
    },
    {
      "epoch": 0.0007736870452741347,
      "grad_norm": 10805.179498740406,
      "learning_rate": 2.166904467384446e-07,
      "loss": 1.7269,
      "step": 213184
    },
    {
      "epoch": 0.0007738031796249714,
      "grad_norm": 9819.970468387366,
      "learning_rate": 2.1667416913966194e-07,
      "loss": 1.6988,
      "step": 213216
    },
    {
      "epoch": 0.0007739193139758081,
      "grad_norm": 11147.136852124853,
      "learning_rate": 2.1665789520860568e-07,
      "loss": 1.7126,
      "step": 213248
    },
    {
      "epoch": 0.0007740354483266449,
      "grad_norm": 9528.978329285885,
      "learning_rate": 2.166416249438986e-07,
      "loss": 1.7165,
      "step": 213280
    },
    {
      "epoch": 0.0007741515826774815,
      "grad_norm": 8381.727029675925,
      "learning_rate": 2.1662535834416425e-07,
      "loss": 1.7046,
      "step": 213312
    },
    {
      "epoch": 0.0007742677170283183,
      "grad_norm": 9788.29464207121,
      "learning_rate": 2.1660909540802697e-07,
      "loss": 1.7197,
      "step": 213344
    },
    {
      "epoch": 0.0007743838513791549,
      "grad_norm": 12138.103476243725,
      "learning_rate": 2.1659283613411168e-07,
      "loss": 1.7091,
      "step": 213376
    },
    {
      "epoch": 0.0007744999857299917,
      "grad_norm": 9238.432442790281,
      "learning_rate": 2.1657658052104417e-07,
      "loss": 1.6978,
      "step": 213408
    },
    {
      "epoch": 0.0007746161200808283,
      "grad_norm": 9746.63654806108,
      "learning_rate": 2.1656032856745085e-07,
      "loss": 1.7153,
      "step": 213440
    },
    {
      "epoch": 0.0007747322544316651,
      "grad_norm": 9088.319316573334,
      "learning_rate": 2.165440802719589e-07,
      "loss": 1.7251,
      "step": 213472
    },
    {
      "epoch": 0.0007748483887825017,
      "grad_norm": 9656.30301927192,
      "learning_rate": 2.165278356331962e-07,
      "loss": 1.7005,
      "step": 213504
    },
    {
      "epoch": 0.0007749645231333385,
      "grad_norm": 9751.089580144364,
      "learning_rate": 2.1651159464979136e-07,
      "loss": 1.7383,
      "step": 213536
    },
    {
      "epoch": 0.0007750806574841752,
      "grad_norm": 9371.825862658781,
      "learning_rate": 2.164953573203737e-07,
      "loss": 1.7412,
      "step": 213568
    },
    {
      "epoch": 0.0007751967918350119,
      "grad_norm": 9229.073409611607,
      "learning_rate": 2.1647912364357326e-07,
      "loss": 1.713,
      "step": 213600
    },
    {
      "epoch": 0.0007753129261858486,
      "grad_norm": 8973.416517692689,
      "learning_rate": 2.164628936180209e-07,
      "loss": 1.7046,
      "step": 213632
    },
    {
      "epoch": 0.0007754290605366853,
      "grad_norm": 8077.901460156592,
      "learning_rate": 2.1644666724234797e-07,
      "loss": 1.7232,
      "step": 213664
    },
    {
      "epoch": 0.000775545194887522,
      "grad_norm": 9743.169504837735,
      "learning_rate": 2.1643044451518676e-07,
      "loss": 1.7071,
      "step": 213696
    },
    {
      "epoch": 0.0007756613292383587,
      "grad_norm": 7579.642207914567,
      "learning_rate": 2.1641473222622842e-07,
      "loss": 1.724,
      "step": 213728
    },
    {
      "epoch": 0.0007757774635891954,
      "grad_norm": 7940.228963953117,
      "learning_rate": 2.1639851667808018e-07,
      "loss": 1.7303,
      "step": 213760
    },
    {
      "epoch": 0.0007758935979400321,
      "grad_norm": 10139.64476695313,
      "learning_rate": 2.163823047743872e-07,
      "loss": 1.7184,
      "step": 213792
    },
    {
      "epoch": 0.0007760097322908688,
      "grad_norm": 10282.646935492825,
      "learning_rate": 2.1636609651378456e-07,
      "loss": 1.7083,
      "step": 213824
    },
    {
      "epoch": 0.0007761258666417056,
      "grad_norm": 9595.378470909836,
      "learning_rate": 2.16349891894908e-07,
      "loss": 1.7284,
      "step": 213856
    },
    {
      "epoch": 0.0007762420009925422,
      "grad_norm": 10366.27782764865,
      "learning_rate": 2.16333690916394e-07,
      "loss": 1.7186,
      "step": 213888
    },
    {
      "epoch": 0.000776358135343379,
      "grad_norm": 9664.15914604059,
      "learning_rate": 2.1631749357687976e-07,
      "loss": 1.7117,
      "step": 213920
    },
    {
      "epoch": 0.0007764742696942156,
      "grad_norm": 9847.349389556563,
      "learning_rate": 2.1630129987500322e-07,
      "loss": 1.7363,
      "step": 213952
    },
    {
      "epoch": 0.0007765904040450524,
      "grad_norm": 9853.677080156422,
      "learning_rate": 2.162851098094029e-07,
      "loss": 1.7371,
      "step": 213984
    },
    {
      "epoch": 0.000776706538395889,
      "grad_norm": 8994.183009034228,
      "learning_rate": 2.1626892337871825e-07,
      "loss": 1.7266,
      "step": 214016
    },
    {
      "epoch": 0.0007768226727467258,
      "grad_norm": 10763.732809764464,
      "learning_rate": 2.1625274058158927e-07,
      "loss": 1.7219,
      "step": 214048
    },
    {
      "epoch": 0.0007769388070975624,
      "grad_norm": 9152.625415693576,
      "learning_rate": 2.1623656141665673e-07,
      "loss": 1.7333,
      "step": 214080
    },
    {
      "epoch": 0.0007770549414483992,
      "grad_norm": 9801.705769915765,
      "learning_rate": 2.1622038588256204e-07,
      "loss": 1.7176,
      "step": 214112
    },
    {
      "epoch": 0.0007771710757992359,
      "grad_norm": 11057.574598437037,
      "learning_rate": 2.1620421397794747e-07,
      "loss": 1.7414,
      "step": 214144
    },
    {
      "epoch": 0.0007772872101500726,
      "grad_norm": 9885.209760040501,
      "learning_rate": 2.1618804570145586e-07,
      "loss": 1.725,
      "step": 214176
    },
    {
      "epoch": 0.0007774033445009093,
      "grad_norm": 9629.795428772099,
      "learning_rate": 2.1617188105173085e-07,
      "loss": 1.7154,
      "step": 214208
    },
    {
      "epoch": 0.000777519478851746,
      "grad_norm": 8222.486606860482,
      "learning_rate": 2.1615572002741672e-07,
      "loss": 1.6858,
      "step": 214240
    },
    {
      "epoch": 0.0007776356132025827,
      "grad_norm": 8820.724800151062,
      "learning_rate": 2.1613956262715852e-07,
      "loss": 1.7014,
      "step": 214272
    },
    {
      "epoch": 0.0007777517475534194,
      "grad_norm": 10418.337679303739,
      "learning_rate": 2.1612340884960195e-07,
      "loss": 1.7143,
      "step": 214304
    },
    {
      "epoch": 0.0007778678819042561,
      "grad_norm": 10787.005145080817,
      "learning_rate": 2.1610725869339348e-07,
      "loss": 1.718,
      "step": 214336
    },
    {
      "epoch": 0.0007779840162550928,
      "grad_norm": 9805.57004972174,
      "learning_rate": 2.1609111215718023e-07,
      "loss": 1.7139,
      "step": 214368
    },
    {
      "epoch": 0.0007781001506059295,
      "grad_norm": 8621.722217747449,
      "learning_rate": 2.1607496923961008e-07,
      "loss": 1.7184,
      "step": 214400
    },
    {
      "epoch": 0.0007782162849567663,
      "grad_norm": 9492.447313522473,
      "learning_rate": 2.160588299393316e-07,
      "loss": 1.7036,
      "step": 214432
    },
    {
      "epoch": 0.0007783324193076029,
      "grad_norm": 8411.359580947661,
      "learning_rate": 2.1604269425499403e-07,
      "loss": 1.7017,
      "step": 214464
    },
    {
      "epoch": 0.0007784485536584397,
      "grad_norm": 9629.72055669322,
      "learning_rate": 2.1602656218524736e-07,
      "loss": 1.7253,
      "step": 214496
    },
    {
      "epoch": 0.0007785646880092763,
      "grad_norm": 9099.925604091497,
      "learning_rate": 2.1601043372874225e-07,
      "loss": 1.7073,
      "step": 214528
    },
    {
      "epoch": 0.0007786808223601131,
      "grad_norm": 9638.095247506117,
      "learning_rate": 2.1599430888413013e-07,
      "loss": 1.7195,
      "step": 214560
    },
    {
      "epoch": 0.0007787969567109497,
      "grad_norm": 8858.621901853583,
      "learning_rate": 2.1597818765006308e-07,
      "loss": 1.7319,
      "step": 214592
    },
    {
      "epoch": 0.0007789130910617865,
      "grad_norm": 8884.628973682582,
      "learning_rate": 2.1596207002519383e-07,
      "loss": 1.7273,
      "step": 214624
    },
    {
      "epoch": 0.0007790292254126231,
      "grad_norm": 8101.310511269149,
      "learning_rate": 2.1594595600817595e-07,
      "loss": 1.7084,
      "step": 214656
    },
    {
      "epoch": 0.0007791453597634599,
      "grad_norm": 9024.095633358504,
      "learning_rate": 2.159298455976636e-07,
      "loss": 1.7339,
      "step": 214688
    },
    {
      "epoch": 0.0007792614941142966,
      "grad_norm": 9762.670741144557,
      "learning_rate": 2.1591424207542203e-07,
      "loss": 1.7561,
      "step": 214720
    },
    {
      "epoch": 0.0007793776284651333,
      "grad_norm": 11140.323155097432,
      "learning_rate": 2.158981387612873e-07,
      "loss": 1.7205,
      "step": 214752
    },
    {
      "epoch": 0.00077949376281597,
      "grad_norm": 10278.291686851468,
      "learning_rate": 2.158820390496669e-07,
      "loss": 1.7267,
      "step": 214784
    },
    {
      "epoch": 0.0007796098971668067,
      "grad_norm": 9821.405907506318,
      "learning_rate": 2.1586594293921786e-07,
      "loss": 1.7316,
      "step": 214816
    },
    {
      "epoch": 0.0007797260315176434,
      "grad_norm": 9943.943282219585,
      "learning_rate": 2.158498504285978e-07,
      "loss": 1.7229,
      "step": 214848
    },
    {
      "epoch": 0.0007798421658684801,
      "grad_norm": 9368.015584957147,
      "learning_rate": 2.1583376151646514e-07,
      "loss": 1.7191,
      "step": 214880
    },
    {
      "epoch": 0.0007799583002193168,
      "grad_norm": 10299.309491417374,
      "learning_rate": 2.15817676201479e-07,
      "loss": 1.7389,
      "step": 214912
    },
    {
      "epoch": 0.0007800744345701535,
      "grad_norm": 8312.141120072492,
      "learning_rate": 2.1580159448229918e-07,
      "loss": 1.7179,
      "step": 214944
    },
    {
      "epoch": 0.0007801905689209902,
      "grad_norm": 9677.673687410626,
      "learning_rate": 2.157855163575861e-07,
      "loss": 1.709,
      "step": 214976
    },
    {
      "epoch": 0.000780306703271827,
      "grad_norm": 8490.477018401261,
      "learning_rate": 2.1576944182600095e-07,
      "loss": 1.7229,
      "step": 215008
    },
    {
      "epoch": 0.0007804228376226636,
      "grad_norm": 9361.04203601287,
      "learning_rate": 2.1575337088620566e-07,
      "loss": 1.7071,
      "step": 215040
    },
    {
      "epoch": 0.0007805389719735004,
      "grad_norm": 8119.997413792693,
      "learning_rate": 2.157373035368628e-07,
      "loss": 1.6975,
      "step": 215072
    },
    {
      "epoch": 0.000780655106324337,
      "grad_norm": 9608.168399856448,
      "learning_rate": 2.1572123977663564e-07,
      "loss": 1.7042,
      "step": 215104
    },
    {
      "epoch": 0.0007807712406751738,
      "grad_norm": 10092.240583735605,
      "learning_rate": 2.1570517960418815e-07,
      "loss": 1.705,
      "step": 215136
    },
    {
      "epoch": 0.0007808873750260104,
      "grad_norm": 9243.589995234535,
      "learning_rate": 2.1568912301818497e-07,
      "loss": 1.7013,
      "step": 215168
    },
    {
      "epoch": 0.0007810035093768472,
      "grad_norm": 11045.502795255632,
      "learning_rate": 2.1567307001729156e-07,
      "loss": 1.7303,
      "step": 215200
    },
    {
      "epoch": 0.0007811196437276838,
      "grad_norm": 9454.233549050923,
      "learning_rate": 2.1565702060017385e-07,
      "loss": 1.7265,
      "step": 215232
    },
    {
      "epoch": 0.0007812357780785206,
      "grad_norm": 8366.799388057538,
      "learning_rate": 2.1564097476549872e-07,
      "loss": 1.7125,
      "step": 215264
    },
    {
      "epoch": 0.0007813519124293573,
      "grad_norm": 11539.692370249739,
      "learning_rate": 2.1562493251193355e-07,
      "loss": 1.7239,
      "step": 215296
    },
    {
      "epoch": 0.000781468046780194,
      "grad_norm": 8133.530106909299,
      "learning_rate": 2.156088938381465e-07,
      "loss": 1.7381,
      "step": 215328
    },
    {
      "epoch": 0.0007815841811310307,
      "grad_norm": 8955.051758644391,
      "learning_rate": 2.155928587428064e-07,
      "loss": 1.7084,
      "step": 215360
    },
    {
      "epoch": 0.0007817003154818674,
      "grad_norm": 9678.112212616674,
      "learning_rate": 2.1557682722458282e-07,
      "loss": 1.715,
      "step": 215392
    },
    {
      "epoch": 0.0007818164498327041,
      "grad_norm": 11550.710800639066,
      "learning_rate": 2.1556079928214593e-07,
      "loss": 1.7238,
      "step": 215424
    },
    {
      "epoch": 0.0007819325841835408,
      "grad_norm": 8660.276900884866,
      "learning_rate": 2.1554477491416668e-07,
      "loss": 1.7219,
      "step": 215456
    },
    {
      "epoch": 0.0007820487185343775,
      "grad_norm": 9045.646687771969,
      "learning_rate": 2.1552875411931664e-07,
      "loss": 1.7107,
      "step": 215488
    },
    {
      "epoch": 0.0007821648528852142,
      "grad_norm": 7612.18667663898,
      "learning_rate": 2.1551273689626814e-07,
      "loss": 1.7201,
      "step": 215520
    },
    {
      "epoch": 0.0007822809872360509,
      "grad_norm": 8230.274114511618,
      "learning_rate": 2.154967232436942e-07,
      "loss": 1.7189,
      "step": 215552
    },
    {
      "epoch": 0.0007823971215868877,
      "grad_norm": 11023.070715549275,
      "learning_rate": 2.1548071316026844e-07,
      "loss": 1.7165,
      "step": 215584
    },
    {
      "epoch": 0.0007825132559377243,
      "grad_norm": 9919.689108031562,
      "learning_rate": 2.1546470664466523e-07,
      "loss": 1.7134,
      "step": 215616
    },
    {
      "epoch": 0.0007826293902885611,
      "grad_norm": 8272.368463747247,
      "learning_rate": 2.1544870369555965e-07,
      "loss": 1.7238,
      "step": 215648
    },
    {
      "epoch": 0.0007827455246393977,
      "grad_norm": 8728.499641977423,
      "learning_rate": 2.1543270431162745e-07,
      "loss": 1.7133,
      "step": 215680
    },
    {
      "epoch": 0.0007828616589902345,
      "grad_norm": 10081.26460321323,
      "learning_rate": 2.1541720830699075e-07,
      "loss": 1.7189,
      "step": 215712
    },
    {
      "epoch": 0.0007829777933410711,
      "grad_norm": 10731.707599445672,
      "learning_rate": 2.1540121593812633e-07,
      "loss": 1.7402,
      "step": 215744
    },
    {
      "epoch": 0.0007830939276919079,
      "grad_norm": 8845.865700992752,
      "learning_rate": 2.1538522713050795e-07,
      "loss": 1.7097,
      "step": 215776
    },
    {
      "epoch": 0.0007832100620427445,
      "grad_norm": 9008.091029735435,
      "learning_rate": 2.1536924188281408e-07,
      "loss": 1.7399,
      "step": 215808
    },
    {
      "epoch": 0.0007833261963935813,
      "grad_norm": 9003.332271997962,
      "learning_rate": 2.153532601937239e-07,
      "loss": 1.7323,
      "step": 215840
    },
    {
      "epoch": 0.000783442330744418,
      "grad_norm": 9895.036129292303,
      "learning_rate": 2.1533728206191724e-07,
      "loss": 1.7375,
      "step": 215872
    },
    {
      "epoch": 0.0007835584650952547,
      "grad_norm": 10309.823664835398,
      "learning_rate": 2.1532130748607465e-07,
      "loss": 1.7118,
      "step": 215904
    },
    {
      "epoch": 0.0007836745994460914,
      "grad_norm": 10430.168742642662,
      "learning_rate": 2.1530533646487734e-07,
      "loss": 1.7189,
      "step": 215936
    },
    {
      "epoch": 0.0007837907337969281,
      "grad_norm": 9482.855266215973,
      "learning_rate": 2.152893689970072e-07,
      "loss": 1.6982,
      "step": 215968
    },
    {
      "epoch": 0.0007839068681477648,
      "grad_norm": 10468.532084299117,
      "learning_rate": 2.1527340508114683e-07,
      "loss": 1.6898,
      "step": 216000
    },
    {
      "epoch": 0.0007840230024986015,
      "grad_norm": 8972.511799936516,
      "learning_rate": 2.152574447159795e-07,
      "loss": 1.7118,
      "step": 216032
    },
    {
      "epoch": 0.0007841391368494382,
      "grad_norm": 9481.339462333368,
      "learning_rate": 2.1524148790018917e-07,
      "loss": 1.7263,
      "step": 216064
    },
    {
      "epoch": 0.0007842552712002749,
      "grad_norm": 10942.846064895548,
      "learning_rate": 2.1522553463246044e-07,
      "loss": 1.7232,
      "step": 216096
    },
    {
      "epoch": 0.0007843714055511116,
      "grad_norm": 10437.337399931077,
      "learning_rate": 2.152095849114787e-07,
      "loss": 1.7012,
      "step": 216128
    },
    {
      "epoch": 0.0007844875399019484,
      "grad_norm": 9113.900591952932,
      "learning_rate": 2.1519363873592983e-07,
      "loss": 1.7199,
      "step": 216160
    },
    {
      "epoch": 0.000784603674252785,
      "grad_norm": 9054.231607375636,
      "learning_rate": 2.1517769610450066e-07,
      "loss": 1.6988,
      "step": 216192
    },
    {
      "epoch": 0.0007847198086036218,
      "grad_norm": 10891.728788397184,
      "learning_rate": 2.1516175701587845e-07,
      "loss": 1.7169,
      "step": 216224
    },
    {
      "epoch": 0.0007848359429544584,
      "grad_norm": 8890.985997064667,
      "learning_rate": 2.1514582146875131e-07,
      "loss": 1.7258,
      "step": 216256
    },
    {
      "epoch": 0.0007849520773052952,
      "grad_norm": 9589.057096503284,
      "learning_rate": 2.1512988946180788e-07,
      "loss": 1.7182,
      "step": 216288
    },
    {
      "epoch": 0.0007850682116561318,
      "grad_norm": 10865.822012162724,
      "learning_rate": 2.1511396099373767e-07,
      "loss": 1.706,
      "step": 216320
    },
    {
      "epoch": 0.0007851843460069686,
      "grad_norm": 10626.087332598016,
      "learning_rate": 2.1509803606323065e-07,
      "loss": 1.717,
      "step": 216352
    },
    {
      "epoch": 0.0007853004803578052,
      "grad_norm": 7892.931394608723,
      "learning_rate": 2.1508211466897768e-07,
      "loss": 1.7121,
      "step": 216384
    },
    {
      "epoch": 0.000785416614708642,
      "grad_norm": 11176.123478201196,
      "learning_rate": 2.1506619680967014e-07,
      "loss": 1.7249,
      "step": 216416
    },
    {
      "epoch": 0.0007855327490594788,
      "grad_norm": 9550.464805442718,
      "learning_rate": 2.1505028248400016e-07,
      "loss": 1.7393,
      "step": 216448
    },
    {
      "epoch": 0.0007856488834103154,
      "grad_norm": 12054.12427345927,
      "learning_rate": 2.1503437169066054e-07,
      "loss": 1.7378,
      "step": 216480
    },
    {
      "epoch": 0.0007857650177611522,
      "grad_norm": 8448.966564024266,
      "learning_rate": 2.1501846442834474e-07,
      "loss": 1.7192,
      "step": 216512
    },
    {
      "epoch": 0.0007858811521119888,
      "grad_norm": 9796.350544973368,
      "learning_rate": 2.1500256069574694e-07,
      "loss": 1.7105,
      "step": 216544
    },
    {
      "epoch": 0.0007859972864628256,
      "grad_norm": 10012.15970707619,
      "learning_rate": 2.1498666049156191e-07,
      "loss": 1.7279,
      "step": 216576
    },
    {
      "epoch": 0.0007861134208136622,
      "grad_norm": 8377.224122583804,
      "learning_rate": 2.1497076381448522e-07,
      "loss": 1.7167,
      "step": 216608
    },
    {
      "epoch": 0.000786229555164499,
      "grad_norm": 11332.453573697092,
      "learning_rate": 2.1495487066321297e-07,
      "loss": 1.7271,
      "step": 216640
    },
    {
      "epoch": 0.0007863456895153356,
      "grad_norm": 9971.924989689804,
      "learning_rate": 2.1493898103644206e-07,
      "loss": 1.746,
      "step": 216672
    },
    {
      "epoch": 0.0007864618238661723,
      "grad_norm": 9879.535414178139,
      "learning_rate": 2.1492309493287e-07,
      "loss": 1.7389,
      "step": 216704
    },
    {
      "epoch": 0.0007865779582170091,
      "grad_norm": 12069.702564686504,
      "learning_rate": 2.149077086285753e-07,
      "loss": 1.698,
      "step": 216736
    },
    {
      "epoch": 0.0007866940925678457,
      "grad_norm": 10592.895921323876,
      "learning_rate": 2.1489182945749727e-07,
      "loss": 1.6942,
      "step": 216768
    },
    {
      "epoch": 0.0007868102269186825,
      "grad_norm": 9465.275062035967,
      "learning_rate": 2.1487595380575532e-07,
      "loss": 1.6992,
      "step": 216800
    },
    {
      "epoch": 0.0007869263612695191,
      "grad_norm": 10166.483757917484,
      "learning_rate": 2.1486008167204966e-07,
      "loss": 1.6967,
      "step": 216832
    },
    {
      "epoch": 0.0007870424956203559,
      "grad_norm": 9851.833331923557,
      "learning_rate": 2.1484421305508114e-07,
      "loss": 1.7102,
      "step": 216864
    },
    {
      "epoch": 0.0007871586299711925,
      "grad_norm": 10552.722302799406,
      "learning_rate": 2.1482834795355132e-07,
      "loss": 1.705,
      "step": 216896
    },
    {
      "epoch": 0.0007872747643220293,
      "grad_norm": 7823.106544078254,
      "learning_rate": 2.148124863661624e-07,
      "loss": 1.7039,
      "step": 216928
    },
    {
      "epoch": 0.000787390898672866,
      "grad_norm": 10248.137196583582,
      "learning_rate": 2.1479662829161726e-07,
      "loss": 1.7189,
      "step": 216960
    },
    {
      "epoch": 0.0007875070330237027,
      "grad_norm": 8150.800083427393,
      "learning_rate": 2.1478077372861945e-07,
      "loss": 1.7342,
      "step": 216992
    },
    {
      "epoch": 0.0007876231673745395,
      "grad_norm": 9101.92045669484,
      "learning_rate": 2.1476492267587317e-07,
      "loss": 1.7027,
      "step": 217024
    },
    {
      "epoch": 0.0007877393017253761,
      "grad_norm": 9375.325274357152,
      "learning_rate": 2.1474907513208337e-07,
      "loss": 1.7367,
      "step": 217056
    },
    {
      "epoch": 0.0007878554360762129,
      "grad_norm": 8354.815377972154,
      "learning_rate": 2.1473323109595553e-07,
      "loss": 1.7383,
      "step": 217088
    },
    {
      "epoch": 0.0007879715704270495,
      "grad_norm": 10158.503826843795,
      "learning_rate": 2.1471739056619593e-07,
      "loss": 1.7235,
      "step": 217120
    },
    {
      "epoch": 0.0007880877047778863,
      "grad_norm": 8698.578734483008,
      "learning_rate": 2.1470155354151145e-07,
      "loss": 1.7011,
      "step": 217152
    },
    {
      "epoch": 0.0007882038391287229,
      "grad_norm": 10051.765218109702,
      "learning_rate": 2.1468572002060965e-07,
      "loss": 1.714,
      "step": 217184
    },
    {
      "epoch": 0.0007883199734795597,
      "grad_norm": 9127.871164734963,
      "learning_rate": 2.146698900021987e-07,
      "loss": 1.7124,
      "step": 217216
    },
    {
      "epoch": 0.0007884361078303963,
      "grad_norm": 9247.038985534775,
      "learning_rate": 2.1465406348498764e-07,
      "loss": 1.7105,
      "step": 217248
    },
    {
      "epoch": 0.0007885522421812331,
      "grad_norm": 9314.745192435486,
      "learning_rate": 2.1463824046768592e-07,
      "loss": 1.7447,
      "step": 217280
    },
    {
      "epoch": 0.0007886683765320698,
      "grad_norm": 10632.835934030018,
      "learning_rate": 2.146224209490038e-07,
      "loss": 1.7265,
      "step": 217312
    },
    {
      "epoch": 0.0007887845108829065,
      "grad_norm": 9328.21419136589,
      "learning_rate": 2.1460660492765217e-07,
      "loss": 1.6971,
      "step": 217344
    },
    {
      "epoch": 0.0007889006452337432,
      "grad_norm": 10419.717270636473,
      "learning_rate": 2.1459079240234256e-07,
      "loss": 1.7046,
      "step": 217376
    },
    {
      "epoch": 0.0007890167795845799,
      "grad_norm": 9082.340116952239,
      "learning_rate": 2.1457498337178724e-07,
      "loss": 1.7197,
      "step": 217408
    },
    {
      "epoch": 0.0007891329139354166,
      "grad_norm": 9674.31093153409,
      "learning_rate": 2.1455917783469908e-07,
      "loss": 1.7065,
      "step": 217440
    },
    {
      "epoch": 0.0007892490482862533,
      "grad_norm": 9473.000791723814,
      "learning_rate": 2.1454337578979163e-07,
      "loss": 1.7367,
      "step": 217472
    },
    {
      "epoch": 0.00078936518263709,
      "grad_norm": 8318.342142518544,
      "learning_rate": 2.1452757723577909e-07,
      "loss": 1.7325,
      "step": 217504
    },
    {
      "epoch": 0.0007894813169879267,
      "grad_norm": 9073.32728385789,
      "learning_rate": 2.1451178217137632e-07,
      "loss": 1.7323,
      "step": 217536
    },
    {
      "epoch": 0.0007895974513387634,
      "grad_norm": 9516.927445347053,
      "learning_rate": 2.1449599059529892e-07,
      "loss": 1.7321,
      "step": 217568
    },
    {
      "epoch": 0.0007897135856896002,
      "grad_norm": 9380.73259399286,
      "learning_rate": 2.1448020250626306e-07,
      "loss": 1.7213,
      "step": 217600
    },
    {
      "epoch": 0.0007898297200404368,
      "grad_norm": 9307.946819787918,
      "learning_rate": 2.1446441790298557e-07,
      "loss": 1.7077,
      "step": 217632
    },
    {
      "epoch": 0.0007899458543912736,
      "grad_norm": 7790.023491620548,
      "learning_rate": 2.14448636784184e-07,
      "loss": 1.7173,
      "step": 217664
    },
    {
      "epoch": 0.0007900619887421102,
      "grad_norm": 9509.489996839999,
      "learning_rate": 2.1443285914857656e-07,
      "loss": 1.7304,
      "step": 217696
    },
    {
      "epoch": 0.000790178123092947,
      "grad_norm": 8588.911921774492,
      "learning_rate": 2.14417577884493e-07,
      "loss": 1.7087,
      "step": 217728
    },
    {
      "epoch": 0.0007902942574437836,
      "grad_norm": 10247.039182124756,
      "learning_rate": 2.1440180710268051e-07,
      "loss": 1.6868,
      "step": 217760
    },
    {
      "epoch": 0.0007904103917946204,
      "grad_norm": 9567.611823229452,
      "learning_rate": 2.1438603980026064e-07,
      "loss": 1.705,
      "step": 217792
    },
    {
      "epoch": 0.000790526526145457,
      "grad_norm": 10060.181708100505,
      "learning_rate": 2.1437027597595414e-07,
      "loss": 1.7285,
      "step": 217824
    },
    {
      "epoch": 0.0007906426604962938,
      "grad_norm": 12340.080226643584,
      "learning_rate": 2.1435451562848254e-07,
      "loss": 1.715,
      "step": 217856
    },
    {
      "epoch": 0.0007907587948471305,
      "grad_norm": 8198.489860943904,
      "learning_rate": 2.1433875875656786e-07,
      "loss": 1.721,
      "step": 217888
    },
    {
      "epoch": 0.0007908749291979672,
      "grad_norm": 10015.07563625957,
      "learning_rate": 2.1432300535893294e-07,
      "loss": 1.7195,
      "step": 217920
    },
    {
      "epoch": 0.0007909910635488039,
      "grad_norm": 8846.829601614354,
      "learning_rate": 2.143072554343012e-07,
      "loss": 1.7096,
      "step": 217952
    },
    {
      "epoch": 0.0007911071978996406,
      "grad_norm": 7739.941860246755,
      "learning_rate": 2.142915089813967e-07,
      "loss": 1.7032,
      "step": 217984
    },
    {
      "epoch": 0.0007912233322504773,
      "grad_norm": 9408.661222511946,
      "learning_rate": 2.142757659989442e-07,
      "loss": 1.716,
      "step": 218016
    },
    {
      "epoch": 0.000791339466601314,
      "grad_norm": 10027.85600215719,
      "learning_rate": 2.142600264856691e-07,
      "loss": 1.7108,
      "step": 218048
    },
    {
      "epoch": 0.0007914556009521507,
      "grad_norm": 9251.754644390436,
      "learning_rate": 2.1424429044029748e-07,
      "loss": 1.7133,
      "step": 218080
    },
    {
      "epoch": 0.0007915717353029874,
      "grad_norm": 9478.266297166376,
      "learning_rate": 2.14228557861556e-07,
      "loss": 1.7268,
      "step": 218112
    },
    {
      "epoch": 0.0007916878696538241,
      "grad_norm": 11259.08770727007,
      "learning_rate": 2.1421282874817202e-07,
      "loss": 1.7374,
      "step": 218144
    },
    {
      "epoch": 0.0007918040040046609,
      "grad_norm": 9471.739650138194,
      "learning_rate": 2.1419710309887365e-07,
      "loss": 1.7034,
      "step": 218176
    },
    {
      "epoch": 0.0007919201383554975,
      "grad_norm": 8829.8373710958,
      "learning_rate": 2.1418138091238943e-07,
      "loss": 1.7117,
      "step": 218208
    },
    {
      "epoch": 0.0007920362727063343,
      "grad_norm": 9011.461146784133,
      "learning_rate": 2.1416566218744877e-07,
      "loss": 1.7347,
      "step": 218240
    },
    {
      "epoch": 0.0007921524070571709,
      "grad_norm": 10125.785302878981,
      "learning_rate": 2.1414994692278163e-07,
      "loss": 1.7145,
      "step": 218272
    },
    {
      "epoch": 0.0007922685414080077,
      "grad_norm": 8439.575937213907,
      "learning_rate": 2.1413423511711862e-07,
      "loss": 1.7332,
      "step": 218304
    },
    {
      "epoch": 0.0007923846757588443,
      "grad_norm": 9305.5507091198,
      "learning_rate": 2.1411852676919106e-07,
      "loss": 1.7296,
      "step": 218336
    },
    {
      "epoch": 0.0007925008101096811,
      "grad_norm": 9920.525187710578,
      "learning_rate": 2.1410282187773083e-07,
      "loss": 1.7278,
      "step": 218368
    },
    {
      "epoch": 0.0007926169444605177,
      "grad_norm": 12990.42201008112,
      "learning_rate": 2.1408712044147057e-07,
      "loss": 1.7172,
      "step": 218400
    },
    {
      "epoch": 0.0007927330788113545,
      "grad_norm": 8960.356466123432,
      "learning_rate": 2.1407142245914345e-07,
      "loss": 1.748,
      "step": 218432
    },
    {
      "epoch": 0.0007928492131621912,
      "grad_norm": 10767.932763534513,
      "learning_rate": 2.140557279294834e-07,
      "loss": 1.7166,
      "step": 218464
    },
    {
      "epoch": 0.0007929653475130279,
      "grad_norm": 7511.748265217626,
      "learning_rate": 2.1404003685122497e-07,
      "loss": 1.6944,
      "step": 218496
    },
    {
      "epoch": 0.0007930814818638646,
      "grad_norm": 8855.55960964636,
      "learning_rate": 2.1402434922310327e-07,
      "loss": 1.7069,
      "step": 218528
    },
    {
      "epoch": 0.0007931976162147013,
      "grad_norm": 9618.23684466129,
      "learning_rate": 2.140086650438542e-07,
      "loss": 1.7111,
      "step": 218560
    },
    {
      "epoch": 0.000793313750565538,
      "grad_norm": 8595.184465734286,
      "learning_rate": 2.1399298431221422e-07,
      "loss": 1.6854,
      "step": 218592
    },
    {
      "epoch": 0.0007934298849163747,
      "grad_norm": 10502.918641977572,
      "learning_rate": 2.1397730702692044e-07,
      "loss": 1.6905,
      "step": 218624
    },
    {
      "epoch": 0.0007935460192672114,
      "grad_norm": 9601.474053498245,
      "learning_rate": 2.1396163318671066e-07,
      "loss": 1.7089,
      "step": 218656
    },
    {
      "epoch": 0.0007936621536180481,
      "grad_norm": 8717.161579321564,
      "learning_rate": 2.139459627903233e-07,
      "loss": 1.7084,
      "step": 218688
    },
    {
      "epoch": 0.0007937782879688848,
      "grad_norm": 21423.985436888255,
      "learning_rate": 2.139302958364974e-07,
      "loss": 1.7446,
      "step": 218720
    },
    {
      "epoch": 0.0007938944223197216,
      "grad_norm": 9539.835428349905,
      "learning_rate": 2.1391512175666158e-07,
      "loss": 1.7227,
      "step": 218752
    },
    {
      "epoch": 0.0007940105566705582,
      "grad_norm": 9212.168691464567,
      "learning_rate": 2.1389946157669622e-07,
      "loss": 1.7174,
      "step": 218784
    },
    {
      "epoch": 0.000794126691021395,
      "grad_norm": 9422.160261850782,
      "learning_rate": 2.1388380483555273e-07,
      "loss": 1.7184,
      "step": 218816
    },
    {
      "epoch": 0.0007942428253722316,
      "grad_norm": 9512.358908283475,
      "learning_rate": 2.138681515319728e-07,
      "loss": 1.7293,
      "step": 218848
    },
    {
      "epoch": 0.0007943589597230684,
      "grad_norm": 11090.805922023881,
      "learning_rate": 2.1385250166469867e-07,
      "loss": 1.7213,
      "step": 218880
    },
    {
      "epoch": 0.000794475094073905,
      "grad_norm": 8604.653857070603,
      "learning_rate": 2.1383685523247324e-07,
      "loss": 1.7089,
      "step": 218912
    },
    {
      "epoch": 0.0007945912284247418,
      "grad_norm": 10028.791153474082,
      "learning_rate": 2.1382121223404012e-07,
      "loss": 1.72,
      "step": 218944
    },
    {
      "epoch": 0.0007947073627755784,
      "grad_norm": 7919.312848978755,
      "learning_rate": 2.138055726681435e-07,
      "loss": 1.7282,
      "step": 218976
    },
    {
      "epoch": 0.0007948234971264152,
      "grad_norm": 9814.748697750747,
      "learning_rate": 2.1378993653352823e-07,
      "loss": 1.7113,
      "step": 219008
    },
    {
      "epoch": 0.0007949396314772519,
      "grad_norm": 9236.366385110543,
      "learning_rate": 2.1377430382893977e-07,
      "loss": 1.712,
      "step": 219040
    },
    {
      "epoch": 0.0007950557658280886,
      "grad_norm": 11091.190197629829,
      "learning_rate": 2.1375867455312433e-07,
      "loss": 1.7135,
      "step": 219072
    },
    {
      "epoch": 0.0007951719001789253,
      "grad_norm": 10969.562251977059,
      "learning_rate": 2.1374304870482863e-07,
      "loss": 1.6906,
      "step": 219104
    },
    {
      "epoch": 0.000795288034529762,
      "grad_norm": 8145.459594154279,
      "learning_rate": 2.1372742628280008e-07,
      "loss": 1.7145,
      "step": 219136
    },
    {
      "epoch": 0.0007954041688805987,
      "grad_norm": 9036.598807073378,
      "learning_rate": 2.1371180728578676e-07,
      "loss": 1.7267,
      "step": 219168
    },
    {
      "epoch": 0.0007955203032314354,
      "grad_norm": 9190.120347416567,
      "learning_rate": 2.136961917125374e-07,
      "loss": 1.7162,
      "step": 219200
    },
    {
      "epoch": 0.0007956364375822721,
      "grad_norm": 8451.154477348051,
      "learning_rate": 2.1368057956180127e-07,
      "loss": 1.72,
      "step": 219232
    },
    {
      "epoch": 0.0007957525719331088,
      "grad_norm": 10052.811149126397,
      "learning_rate": 2.1366497083232843e-07,
      "loss": 1.7389,
      "step": 219264
    },
    {
      "epoch": 0.0007958687062839455,
      "grad_norm": 9767.172774145034,
      "learning_rate": 2.136493655228694e-07,
      "loss": 1.7339,
      "step": 219296
    },
    {
      "epoch": 0.0007959848406347823,
      "grad_norm": 9182.997549819993,
      "learning_rate": 2.136337636321755e-07,
      "loss": 1.7197,
      "step": 219328
    },
    {
      "epoch": 0.0007961009749856189,
      "grad_norm": 9193.886882053748,
      "learning_rate": 2.136181651589986e-07,
      "loss": 1.7163,
      "step": 219360
    },
    {
      "epoch": 0.0007962171093364557,
      "grad_norm": 9601.782542840678,
      "learning_rate": 2.1360257010209128e-07,
      "loss": 1.7179,
      "step": 219392
    },
    {
      "epoch": 0.0007963332436872923,
      "grad_norm": 10341.0284788313,
      "learning_rate": 2.1358697846020665e-07,
      "loss": 1.7102,
      "step": 219424
    },
    {
      "epoch": 0.0007964493780381291,
      "grad_norm": 9239.985714274671,
      "learning_rate": 2.1357139023209853e-07,
      "loss": 1.7166,
      "step": 219456
    },
    {
      "epoch": 0.0007965655123889657,
      "grad_norm": 10195.632202075554,
      "learning_rate": 2.1355580541652138e-07,
      "loss": 1.7025,
      "step": 219488
    },
    {
      "epoch": 0.0007966816467398025,
      "grad_norm": 9957.1889607459,
      "learning_rate": 2.1354022401223027e-07,
      "loss": 1.6888,
      "step": 219520
    },
    {
      "epoch": 0.0007967977810906391,
      "grad_norm": 8592.158867246346,
      "learning_rate": 2.1352464601798088e-07,
      "loss": 1.7317,
      "step": 219552
    },
    {
      "epoch": 0.0007969139154414759,
      "grad_norm": 9277.689690865933,
      "learning_rate": 2.1350907143252962e-07,
      "loss": 1.7355,
      "step": 219584
    },
    {
      "epoch": 0.0007970300497923126,
      "grad_norm": 10688.574086378407,
      "learning_rate": 2.1349350025463346e-07,
      "loss": 1.7216,
      "step": 219616
    },
    {
      "epoch": 0.0007971461841431493,
      "grad_norm": 11362.592309856056,
      "learning_rate": 2.1347793248305e-07,
      "loss": 1.7077,
      "step": 219648
    },
    {
      "epoch": 0.000797262318493986,
      "grad_norm": 9406.907036853292,
      "learning_rate": 2.134623681165375e-07,
      "loss": 1.7178,
      "step": 219680
    },
    {
      "epoch": 0.0007973784528448227,
      "grad_norm": 8838.084294687396,
      "learning_rate": 2.1344680715385486e-07,
      "loss": 1.6961,
      "step": 219712
    },
    {
      "epoch": 0.0007974945871956594,
      "grad_norm": 16844.188315261737,
      "learning_rate": 2.1343124959376155e-07,
      "loss": 1.7108,
      "step": 219744
    },
    {
      "epoch": 0.0007976107215464961,
      "grad_norm": 10143.314645617575,
      "learning_rate": 2.1341618145100556e-07,
      "loss": 1.7213,
      "step": 219776
    },
    {
      "epoch": 0.0007977268558973328,
      "grad_norm": 8941.03651709353,
      "learning_rate": 2.134006305861374e-07,
      "loss": 1.7182,
      "step": 219808
    },
    {
      "epoch": 0.0007978429902481695,
      "grad_norm": 9864.158555092268,
      "learning_rate": 2.133850831201797e-07,
      "loss": 1.7076,
      "step": 219840
    },
    {
      "epoch": 0.0007979591245990062,
      "grad_norm": 10756.359235354683,
      "learning_rate": 2.133695390518944e-07,
      "loss": 1.7315,
      "step": 219872
    },
    {
      "epoch": 0.000798075258949843,
      "grad_norm": 10477.977476593467,
      "learning_rate": 2.1335399838004424e-07,
      "loss": 1.7272,
      "step": 219904
    },
    {
      "epoch": 0.0007981913933006796,
      "grad_norm": 9477.224488213837,
      "learning_rate": 2.1333846110339247e-07,
      "loss": 1.7108,
      "step": 219936
    },
    {
      "epoch": 0.0007983075276515164,
      "grad_norm": 9408.414531683859,
      "learning_rate": 2.1332292722070305e-07,
      "loss": 1.7364,
      "step": 219968
    },
    {
      "epoch": 0.000798423662002353,
      "grad_norm": 10533.6198906169,
      "learning_rate": 2.1330739673074046e-07,
      "loss": 1.7529,
      "step": 220000
    },
    {
      "epoch": 0.0007985397963531898,
      "grad_norm": 9337.23192386266,
      "learning_rate": 2.1329186963226994e-07,
      "loss": 1.7372,
      "step": 220032
    },
    {
      "epoch": 0.0007986559307040264,
      "grad_norm": 10393.456787806452,
      "learning_rate": 2.132763459240573e-07,
      "loss": 1.7266,
      "step": 220064
    },
    {
      "epoch": 0.0007987720650548632,
      "grad_norm": 11035.959043055569,
      "learning_rate": 2.1326082560486895e-07,
      "loss": 1.7421,
      "step": 220096
    },
    {
      "epoch": 0.0007988881994056998,
      "grad_norm": 10556.26581703966,
      "learning_rate": 2.1324530867347196e-07,
      "loss": 1.7266,
      "step": 220128
    },
    {
      "epoch": 0.0007990043337565366,
      "grad_norm": 8873.230527829197,
      "learning_rate": 2.1322979512863408e-07,
      "loss": 1.7375,
      "step": 220160
    },
    {
      "epoch": 0.0007991204681073733,
      "grad_norm": 9281.864036927067,
      "learning_rate": 2.1321428496912355e-07,
      "loss": 1.7553,
      "step": 220192
    },
    {
      "epoch": 0.00079923660245821,
      "grad_norm": 8676.995217239664,
      "learning_rate": 2.1319877819370936e-07,
      "loss": 1.7356,
      "step": 220224
    },
    {
      "epoch": 0.0007993527368090467,
      "grad_norm": 9746.77649276929,
      "learning_rate": 2.131832748011611e-07,
      "loss": 1.6962,
      "step": 220256
    },
    {
      "epoch": 0.0007994688711598834,
      "grad_norm": 9682.693530211518,
      "learning_rate": 2.1316777479024896e-07,
      "loss": 1.708,
      "step": 220288
    },
    {
      "epoch": 0.0007995850055107201,
      "grad_norm": 10312.545757474243,
      "learning_rate": 2.1315227815974377e-07,
      "loss": 1.7155,
      "step": 220320
    },
    {
      "epoch": 0.0007997011398615568,
      "grad_norm": 9191.664702326778,
      "learning_rate": 2.1313678490841697e-07,
      "loss": 1.712,
      "step": 220352
    },
    {
      "epoch": 0.0007998172742123935,
      "grad_norm": 10384.014252686675,
      "learning_rate": 2.1312129503504065e-07,
      "loss": 1.7271,
      "step": 220384
    },
    {
      "epoch": 0.0007999334085632302,
      "grad_norm": 9623.36770574626,
      "learning_rate": 2.131058085383875e-07,
      "loss": 1.73,
      "step": 220416
    },
    {
      "epoch": 0.0008000495429140669,
      "grad_norm": 10011.187142392255,
      "learning_rate": 2.1309032541723088e-07,
      "loss": 1.7251,
      "step": 220448
    },
    {
      "epoch": 0.0008001656772649037,
      "grad_norm": 8905.739609936954,
      "learning_rate": 2.130748456703447e-07,
      "loss": 1.7166,
      "step": 220480
    },
    {
      "epoch": 0.0008002818116157403,
      "grad_norm": 9536.83133960122,
      "learning_rate": 2.1305936929650356e-07,
      "loss": 1.718,
      "step": 220512
    },
    {
      "epoch": 0.0008003979459665771,
      "grad_norm": 8717.042503051136,
      "learning_rate": 2.1304389629448266e-07,
      "loss": 1.7078,
      "step": 220544
    },
    {
      "epoch": 0.0008005140803174137,
      "grad_norm": 8002.640689172544,
      "learning_rate": 2.130284266630578e-07,
      "loss": 1.7231,
      "step": 220576
    },
    {
      "epoch": 0.0008006302146682505,
      "grad_norm": 9414.345436619586,
      "learning_rate": 2.1301296040100543e-07,
      "loss": 1.7336,
      "step": 220608
    },
    {
      "epoch": 0.0008007463490190871,
      "grad_norm": 7785.501139939548,
      "learning_rate": 2.1299749750710263e-07,
      "loss": 1.7391,
      "step": 220640
    },
    {
      "epoch": 0.0008008624833699239,
      "grad_norm": 10571.161714778562,
      "learning_rate": 2.1298203798012705e-07,
      "loss": 1.7033,
      "step": 220672
    },
    {
      "epoch": 0.0008009786177207605,
      "grad_norm": 10048.563877490156,
      "learning_rate": 2.1296658181885702e-07,
      "loss": 1.7119,
      "step": 220704
    },
    {
      "epoch": 0.0008010947520715973,
      "grad_norm": 8541.276719554284,
      "learning_rate": 2.1295112902207149e-07,
      "loss": 1.7233,
      "step": 220736
    },
    {
      "epoch": 0.000801210886422434,
      "grad_norm": 10065.203823072834,
      "learning_rate": 2.1293616233245085e-07,
      "loss": 1.7035,
      "step": 220768
    },
    {
      "epoch": 0.0008013270207732707,
      "grad_norm": 8602.732124156837,
      "learning_rate": 2.1292071615592808e-07,
      "loss": 1.7298,
      "step": 220800
    },
    {
      "epoch": 0.0008014431551241075,
      "grad_norm": 9627.18941332308,
      "learning_rate": 2.1290527334026838e-07,
      "loss": 1.7337,
      "step": 220832
    },
    {
      "epoch": 0.0008015592894749441,
      "grad_norm": 11473.224481374014,
      "learning_rate": 2.1288983388425314e-07,
      "loss": 1.7091,
      "step": 220864
    },
    {
      "epoch": 0.0008016754238257809,
      "grad_norm": 8982.643931493667,
      "learning_rate": 2.1287439778666437e-07,
      "loss": 1.7057,
      "step": 220896
    },
    {
      "epoch": 0.0008017915581766175,
      "grad_norm": 11008.331572041241,
      "learning_rate": 2.1285896504628465e-07,
      "loss": 1.7304,
      "step": 220928
    },
    {
      "epoch": 0.0008019076925274543,
      "grad_norm": 9909.69787632297,
      "learning_rate": 2.1284353566189725e-07,
      "loss": 1.7126,
      "step": 220960
    },
    {
      "epoch": 0.0008020238268782909,
      "grad_norm": 9789.230408974956,
      "learning_rate": 2.12828109632286e-07,
      "loss": 1.7465,
      "step": 220992
    },
    {
      "epoch": 0.0008021399612291277,
      "grad_norm": 9337.463467130674,
      "learning_rate": 2.1281268695623544e-07,
      "loss": 1.7409,
      "step": 221024
    },
    {
      "epoch": 0.0008022560955799643,
      "grad_norm": 8888.922994379016,
      "learning_rate": 2.127972676325306e-07,
      "loss": 1.7367,
      "step": 221056
    },
    {
      "epoch": 0.000802372229930801,
      "grad_norm": 10194.611125491741,
      "learning_rate": 2.1278185165995722e-07,
      "loss": 1.7129,
      "step": 221088
    },
    {
      "epoch": 0.0008024883642816378,
      "grad_norm": 10212.69484514249,
      "learning_rate": 2.1276643903730158e-07,
      "loss": 1.7147,
      "step": 221120
    },
    {
      "epoch": 0.0008026044986324745,
      "grad_norm": 10758.240190663155,
      "learning_rate": 2.1275102976335064e-07,
      "loss": 1.7197,
      "step": 221152
    },
    {
      "epoch": 0.0008027206329833112,
      "grad_norm": 10233.702164905915,
      "learning_rate": 2.1273562383689197e-07,
      "loss": 1.7241,
      "step": 221184
    },
    {
      "epoch": 0.0008028367673341479,
      "grad_norm": 10286.313333745962,
      "learning_rate": 2.127202212567137e-07,
      "loss": 1.7419,
      "step": 221216
    },
    {
      "epoch": 0.0008029529016849846,
      "grad_norm": 12129.384155842374,
      "learning_rate": 2.1270482202160467e-07,
      "loss": 1.7236,
      "step": 221248
    },
    {
      "epoch": 0.0008030690360358213,
      "grad_norm": 9681.79074345237,
      "learning_rate": 2.126894261303542e-07,
      "loss": 1.6998,
      "step": 221280
    },
    {
      "epoch": 0.000803185170386658,
      "grad_norm": 9842.765058661107,
      "learning_rate": 2.1267403358175237e-07,
      "loss": 1.7157,
      "step": 221312
    },
    {
      "epoch": 0.0008033013047374947,
      "grad_norm": 9397.848477178168,
      "learning_rate": 2.1265864437458976e-07,
      "loss": 1.7352,
      "step": 221344
    },
    {
      "epoch": 0.0008034174390883314,
      "grad_norm": 10621.786478742642,
      "learning_rate": 2.126432585076576e-07,
      "loss": 1.7057,
      "step": 221376
    },
    {
      "epoch": 0.0008035335734391682,
      "grad_norm": 10040.999751020812,
      "learning_rate": 2.1262787597974775e-07,
      "loss": 1.7371,
      "step": 221408
    },
    {
      "epoch": 0.0008036497077900048,
      "grad_norm": 9579.370960558945,
      "learning_rate": 2.1261249678965266e-07,
      "loss": 1.7198,
      "step": 221440
    },
    {
      "epoch": 0.0008037658421408416,
      "grad_norm": 10556.258617521646,
      "learning_rate": 2.125971209361654e-07,
      "loss": 1.7039,
      "step": 221472
    },
    {
      "epoch": 0.0008038819764916782,
      "grad_norm": 9709.591134543205,
      "learning_rate": 2.1258174841807964e-07,
      "loss": 1.7026,
      "step": 221504
    },
    {
      "epoch": 0.000803998110842515,
      "grad_norm": 8931.06040736485,
      "learning_rate": 2.1256637923418968e-07,
      "loss": 1.7073,
      "step": 221536
    },
    {
      "epoch": 0.0008041142451933516,
      "grad_norm": 10952.873412945117,
      "learning_rate": 2.1255101338329043e-07,
      "loss": 1.7049,
      "step": 221568
    },
    {
      "epoch": 0.0008042303795441884,
      "grad_norm": 11441.885683749859,
      "learning_rate": 2.125356508641774e-07,
      "loss": 1.7094,
      "step": 221600
    },
    {
      "epoch": 0.000804346513895025,
      "grad_norm": 9861.130462578822,
      "learning_rate": 2.1252029167564665e-07,
      "loss": 1.7192,
      "step": 221632
    },
    {
      "epoch": 0.0008044626482458618,
      "grad_norm": 10215.978269358251,
      "learning_rate": 2.12504935816495e-07,
      "loss": 1.7205,
      "step": 221664
    },
    {
      "epoch": 0.0008045787825966985,
      "grad_norm": 8886.780856980777,
      "learning_rate": 2.1248958328551973e-07,
      "loss": 1.7099,
      "step": 221696
    },
    {
      "epoch": 0.0008046949169475352,
      "grad_norm": 10303.877910767382,
      "learning_rate": 2.1247423408151877e-07,
      "loss": 1.72,
      "step": 221728
    },
    {
      "epoch": 0.0008048110512983719,
      "grad_norm": 8220.192455167944,
      "learning_rate": 2.12459367711656e-07,
      "loss": 1.7383,
      "step": 221760
    },
    {
      "epoch": 0.0008049271856492086,
      "grad_norm": 9787.117655367181,
      "learning_rate": 2.124440250541253e-07,
      "loss": 1.7351,
      "step": 221792
    },
    {
      "epoch": 0.0008050433200000453,
      "grad_norm": 9411.673177496124,
      "learning_rate": 2.124286857200039e-07,
      "loss": 1.7556,
      "step": 221824
    },
    {
      "epoch": 0.000805159454350882,
      "grad_norm": 9167.85100227965,
      "learning_rate": 2.1241334970809218e-07,
      "loss": 1.7477,
      "step": 221856
    },
    {
      "epoch": 0.0008052755887017187,
      "grad_norm": 11458.970634398189,
      "learning_rate": 2.1239801701719107e-07,
      "loss": 1.7408,
      "step": 221888
    },
    {
      "epoch": 0.0008053917230525554,
      "grad_norm": 9139.710061046795,
      "learning_rate": 2.1238268764610218e-07,
      "loss": 1.7305,
      "step": 221920
    },
    {
      "epoch": 0.0008055078574033921,
      "grad_norm": 10586.9655709273,
      "learning_rate": 2.1236736159362763e-07,
      "loss": 1.7401,
      "step": 221952
    },
    {
      "epoch": 0.0008056239917542289,
      "grad_norm": 9727.408082320799,
      "learning_rate": 2.1235203885857022e-07,
      "loss": 1.7235,
      "step": 221984
    },
    {
      "epoch": 0.0008057401261050655,
      "grad_norm": 9532.164706927802,
      "learning_rate": 2.1233671943973333e-07,
      "loss": 1.7067,
      "step": 222016
    },
    {
      "epoch": 0.0008058562604559023,
      "grad_norm": 8272.515457827807,
      "learning_rate": 2.1232140333592095e-07,
      "loss": 1.7199,
      "step": 222048
    },
    {
      "epoch": 0.0008059723948067389,
      "grad_norm": 10036.906894058548,
      "learning_rate": 2.1230609054593767e-07,
      "loss": 1.7194,
      "step": 222080
    },
    {
      "epoch": 0.0008060885291575757,
      "grad_norm": 9935.649349690235,
      "learning_rate": 2.1229078106858866e-07,
      "loss": 1.6988,
      "step": 222112
    },
    {
      "epoch": 0.0008062046635084123,
      "grad_norm": 9653.490560413886,
      "learning_rate": 2.1227547490267972e-07,
      "loss": 1.7064,
      "step": 222144
    },
    {
      "epoch": 0.0008063207978592491,
      "grad_norm": 10967.959700874178,
      "learning_rate": 2.122601720470173e-07,
      "loss": 1.727,
      "step": 222176
    },
    {
      "epoch": 0.0008064369322100857,
      "grad_norm": 9601.751715181976,
      "learning_rate": 2.1224487250040834e-07,
      "loss": 1.7085,
      "step": 222208
    },
    {
      "epoch": 0.0008065530665609225,
      "grad_norm": 9997.327742952113,
      "learning_rate": 2.1222957626166045e-07,
      "loss": 1.7303,
      "step": 222240
    },
    {
      "epoch": 0.0008066692009117592,
      "grad_norm": 9425.83089175697,
      "learning_rate": 2.1221428332958186e-07,
      "loss": 1.7218,
      "step": 222272
    },
    {
      "epoch": 0.0008067853352625959,
      "grad_norm": 7819.677614838095,
      "learning_rate": 2.1219899370298137e-07,
      "loss": 1.7204,
      "step": 222304
    },
    {
      "epoch": 0.0008069014696134326,
      "grad_norm": 11455.12531577023,
      "learning_rate": 2.1218370738066834e-07,
      "loss": 1.7099,
      "step": 222336
    },
    {
      "epoch": 0.0008070176039642693,
      "grad_norm": 9165.277518984354,
      "learning_rate": 2.1216842436145286e-07,
      "loss": 1.7259,
      "step": 222368
    },
    {
      "epoch": 0.000807133738315106,
      "grad_norm": 8977.685670594621,
      "learning_rate": 2.1215314464414545e-07,
      "loss": 1.7263,
      "step": 222400
    },
    {
      "epoch": 0.0008072498726659427,
      "grad_norm": 11214.676277093335,
      "learning_rate": 2.1213786822755737e-07,
      "loss": 1.7085,
      "step": 222432
    },
    {
      "epoch": 0.0008073660070167794,
      "grad_norm": 10463.348030147903,
      "learning_rate": 2.1212259511050038e-07,
      "loss": 1.7187,
      "step": 222464
    },
    {
      "epoch": 0.0008074821413676161,
      "grad_norm": 8528.742814741221,
      "learning_rate": 2.1210732529178694e-07,
      "loss": 1.7249,
      "step": 222496
    },
    {
      "epoch": 0.0008075982757184528,
      "grad_norm": 10662.015756881998,
      "learning_rate": 2.1209205877023e-07,
      "loss": 1.6926,
      "step": 222528
    },
    {
      "epoch": 0.0008077144100692896,
      "grad_norm": 10751.792594725774,
      "learning_rate": 2.120767955446432e-07,
      "loss": 1.7107,
      "step": 222560
    },
    {
      "epoch": 0.0008078305444201262,
      "grad_norm": 9846.245985145812,
      "learning_rate": 2.1206153561384066e-07,
      "loss": 1.725,
      "step": 222592
    },
    {
      "epoch": 0.000807946678770963,
      "grad_norm": 9083.785774664659,
      "learning_rate": 2.1204627897663727e-07,
      "loss": 1.7042,
      "step": 222624
    },
    {
      "epoch": 0.0008080628131217996,
      "grad_norm": 10638.551217153585,
      "learning_rate": 2.1203102563184837e-07,
      "loss": 1.7353,
      "step": 222656
    },
    {
      "epoch": 0.0008081789474726364,
      "grad_norm": 9018.546667839559,
      "learning_rate": 2.1201577557828998e-07,
      "loss": 1.7319,
      "step": 222688
    },
    {
      "epoch": 0.000808295081823473,
      "grad_norm": 9842.438925388362,
      "learning_rate": 2.1200052881477862e-07,
      "loss": 1.736,
      "step": 222720
    },
    {
      "epoch": 0.0008084112161743098,
      "grad_norm": 10004.787553966351,
      "learning_rate": 2.119852853401315e-07,
      "loss": 1.7336,
      "step": 222752
    },
    {
      "epoch": 0.0008085273505251464,
      "grad_norm": 12047.453506861937,
      "learning_rate": 2.1197052135925613e-07,
      "loss": 1.747,
      "step": 222784
    },
    {
      "epoch": 0.0008086434848759832,
      "grad_norm": 10556.590358633794,
      "learning_rate": 2.119552843561062e-07,
      "loss": 1.7419,
      "step": 222816
    },
    {
      "epoch": 0.0008087596192268199,
      "grad_norm": 9046.93119239889,
      "learning_rate": 2.1194005063831251e-07,
      "loss": 1.7254,
      "step": 222848
    },
    {
      "epoch": 0.0008088757535776566,
      "grad_norm": 9257.890688488387,
      "learning_rate": 2.119248202046946e-07,
      "loss": 1.7368,
      "step": 222880
    },
    {
      "epoch": 0.0008089918879284933,
      "grad_norm": 8742.347853980646,
      "learning_rate": 2.1190959305407263e-07,
      "loss": 1.7357,
      "step": 222912
    },
    {
      "epoch": 0.00080910802227933,
      "grad_norm": 9848.255886196297,
      "learning_rate": 2.1189436918526733e-07,
      "loss": 1.7125,
      "step": 222944
    },
    {
      "epoch": 0.0008092241566301667,
      "grad_norm": 10041.448102738967,
      "learning_rate": 2.1187914859709996e-07,
      "loss": 1.7316,
      "step": 222976
    },
    {
      "epoch": 0.0008093402909810034,
      "grad_norm": 12761.34804791406,
      "learning_rate": 2.1186393128839247e-07,
      "loss": 1.719,
      "step": 223008
    },
    {
      "epoch": 0.0008094564253318401,
      "grad_norm": 9946.91228472434,
      "learning_rate": 2.118487172579674e-07,
      "loss": 1.7032,
      "step": 223040
    },
    {
      "epoch": 0.0008095725596826768,
      "grad_norm": 9179.009096847001,
      "learning_rate": 2.1183350650464778e-07,
      "loss": 1.7318,
      "step": 223072
    },
    {
      "epoch": 0.0008096886940335135,
      "grad_norm": 9021.244703476345,
      "learning_rate": 2.1181829902725733e-07,
      "loss": 1.7299,
      "step": 223104
    },
    {
      "epoch": 0.0008098048283843503,
      "grad_norm": 9516.435151883294,
      "learning_rate": 2.1180309482462033e-07,
      "loss": 1.7062,
      "step": 223136
    },
    {
      "epoch": 0.0008099209627351869,
      "grad_norm": 9925.426741455502,
      "learning_rate": 2.1178789389556163e-07,
      "loss": 1.7042,
      "step": 223168
    },
    {
      "epoch": 0.0008100370970860237,
      "grad_norm": 9034.862810247867,
      "learning_rate": 2.117726962389067e-07,
      "loss": 1.715,
      "step": 223200
    },
    {
      "epoch": 0.0008101532314368603,
      "grad_norm": 9649.61408554767,
      "learning_rate": 2.117575018534816e-07,
      "loss": 1.7023,
      "step": 223232
    },
    {
      "epoch": 0.0008102693657876971,
      "grad_norm": 10788.211158482207,
      "learning_rate": 2.1174231073811296e-07,
      "loss": 1.7113,
      "step": 223264
    },
    {
      "epoch": 0.0008103855001385337,
      "grad_norm": 9322.94352659073,
      "learning_rate": 2.1172712289162798e-07,
      "loss": 1.7183,
      "step": 223296
    },
    {
      "epoch": 0.0008105016344893705,
      "grad_norm": 10231.931684682027,
      "learning_rate": 2.1171193831285453e-07,
      "loss": 1.7192,
      "step": 223328
    },
    {
      "epoch": 0.0008106177688402071,
      "grad_norm": 9880.291696098855,
      "learning_rate": 2.11696757000621e-07,
      "loss": 1.7013,
      "step": 223360
    },
    {
      "epoch": 0.0008107339031910439,
      "grad_norm": 9765.825720337221,
      "learning_rate": 2.1168157895375633e-07,
      "loss": 1.7009,
      "step": 223392
    },
    {
      "epoch": 0.0008108500375418806,
      "grad_norm": 9515.187018656017,
      "learning_rate": 2.1166640417109017e-07,
      "loss": 1.7125,
      "step": 223424
    },
    {
      "epoch": 0.0008109661718927173,
      "grad_norm": 9299.133723094856,
      "learning_rate": 2.1165123265145263e-07,
      "loss": 1.7046,
      "step": 223456
    },
    {
      "epoch": 0.000811082306243554,
      "grad_norm": 10076.246622626899,
      "learning_rate": 2.116360643936745e-07,
      "loss": 1.7459,
      "step": 223488
    },
    {
      "epoch": 0.0008111984405943907,
      "grad_norm": 9122.918940777672,
      "learning_rate": 2.1162089939658712e-07,
      "loss": 1.7423,
      "step": 223520
    },
    {
      "epoch": 0.0008113145749452274,
      "grad_norm": 9294.42886895155,
      "learning_rate": 2.1160573765902239e-07,
      "loss": 1.7371,
      "step": 223552
    },
    {
      "epoch": 0.0008114307092960641,
      "grad_norm": 9719.421381954791,
      "learning_rate": 2.1159057917981285e-07,
      "loss": 1.7457,
      "step": 223584
    },
    {
      "epoch": 0.0008115468436469008,
      "grad_norm": 7571.719619742928,
      "learning_rate": 2.115754239577916e-07,
      "loss": 1.7433,
      "step": 223616
    },
    {
      "epoch": 0.0008116629779977375,
      "grad_norm": 8372.741844820011,
      "learning_rate": 2.115602719917923e-07,
      "loss": 1.7331,
      "step": 223648
    },
    {
      "epoch": 0.0008117791123485742,
      "grad_norm": 10600.829024184855,
      "learning_rate": 2.1154512328064923e-07,
      "loss": 1.7454,
      "step": 223680
    },
    {
      "epoch": 0.000811895246699411,
      "grad_norm": 10431.698231831671,
      "learning_rate": 2.1152997782319724e-07,
      "loss": 1.7429,
      "step": 223712
    },
    {
      "epoch": 0.0008120113810502476,
      "grad_norm": 8732.204647166716,
      "learning_rate": 2.1151483561827175e-07,
      "loss": 1.7194,
      "step": 223744
    },
    {
      "epoch": 0.0008121275154010844,
      "grad_norm": 10299.312501327455,
      "learning_rate": 2.1150016970780427e-07,
      "loss": 1.6953,
      "step": 223776
    },
    {
      "epoch": 0.000812243649751921,
      "grad_norm": 8400.308684804386,
      "learning_rate": 2.1148503390288938e-07,
      "loss": 1.7075,
      "step": 223808
    },
    {
      "epoch": 0.0008123597841027578,
      "grad_norm": 7668.224305535148,
      "learning_rate": 2.114699013470471e-07,
      "loss": 1.71,
      "step": 223840
    },
    {
      "epoch": 0.0008124759184535944,
      "grad_norm": 10286.59691054335,
      "learning_rate": 2.1145477203911522e-07,
      "loss": 1.6965,
      "step": 223872
    },
    {
      "epoch": 0.0008125920528044312,
      "grad_norm": 9368.94636552051,
      "learning_rate": 2.1143964597793207e-07,
      "loss": 1.715,
      "step": 223904
    },
    {
      "epoch": 0.0008127081871552678,
      "grad_norm": 9312.882045854549,
      "learning_rate": 2.114245231623366e-07,
      "loss": 1.7316,
      "step": 223936
    },
    {
      "epoch": 0.0008128243215061046,
      "grad_norm": 12621.083313250096,
      "learning_rate": 2.1140940359116824e-07,
      "loss": 1.7214,
      "step": 223968
    },
    {
      "epoch": 0.0008129404558569413,
      "grad_norm": 8367.30219365836,
      "learning_rate": 2.113942872632671e-07,
      "loss": 1.7094,
      "step": 224000
    },
    {
      "epoch": 0.000813056590207778,
      "grad_norm": 10024.828178078666,
      "learning_rate": 2.1137917417747384e-07,
      "loss": 1.7229,
      "step": 224032
    },
    {
      "epoch": 0.0008131727245586147,
      "grad_norm": 11292.574728555042,
      "learning_rate": 2.1136406433262966e-07,
      "loss": 1.7078,
      "step": 224064
    },
    {
      "epoch": 0.0008132888589094514,
      "grad_norm": 8597.214897860818,
      "learning_rate": 2.1134895772757642e-07,
      "loss": 1.7204,
      "step": 224096
    },
    {
      "epoch": 0.0008134049932602881,
      "grad_norm": 9774.570271884078,
      "learning_rate": 2.1133385436115646e-07,
      "loss": 1.7402,
      "step": 224128
    },
    {
      "epoch": 0.0008135211276111248,
      "grad_norm": 8280.241783909452,
      "learning_rate": 2.113187542322128e-07,
      "loss": 1.7382,
      "step": 224160
    },
    {
      "epoch": 0.0008136372619619615,
      "grad_norm": 10390.34494133857,
      "learning_rate": 2.11303657339589e-07,
      "loss": 1.704,
      "step": 224192
    },
    {
      "epoch": 0.0008137533963127982,
      "grad_norm": 8143.671039525111,
      "learning_rate": 2.1128856368212917e-07,
      "loss": 1.7247,
      "step": 224224
    },
    {
      "epoch": 0.0008138695306636349,
      "grad_norm": 11093.380729065419,
      "learning_rate": 2.11273473258678e-07,
      "loss": 1.7201,
      "step": 224256
    },
    {
      "epoch": 0.0008139856650144717,
      "grad_norm": 9421.930693865244,
      "learning_rate": 2.1125838606808077e-07,
      "loss": 1.7079,
      "step": 224288
    },
    {
      "epoch": 0.0008141017993653083,
      "grad_norm": 8658.311267216026,
      "learning_rate": 2.1124330210918339e-07,
      "loss": 1.744,
      "step": 224320
    },
    {
      "epoch": 0.0008142179337161451,
      "grad_norm": 9168.735572585785,
      "learning_rate": 2.1122822138083226e-07,
      "loss": 1.7474,
      "step": 224352
    },
    {
      "epoch": 0.0008143340680669817,
      "grad_norm": 9753.39243545547,
      "learning_rate": 2.1121314388187442e-07,
      "loss": 1.7216,
      "step": 224384
    },
    {
      "epoch": 0.0008144502024178185,
      "grad_norm": 8698.987527293048,
      "learning_rate": 2.1119806961115743e-07,
      "loss": 1.7307,
      "step": 224416
    },
    {
      "epoch": 0.0008145663367686551,
      "grad_norm": 9823.395441495777,
      "learning_rate": 2.111829985675295e-07,
      "loss": 1.7499,
      "step": 224448
    },
    {
      "epoch": 0.0008146824711194919,
      "grad_norm": 8929.79675020658,
      "learning_rate": 2.1116793074983937e-07,
      "loss": 1.73,
      "step": 224480
    },
    {
      "epoch": 0.0008147986054703285,
      "grad_norm": 10385.282663461789,
      "learning_rate": 2.111528661569363e-07,
      "loss": 1.7561,
      "step": 224512
    },
    {
      "epoch": 0.0008149147398211653,
      "grad_norm": 10451.202705909018,
      "learning_rate": 2.1113780478767026e-07,
      "loss": 1.7516,
      "step": 224544
    },
    {
      "epoch": 0.000815030874172002,
      "grad_norm": 10190.894563285403,
      "learning_rate": 2.1112274664089166e-07,
      "loss": 1.7518,
      "step": 224576
    },
    {
      "epoch": 0.0008151470085228387,
      "grad_norm": 9203.228129303327,
      "learning_rate": 2.1110769171545154e-07,
      "loss": 1.7178,
      "step": 224608
    },
    {
      "epoch": 0.0008152631428736754,
      "grad_norm": 10047.891520115054,
      "learning_rate": 2.1109264001020157e-07,
      "loss": 1.7238,
      "step": 224640
    },
    {
      "epoch": 0.0008153792772245121,
      "grad_norm": 10470.831294601208,
      "learning_rate": 2.1107759152399388e-07,
      "loss": 1.7295,
      "step": 224672
    },
    {
      "epoch": 0.0008154954115753488,
      "grad_norm": 9091.130512758025,
      "learning_rate": 2.1106254625568122e-07,
      "loss": 1.7373,
      "step": 224704
    },
    {
      "epoch": 0.0008156115459261855,
      "grad_norm": 8440.946392437285,
      "learning_rate": 2.11047504204117e-07,
      "loss": 1.7537,
      "step": 224736
    },
    {
      "epoch": 0.0008157276802770222,
      "grad_norm": 10633.337763844427,
      "learning_rate": 2.1103246536815503e-07,
      "loss": 1.7389,
      "step": 224768
    },
    {
      "epoch": 0.0008158438146278589,
      "grad_norm": 8987.425437799193,
      "learning_rate": 2.1101789956117695e-07,
      "loss": 1.7169,
      "step": 224800
    },
    {
      "epoch": 0.0008159599489786956,
      "grad_norm": 9023.708106981298,
      "learning_rate": 2.110028670525849e-07,
      "loss": 1.7337,
      "step": 224832
    },
    {
      "epoch": 0.0008160760833295324,
      "grad_norm": 13098.514419582092,
      "learning_rate": 2.1098783775619608e-07,
      "loss": 1.7209,
      "step": 224864
    },
    {
      "epoch": 0.000816192217680369,
      "grad_norm": 8828.491830431742,
      "learning_rate": 2.1097281167086655e-07,
      "loss": 1.6831,
      "step": 224896
    },
    {
      "epoch": 0.0008163083520312058,
      "grad_norm": 9505.509560249782,
      "learning_rate": 2.1095778879545317e-07,
      "loss": 1.7256,
      "step": 224928
    },
    {
      "epoch": 0.0008164244863820424,
      "grad_norm": 8611.236380450835,
      "learning_rate": 2.1094276912881316e-07,
      "loss": 1.7109,
      "step": 224960
    },
    {
      "epoch": 0.0008165406207328792,
      "grad_norm": 9535.466637768704,
      "learning_rate": 2.1092775266980445e-07,
      "loss": 1.7019,
      "step": 224992
    },
    {
      "epoch": 0.0008166567550837158,
      "grad_norm": 9812.742124401313,
      "learning_rate": 2.109127394172855e-07,
      "loss": 1.6888,
      "step": 225024
    },
    {
      "epoch": 0.0008167728894345526,
      "grad_norm": 7740.992572015555,
      "learning_rate": 2.108977293701153e-07,
      "loss": 1.7087,
      "step": 225056
    },
    {
      "epoch": 0.0008168890237853892,
      "grad_norm": 9386.412946381593,
      "learning_rate": 2.108827225271534e-07,
      "loss": 1.7071,
      "step": 225088
    },
    {
      "epoch": 0.000817005158136226,
      "grad_norm": 9496.013268735465,
      "learning_rate": 2.1086771888726e-07,
      "loss": 1.7135,
      "step": 225120
    },
    {
      "epoch": 0.0008171212924870628,
      "grad_norm": 9687.842071379982,
      "learning_rate": 2.1085271844929587e-07,
      "loss": 1.7161,
      "step": 225152
    },
    {
      "epoch": 0.0008172374268378994,
      "grad_norm": 11277.979783631465,
      "learning_rate": 2.1083772121212216e-07,
      "loss": 1.7192,
      "step": 225184
    },
    {
      "epoch": 0.0008173535611887362,
      "grad_norm": 11528.984170342155,
      "learning_rate": 2.1082272717460083e-07,
      "loss": 1.7023,
      "step": 225216
    },
    {
      "epoch": 0.0008174696955395728,
      "grad_norm": 8720.649975775888,
      "learning_rate": 2.108077363355943e-07,
      "loss": 1.7151,
      "step": 225248
    },
    {
      "epoch": 0.0008175858298904096,
      "grad_norm": 8638.311293302644,
      "learning_rate": 2.1079274869396553e-07,
      "loss": 1.7416,
      "step": 225280
    },
    {
      "epoch": 0.0008177019642412462,
      "grad_norm": 9154.502061827285,
      "learning_rate": 2.1077776424857805e-07,
      "loss": 1.7301,
      "step": 225312
    },
    {
      "epoch": 0.000817818098592083,
      "grad_norm": 9592.615284686444,
      "learning_rate": 2.1076278299829603e-07,
      "loss": 1.7599,
      "step": 225344
    },
    {
      "epoch": 0.0008179342329429196,
      "grad_norm": 8292.331155953674,
      "learning_rate": 2.1074780494198413e-07,
      "loss": 1.7428,
      "step": 225376
    },
    {
      "epoch": 0.0008180503672937564,
      "grad_norm": 9547.962924100617,
      "learning_rate": 2.107328300785076e-07,
      "loss": 1.7314,
      "step": 225408
    },
    {
      "epoch": 0.0008181665016445931,
      "grad_norm": 9366.340587443956,
      "learning_rate": 2.1071785840673226e-07,
      "loss": 1.7196,
      "step": 225440
    },
    {
      "epoch": 0.0008182826359954298,
      "grad_norm": 10209.019541562255,
      "learning_rate": 2.1070288992552447e-07,
      "loss": 1.7218,
      "step": 225472
    },
    {
      "epoch": 0.0008183987703462665,
      "grad_norm": 10124.152902835871,
      "learning_rate": 2.1068792463375117e-07,
      "loss": 1.7064,
      "step": 225504
    },
    {
      "epoch": 0.0008185149046971032,
      "grad_norm": 10543.254905388563,
      "learning_rate": 2.106729625302799e-07,
      "loss": 1.7007,
      "step": 225536
    },
    {
      "epoch": 0.0008186310390479399,
      "grad_norm": 10137.032701930088,
      "learning_rate": 2.106580036139787e-07,
      "loss": 1.7178,
      "step": 225568
    },
    {
      "epoch": 0.0008187471733987766,
      "grad_norm": 10077.390535252665,
      "learning_rate": 2.1064304788371617e-07,
      "loss": 1.7201,
      "step": 225600
    },
    {
      "epoch": 0.0008188633077496133,
      "grad_norm": 14318.722289366464,
      "learning_rate": 2.1062809533836158e-07,
      "loss": 1.6914,
      "step": 225632
    },
    {
      "epoch": 0.00081897944210045,
      "grad_norm": 9372.02134013789,
      "learning_rate": 2.106131459767846e-07,
      "loss": 1.7062,
      "step": 225664
    },
    {
      "epoch": 0.0008190955764512867,
      "grad_norm": 9747.660847608517,
      "learning_rate": 2.1059819979785556e-07,
      "loss": 1.7286,
      "step": 225696
    },
    {
      "epoch": 0.0008192117108021235,
      "grad_norm": 10250.389260901266,
      "learning_rate": 2.105832568004454e-07,
      "loss": 1.7071,
      "step": 225728
    },
    {
      "epoch": 0.0008193278451529601,
      "grad_norm": 9127.118712934547,
      "learning_rate": 2.1056831698342552e-07,
      "loss": 1.724,
      "step": 225760
    },
    {
      "epoch": 0.0008194439795037969,
      "grad_norm": 10469.684235926125,
      "learning_rate": 2.105538470674854e-07,
      "loss": 1.7202,
      "step": 225792
    },
    {
      "epoch": 0.0008195601138546335,
      "grad_norm": 10684.698217544565,
      "learning_rate": 2.1053891350856295e-07,
      "loss": 1.7089,
      "step": 225824
    },
    {
      "epoch": 0.0008196762482054703,
      "grad_norm": 11445.886422641106,
      "learning_rate": 2.1052398312668363e-07,
      "loss": 1.6989,
      "step": 225856
    },
    {
      "epoch": 0.0008197923825563069,
      "grad_norm": 11842.925652050679,
      "learning_rate": 2.1050905592072114e-07,
      "loss": 1.7284,
      "step": 225888
    },
    {
      "epoch": 0.0008199085169071437,
      "grad_norm": 10743.31475849051,
      "learning_rate": 2.1049413188954966e-07,
      "loss": 1.7242,
      "step": 225920
    },
    {
      "epoch": 0.0008200246512579803,
      "grad_norm": 9745.676785118621,
      "learning_rate": 2.10479211032044e-07,
      "loss": 1.715,
      "step": 225952
    },
    {
      "epoch": 0.0008201407856088171,
      "grad_norm": 10264.259544652989,
      "learning_rate": 2.1046429334707948e-07,
      "loss": 1.7182,
      "step": 225984
    },
    {
      "epoch": 0.0008202569199596538,
      "grad_norm": 11246.852715315516,
      "learning_rate": 2.1044937883353199e-07,
      "loss": 1.7135,
      "step": 226016
    },
    {
      "epoch": 0.0008203730543104905,
      "grad_norm": 9560.755200296679,
      "learning_rate": 2.1043446749027799e-07,
      "loss": 1.6866,
      "step": 226048
    },
    {
      "epoch": 0.0008204891886613272,
      "grad_norm": 9981.158850554379,
      "learning_rate": 2.1041955931619445e-07,
      "loss": 1.7103,
      "step": 226080
    },
    {
      "epoch": 0.0008206053230121639,
      "grad_norm": 10636.759468936016,
      "learning_rate": 2.1040465431015898e-07,
      "loss": 1.7241,
      "step": 226112
    },
    {
      "epoch": 0.0008207214573630006,
      "grad_norm": 9337.510589016754,
      "learning_rate": 2.1038975247104965e-07,
      "loss": 1.6976,
      "step": 226144
    },
    {
      "epoch": 0.0008208375917138373,
      "grad_norm": 8995.340126976856,
      "learning_rate": 2.1037485379774518e-07,
      "loss": 1.7297,
      "step": 226176
    },
    {
      "epoch": 0.000820953726064674,
      "grad_norm": 8279.934782351851,
      "learning_rate": 2.1035995828912482e-07,
      "loss": 1.7301,
      "step": 226208
    },
    {
      "epoch": 0.0008210698604155107,
      "grad_norm": 9253.540943876566,
      "learning_rate": 2.1034506594406827e-07,
      "loss": 1.722,
      "step": 226240
    },
    {
      "epoch": 0.0008211859947663474,
      "grad_norm": 7998.637133912252,
      "learning_rate": 2.10330176761456e-07,
      "loss": 1.7237,
      "step": 226272
    },
    {
      "epoch": 0.0008213021291171842,
      "grad_norm": 9641.552986941471,
      "learning_rate": 2.103152907401688e-07,
      "loss": 1.7401,
      "step": 226304
    },
    {
      "epoch": 0.0008214182634680208,
      "grad_norm": 9284.69891811253,
      "learning_rate": 2.1030040787908813e-07,
      "loss": 1.7293,
      "step": 226336
    },
    {
      "epoch": 0.0008215343978188576,
      "grad_norm": 9221.411714048994,
      "learning_rate": 2.1028552817709608e-07,
      "loss": 1.7141,
      "step": 226368
    },
    {
      "epoch": 0.0008216505321696942,
      "grad_norm": 8602.583797906302,
      "learning_rate": 2.102706516330751e-07,
      "loss": 1.718,
      "step": 226400
    },
    {
      "epoch": 0.000821766666520531,
      "grad_norm": 12515.139152242775,
      "learning_rate": 2.102557782459084e-07,
      "loss": 1.7236,
      "step": 226432
    },
    {
      "epoch": 0.0008218828008713676,
      "grad_norm": 10541.112654743805,
      "learning_rate": 2.1024090801447955e-07,
      "loss": 1.7087,
      "step": 226464
    },
    {
      "epoch": 0.0008219989352222044,
      "grad_norm": 10873.042444504666,
      "learning_rate": 2.1022604093767286e-07,
      "loss": 1.719,
      "step": 226496
    },
    {
      "epoch": 0.000822115069573041,
      "grad_norm": 9365.898675514272,
      "learning_rate": 2.1021117701437307e-07,
      "loss": 1.7165,
      "step": 226528
    },
    {
      "epoch": 0.0008222312039238778,
      "grad_norm": 9722.451954111164,
      "learning_rate": 2.1019631624346545e-07,
      "loss": 1.6981,
      "step": 226560
    },
    {
      "epoch": 0.0008223473382747145,
      "grad_norm": 8544.571844159309,
      "learning_rate": 2.1018145862383594e-07,
      "loss": 1.7317,
      "step": 226592
    },
    {
      "epoch": 0.0008224634726255512,
      "grad_norm": 10528.60351613641,
      "learning_rate": 2.1016660415437096e-07,
      "loss": 1.7266,
      "step": 226624
    },
    {
      "epoch": 0.0008225796069763879,
      "grad_norm": 10621.236086256627,
      "learning_rate": 2.1015175283395745e-07,
      "loss": 1.7018,
      "step": 226656
    },
    {
      "epoch": 0.0008226957413272246,
      "grad_norm": 9448.541051400476,
      "learning_rate": 2.1013690466148294e-07,
      "loss": 1.7074,
      "step": 226688
    },
    {
      "epoch": 0.0008228118756780613,
      "grad_norm": 10166.671825135303,
      "learning_rate": 2.1012205963583553e-07,
      "loss": 1.7209,
      "step": 226720
    },
    {
      "epoch": 0.000822928010028898,
      "grad_norm": 8942.290869793937,
      "learning_rate": 2.1010721775590386e-07,
      "loss": 1.697,
      "step": 226752
    },
    {
      "epoch": 0.0008230441443797347,
      "grad_norm": 9627.95845441805,
      "learning_rate": 2.1009284268346804e-07,
      "loss": 1.7127,
      "step": 226784
    },
    {
      "epoch": 0.0008231602787305714,
      "grad_norm": 8762.472824494236,
      "learning_rate": 2.1007800699341846e-07,
      "loss": 1.7184,
      "step": 226816
    },
    {
      "epoch": 0.0008232764130814081,
      "grad_norm": 10059.03335316073,
      "learning_rate": 2.1006317444578843e-07,
      "loss": 1.7245,
      "step": 226848
    },
    {
      "epoch": 0.0008233925474322449,
      "grad_norm": 9630.604134736304,
      "learning_rate": 2.1004834503946878e-07,
      "loss": 1.7018,
      "step": 226880
    },
    {
      "epoch": 0.0008235086817830815,
      "grad_norm": 8519.91197137623,
      "learning_rate": 2.1003351877335083e-07,
      "loss": 1.712,
      "step": 226912
    },
    {
      "epoch": 0.0008236248161339183,
      "grad_norm": 9365.60345092616,
      "learning_rate": 2.100186956463265e-07,
      "loss": 1.7192,
      "step": 226944
    },
    {
      "epoch": 0.0008237409504847549,
      "grad_norm": 9174.335289273005,
      "learning_rate": 2.1000387565728823e-07,
      "loss": 1.7057,
      "step": 226976
    },
    {
      "epoch": 0.0008238570848355917,
      "grad_norm": 9765.364509325804,
      "learning_rate": 2.0998905880512902e-07,
      "loss": 1.7346,
      "step": 227008
    },
    {
      "epoch": 0.0008239732191864283,
      "grad_norm": 9799.809181815735,
      "learning_rate": 2.0997424508874244e-07,
      "loss": 1.7362,
      "step": 227040
    },
    {
      "epoch": 0.0008240893535372651,
      "grad_norm": 10860.301469112172,
      "learning_rate": 2.099594345070225e-07,
      "loss": 1.7364,
      "step": 227072
    },
    {
      "epoch": 0.0008242054878881017,
      "grad_norm": 10809.863088864724,
      "learning_rate": 2.0994462705886393e-07,
      "loss": 1.742,
      "step": 227104
    },
    {
      "epoch": 0.0008243216222389385,
      "grad_norm": 10117.356374073219,
      "learning_rate": 2.0992982274316184e-07,
      "loss": 1.7401,
      "step": 227136
    },
    {
      "epoch": 0.0008244377565897752,
      "grad_norm": 8799.871476334185,
      "learning_rate": 2.09915021558812e-07,
      "loss": 1.7159,
      "step": 227168
    },
    {
      "epoch": 0.0008245538909406119,
      "grad_norm": 10245.785670215828,
      "learning_rate": 2.0990022350471065e-07,
      "loss": 1.7358,
      "step": 227200
    },
    {
      "epoch": 0.0008246700252914486,
      "grad_norm": 10508.828859582783,
      "learning_rate": 2.098854285797546e-07,
      "loss": 1.7244,
      "step": 227232
    },
    {
      "epoch": 0.0008247861596422853,
      "grad_norm": 11078.65298671278,
      "learning_rate": 2.0987063678284123e-07,
      "loss": 1.7194,
      "step": 227264
    },
    {
      "epoch": 0.000824902293993122,
      "grad_norm": 8924.53404946163,
      "learning_rate": 2.0985584811286844e-07,
      "loss": 1.6996,
      "step": 227296
    },
    {
      "epoch": 0.0008250184283439587,
      "grad_norm": 9771.04088621064,
      "learning_rate": 2.0984106256873465e-07,
      "loss": 1.7034,
      "step": 227328
    },
    {
      "epoch": 0.0008251345626947954,
      "grad_norm": 9148.102754123393,
      "learning_rate": 2.0982628014933887e-07,
      "loss": 1.7107,
      "step": 227360
    },
    {
      "epoch": 0.0008252506970456321,
      "grad_norm": 8608.084339735526,
      "learning_rate": 2.0981150085358066e-07,
      "loss": 1.6937,
      "step": 227392
    },
    {
      "epoch": 0.0008253668313964688,
      "grad_norm": 10475.96296289749,
      "learning_rate": 2.0979672468036e-07,
      "loss": 1.7207,
      "step": 227424
    },
    {
      "epoch": 0.0008254829657473056,
      "grad_norm": 8904.486509619743,
      "learning_rate": 2.0978195162857764e-07,
      "loss": 1.7287,
      "step": 227456
    },
    {
      "epoch": 0.0008255991000981422,
      "grad_norm": 9521.365868403545,
      "learning_rate": 2.097671816971346e-07,
      "loss": 1.7152,
      "step": 227488
    },
    {
      "epoch": 0.000825715234448979,
      "grad_norm": 11870.900555560223,
      "learning_rate": 2.097524148849327e-07,
      "loss": 1.6981,
      "step": 227520
    },
    {
      "epoch": 0.0008258313687998156,
      "grad_norm": 11065.550325221064,
      "learning_rate": 2.097376511908741e-07,
      "loss": 1.7165,
      "step": 227552
    },
    {
      "epoch": 0.0008259475031506524,
      "grad_norm": 8483.771095450418,
      "learning_rate": 2.0972289061386164e-07,
      "loss": 1.7081,
      "step": 227584
    },
    {
      "epoch": 0.000826063637501489,
      "grad_norm": 9280.666786389866,
      "learning_rate": 2.097081331527986e-07,
      "loss": 1.7162,
      "step": 227616
    },
    {
      "epoch": 0.0008261797718523258,
      "grad_norm": 8495.795783798007,
      "learning_rate": 2.0969337880658884e-07,
      "loss": 1.745,
      "step": 227648
    },
    {
      "epoch": 0.0008262959062031624,
      "grad_norm": 9825.621405285267,
      "learning_rate": 2.096786275741368e-07,
      "loss": 1.7354,
      "step": 227680
    },
    {
      "epoch": 0.0008264120405539992,
      "grad_norm": 9444.221301939086,
      "learning_rate": 2.096638794543474e-07,
      "loss": 1.7152,
      "step": 227712
    },
    {
      "epoch": 0.0008265281749048359,
      "grad_norm": 10850.551138075889,
      "learning_rate": 2.096491344461261e-07,
      "loss": 1.7207,
      "step": 227744
    },
    {
      "epoch": 0.0008266443092556726,
      "grad_norm": 10065.3915969524,
      "learning_rate": 2.0963439254837895e-07,
      "loss": 1.715,
      "step": 227776
    },
    {
      "epoch": 0.0008267604436065093,
      "grad_norm": 11095.100900848085,
      "learning_rate": 2.09620114300094e-07,
      "loss": 1.7113,
      "step": 227808
    },
    {
      "epoch": 0.000826876577957346,
      "grad_norm": 9553.17224800223,
      "learning_rate": 2.0960537852289787e-07,
      "loss": 1.7363,
      "step": 227840
    },
    {
      "epoch": 0.0008269927123081827,
      "grad_norm": 12205.152354641052,
      "learning_rate": 2.095906458529313e-07,
      "loss": 1.7294,
      "step": 227872
    },
    {
      "epoch": 0.0008271088466590194,
      "grad_norm": 8831.42027082847,
      "learning_rate": 2.0957591628910247e-07,
      "loss": 1.6982,
      "step": 227904
    },
    {
      "epoch": 0.0008272249810098561,
      "grad_norm": 9123.071631857332,
      "learning_rate": 2.0956118983031998e-07,
      "loss": 1.7146,
      "step": 227936
    },
    {
      "epoch": 0.0008273411153606928,
      "grad_norm": 8867.909336478357,
      "learning_rate": 2.0954646647549315e-07,
      "loss": 1.7352,
      "step": 227968
    },
    {
      "epoch": 0.0008274572497115295,
      "grad_norm": 11114.118408582843,
      "learning_rate": 2.0953174622353173e-07,
      "loss": 1.7147,
      "step": 228000
    },
    {
      "epoch": 0.0008275733840623663,
      "grad_norm": 8939.943288410726,
      "learning_rate": 2.09517029073346e-07,
      "loss": 1.7308,
      "step": 228032
    },
    {
      "epoch": 0.0008276895184132029,
      "grad_norm": 9033.430356182529,
      "learning_rate": 2.0950231502384678e-07,
      "loss": 1.74,
      "step": 228064
    },
    {
      "epoch": 0.0008278056527640397,
      "grad_norm": 8802.288793262807,
      "learning_rate": 2.0948760407394544e-07,
      "loss": 1.7288,
      "step": 228096
    },
    {
      "epoch": 0.0008279217871148763,
      "grad_norm": 9372.300678061923,
      "learning_rate": 2.0947289622255397e-07,
      "loss": 1.6964,
      "step": 228128
    },
    {
      "epoch": 0.0008280379214657131,
      "grad_norm": 11918.760170420412,
      "learning_rate": 2.094581914685847e-07,
      "loss": 1.711,
      "step": 228160
    },
    {
      "epoch": 0.0008281540558165497,
      "grad_norm": 9986.387535039885,
      "learning_rate": 2.0944348981095065e-07,
      "loss": 1.7154,
      "step": 228192
    },
    {
      "epoch": 0.0008282701901673865,
      "grad_norm": 9844.178584320785,
      "learning_rate": 2.0942879124856534e-07,
      "loss": 1.7215,
      "step": 228224
    },
    {
      "epoch": 0.0008283863245182231,
      "grad_norm": 8711.448674015133,
      "learning_rate": 2.0941409578034279e-07,
      "loss": 1.7325,
      "step": 228256
    },
    {
      "epoch": 0.0008285024588690599,
      "grad_norm": 8442.277536304999,
      "learning_rate": 2.0939940340519763e-07,
      "loss": 1.7395,
      "step": 228288
    },
    {
      "epoch": 0.0008286185932198966,
      "grad_norm": 10415.915322236448,
      "learning_rate": 2.093847141220449e-07,
      "loss": 1.6989,
      "step": 228320
    },
    {
      "epoch": 0.0008287347275707333,
      "grad_norm": 9060.533096898878,
      "learning_rate": 2.093700279298003e-07,
      "loss": 1.7133,
      "step": 228352
    },
    {
      "epoch": 0.00082885086192157,
      "grad_norm": 10002.23894935529,
      "learning_rate": 2.0935534482737994e-07,
      "loss": 1.7168,
      "step": 228384
    },
    {
      "epoch": 0.0008289669962724067,
      "grad_norm": 9421.713007728478,
      "learning_rate": 2.093406648137006e-07,
      "loss": 1.6978,
      "step": 228416
    },
    {
      "epoch": 0.0008290831306232434,
      "grad_norm": 9793.53705256686,
      "learning_rate": 2.0932598788767947e-07,
      "loss": 1.7258,
      "step": 228448
    },
    {
      "epoch": 0.0008291992649740801,
      "grad_norm": 9795.650871687905,
      "learning_rate": 2.093113140482343e-07,
      "loss": 1.7154,
      "step": 228480
    },
    {
      "epoch": 0.0008293153993249168,
      "grad_norm": 8824.218378984056,
      "learning_rate": 2.0929664329428347e-07,
      "loss": 1.7069,
      "step": 228512
    },
    {
      "epoch": 0.0008294315336757535,
      "grad_norm": 9846.47530845429,
      "learning_rate": 2.0928197562474572e-07,
      "loss": 1.7021,
      "step": 228544
    },
    {
      "epoch": 0.0008295476680265902,
      "grad_norm": 9496.90991849454,
      "learning_rate": 2.0926731103854048e-07,
      "loss": 1.719,
      "step": 228576
    },
    {
      "epoch": 0.000829663802377427,
      "grad_norm": 10504.124142449955,
      "learning_rate": 2.092526495345876e-07,
      "loss": 1.7163,
      "step": 228608
    },
    {
      "epoch": 0.0008297799367282636,
      "grad_norm": 10692.77419568935,
      "learning_rate": 2.0923799111180753e-07,
      "loss": 1.7216,
      "step": 228640
    },
    {
      "epoch": 0.0008298960710791004,
      "grad_norm": 9365.885863067091,
      "learning_rate": 2.092233357691212e-07,
      "loss": 1.7109,
      "step": 228672
    },
    {
      "epoch": 0.000830012205429937,
      "grad_norm": 8979.620259231457,
      "learning_rate": 2.092086835054501e-07,
      "loss": 1.7219,
      "step": 228704
    },
    {
      "epoch": 0.0008301283397807738,
      "grad_norm": 9358.331795784974,
      "learning_rate": 2.0919403431971622e-07,
      "loss": 1.7049,
      "step": 228736
    },
    {
      "epoch": 0.0008302444741316104,
      "grad_norm": 10115.41753957789,
      "learning_rate": 2.0917938821084212e-07,
      "loss": 1.7203,
      "step": 228768
    },
    {
      "epoch": 0.0008303606084824472,
      "grad_norm": 8444.54640581719,
      "learning_rate": 2.0916520272598836e-07,
      "loss": 1.7326,
      "step": 228800
    },
    {
      "epoch": 0.0008304767428332838,
      "grad_norm": 9665.058716841817,
      "learning_rate": 2.0915056267153523e-07,
      "loss": 1.7282,
      "step": 228832
    },
    {
      "epoch": 0.0008305928771841206,
      "grad_norm": 9879.020194331015,
      "learning_rate": 2.0913592569074626e-07,
      "loss": 1.7575,
      "step": 228864
    },
    {
      "epoch": 0.0008307090115349573,
      "grad_norm": 8388.157366191934,
      "learning_rate": 2.0912129178254606e-07,
      "loss": 1.7499,
      "step": 228896
    },
    {
      "epoch": 0.000830825145885794,
      "grad_norm": 11036.052917596942,
      "learning_rate": 2.091066609458598e-07,
      "loss": 1.7331,
      "step": 228928
    },
    {
      "epoch": 0.0008309412802366307,
      "grad_norm": 9740.417444853172,
      "learning_rate": 2.0909203317961317e-07,
      "loss": 1.7243,
      "step": 228960
    },
    {
      "epoch": 0.0008310574145874674,
      "grad_norm": 11218.535020224343,
      "learning_rate": 2.090774084827324e-07,
      "loss": 1.7166,
      "step": 228992
    },
    {
      "epoch": 0.0008311735489383041,
      "grad_norm": 8953.590564684093,
      "learning_rate": 2.0906278685414417e-07,
      "loss": 1.7008,
      "step": 229024
    },
    {
      "epoch": 0.0008312896832891408,
      "grad_norm": 8467.298034201938,
      "learning_rate": 2.0904816829277578e-07,
      "loss": 1.7155,
      "step": 229056
    },
    {
      "epoch": 0.0008314058176399775,
      "grad_norm": 11645.15830721077,
      "learning_rate": 2.09033552797555e-07,
      "loss": 1.7187,
      "step": 229088
    },
    {
      "epoch": 0.0008315219519908142,
      "grad_norm": 10683.995132907914,
      "learning_rate": 2.0901894036741012e-07,
      "loss": 1.7161,
      "step": 229120
    },
    {
      "epoch": 0.000831638086341651,
      "grad_norm": 10101.12171988834,
      "learning_rate": 2.0900433100127e-07,
      "loss": 1.6939,
      "step": 229152
    },
    {
      "epoch": 0.0008317542206924877,
      "grad_norm": 9341.751655872682,
      "learning_rate": 2.0898972469806402e-07,
      "loss": 1.7172,
      "step": 229184
    },
    {
      "epoch": 0.0008318703550433243,
      "grad_norm": 10979.399437127697,
      "learning_rate": 2.08975121456722e-07,
      "loss": 1.7336,
      "step": 229216
    },
    {
      "epoch": 0.0008319864893941611,
      "grad_norm": 8350.583213165413,
      "learning_rate": 2.089605212761744e-07,
      "loss": 1.7185,
      "step": 229248
    },
    {
      "epoch": 0.0008321026237449977,
      "grad_norm": 9535.835569052142,
      "learning_rate": 2.0894592415535213e-07,
      "loss": 1.7301,
      "step": 229280
    },
    {
      "epoch": 0.0008322187580958345,
      "grad_norm": 8195.869569484375,
      "learning_rate": 2.0893133009318663e-07,
      "loss": 1.7285,
      "step": 229312
    },
    {
      "epoch": 0.0008323348924466711,
      "grad_norm": 9245.321411395063,
      "learning_rate": 2.0891673908860986e-07,
      "loss": 1.7237,
      "step": 229344
    },
    {
      "epoch": 0.0008324510267975079,
      "grad_norm": 9336.755432161646,
      "learning_rate": 2.0890215114055434e-07,
      "loss": 1.7176,
      "step": 229376
    },
    {
      "epoch": 0.0008325671611483445,
      "grad_norm": 7664.77018572638,
      "learning_rate": 2.088875662479531e-07,
      "loss": 1.7462,
      "step": 229408
    },
    {
      "epoch": 0.0008326832954991813,
      "grad_norm": 10316.614464057478,
      "learning_rate": 2.0887298440973966e-07,
      "loss": 1.7315,
      "step": 229440
    },
    {
      "epoch": 0.000832799429850018,
      "grad_norm": 9566.449184519824,
      "learning_rate": 2.0885840562484804e-07,
      "loss": 1.7367,
      "step": 229472
    },
    {
      "epoch": 0.0008329155642008547,
      "grad_norm": 8965.160567441055,
      "learning_rate": 2.0884382989221287e-07,
      "loss": 1.7311,
      "step": 229504
    },
    {
      "epoch": 0.0008330316985516915,
      "grad_norm": 11395.36554920464,
      "learning_rate": 2.0882925721076925e-07,
      "loss": 1.7308,
      "step": 229536
    },
    {
      "epoch": 0.0008331478329025281,
      "grad_norm": 11586.101328747302,
      "learning_rate": 2.0881468757945276e-07,
      "loss": 1.7102,
      "step": 229568
    },
    {
      "epoch": 0.0008332639672533649,
      "grad_norm": 8832.719513264305,
      "learning_rate": 2.0880012099719958e-07,
      "loss": 1.723,
      "step": 229600
    },
    {
      "epoch": 0.0008333801016042015,
      "grad_norm": 10458.381136676939,
      "learning_rate": 2.0878555746294634e-07,
      "loss": 1.7405,
      "step": 229632
    },
    {
      "epoch": 0.0008334962359550383,
      "grad_norm": 8854.761205137042,
      "learning_rate": 2.0877099697563028e-07,
      "loss": 1.7188,
      "step": 229664
    },
    {
      "epoch": 0.0008336123703058749,
      "grad_norm": 10324.911040778996,
      "learning_rate": 2.08756439534189e-07,
      "loss": 1.7435,
      "step": 229696
    },
    {
      "epoch": 0.0008337285046567117,
      "grad_norm": 9193.813028335959,
      "learning_rate": 2.0874188513756078e-07,
      "loss": 1.7509,
      "step": 229728
    },
    {
      "epoch": 0.0008338446390075484,
      "grad_norm": 8524.783750922952,
      "learning_rate": 2.0872733378468434e-07,
      "loss": 1.7434,
      "step": 229760
    },
    {
      "epoch": 0.000833960773358385,
      "grad_norm": 8583.447675613805,
      "learning_rate": 2.0871324006314643e-07,
      "loss": 1.7167,
      "step": 229792
    },
    {
      "epoch": 0.0008340769077092218,
      "grad_norm": 10001.07094265409,
      "learning_rate": 2.086986946995569e-07,
      "loss": 1.7301,
      "step": 229824
    },
    {
      "epoch": 0.0008341930420600585,
      "grad_norm": 10751.9551710375,
      "learning_rate": 2.0868415237657157e-07,
      "loss": 1.71,
      "step": 229856
    },
    {
      "epoch": 0.0008343091764108952,
      "grad_norm": 8591.45785067936,
      "learning_rate": 2.086696130931312e-07,
      "loss": 1.7076,
      "step": 229888
    },
    {
      "epoch": 0.0008344253107617319,
      "grad_norm": 10464.921595501803,
      "learning_rate": 2.0865507684817713e-07,
      "loss": 1.7098,
      "step": 229920
    },
    {
      "epoch": 0.0008345414451125686,
      "grad_norm": 10504.972536851299,
      "learning_rate": 2.0864054364065115e-07,
      "loss": 1.7167,
      "step": 229952
    },
    {
      "epoch": 0.0008346575794634053,
      "grad_norm": 11098.397001369161,
      "learning_rate": 2.0862601346949564e-07,
      "loss": 1.7143,
      "step": 229984
    },
    {
      "epoch": 0.000834773713814242,
      "grad_norm": 8613.466201245583,
      "learning_rate": 2.0861148633365338e-07,
      "loss": 1.7134,
      "step": 230016
    },
    {
      "epoch": 0.0008348898481650788,
      "grad_norm": 8915.083286206585,
      "learning_rate": 2.085969622320678e-07,
      "loss": 1.7255,
      "step": 230048
    },
    {
      "epoch": 0.0008350059825159154,
      "grad_norm": 9432.626569519223,
      "learning_rate": 2.0858244116368276e-07,
      "loss": 1.7069,
      "step": 230080
    },
    {
      "epoch": 0.0008351221168667522,
      "grad_norm": 7002.221790260574,
      "learning_rate": 2.0856792312744262e-07,
      "loss": 1.7246,
      "step": 230112
    },
    {
      "epoch": 0.0008352382512175888,
      "grad_norm": 8202.313941809347,
      "learning_rate": 2.0855340812229237e-07,
      "loss": 1.7153,
      "step": 230144
    },
    {
      "epoch": 0.0008353543855684256,
      "grad_norm": 9285.658619613367,
      "learning_rate": 2.0853889614717733e-07,
      "loss": 1.6948,
      "step": 230176
    },
    {
      "epoch": 0.0008354705199192622,
      "grad_norm": 9907.646743803494,
      "learning_rate": 2.0852438720104353e-07,
      "loss": 1.7117,
      "step": 230208
    },
    {
      "epoch": 0.000835586654270099,
      "grad_norm": 9110.094291498854,
      "learning_rate": 2.0850988128283737e-07,
      "loss": 1.724,
      "step": 230240
    },
    {
      "epoch": 0.0008357027886209356,
      "grad_norm": 9979.182130816132,
      "learning_rate": 2.0849537839150584e-07,
      "loss": 1.6904,
      "step": 230272
    },
    {
      "epoch": 0.0008358189229717724,
      "grad_norm": 8702.66625810734,
      "learning_rate": 2.0848087852599638e-07,
      "loss": 1.7163,
      "step": 230304
    },
    {
      "epoch": 0.0008359350573226091,
      "grad_norm": 10847.348247382859,
      "learning_rate": 2.08466381685257e-07,
      "loss": 1.711,
      "step": 230336
    },
    {
      "epoch": 0.0008360511916734458,
      "grad_norm": 8840.677915182749,
      "learning_rate": 2.084518878682362e-07,
      "loss": 1.7113,
      "step": 230368
    },
    {
      "epoch": 0.0008361673260242825,
      "grad_norm": 10066.785385613422,
      "learning_rate": 2.0843739707388302e-07,
      "loss": 1.6997,
      "step": 230400
    },
    {
      "epoch": 0.0008362834603751192,
      "grad_norm": 9389.415104254365,
      "learning_rate": 2.0842290930114692e-07,
      "loss": 1.7002,
      "step": 230432
    },
    {
      "epoch": 0.0008363995947259559,
      "grad_norm": 9913.367339103297,
      "learning_rate": 2.0840842454897799e-07,
      "loss": 1.7054,
      "step": 230464
    },
    {
      "epoch": 0.0008365157290767926,
      "grad_norm": 10636.79707430766,
      "learning_rate": 2.0839394281632677e-07,
      "loss": 1.7015,
      "step": 230496
    },
    {
      "epoch": 0.0008366318634276293,
      "grad_norm": 9049.513357081694,
      "learning_rate": 2.0837946410214426e-07,
      "loss": 1.7292,
      "step": 230528
    },
    {
      "epoch": 0.000836747997778466,
      "grad_norm": 9545.265213706742,
      "learning_rate": 2.083649884053821e-07,
      "loss": 1.7386,
      "step": 230560
    },
    {
      "epoch": 0.0008368641321293027,
      "grad_norm": 8717.076918325316,
      "learning_rate": 2.083505157249923e-07,
      "loss": 1.7324,
      "step": 230592
    },
    {
      "epoch": 0.0008369802664801395,
      "grad_norm": 9964.513435185885,
      "learning_rate": 2.083360460599275e-07,
      "loss": 1.7447,
      "step": 230624
    },
    {
      "epoch": 0.0008370964008309761,
      "grad_norm": 10150.446098571234,
      "learning_rate": 2.083215794091408e-07,
      "loss": 1.7448,
      "step": 230656
    },
    {
      "epoch": 0.0008372125351818129,
      "grad_norm": 8754.01050947507,
      "learning_rate": 2.0830711577158572e-07,
      "loss": 1.715,
      "step": 230688
    },
    {
      "epoch": 0.0008373286695326495,
      "grad_norm": 9876.813656235498,
      "learning_rate": 2.0829265514621646e-07,
      "loss": 1.7338,
      "step": 230720
    },
    {
      "epoch": 0.0008374448038834863,
      "grad_norm": 7718.679161618263,
      "learning_rate": 2.0827819753198757e-07,
      "loss": 1.7135,
      "step": 230752
    },
    {
      "epoch": 0.0008375609382343229,
      "grad_norm": 9036.15891847858,
      "learning_rate": 2.0826374292785425e-07,
      "loss": 1.7105,
      "step": 230784
    },
    {
      "epoch": 0.0008376770725851597,
      "grad_norm": 10497.77576441791,
      "learning_rate": 2.082497428995816e-07,
      "loss": 1.699,
      "step": 230816
    },
    {
      "epoch": 0.0008377932069359963,
      "grad_norm": 9908.92769173335,
      "learning_rate": 2.082352942185223e-07,
      "loss": 1.7038,
      "step": 230848
    },
    {
      "epoch": 0.0008379093412868331,
      "grad_norm": 9347.701963584419,
      "learning_rate": 2.0822084854445954e-07,
      "loss": 1.7057,
      "step": 230880
    },
    {
      "epoch": 0.0008380254756376698,
      "grad_norm": 10553.03956213564,
      "learning_rate": 2.082064058763505e-07,
      "loss": 1.6936,
      "step": 230912
    },
    {
      "epoch": 0.0008381416099885065,
      "grad_norm": 9707.931602560866,
      "learning_rate": 2.081919662131528e-07,
      "loss": 1.7155,
      "step": 230944
    },
    {
      "epoch": 0.0008382577443393432,
      "grad_norm": 9265.25941352966,
      "learning_rate": 2.081775295538246e-07,
      "loss": 1.7303,
      "step": 230976
    },
    {
      "epoch": 0.0008383738786901799,
      "grad_norm": 9142.607068008556,
      "learning_rate": 2.081630958973245e-07,
      "loss": 1.7053,
      "step": 231008
    },
    {
      "epoch": 0.0008384900130410166,
      "grad_norm": 10273.349210457123,
      "learning_rate": 2.0814866524261175e-07,
      "loss": 1.7002,
      "step": 231040
    },
    {
      "epoch": 0.0008386061473918533,
      "grad_norm": 10076.20285623508,
      "learning_rate": 2.0813423758864596e-07,
      "loss": 1.7107,
      "step": 231072
    },
    {
      "epoch": 0.00083872228174269,
      "grad_norm": 8463.382657070399,
      "learning_rate": 2.0811981293438737e-07,
      "loss": 1.6973,
      "step": 231104
    },
    {
      "epoch": 0.0008388384160935267,
      "grad_norm": 9657.234179618925,
      "learning_rate": 2.0810539127879657e-07,
      "loss": 1.7147,
      "step": 231136
    },
    {
      "epoch": 0.0008389545504443634,
      "grad_norm": 10045.770652369085,
      "learning_rate": 2.0809097262083477e-07,
      "loss": 1.728,
      "step": 231168
    },
    {
      "epoch": 0.0008390706847952002,
      "grad_norm": 8724.638101377042,
      "learning_rate": 2.080765569594637e-07,
      "loss": 1.7274,
      "step": 231200
    },
    {
      "epoch": 0.0008391868191460368,
      "grad_norm": 10568.031226297546,
      "learning_rate": 2.0806214429364548e-07,
      "loss": 1.7135,
      "step": 231232
    },
    {
      "epoch": 0.0008393029534968736,
      "grad_norm": 10877.789849045623,
      "learning_rate": 2.0804773462234285e-07,
      "loss": 1.715,
      "step": 231264
    },
    {
      "epoch": 0.0008394190878477102,
      "grad_norm": 10006.693359946632,
      "learning_rate": 2.08033327944519e-07,
      "loss": 1.7062,
      "step": 231296
    },
    {
      "epoch": 0.000839535222198547,
      "grad_norm": 9028.989755227325,
      "learning_rate": 2.080189242591376e-07,
      "loss": 1.698,
      "step": 231328
    },
    {
      "epoch": 0.0008396513565493836,
      "grad_norm": 8414.03315895534,
      "learning_rate": 2.0800452356516286e-07,
      "loss": 1.719,
      "step": 231360
    },
    {
      "epoch": 0.0008397674909002204,
      "grad_norm": 9816.253256716638,
      "learning_rate": 2.079901258615595e-07,
      "loss": 1.7315,
      "step": 231392
    },
    {
      "epoch": 0.000839883625251057,
      "grad_norm": 9460.061416291122,
      "learning_rate": 2.079757311472927e-07,
      "loss": 1.6967,
      "step": 231424
    },
    {
      "epoch": 0.0008399997596018938,
      "grad_norm": 11224.902672183845,
      "learning_rate": 2.0796133942132814e-07,
      "loss": 1.7124,
      "step": 231456
    },
    {
      "epoch": 0.0008401158939527305,
      "grad_norm": 10777.437172166674,
      "learning_rate": 2.0794695068263212e-07,
      "loss": 1.731,
      "step": 231488
    },
    {
      "epoch": 0.0008402320283035672,
      "grad_norm": 9531.749262333751,
      "learning_rate": 2.0793256493017124e-07,
      "loss": 1.7151,
      "step": 231520
    },
    {
      "epoch": 0.0008403481626544039,
      "grad_norm": 9245.057057693046,
      "learning_rate": 2.0791818216291275e-07,
      "loss": 1.7384,
      "step": 231552
    },
    {
      "epoch": 0.0008404642970052406,
      "grad_norm": 8944.75198091037,
      "learning_rate": 2.0790380237982435e-07,
      "loss": 1.7424,
      "step": 231584
    },
    {
      "epoch": 0.0008405804313560773,
      "grad_norm": 8737.578611949652,
      "learning_rate": 2.0788942557987425e-07,
      "loss": 1.7183,
      "step": 231616
    },
    {
      "epoch": 0.000840696565706914,
      "grad_norm": 10196.872461691379,
      "learning_rate": 2.0787505176203118e-07,
      "loss": 1.6998,
      "step": 231648
    },
    {
      "epoch": 0.0008408127000577507,
      "grad_norm": 9325.988419465253,
      "learning_rate": 2.0786068092526426e-07,
      "loss": 1.7129,
      "step": 231680
    },
    {
      "epoch": 0.0008409288344085874,
      "grad_norm": 9351.161959884985,
      "learning_rate": 2.078463130685433e-07,
      "loss": 1.7158,
      "step": 231712
    },
    {
      "epoch": 0.0008410449687594241,
      "grad_norm": 9502.498197842502,
      "learning_rate": 2.078319481908384e-07,
      "loss": 1.7213,
      "step": 231744
    },
    {
      "epoch": 0.0008411611031102608,
      "grad_norm": 9239.510809561294,
      "learning_rate": 2.0781758629112035e-07,
      "loss": 1.7415,
      "step": 231776
    },
    {
      "epoch": 0.0008412772374610975,
      "grad_norm": 22304.423955798546,
      "learning_rate": 2.0780322736836026e-07,
      "loss": 1.737,
      "step": 231808
    },
    {
      "epoch": 0.0008413933718119343,
      "grad_norm": 11299.486891005272,
      "learning_rate": 2.0778931999983274e-07,
      "loss": 1.711,
      "step": 231840
    },
    {
      "epoch": 0.0008415095061627709,
      "grad_norm": 11077.899439875775,
      "learning_rate": 2.0777496693495412e-07,
      "loss": 1.7237,
      "step": 231872
    },
    {
      "epoch": 0.0008416256405136077,
      "grad_norm": 9278.911574101781,
      "learning_rate": 2.0776061684398219e-07,
      "loss": 1.7151,
      "step": 231904
    },
    {
      "epoch": 0.0008417417748644443,
      "grad_norm": 9034.17511453038,
      "learning_rate": 2.077462697258901e-07,
      "loss": 1.6874,
      "step": 231936
    },
    {
      "epoch": 0.0008418579092152811,
      "grad_norm": 9916.287208426347,
      "learning_rate": 2.0773192557965148e-07,
      "loss": 1.7223,
      "step": 231968
    },
    {
      "epoch": 0.0008419740435661177,
      "grad_norm": 9425.350073074209,
      "learning_rate": 2.0771758440424055e-07,
      "loss": 1.723,
      "step": 232000
    },
    {
      "epoch": 0.0008420901779169545,
      "grad_norm": 8031.241373536223,
      "learning_rate": 2.0770324619863196e-07,
      "loss": 1.7055,
      "step": 232032
    },
    {
      "epoch": 0.0008422063122677911,
      "grad_norm": 7763.060736590948,
      "learning_rate": 2.0768891096180087e-07,
      "loss": 1.6916,
      "step": 232064
    },
    {
      "epoch": 0.0008423224466186279,
      "grad_norm": 9159.085980598718,
      "learning_rate": 2.0767457869272288e-07,
      "loss": 1.7056,
      "step": 232096
    },
    {
      "epoch": 0.0008424385809694646,
      "grad_norm": 10348.366247867341,
      "learning_rate": 2.0766024939037416e-07,
      "loss": 1.7068,
      "step": 232128
    },
    {
      "epoch": 0.0008425547153203013,
      "grad_norm": 11376.353018432577,
      "learning_rate": 2.0764592305373137e-07,
      "loss": 1.7068,
      "step": 232160
    },
    {
      "epoch": 0.000842670849671138,
      "grad_norm": 11111.44653049278,
      "learning_rate": 2.076315996817716e-07,
      "loss": 1.7195,
      "step": 232192
    },
    {
      "epoch": 0.0008427869840219747,
      "grad_norm": 10353.765788349667,
      "learning_rate": 2.076172792734725e-07,
      "loss": 1.72,
      "step": 232224
    },
    {
      "epoch": 0.0008429031183728114,
      "grad_norm": 9712.110378285453,
      "learning_rate": 2.0760296182781218e-07,
      "loss": 1.7006,
      "step": 232256
    },
    {
      "epoch": 0.0008430192527236481,
      "grad_norm": 10185.30706459064,
      "learning_rate": 2.0758864734376923e-07,
      "loss": 1.7222,
      "step": 232288
    },
    {
      "epoch": 0.0008431353870744848,
      "grad_norm": 10853.366850890096,
      "learning_rate": 2.075743358203228e-07,
      "loss": 1.7345,
      "step": 232320
    },
    {
      "epoch": 0.0008432515214253215,
      "grad_norm": 10374.124734164323,
      "learning_rate": 2.0756002725645247e-07,
      "loss": 1.7263,
      "step": 232352
    },
    {
      "epoch": 0.0008433676557761582,
      "grad_norm": 9885.108800615197,
      "learning_rate": 2.0754572165113833e-07,
      "loss": 1.7581,
      "step": 232384
    },
    {
      "epoch": 0.000843483790126995,
      "grad_norm": 8949.373609365071,
      "learning_rate": 2.0753141900336094e-07,
      "loss": 1.7439,
      "step": 232416
    },
    {
      "epoch": 0.0008435999244778316,
      "grad_norm": 9768.223789410233,
      "learning_rate": 2.075171193121014e-07,
      "loss": 1.7232,
      "step": 232448
    },
    {
      "epoch": 0.0008437160588286684,
      "grad_norm": 9583.093863674716,
      "learning_rate": 2.075028225763413e-07,
      "loss": 1.7181,
      "step": 232480
    },
    {
      "epoch": 0.000843832193179505,
      "grad_norm": 9808.121328776475,
      "learning_rate": 2.0748852879506263e-07,
      "loss": 1.7184,
      "step": 232512
    },
    {
      "epoch": 0.0008439483275303418,
      "grad_norm": 7357.309834443565,
      "learning_rate": 2.0747423796724795e-07,
      "loss": 1.7031,
      "step": 232544
    },
    {
      "epoch": 0.0008440644618811784,
      "grad_norm": 9927.212096051942,
      "learning_rate": 2.0745995009188032e-07,
      "loss": 1.7214,
      "step": 232576
    },
    {
      "epoch": 0.0008441805962320152,
      "grad_norm": 9870.286318035562,
      "learning_rate": 2.0744566516794328e-07,
      "loss": 1.7181,
      "step": 232608
    },
    {
      "epoch": 0.0008442967305828518,
      "grad_norm": 10135.307494101991,
      "learning_rate": 2.074313831944208e-07,
      "loss": 1.7205,
      "step": 232640
    },
    {
      "epoch": 0.0008444128649336886,
      "grad_norm": 9411.350806340182,
      "learning_rate": 2.0741710417029742e-07,
      "loss": 1.7007,
      "step": 232672
    },
    {
      "epoch": 0.0008445289992845253,
      "grad_norm": 9947.381967130848,
      "learning_rate": 2.0740282809455816e-07,
      "loss": 1.7179,
      "step": 232704
    },
    {
      "epoch": 0.000844645133635362,
      "grad_norm": 9970.642506879885,
      "learning_rate": 2.0738855496618846e-07,
      "loss": 1.7325,
      "step": 232736
    },
    {
      "epoch": 0.0008447612679861987,
      "grad_norm": 8766.792686039746,
      "learning_rate": 2.073742847841743e-07,
      "loss": 1.7216,
      "step": 232768
    },
    {
      "epoch": 0.0008448774023370354,
      "grad_norm": 8125.179013412566,
      "learning_rate": 2.0736001754750217e-07,
      "loss": 1.7175,
      "step": 232800
    },
    {
      "epoch": 0.0008449935366878721,
      "grad_norm": 9694.64677025419,
      "learning_rate": 2.0734619896973728e-07,
      "loss": 1.7103,
      "step": 232832
    },
    {
      "epoch": 0.0008451096710387088,
      "grad_norm": 9874.948708727556,
      "learning_rate": 2.073319375287472e-07,
      "loss": 1.7059,
      "step": 232864
    },
    {
      "epoch": 0.0008452258053895455,
      "grad_norm": 9151.928758463977,
      "learning_rate": 2.0731767903009305e-07,
      "loss": 1.7026,
      "step": 232896
    },
    {
      "epoch": 0.0008453419397403822,
      "grad_norm": 9246.284442953289,
      "learning_rate": 2.0730342347276322e-07,
      "loss": 1.7351,
      "step": 232928
    },
    {
      "epoch": 0.0008454580740912189,
      "grad_norm": 8955.669377550737,
      "learning_rate": 2.0728917085574665e-07,
      "loss": 1.7129,
      "step": 232960
    },
    {
      "epoch": 0.0008455742084420557,
      "grad_norm": 9599.882499280915,
      "learning_rate": 2.0727492117803265e-07,
      "loss": 1.7214,
      "step": 232992
    },
    {
      "epoch": 0.0008456903427928923,
      "grad_norm": 8929.872339513035,
      "learning_rate": 2.0726067443861108e-07,
      "loss": 1.7158,
      "step": 233024
    },
    {
      "epoch": 0.0008458064771437291,
      "grad_norm": 8147.579640604932,
      "learning_rate": 2.0724643063647234e-07,
      "loss": 1.7058,
      "step": 233056
    },
    {
      "epoch": 0.0008459226114945657,
      "grad_norm": 7210.629792188752,
      "learning_rate": 2.072321897706072e-07,
      "loss": 1.6964,
      "step": 233088
    },
    {
      "epoch": 0.0008460387458454025,
      "grad_norm": 10491.933282288826,
      "learning_rate": 2.07217951840007e-07,
      "loss": 1.7117,
      "step": 233120
    },
    {
      "epoch": 0.0008461548801962391,
      "grad_norm": 9039.624328477374,
      "learning_rate": 2.0720371684366357e-07,
      "loss": 1.7249,
      "step": 233152
    },
    {
      "epoch": 0.0008462710145470759,
      "grad_norm": 13214.718612214185,
      "learning_rate": 2.0718948478056913e-07,
      "loss": 1.709,
      "step": 233184
    },
    {
      "epoch": 0.0008463871488979125,
      "grad_norm": 9308.047056176714,
      "learning_rate": 2.0717525564971646e-07,
      "loss": 1.7284,
      "step": 233216
    },
    {
      "epoch": 0.0008465032832487493,
      "grad_norm": 8578.548711757718,
      "learning_rate": 2.0716102945009884e-07,
      "loss": 1.7332,
      "step": 233248
    },
    {
      "epoch": 0.000846619417599586,
      "grad_norm": 8978.628514422457,
      "learning_rate": 2.0714680618070999e-07,
      "loss": 1.7187,
      "step": 233280
    },
    {
      "epoch": 0.0008467355519504227,
      "grad_norm": 9426.180668754447,
      "learning_rate": 2.0713258584054413e-07,
      "loss": 1.7164,
      "step": 233312
    },
    {
      "epoch": 0.0008468516863012594,
      "grad_norm": 9328.75704475146,
      "learning_rate": 2.0711836842859597e-07,
      "loss": 1.7421,
      "step": 233344
    },
    {
      "epoch": 0.0008469678206520961,
      "grad_norm": 10289.816324891324,
      "learning_rate": 2.0710415394386067e-07,
      "loss": 1.708,
      "step": 233376
    },
    {
      "epoch": 0.0008470839550029328,
      "grad_norm": 9763.500601730919,
      "learning_rate": 2.0708994238533396e-07,
      "loss": 1.7129,
      "step": 233408
    },
    {
      "epoch": 0.0008472000893537695,
      "grad_norm": 9195.127405316362,
      "learning_rate": 2.0707573375201192e-07,
      "loss": 1.7181,
      "step": 233440
    },
    {
      "epoch": 0.0008473162237046062,
      "grad_norm": 9786.190474336783,
      "learning_rate": 2.0706152804289122e-07,
      "loss": 1.7266,
      "step": 233472
    },
    {
      "epoch": 0.0008474323580554429,
      "grad_norm": 10818.30263950866,
      "learning_rate": 2.0704732525696894e-07,
      "loss": 1.7176,
      "step": 233504
    },
    {
      "epoch": 0.0008475484924062796,
      "grad_norm": 8454.272647602513,
      "learning_rate": 2.0703312539324275e-07,
      "loss": 1.7349,
      "step": 233536
    },
    {
      "epoch": 0.0008476646267571164,
      "grad_norm": 9412.980824372267,
      "learning_rate": 2.0701892845071066e-07,
      "loss": 1.7405,
      "step": 233568
    },
    {
      "epoch": 0.000847780761107953,
      "grad_norm": 9637.72130744607,
      "learning_rate": 2.0700473442837122e-07,
      "loss": 1.7142,
      "step": 233600
    },
    {
      "epoch": 0.0008478968954587898,
      "grad_norm": 9825.80826191922,
      "learning_rate": 2.0699054332522353e-07,
      "loss": 1.7303,
      "step": 233632
    },
    {
      "epoch": 0.0008480130298096264,
      "grad_norm": 10013.875373700235,
      "learning_rate": 2.0697635514026707e-07,
      "loss": 1.7149,
      "step": 233664
    },
    {
      "epoch": 0.0008481291641604632,
      "grad_norm": 10024.730819328766,
      "learning_rate": 2.0696216987250183e-07,
      "loss": 1.6986,
      "step": 233696
    },
    {
      "epoch": 0.0008482452985112998,
      "grad_norm": 10421.431571526055,
      "learning_rate": 2.069479875209283e-07,
      "loss": 1.7015,
      "step": 233728
    },
    {
      "epoch": 0.0008483614328621366,
      "grad_norm": 9439.657409037682,
      "learning_rate": 2.0693380808454742e-07,
      "loss": 1.7207,
      "step": 233760
    },
    {
      "epoch": 0.0008484775672129732,
      "grad_norm": 9126.329820908293,
      "learning_rate": 2.069196315623607e-07,
      "loss": 1.685,
      "step": 233792
    },
    {
      "epoch": 0.00084859370156381,
      "grad_norm": 10613.11641319363,
      "learning_rate": 2.0690545795336997e-07,
      "loss": 1.7196,
      "step": 233824
    },
    {
      "epoch": 0.0008487098359146468,
      "grad_norm": 10997.329857742741,
      "learning_rate": 2.068917300467812e-07,
      "loss": 1.7176,
      "step": 233856
    },
    {
      "epoch": 0.0008488259702654834,
      "grad_norm": 10529.00223193062,
      "learning_rate": 2.0687756217023023e-07,
      "loss": 1.7137,
      "step": 233888
    },
    {
      "epoch": 0.0008489421046163202,
      "grad_norm": 10627.270392720795,
      "learning_rate": 2.06863397203915e-07,
      "loss": 1.7039,
      "step": 233920
    },
    {
      "epoch": 0.0008490582389671568,
      "grad_norm": 11548.742096003356,
      "learning_rate": 2.0684923514683942e-07,
      "loss": 1.7005,
      "step": 233952
    },
    {
      "epoch": 0.0008491743733179936,
      "grad_norm": 10391.541752791065,
      "learning_rate": 2.0683507599800767e-07,
      "loss": 1.7043,
      "step": 233984
    },
    {
      "epoch": 0.0008492905076688302,
      "grad_norm": 10287.456828584993,
      "learning_rate": 2.0682091975642458e-07,
      "loss": 1.7131,
      "step": 234016
    },
    {
      "epoch": 0.000849406642019667,
      "grad_norm": 10245.1949713024,
      "learning_rate": 2.068067664210954e-07,
      "loss": 1.7309,
      "step": 234048
    },
    {
      "epoch": 0.0008495227763705036,
      "grad_norm": 9512.36752864396,
      "learning_rate": 2.0679261599102582e-07,
      "loss": 1.7379,
      "step": 234080
    },
    {
      "epoch": 0.0008496389107213404,
      "grad_norm": 9407.912095677766,
      "learning_rate": 2.0677846846522202e-07,
      "loss": 1.743,
      "step": 234112
    },
    {
      "epoch": 0.0008497550450721771,
      "grad_norm": 8670.242557160671,
      "learning_rate": 2.0676432384269076e-07,
      "loss": 1.7498,
      "step": 234144
    },
    {
      "epoch": 0.0008498711794230138,
      "grad_norm": 11448.612492350328,
      "learning_rate": 2.0675018212243908e-07,
      "loss": 1.7548,
      "step": 234176
    },
    {
      "epoch": 0.0008499873137738505,
      "grad_norm": 9406.004890494158,
      "learning_rate": 2.0673604330347468e-07,
      "loss": 1.7352,
      "step": 234208
    },
    {
      "epoch": 0.0008501034481246872,
      "grad_norm": 11615.530465717009,
      "learning_rate": 2.0672190738480564e-07,
      "loss": 1.7442,
      "step": 234240
    },
    {
      "epoch": 0.0008502195824755239,
      "grad_norm": 8597.777387208858,
      "learning_rate": 2.0670777436544048e-07,
      "loss": 1.7272,
      "step": 234272
    },
    {
      "epoch": 0.0008503357168263606,
      "grad_norm": 10120.073517519524,
      "learning_rate": 2.0669364424438834e-07,
      "loss": 1.7186,
      "step": 234304
    },
    {
      "epoch": 0.0008504518511771973,
      "grad_norm": 8939.51587056033,
      "learning_rate": 2.0667951702065867e-07,
      "loss": 1.7264,
      "step": 234336
    },
    {
      "epoch": 0.000850567985528034,
      "grad_norm": 10465.05594824987,
      "learning_rate": 2.0666539269326147e-07,
      "loss": 1.7193,
      "step": 234368
    },
    {
      "epoch": 0.0008506841198788707,
      "grad_norm": 12231.71991177038,
      "learning_rate": 2.0665127126120727e-07,
      "loss": 1.7108,
      "step": 234400
    },
    {
      "epoch": 0.0008508002542297075,
      "grad_norm": 10148.100906080901,
      "learning_rate": 2.0663715272350695e-07,
      "loss": 1.7117,
      "step": 234432
    },
    {
      "epoch": 0.0008509163885805441,
      "grad_norm": 10302.059017497424,
      "learning_rate": 2.0662303707917194e-07,
      "loss": 1.7389,
      "step": 234464
    },
    {
      "epoch": 0.0008510325229313809,
      "grad_norm": 12129.892662344544,
      "learning_rate": 2.0660892432721413e-07,
      "loss": 1.7477,
      "step": 234496
    },
    {
      "epoch": 0.0008511486572822175,
      "grad_norm": 9357.259000369713,
      "learning_rate": 2.065948144666459e-07,
      "loss": 1.7134,
      "step": 234528
    },
    {
      "epoch": 0.0008512647916330543,
      "grad_norm": 8221.172300833987,
      "learning_rate": 2.0658070749648006e-07,
      "loss": 1.7114,
      "step": 234560
    },
    {
      "epoch": 0.0008513809259838909,
      "grad_norm": 8731.497237014966,
      "learning_rate": 2.0656660341572993e-07,
      "loss": 1.7266,
      "step": 234592
    },
    {
      "epoch": 0.0008514970603347277,
      "grad_norm": 7516.506901480236,
      "learning_rate": 2.0655250222340926e-07,
      "loss": 1.704,
      "step": 234624
    },
    {
      "epoch": 0.0008516131946855643,
      "grad_norm": 8884.088923463114,
      "learning_rate": 2.0653840391853232e-07,
      "loss": 1.7348,
      "step": 234656
    },
    {
      "epoch": 0.0008517293290364011,
      "grad_norm": 8416.113711208993,
      "learning_rate": 2.065243085001138e-07,
      "loss": 1.7486,
      "step": 234688
    },
    {
      "epoch": 0.0008518454633872378,
      "grad_norm": 8364.865091560054,
      "learning_rate": 2.0651021596716893e-07,
      "loss": 1.7276,
      "step": 234720
    },
    {
      "epoch": 0.0008519615977380745,
      "grad_norm": 8379.629586085533,
      "learning_rate": 2.0649612631871333e-07,
      "loss": 1.7138,
      "step": 234752
    },
    {
      "epoch": 0.0008520777320889112,
      "grad_norm": 8346.977057593964,
      "learning_rate": 2.0648203955376319e-07,
      "loss": 1.7111,
      "step": 234784
    },
    {
      "epoch": 0.0008521938664397479,
      "grad_norm": 8394.779091792709,
      "learning_rate": 2.0646795567133504e-07,
      "loss": 1.6948,
      "step": 234816
    },
    {
      "epoch": 0.0008523100007905846,
      "grad_norm": 10850.022857118782,
      "learning_rate": 2.0645431465811644e-07,
      "loss": 1.7022,
      "step": 234848
    },
    {
      "epoch": 0.0008524261351414213,
      "grad_norm": 10473.888962558272,
      "learning_rate": 2.0644023644778145e-07,
      "loss": 1.713,
      "step": 234880
    },
    {
      "epoch": 0.000852542269492258,
      "grad_norm": 8534.86930186983,
      "learning_rate": 2.0642616111705177e-07,
      "loss": 1.7313,
      "step": 234912
    },
    {
      "epoch": 0.0008526584038430947,
      "grad_norm": 10385.273419607209,
      "learning_rate": 2.0641208866494586e-07,
      "loss": 1.7047,
      "step": 234944
    },
    {
      "epoch": 0.0008527745381939314,
      "grad_norm": 10398.169069600666,
      "learning_rate": 2.0639801909048264e-07,
      "loss": 1.7146,
      "step": 234976
    },
    {
      "epoch": 0.0008528906725447682,
      "grad_norm": 10485.049928350365,
      "learning_rate": 2.063839523926815e-07,
      "loss": 1.7379,
      "step": 235008
    },
    {
      "epoch": 0.0008530068068956048,
      "grad_norm": 10266.637424200779,
      "learning_rate": 2.0636988857056232e-07,
      "loss": 1.7096,
      "step": 235040
    },
    {
      "epoch": 0.0008531229412464416,
      "grad_norm": 9153.98601703105,
      "learning_rate": 2.0635582762314542e-07,
      "loss": 1.7326,
      "step": 235072
    },
    {
      "epoch": 0.0008532390755972782,
      "grad_norm": 8962.556666487526,
      "learning_rate": 2.0634176954945163e-07,
      "loss": 1.7388,
      "step": 235104
    },
    {
      "epoch": 0.000853355209948115,
      "grad_norm": 9686.557076691388,
      "learning_rate": 2.0632771434850216e-07,
      "loss": 1.7141,
      "step": 235136
    },
    {
      "epoch": 0.0008534713442989516,
      "grad_norm": 9195.484979053579,
      "learning_rate": 2.0631366201931878e-07,
      "loss": 1.6979,
      "step": 235168
    },
    {
      "epoch": 0.0008535874786497884,
      "grad_norm": 12189.979819507496,
      "learning_rate": 2.062996125609237e-07,
      "loss": 1.7127,
      "step": 235200
    },
    {
      "epoch": 0.000853703613000625,
      "grad_norm": 10329.357966495303,
      "learning_rate": 2.0628556597233957e-07,
      "loss": 1.7071,
      "step": 235232
    },
    {
      "epoch": 0.0008538197473514618,
      "grad_norm": 9358.002457789804,
      "learning_rate": 2.0627152225258948e-07,
      "loss": 1.7134,
      "step": 235264
    },
    {
      "epoch": 0.0008539358817022985,
      "grad_norm": 8894.336175342149,
      "learning_rate": 2.0625748140069708e-07,
      "loss": 1.7263,
      "step": 235296
    },
    {
      "epoch": 0.0008540520160531352,
      "grad_norm": 9758.19163574891,
      "learning_rate": 2.0624344341568642e-07,
      "loss": 1.7392,
      "step": 235328
    },
    {
      "epoch": 0.0008541681504039719,
      "grad_norm": 9525.029763733024,
      "learning_rate": 2.0622940829658198e-07,
      "loss": 1.7129,
      "step": 235360
    },
    {
      "epoch": 0.0008542842847548086,
      "grad_norm": 7188.790718890069,
      "learning_rate": 2.0621537604240883e-07,
      "loss": 1.7105,
      "step": 235392
    },
    {
      "epoch": 0.0008544004191056453,
      "grad_norm": 9950.843984306055,
      "learning_rate": 2.0620134665219239e-07,
      "loss": 1.7043,
      "step": 235424
    },
    {
      "epoch": 0.000854516553456482,
      "grad_norm": 10703.17186632075,
      "learning_rate": 2.0618732012495855e-07,
      "loss": 1.6858,
      "step": 235456
    },
    {
      "epoch": 0.0008546326878073187,
      "grad_norm": 9233.029838574117,
      "learning_rate": 2.0617329645973374e-07,
      "loss": 1.7145,
      "step": 235488
    },
    {
      "epoch": 0.0008547488221581554,
      "grad_norm": 9526.172998639066,
      "learning_rate": 2.0615927565554474e-07,
      "loss": 1.7234,
      "step": 235520
    },
    {
      "epoch": 0.0008548649565089921,
      "grad_norm": 9160.37957728827,
      "learning_rate": 2.0614525771141893e-07,
      "loss": 1.7037,
      "step": 235552
    },
    {
      "epoch": 0.0008549810908598289,
      "grad_norm": 8335.232450267958,
      "learning_rate": 2.0613124262638403e-07,
      "loss": 1.6948,
      "step": 235584
    },
    {
      "epoch": 0.0008550972252106655,
      "grad_norm": 9429.071322245898,
      "learning_rate": 2.0611723039946834e-07,
      "loss": 1.7084,
      "step": 235616
    },
    {
      "epoch": 0.0008552133595615023,
      "grad_norm": 10089.557968513784,
      "learning_rate": 2.061032210297005e-07,
      "loss": 1.7058,
      "step": 235648
    },
    {
      "epoch": 0.0008553294939123389,
      "grad_norm": 10306.310494061394,
      "learning_rate": 2.0608921451610966e-07,
      "loss": 1.7012,
      "step": 235680
    },
    {
      "epoch": 0.0008554456282631757,
      "grad_norm": 8985.056037666098,
      "learning_rate": 2.0607521085772549e-07,
      "loss": 1.7087,
      "step": 235712
    },
    {
      "epoch": 0.0008555617626140123,
      "grad_norm": 9973.249019251449,
      "learning_rate": 2.0606121005357804e-07,
      "loss": 1.7153,
      "step": 235744
    },
    {
      "epoch": 0.0008556778969648491,
      "grad_norm": 9922.610342042057,
      "learning_rate": 2.0604721210269787e-07,
      "loss": 1.7003,
      "step": 235776
    },
    {
      "epoch": 0.0008557940313156857,
      "grad_norm": 8507.731307463817,
      "learning_rate": 2.0603321700411594e-07,
      "loss": 1.7118,
      "step": 235808
    },
    {
      "epoch": 0.0008559101656665225,
      "grad_norm": 19642.336724534583,
      "learning_rate": 2.0601922475686378e-07,
      "loss": 1.7282,
      "step": 235840
    },
    {
      "epoch": 0.0008560263000173592,
      "grad_norm": 9389.186333223981,
      "learning_rate": 2.0600567248549062e-07,
      "loss": 1.726,
      "step": 235872
    },
    {
      "epoch": 0.0008561424343681959,
      "grad_norm": 10711.190596754406,
      "learning_rate": 2.0599168584896527e-07,
      "loss": 1.7521,
      "step": 235904
    },
    {
      "epoch": 0.0008562585687190326,
      "grad_norm": 10586.47967928905,
      "learning_rate": 2.0597770206089703e-07,
      "loss": 1.7553,
      "step": 235936
    },
    {
      "epoch": 0.0008563747030698693,
      "grad_norm": 9524.69611063786,
      "learning_rate": 2.0596372112031917e-07,
      "loss": 1.7241,
      "step": 235968
    },
    {
      "epoch": 0.000856490837420706,
      "grad_norm": 10273.240189930342,
      "learning_rate": 2.059497430262655e-07,
      "loss": 1.7012,
      "step": 236000
    },
    {
      "epoch": 0.0008566069717715427,
      "grad_norm": 9367.254240170916,
      "learning_rate": 2.0593576777777016e-07,
      "loss": 1.7028,
      "step": 236032
    },
    {
      "epoch": 0.0008567231061223794,
      "grad_norm": 8347.18156026332,
      "learning_rate": 2.059217953738679e-07,
      "loss": 1.6936,
      "step": 236064
    },
    {
      "epoch": 0.0008568392404732161,
      "grad_norm": 8299.587941578786,
      "learning_rate": 2.0590782581359377e-07,
      "loss": 1.7127,
      "step": 236096
    },
    {
      "epoch": 0.0008569553748240528,
      "grad_norm": 12012.7712040145,
      "learning_rate": 2.0589385909598339e-07,
      "loss": 1.7191,
      "step": 236128
    },
    {
      "epoch": 0.0008570715091748896,
      "grad_norm": 9065.843148874792,
      "learning_rate": 2.058798952200728e-07,
      "loss": 1.7105,
      "step": 236160
    },
    {
      "epoch": 0.0008571876435257262,
      "grad_norm": 9275.12565952613,
      "learning_rate": 2.0586593418489848e-07,
      "loss": 1.6958,
      "step": 236192
    },
    {
      "epoch": 0.000857303777876563,
      "grad_norm": 11970.263155002065,
      "learning_rate": 2.0585197598949743e-07,
      "loss": 1.7169,
      "step": 236224
    },
    {
      "epoch": 0.0008574199122273996,
      "grad_norm": 9351.260449800337,
      "learning_rate": 2.05838020632907e-07,
      "loss": 1.7236,
      "step": 236256
    },
    {
      "epoch": 0.0008575360465782364,
      "grad_norm": 9183.19225541968,
      "learning_rate": 2.0582406811416512e-07,
      "loss": 1.6917,
      "step": 236288
    },
    {
      "epoch": 0.000857652180929073,
      "grad_norm": 9732.405047057999,
      "learning_rate": 2.058101184323101e-07,
      "loss": 1.7148,
      "step": 236320
    },
    {
      "epoch": 0.0008577683152799098,
      "grad_norm": 7934.303750172412,
      "learning_rate": 2.057961715863807e-07,
      "loss": 1.708,
      "step": 236352
    },
    {
      "epoch": 0.0008578844496307464,
      "grad_norm": 10584.20615823407,
      "learning_rate": 2.0578222757541617e-07,
      "loss": 1.7019,
      "step": 236384
    },
    {
      "epoch": 0.0008580005839815832,
      "grad_norm": 10437.3638434233,
      "learning_rate": 2.0576828639845623e-07,
      "loss": 1.6963,
      "step": 236416
    },
    {
      "epoch": 0.0008581167183324199,
      "grad_norm": 9004.413362346268,
      "learning_rate": 2.05754348054541e-07,
      "loss": 1.7226,
      "step": 236448
    },
    {
      "epoch": 0.0008582328526832566,
      "grad_norm": 13572.322424699467,
      "learning_rate": 2.0574041254271108e-07,
      "loss": 1.7158,
      "step": 236480
    },
    {
      "epoch": 0.0008583489870340933,
      "grad_norm": 9824.543144594561,
      "learning_rate": 2.0572647986200758e-07,
      "loss": 1.7328,
      "step": 236512
    },
    {
      "epoch": 0.00085846512138493,
      "grad_norm": 11052.703922570261,
      "learning_rate": 2.05712550011472e-07,
      "loss": 1.7173,
      "step": 236544
    },
    {
      "epoch": 0.0008585812557357667,
      "grad_norm": 8196.971513919028,
      "learning_rate": 2.0569862299014623e-07,
      "loss": 1.7115,
      "step": 236576
    },
    {
      "epoch": 0.0008586973900866034,
      "grad_norm": 9796.03368716135,
      "learning_rate": 2.056846987970728e-07,
      "loss": 1.7023,
      "step": 236608
    },
    {
      "epoch": 0.0008588135244374401,
      "grad_norm": 9904.963402254447,
      "learning_rate": 2.0567077743129451e-07,
      "loss": 1.706,
      "step": 236640
    },
    {
      "epoch": 0.0008589296587882768,
      "grad_norm": 9846.85371070374,
      "learning_rate": 2.0565685889185472e-07,
      "loss": 1.7235,
      "step": 236672
    },
    {
      "epoch": 0.0008590457931391135,
      "grad_norm": 9869.736774605492,
      "learning_rate": 2.056429431777972e-07,
      "loss": 1.7262,
      "step": 236704
    },
    {
      "epoch": 0.0008591619274899503,
      "grad_norm": 10851.991890892656,
      "learning_rate": 2.0562903028816624e-07,
      "loss": 1.7317,
      "step": 236736
    },
    {
      "epoch": 0.0008592780618407869,
      "grad_norm": 8785.589906204365,
      "learning_rate": 2.0561512022200645e-07,
      "loss": 1.742,
      "step": 236768
    },
    {
      "epoch": 0.0008593941961916237,
      "grad_norm": 8250.63718751467,
      "learning_rate": 2.0560121297836304e-07,
      "loss": 1.7288,
      "step": 236800
    },
    {
      "epoch": 0.0008595103305424603,
      "grad_norm": 10468.154374100528,
      "learning_rate": 2.0558730855628156e-07,
      "loss": 1.7154,
      "step": 236832
    },
    {
      "epoch": 0.0008596264648932971,
      "grad_norm": 9533.342330998084,
      "learning_rate": 2.0557384133716881e-07,
      "loss": 1.7359,
      "step": 236864
    },
    {
      "epoch": 0.0008597425992441337,
      "grad_norm": 9426.485028895977,
      "learning_rate": 2.0555994246725003e-07,
      "loss": 1.6904,
      "step": 236896
    },
    {
      "epoch": 0.0008598587335949705,
      "grad_norm": 8704.755481919063,
      "learning_rate": 2.055460464160624e-07,
      "loss": 1.7143,
      "step": 236928
    },
    {
      "epoch": 0.0008599748679458071,
      "grad_norm": 9724.561069786132,
      "learning_rate": 2.055321531826534e-07,
      "loss": 1.7185,
      "step": 236960
    },
    {
      "epoch": 0.0008600910022966439,
      "grad_norm": 9948.073180269634,
      "learning_rate": 2.0551826276607084e-07,
      "loss": 1.7159,
      "step": 236992
    },
    {
      "epoch": 0.0008602071366474806,
      "grad_norm": 10413.869597800809,
      "learning_rate": 2.0550437516536296e-07,
      "loss": 1.702,
      "step": 237024
    },
    {
      "epoch": 0.0008603232709983173,
      "grad_norm": 9851.837392080728,
      "learning_rate": 2.054904903795785e-07,
      "loss": 1.719,
      "step": 237056
    },
    {
      "epoch": 0.000860439405349154,
      "grad_norm": 9051.046127382182,
      "learning_rate": 2.0547660840776668e-07,
      "loss": 1.7335,
      "step": 237088
    },
    {
      "epoch": 0.0008605555396999907,
      "grad_norm": 8616.026346292123,
      "learning_rate": 2.0546272924897714e-07,
      "loss": 1.7285,
      "step": 237120
    },
    {
      "epoch": 0.0008606716740508274,
      "grad_norm": 9642.803119425389,
      "learning_rate": 2.0544885290225997e-07,
      "loss": 1.7202,
      "step": 237152
    },
    {
      "epoch": 0.0008607878084016641,
      "grad_norm": 9535.75482067361,
      "learning_rate": 2.0543497936666562e-07,
      "loss": 1.712,
      "step": 237184
    },
    {
      "epoch": 0.0008609039427525008,
      "grad_norm": 9379.326841516933,
      "learning_rate": 2.0542110864124515e-07,
      "loss": 1.7023,
      "step": 237216
    },
    {
      "epoch": 0.0008610200771033375,
      "grad_norm": 9320.98921788884,
      "learning_rate": 2.0540724072504996e-07,
      "loss": 1.6973,
      "step": 237248
    },
    {
      "epoch": 0.0008611362114541742,
      "grad_norm": 9720.104114668731,
      "learning_rate": 2.0539337561713198e-07,
      "loss": 1.7178,
      "step": 237280
    },
    {
      "epoch": 0.000861252345805011,
      "grad_norm": 11889.607226481454,
      "learning_rate": 2.0537951331654345e-07,
      "loss": 1.6899,
      "step": 237312
    },
    {
      "epoch": 0.0008613684801558476,
      "grad_norm": 9358.131437418477,
      "learning_rate": 2.0536565382233722e-07,
      "loss": 1.7049,
      "step": 237344
    },
    {
      "epoch": 0.0008614846145066844,
      "grad_norm": 8839.298840971494,
      "learning_rate": 2.0535179713356648e-07,
      "loss": 1.7151,
      "step": 237376
    },
    {
      "epoch": 0.000861600748857521,
      "grad_norm": 9565.99686389244,
      "learning_rate": 2.0533794324928488e-07,
      "loss": 1.7106,
      "step": 237408
    },
    {
      "epoch": 0.0008617168832083578,
      "grad_norm": 8737.148161728746,
      "learning_rate": 2.0532409216854654e-07,
      "loss": 1.6962,
      "step": 237440
    },
    {
      "epoch": 0.0008618330175591944,
      "grad_norm": 9340.064025476486,
      "learning_rate": 2.0531024389040607e-07,
      "loss": 1.7068,
      "step": 237472
    },
    {
      "epoch": 0.0008619491519100312,
      "grad_norm": 9212.891619898717,
      "learning_rate": 2.0529639841391843e-07,
      "loss": 1.7095,
      "step": 237504
    },
    {
      "epoch": 0.0008620652862608678,
      "grad_norm": 9378.2057985523,
      "learning_rate": 2.0528255573813908e-07,
      "loss": 1.7119,
      "step": 237536
    },
    {
      "epoch": 0.0008621814206117046,
      "grad_norm": 11366.691339171659,
      "learning_rate": 2.0526871586212396e-07,
      "loss": 1.7371,
      "step": 237568
    },
    {
      "epoch": 0.0008622975549625413,
      "grad_norm": 8633.224426597515,
      "learning_rate": 2.0525487878492934e-07,
      "loss": 1.7449,
      "step": 237600
    },
    {
      "epoch": 0.000862413689313378,
      "grad_norm": 9245.351047959184,
      "learning_rate": 2.0524104450561208e-07,
      "loss": 1.7458,
      "step": 237632
    },
    {
      "epoch": 0.0008625298236642147,
      "grad_norm": 10271.491809858975,
      "learning_rate": 2.0522721302322937e-07,
      "loss": 1.7529,
      "step": 237664
    },
    {
      "epoch": 0.0008626459580150514,
      "grad_norm": 9947.393829541485,
      "learning_rate": 2.0521338433683892e-07,
      "loss": 1.7645,
      "step": 237696
    },
    {
      "epoch": 0.0008627620923658881,
      "grad_norm": 9723.047053264732,
      "learning_rate": 2.0519955844549885e-07,
      "loss": 1.7175,
      "step": 237728
    },
    {
      "epoch": 0.0008628782267167248,
      "grad_norm": 8299.607822060028,
      "learning_rate": 2.051857353482677e-07,
      "loss": 1.7118,
      "step": 237760
    },
    {
      "epoch": 0.0008629943610675615,
      "grad_norm": 9121.024174948776,
      "learning_rate": 2.0517191504420452e-07,
      "loss": 1.711,
      "step": 237792
    },
    {
      "epoch": 0.0008631104954183982,
      "grad_norm": 10439.772794462531,
      "learning_rate": 2.0515809753236875e-07,
      "loss": 1.7049,
      "step": 237824
    },
    {
      "epoch": 0.000863226629769235,
      "grad_norm": 11988.1760080506,
      "learning_rate": 2.051442828118203e-07,
      "loss": 1.7034,
      "step": 237856
    },
    {
      "epoch": 0.0008633427641200717,
      "grad_norm": 19565.368588401292,
      "learning_rate": 2.0513047088161952e-07,
      "loss": 1.7084,
      "step": 237888
    },
    {
      "epoch": 0.0008634588984709083,
      "grad_norm": 9844.279760348138,
      "learning_rate": 2.0511709323426373e-07,
      "loss": 1.6909,
      "step": 237920
    },
    {
      "epoch": 0.0008635750328217451,
      "grad_norm": 8792.230433740919,
      "learning_rate": 2.0510328679481558e-07,
      "loss": 1.6982,
      "step": 237952
    },
    {
      "epoch": 0.0008636911671725817,
      "grad_norm": 9386.309604951246,
      "learning_rate": 2.0508948314292807e-07,
      "loss": 1.7207,
      "step": 237984
    },
    {
      "epoch": 0.0008638073015234185,
      "grad_norm": 10270.791692951425,
      "learning_rate": 2.050756822776633e-07,
      "loss": 1.7335,
      "step": 238016
    },
    {
      "epoch": 0.0008639234358742551,
      "grad_norm": 9388.11567887827,
      "learning_rate": 2.0506188419808383e-07,
      "loss": 1.6994,
      "step": 238048
    },
    {
      "epoch": 0.0008640395702250919,
      "grad_norm": 11262.847064574747,
      "learning_rate": 2.0504808890325264e-07,
      "loss": 1.6963,
      "step": 238080
    },
    {
      "epoch": 0.0008641557045759285,
      "grad_norm": 9044.114218650713,
      "learning_rate": 2.0503429639223313e-07,
      "loss": 1.7107,
      "step": 238112
    },
    {
      "epoch": 0.0008642718389267653,
      "grad_norm": 11528.31019707572,
      "learning_rate": 2.050205066640892e-07,
      "loss": 1.6935,
      "step": 238144
    },
    {
      "epoch": 0.0008643879732776021,
      "grad_norm": 9609.190184401597,
      "learning_rate": 2.0500671971788516e-07,
      "loss": 1.7141,
      "step": 238176
    },
    {
      "epoch": 0.0008645041076284387,
      "grad_norm": 9711.259856475883,
      "learning_rate": 2.049929355526857e-07,
      "loss": 1.7208,
      "step": 238208
    },
    {
      "epoch": 0.0008646202419792755,
      "grad_norm": 10079.761901949867,
      "learning_rate": 2.04979154167556e-07,
      "loss": 1.7203,
      "step": 238240
    },
    {
      "epoch": 0.0008647363763301121,
      "grad_norm": 9916.51813894373,
      "learning_rate": 2.0496537556156177e-07,
      "loss": 1.7205,
      "step": 238272
    },
    {
      "epoch": 0.0008648525106809489,
      "grad_norm": 10050.123382327203,
      "learning_rate": 2.0495159973376902e-07,
      "loss": 1.716,
      "step": 238304
    },
    {
      "epoch": 0.0008649686450317855,
      "grad_norm": 9537.51875489637,
      "learning_rate": 2.0493782668324423e-07,
      "loss": 1.6977,
      "step": 238336
    },
    {
      "epoch": 0.0008650847793826223,
      "grad_norm": 15950.229841604165,
      "learning_rate": 2.049240564090544e-07,
      "loss": 1.7074,
      "step": 238368
    },
    {
      "epoch": 0.0008652009137334589,
      "grad_norm": 9295.09978429495,
      "learning_rate": 2.0491028891026688e-07,
      "loss": 1.7198,
      "step": 238400
    },
    {
      "epoch": 0.0008653170480842957,
      "grad_norm": 9936.334132868118,
      "learning_rate": 2.0489652418594947e-07,
      "loss": 1.7303,
      "step": 238432
    },
    {
      "epoch": 0.0008654331824351324,
      "grad_norm": 8833.802012723627,
      "learning_rate": 2.0488276223517047e-07,
      "loss": 1.7284,
      "step": 238464
    },
    {
      "epoch": 0.0008655493167859691,
      "grad_norm": 9103.903118992424,
      "learning_rate": 2.0486900305699858e-07,
      "loss": 1.7333,
      "step": 238496
    },
    {
      "epoch": 0.0008656654511368058,
      "grad_norm": 11152.534958474687,
      "learning_rate": 2.048552466505029e-07,
      "loss": 1.7312,
      "step": 238528
    },
    {
      "epoch": 0.0008657815854876425,
      "grad_norm": 9074.256222963952,
      "learning_rate": 2.0484149301475297e-07,
      "loss": 1.713,
      "step": 238560
    },
    {
      "epoch": 0.0008658977198384792,
      "grad_norm": 11629.923645493122,
      "learning_rate": 2.0482774214881887e-07,
      "loss": 1.734,
      "step": 238592
    },
    {
      "epoch": 0.0008660138541893159,
      "grad_norm": 9250.157944597486,
      "learning_rate": 2.0481399405177103e-07,
      "loss": 1.7305,
      "step": 238624
    },
    {
      "epoch": 0.0008661299885401526,
      "grad_norm": 11247.113763094956,
      "learning_rate": 2.048002487226803e-07,
      "loss": 1.7021,
      "step": 238656
    },
    {
      "epoch": 0.0008662461228909893,
      "grad_norm": 8287.287372837991,
      "learning_rate": 2.0478650616061804e-07,
      "loss": 1.6895,
      "step": 238688
    },
    {
      "epoch": 0.000866362257241826,
      "grad_norm": 9791.927695811484,
      "learning_rate": 2.0477276636465597e-07,
      "loss": 1.7051,
      "step": 238720
    },
    {
      "epoch": 0.0008664783915926628,
      "grad_norm": 11469.225780321878,
      "learning_rate": 2.047590293338663e-07,
      "loss": 1.7126,
      "step": 238752
    },
    {
      "epoch": 0.0008665945259434994,
      "grad_norm": 9263.072276518196,
      "learning_rate": 2.0474529506732166e-07,
      "loss": 1.7112,
      "step": 238784
    },
    {
      "epoch": 0.0008667106602943362,
      "grad_norm": 9645.612474073381,
      "learning_rate": 2.047315635640951e-07,
      "loss": 1.7306,
      "step": 238816
    },
    {
      "epoch": 0.0008668267946451728,
      "grad_norm": 8623.227817934536,
      "learning_rate": 2.047178348232601e-07,
      "loss": 1.7457,
      "step": 238848
    },
    {
      "epoch": 0.0008669429289960096,
      "grad_norm": 11152.440450412636,
      "learning_rate": 2.0470410884389064e-07,
      "loss": 1.7206,
      "step": 238880
    },
    {
      "epoch": 0.0008670590633468462,
      "grad_norm": 8737.659640887827,
      "learning_rate": 2.0469081443387315e-07,
      "loss": 1.699,
      "step": 238912
    },
    {
      "epoch": 0.000867175197697683,
      "grad_norm": 10612.500365135447,
      "learning_rate": 2.046770938884343e-07,
      "loss": 1.709,
      "step": 238944
    },
    {
      "epoch": 0.0008672913320485196,
      "grad_norm": 10472.956029698587,
      "learning_rate": 2.0466337610171423e-07,
      "loss": 1.6932,
      "step": 238976
    },
    {
      "epoch": 0.0008674074663993564,
      "grad_norm": 9825.072009914227,
      "learning_rate": 2.0464966107278862e-07,
      "loss": 1.7172,
      "step": 239008
    },
    {
      "epoch": 0.0008675236007501931,
      "grad_norm": 9323.438850552944,
      "learning_rate": 2.046359488007336e-07,
      "loss": 1.7319,
      "step": 239040
    },
    {
      "epoch": 0.0008676397351010298,
      "grad_norm": 9974.406849532457,
      "learning_rate": 2.0462223928462563e-07,
      "loss": 1.7169,
      "step": 239072
    },
    {
      "epoch": 0.0008677558694518665,
      "grad_norm": 9237.735220279914,
      "learning_rate": 2.046085325235417e-07,
      "loss": 1.704,
      "step": 239104
    },
    {
      "epoch": 0.0008678720038027032,
      "grad_norm": 9616.726678033436,
      "learning_rate": 2.0459482851655915e-07,
      "loss": 1.7311,
      "step": 239136
    },
    {
      "epoch": 0.0008679881381535399,
      "grad_norm": 9342.637100947462,
      "learning_rate": 2.0458112726275588e-07,
      "loss": 1.7103,
      "step": 239168
    },
    {
      "epoch": 0.0008681042725043766,
      "grad_norm": 10890.500814930414,
      "learning_rate": 2.0456742876121009e-07,
      "loss": 1.7173,
      "step": 239200
    },
    {
      "epoch": 0.0008682204068552133,
      "grad_norm": 10378.471371064237,
      "learning_rate": 2.0455373301100045e-07,
      "loss": 1.7308,
      "step": 239232
    },
    {
      "epoch": 0.00086833654120605,
      "grad_norm": 8605.217022248771,
      "learning_rate": 2.0454004001120613e-07,
      "loss": 1.7293,
      "step": 239264
    },
    {
      "epoch": 0.0008684526755568867,
      "grad_norm": 10185.604940306688,
      "learning_rate": 2.0452634976090662e-07,
      "loss": 1.715,
      "step": 239296
    },
    {
      "epoch": 0.0008685688099077235,
      "grad_norm": 9419.885561937575,
      "learning_rate": 2.0451266225918194e-07,
      "loss": 1.7314,
      "step": 239328
    },
    {
      "epoch": 0.0008686849442585601,
      "grad_norm": 10134.678879964575,
      "learning_rate": 2.044989775051125e-07,
      "loss": 1.747,
      "step": 239360
    },
    {
      "epoch": 0.0008688010786093969,
      "grad_norm": 11650.003090128344,
      "learning_rate": 2.0448529549777911e-07,
      "loss": 1.7377,
      "step": 239392
    },
    {
      "epoch": 0.0008689172129602335,
      "grad_norm": 9245.841551746385,
      "learning_rate": 2.0447161623626305e-07,
      "loss": 1.7704,
      "step": 239424
    },
    {
      "epoch": 0.0008690333473110703,
      "grad_norm": 8956.7974187206,
      "learning_rate": 2.0445793971964603e-07,
      "loss": 1.7679,
      "step": 239456
    },
    {
      "epoch": 0.0008691494816619069,
      "grad_norm": 8968.138491348134,
      "learning_rate": 2.044442659470102e-07,
      "loss": 1.7406,
      "step": 239488
    },
    {
      "epoch": 0.0008692656160127437,
      "grad_norm": 8224.785468326818,
      "learning_rate": 2.0443059491743806e-07,
      "loss": 1.7091,
      "step": 239520
    },
    {
      "epoch": 0.0008693817503635803,
      "grad_norm": 9114.592475804939,
      "learning_rate": 2.0441692663001267e-07,
      "loss": 1.722,
      "step": 239552
    },
    {
      "epoch": 0.0008694978847144171,
      "grad_norm": 9903.432435272125,
      "learning_rate": 2.0440326108381743e-07,
      "loss": 1.7118,
      "step": 239584
    },
    {
      "epoch": 0.0008696140190652538,
      "grad_norm": 10207.529769733714,
      "learning_rate": 2.0438959827793614e-07,
      "loss": 1.7273,
      "step": 239616
    },
    {
      "epoch": 0.0008697301534160905,
      "grad_norm": 9977.647017208015,
      "learning_rate": 2.0437593821145314e-07,
      "loss": 1.724,
      "step": 239648
    },
    {
      "epoch": 0.0008698462877669272,
      "grad_norm": 10585.422806860386,
      "learning_rate": 2.0436228088345308e-07,
      "loss": 1.6974,
      "step": 239680
    },
    {
      "epoch": 0.0008699624221177639,
      "grad_norm": 10484.494265342511,
      "learning_rate": 2.0434862629302114e-07,
      "loss": 1.6995,
      "step": 239712
    },
    {
      "epoch": 0.0008700785564686006,
      "grad_norm": 9561.75025819018,
      "learning_rate": 2.0433497443924285e-07,
      "loss": 1.7148,
      "step": 239744
    },
    {
      "epoch": 0.0008701946908194373,
      "grad_norm": 9837.922341632911,
      "learning_rate": 2.0432132532120425e-07,
      "loss": 1.7034,
      "step": 239776
    },
    {
      "epoch": 0.000870310825170274,
      "grad_norm": 10110.115924162294,
      "learning_rate": 2.043076789379917e-07,
      "loss": 1.7018,
      "step": 239808
    },
    {
      "epoch": 0.0008704269595211107,
      "grad_norm": 9794.52183621028,
      "learning_rate": 2.04294035288692e-07,
      "loss": 1.7072,
      "step": 239840
    },
    {
      "epoch": 0.0008705430938719474,
      "grad_norm": 11410.832222059878,
      "learning_rate": 2.0428039437239253e-07,
      "loss": 1.6967,
      "step": 239872
    },
    {
      "epoch": 0.0008706592282227842,
      "grad_norm": 17221.078711857746,
      "learning_rate": 2.0426675618818094e-07,
      "loss": 1.6943,
      "step": 239904
    },
    {
      "epoch": 0.0008707753625736208,
      "grad_norm": 10681.251986541653,
      "learning_rate": 2.042535468017207e-07,
      "loss": 1.7151,
      "step": 239936
    },
    {
      "epoch": 0.0008708914969244576,
      "grad_norm": 11362.866891766354,
      "learning_rate": 2.0423991399364266e-07,
      "loss": 1.6997,
      "step": 239968
    },
    {
      "epoch": 0.0008710076312752942,
      "grad_norm": 9853.440515880735,
      "learning_rate": 2.042262839149466e-07,
      "loss": 1.7302,
      "step": 240000
    },
    {
      "epoch": 0.000871123765626131,
      "grad_norm": 11210.441739735326,
      "learning_rate": 2.0421265656472192e-07,
      "loss": 1.7289,
      "step": 240032
    },
    {
      "epoch": 0.0008712398999769676,
      "grad_norm": 8957.026515535164,
      "learning_rate": 2.0419903194205842e-07,
      "loss": 1.7073,
      "step": 240064
    },
    {
      "epoch": 0.0008713560343278044,
      "grad_norm": 8531.889005372726,
      "learning_rate": 2.041854100460463e-07,
      "loss": 1.7068,
      "step": 240096
    },
    {
      "epoch": 0.000871472168678641,
      "grad_norm": 9965.212190415215,
      "learning_rate": 2.041717908757763e-07,
      "loss": 1.7027,
      "step": 240128
    },
    {
      "epoch": 0.0008715883030294778,
      "grad_norm": 10143.877759515835,
      "learning_rate": 2.0415817443033945e-07,
      "loss": 1.7151,
      "step": 240160
    },
    {
      "epoch": 0.0008717044373803145,
      "grad_norm": 9838.52366973826,
      "learning_rate": 2.041445607088273e-07,
      "loss": 1.7206,
      "step": 240192
    },
    {
      "epoch": 0.0008718205717311512,
      "grad_norm": 10520.570516849359,
      "learning_rate": 2.0413094971033172e-07,
      "loss": 1.7169,
      "step": 240224
    },
    {
      "epoch": 0.0008719367060819879,
      "grad_norm": 10653.610092358365,
      "learning_rate": 2.0411734143394514e-07,
      "loss": 1.7165,
      "step": 240256
    },
    {
      "epoch": 0.0008720528404328246,
      "grad_norm": 10360.683954257074,
      "learning_rate": 2.0410373587876026e-07,
      "loss": 1.7234,
      "step": 240288
    },
    {
      "epoch": 0.0008721689747836613,
      "grad_norm": 10121.878481783902,
      "learning_rate": 2.0409013304387038e-07,
      "loss": 1.7142,
      "step": 240320
    },
    {
      "epoch": 0.000872285109134498,
      "grad_norm": 9560.967733446234,
      "learning_rate": 2.0407653292836908e-07,
      "loss": 1.734,
      "step": 240352
    },
    {
      "epoch": 0.0008724012434853347,
      "grad_norm": 9708.77870795292,
      "learning_rate": 2.0406293553135038e-07,
      "loss": 1.7214,
      "step": 240384
    },
    {
      "epoch": 0.0008725173778361714,
      "grad_norm": 10822.358522983795,
      "learning_rate": 2.040493408519088e-07,
      "loss": 1.6944,
      "step": 240416
    },
    {
      "epoch": 0.0008726335121870081,
      "grad_norm": 12343.49172641194,
      "learning_rate": 2.0403574888913925e-07,
      "loss": 1.7015,
      "step": 240448
    },
    {
      "epoch": 0.0008727496465378449,
      "grad_norm": 11247.876421796249,
      "learning_rate": 2.04022159642137e-07,
      "loss": 1.7009,
      "step": 240480
    },
    {
      "epoch": 0.0008728657808886815,
      "grad_norm": 10650.102346926062,
      "learning_rate": 2.0400857310999776e-07,
      "loss": 1.7165,
      "step": 240512
    },
    {
      "epoch": 0.0008729819152395183,
      "grad_norm": 8720.627615028634,
      "learning_rate": 2.039949892918178e-07,
      "loss": 1.72,
      "step": 240544
    },
    {
      "epoch": 0.0008730980495903549,
      "grad_norm": 10620.680015893522,
      "learning_rate": 2.039814081866936e-07,
      "loss": 1.7153,
      "step": 240576
    },
    {
      "epoch": 0.0008732141839411917,
      "grad_norm": 9442.47891181124,
      "learning_rate": 2.0396782979372223e-07,
      "loss": 1.7209,
      "step": 240608
    },
    {
      "epoch": 0.0008733303182920283,
      "grad_norm": 9500.507354873213,
      "learning_rate": 2.0395425411200107e-07,
      "loss": 1.7329,
      "step": 240640
    },
    {
      "epoch": 0.0008734464526428651,
      "grad_norm": 9551.28054241943,
      "learning_rate": 2.0394068114062795e-07,
      "loss": 1.6957,
      "step": 240672
    },
    {
      "epoch": 0.0008735625869937017,
      "grad_norm": 9254.572491476849,
      "learning_rate": 2.0392711087870118e-07,
      "loss": 1.7104,
      "step": 240704
    },
    {
      "epoch": 0.0008736787213445385,
      "grad_norm": 11755.960020347125,
      "learning_rate": 2.0391354332531946e-07,
      "loss": 1.7087,
      "step": 240736
    },
    {
      "epoch": 0.0008737948556953752,
      "grad_norm": 10008.51127790742,
      "learning_rate": 2.0389997847958182e-07,
      "loss": 1.6845,
      "step": 240768
    },
    {
      "epoch": 0.0008739109900462119,
      "grad_norm": 8783.419152015915,
      "learning_rate": 2.0388641634058782e-07,
      "loss": 1.7106,
      "step": 240800
    },
    {
      "epoch": 0.0008740271243970486,
      "grad_norm": 9690.880868115137,
      "learning_rate": 2.0387285690743743e-07,
      "loss": 1.695,
      "step": 240832
    },
    {
      "epoch": 0.0008741432587478853,
      "grad_norm": 9610.823481887492,
      "learning_rate": 2.0385930017923096e-07,
      "loss": 1.7048,
      "step": 240864
    },
    {
      "epoch": 0.000874259393098722,
      "grad_norm": 9849.892385198937,
      "learning_rate": 2.0384574615506925e-07,
      "loss": 1.7173,
      "step": 240896
    },
    {
      "epoch": 0.0008743755274495587,
      "grad_norm": 9781.417075250396,
      "learning_rate": 2.0383261827192734e-07,
      "loss": 1.6884,
      "step": 240928
    },
    {
      "epoch": 0.0008744916618003954,
      "grad_norm": 8400.726992350126,
      "learning_rate": 2.0381906956872743e-07,
      "loss": 1.6932,
      "step": 240960
    },
    {
      "epoch": 0.0008746077961512321,
      "grad_norm": 11214.512026833801,
      "learning_rate": 2.0380552356690513e-07,
      "loss": 1.7066,
      "step": 240992
    },
    {
      "epoch": 0.0008747239305020688,
      "grad_norm": 8450.399280507401,
      "learning_rate": 2.0379198026556294e-07,
      "loss": 1.6953,
      "step": 241024
    },
    {
      "epoch": 0.0008748400648529056,
      "grad_norm": 9683.142878218827,
      "learning_rate": 2.0377843966380364e-07,
      "loss": 1.7189,
      "step": 241056
    },
    {
      "epoch": 0.0008749561992037422,
      "grad_norm": 11215.310071504933,
      "learning_rate": 2.0376490176073057e-07,
      "loss": 1.7275,
      "step": 241088
    },
    {
      "epoch": 0.000875072333554579,
      "grad_norm": 9543.72862145608,
      "learning_rate": 2.0375136655544737e-07,
      "loss": 1.7285,
      "step": 241120
    },
    {
      "epoch": 0.0008751884679054156,
      "grad_norm": 10410.164167773724,
      "learning_rate": 2.037378340470582e-07,
      "loss": 1.7464,
      "step": 241152
    },
    {
      "epoch": 0.0008753046022562524,
      "grad_norm": 8551.374041637988,
      "learning_rate": 2.037243042346675e-07,
      "loss": 1.7298,
      "step": 241184
    },
    {
      "epoch": 0.000875420736607089,
      "grad_norm": 8554.733660377744,
      "learning_rate": 2.037107771173803e-07,
      "loss": 1.7526,
      "step": 241216
    },
    {
      "epoch": 0.0008755368709579258,
      "grad_norm": 8623.46044230505,
      "learning_rate": 2.0369725269430188e-07,
      "loss": 1.739,
      "step": 241248
    },
    {
      "epoch": 0.0008756530053087624,
      "grad_norm": 9660.9556463116,
      "learning_rate": 2.0368373096453808e-07,
      "loss": 1.6889,
      "step": 241280
    },
    {
      "epoch": 0.0008757691396595992,
      "grad_norm": 12149.54089667589,
      "learning_rate": 2.0367021192719508e-07,
      "loss": 1.6961,
      "step": 241312
    },
    {
      "epoch": 0.0008758852740104359,
      "grad_norm": 9781.627880879543,
      "learning_rate": 2.0365669558137942e-07,
      "loss": 1.6978,
      "step": 241344
    },
    {
      "epoch": 0.0008760014083612726,
      "grad_norm": 7763.077740174962,
      "learning_rate": 2.0364318192619817e-07,
      "loss": 1.7061,
      "step": 241376
    },
    {
      "epoch": 0.0008761175427121093,
      "grad_norm": 9788.369833634199,
      "learning_rate": 2.0362967096075877e-07,
      "loss": 1.721,
      "step": 241408
    },
    {
      "epoch": 0.000876233677062946,
      "grad_norm": 8706.101308852316,
      "learning_rate": 2.0361616268416902e-07,
      "loss": 1.7006,
      "step": 241440
    },
    {
      "epoch": 0.0008763498114137827,
      "grad_norm": 9778.80197161186,
      "learning_rate": 2.0360265709553722e-07,
      "loss": 1.6979,
      "step": 241472
    },
    {
      "epoch": 0.0008764659457646194,
      "grad_norm": 10101.220718309249,
      "learning_rate": 2.0358915419397201e-07,
      "loss": 1.7228,
      "step": 241504
    },
    {
      "epoch": 0.0008765820801154561,
      "grad_norm": 9915.137316245296,
      "learning_rate": 2.0357565397858252e-07,
      "loss": 1.699,
      "step": 241536
    },
    {
      "epoch": 0.0008766982144662928,
      "grad_norm": 9467.807877222689,
      "learning_rate": 2.0356215644847823e-07,
      "loss": 1.7186,
      "step": 241568
    },
    {
      "epoch": 0.0008768143488171295,
      "grad_norm": 9493.239278560295,
      "learning_rate": 2.0354866160276906e-07,
      "loss": 1.712,
      "step": 241600
    },
    {
      "epoch": 0.0008769304831679663,
      "grad_norm": 8328.41137312513,
      "learning_rate": 2.0353516944056535e-07,
      "loss": 1.7035,
      "step": 241632
    },
    {
      "epoch": 0.0008770466175188029,
      "grad_norm": 10172.88081125499,
      "learning_rate": 2.0352167996097782e-07,
      "loss": 1.7024,
      "step": 241664
    },
    {
      "epoch": 0.0008771627518696397,
      "grad_norm": 9960.1128507663,
      "learning_rate": 2.0350819316311762e-07,
      "loss": 1.7125,
      "step": 241696
    },
    {
      "epoch": 0.0008772788862204763,
      "grad_norm": 9203.693389069413,
      "learning_rate": 2.0349470904609631e-07,
      "loss": 1.7119,
      "step": 241728
    },
    {
      "epoch": 0.0008773950205713131,
      "grad_norm": 9736.334834012232,
      "learning_rate": 2.0348122760902592e-07,
      "loss": 1.738,
      "step": 241760
    },
    {
      "epoch": 0.0008775111549221497,
      "grad_norm": 9034.183748408042,
      "learning_rate": 2.0346774885101878e-07,
      "loss": 1.722,
      "step": 241792
    },
    {
      "epoch": 0.0008776272892729865,
      "grad_norm": 10142.56377845365,
      "learning_rate": 2.0345427277118774e-07,
      "loss": 1.6991,
      "step": 241824
    },
    {
      "epoch": 0.0008777434236238231,
      "grad_norm": 8706.909899614215,
      "learning_rate": 2.0344079936864596e-07,
      "loss": 1.7185,
      "step": 241856
    },
    {
      "epoch": 0.0008778595579746599,
      "grad_norm": 9743.44025485865,
      "learning_rate": 2.034273286425071e-07,
      "loss": 1.6961,
      "step": 241888
    },
    {
      "epoch": 0.0008779756923254967,
      "grad_norm": 7835.745146442679,
      "learning_rate": 2.034138605918852e-07,
      "loss": 1.7204,
      "step": 241920
    },
    {
      "epoch": 0.0008780918266763333,
      "grad_norm": 9947.589858855259,
      "learning_rate": 2.0340081596841804e-07,
      "loss": 1.7204,
      "step": 241952
    },
    {
      "epoch": 0.00087820796102717,
      "grad_norm": 9760.536665573261,
      "learning_rate": 2.0338735318263262e-07,
      "loss": 1.7213,
      "step": 241984
    },
    {
      "epoch": 0.0008783240953780067,
      "grad_norm": 9639.988796673988,
      "learning_rate": 2.0337389306973632e-07,
      "loss": 1.7182,
      "step": 242016
    },
    {
      "epoch": 0.0008784402297288435,
      "grad_norm": 11547.230317266562,
      "learning_rate": 2.033604356288448e-07,
      "loss": 1.7307,
      "step": 242048
    },
    {
      "epoch": 0.0008785563640796801,
      "grad_norm": 9195.65212478158,
      "learning_rate": 2.0334698085907424e-07,
      "loss": 1.7219,
      "step": 242080
    },
    {
      "epoch": 0.0008786724984305169,
      "grad_norm": 10222.69357850464,
      "learning_rate": 2.03333528759541e-07,
      "loss": 1.7319,
      "step": 242112
    },
    {
      "epoch": 0.0008787886327813535,
      "grad_norm": 9486.296643053072,
      "learning_rate": 2.0332007932936206e-07,
      "loss": 1.7162,
      "step": 242144
    },
    {
      "epoch": 0.0008789047671321903,
      "grad_norm": 9941.8778910224,
      "learning_rate": 2.0330663256765464e-07,
      "loss": 1.6945,
      "step": 242176
    },
    {
      "epoch": 0.000879020901483027,
      "grad_norm": 10519.293512399016,
      "learning_rate": 2.0329318847353648e-07,
      "loss": 1.7061,
      "step": 242208
    },
    {
      "epoch": 0.0008791370358338636,
      "grad_norm": 8421.767866665527,
      "learning_rate": 2.0327974704612572e-07,
      "loss": 1.691,
      "step": 242240
    },
    {
      "epoch": 0.0008792531701847004,
      "grad_norm": 8523.258297153736,
      "learning_rate": 2.0326630828454083e-07,
      "loss": 1.7194,
      "step": 242272
    },
    {
      "epoch": 0.000879369304535537,
      "grad_norm": 9870.321372680832,
      "learning_rate": 2.0325287218790083e-07,
      "loss": 1.7148,
      "step": 242304
    },
    {
      "epoch": 0.0008794854388863738,
      "grad_norm": 9618.396332029575,
      "learning_rate": 2.03239438755325e-07,
      "loss": 1.7114,
      "step": 242336
    },
    {
      "epoch": 0.0008796015732372104,
      "grad_norm": 9862.439454820496,
      "learning_rate": 2.0322600798593303e-07,
      "loss": 1.7402,
      "step": 242368
    },
    {
      "epoch": 0.0008797177075880472,
      "grad_norm": 9584.073455478103,
      "learning_rate": 2.032125798788452e-07,
      "loss": 1.737,
      "step": 242400
    },
    {
      "epoch": 0.0008798338419388838,
      "grad_norm": 10164.127704825436,
      "learning_rate": 2.0319915443318196e-07,
      "loss": 1.6998,
      "step": 242432
    },
    {
      "epoch": 0.0008799499762897206,
      "grad_norm": 8342.694049286478,
      "learning_rate": 2.0318573164806436e-07,
      "loss": 1.7097,
      "step": 242464
    },
    {
      "epoch": 0.0008800661106405572,
      "grad_norm": 10536.690087499015,
      "learning_rate": 2.0317231152261372e-07,
      "loss": 1.7072,
      "step": 242496
    },
    {
      "epoch": 0.000880182244991394,
      "grad_norm": 7930.53882154296,
      "learning_rate": 2.0315889405595183e-07,
      "loss": 1.6986,
      "step": 242528
    },
    {
      "epoch": 0.0008802983793422308,
      "grad_norm": 10589.369575191906,
      "learning_rate": 2.031454792472009e-07,
      "loss": 1.7283,
      "step": 242560
    },
    {
      "epoch": 0.0008804145136930674,
      "grad_norm": 9141.875081185479,
      "learning_rate": 2.031320670954835e-07,
      "loss": 1.7076,
      "step": 242592
    },
    {
      "epoch": 0.0008805306480439042,
      "grad_norm": 10234.069083214163,
      "learning_rate": 2.0311865759992263e-07,
      "loss": 1.7265,
      "step": 242624
    },
    {
      "epoch": 0.0008806467823947408,
      "grad_norm": 10189.77271581658,
      "learning_rate": 2.0310525075964172e-07,
      "loss": 1.7198,
      "step": 242656
    },
    {
      "epoch": 0.0008807629167455776,
      "grad_norm": 10872.650274886984,
      "learning_rate": 2.0309184657376453e-07,
      "loss": 1.6876,
      "step": 242688
    },
    {
      "epoch": 0.0008808790510964142,
      "grad_norm": 8919.8087423442,
      "learning_rate": 2.0307844504141532e-07,
      "loss": 1.6948,
      "step": 242720
    },
    {
      "epoch": 0.000880995185447251,
      "grad_norm": 9313.097658674045,
      "learning_rate": 2.0306504616171866e-07,
      "loss": 1.707,
      "step": 242752
    },
    {
      "epoch": 0.0008811113197980876,
      "grad_norm": 8156.5486573672815,
      "learning_rate": 2.0305164993379957e-07,
      "loss": 1.7108,
      "step": 242784
    },
    {
      "epoch": 0.0008812274541489244,
      "grad_norm": 8135.965462070251,
      "learning_rate": 2.0303825635678354e-07,
      "loss": 1.7158,
      "step": 242816
    },
    {
      "epoch": 0.0008813435884997611,
      "grad_norm": 9848.854349618538,
      "learning_rate": 2.0302486542979634e-07,
      "loss": 1.7155,
      "step": 242848
    },
    {
      "epoch": 0.0008814597228505978,
      "grad_norm": 9496.305808049781,
      "learning_rate": 2.0301147715196422e-07,
      "loss": 1.7195,
      "step": 242880
    },
    {
      "epoch": 0.0008815758572014345,
      "grad_norm": 8425.319934578152,
      "learning_rate": 2.0299809152241378e-07,
      "loss": 1.7404,
      "step": 242912
    },
    {
      "epoch": 0.0008816919915522712,
      "grad_norm": 9341.189003547675,
      "learning_rate": 2.0298470854027214e-07,
      "loss": 1.7309,
      "step": 242944
    },
    {
      "epoch": 0.0008818081259031079,
      "grad_norm": 9851.03892998094,
      "learning_rate": 2.0297174630010312e-07,
      "loss": 1.7563,
      "step": 242976
    },
    {
      "epoch": 0.0008819242602539446,
      "grad_norm": 11610.12868145741,
      "learning_rate": 2.0295836852749792e-07,
      "loss": 1.7364,
      "step": 243008
    },
    {
      "epoch": 0.0008820403946047813,
      "grad_norm": 9228.5550331566,
      "learning_rate": 2.029449933997122e-07,
      "loss": 1.6933,
      "step": 243040
    },
    {
      "epoch": 0.000882156528955618,
      "grad_norm": 10430.487236941522,
      "learning_rate": 2.029316209158746e-07,
      "loss": 1.7011,
      "step": 243072
    },
    {
      "epoch": 0.0008822726633064547,
      "grad_norm": 11554.658973764652,
      "learning_rate": 2.0291825107511425e-07,
      "loss": 1.6933,
      "step": 243104
    },
    {
      "epoch": 0.0008823887976572915,
      "grad_norm": 9941.335222192238,
      "learning_rate": 2.0290488387656052e-07,
      "loss": 1.7037,
      "step": 243136
    },
    {
      "epoch": 0.0008825049320081281,
      "grad_norm": 8736.23351336261,
      "learning_rate": 2.0289151931934324e-07,
      "loss": 1.7153,
      "step": 243168
    },
    {
      "epoch": 0.0008826210663589649,
      "grad_norm": 10209.17019154838,
      "learning_rate": 2.028781574025927e-07,
      "loss": 1.7032,
      "step": 243200
    },
    {
      "epoch": 0.0008827372007098015,
      "grad_norm": 8152.97099222118,
      "learning_rate": 2.028647981254395e-07,
      "loss": 1.7065,
      "step": 243232
    },
    {
      "epoch": 0.0008828533350606383,
      "grad_norm": 10292.518642198322,
      "learning_rate": 2.028514414870147e-07,
      "loss": 1.7206,
      "step": 243264
    },
    {
      "epoch": 0.0008829694694114749,
      "grad_norm": 9967.506809628974,
      "learning_rate": 2.0283808748644976e-07,
      "loss": 1.7008,
      "step": 243296
    },
    {
      "epoch": 0.0008830856037623117,
      "grad_norm": 8256.42174310397,
      "learning_rate": 2.028247361228765e-07,
      "loss": 1.7167,
      "step": 243328
    },
    {
      "epoch": 0.0008832017381131483,
      "grad_norm": 11268.189561770781,
      "learning_rate": 2.0281138739542717e-07,
      "loss": 1.7203,
      "step": 243360
    },
    {
      "epoch": 0.0008833178724639851,
      "grad_norm": 9194.259078359713,
      "learning_rate": 2.027980413032344e-07,
      "loss": 1.7003,
      "step": 243392
    },
    {
      "epoch": 0.0008834340068148218,
      "grad_norm": 8355.553721926513,
      "learning_rate": 2.0278469784543125e-07,
      "loss": 1.7165,
      "step": 243424
    },
    {
      "epoch": 0.0008835501411656585,
      "grad_norm": 10195.937622406289,
      "learning_rate": 2.027713570211512e-07,
      "loss": 1.7051,
      "step": 243456
    },
    {
      "epoch": 0.0008836662755164952,
      "grad_norm": 10166.507561596558,
      "learning_rate": 2.0275801882952799e-07,
      "loss": 1.71,
      "step": 243488
    },
    {
      "epoch": 0.0008837824098673319,
      "grad_norm": 8833.319647788141,
      "learning_rate": 2.0274468326969595e-07,
      "loss": 1.7406,
      "step": 243520
    },
    {
      "epoch": 0.0008838985442181686,
      "grad_norm": 11048.17070831185,
      "learning_rate": 2.0273135034078967e-07,
      "loss": 1.7237,
      "step": 243552
    },
    {
      "epoch": 0.0008840146785690053,
      "grad_norm": 10161.5122890247,
      "learning_rate": 2.0271802004194425e-07,
      "loss": 1.7024,
      "step": 243584
    },
    {
      "epoch": 0.000884130812919842,
      "grad_norm": 10694.086029203243,
      "learning_rate": 2.0270469237229505e-07,
      "loss": 1.7075,
      "step": 243616
    },
    {
      "epoch": 0.0008842469472706787,
      "grad_norm": 9373.766905572167,
      "learning_rate": 2.0269136733097792e-07,
      "loss": 1.6944,
      "step": 243648
    },
    {
      "epoch": 0.0008843630816215154,
      "grad_norm": 10273.42338268992,
      "learning_rate": 2.0267804491712914e-07,
      "loss": 1.7229,
      "step": 243680
    },
    {
      "epoch": 0.0008844792159723522,
      "grad_norm": 10114.147912701297,
      "learning_rate": 2.026647251298853e-07,
      "loss": 1.7248,
      "step": 243712
    },
    {
      "epoch": 0.0008845953503231888,
      "grad_norm": 10187.300721977339,
      "learning_rate": 2.0265140796838341e-07,
      "loss": 1.715,
      "step": 243744
    },
    {
      "epoch": 0.0008847114846740256,
      "grad_norm": 8732.951276630369,
      "learning_rate": 2.026380934317609e-07,
      "loss": 1.7283,
      "step": 243776
    },
    {
      "epoch": 0.0008848276190248622,
      "grad_norm": 8711.28555380892,
      "learning_rate": 2.0262478151915563e-07,
      "loss": 1.7212,
      "step": 243808
    },
    {
      "epoch": 0.000884943753375699,
      "grad_norm": 9533.833017207717,
      "learning_rate": 2.0261147222970576e-07,
      "loss": 1.7201,
      "step": 243840
    },
    {
      "epoch": 0.0008850598877265356,
      "grad_norm": 9000.440433667676,
      "learning_rate": 2.0259816556254993e-07,
      "loss": 1.7364,
      "step": 243872
    },
    {
      "epoch": 0.0008851760220773724,
      "grad_norm": 11188.356090150151,
      "learning_rate": 2.0258486151682714e-07,
      "loss": 1.7058,
      "step": 243904
    },
    {
      "epoch": 0.000885292156428209,
      "grad_norm": 9316.429358933603,
      "learning_rate": 2.0257156009167677e-07,
      "loss": 1.6967,
      "step": 243936
    },
    {
      "epoch": 0.0008854082907790458,
      "grad_norm": 9906.68057423878,
      "learning_rate": 2.0255867683426331e-07,
      "loss": 1.7112,
      "step": 243968
    },
    {
      "epoch": 0.0008855244251298825,
      "grad_norm": 8371.302765997654,
      "learning_rate": 2.025453805658515e-07,
      "loss": 1.7047,
      "step": 244000
    },
    {
      "epoch": 0.0008856405594807192,
      "grad_norm": 9732.188654151747,
      "learning_rate": 2.0253208691545958e-07,
      "loss": 1.7254,
      "step": 244032
    },
    {
      "epoch": 0.0008857566938315559,
      "grad_norm": 8773.255496108613,
      "learning_rate": 2.025187958822285e-07,
      "loss": 1.7344,
      "step": 244064
    },
    {
      "epoch": 0.0008858728281823926,
      "grad_norm": 7823.264919456582,
      "learning_rate": 2.0250550746529964e-07,
      "loss": 1.7256,
      "step": 244096
    },
    {
      "epoch": 0.0008859889625332293,
      "grad_norm": 9497.893029509229,
      "learning_rate": 2.0249222166381474e-07,
      "loss": 1.749,
      "step": 244128
    },
    {
      "epoch": 0.000886105096884066,
      "grad_norm": 9337.295968319737,
      "learning_rate": 2.0247893847691595e-07,
      "loss": 1.7458,
      "step": 244160
    },
    {
      "epoch": 0.0008862212312349027,
      "grad_norm": 9917.226830117379,
      "learning_rate": 2.0246565790374586e-07,
      "loss": 1.7188,
      "step": 244192
    },
    {
      "epoch": 0.0008863373655857394,
      "grad_norm": 9205.439587548222,
      "learning_rate": 2.0245237994344736e-07,
      "loss": 1.7197,
      "step": 244224
    },
    {
      "epoch": 0.0008864534999365761,
      "grad_norm": 8240.947882373726,
      "learning_rate": 2.0243910459516382e-07,
      "loss": 1.714,
      "step": 244256
    },
    {
      "epoch": 0.0008865696342874129,
      "grad_norm": 9702.753938959804,
      "learning_rate": 2.0242583185803896e-07,
      "loss": 1.7051,
      "step": 244288
    },
    {
      "epoch": 0.0008866857686382495,
      "grad_norm": 10433.298998878543,
      "learning_rate": 2.0241256173121688e-07,
      "loss": 1.7283,
      "step": 244320
    },
    {
      "epoch": 0.0008868019029890863,
      "grad_norm": 9209.253172760536,
      "learning_rate": 2.0239929421384214e-07,
      "loss": 1.7164,
      "step": 244352
    },
    {
      "epoch": 0.0008869180373399229,
      "grad_norm": 10708.237576744363,
      "learning_rate": 2.023860293050596e-07,
      "loss": 1.7212,
      "step": 244384
    },
    {
      "epoch": 0.0008870341716907597,
      "grad_norm": 9585.584593544621,
      "learning_rate": 2.0237276700401458e-07,
      "loss": 1.7294,
      "step": 244416
    },
    {
      "epoch": 0.0008871503060415963,
      "grad_norm": 7306.772611762323,
      "learning_rate": 2.0235950730985273e-07,
      "loss": 1.706,
      "step": 244448
    },
    {
      "epoch": 0.0008872664403924331,
      "grad_norm": 9535.845111997152,
      "learning_rate": 2.023462502217202e-07,
      "loss": 1.7143,
      "step": 244480
    },
    {
      "epoch": 0.0008873825747432697,
      "grad_norm": 9480.299573325728,
      "learning_rate": 2.0233299573876345e-07,
      "loss": 1.7167,
      "step": 244512
    },
    {
      "epoch": 0.0008874987090941065,
      "grad_norm": 9704.418787336004,
      "learning_rate": 2.023197438601293e-07,
      "loss": 1.7268,
      "step": 244544
    },
    {
      "epoch": 0.0008876148434449432,
      "grad_norm": 10144.48145545153,
      "learning_rate": 2.0230649458496506e-07,
      "loss": 1.7232,
      "step": 244576
    },
    {
      "epoch": 0.0008877309777957799,
      "grad_norm": 9578.638525385537,
      "learning_rate": 2.022932479124183e-07,
      "loss": 1.7074,
      "step": 244608
    },
    {
      "epoch": 0.0008878471121466166,
      "grad_norm": 9512.888940800265,
      "learning_rate": 2.0228000384163716e-07,
      "loss": 1.7195,
      "step": 244640
    },
    {
      "epoch": 0.0008879632464974533,
      "grad_norm": 9415.705071846718,
      "learning_rate": 2.0226676237177e-07,
      "loss": 1.7412,
      "step": 244672
    },
    {
      "epoch": 0.00088807938084829,
      "grad_norm": 8363.247933667877,
      "learning_rate": 2.0225352350196562e-07,
      "loss": 1.7268,
      "step": 244704
    },
    {
      "epoch": 0.0008881955151991267,
      "grad_norm": 9579.224707668152,
      "learning_rate": 2.022402872313733e-07,
      "loss": 1.7465,
      "step": 244736
    },
    {
      "epoch": 0.0008883116495499634,
      "grad_norm": 9006.895802661425,
      "learning_rate": 2.0222705355914258e-07,
      "loss": 1.7297,
      "step": 244768
    },
    {
      "epoch": 0.0008884277839008001,
      "grad_norm": 9395.290096638848,
      "learning_rate": 2.0221382248442347e-07,
      "loss": 1.6801,
      "step": 244800
    },
    {
      "epoch": 0.0008885439182516368,
      "grad_norm": 10519.21460946586,
      "learning_rate": 2.0220059400636633e-07,
      "loss": 1.7013,
      "step": 244832
    },
    {
      "epoch": 0.0008886600526024736,
      "grad_norm": 9678.457418411263,
      "learning_rate": 2.0218736812412195e-07,
      "loss": 1.6996,
      "step": 244864
    },
    {
      "epoch": 0.0008887761869533102,
      "grad_norm": 9704.698037548618,
      "learning_rate": 2.021741448368415e-07,
      "loss": 1.7049,
      "step": 244896
    },
    {
      "epoch": 0.000888892321304147,
      "grad_norm": 9650.472527291085,
      "learning_rate": 2.0216092414367642e-07,
      "loss": 1.7153,
      "step": 244928
    },
    {
      "epoch": 0.0008890084556549836,
      "grad_norm": 15524.882479426375,
      "learning_rate": 2.0214770604377878e-07,
      "loss": 1.6915,
      "step": 244960
    },
    {
      "epoch": 0.0008891245900058204,
      "grad_norm": 8802.073619324028,
      "learning_rate": 2.021349034816772e-07,
      "loss": 1.7089,
      "step": 244992
    },
    {
      "epoch": 0.000889240724356657,
      "grad_norm": 9948.951301519171,
      "learning_rate": 2.0212169048479783e-07,
      "loss": 1.7197,
      "step": 245024
    },
    {
      "epoch": 0.0008893568587074938,
      "grad_norm": 9560.70436735704,
      "learning_rate": 2.021084800786704e-07,
      "loss": 1.6982,
      "step": 245056
    },
    {
      "epoch": 0.0008894729930583304,
      "grad_norm": 9018.066089799964,
      "learning_rate": 2.020952722624484e-07,
      "loss": 1.7065,
      "step": 245088
    },
    {
      "epoch": 0.0008895891274091672,
      "grad_norm": 10737.724526174063,
      "learning_rate": 2.020820670352857e-07,
      "loss": 1.7117,
      "step": 245120
    },
    {
      "epoch": 0.0008897052617600039,
      "grad_norm": 10937.577976864897,
      "learning_rate": 2.0206886439633653e-07,
      "loss": 1.6857,
      "step": 245152
    },
    {
      "epoch": 0.0008898213961108406,
      "grad_norm": 7316.7725125221705,
      "learning_rate": 2.020556643447555e-07,
      "loss": 1.702,
      "step": 245184
    },
    {
      "epoch": 0.0008899375304616773,
      "grad_norm": 11290.518765760944,
      "learning_rate": 2.0204246687969766e-07,
      "loss": 1.6995,
      "step": 245216
    },
    {
      "epoch": 0.000890053664812514,
      "grad_norm": 8888.59235199815,
      "learning_rate": 2.020292720003184e-07,
      "loss": 1.7086,
      "step": 245248
    },
    {
      "epoch": 0.0008901697991633507,
      "grad_norm": 9219.073923122647,
      "learning_rate": 2.020160797057735e-07,
      "loss": 1.7272,
      "step": 245280
    },
    {
      "epoch": 0.0008902859335141874,
      "grad_norm": 9074.997300275081,
      "learning_rate": 2.0200288999521917e-07,
      "loss": 1.7129,
      "step": 245312
    },
    {
      "epoch": 0.0008904020678650241,
      "grad_norm": 10393.446204219272,
      "learning_rate": 2.0198970286781194e-07,
      "loss": 1.7119,
      "step": 245344
    },
    {
      "epoch": 0.0008905182022158608,
      "grad_norm": 12045.562668468418,
      "learning_rate": 2.0197651832270876e-07,
      "loss": 1.7057,
      "step": 245376
    },
    {
      "epoch": 0.0008906343365666975,
      "grad_norm": 10154.724417727937,
      "learning_rate": 2.0196333635906695e-07,
      "loss": 1.7052,
      "step": 245408
    },
    {
      "epoch": 0.0008907504709175343,
      "grad_norm": 8613.00272843333,
      "learning_rate": 2.0195015697604426e-07,
      "loss": 1.7186,
      "step": 245440
    },
    {
      "epoch": 0.0008908666052683709,
      "grad_norm": 10408.75861954729,
      "learning_rate": 2.0193698017279877e-07,
      "loss": 1.7161,
      "step": 245472
    },
    {
      "epoch": 0.0008909827396192077,
      "grad_norm": 10466.485943238065,
      "learning_rate": 2.0192380594848895e-07,
      "loss": 1.7171,
      "step": 245504
    },
    {
      "epoch": 0.0008910988739700443,
      "grad_norm": 8461.579876122425,
      "learning_rate": 2.0191063430227372e-07,
      "loss": 1.7293,
      "step": 245536
    },
    {
      "epoch": 0.0008912150083208811,
      "grad_norm": 8830.427849204138,
      "learning_rate": 2.018974652333123e-07,
      "loss": 1.7081,
      "step": 245568
    },
    {
      "epoch": 0.0008913311426717177,
      "grad_norm": 9126.745641245843,
      "learning_rate": 2.0188429874076433e-07,
      "loss": 1.7255,
      "step": 245600
    },
    {
      "epoch": 0.0008914472770225545,
      "grad_norm": 9053.38367683597,
      "learning_rate": 2.018711348237898e-07,
      "loss": 1.7335,
      "step": 245632
    },
    {
      "epoch": 0.0008915634113733911,
      "grad_norm": 10089.21324980298,
      "learning_rate": 2.018579734815492e-07,
      "loss": 1.6987,
      "step": 245664
    },
    {
      "epoch": 0.0008916795457242279,
      "grad_norm": 9374.54084208928,
      "learning_rate": 2.0184481471320326e-07,
      "loss": 1.6955,
      "step": 245696
    },
    {
      "epoch": 0.0008917956800750646,
      "grad_norm": 9081.24462835354,
      "learning_rate": 2.0183165851791314e-07,
      "loss": 1.7038,
      "step": 245728
    },
    {
      "epoch": 0.0008919118144259013,
      "grad_norm": 10828.813785452217,
      "learning_rate": 2.018185048948404e-07,
      "loss": 1.6925,
      "step": 245760
    },
    {
      "epoch": 0.000892027948776738,
      "grad_norm": 11546.786739175535,
      "learning_rate": 2.01805353843147e-07,
      "loss": 1.7121,
      "step": 245792
    },
    {
      "epoch": 0.0008921440831275747,
      "grad_norm": 8031.294914271297,
      "learning_rate": 2.0179220536199527e-07,
      "loss": 1.712,
      "step": 245824
    },
    {
      "epoch": 0.0008922602174784114,
      "grad_norm": 10345.31488162637,
      "learning_rate": 2.0177905945054783e-07,
      "loss": 1.7024,
      "step": 245856
    },
    {
      "epoch": 0.0008923763518292481,
      "grad_norm": 9316.223269115011,
      "learning_rate": 2.0176591610796787e-07,
      "loss": 1.7346,
      "step": 245888
    },
    {
      "epoch": 0.0008924924861800848,
      "grad_norm": 9827.786322463468,
      "learning_rate": 2.0175277533341877e-07,
      "loss": 1.7194,
      "step": 245920
    },
    {
      "epoch": 0.0008926086205309215,
      "grad_norm": 8560.40069155644,
      "learning_rate": 2.0173963712606443e-07,
      "loss": 1.711,
      "step": 245952
    },
    {
      "epoch": 0.0008927247548817582,
      "grad_norm": 8845.880397111414,
      "learning_rate": 2.0172691193501218e-07,
      "loss": 1.7058,
      "step": 245984
    },
    {
      "epoch": 0.000892840889232595,
      "grad_norm": 10078.147647261376,
      "learning_rate": 2.017137787793804e-07,
      "loss": 1.6986,
      "step": 246016
    },
    {
      "epoch": 0.0008929570235834316,
      "grad_norm": 10968.03154627119,
      "learning_rate": 2.0170064818846325e-07,
      "loss": 1.6992,
      "step": 246048
    },
    {
      "epoch": 0.0008930731579342684,
      "grad_norm": 9019.915742400259,
      "learning_rate": 2.016875201614261e-07,
      "loss": 1.7125,
      "step": 246080
    },
    {
      "epoch": 0.000893189292285105,
      "grad_norm": 10305.372191240838,
      "learning_rate": 2.0167439469743468e-07,
      "loss": 1.7019,
      "step": 246112
    },
    {
      "epoch": 0.0008933054266359418,
      "grad_norm": 9704.431565011935,
      "learning_rate": 2.0166127179565513e-07,
      "loss": 1.7066,
      "step": 246144
    },
    {
      "epoch": 0.0008934215609867784,
      "grad_norm": 8986.75469788733,
      "learning_rate": 2.0164815145525392e-07,
      "loss": 1.7067,
      "step": 246176
    },
    {
      "epoch": 0.0008935376953376152,
      "grad_norm": 9829.317778971234,
      "learning_rate": 2.0163503367539798e-07,
      "loss": 1.6841,
      "step": 246208
    },
    {
      "epoch": 0.0008936538296884518,
      "grad_norm": 11152.102223347849,
      "learning_rate": 2.0162191845525445e-07,
      "loss": 1.7,
      "step": 246240
    },
    {
      "epoch": 0.0008937699640392886,
      "grad_norm": 11020.554069555668,
      "learning_rate": 2.0160880579399108e-07,
      "loss": 1.6952,
      "step": 246272
    },
    {
      "epoch": 0.0008938860983901254,
      "grad_norm": 10019.82494857071,
      "learning_rate": 2.015956956907758e-07,
      "loss": 1.7125,
      "step": 246304
    },
    {
      "epoch": 0.000894002232740962,
      "grad_norm": 8807.911216627925,
      "learning_rate": 2.015825881447771e-07,
      "loss": 1.7189,
      "step": 246336
    },
    {
      "epoch": 0.0008941183670917988,
      "grad_norm": 8852.278350797607,
      "learning_rate": 2.0156948315516363e-07,
      "loss": 1.7099,
      "step": 246368
    },
    {
      "epoch": 0.0008942345014426354,
      "grad_norm": 9836.983887350838,
      "learning_rate": 2.015563807211046e-07,
      "loss": 1.7256,
      "step": 246400
    },
    {
      "epoch": 0.0008943506357934722,
      "grad_norm": 12119.779865987666,
      "learning_rate": 2.0154328084176957e-07,
      "loss": 1.7318,
      "step": 246432
    },
    {
      "epoch": 0.0008944667701443088,
      "grad_norm": 9283.227671451348,
      "learning_rate": 2.015301835163284e-07,
      "loss": 1.7443,
      "step": 246464
    },
    {
      "epoch": 0.0008945829044951456,
      "grad_norm": 9252.804115510065,
      "learning_rate": 2.0151708874395136e-07,
      "loss": 1.7561,
      "step": 246496
    },
    {
      "epoch": 0.0008946990388459822,
      "grad_norm": 10209.107306713942,
      "learning_rate": 2.0150399652380913e-07,
      "loss": 1.7315,
      "step": 246528
    },
    {
      "epoch": 0.000894815173196819,
      "grad_norm": 10572.116155245363,
      "learning_rate": 2.0149090685507276e-07,
      "loss": 1.6949,
      "step": 246560
    },
    {
      "epoch": 0.0008949313075476557,
      "grad_norm": 12167.053217603678,
      "learning_rate": 2.0147781973691364e-07,
      "loss": 1.7016,
      "step": 246592
    },
    {
      "epoch": 0.0008950474418984924,
      "grad_norm": 9930.273510835439,
      "learning_rate": 2.0146473516850358e-07,
      "loss": 1.6912,
      "step": 246624
    },
    {
      "epoch": 0.0008951635762493291,
      "grad_norm": 9300.470095645704,
      "learning_rate": 2.014516531490147e-07,
      "loss": 1.7228,
      "step": 246656
    },
    {
      "epoch": 0.0008952797106001658,
      "grad_norm": 10612.151148565497,
      "learning_rate": 2.014385736776196e-07,
      "loss": 1.7178,
      "step": 246688
    },
    {
      "epoch": 0.0008953958449510025,
      "grad_norm": 11172.884497747213,
      "learning_rate": 2.0142549675349117e-07,
      "loss": 1.6967,
      "step": 246720
    },
    {
      "epoch": 0.0008955119793018392,
      "grad_norm": 8893.947155228663,
      "learning_rate": 2.014124223758027e-07,
      "loss": 1.7072,
      "step": 246752
    },
    {
      "epoch": 0.0008956281136526759,
      "grad_norm": 8911.663368866668,
      "learning_rate": 2.0139935054372788e-07,
      "loss": 1.7215,
      "step": 246784
    },
    {
      "epoch": 0.0008957442480035126,
      "grad_norm": 9951.9226283166,
      "learning_rate": 2.0138628125644075e-07,
      "loss": 1.702,
      "step": 246816
    },
    {
      "epoch": 0.0008958603823543493,
      "grad_norm": 9477.724410426797,
      "learning_rate": 2.0137321451311572e-07,
      "loss": 1.7087,
      "step": 246848
    },
    {
      "epoch": 0.0008959765167051861,
      "grad_norm": 11472.86119501147,
      "learning_rate": 2.0136015031292755e-07,
      "loss": 1.7056,
      "step": 246880
    },
    {
      "epoch": 0.0008960926510560227,
      "grad_norm": 8117.490252534954,
      "learning_rate": 2.0134708865505149e-07,
      "loss": 1.6837,
      "step": 246912
    },
    {
      "epoch": 0.0008962087854068595,
      "grad_norm": 9630.33270453311,
      "learning_rate": 2.0133402953866303e-07,
      "loss": 1.7066,
      "step": 246944
    },
    {
      "epoch": 0.0008963249197576961,
      "grad_norm": 8738.638452299076,
      "learning_rate": 2.0132097296293811e-07,
      "loss": 1.7004,
      "step": 246976
    },
    {
      "epoch": 0.0008964410541085329,
      "grad_norm": 8920.111658493968,
      "learning_rate": 2.0130832682723774e-07,
      "loss": 1.7169,
      "step": 247008
    },
    {
      "epoch": 0.0008965571884593695,
      "grad_norm": 8155.210359028147,
      "learning_rate": 2.0129527525103735e-07,
      "loss": 1.7272,
      "step": 247040
    },
    {
      "epoch": 0.0008966733228102063,
      "grad_norm": 10065.158915784688,
      "learning_rate": 2.0128222621305617e-07,
      "loss": 1.7155,
      "step": 247072
    },
    {
      "epoch": 0.0008967894571610429,
      "grad_norm": 9751.464710493496,
      "learning_rate": 2.0126917971247161e-07,
      "loss": 1.7162,
      "step": 247104
    },
    {
      "epoch": 0.0008969055915118797,
      "grad_norm": 12172.372817162643,
      "learning_rate": 2.012561357484615e-07,
      "loss": 1.6991,
      "step": 247136
    },
    {
      "epoch": 0.0008970217258627164,
      "grad_norm": 8708.190971723117,
      "learning_rate": 2.0124309432020394e-07,
      "loss": 1.6959,
      "step": 247168
    },
    {
      "epoch": 0.0008971378602135531,
      "grad_norm": 8612.049465719527,
      "learning_rate": 2.012300554268775e-07,
      "loss": 1.718,
      "step": 247200
    },
    {
      "epoch": 0.0008972539945643898,
      "grad_norm": 8597.145340169607,
      "learning_rate": 2.0121701906766103e-07,
      "loss": 1.7122,
      "step": 247232
    },
    {
      "epoch": 0.0008973701289152265,
      "grad_norm": 9924.98705288828,
      "learning_rate": 2.0120398524173378e-07,
      "loss": 1.7163,
      "step": 247264
    },
    {
      "epoch": 0.0008974862632660632,
      "grad_norm": 10466.436451820648,
      "learning_rate": 2.0119095394827544e-07,
      "loss": 1.741,
      "step": 247296
    },
    {
      "epoch": 0.0008976023976168999,
      "grad_norm": 8837.930526995558,
      "learning_rate": 2.01177925186466e-07,
      "loss": 1.7131,
      "step": 247328
    },
    {
      "epoch": 0.0008977185319677366,
      "grad_norm": 9062.716811199609,
      "learning_rate": 2.0116489895548582e-07,
      "loss": 1.7378,
      "step": 247360
    },
    {
      "epoch": 0.0008978346663185733,
      "grad_norm": 10279.332274034146,
      "learning_rate": 2.0115187525451566e-07,
      "loss": 1.7336,
      "step": 247392
    },
    {
      "epoch": 0.00089795080066941,
      "grad_norm": 8762.789852552667,
      "learning_rate": 2.0113885408273665e-07,
      "loss": 1.6988,
      "step": 247424
    },
    {
      "epoch": 0.0008980669350202468,
      "grad_norm": 7912.175048619691,
      "learning_rate": 2.011258354393303e-07,
      "loss": 1.7037,
      "step": 247456
    },
    {
      "epoch": 0.0008981830693710834,
      "grad_norm": 9019.694784193089,
      "learning_rate": 2.0111281932347846e-07,
      "loss": 1.7136,
      "step": 247488
    },
    {
      "epoch": 0.0008982992037219202,
      "grad_norm": 9271.259029926841,
      "learning_rate": 2.0109980573436335e-07,
      "loss": 1.7121,
      "step": 247520
    },
    {
      "epoch": 0.0008984153380727568,
      "grad_norm": 9065.673278913155,
      "learning_rate": 2.0108679467116763e-07,
      "loss": 1.7288,
      "step": 247552
    },
    {
      "epoch": 0.0008985314724235936,
      "grad_norm": 8837.778793339421,
      "learning_rate": 2.010737861330742e-07,
      "loss": 1.7123,
      "step": 247584
    },
    {
      "epoch": 0.0008986476067744302,
      "grad_norm": 10035.582494304952,
      "learning_rate": 2.0106078011926647e-07,
      "loss": 1.7075,
      "step": 247616
    },
    {
      "epoch": 0.000898763741125267,
      "grad_norm": 8847.26895714152,
      "learning_rate": 2.0104777662892813e-07,
      "loss": 1.7407,
      "step": 247648
    },
    {
      "epoch": 0.0008988798754761036,
      "grad_norm": 9394.519891937001,
      "learning_rate": 2.010347756612433e-07,
      "loss": 1.7167,
      "step": 247680
    },
    {
      "epoch": 0.0008989960098269404,
      "grad_norm": 10743.932799492,
      "learning_rate": 2.0102177721539637e-07,
      "loss": 1.7162,
      "step": 247712
    },
    {
      "epoch": 0.0008991121441777771,
      "grad_norm": 11121.247771720582,
      "learning_rate": 2.010087812905722e-07,
      "loss": 1.7029,
      "step": 247744
    },
    {
      "epoch": 0.0008992282785286138,
      "grad_norm": 8943.516757964957,
      "learning_rate": 2.00995787885956e-07,
      "loss": 1.6905,
      "step": 247776
    },
    {
      "epoch": 0.0008993444128794505,
      "grad_norm": 9248.657199831769,
      "learning_rate": 2.009827970007333e-07,
      "loss": 1.6922,
      "step": 247808
    },
    {
      "epoch": 0.0008994605472302872,
      "grad_norm": 9597.787661747889,
      "learning_rate": 2.0096980863409005e-07,
      "loss": 1.6988,
      "step": 247840
    },
    {
      "epoch": 0.0008995766815811239,
      "grad_norm": 10376.447754409985,
      "learning_rate": 2.0095682278521252e-07,
      "loss": 1.7133,
      "step": 247872
    },
    {
      "epoch": 0.0008996928159319606,
      "grad_norm": 9792.867812852372,
      "learning_rate": 2.0094383945328743e-07,
      "loss": 1.7155,
      "step": 247904
    },
    {
      "epoch": 0.0008998089502827973,
      "grad_norm": 8339.246728572072,
      "learning_rate": 2.0093085863750177e-07,
      "loss": 1.706,
      "step": 247936
    },
    {
      "epoch": 0.000899925084633634,
      "grad_norm": 9054.367564882707,
      "learning_rate": 2.0091788033704297e-07,
      "loss": 1.6893,
      "step": 247968
    },
    {
      "epoch": 0.0009000412189844707,
      "grad_norm": 20889.17422972962,
      "learning_rate": 2.0090490455109878e-07,
      "loss": 1.706,
      "step": 248000
    },
    {
      "epoch": 0.0009001573533353075,
      "grad_norm": 9856.98331133821,
      "learning_rate": 2.0089233665557372e-07,
      "loss": 1.6928,
      "step": 248032
    },
    {
      "epoch": 0.0009002734876861441,
      "grad_norm": 8908.39974406178,
      "learning_rate": 2.0087936581770796e-07,
      "loss": 1.7132,
      "step": 248064
    },
    {
      "epoch": 0.0009003896220369809,
      "grad_norm": 10935.407079757022,
      "learning_rate": 2.0086639749194766e-07,
      "loss": 1.7158,
      "step": 248096
    },
    {
      "epoch": 0.0009005057563878175,
      "grad_norm": 9668.03040955085,
      "learning_rate": 2.0085343167748205e-07,
      "loss": 1.7046,
      "step": 248128
    },
    {
      "epoch": 0.0009006218907386543,
      "grad_norm": 8807.167876224456,
      "learning_rate": 2.0084046837350073e-07,
      "loss": 1.7331,
      "step": 248160
    },
    {
      "epoch": 0.0009007380250894909,
      "grad_norm": 8547.120099776299,
      "learning_rate": 2.0082750757919363e-07,
      "loss": 1.7337,
      "step": 248192
    },
    {
      "epoch": 0.0009008541594403277,
      "grad_norm": 9704.884234239995,
      "learning_rate": 2.0081454929375113e-07,
      "loss": 1.7429,
      "step": 248224
    },
    {
      "epoch": 0.0009009702937911643,
      "grad_norm": 8446.492644879294,
      "learning_rate": 2.0080159351636384e-07,
      "loss": 1.757,
      "step": 248256
    },
    {
      "epoch": 0.0009010864281420011,
      "grad_norm": 9536.339549323944,
      "learning_rate": 2.0078864024622287e-07,
      "loss": 1.7332,
      "step": 248288
    },
    {
      "epoch": 0.0009012025624928378,
      "grad_norm": 9647.112417713395,
      "learning_rate": 2.0077568948251963e-07,
      "loss": 1.698,
      "step": 248320
    },
    {
      "epoch": 0.0009013186968436745,
      "grad_norm": 9882.67372728656,
      "learning_rate": 2.007627412244459e-07,
      "loss": 1.718,
      "step": 248352
    },
    {
      "epoch": 0.0009014348311945112,
      "grad_norm": 8570.227418219425,
      "learning_rate": 2.0074979547119383e-07,
      "loss": 1.6964,
      "step": 248384
    },
    {
      "epoch": 0.0009015509655453479,
      "grad_norm": 9173.282291524665,
      "learning_rate": 2.0073685222195593e-07,
      "loss": 1.7118,
      "step": 248416
    },
    {
      "epoch": 0.0009016670998961846,
      "grad_norm": 9683.360367145282,
      "learning_rate": 2.0072391147592506e-07,
      "loss": 1.7179,
      "step": 248448
    },
    {
      "epoch": 0.0009017832342470213,
      "grad_norm": 10730.62477211835,
      "learning_rate": 2.0071097323229454e-07,
      "loss": 1.6982,
      "step": 248480
    },
    {
      "epoch": 0.000901899368597858,
      "grad_norm": 11116.061712675042,
      "learning_rate": 2.0069803749025787e-07,
      "loss": 1.705,
      "step": 248512
    },
    {
      "epoch": 0.0009020155029486947,
      "grad_norm": 9777.409677414566,
      "learning_rate": 2.0068510424900908e-07,
      "loss": 1.7153,
      "step": 248544
    },
    {
      "epoch": 0.0009021316372995314,
      "grad_norm": 8693.761901501559,
      "learning_rate": 2.0067217350774252e-07,
      "loss": 1.6901,
      "step": 248576
    },
    {
      "epoch": 0.0009022477716503682,
      "grad_norm": 8673.474736228844,
      "learning_rate": 2.006592452656528e-07,
      "loss": 1.7146,
      "step": 248608
    },
    {
      "epoch": 0.0009023639060012048,
      "grad_norm": 10338.324815945763,
      "learning_rate": 2.006463195219351e-07,
      "loss": 1.7061,
      "step": 248640
    },
    {
      "epoch": 0.0009024800403520416,
      "grad_norm": 12382.044419238691,
      "learning_rate": 2.006333962757848e-07,
      "loss": 1.685,
      "step": 248672
    },
    {
      "epoch": 0.0009025961747028782,
      "grad_norm": 8882.490979449401,
      "learning_rate": 2.006204755263976e-07,
      "loss": 1.7112,
      "step": 248704
    },
    {
      "epoch": 0.000902712309053715,
      "grad_norm": 11513.233081980057,
      "learning_rate": 2.0060755727296976e-07,
      "loss": 1.6899,
      "step": 248736
    },
    {
      "epoch": 0.0009028284434045516,
      "grad_norm": 9830.678918569154,
      "learning_rate": 2.005946415146978e-07,
      "loss": 1.7159,
      "step": 248768
    },
    {
      "epoch": 0.0009029445777553884,
      "grad_norm": 9120.176971967156,
      "learning_rate": 2.0058172825077849e-07,
      "loss": 1.7294,
      "step": 248800
    },
    {
      "epoch": 0.000903060712106225,
      "grad_norm": 9819.850915365263,
      "learning_rate": 2.005688174804091e-07,
      "loss": 1.7156,
      "step": 248832
    },
    {
      "epoch": 0.0009031768464570618,
      "grad_norm": 9473.862570250847,
      "learning_rate": 2.0055590920278726e-07,
      "loss": 1.7178,
      "step": 248864
    },
    {
      "epoch": 0.0009032929808078985,
      "grad_norm": 9569.394651700806,
      "learning_rate": 2.0054300341711093e-07,
      "loss": 1.7112,
      "step": 248896
    },
    {
      "epoch": 0.0009034091151587352,
      "grad_norm": 9049.916021709814,
      "learning_rate": 2.0053010012257837e-07,
      "loss": 1.7097,
      "step": 248928
    },
    {
      "epoch": 0.0009035252495095719,
      "grad_norm": 10851.68429323301,
      "learning_rate": 2.0051719931838833e-07,
      "loss": 1.7279,
      "step": 248960
    },
    {
      "epoch": 0.0009036413838604086,
      "grad_norm": 8370.340972744181,
      "learning_rate": 2.0050430100373982e-07,
      "loss": 1.7302,
      "step": 248992
    },
    {
      "epoch": 0.0009037575182112453,
      "grad_norm": 8769.435557662762,
      "learning_rate": 2.004918081347284e-07,
      "loss": 1.7283,
      "step": 249024
    },
    {
      "epoch": 0.000903873652562082,
      "grad_norm": 9144.418188162657,
      "learning_rate": 2.004789147190255e-07,
      "loss": 1.7536,
      "step": 249056
    },
    {
      "epoch": 0.0009039897869129187,
      "grad_norm": 11553.424773633142,
      "learning_rate": 2.0046602379048836e-07,
      "loss": 1.7249,
      "step": 249088
    },
    {
      "epoch": 0.0009041059212637554,
      "grad_norm": 8562.920062688896,
      "learning_rate": 2.0045313534831754e-07,
      "loss": 1.7482,
      "step": 249120
    },
    {
      "epoch": 0.0009042220556145921,
      "grad_norm": 11136.640426986947,
      "learning_rate": 2.004402493917138e-07,
      "loss": 1.7373,
      "step": 249152
    },
    {
      "epoch": 0.0009043381899654289,
      "grad_norm": 10216.127642115674,
      "learning_rate": 2.0042736591987836e-07,
      "loss": 1.7042,
      "step": 249184
    },
    {
      "epoch": 0.0009044543243162655,
      "grad_norm": 9978.105431393276,
      "learning_rate": 2.0041448493201276e-07,
      "loss": 1.711,
      "step": 249216
    },
    {
      "epoch": 0.0009045704586671023,
      "grad_norm": 9210.166013704638,
      "learning_rate": 2.0040160642731893e-07,
      "loss": 1.7142,
      "step": 249248
    },
    {
      "epoch": 0.0009046865930179389,
      "grad_norm": 9589.200175197095,
      "learning_rate": 2.0038873040499912e-07,
      "loss": 1.7109,
      "step": 249280
    },
    {
      "epoch": 0.0009048027273687757,
      "grad_norm": 8614.07302035454,
      "learning_rate": 2.0037585686425596e-07,
      "loss": 1.7227,
      "step": 249312
    },
    {
      "epoch": 0.0009049188617196123,
      "grad_norm": 11700.93090313758,
      "learning_rate": 2.0036298580429244e-07,
      "loss": 1.7197,
      "step": 249344
    },
    {
      "epoch": 0.0009050349960704491,
      "grad_norm": 9016.130877488415,
      "learning_rate": 2.003501172243119e-07,
      "loss": 1.717,
      "step": 249376
    },
    {
      "epoch": 0.0009051511304212857,
      "grad_norm": 9507.438982186528,
      "learning_rate": 2.00337251123518e-07,
      "loss": 1.7569,
      "step": 249408
    },
    {
      "epoch": 0.0009052672647721225,
      "grad_norm": 10746.351566927262,
      "learning_rate": 2.0032438750111487e-07,
      "loss": 1.7247,
      "step": 249440
    },
    {
      "epoch": 0.0009053833991229592,
      "grad_norm": 10068.975717519632,
      "learning_rate": 2.003115263563069e-07,
      "loss": 1.73,
      "step": 249472
    },
    {
      "epoch": 0.0009054995334737959,
      "grad_norm": 11248.465317544433,
      "learning_rate": 2.0029866768829884e-07,
      "loss": 1.716,
      "step": 249504
    },
    {
      "epoch": 0.0009056156678246326,
      "grad_norm": 10443.782360811623,
      "learning_rate": 2.0028581149629585e-07,
      "loss": 1.6848,
      "step": 249536
    },
    {
      "epoch": 0.0009057318021754693,
      "grad_norm": 8450.008875734984,
      "learning_rate": 2.0027295777950342e-07,
      "loss": 1.6879,
      "step": 249568
    },
    {
      "epoch": 0.000905847936526306,
      "grad_norm": 9898.644351627147,
      "learning_rate": 2.0026010653712737e-07,
      "loss": 1.7022,
      "step": 249600
    },
    {
      "epoch": 0.0009059640708771427,
      "grad_norm": 9141.571418525373,
      "learning_rate": 2.0024725776837393e-07,
      "loss": 1.7024,
      "step": 249632
    },
    {
      "epoch": 0.0009060802052279794,
      "grad_norm": 10055.748405762746,
      "learning_rate": 2.0023441147244962e-07,
      "loss": 1.7093,
      "step": 249664
    },
    {
      "epoch": 0.0009061963395788161,
      "grad_norm": 8890.755873377697,
      "learning_rate": 2.002215676485614e-07,
      "loss": 1.6948,
      "step": 249696
    },
    {
      "epoch": 0.0009063124739296528,
      "grad_norm": 8175.276631405203,
      "learning_rate": 2.0020872629591652e-07,
      "loss": 1.6821,
      "step": 249728
    },
    {
      "epoch": 0.0009064286082804896,
      "grad_norm": 9176.692214518258,
      "learning_rate": 2.0019588741372262e-07,
      "loss": 1.7111,
      "step": 249760
    },
    {
      "epoch": 0.0009065447426313262,
      "grad_norm": 9500.259891181924,
      "learning_rate": 2.0018305100118763e-07,
      "loss": 1.6963,
      "step": 249792
    },
    {
      "epoch": 0.000906660876982163,
      "grad_norm": 11171.54778891448,
      "learning_rate": 2.0017021705752e-07,
      "loss": 1.7125,
      "step": 249824
    },
    {
      "epoch": 0.0009067770113329996,
      "grad_norm": 10185.814645869028,
      "learning_rate": 2.001573855819283e-07,
      "loss": 1.7188,
      "step": 249856
    },
    {
      "epoch": 0.0009068931456838364,
      "grad_norm": 9687.636966773682,
      "learning_rate": 2.0014455657362162e-07,
      "loss": 1.7001,
      "step": 249888
    },
    {
      "epoch": 0.000907009280034673,
      "grad_norm": 8749.17321808181,
      "learning_rate": 2.0013173003180938e-07,
      "loss": 1.7257,
      "step": 249920
    },
    {
      "epoch": 0.0009071254143855098,
      "grad_norm": 9421.632024230197,
      "learning_rate": 2.0011890595570134e-07,
      "loss": 1.734,
      "step": 249952
    },
    {
      "epoch": 0.0009072415487363464,
      "grad_norm": 10024.630866021951,
      "learning_rate": 2.0010608434450761e-07,
      "loss": 1.7469,
      "step": 249984
    },
    {
      "epoch": 0.0009073576830871832,
      "grad_norm": 11236.505150624014,
      "learning_rate": 2.0009366575849362e-07,
      "loss": 1.7509,
      "step": 250016
    },
    {
      "epoch": 0.00090747381743802,
      "grad_norm": 9088.842610585794,
      "learning_rate": 2.0008084899779296e-07,
      "loss": 1.7243,
      "step": 250048
    },
    {
      "epoch": 0.0009075899517888566,
      "grad_norm": 9424.38623996279,
      "learning_rate": 2.0006803469966373e-07,
      "loss": 1.6846,
      "step": 250080
    },
    {
      "epoch": 0.0009077060861396933,
      "grad_norm": 9339.154565591041,
      "learning_rate": 2.0005522286331738e-07,
      "loss": 1.7054,
      "step": 250112
    },
    {
      "epoch": 0.00090782222049053,
      "grad_norm": 10787.01385926615,
      "learning_rate": 2.0004241348796584e-07,
      "loss": 1.6913,
      "step": 250144
    },
    {
      "epoch": 0.0009079383548413667,
      "grad_norm": 10098.76101311443,
      "learning_rate": 2.0002960657282133e-07,
      "loss": 1.7065,
      "step": 250176
    },
    {
      "epoch": 0.0009080544891922034,
      "grad_norm": 8980.864657704178,
      "learning_rate": 2.000168021170964e-07,
      "loss": 1.7122,
      "step": 250208
    },
    {
      "epoch": 0.0009081706235430401,
      "grad_norm": 11430.977910922582,
      "learning_rate": 2.0000400012000402e-07,
      "loss": 1.693,
      "step": 250240
    },
    {
      "epoch": 0.0009082867578938768,
      "grad_norm": 10372.25529959613,
      "learning_rate": 1.999912005807574e-07,
      "loss": 1.7103,
      "step": 250272
    },
    {
      "epoch": 0.0009084028922447135,
      "grad_norm": 8017.559354317248,
      "learning_rate": 1.999784034985703e-07,
      "loss": 1.7073,
      "step": 250304
    },
    {
      "epoch": 0.0009085190265955503,
      "grad_norm": 10393.056528278868,
      "learning_rate": 1.9996560887265655e-07,
      "loss": 1.7001,
      "step": 250336
    },
    {
      "epoch": 0.0009086351609463869,
      "grad_norm": 10669.40588786461,
      "learning_rate": 1.9995281670223059e-07,
      "loss": 1.7061,
      "step": 250368
    },
    {
      "epoch": 0.0009087512952972237,
      "grad_norm": 11828.096211986105,
      "learning_rate": 1.999400269865071e-07,
      "loss": 1.7002,
      "step": 250400
    },
    {
      "epoch": 0.0009088674296480603,
      "grad_norm": 10420.011516308416,
      "learning_rate": 1.999272397247011e-07,
      "loss": 1.6865,
      "step": 250432
    },
    {
      "epoch": 0.0009089835639988971,
      "grad_norm": 11058.26803798859,
      "learning_rate": 1.9991445491602797e-07,
      "loss": 1.7061,
      "step": 250464
    },
    {
      "epoch": 0.0009090996983497337,
      "grad_norm": 9283.52626968869,
      "learning_rate": 1.999016725597035e-07,
      "loss": 1.6916,
      "step": 250496
    },
    {
      "epoch": 0.0009092158327005705,
      "grad_norm": 10349.420853361797,
      "learning_rate": 1.9988889265494374e-07,
      "loss": 1.7151,
      "step": 250528
    },
    {
      "epoch": 0.0009093319670514071,
      "grad_norm": 8590.226772326794,
      "learning_rate": 1.9987611520096516e-07,
      "loss": 1.7229,
      "step": 250560
    },
    {
      "epoch": 0.0009094481014022439,
      "grad_norm": 11162.776536328227,
      "learning_rate": 1.9986334019698454e-07,
      "loss": 1.7091,
      "step": 250592
    },
    {
      "epoch": 0.0009095642357530807,
      "grad_norm": 10259.353195986578,
      "learning_rate": 1.9985056764221906e-07,
      "loss": 1.7157,
      "step": 250624
    },
    {
      "epoch": 0.0009096803701039173,
      "grad_norm": 8308.003610976586,
      "learning_rate": 1.998377975358862e-07,
      "loss": 1.6986,
      "step": 250656
    },
    {
      "epoch": 0.000909796504454754,
      "grad_norm": 9189.499007018827,
      "learning_rate": 1.9982502987720376e-07,
      "loss": 1.698,
      "step": 250688
    },
    {
      "epoch": 0.0009099126388055907,
      "grad_norm": 10162.955672440965,
      "learning_rate": 1.9981226466539e-07,
      "loss": 1.7109,
      "step": 250720
    },
    {
      "epoch": 0.0009100287731564275,
      "grad_norm": 9240.250213062414,
      "learning_rate": 1.9979950189966346e-07,
      "loss": 1.7064,
      "step": 250752
    },
    {
      "epoch": 0.0009101449075072641,
      "grad_norm": 10962.571413678454,
      "learning_rate": 1.99786741579243e-07,
      "loss": 1.7163,
      "step": 250784
    },
    {
      "epoch": 0.0009102610418581009,
      "grad_norm": 10418.609696115887,
      "learning_rate": 1.997739837033479e-07,
      "loss": 1.733,
      "step": 250816
    },
    {
      "epoch": 0.0009103771762089375,
      "grad_norm": 8883.347792358465,
      "learning_rate": 1.9976122827119773e-07,
      "loss": 1.7113,
      "step": 250848
    },
    {
      "epoch": 0.0009104933105597743,
      "grad_norm": 9753.891531076199,
      "learning_rate": 1.9974847528201242e-07,
      "loss": 1.73,
      "step": 250880
    },
    {
      "epoch": 0.000910609444910611,
      "grad_norm": 9060.876778767053,
      "learning_rate": 1.9973572473501232e-07,
      "loss": 1.7133,
      "step": 250912
    },
    {
      "epoch": 0.0009107255792614477,
      "grad_norm": 9483.311868751338,
      "learning_rate": 1.99722976629418e-07,
      "loss": 1.688,
      "step": 250944
    },
    {
      "epoch": 0.0009108417136122844,
      "grad_norm": 9643.839795434182,
      "learning_rate": 1.997102309644505e-07,
      "loss": 1.7026,
      "step": 250976
    },
    {
      "epoch": 0.000910957847963121,
      "grad_norm": 10308.823987245101,
      "learning_rate": 1.996974877393311e-07,
      "loss": 1.6993,
      "step": 251008
    },
    {
      "epoch": 0.0009110739823139578,
      "grad_norm": 9485.492712558478,
      "learning_rate": 1.9968514506593384e-07,
      "loss": 1.7122,
      "step": 251040
    },
    {
      "epoch": 0.0009111901166647945,
      "grad_norm": 11286.440182803433,
      "learning_rate": 1.9967240664199124e-07,
      "loss": 1.7019,
      "step": 251072
    },
    {
      "epoch": 0.0009113062510156312,
      "grad_norm": 8179.951711348912,
      "learning_rate": 1.9965967065558722e-07,
      "loss": 1.704,
      "step": 251104
    },
    {
      "epoch": 0.0009114223853664679,
      "grad_norm": 10949.06187762221,
      "learning_rate": 1.9964693710594442e-07,
      "loss": 1.7069,
      "step": 251136
    },
    {
      "epoch": 0.0009115385197173046,
      "grad_norm": 10365.320448495551,
      "learning_rate": 1.9963420599228592e-07,
      "loss": 1.7361,
      "step": 251168
    },
    {
      "epoch": 0.0009116546540681414,
      "grad_norm": 11397.369345599009,
      "learning_rate": 1.996214773138351e-07,
      "loss": 1.7064,
      "step": 251200
    },
    {
      "epoch": 0.000911770788418978,
      "grad_norm": 8945.337780095282,
      "learning_rate": 1.9960875106981576e-07,
      "loss": 1.7191,
      "step": 251232
    },
    {
      "epoch": 0.0009118869227698148,
      "grad_norm": 10001.40210170554,
      "learning_rate": 1.9959602725945196e-07,
      "loss": 1.7051,
      "step": 251264
    },
    {
      "epoch": 0.0009120030571206514,
      "grad_norm": 9071.874337754023,
      "learning_rate": 1.995833058819682e-07,
      "loss": 1.6873,
      "step": 251296
    },
    {
      "epoch": 0.0009121191914714882,
      "grad_norm": 10670.929294114923,
      "learning_rate": 1.9957058693658923e-07,
      "loss": 1.6985,
      "step": 251328
    },
    {
      "epoch": 0.0009122353258223248,
      "grad_norm": 9981.184999788353,
      "learning_rate": 1.995578704225402e-07,
      "loss": 1.6965,
      "step": 251360
    },
    {
      "epoch": 0.0009123514601731616,
      "grad_norm": 9463.833367087567,
      "learning_rate": 1.9954515633904658e-07,
      "loss": 1.7185,
      "step": 251392
    },
    {
      "epoch": 0.0009124675945239982,
      "grad_norm": 9466.631713550496,
      "learning_rate": 1.9953244468533424e-07,
      "loss": 1.7216,
      "step": 251424
    },
    {
      "epoch": 0.000912583728874835,
      "grad_norm": 10105.2633810307,
      "learning_rate": 1.995197354606293e-07,
      "loss": 1.6984,
      "step": 251456
    },
    {
      "epoch": 0.0009126998632256717,
      "grad_norm": 9782.247798946824,
      "learning_rate": 1.995070286641583e-07,
      "loss": 1.6956,
      "step": 251488
    },
    {
      "epoch": 0.0009128159975765084,
      "grad_norm": 9740.124639859594,
      "learning_rate": 1.9949432429514816e-07,
      "loss": 1.7073,
      "step": 251520
    },
    {
      "epoch": 0.0009129321319273451,
      "grad_norm": 10304.774621504344,
      "learning_rate": 1.9948162235282603e-07,
      "loss": 1.6979,
      "step": 251552
    },
    {
      "epoch": 0.0009130482662781818,
      "grad_norm": 9387.71196831262,
      "learning_rate": 1.9946892283641947e-07,
      "loss": 1.7241,
      "step": 251584
    },
    {
      "epoch": 0.0009131644006290185,
      "grad_norm": 11308.990052166462,
      "learning_rate": 1.9945622574515638e-07,
      "loss": 1.7208,
      "step": 251616
    },
    {
      "epoch": 0.0009132805349798552,
      "grad_norm": 9000.716415930456,
      "learning_rate": 1.99443531078265e-07,
      "loss": 1.708,
      "step": 251648
    },
    {
      "epoch": 0.0009133966693306919,
      "grad_norm": 9306.275194727481,
      "learning_rate": 1.9943083883497393e-07,
      "loss": 1.7299,
      "step": 251680
    },
    {
      "epoch": 0.0009135128036815286,
      "grad_norm": 12015.803427153758,
      "learning_rate": 1.994181490145121e-07,
      "loss": 1.727,
      "step": 251712
    },
    {
      "epoch": 0.0009136289380323653,
      "grad_norm": 9364.684938640487,
      "learning_rate": 1.9940546161610877e-07,
      "loss": 1.7498,
      "step": 251744
    },
    {
      "epoch": 0.0009137450723832021,
      "grad_norm": 8230.33061304344,
      "learning_rate": 1.9939277663899355e-07,
      "loss": 1.7484,
      "step": 251776
    },
    {
      "epoch": 0.0009138612067340387,
      "grad_norm": 9302.091162743998,
      "learning_rate": 1.9938009408239643e-07,
      "loss": 1.7126,
      "step": 251808
    },
    {
      "epoch": 0.0009139773410848755,
      "grad_norm": 8757.91687560461,
      "learning_rate": 1.993674139455477e-07,
      "loss": 1.6942,
      "step": 251840
    },
    {
      "epoch": 0.0009140934754357121,
      "grad_norm": 10134.226758860294,
      "learning_rate": 1.9935473622767798e-07,
      "loss": 1.7025,
      "step": 251872
    },
    {
      "epoch": 0.0009142096097865489,
      "grad_norm": 8537.103724331806,
      "learning_rate": 1.9934206092801826e-07,
      "loss": 1.7022,
      "step": 251904
    },
    {
      "epoch": 0.0009143257441373855,
      "grad_norm": 10679.644750645968,
      "learning_rate": 1.993293880457999e-07,
      "loss": 1.7082,
      "step": 251936
    },
    {
      "epoch": 0.0009144418784882223,
      "grad_norm": 9207.485867488474,
      "learning_rate": 1.9931671758025454e-07,
      "loss": 1.7084,
      "step": 251968
    },
    {
      "epoch": 0.0009145580128390589,
      "grad_norm": 9591.84049075046,
      "learning_rate": 1.9930404953061422e-07,
      "loss": 1.7081,
      "step": 252000
    },
    {
      "epoch": 0.0009146741471898957,
      "grad_norm": 19525.450468555136,
      "learning_rate": 1.9929138389611127e-07,
      "loss": 1.712,
      "step": 252032
    },
    {
      "epoch": 0.0009147902815407324,
      "grad_norm": 7944.8627426784415,
      "learning_rate": 1.9927911636506955e-07,
      "loss": 1.6975,
      "step": 252064
    },
    {
      "epoch": 0.0009149064158915691,
      "grad_norm": 9338.857210601305,
      "learning_rate": 1.9926645548312633e-07,
      "loss": 1.701,
      "step": 252096
    },
    {
      "epoch": 0.0009150225502424058,
      "grad_norm": 9609.692919131183,
      "learning_rate": 1.992537970140435e-07,
      "loss": 1.7055,
      "step": 252128
    },
    {
      "epoch": 0.0009151386845932425,
      "grad_norm": 9161.765877820717,
      "learning_rate": 1.9924114095705486e-07,
      "loss": 1.6913,
      "step": 252160
    },
    {
      "epoch": 0.0009152548189440792,
      "grad_norm": 11717.290471777167,
      "learning_rate": 1.9922848731139441e-07,
      "loss": 1.6929,
      "step": 252192
    },
    {
      "epoch": 0.0009153709532949159,
      "grad_norm": 9961.409739590075,
      "learning_rate": 1.9921583607629656e-07,
      "loss": 1.7066,
      "step": 252224
    },
    {
      "epoch": 0.0009154870876457526,
      "grad_norm": 8842.305581690784,
      "learning_rate": 1.9920318725099604e-07,
      "loss": 1.6983,
      "step": 252256
    },
    {
      "epoch": 0.0009156032219965893,
      "grad_norm": 10379.732944541493,
      "learning_rate": 1.991905408347279e-07,
      "loss": 1.7253,
      "step": 252288
    },
    {
      "epoch": 0.000915719356347426,
      "grad_norm": 8786.699607930159,
      "learning_rate": 1.9917789682672756e-07,
      "loss": 1.7197,
      "step": 252320
    },
    {
      "epoch": 0.0009158354906982628,
      "grad_norm": 9885.146028258763,
      "learning_rate": 1.9916525522623077e-07,
      "loss": 1.7104,
      "step": 252352
    },
    {
      "epoch": 0.0009159516250490994,
      "grad_norm": 9104.518109158771,
      "learning_rate": 1.991526160324736e-07,
      "loss": 1.7289,
      "step": 252384
    },
    {
      "epoch": 0.0009160677593999362,
      "grad_norm": 8948.001788108895,
      "learning_rate": 1.9913997924469252e-07,
      "loss": 1.7173,
      "step": 252416
    },
    {
      "epoch": 0.0009161838937507728,
      "grad_norm": 10430.404210767672,
      "learning_rate": 1.9912734486212425e-07,
      "loss": 1.7159,
      "step": 252448
    },
    {
      "epoch": 0.0009163000281016096,
      "grad_norm": 9517.678813660397,
      "learning_rate": 1.9911471288400597e-07,
      "loss": 1.7328,
      "step": 252480
    },
    {
      "epoch": 0.0009164161624524462,
      "grad_norm": 9776.692180896358,
      "learning_rate": 1.9910208330957502e-07,
      "loss": 1.7163,
      "step": 252512
    },
    {
      "epoch": 0.000916532296803283,
      "grad_norm": 8824.018585655858,
      "learning_rate": 1.9908945613806929e-07,
      "loss": 1.7126,
      "step": 252544
    },
    {
      "epoch": 0.0009166484311541196,
      "grad_norm": 9151.577350380643,
      "learning_rate": 1.9907683136872683e-07,
      "loss": 1.7388,
      "step": 252576
    },
    {
      "epoch": 0.0009167645655049564,
      "grad_norm": 8962.588465393243,
      "learning_rate": 1.9906420900078613e-07,
      "loss": 1.7146,
      "step": 252608
    },
    {
      "epoch": 0.0009168806998557931,
      "grad_norm": 8621.703775936634,
      "learning_rate": 1.9905158903348598e-07,
      "loss": 1.7379,
      "step": 252640
    },
    {
      "epoch": 0.0009169968342066298,
      "grad_norm": 12088.832036222522,
      "learning_rate": 1.990389714660655e-07,
      "loss": 1.7028,
      "step": 252672
    },
    {
      "epoch": 0.0009171129685574665,
      "grad_norm": 9782.26129276866,
      "learning_rate": 1.9902635629776418e-07,
      "loss": 1.6858,
      "step": 252704
    },
    {
      "epoch": 0.0009172291029083032,
      "grad_norm": 8706.642751370933,
      "learning_rate": 1.9901374352782186e-07,
      "loss": 1.6993,
      "step": 252736
    },
    {
      "epoch": 0.0009173452372591399,
      "grad_norm": 10394.37290075741,
      "learning_rate": 1.9900113315547864e-07,
      "loss": 1.6858,
      "step": 252768
    },
    {
      "epoch": 0.0009174613716099766,
      "grad_norm": 9085.530144135784,
      "learning_rate": 1.9898852517997503e-07,
      "loss": 1.7097,
      "step": 252800
    },
    {
      "epoch": 0.0009175775059608133,
      "grad_norm": 8859.739612426541,
      "learning_rate": 1.9897591960055183e-07,
      "loss": 1.7109,
      "step": 252832
    },
    {
      "epoch": 0.00091769364031165,
      "grad_norm": 8608.971831757843,
      "learning_rate": 1.9896331641645022e-07,
      "loss": 1.7015,
      "step": 252864
    },
    {
      "epoch": 0.0009178097746624867,
      "grad_norm": 9463.478747268364,
      "learning_rate": 1.989507156269117e-07,
      "loss": 1.7141,
      "step": 252896
    },
    {
      "epoch": 0.0009179259090133235,
      "grad_norm": 9475.663776221696,
      "learning_rate": 1.989381172311781e-07,
      "loss": 1.7328,
      "step": 252928
    },
    {
      "epoch": 0.0009180420433641601,
      "grad_norm": 8847.328862430739,
      "learning_rate": 1.9892552122849153e-07,
      "loss": 1.7101,
      "step": 252960
    },
    {
      "epoch": 0.0009181581777149969,
      "grad_norm": 7990.988299328187,
      "learning_rate": 1.9891292761809458e-07,
      "loss": 1.7188,
      "step": 252992
    },
    {
      "epoch": 0.0009182743120658335,
      "grad_norm": 9365.363207051823,
      "learning_rate": 1.9890033639923002e-07,
      "loss": 1.7007,
      "step": 253024
    },
    {
      "epoch": 0.0009183904464166703,
      "grad_norm": 17355.352603735828,
      "learning_rate": 1.9888774757114106e-07,
      "loss": 1.6809,
      "step": 253056
    },
    {
      "epoch": 0.0009185065807675069,
      "grad_norm": 10894.107581624114,
      "learning_rate": 1.9887555442309138e-07,
      "loss": 1.7105,
      "step": 253088
    },
    {
      "epoch": 0.0009186227151183437,
      "grad_norm": 8836.770903446575,
      "learning_rate": 1.9886297029963144e-07,
      "loss": 1.6906,
      "step": 253120
    },
    {
      "epoch": 0.0009187388494691803,
      "grad_norm": 9925.526887777796,
      "learning_rate": 1.9885038856470227e-07,
      "loss": 1.7141,
      "step": 253152
    },
    {
      "epoch": 0.0009188549838200171,
      "grad_norm": 9532.165756007393,
      "learning_rate": 1.988378092175483e-07,
      "loss": 1.7219,
      "step": 253184
    },
    {
      "epoch": 0.0009189711181708537,
      "grad_norm": 9026.258914965823,
      "learning_rate": 1.9882523225741443e-07,
      "loss": 1.6959,
      "step": 253216
    },
    {
      "epoch": 0.0009190872525216905,
      "grad_norm": 9621.350840708388,
      "learning_rate": 1.9881265768354582e-07,
      "loss": 1.703,
      "step": 253248
    },
    {
      "epoch": 0.0009192033868725272,
      "grad_norm": 10357.88376069166,
      "learning_rate": 1.9880008549518798e-07,
      "loss": 1.713,
      "step": 253280
    },
    {
      "epoch": 0.0009193195212233639,
      "grad_norm": 9241.036305523316,
      "learning_rate": 1.987875156915867e-07,
      "loss": 1.711,
      "step": 253312
    },
    {
      "epoch": 0.0009194356555742006,
      "grad_norm": 8814.707936171226,
      "learning_rate": 1.987749482719882e-07,
      "loss": 1.7129,
      "step": 253344
    },
    {
      "epoch": 0.0009195517899250373,
      "grad_norm": 10150.605302148242,
      "learning_rate": 1.98762383235639e-07,
      "loss": 1.7119,
      "step": 253376
    },
    {
      "epoch": 0.000919667924275874,
      "grad_norm": 8904.09456373864,
      "learning_rate": 1.9874982058178592e-07,
      "loss": 1.7197,
      "step": 253408
    },
    {
      "epoch": 0.0009197840586267107,
      "grad_norm": 10510.782463736941,
      "learning_rate": 1.9873726030967612e-07,
      "loss": 1.7216,
      "step": 253440
    },
    {
      "epoch": 0.0009199001929775474,
      "grad_norm": 8447.029063522867,
      "learning_rate": 1.9872470241855715e-07,
      "loss": 1.7194,
      "step": 253472
    },
    {
      "epoch": 0.0009200163273283841,
      "grad_norm": 8862.414682240953,
      "learning_rate": 1.9871214690767681e-07,
      "loss": 1.7356,
      "step": 253504
    },
    {
      "epoch": 0.0009201324616792208,
      "grad_norm": 9637.392178385187,
      "learning_rate": 1.9869959377628328e-07,
      "loss": 1.7459,
      "step": 253536
    },
    {
      "epoch": 0.0009202485960300576,
      "grad_norm": 11380.283652000946,
      "learning_rate": 1.9868704302362508e-07,
      "loss": 1.7152,
      "step": 253568
    },
    {
      "epoch": 0.0009203647303808942,
      "grad_norm": 9624.621966602117,
      "learning_rate": 1.9867449464895105e-07,
      "loss": 1.7011,
      "step": 253600
    },
    {
      "epoch": 0.000920480864731731,
      "grad_norm": 11220.39197176284,
      "learning_rate": 1.9866194865151033e-07,
      "loss": 1.6977,
      "step": 253632
    },
    {
      "epoch": 0.0009205969990825676,
      "grad_norm": 12258.986254988624,
      "learning_rate": 1.9864940503055243e-07,
      "loss": 1.7003,
      "step": 253664
    },
    {
      "epoch": 0.0009207131334334044,
      "grad_norm": 8555.280240880482,
      "learning_rate": 1.986368637853272e-07,
      "loss": 1.7021,
      "step": 253696
    },
    {
      "epoch": 0.000920829267784241,
      "grad_norm": 8134.635087082887,
      "learning_rate": 1.986243249150848e-07,
      "loss": 1.7071,
      "step": 253728
    },
    {
      "epoch": 0.0009209454021350778,
      "grad_norm": 9910.271439269462,
      "learning_rate": 1.9861178841907572e-07,
      "loss": 1.7132,
      "step": 253760
    },
    {
      "epoch": 0.0009210615364859144,
      "grad_norm": 8963.538810090577,
      "learning_rate": 1.985992542965508e-07,
      "loss": 1.7177,
      "step": 253792
    },
    {
      "epoch": 0.0009211776708367512,
      "grad_norm": 11738.883422199915,
      "learning_rate": 1.9858672254676114e-07,
      "loss": 1.7009,
      "step": 253824
    },
    {
      "epoch": 0.0009212938051875879,
      "grad_norm": 8900.386957879977,
      "learning_rate": 1.9857419316895832e-07,
      "loss": 1.711,
      "step": 253856
    },
    {
      "epoch": 0.0009214099395384246,
      "grad_norm": 8490.48078732883,
      "learning_rate": 1.985616661623941e-07,
      "loss": 1.7152,
      "step": 253888
    },
    {
      "epoch": 0.0009215260738892613,
      "grad_norm": 9046.894715867982,
      "learning_rate": 1.9854914152632062e-07,
      "loss": 1.7024,
      "step": 253920
    },
    {
      "epoch": 0.000921642208240098,
      "grad_norm": 10472.273487643455,
      "learning_rate": 1.985366192599904e-07,
      "loss": 1.7064,
      "step": 253952
    },
    {
      "epoch": 0.0009217583425909347,
      "grad_norm": 8438.458864034357,
      "learning_rate": 1.9852409936265623e-07,
      "loss": 1.7151,
      "step": 253984
    },
    {
      "epoch": 0.0009218744769417714,
      "grad_norm": 9062.55615155018,
      "learning_rate": 1.9851158183357125e-07,
      "loss": 1.7093,
      "step": 254016
    },
    {
      "epoch": 0.0009219906112926081,
      "grad_norm": 8790.531041979204,
      "learning_rate": 1.9849906667198894e-07,
      "loss": 1.7338,
      "step": 254048
    },
    {
      "epoch": 0.0009221067456434448,
      "grad_norm": 11353.90787350329,
      "learning_rate": 1.9848694486618386e-07,
      "loss": 1.7254,
      "step": 254080
    },
    {
      "epoch": 0.0009222228799942815,
      "grad_norm": 9614.382351456592,
      "learning_rate": 1.9847443436344203e-07,
      "loss": 1.7194,
      "step": 254112
    },
    {
      "epoch": 0.0009223390143451183,
      "grad_norm": 9172.866073370962,
      "learning_rate": 1.984619262259885e-07,
      "loss": 1.7371,
      "step": 254144
    },
    {
      "epoch": 0.0009224551486959549,
      "grad_norm": 10931.870837144024,
      "learning_rate": 1.9844942045307813e-07,
      "loss": 1.7201,
      "step": 254176
    },
    {
      "epoch": 0.0009225712830467917,
      "grad_norm": 9069.0521004127,
      "learning_rate": 1.9843691704396597e-07,
      "loss": 1.7264,
      "step": 254208
    },
    {
      "epoch": 0.0009226874173976283,
      "grad_norm": 10145.465982398246,
      "learning_rate": 1.9842441599790744e-07,
      "loss": 1.7296,
      "step": 254240
    },
    {
      "epoch": 0.0009228035517484651,
      "grad_norm": 8096.593234194243,
      "learning_rate": 1.984119173141583e-07,
      "loss": 1.7167,
      "step": 254272
    },
    {
      "epoch": 0.0009229196860993017,
      "grad_norm": 8908.268518629195,
      "learning_rate": 1.9839942099197467e-07,
      "loss": 1.7308,
      "step": 254304
    },
    {
      "epoch": 0.0009230358204501385,
      "grad_norm": 8559.407222465818,
      "learning_rate": 1.9838692703061295e-07,
      "loss": 1.746,
      "step": 254336
    },
    {
      "epoch": 0.0009231519548009751,
      "grad_norm": 10680.757276523045,
      "learning_rate": 1.9837443542932986e-07,
      "loss": 1.7325,
      "step": 254368
    },
    {
      "epoch": 0.0009232680891518119,
      "grad_norm": 11074.984243781117,
      "learning_rate": 1.983619461873825e-07,
      "loss": 1.7448,
      "step": 254400
    },
    {
      "epoch": 0.0009233842235026486,
      "grad_norm": 10729.320947758064,
      "learning_rate": 1.983494593040283e-07,
      "loss": 1.7178,
      "step": 254432
    },
    {
      "epoch": 0.0009235003578534853,
      "grad_norm": 10373.420458074568,
      "learning_rate": 1.9833697477852495e-07,
      "loss": 1.68,
      "step": 254464
    },
    {
      "epoch": 0.000923616492204322,
      "grad_norm": 8685.623408829098,
      "learning_rate": 1.9832449261013047e-07,
      "loss": 1.6973,
      "step": 254496
    },
    {
      "epoch": 0.0009237326265551587,
      "grad_norm": 9985.136353600787,
      "learning_rate": 1.983120127981033e-07,
      "loss": 1.6865,
      "step": 254528
    },
    {
      "epoch": 0.0009238487609059954,
      "grad_norm": 9542.493175266094,
      "learning_rate": 1.982995353417021e-07,
      "loss": 1.7032,
      "step": 254560
    },
    {
      "epoch": 0.0009239648952568321,
      "grad_norm": 9147.961193621231,
      "learning_rate": 1.9828706024018595e-07,
      "loss": 1.6999,
      "step": 254592
    },
    {
      "epoch": 0.0009240810296076688,
      "grad_norm": 8948.280058201128,
      "learning_rate": 1.9827458749281415e-07,
      "loss": 1.6922,
      "step": 254624
    },
    {
      "epoch": 0.0009241971639585055,
      "grad_norm": 12713.550723539038,
      "learning_rate": 1.9826211709884645e-07,
      "loss": 1.7129,
      "step": 254656
    },
    {
      "epoch": 0.0009243132983093422,
      "grad_norm": 9706.563346519715,
      "learning_rate": 1.9824964905754283e-07,
      "loss": 1.7204,
      "step": 254688
    },
    {
      "epoch": 0.000924429432660179,
      "grad_norm": 10966.897829377276,
      "learning_rate": 1.982371833681636e-07,
      "loss": 1.7175,
      "step": 254720
    },
    {
      "epoch": 0.0009245455670110156,
      "grad_norm": 8963.188272038025,
      "learning_rate": 1.9822472002996946e-07,
      "loss": 1.7227,
      "step": 254752
    },
    {
      "epoch": 0.0009246617013618524,
      "grad_norm": 11979.803170336314,
      "learning_rate": 1.982122590422214e-07,
      "loss": 1.6996,
      "step": 254784
    },
    {
      "epoch": 0.000924777835712689,
      "grad_norm": 8539.309573964396,
      "learning_rate": 1.9819980040418073e-07,
      "loss": 1.6812,
      "step": 254816
    },
    {
      "epoch": 0.0009248939700635258,
      "grad_norm": 9787.980588456436,
      "learning_rate": 1.9818734411510905e-07,
      "loss": 1.7082,
      "step": 254848
    },
    {
      "epoch": 0.0009250101044143624,
      "grad_norm": 9211.827614539907,
      "learning_rate": 1.9817489017426836e-07,
      "loss": 1.6902,
      "step": 254880
    },
    {
      "epoch": 0.0009251262387651992,
      "grad_norm": 10319.91472832988,
      "learning_rate": 1.9816243858092095e-07,
      "loss": 1.7256,
      "step": 254912
    },
    {
      "epoch": 0.0009252423731160358,
      "grad_norm": 8256.693648186301,
      "learning_rate": 1.9814998933432943e-07,
      "loss": 1.7206,
      "step": 254944
    },
    {
      "epoch": 0.0009253585074668726,
      "grad_norm": 8906.155736343262,
      "learning_rate": 1.981375424337567e-07,
      "loss": 1.6861,
      "step": 254976
    },
    {
      "epoch": 0.0009254746418177094,
      "grad_norm": 8653.864570236814,
      "learning_rate": 1.981250978784661e-07,
      "loss": 1.6937,
      "step": 255008
    },
    {
      "epoch": 0.000925590776168546,
      "grad_norm": 9920.546154320336,
      "learning_rate": 1.9811265566772114e-07,
      "loss": 1.693,
      "step": 255040
    },
    {
      "epoch": 0.0009257069105193828,
      "grad_norm": 19766.980143663826,
      "learning_rate": 1.9810021580078574e-07,
      "loss": 1.7045,
      "step": 255072
    },
    {
      "epoch": 0.0009258230448702194,
      "grad_norm": 11372.417509043536,
      "learning_rate": 1.9808816691408568e-07,
      "loss": 1.7144,
      "step": 255104
    },
    {
      "epoch": 0.0009259391792210562,
      "grad_norm": 10522.133814013201,
      "learning_rate": 1.9807573165937548e-07,
      "loss": 1.7012,
      "step": 255136
    },
    {
      "epoch": 0.0009260553135718928,
      "grad_norm": 10596.617007328328,
      "learning_rate": 1.9806329874629152e-07,
      "loss": 1.7147,
      "step": 255168
    },
    {
      "epoch": 0.0009261714479227296,
      "grad_norm": 10456.306422441912,
      "learning_rate": 1.9805086817409894e-07,
      "loss": 1.7271,
      "step": 255200
    },
    {
      "epoch": 0.0009262875822735662,
      "grad_norm": 10371.362880547571,
      "learning_rate": 1.9803843994206324e-07,
      "loss": 1.7168,
      "step": 255232
    },
    {
      "epoch": 0.000926403716624403,
      "grad_norm": 9828.074480792257,
      "learning_rate": 1.980260140494503e-07,
      "loss": 1.7425,
      "step": 255264
    },
    {
      "epoch": 0.0009265198509752397,
      "grad_norm": 10197.938026875825,
      "learning_rate": 1.9801359049552628e-07,
      "loss": 1.7388,
      "step": 255296
    },
    {
      "epoch": 0.0009266359853260764,
      "grad_norm": 8779.978132091219,
      "learning_rate": 1.9800116927955762e-07,
      "loss": 1.7015,
      "step": 255328
    },
    {
      "epoch": 0.0009267521196769131,
      "grad_norm": 12001.122780806803,
      "learning_rate": 1.9798875040081113e-07,
      "loss": 1.7044,
      "step": 255360
    },
    {
      "epoch": 0.0009268682540277498,
      "grad_norm": 8567.017333938342,
      "learning_rate": 1.97976333858554e-07,
      "loss": 1.6937,
      "step": 255392
    },
    {
      "epoch": 0.0009269843883785865,
      "grad_norm": 8580.145686408827,
      "learning_rate": 1.9796391965205357e-07,
      "loss": 1.7026,
      "step": 255424
    },
    {
      "epoch": 0.0009271005227294232,
      "grad_norm": 8364.448218501924,
      "learning_rate": 1.979515077805777e-07,
      "loss": 1.6986,
      "step": 255456
    },
    {
      "epoch": 0.0009272166570802599,
      "grad_norm": 9534.136772671136,
      "learning_rate": 1.979390982433944e-07,
      "loss": 1.6965,
      "step": 255488
    },
    {
      "epoch": 0.0009273327914310966,
      "grad_norm": 10327.199620419855,
      "learning_rate": 1.9792669103977215e-07,
      "loss": 1.7131,
      "step": 255520
    },
    {
      "epoch": 0.0009274489257819333,
      "grad_norm": 10938.417618650332,
      "learning_rate": 1.9791428616897967e-07,
      "loss": 1.7147,
      "step": 255552
    },
    {
      "epoch": 0.0009275650601327701,
      "grad_norm": 10025.436848337333,
      "learning_rate": 1.9790188363028597e-07,
      "loss": 1.6858,
      "step": 255584
    },
    {
      "epoch": 0.0009276811944836067,
      "grad_norm": 8646.09565063908,
      "learning_rate": 1.9788948342296046e-07,
      "loss": 1.6959,
      "step": 255616
    },
    {
      "epoch": 0.0009277973288344435,
      "grad_norm": 8867.479799807836,
      "learning_rate": 1.978770855462728e-07,
      "loss": 1.7039,
      "step": 255648
    },
    {
      "epoch": 0.0009279134631852801,
      "grad_norm": 8775.819505892314,
      "learning_rate": 1.9786468999949304e-07,
      "loss": 1.6853,
      "step": 255680
    },
    {
      "epoch": 0.0009280295975361169,
      "grad_norm": 9346.723062121826,
      "learning_rate": 1.978522967818915e-07,
      "loss": 1.6961,
      "step": 255712
    },
    {
      "epoch": 0.0009281457318869535,
      "grad_norm": 9649.266811525113,
      "learning_rate": 1.9783990589273882e-07,
      "loss": 1.6938,
      "step": 255744
    },
    {
      "epoch": 0.0009282618662377903,
      "grad_norm": 10159.983267702757,
      "learning_rate": 1.97827517331306e-07,
      "loss": 1.7008,
      "step": 255776
    },
    {
      "epoch": 0.0009283780005886269,
      "grad_norm": 8880.656957680552,
      "learning_rate": 1.978151310968643e-07,
      "loss": 1.7205,
      "step": 255808
    },
    {
      "epoch": 0.0009284941349394637,
      "grad_norm": 11208.188970569689,
      "learning_rate": 1.9780274718868535e-07,
      "loss": 1.703,
      "step": 255840
    },
    {
      "epoch": 0.0009286102692903004,
      "grad_norm": 9273.598222912184,
      "learning_rate": 1.977903656060411e-07,
      "loss": 1.7059,
      "step": 255872
    },
    {
      "epoch": 0.0009287264036411371,
      "grad_norm": 9475.193507258837,
      "learning_rate": 1.9777798634820372e-07,
      "loss": 1.7273,
      "step": 255904
    },
    {
      "epoch": 0.0009288425379919738,
      "grad_norm": 9699.518750948419,
      "learning_rate": 1.977656094144459e-07,
      "loss": 1.7233,
      "step": 255936
    },
    {
      "epoch": 0.0009289586723428105,
      "grad_norm": 10725.397335297186,
      "learning_rate": 1.9775323480404043e-07,
      "loss": 1.7153,
      "step": 255968
    },
    {
      "epoch": 0.0009290748066936472,
      "grad_norm": 13601.662986561605,
      "learning_rate": 1.9774086251626057e-07,
      "loss": 1.7151,
      "step": 256000
    },
    {
      "epoch": 0.0009291909410444839,
      "grad_norm": 11469.615861047832,
      "learning_rate": 1.9772849255037982e-07,
      "loss": 1.7097,
      "step": 256032
    },
    {
      "epoch": 0.0009293070753953206,
      "grad_norm": 9401.922994792076,
      "learning_rate": 1.97716124905672e-07,
      "loss": 1.7161,
      "step": 256064
    },
    {
      "epoch": 0.0009294232097461573,
      "grad_norm": 19504.063166427655,
      "learning_rate": 1.977037595814113e-07,
      "loss": 1.7218,
      "step": 256096
    },
    {
      "epoch": 0.000929539344096994,
      "grad_norm": 9966.842830104226,
      "learning_rate": 1.976917828856583e-07,
      "loss": 1.7242,
      "step": 256128
    },
    {
      "epoch": 0.0009296554784478308,
      "grad_norm": 8399.130669301438,
      "learning_rate": 1.9767942212765795e-07,
      "loss": 1.7238,
      "step": 256160
    },
    {
      "epoch": 0.0009297716127986674,
      "grad_norm": 9138.757245927916,
      "learning_rate": 1.976670636879517e-07,
      "loss": 1.6891,
      "step": 256192
    },
    {
      "epoch": 0.0009298877471495042,
      "grad_norm": 8245.187323523947,
      "learning_rate": 1.9765470756581507e-07,
      "loss": 1.6911,
      "step": 256224
    },
    {
      "epoch": 0.0009300038815003408,
      "grad_norm": 10083.418567132876,
      "learning_rate": 1.9764235376052373e-07,
      "loss": 1.7067,
      "step": 256256
    },
    {
      "epoch": 0.0009301200158511776,
      "grad_norm": 9610.05598318761,
      "learning_rate": 1.9763000227135374e-07,
      "loss": 1.6899,
      "step": 256288
    },
    {
      "epoch": 0.0009302361502020142,
      "grad_norm": 9414.800794493742,
      "learning_rate": 1.9761765309758148e-07,
      "loss": 1.7069,
      "step": 256320
    },
    {
      "epoch": 0.000930352284552851,
      "grad_norm": 8445.022676109284,
      "learning_rate": 1.9760530623848363e-07,
      "loss": 1.7079,
      "step": 256352
    },
    {
      "epoch": 0.0009304684189036876,
      "grad_norm": 9145.78219727542,
      "learning_rate": 1.9759296169333724e-07,
      "loss": 1.705,
      "step": 256384
    },
    {
      "epoch": 0.0009305845532545244,
      "grad_norm": 8255.079406038443,
      "learning_rate": 1.9758061946141955e-07,
      "loss": 1.7234,
      "step": 256416
    },
    {
      "epoch": 0.0009307006876053611,
      "grad_norm": 9881.657249672244,
      "learning_rate": 1.9756827954200822e-07,
      "loss": 1.7152,
      "step": 256448
    },
    {
      "epoch": 0.0009308168219561978,
      "grad_norm": 10192.853967363606,
      "learning_rate": 1.9755594193438126e-07,
      "loss": 1.7223,
      "step": 256480
    },
    {
      "epoch": 0.0009309329563070345,
      "grad_norm": 8526.073891305423,
      "learning_rate": 1.9754360663781688e-07,
      "loss": 1.7321,
      "step": 256512
    },
    {
      "epoch": 0.0009310490906578712,
      "grad_norm": 11278.58342168909,
      "learning_rate": 1.9753127365159366e-07,
      "loss": 1.6991,
      "step": 256544
    },
    {
      "epoch": 0.0009311652250087079,
      "grad_norm": 9713.049160793948,
      "learning_rate": 1.9751894297499052e-07,
      "loss": 1.6869,
      "step": 256576
    },
    {
      "epoch": 0.0009312813593595446,
      "grad_norm": 8971.806507053081,
      "learning_rate": 1.9750661460728668e-07,
      "loss": 1.7048,
      "step": 256608
    },
    {
      "epoch": 0.0009313974937103813,
      "grad_norm": 8801.46237849143,
      "learning_rate": 1.9749428854776162e-07,
      "loss": 1.6864,
      "step": 256640
    },
    {
      "epoch": 0.000931513628061218,
      "grad_norm": 11016.537931673452,
      "learning_rate": 1.974819647956952e-07,
      "loss": 1.721,
      "step": 256672
    },
    {
      "epoch": 0.0009316297624120547,
      "grad_norm": 9143.535858736488,
      "learning_rate": 1.9746964335036764e-07,
      "loss": 1.7159,
      "step": 256704
    },
    {
      "epoch": 0.0009317458967628915,
      "grad_norm": 10346.539518119089,
      "learning_rate": 1.9745732421105933e-07,
      "loss": 1.6838,
      "step": 256736
    },
    {
      "epoch": 0.0009318620311137281,
      "grad_norm": 8084.642107106535,
      "learning_rate": 1.9744500737705108e-07,
      "loss": 1.6991,
      "step": 256768
    },
    {
      "epoch": 0.0009319781654645649,
      "grad_norm": 8656.712771023422,
      "learning_rate": 1.97432692847624e-07,
      "loss": 1.6946,
      "step": 256800
    },
    {
      "epoch": 0.0009320942998154015,
      "grad_norm": 9513.047040775105,
      "learning_rate": 1.974203806220595e-07,
      "loss": 1.7084,
      "step": 256832
    },
    {
      "epoch": 0.0009322104341662383,
      "grad_norm": 9846.576054649657,
      "learning_rate": 1.9740807069963932e-07,
      "loss": 1.7221,
      "step": 256864
    },
    {
      "epoch": 0.0009323265685170749,
      "grad_norm": 8720.871172079083,
      "learning_rate": 1.9739576307964544e-07,
      "loss": 1.7055,
      "step": 256896
    },
    {
      "epoch": 0.0009324427028679117,
      "grad_norm": 9412.955221395669,
      "learning_rate": 1.9738345776136028e-07,
      "loss": 1.7185,
      "step": 256928
    },
    {
      "epoch": 0.0009325588372187483,
      "grad_norm": 9796.933397752584,
      "learning_rate": 1.973711547440665e-07,
      "loss": 1.7253,
      "step": 256960
    },
    {
      "epoch": 0.0009326749715695851,
      "grad_norm": 11124.114526558957,
      "learning_rate": 1.9735885402704702e-07,
      "loss": 1.7178,
      "step": 256992
    },
    {
      "epoch": 0.0009327911059204218,
      "grad_norm": 10188.981107058742,
      "learning_rate": 1.9734655560958518e-07,
      "loss": 1.7379,
      "step": 257024
    },
    {
      "epoch": 0.0009329072402712585,
      "grad_norm": 11543.61347239243,
      "learning_rate": 1.9733425949096462e-07,
      "loss": 1.7313,
      "step": 257056
    },
    {
      "epoch": 0.0009330233746220952,
      "grad_norm": 11827.941832795763,
      "learning_rate": 1.973219656704692e-07,
      "loss": 1.7036,
      "step": 257088
    },
    {
      "epoch": 0.0009331395089729319,
      "grad_norm": 8922.118470408248,
      "learning_rate": 1.9731005822271144e-07,
      "loss": 1.709,
      "step": 257120
    },
    {
      "epoch": 0.0009332556433237686,
      "grad_norm": 10554.971909010465,
      "learning_rate": 1.9729776892455846e-07,
      "loss": 1.6907,
      "step": 257152
    },
    {
      "epoch": 0.0009333717776746053,
      "grad_norm": 10579.238535924975,
      "learning_rate": 1.972854819224066e-07,
      "loss": 1.7123,
      "step": 257184
    },
    {
      "epoch": 0.000933487912025442,
      "grad_norm": 9180.96835851208,
      "learning_rate": 1.97273197215541e-07,
      "loss": 1.7058,
      "step": 257216
    },
    {
      "epoch": 0.0009336040463762787,
      "grad_norm": 9703.782973665477,
      "learning_rate": 1.9726091480324716e-07,
      "loss": 1.7036,
      "step": 257248
    },
    {
      "epoch": 0.0009337201807271154,
      "grad_norm": 7933.566411141966,
      "learning_rate": 1.9724863468481086e-07,
      "loss": 1.7214,
      "step": 257280
    },
    {
      "epoch": 0.0009338363150779522,
      "grad_norm": 8699.76574397265,
      "learning_rate": 1.9723635685951813e-07,
      "loss": 1.7232,
      "step": 257312
    },
    {
      "epoch": 0.0009339524494287888,
      "grad_norm": 8655.947319617882,
      "learning_rate": 1.9722408132665545e-07,
      "loss": 1.6954,
      "step": 257344
    },
    {
      "epoch": 0.0009340685837796256,
      "grad_norm": 10636.952759131724,
      "learning_rate": 1.9721180808550948e-07,
      "loss": 1.7109,
      "step": 257376
    },
    {
      "epoch": 0.0009341847181304622,
      "grad_norm": 10322.025963927817,
      "learning_rate": 1.9719953713536725e-07,
      "loss": 1.7182,
      "step": 257408
    },
    {
      "epoch": 0.000934300852481299,
      "grad_norm": 9246.480195187789,
      "learning_rate": 1.9718726847551611e-07,
      "loss": 1.7028,
      "step": 257440
    },
    {
      "epoch": 0.0009344169868321356,
      "grad_norm": 10634.041376635694,
      "learning_rate": 1.9717500210524368e-07,
      "loss": 1.697,
      "step": 257472
    },
    {
      "epoch": 0.0009345331211829724,
      "grad_norm": 8778.675298699685,
      "learning_rate": 1.9716273802383792e-07,
      "loss": 1.6927,
      "step": 257504
    },
    {
      "epoch": 0.000934649255533809,
      "grad_norm": 8297.65268012587,
      "learning_rate": 1.971504762305871e-07,
      "loss": 1.7065,
      "step": 257536
    },
    {
      "epoch": 0.0009347653898846458,
      "grad_norm": 9179.054635418617,
      "learning_rate": 1.971382167247798e-07,
      "loss": 1.7189,
      "step": 257568
    },
    {
      "epoch": 0.0009348815242354825,
      "grad_norm": 9661.529692548691,
      "learning_rate": 1.9712595950570487e-07,
      "loss": 1.6983,
      "step": 257600
    },
    {
      "epoch": 0.0009349976585863192,
      "grad_norm": 10461.03608635397,
      "learning_rate": 1.971137045726515e-07,
      "loss": 1.7023,
      "step": 257632
    },
    {
      "epoch": 0.0009351137929371559,
      "grad_norm": 9885.540349419449,
      "learning_rate": 1.9710145192490927e-07,
      "loss": 1.7197,
      "step": 257664
    },
    {
      "epoch": 0.0009352299272879926,
      "grad_norm": 9172.597669144767,
      "learning_rate": 1.970892015617679e-07,
      "loss": 1.7141,
      "step": 257696
    },
    {
      "epoch": 0.0009353460616388293,
      "grad_norm": 9028.651948103881,
      "learning_rate": 1.9707695348251756e-07,
      "loss": 1.7127,
      "step": 257728
    },
    {
      "epoch": 0.000935462195989666,
      "grad_norm": 8915.872812013415,
      "learning_rate": 1.9706470768644865e-07,
      "loss": 1.7169,
      "step": 257760
    },
    {
      "epoch": 0.0009355783303405027,
      "grad_norm": 11868.573124011158,
      "learning_rate": 1.9705246417285192e-07,
      "loss": 1.7125,
      "step": 257792
    },
    {
      "epoch": 0.0009356944646913394,
      "grad_norm": 9085.259710101853,
      "learning_rate": 1.970402229410184e-07,
      "loss": 1.724,
      "step": 257824
    },
    {
      "epoch": 0.0009358105990421761,
      "grad_norm": 10828.967633158758,
      "learning_rate": 1.9702798399023946e-07,
      "loss": 1.718,
      "step": 257856
    },
    {
      "epoch": 0.0009359267333930129,
      "grad_norm": 10306.638249206188,
      "learning_rate": 1.9701574731980677e-07,
      "loss": 1.733,
      "step": 257888
    },
    {
      "epoch": 0.0009360428677438495,
      "grad_norm": 8372.878955293692,
      "learning_rate": 1.9700351292901228e-07,
      "loss": 1.7186,
      "step": 257920
    },
    {
      "epoch": 0.0009361590020946863,
      "grad_norm": 9410.773081952406,
      "learning_rate": 1.9699128081714825e-07,
      "loss": 1.6849,
      "step": 257952
    },
    {
      "epoch": 0.0009362751364455229,
      "grad_norm": 9833.579409350392,
      "learning_rate": 1.969790509835073e-07,
      "loss": 1.6857,
      "step": 257984
    },
    {
      "epoch": 0.0009363912707963597,
      "grad_norm": 9876.187118518968,
      "learning_rate": 1.9696682342738232e-07,
      "loss": 1.707,
      "step": 258016
    },
    {
      "epoch": 0.0009365074051471963,
      "grad_norm": 10516.921792996276,
      "learning_rate": 1.9695459814806647e-07,
      "loss": 1.7008,
      "step": 258048
    },
    {
      "epoch": 0.0009366235394980331,
      "grad_norm": 8127.109080109605,
      "learning_rate": 1.9694237514485333e-07,
      "loss": 1.7012,
      "step": 258080
    },
    {
      "epoch": 0.0009367396738488697,
      "grad_norm": 9027.627152247705,
      "learning_rate": 1.9693015441703663e-07,
      "loss": 1.708,
      "step": 258112
    },
    {
      "epoch": 0.0009368558081997065,
      "grad_norm": 7899.424915777097,
      "learning_rate": 1.9691831775614637e-07,
      "loss": 1.708,
      "step": 258144
    },
    {
      "epoch": 0.0009369719425505432,
      "grad_norm": 9162.097139847405,
      "learning_rate": 1.9690610150595393e-07,
      "loss": 1.7326,
      "step": 258176
    },
    {
      "epoch": 0.0009370880769013799,
      "grad_norm": 10411.72915514037,
      "learning_rate": 1.968938875290633e-07,
      "loss": 1.7146,
      "step": 258208
    },
    {
      "epoch": 0.0009372042112522166,
      "grad_norm": 9215.712886152649,
      "learning_rate": 1.9688167582476947e-07,
      "loss": 1.7376,
      "step": 258240
    },
    {
      "epoch": 0.0009373203456030533,
      "grad_norm": 9059.617872736133,
      "learning_rate": 1.968694663923678e-07,
      "loss": 1.723,
      "step": 258272
    },
    {
      "epoch": 0.00093743647995389,
      "grad_norm": 10106.743491352692,
      "learning_rate": 1.9685725923115393e-07,
      "loss": 1.6935,
      "step": 258304
    },
    {
      "epoch": 0.0009375526143047267,
      "grad_norm": 10817.380829017715,
      "learning_rate": 1.9684505434042378e-07,
      "loss": 1.6951,
      "step": 258336
    },
    {
      "epoch": 0.0009376687486555634,
      "grad_norm": 10822.714446939824,
      "learning_rate": 1.9683285171947367e-07,
      "loss": 1.6999,
      "step": 258368
    },
    {
      "epoch": 0.0009377848830064001,
      "grad_norm": 9723.887082849122,
      "learning_rate": 1.968206513676001e-07,
      "loss": 1.6896,
      "step": 258400
    },
    {
      "epoch": 0.0009379010173572368,
      "grad_norm": 9473.244006146997,
      "learning_rate": 1.968084532840999e-07,
      "loss": 1.7156,
      "step": 258432
    },
    {
      "epoch": 0.0009380171517080736,
      "grad_norm": 9560.562117365276,
      "learning_rate": 1.967962574682703e-07,
      "loss": 1.7091,
      "step": 258464
    },
    {
      "epoch": 0.0009381332860589102,
      "grad_norm": 10057.087650010812,
      "learning_rate": 1.9678406391940878e-07,
      "loss": 1.6911,
      "step": 258496
    },
    {
      "epoch": 0.000938249420409747,
      "grad_norm": 10343.379718447932,
      "learning_rate": 1.9677187263681308e-07,
      "loss": 1.7069,
      "step": 258528
    },
    {
      "epoch": 0.0009383655547605836,
      "grad_norm": 9557.729228221524,
      "learning_rate": 1.9675968361978127e-07,
      "loss": 1.6883,
      "step": 258560
    },
    {
      "epoch": 0.0009384816891114204,
      "grad_norm": 10301.313896780352,
      "learning_rate": 1.9674749686761178e-07,
      "loss": 1.7161,
      "step": 258592
    },
    {
      "epoch": 0.000938597823462257,
      "grad_norm": 8791.714963532428,
      "learning_rate": 1.9673531237960328e-07,
      "loss": 1.7139,
      "step": 258624
    },
    {
      "epoch": 0.0009387139578130938,
      "grad_norm": 10535.061461614736,
      "learning_rate": 1.9672313015505476e-07,
      "loss": 1.7028,
      "step": 258656
    },
    {
      "epoch": 0.0009388300921639304,
      "grad_norm": 10167.283019568207,
      "learning_rate": 1.9671095019326555e-07,
      "loss": 1.7217,
      "step": 258688
    },
    {
      "epoch": 0.0009389462265147672,
      "grad_norm": 11167.437127649298,
      "learning_rate": 1.9669877249353518e-07,
      "loss": 1.7251,
      "step": 258720
    },
    {
      "epoch": 0.000939062360865604,
      "grad_norm": 9540.6354086088,
      "learning_rate": 1.9668659705516364e-07,
      "loss": 1.7268,
      "step": 258752
    },
    {
      "epoch": 0.0009391784952164406,
      "grad_norm": 9601.077231227755,
      "learning_rate": 1.9667442387745107e-07,
      "loss": 1.746,
      "step": 258784
    },
    {
      "epoch": 0.0009392946295672773,
      "grad_norm": 8880.548181278,
      "learning_rate": 1.96662252959698e-07,
      "loss": 1.7345,
      "step": 258816
    },
    {
      "epoch": 0.000939410763918114,
      "grad_norm": 9001.680509771495,
      "learning_rate": 1.9665008430120528e-07,
      "loss": 1.7231,
      "step": 258848
    },
    {
      "epoch": 0.0009395268982689507,
      "grad_norm": 10213.220256119026,
      "learning_rate": 1.96637917901274e-07,
      "loss": 1.7306,
      "step": 258880
    },
    {
      "epoch": 0.0009396430326197874,
      "grad_norm": 8241.100654645592,
      "learning_rate": 1.9662575375920556e-07,
      "loss": 1.6989,
      "step": 258912
    },
    {
      "epoch": 0.0009397591669706241,
      "grad_norm": 10181.11644172681,
      "learning_rate": 1.966135918743017e-07,
      "loss": 1.7229,
      "step": 258944
    },
    {
      "epoch": 0.0009398753013214608,
      "grad_norm": 9836.543498607629,
      "learning_rate": 1.966014322458645e-07,
      "loss": 1.7196,
      "step": 258976
    },
    {
      "epoch": 0.0009399914356722975,
      "grad_norm": 8566.522048065948,
      "learning_rate": 1.9658927487319618e-07,
      "loss": 1.7074,
      "step": 259008
    },
    {
      "epoch": 0.0009401075700231343,
      "grad_norm": 9367.558913612447,
      "learning_rate": 1.9657711975559946e-07,
      "loss": 1.7337,
      "step": 259040
    },
    {
      "epoch": 0.000940223704373971,
      "grad_norm": 8280.75322660928,
      "learning_rate": 1.965649668923772e-07,
      "loss": 1.7277,
      "step": 259072
    },
    {
      "epoch": 0.0009403398387248077,
      "grad_norm": 10008.784141942517,
      "learning_rate": 1.9655281628283269e-07,
      "loss": 1.704,
      "step": 259104
    },
    {
      "epoch": 0.0009404559730756443,
      "grad_norm": 9502.377807685822,
      "learning_rate": 1.9654104752831623e-07,
      "loss": 1.7133,
      "step": 259136
    },
    {
      "epoch": 0.0009405721074264811,
      "grad_norm": 9066.630906792225,
      "learning_rate": 1.965289013536647e-07,
      "loss": 1.7143,
      "step": 259168
    },
    {
      "epoch": 0.0009406882417773177,
      "grad_norm": 9220.592063419788,
      "learning_rate": 1.9651675743062414e-07,
      "loss": 1.6952,
      "step": 259200
    },
    {
      "epoch": 0.0009408043761281545,
      "grad_norm": 8545.640174966415,
      "learning_rate": 1.9650461575849898e-07,
      "loss": 1.7153,
      "step": 259232
    },
    {
      "epoch": 0.0009409205104789911,
      "grad_norm": 10805.146921722073,
      "learning_rate": 1.9649247633659397e-07,
      "loss": 1.6985,
      "step": 259264
    },
    {
      "epoch": 0.0009410366448298279,
      "grad_norm": 10316.705675747467,
      "learning_rate": 1.9648033916421407e-07,
      "loss": 1.7208,
      "step": 259296
    },
    {
      "epoch": 0.0009411527791806647,
      "grad_norm": 9343.6923108587,
      "learning_rate": 1.9646820424066473e-07,
      "loss": 1.7301,
      "step": 259328
    },
    {
      "epoch": 0.0009412689135315013,
      "grad_norm": 8457.916410085878,
      "learning_rate": 1.9645607156525144e-07,
      "loss": 1.7172,
      "step": 259360
    },
    {
      "epoch": 0.0009413850478823381,
      "grad_norm": 9532.535654273735,
      "learning_rate": 1.9644394113728022e-07,
      "loss": 1.7012,
      "step": 259392
    },
    {
      "epoch": 0.0009415011822331747,
      "grad_norm": 11657.321990920556,
      "learning_rate": 1.964318129560573e-07,
      "loss": 1.7101,
      "step": 259424
    },
    {
      "epoch": 0.0009416173165840115,
      "grad_norm": 8798.91902451659,
      "learning_rate": 1.964196870208892e-07,
      "loss": 1.7208,
      "step": 259456
    },
    {
      "epoch": 0.0009417334509348481,
      "grad_norm": 8319.434836573937,
      "learning_rate": 1.964075633310827e-07,
      "loss": 1.7126,
      "step": 259488
    },
    {
      "epoch": 0.0009418495852856849,
      "grad_norm": 11742.368755919735,
      "learning_rate": 1.9639544188594497e-07,
      "loss": 1.705,
      "step": 259520
    },
    {
      "epoch": 0.0009419657196365215,
      "grad_norm": 10965.740102701688,
      "learning_rate": 1.9638332268478344e-07,
      "loss": 1.7028,
      "step": 259552
    },
    {
      "epoch": 0.0009420818539873583,
      "grad_norm": 9515.021702550132,
      "learning_rate": 1.9637120572690584e-07,
      "loss": 1.7239,
      "step": 259584
    },
    {
      "epoch": 0.000942197988338195,
      "grad_norm": 9259.527201752799,
      "learning_rate": 1.9635909101162018e-07,
      "loss": 1.7059,
      "step": 259616
    },
    {
      "epoch": 0.0009423141226890317,
      "grad_norm": 8580.637738536689,
      "learning_rate": 1.963469785382348e-07,
      "loss": 1.7366,
      "step": 259648
    },
    {
      "epoch": 0.0009424302570398684,
      "grad_norm": 9932.036649147041,
      "learning_rate": 1.9633486830605832e-07,
      "loss": 1.7234,
      "step": 259680
    },
    {
      "epoch": 0.000942546391390705,
      "grad_norm": 8152.401364015391,
      "learning_rate": 1.9632276031439963e-07,
      "loss": 1.6836,
      "step": 259712
    },
    {
      "epoch": 0.0009426625257415418,
      "grad_norm": 10975.893767707485,
      "learning_rate": 1.96310654562568e-07,
      "loss": 1.6845,
      "step": 259744
    },
    {
      "epoch": 0.0009427786600923785,
      "grad_norm": 8968.453266868262,
      "learning_rate": 1.9629855104987292e-07,
      "loss": 1.6928,
      "step": 259776
    },
    {
      "epoch": 0.0009428947944432152,
      "grad_norm": 10247.938524405774,
      "learning_rate": 1.9628644977562423e-07,
      "loss": 1.7081,
      "step": 259808
    },
    {
      "epoch": 0.0009430109287940519,
      "grad_norm": 9181.54464128994,
      "learning_rate": 1.9627435073913201e-07,
      "loss": 1.7105,
      "step": 259840
    },
    {
      "epoch": 0.0009431270631448886,
      "grad_norm": 10644.471804650524,
      "learning_rate": 1.9626225393970672e-07,
      "loss": 1.7007,
      "step": 259872
    },
    {
      "epoch": 0.0009432431974957254,
      "grad_norm": 9338.021846194193,
      "learning_rate": 1.96250159376659e-07,
      "loss": 1.7017,
      "step": 259904
    },
    {
      "epoch": 0.000943359331846562,
      "grad_norm": 10008.321737434304,
      "learning_rate": 1.962380670492999e-07,
      "loss": 1.7252,
      "step": 259936
    },
    {
      "epoch": 0.0009434754661973988,
      "grad_norm": 9663.147313375699,
      "learning_rate": 1.9622597695694076e-07,
      "loss": 1.6976,
      "step": 259968
    },
    {
      "epoch": 0.0009435916005482354,
      "grad_norm": 9503.04466999919,
      "learning_rate": 1.962138890988931e-07,
      "loss": 1.7226,
      "step": 260000
    },
    {
      "epoch": 0.0009437077348990722,
      "grad_norm": 8948.87300166898,
      "learning_rate": 1.9620180347446887e-07,
      "loss": 1.7258,
      "step": 260032
    },
    {
      "epoch": 0.0009438238692499088,
      "grad_norm": 10628.12175316034,
      "learning_rate": 1.9618972008298025e-07,
      "loss": 1.6823,
      "step": 260064
    },
    {
      "epoch": 0.0009439400036007456,
      "grad_norm": 9248.75710568723,
      "learning_rate": 1.9617763892373973e-07,
      "loss": 1.6937,
      "step": 260096
    },
    {
      "epoch": 0.0009440561379515822,
      "grad_norm": 19289.85204712571,
      "learning_rate": 1.961655599960601e-07,
      "loss": 1.6947,
      "step": 260128
    },
    {
      "epoch": 0.000944172272302419,
      "grad_norm": 10871.317307483947,
      "learning_rate": 1.9615386066226835e-07,
      "loss": 1.6914,
      "step": 260160
    },
    {
      "epoch": 0.0009442884066532557,
      "grad_norm": 11193.874396293715,
      "learning_rate": 1.961417861259671e-07,
      "loss": 1.7159,
      "step": 260192
    },
    {
      "epoch": 0.0009444045410040924,
      "grad_norm": 10435.588914862448,
      "learning_rate": 1.961297138191883e-07,
      "loss": 1.7138,
      "step": 260224
    },
    {
      "epoch": 0.0009445206753549291,
      "grad_norm": 10032.233051519486,
      "learning_rate": 1.9611764374124593e-07,
      "loss": 1.6787,
      "step": 260256
    },
    {
      "epoch": 0.0009446368097057658,
      "grad_norm": 9526.994699274268,
      "learning_rate": 1.9610557589145426e-07,
      "loss": 1.7038,
      "step": 260288
    },
    {
      "epoch": 0.0009447529440566025,
      "grad_norm": 9608.448782191641,
      "learning_rate": 1.9609351026912784e-07,
      "loss": 1.6872,
      "step": 260320
    },
    {
      "epoch": 0.0009448690784074392,
      "grad_norm": 8499.192902858482,
      "learning_rate": 1.960814468735815e-07,
      "loss": 1.711,
      "step": 260352
    },
    {
      "epoch": 0.0009449852127582759,
      "grad_norm": 9074.604343992083,
      "learning_rate": 1.960693857041304e-07,
      "loss": 1.7172,
      "step": 260384
    },
    {
      "epoch": 0.0009451013471091126,
      "grad_norm": 9253.914090805036,
      "learning_rate": 1.9605732676008992e-07,
      "loss": 1.6974,
      "step": 260416
    },
    {
      "epoch": 0.0009452174814599493,
      "grad_norm": 10642.659066229642,
      "learning_rate": 1.9604527004077588e-07,
      "loss": 1.7219,
      "step": 260448
    },
    {
      "epoch": 0.0009453336158107861,
      "grad_norm": 10850.03907827064,
      "learning_rate": 1.9603321554550428e-07,
      "loss": 1.7142,
      "step": 260480
    },
    {
      "epoch": 0.0009454497501616227,
      "grad_norm": 9345.380463095122,
      "learning_rate": 1.9602116327359142e-07,
      "loss": 1.7235,
      "step": 260512
    },
    {
      "epoch": 0.0009455658845124595,
      "grad_norm": 9247.545728462228,
      "learning_rate": 1.960091132243539e-07,
      "loss": 1.7314,
      "step": 260544
    },
    {
      "epoch": 0.0009456820188632961,
      "grad_norm": 7974.451203687938,
      "learning_rate": 1.959970653971087e-07,
      "loss": 1.7069,
      "step": 260576
    },
    {
      "epoch": 0.0009457981532141329,
      "grad_norm": 8427.41894057724,
      "learning_rate": 1.9598501979117293e-07,
      "loss": 1.7013,
      "step": 260608
    },
    {
      "epoch": 0.0009459142875649695,
      "grad_norm": 10422.720949924736,
      "learning_rate": 1.9597297640586414e-07,
      "loss": 1.7215,
      "step": 260640
    },
    {
      "epoch": 0.0009460304219158063,
      "grad_norm": 8869.266711515671,
      "learning_rate": 1.9596093524050012e-07,
      "loss": 1.682,
      "step": 260672
    },
    {
      "epoch": 0.0009461465562666429,
      "grad_norm": 11381.599536093334,
      "learning_rate": 1.9594889629439895e-07,
      "loss": 1.7067,
      "step": 260704
    },
    {
      "epoch": 0.0009462626906174797,
      "grad_norm": 7766.023821750742,
      "learning_rate": 1.9593685956687898e-07,
      "loss": 1.7031,
      "step": 260736
    },
    {
      "epoch": 0.0009463788249683164,
      "grad_norm": 9514.461414079096,
      "learning_rate": 1.9592482505725893e-07,
      "loss": 1.6888,
      "step": 260768
    },
    {
      "epoch": 0.0009464949593191531,
      "grad_norm": 10728.459162433346,
      "learning_rate": 1.959127927648577e-07,
      "loss": 1.7173,
      "step": 260800
    },
    {
      "epoch": 0.0009466110936699898,
      "grad_norm": 9108.35693196089,
      "learning_rate": 1.959007626889946e-07,
      "loss": 1.7111,
      "step": 260832
    },
    {
      "epoch": 0.0009467272280208265,
      "grad_norm": 8759.78024838523,
      "learning_rate": 1.9588873482898916e-07,
      "loss": 1.7012,
      "step": 260864
    },
    {
      "epoch": 0.0009468433623716632,
      "grad_norm": 10497.059397755163,
      "learning_rate": 1.9587670918416122e-07,
      "loss": 1.7061,
      "step": 260896
    },
    {
      "epoch": 0.0009469594967224999,
      "grad_norm": 8828.447428625263,
      "learning_rate": 1.958646857538309e-07,
      "loss": 1.6894,
      "step": 260928
    },
    {
      "epoch": 0.0009470756310733366,
      "grad_norm": 10598.112662167732,
      "learning_rate": 1.9585266453731866e-07,
      "loss": 1.6834,
      "step": 260960
    },
    {
      "epoch": 0.0009471917654241733,
      "grad_norm": 8591.243216205674,
      "learning_rate": 1.958406455339452e-07,
      "loss": 1.7018,
      "step": 260992
    },
    {
      "epoch": 0.00094730789977501,
      "grad_norm": 10628.791746948475,
      "learning_rate": 1.958286287430315e-07,
      "loss": 1.6824,
      "step": 261024
    },
    {
      "epoch": 0.0009474240341258468,
      "grad_norm": 8896.904855060551,
      "learning_rate": 1.9581661416389885e-07,
      "loss": 1.7082,
      "step": 261056
    },
    {
      "epoch": 0.0009475401684766834,
      "grad_norm": 9828.111313980931,
      "learning_rate": 1.958046017958689e-07,
      "loss": 1.7096,
      "step": 261088
    },
    {
      "epoch": 0.0009476563028275202,
      "grad_norm": 8310.661586179527,
      "learning_rate": 1.9579259163826353e-07,
      "loss": 1.6935,
      "step": 261120
    },
    {
      "epoch": 0.0009477724371783568,
      "grad_norm": 9639.493140201927,
      "learning_rate": 1.9578095890533386e-07,
      "loss": 1.7104,
      "step": 261152
    },
    {
      "epoch": 0.0009478885715291936,
      "grad_norm": 9297.983437283592,
      "learning_rate": 1.9576895309752126e-07,
      "loss": 1.7155,
      "step": 261184
    },
    {
      "epoch": 0.0009480047058800302,
      "grad_norm": 8649.406222394691,
      "learning_rate": 1.9575694949812173e-07,
      "loss": 1.7243,
      "step": 261216
    },
    {
      "epoch": 0.000948120840230867,
      "grad_norm": 8080.8661664452775,
      "learning_rate": 1.9574494810645836e-07,
      "loss": 1.7233,
      "step": 261248
    },
    {
      "epoch": 0.0009482369745817036,
      "grad_norm": 9823.242641816398,
      "learning_rate": 1.9573294892185446e-07,
      "loss": 1.7085,
      "step": 261280
    },
    {
      "epoch": 0.0009483531089325404,
      "grad_norm": 9140.330409782788,
      "learning_rate": 1.9572095194363368e-07,
      "loss": 1.7188,
      "step": 261312
    },
    {
      "epoch": 0.0009484692432833771,
      "grad_norm": 9656.969296834282,
      "learning_rate": 1.957089571711199e-07,
      "loss": 1.7238,
      "step": 261344
    },
    {
      "epoch": 0.0009485853776342138,
      "grad_norm": 9807.178391362115,
      "learning_rate": 1.956969646036373e-07,
      "loss": 1.7286,
      "step": 261376
    },
    {
      "epoch": 0.0009487015119850505,
      "grad_norm": 8137.786062560259,
      "learning_rate": 1.9568497424051038e-07,
      "loss": 1.7367,
      "step": 261408
    },
    {
      "epoch": 0.0009488176463358872,
      "grad_norm": 8875.607021494361,
      "learning_rate": 1.9567298608106395e-07,
      "loss": 1.7117,
      "step": 261440
    },
    {
      "epoch": 0.0009489337806867239,
      "grad_norm": 9732.420048477152,
      "learning_rate": 1.9566100012462305e-07,
      "loss": 1.6932,
      "step": 261472
    },
    {
      "epoch": 0.0009490499150375606,
      "grad_norm": 10770.811482892086,
      "learning_rate": 1.9564901637051303e-07,
      "loss": 1.6919,
      "step": 261504
    },
    {
      "epoch": 0.0009491660493883973,
      "grad_norm": 10020.687401570813,
      "learning_rate": 1.9563703481805956e-07,
      "loss": 1.6877,
      "step": 261536
    },
    {
      "epoch": 0.000949282183739234,
      "grad_norm": 8872.395392451805,
      "learning_rate": 1.9562505546658854e-07,
      "loss": 1.706,
      "step": 261568
    },
    {
      "epoch": 0.0009493983180900707,
      "grad_norm": 10279.880933162602,
      "learning_rate": 1.956130783154262e-07,
      "loss": 1.7053,
      "step": 261600
    },
    {
      "epoch": 0.0009495144524409075,
      "grad_norm": 9087.964788664181,
      "learning_rate": 1.9560110336389913e-07,
      "loss": 1.6999,
      "step": 261632
    },
    {
      "epoch": 0.0009496305867917441,
      "grad_norm": 8909.432978590725,
      "learning_rate": 1.9558913061133403e-07,
      "loss": 1.7022,
      "step": 261664
    },
    {
      "epoch": 0.0009497467211425809,
      "grad_norm": 9871.057694087296,
      "learning_rate": 1.9557716005705802e-07,
      "loss": 1.7216,
      "step": 261696
    },
    {
      "epoch": 0.0009498628554934175,
      "grad_norm": 11892.01109989391,
      "learning_rate": 1.955651917003985e-07,
      "loss": 1.6919,
      "step": 261728
    },
    {
      "epoch": 0.0009499789898442543,
      "grad_norm": 9725.82078798494,
      "learning_rate": 1.955532255406831e-07,
      "loss": 1.722,
      "step": 261760
    },
    {
      "epoch": 0.0009500951241950909,
      "grad_norm": 9966.90443417614,
      "learning_rate": 1.9554126157723984e-07,
      "loss": 1.7308,
      "step": 261792
    },
    {
      "epoch": 0.0009502112585459277,
      "grad_norm": 11505.355100995363,
      "learning_rate": 1.9552929980939687e-07,
      "loss": 1.6978,
      "step": 261824
    },
    {
      "epoch": 0.0009503273928967643,
      "grad_norm": 8855.840106957668,
      "learning_rate": 1.955173402364828e-07,
      "loss": 1.696,
      "step": 261856
    },
    {
      "epoch": 0.0009504435272476011,
      "grad_norm": 12243.293347788413,
      "learning_rate": 1.9550538285782638e-07,
      "loss": 1.6967,
      "step": 261888
    },
    {
      "epoch": 0.0009505596615984378,
      "grad_norm": 9321.271908918869,
      "learning_rate": 1.954934276727568e-07,
      "loss": 1.6991,
      "step": 261920
    },
    {
      "epoch": 0.0009506757959492745,
      "grad_norm": 9277.680529097775,
      "learning_rate": 1.9548147468060334e-07,
      "loss": 1.7106,
      "step": 261952
    },
    {
      "epoch": 0.0009507919303001112,
      "grad_norm": 10531.706414442058,
      "learning_rate": 1.9546952388069576e-07,
      "loss": 1.7037,
      "step": 261984
    },
    {
      "epoch": 0.0009509080646509479,
      "grad_norm": 9373.569010787727,
      "learning_rate": 1.9545757527236402e-07,
      "loss": 1.6903,
      "step": 262016
    },
    {
      "epoch": 0.0009510241990017846,
      "grad_norm": 10041.183993932189,
      "learning_rate": 1.9544562885493832e-07,
      "loss": 1.7011,
      "step": 262048
    },
    {
      "epoch": 0.0009511403333526213,
      "grad_norm": 9421.251721507073,
      "learning_rate": 1.9543368462774924e-07,
      "loss": 1.6919,
      "step": 262080
    },
    {
      "epoch": 0.000951256467703458,
      "grad_norm": 10527.953837284813,
      "learning_rate": 1.954217425901276e-07,
      "loss": 1.7181,
      "step": 262112
    },
    {
      "epoch": 0.0009513726020542947,
      "grad_norm": 11070.64876147735,
      "learning_rate": 1.9540980274140452e-07,
      "loss": 1.7243,
      "step": 262144
    },
    {
      "epoch": 0.0009513726020542947,
      "eval_loss": 1.7109375,
      "eval_runtime": 24797.1786,
      "eval_samples_per_second": 214.912,
      "eval_steps_per_second": 0.42,
      "step": 262144
    },
    {
      "epoch": 0.0009514887364051314,
      "grad_norm": 9276.096269444382,
      "learning_rate": 1.9539823809968578e-07,
      "loss": 1.7165,
      "step": 262176
    },
    {
      "epoch": 0.0009516048707559682,
      "grad_norm": 9323.282898207048,
      "learning_rate": 1.9538630255840307e-07,
      "loss": 1.72,
      "step": 262208
    },
    {
      "epoch": 0.0009517210051068048,
      "grad_norm": 9972.52666078161,
      "learning_rate": 1.953743692040348e-07,
      "loss": 1.7315,
      "step": 262240
    },
    {
      "epoch": 0.0009518371394576416,
      "grad_norm": 9730.227129928673,
      "learning_rate": 1.953624380359133e-07,
      "loss": 1.7436,
      "step": 262272
    },
    {
      "epoch": 0.0009519532738084782,
      "grad_norm": 10478.560779038313,
      "learning_rate": 1.95350509053371e-07,
      "loss": 1.7339,
      "step": 262304
    },
    {
      "epoch": 0.000952069408159315,
      "grad_norm": 9680.908221856047,
      "learning_rate": 1.9533858225574076e-07,
      "loss": 1.7123,
      "step": 262336
    },
    {
      "epoch": 0.0009521855425101516,
      "grad_norm": 9279.333381229495,
      "learning_rate": 1.9532665764235568e-07,
      "loss": 1.7436,
      "step": 262368
    },
    {
      "epoch": 0.0009523016768609884,
      "grad_norm": 8670.907218970804,
      "learning_rate": 1.9531473521254914e-07,
      "loss": 1.7354,
      "step": 262400
    },
    {
      "epoch": 0.000952417811211825,
      "grad_norm": 8423.323809518426,
      "learning_rate": 1.9530281496565478e-07,
      "loss": 1.7118,
      "step": 262432
    },
    {
      "epoch": 0.0009525339455626618,
      "grad_norm": 9048.746211492507,
      "learning_rate": 1.9529089690100665e-07,
      "loss": 1.7068,
      "step": 262464
    },
    {
      "epoch": 0.0009526500799134985,
      "grad_norm": 11332.748298625536,
      "learning_rate": 1.9527898101793888e-07,
      "loss": 1.6908,
      "step": 262496
    },
    {
      "epoch": 0.0009527662142643352,
      "grad_norm": 11056.48750734156,
      "learning_rate": 1.952670673157861e-07,
      "loss": 1.7037,
      "step": 262528
    },
    {
      "epoch": 0.0009528823486151719,
      "grad_norm": 7876.137886045418,
      "learning_rate": 1.9525515579388297e-07,
      "loss": 1.7185,
      "step": 262560
    },
    {
      "epoch": 0.0009529984829660086,
      "grad_norm": 9534.012376748837,
      "learning_rate": 1.9524324645156474e-07,
      "loss": 1.7183,
      "step": 262592
    },
    {
      "epoch": 0.0009531146173168453,
      "grad_norm": 8243.009765856159,
      "learning_rate": 1.9523133928816667e-07,
      "loss": 1.683,
      "step": 262624
    },
    {
      "epoch": 0.000953230751667682,
      "grad_norm": 8814.814348583866,
      "learning_rate": 1.9521943430302454e-07,
      "loss": 1.692,
      "step": 262656
    },
    {
      "epoch": 0.0009533468860185187,
      "grad_norm": 9840.769075636314,
      "learning_rate": 1.9520753149547418e-07,
      "loss": 1.7088,
      "step": 262688
    },
    {
      "epoch": 0.0009534630203693554,
      "grad_norm": 10941.700873264632,
      "learning_rate": 1.9519563086485182e-07,
      "loss": 1.7085,
      "step": 262720
    },
    {
      "epoch": 0.0009535791547201921,
      "grad_norm": 10057.257479054615,
      "learning_rate": 1.9518373241049405e-07,
      "loss": 1.7141,
      "step": 262752
    },
    {
      "epoch": 0.0009536952890710289,
      "grad_norm": 11385.154544405623,
      "learning_rate": 1.951718361317376e-07,
      "loss": 1.7014,
      "step": 262784
    },
    {
      "epoch": 0.0009538114234218655,
      "grad_norm": 12701.730118373638,
      "learning_rate": 1.951599420279196e-07,
      "loss": 1.7023,
      "step": 262816
    },
    {
      "epoch": 0.0009539275577727023,
      "grad_norm": 9705.07454891512,
      "learning_rate": 1.9514805009837736e-07,
      "loss": 1.6914,
      "step": 262848
    },
    {
      "epoch": 0.0009540436921235389,
      "grad_norm": 9428.918283663295,
      "learning_rate": 1.9513616034244846e-07,
      "loss": 1.7071,
      "step": 262880
    },
    {
      "epoch": 0.0009541598264743757,
      "grad_norm": 9953.87663174504,
      "learning_rate": 1.95124272759471e-07,
      "loss": 1.7159,
      "step": 262912
    },
    {
      "epoch": 0.0009542759608252123,
      "grad_norm": 10109.998219584413,
      "learning_rate": 1.9511238734878306e-07,
      "loss": 1.7142,
      "step": 262944
    },
    {
      "epoch": 0.0009543920951760491,
      "grad_norm": 9512.131832559935,
      "learning_rate": 1.9510050410972313e-07,
      "loss": 1.7258,
      "step": 262976
    },
    {
      "epoch": 0.0009545082295268857,
      "grad_norm": 9159.450092663861,
      "learning_rate": 1.9508862304163e-07,
      "loss": 1.7289,
      "step": 263008
    },
    {
      "epoch": 0.0009546243638777225,
      "grad_norm": 9852.547487832779,
      "learning_rate": 1.9507674414384273e-07,
      "loss": 1.7267,
      "step": 263040
    },
    {
      "epoch": 0.0009547404982285592,
      "grad_norm": 8244.683377789592,
      "learning_rate": 1.950648674157007e-07,
      "loss": 1.7368,
      "step": 263072
    },
    {
      "epoch": 0.0009548566325793959,
      "grad_norm": 13488.329770583161,
      "learning_rate": 1.950529928565434e-07,
      "loss": 1.7299,
      "step": 263104
    },
    {
      "epoch": 0.0009549727669302326,
      "grad_norm": 9652.502266252,
      "learning_rate": 1.9504112046571083e-07,
      "loss": 1.724,
      "step": 263136
    },
    {
      "epoch": 0.0009550889012810693,
      "grad_norm": 15488.21358323806,
      "learning_rate": 1.9502925024254315e-07,
      "loss": 1.7437,
      "step": 263168
    },
    {
      "epoch": 0.000955205035631906,
      "grad_norm": 9938.063996573981,
      "learning_rate": 1.9501775303034102e-07,
      "loss": 1.7035,
      "step": 263200
    },
    {
      "epoch": 0.0009553211699827427,
      "grad_norm": 8281.757905179311,
      "learning_rate": 1.9500588707283642e-07,
      "loss": 1.7048,
      "step": 263232
    },
    {
      "epoch": 0.0009554373043335794,
      "grad_norm": 9771.855299788265,
      "learning_rate": 1.9499402328103948e-07,
      "loss": 1.6876,
      "step": 263264
    },
    {
      "epoch": 0.0009555534386844161,
      "grad_norm": 12700.721554305488,
      "learning_rate": 1.9498216165429152e-07,
      "loss": 1.7037,
      "step": 263296
    },
    {
      "epoch": 0.0009556695730352528,
      "grad_norm": 8313.988814041068,
      "learning_rate": 1.9497030219193408e-07,
      "loss": 1.7115,
      "step": 263328
    },
    {
      "epoch": 0.0009557857073860896,
      "grad_norm": 8604.524391272304,
      "learning_rate": 1.9495844489330907e-07,
      "loss": 1.7029,
      "step": 263360
    },
    {
      "epoch": 0.0009559018417369262,
      "grad_norm": 10937.460217070506,
      "learning_rate": 1.9494658975775858e-07,
      "loss": 1.7135,
      "step": 263392
    },
    {
      "epoch": 0.000956017976087763,
      "grad_norm": 9129.78641590262,
      "learning_rate": 1.9493473678462508e-07,
      "loss": 1.7172,
      "step": 263424
    },
    {
      "epoch": 0.0009561341104385996,
      "grad_norm": 9005.694198672305,
      "learning_rate": 1.949228859732512e-07,
      "loss": 1.7087,
      "step": 263456
    },
    {
      "epoch": 0.0009562502447894364,
      "grad_norm": 12094.825505148885,
      "learning_rate": 1.9491103732297993e-07,
      "loss": 1.7022,
      "step": 263488
    },
    {
      "epoch": 0.000956366379140273,
      "grad_norm": 9700.023917496286,
      "learning_rate": 1.9489919083315453e-07,
      "loss": 1.7232,
      "step": 263520
    },
    {
      "epoch": 0.0009564825134911098,
      "grad_norm": 9412.353584518592,
      "learning_rate": 1.9488734650311851e-07,
      "loss": 1.7176,
      "step": 263552
    },
    {
      "epoch": 0.0009565986478419464,
      "grad_norm": 9761.938741868851,
      "learning_rate": 1.948755043322157e-07,
      "loss": 1.7048,
      "step": 263584
    },
    {
      "epoch": 0.0009567147821927832,
      "grad_norm": 10171.523779650717,
      "learning_rate": 1.9486366431979024e-07,
      "loss": 1.7112,
      "step": 263616
    },
    {
      "epoch": 0.00095683091654362,
      "grad_norm": 11572.643431817985,
      "learning_rate": 1.9485182646518637e-07,
      "loss": 1.7098,
      "step": 263648
    },
    {
      "epoch": 0.0009569470508944566,
      "grad_norm": 10445.230299040802,
      "learning_rate": 1.9483999076774884e-07,
      "loss": 1.7168,
      "step": 263680
    },
    {
      "epoch": 0.0009570631852452934,
      "grad_norm": 9401.614116735487,
      "learning_rate": 1.9482815722682258e-07,
      "loss": 1.7205,
      "step": 263712
    },
    {
      "epoch": 0.00095717931959613,
      "grad_norm": 10814.78081146354,
      "learning_rate": 1.9481632584175276e-07,
      "loss": 1.7066,
      "step": 263744
    },
    {
      "epoch": 0.0009572954539469668,
      "grad_norm": 9728.72694652286,
      "learning_rate": 1.9480449661188485e-07,
      "loss": 1.7092,
      "step": 263776
    },
    {
      "epoch": 0.0009574115882978034,
      "grad_norm": 9089.729588937176,
      "learning_rate": 1.9479266953656467e-07,
      "loss": 1.7145,
      "step": 263808
    },
    {
      "epoch": 0.0009575277226486402,
      "grad_norm": 9053.262174487161,
      "learning_rate": 1.9478084461513816e-07,
      "loss": 1.7253,
      "step": 263840
    },
    {
      "epoch": 0.0009576438569994768,
      "grad_norm": 11007.560129292959,
      "learning_rate": 1.9476902184695172e-07,
      "loss": 1.7217,
      "step": 263872
    },
    {
      "epoch": 0.0009577599913503136,
      "grad_norm": 13514.078880930065,
      "learning_rate": 1.9475720123135195e-07,
      "loss": 1.7125,
      "step": 263904
    },
    {
      "epoch": 0.0009578761257011502,
      "grad_norm": 9761.47837164023,
      "learning_rate": 1.9474538276768562e-07,
      "loss": 1.7496,
      "step": 263936
    },
    {
      "epoch": 0.000957992260051987,
      "grad_norm": 9582.898100261737,
      "learning_rate": 1.9473356645530002e-07,
      "loss": 1.7427,
      "step": 263968
    },
    {
      "epoch": 0.0009581083944028237,
      "grad_norm": 8750.394048270055,
      "learning_rate": 1.947217522935425e-07,
      "loss": 1.7466,
      "step": 264000
    },
    {
      "epoch": 0.0009582245287536604,
      "grad_norm": 8898.632591583944,
      "learning_rate": 1.9470994028176077e-07,
      "loss": 1.7525,
      "step": 264032
    },
    {
      "epoch": 0.0009583406631044971,
      "grad_norm": 9286.02756834159,
      "learning_rate": 1.946981304193028e-07,
      "loss": 1.7156,
      "step": 264064
    },
    {
      "epoch": 0.0009584567974553338,
      "grad_norm": 9949.826129134117,
      "learning_rate": 1.9468632270551688e-07,
      "loss": 1.7145,
      "step": 264096
    },
    {
      "epoch": 0.0009585729318061705,
      "grad_norm": 9959.54396546348,
      "learning_rate": 1.946745171397515e-07,
      "loss": 1.7272,
      "step": 264128
    },
    {
      "epoch": 0.0009586890661570072,
      "grad_norm": 8945.726018607993,
      "learning_rate": 1.9466271372135554e-07,
      "loss": 1.7409,
      "step": 264160
    },
    {
      "epoch": 0.0009588052005078439,
      "grad_norm": 21020.695326273108,
      "learning_rate": 1.9465091244967797e-07,
      "loss": 1.7187,
      "step": 264192
    },
    {
      "epoch": 0.0009589213348586806,
      "grad_norm": 9440.72984466773,
      "learning_rate": 1.9463948201426565e-07,
      "loss": 1.6988,
      "step": 264224
    },
    {
      "epoch": 0.0009590374692095173,
      "grad_norm": 9601.029319817744,
      "learning_rate": 1.9462768496703894e-07,
      "loss": 1.7296,
      "step": 264256
    },
    {
      "epoch": 0.0009591536035603541,
      "grad_norm": 8902.691952437757,
      "learning_rate": 1.9461589006459997e-07,
      "loss": 1.7376,
      "step": 264288
    },
    {
      "epoch": 0.0009592697379111907,
      "grad_norm": 9669.568346105218,
      "learning_rate": 1.9460409730629886e-07,
      "loss": 1.7319,
      "step": 264320
    },
    {
      "epoch": 0.0009593858722620275,
      "grad_norm": 9413.172472657663,
      "learning_rate": 1.945923066914861e-07,
      "loss": 1.7089,
      "step": 264352
    },
    {
      "epoch": 0.0009595020066128641,
      "grad_norm": 11186.25710414346,
      "learning_rate": 1.9458051821951236e-07,
      "loss": 1.6815,
      "step": 264384
    },
    {
      "epoch": 0.0009596181409637009,
      "grad_norm": 10639.865788627223,
      "learning_rate": 1.9456873188972876e-07,
      "loss": 1.6893,
      "step": 264416
    },
    {
      "epoch": 0.0009597342753145375,
      "grad_norm": 11737.130824865164,
      "learning_rate": 1.945569477014865e-07,
      "loss": 1.6946,
      "step": 264448
    },
    {
      "epoch": 0.0009598504096653743,
      "grad_norm": 10043.768615415232,
      "learning_rate": 1.9454516565413712e-07,
      "loss": 1.706,
      "step": 264480
    },
    {
      "epoch": 0.0009599665440162109,
      "grad_norm": 9219.392821655882,
      "learning_rate": 1.945333857470325e-07,
      "loss": 1.6954,
      "step": 264512
    },
    {
      "epoch": 0.0009600826783670477,
      "grad_norm": 10645.135226947566,
      "learning_rate": 1.9452160797952479e-07,
      "loss": 1.691,
      "step": 264544
    },
    {
      "epoch": 0.0009601988127178844,
      "grad_norm": 8741.965454061232,
      "learning_rate": 1.945098323509663e-07,
      "loss": 1.7151,
      "step": 264576
    },
    {
      "epoch": 0.0009603149470687211,
      "grad_norm": 9709.60678915475,
      "learning_rate": 1.944980588607097e-07,
      "loss": 1.7119,
      "step": 264608
    },
    {
      "epoch": 0.0009604310814195578,
      "grad_norm": 9736.359073082709,
      "learning_rate": 1.9448628750810793e-07,
      "loss": 1.7109,
      "step": 264640
    },
    {
      "epoch": 0.0009605472157703945,
      "grad_norm": 9930.691818800944,
      "learning_rate": 1.9447451829251422e-07,
      "loss": 1.7072,
      "step": 264672
    },
    {
      "epoch": 0.0009606633501212312,
      "grad_norm": 10122.08812449289,
      "learning_rate": 1.9446275121328203e-07,
      "loss": 1.7089,
      "step": 264704
    },
    {
      "epoch": 0.0009607794844720679,
      "grad_norm": 10748.610886993723,
      "learning_rate": 1.944509862697651e-07,
      "loss": 1.7215,
      "step": 264736
    },
    {
      "epoch": 0.0009608956188229046,
      "grad_norm": 10085.651391952828,
      "learning_rate": 1.9443922346131746e-07,
      "loss": 1.7402,
      "step": 264768
    },
    {
      "epoch": 0.0009610117531737413,
      "grad_norm": 12369.080321511377,
      "learning_rate": 1.9442746278729347e-07,
      "loss": 1.7256,
      "step": 264800
    },
    {
      "epoch": 0.000961127887524578,
      "grad_norm": 10594.709245656531,
      "learning_rate": 1.9441570424704759e-07,
      "loss": 1.712,
      "step": 264832
    },
    {
      "epoch": 0.0009612440218754148,
      "grad_norm": 8361.516010867887,
      "learning_rate": 1.9440394783993478e-07,
      "loss": 1.7182,
      "step": 264864
    },
    {
      "epoch": 0.0009613601562262514,
      "grad_norm": 11194.818622916586,
      "learning_rate": 1.943921935653101e-07,
      "loss": 1.7297,
      "step": 264896
    },
    {
      "epoch": 0.0009614762905770882,
      "grad_norm": 9025.755148462647,
      "learning_rate": 1.9438044142252892e-07,
      "loss": 1.7349,
      "step": 264928
    },
    {
      "epoch": 0.0009615924249279248,
      "grad_norm": 9623.543214429912,
      "learning_rate": 1.94368691410947e-07,
      "loss": 1.7013,
      "step": 264960
    },
    {
      "epoch": 0.0009617085592787616,
      "grad_norm": 12268.244373177444,
      "learning_rate": 1.943569435299202e-07,
      "loss": 1.6865,
      "step": 264992
    },
    {
      "epoch": 0.0009618246936295982,
      "grad_norm": 10692.870896069026,
      "learning_rate": 1.9434519777880475e-07,
      "loss": 1.6865,
      "step": 265024
    },
    {
      "epoch": 0.000961940827980435,
      "grad_norm": 8327.185118634028,
      "learning_rate": 1.9433345415695714e-07,
      "loss": 1.6955,
      "step": 265056
    },
    {
      "epoch": 0.0009620569623312716,
      "grad_norm": 8786.743765468525,
      "learning_rate": 1.943217126637341e-07,
      "loss": 1.7046,
      "step": 265088
    },
    {
      "epoch": 0.0009621730966821084,
      "grad_norm": 10574.309433717171,
      "learning_rate": 1.943099732984927e-07,
      "loss": 1.7025,
      "step": 265120
    },
    {
      "epoch": 0.0009622892310329451,
      "grad_norm": 8946.589517799506,
      "learning_rate": 1.9429823606059022e-07,
      "loss": 1.6983,
      "step": 265152
    },
    {
      "epoch": 0.0009624053653837818,
      "grad_norm": 9138.969088469443,
      "learning_rate": 1.9428650094938418e-07,
      "loss": 1.7166,
      "step": 265184
    },
    {
      "epoch": 0.0009625214997346185,
      "grad_norm": 18569.852988109516,
      "learning_rate": 1.9427476796423251e-07,
      "loss": 1.7199,
      "step": 265216
    },
    {
      "epoch": 0.0009626376340854552,
      "grad_norm": 10564.266373014267,
      "learning_rate": 1.9426340366169478e-07,
      "loss": 1.716,
      "step": 265248
    },
    {
      "epoch": 0.0009627537684362919,
      "grad_norm": 11264.820992807654,
      "learning_rate": 1.94251674860337e-07,
      "loss": 1.7171,
      "step": 265280
    },
    {
      "epoch": 0.0009628699027871286,
      "grad_norm": 11867.91978402281,
      "learning_rate": 1.9423994818312873e-07,
      "loss": 1.7085,
      "step": 265312
    },
    {
      "epoch": 0.0009629860371379653,
      "grad_norm": 11478.839488380348,
      "learning_rate": 1.9422822362942894e-07,
      "loss": 1.7025,
      "step": 265344
    },
    {
      "epoch": 0.000963102171488802,
      "grad_norm": 9738.19202932454,
      "learning_rate": 1.9421650119859676e-07,
      "loss": 1.6947,
      "step": 265376
    },
    {
      "epoch": 0.0009632183058396387,
      "grad_norm": 10671.715326038266,
      "learning_rate": 1.9420478088999169e-07,
      "loss": 1.7026,
      "step": 265408
    },
    {
      "epoch": 0.0009633344401904755,
      "grad_norm": 10802.398807672304,
      "learning_rate": 1.9419306270297343e-07,
      "loss": 1.703,
      "step": 265440
    },
    {
      "epoch": 0.0009634505745413121,
      "grad_norm": 8828.497720450518,
      "learning_rate": 1.9418134663690201e-07,
      "loss": 1.6815,
      "step": 265472
    },
    {
      "epoch": 0.0009635667088921489,
      "grad_norm": 9022.375297004664,
      "learning_rate": 1.9416963269113768e-07,
      "loss": 1.7077,
      "step": 265504
    },
    {
      "epoch": 0.0009636828432429855,
      "grad_norm": 8614.397715452891,
      "learning_rate": 1.94157920865041e-07,
      "loss": 1.717,
      "step": 265536
    },
    {
      "epoch": 0.0009637989775938223,
      "grad_norm": 10015.913937329933,
      "learning_rate": 1.9414621115797278e-07,
      "loss": 1.7111,
      "step": 265568
    },
    {
      "epoch": 0.0009639151119446589,
      "grad_norm": 8818.47072910037,
      "learning_rate": 1.9413450356929408e-07,
      "loss": 1.7146,
      "step": 265600
    },
    {
      "epoch": 0.0009640312462954957,
      "grad_norm": 10693.17258815175,
      "learning_rate": 1.9412279809836629e-07,
      "loss": 1.694,
      "step": 265632
    },
    {
      "epoch": 0.0009641473806463323,
      "grad_norm": 9900.017171702279,
      "learning_rate": 1.9411109474455097e-07,
      "loss": 1.7023,
      "step": 265664
    },
    {
      "epoch": 0.0009642635149971691,
      "grad_norm": 9763.73678465371,
      "learning_rate": 1.940993935072101e-07,
      "loss": 1.7227,
      "step": 265696
    },
    {
      "epoch": 0.0009643796493480058,
      "grad_norm": 8995.070983599851,
      "learning_rate": 1.9408769438570576e-07,
      "loss": 1.729,
      "step": 265728
    },
    {
      "epoch": 0.0009644957836988425,
      "grad_norm": 7830.69460520585,
      "learning_rate": 1.9407599737940036e-07,
      "loss": 1.7238,
      "step": 265760
    },
    {
      "epoch": 0.0009646119180496792,
      "grad_norm": 9537.082782486477,
      "learning_rate": 1.940643024876567e-07,
      "loss": 1.724,
      "step": 265792
    },
    {
      "epoch": 0.0009647280524005159,
      "grad_norm": 9711.469301810102,
      "learning_rate": 1.9405260970983765e-07,
      "loss": 1.727,
      "step": 265824
    },
    {
      "epoch": 0.0009648441867513526,
      "grad_norm": 8789.767687487536,
      "learning_rate": 1.9404091904530656e-07,
      "loss": 1.7109,
      "step": 265856
    },
    {
      "epoch": 0.0009649603211021893,
      "grad_norm": 8739.450325964443,
      "learning_rate": 1.9402923049342675e-07,
      "loss": 1.7228,
      "step": 265888
    },
    {
      "epoch": 0.000965076455453026,
      "grad_norm": 9019.226574379867,
      "learning_rate": 1.9401754405356215e-07,
      "loss": 1.7303,
      "step": 265920
    },
    {
      "epoch": 0.0009651925898038627,
      "grad_norm": 10431.72219722132,
      "learning_rate": 1.940058597250768e-07,
      "loss": 1.6908,
      "step": 265952
    },
    {
      "epoch": 0.0009653087241546994,
      "grad_norm": 10035.431032098222,
      "learning_rate": 1.939941775073349e-07,
      "loss": 1.6865,
      "step": 265984
    },
    {
      "epoch": 0.0009654248585055362,
      "grad_norm": 9161.870769662712,
      "learning_rate": 1.9398249739970108e-07,
      "loss": 1.6999,
      "step": 266016
    },
    {
      "epoch": 0.0009655409928563728,
      "grad_norm": 9412.265402122914,
      "learning_rate": 1.9397081940154022e-07,
      "loss": 1.7175,
      "step": 266048
    },
    {
      "epoch": 0.0009656571272072096,
      "grad_norm": 8928.440625327583,
      "learning_rate": 1.9395914351221738e-07,
      "loss": 1.7087,
      "step": 266080
    },
    {
      "epoch": 0.0009657732615580462,
      "grad_norm": 9307.447125823493,
      "learning_rate": 1.9394746973109792e-07,
      "loss": 1.7012,
      "step": 266112
    },
    {
      "epoch": 0.000965889395908883,
      "grad_norm": 10076.621060653219,
      "learning_rate": 1.9393579805754756e-07,
      "loss": 1.7062,
      "step": 266144
    },
    {
      "epoch": 0.0009660055302597196,
      "grad_norm": 8401.57532847263,
      "learning_rate": 1.939241284909322e-07,
      "loss": 1.7086,
      "step": 266176
    },
    {
      "epoch": 0.0009661216646105564,
      "grad_norm": 9443.543402769958,
      "learning_rate": 1.9391246103061797e-07,
      "loss": 1.7068,
      "step": 266208
    },
    {
      "epoch": 0.000966237798961393,
      "grad_norm": 11470.66536866977,
      "learning_rate": 1.9390116018643747e-07,
      "loss": 1.7122,
      "step": 266240
    },
    {
      "epoch": 0.0009663539333122298,
      "grad_norm": 8706.46472455956,
      "learning_rate": 1.9388949687105242e-07,
      "loss": 1.6911,
      "step": 266272
    },
    {
      "epoch": 0.0009664700676630665,
      "grad_norm": 10834.79247609293,
      "learning_rate": 1.9387783566008848e-07,
      "loss": 1.7038,
      "step": 266304
    },
    {
      "epoch": 0.0009665862020139032,
      "grad_norm": 8628.959728727445,
      "learning_rate": 1.9386617655291283e-07,
      "loss": 1.709,
      "step": 266336
    },
    {
      "epoch": 0.0009667023363647399,
      "grad_norm": 9540.005660375678,
      "learning_rate": 1.93854519548893e-07,
      "loss": 1.7139,
      "step": 266368
    },
    {
      "epoch": 0.0009668184707155766,
      "grad_norm": 9075.054380002359,
      "learning_rate": 1.938428646473968e-07,
      "loss": 1.6994,
      "step": 266400
    },
    {
      "epoch": 0.0009669346050664133,
      "grad_norm": 10601.529700944106,
      "learning_rate": 1.9383121184779216e-07,
      "loss": 1.7091,
      "step": 266432
    },
    {
      "epoch": 0.00096705073941725,
      "grad_norm": 9328.453247993475,
      "learning_rate": 1.938195611494475e-07,
      "loss": 1.7296,
      "step": 266464
    },
    {
      "epoch": 0.0009671668737680867,
      "grad_norm": 8824.804247120726,
      "learning_rate": 1.9380791255173136e-07,
      "loss": 1.732,
      "step": 266496
    },
    {
      "epoch": 0.0009672830081189234,
      "grad_norm": 10016.573266342137,
      "learning_rate": 1.9379626605401252e-07,
      "loss": 1.7396,
      "step": 266528
    },
    {
      "epoch": 0.0009673991424697601,
      "grad_norm": 9300.681265369758,
      "learning_rate": 1.9378462165566013e-07,
      "loss": 1.7187,
      "step": 266560
    },
    {
      "epoch": 0.0009675152768205969,
      "grad_norm": 10850.025253426833,
      "learning_rate": 1.9377297935604355e-07,
      "loss": 1.7076,
      "step": 266592
    },
    {
      "epoch": 0.0009676314111714335,
      "grad_norm": 9755.538939494834,
      "learning_rate": 1.9376133915453242e-07,
      "loss": 1.7136,
      "step": 266624
    },
    {
      "epoch": 0.0009677475455222703,
      "grad_norm": 9547.432743936979,
      "learning_rate": 1.937497010504966e-07,
      "loss": 1.73,
      "step": 266656
    },
    {
      "epoch": 0.0009678636798731069,
      "grad_norm": 9819.861302482841,
      "learning_rate": 1.9373806504330626e-07,
      "loss": 1.7316,
      "step": 266688
    },
    {
      "epoch": 0.0009679798142239437,
      "grad_norm": 9452.405196562408,
      "learning_rate": 1.9372643113233188e-07,
      "loss": 1.6953,
      "step": 266720
    },
    {
      "epoch": 0.0009680959485747803,
      "grad_norm": 9374.140493933297,
      "learning_rate": 1.9371479931694406e-07,
      "loss": 1.694,
      "step": 266752
    },
    {
      "epoch": 0.0009682120829256171,
      "grad_norm": 12156.460175560976,
      "learning_rate": 1.9370316959651382e-07,
      "loss": 1.7059,
      "step": 266784
    },
    {
      "epoch": 0.0009683282172764537,
      "grad_norm": 10441.021022869363,
      "learning_rate": 1.9369154197041236e-07,
      "loss": 1.7136,
      "step": 266816
    },
    {
      "epoch": 0.0009684443516272905,
      "grad_norm": 11239.231646335971,
      "learning_rate": 1.9367991643801118e-07,
      "loss": 1.7041,
      "step": 266848
    },
    {
      "epoch": 0.0009685604859781272,
      "grad_norm": 9442.973895971543,
      "learning_rate": 1.9366829299868195e-07,
      "loss": 1.6961,
      "step": 266880
    },
    {
      "epoch": 0.0009686766203289639,
      "grad_norm": 10267.547126748434,
      "learning_rate": 1.9365667165179677e-07,
      "loss": 1.6969,
      "step": 266912
    },
    {
      "epoch": 0.0009687927546798006,
      "grad_norm": 9440.613327533334,
      "learning_rate": 1.936450523967279e-07,
      "loss": 1.7114,
      "step": 266944
    },
    {
      "epoch": 0.0009689088890306373,
      "grad_norm": 10547.330467943062,
      "learning_rate": 1.9363343523284786e-07,
      "loss": 1.7128,
      "step": 266976
    },
    {
      "epoch": 0.000969025023381474,
      "grad_norm": 10473.060488701476,
      "learning_rate": 1.9362182015952946e-07,
      "loss": 1.7078,
      "step": 267008
    },
    {
      "epoch": 0.0009691411577323107,
      "grad_norm": 9473.708671898245,
      "learning_rate": 1.9361020717614575e-07,
      "loss": 1.7034,
      "step": 267040
    },
    {
      "epoch": 0.0009692572920831474,
      "grad_norm": 8696.405694308425,
      "learning_rate": 1.9359859628207007e-07,
      "loss": 1.7241,
      "step": 267072
    },
    {
      "epoch": 0.0009693734264339841,
      "grad_norm": 9399.488922276572,
      "learning_rate": 1.93586987476676e-07,
      "loss": 1.7289,
      "step": 267104
    },
    {
      "epoch": 0.0009694895607848208,
      "grad_norm": 10607.552780919828,
      "learning_rate": 1.9357538075933744e-07,
      "loss": 1.7225,
      "step": 267136
    },
    {
      "epoch": 0.0009696056951356576,
      "grad_norm": 10442.182913548297,
      "learning_rate": 1.9356377612942844e-07,
      "loss": 1.7107,
      "step": 267168
    },
    {
      "epoch": 0.0009697218294864942,
      "grad_norm": 8547.556843917448,
      "learning_rate": 1.9355217358632338e-07,
      "loss": 1.7089,
      "step": 267200
    },
    {
      "epoch": 0.000969837963837331,
      "grad_norm": 10636.624840615561,
      "learning_rate": 1.9354057312939697e-07,
      "loss": 1.6969,
      "step": 267232
    },
    {
      "epoch": 0.0009699540981881676,
      "grad_norm": 9197.80495553151,
      "learning_rate": 1.9352933717556724e-07,
      "loss": 1.7083,
      "step": 267264
    },
    {
      "epoch": 0.0009700702325390044,
      "grad_norm": 10537.114595561728,
      "learning_rate": 1.9351774082397847e-07,
      "loss": 1.7342,
      "step": 267296
    },
    {
      "epoch": 0.000970186366889841,
      "grad_norm": 11537.720052072678,
      "learning_rate": 1.9350614655671332e-07,
      "loss": 1.7043,
      "step": 267328
    },
    {
      "epoch": 0.0009703025012406778,
      "grad_norm": 9533.020717485093,
      "learning_rate": 1.9349455437314744e-07,
      "loss": 1.6901,
      "step": 267360
    },
    {
      "epoch": 0.0009704186355915144,
      "grad_norm": 8520.65326134094,
      "learning_rate": 1.9348296427265688e-07,
      "loss": 1.7187,
      "step": 267392
    },
    {
      "epoch": 0.0009705347699423512,
      "grad_norm": 9416.586430336632,
      "learning_rate": 1.9347137625461772e-07,
      "loss": 1.7226,
      "step": 267424
    },
    {
      "epoch": 0.000970650904293188,
      "grad_norm": 9268.71404241171,
      "learning_rate": 1.9345979031840648e-07,
      "loss": 1.7386,
      "step": 267456
    },
    {
      "epoch": 0.0009707670386440246,
      "grad_norm": 10126.085127037004,
      "learning_rate": 1.9344820646339993e-07,
      "loss": 1.7279,
      "step": 267488
    },
    {
      "epoch": 0.0009708831729948614,
      "grad_norm": 10885.098254035192,
      "learning_rate": 1.93436624688975e-07,
      "loss": 1.709,
      "step": 267520
    },
    {
      "epoch": 0.000970999307345698,
      "grad_norm": 9527.524757249388,
      "learning_rate": 1.9342504499450898e-07,
      "loss": 1.7108,
      "step": 267552
    },
    {
      "epoch": 0.0009711154416965348,
      "grad_norm": 9726.492070628547,
      "learning_rate": 1.9341346737937937e-07,
      "loss": 1.7061,
      "step": 267584
    },
    {
      "epoch": 0.0009712315760473714,
      "grad_norm": 11292.056854267074,
      "learning_rate": 1.9340189184296394e-07,
      "loss": 1.7147,
      "step": 267616
    },
    {
      "epoch": 0.0009713477103982082,
      "grad_norm": 10265.091719025213,
      "learning_rate": 1.9339031838464068e-07,
      "loss": 1.7139,
      "step": 267648
    },
    {
      "epoch": 0.0009714638447490448,
      "grad_norm": 9028.936592977048,
      "learning_rate": 1.9337874700378794e-07,
      "loss": 1.7114,
      "step": 267680
    },
    {
      "epoch": 0.0009715799790998815,
      "grad_norm": 8537.66267780591,
      "learning_rate": 1.933671776997843e-07,
      "loss": 1.7276,
      "step": 267712
    },
    {
      "epoch": 0.0009716961134507183,
      "grad_norm": 9660.34864795262,
      "learning_rate": 1.9335561047200852e-07,
      "loss": 1.7046,
      "step": 267744
    },
    {
      "epoch": 0.000971812247801555,
      "grad_norm": 10040.150596480114,
      "learning_rate": 1.9334404531983963e-07,
      "loss": 1.7117,
      "step": 267776
    },
    {
      "epoch": 0.0009719283821523917,
      "grad_norm": 10556.132246234887,
      "learning_rate": 1.9333248224265707e-07,
      "loss": 1.717,
      "step": 267808
    },
    {
      "epoch": 0.0009720445165032283,
      "grad_norm": 9185.296620142433,
      "learning_rate": 1.933209212398404e-07,
      "loss": 1.6936,
      "step": 267840
    },
    {
      "epoch": 0.0009721606508540651,
      "grad_norm": 8908.964249563469,
      "learning_rate": 1.9330936231076942e-07,
      "loss": 1.7018,
      "step": 267872
    },
    {
      "epoch": 0.0009722767852049017,
      "grad_norm": 9830.593471403443,
      "learning_rate": 1.9329780545482432e-07,
      "loss": 1.697,
      "step": 267904
    },
    {
      "epoch": 0.0009723929195557385,
      "grad_norm": 10035.789953959777,
      "learning_rate": 1.932862506713854e-07,
      "loss": 1.712,
      "step": 267936
    },
    {
      "epoch": 0.0009725090539065751,
      "grad_norm": 9740.01457904453,
      "learning_rate": 1.9327469795983336e-07,
      "loss": 1.6825,
      "step": 267968
    },
    {
      "epoch": 0.0009726251882574119,
      "grad_norm": 10137.020666842896,
      "learning_rate": 1.9326314731954903e-07,
      "loss": 1.6993,
      "step": 268000
    },
    {
      "epoch": 0.0009727413226082487,
      "grad_norm": 8505.477411644804,
      "learning_rate": 1.932515987499136e-07,
      "loss": 1.715,
      "step": 268032
    },
    {
      "epoch": 0.0009728574569590853,
      "grad_norm": 10901.760224844427,
      "learning_rate": 1.9324005225030848e-07,
      "loss": 1.7206,
      "step": 268064
    },
    {
      "epoch": 0.0009729735913099221,
      "grad_norm": 9736.855960729828,
      "learning_rate": 1.932285078201153e-07,
      "loss": 1.7258,
      "step": 268096
    },
    {
      "epoch": 0.0009730897256607587,
      "grad_norm": 9459.146895994374,
      "learning_rate": 1.9321696545871602e-07,
      "loss": 1.6981,
      "step": 268128
    },
    {
      "epoch": 0.0009732058600115955,
      "grad_norm": 10674.402278347954,
      "learning_rate": 1.9320542516549282e-07,
      "loss": 1.6917,
      "step": 268160
    },
    {
      "epoch": 0.0009733219943624321,
      "grad_norm": 10248.227163758616,
      "learning_rate": 1.9319388693982815e-07,
      "loss": 1.7046,
      "step": 268192
    },
    {
      "epoch": 0.0009734381287132689,
      "grad_norm": 8697.09146784142,
      "learning_rate": 1.9318235078110468e-07,
      "loss": 1.7289,
      "step": 268224
    },
    {
      "epoch": 0.0009735542630641055,
      "grad_norm": 19257.669640950848,
      "learning_rate": 1.931708166887054e-07,
      "loss": 1.7302,
      "step": 268256
    },
    {
      "epoch": 0.0009736703974149423,
      "grad_norm": 9151.859264652183,
      "learning_rate": 1.9315964500658573e-07,
      "loss": 1.717,
      "step": 268288
    },
    {
      "epoch": 0.000973786531765779,
      "grad_norm": 11227.842357283076,
      "learning_rate": 1.9314811498045995e-07,
      "loss": 1.7185,
      "step": 268320
    },
    {
      "epoch": 0.0009739026661166157,
      "grad_norm": 8975.254648198012,
      "learning_rate": 1.9313658701882804e-07,
      "loss": 1.7347,
      "step": 268352
    },
    {
      "epoch": 0.0009740188004674524,
      "grad_norm": 10172.207430051749,
      "learning_rate": 1.9312506112107395e-07,
      "loss": 1.7313,
      "step": 268384
    },
    {
      "epoch": 0.0009741349348182891,
      "grad_norm": 9965.413789702865,
      "learning_rate": 1.9311353728658193e-07,
      "loss": 1.7286,
      "step": 268416
    },
    {
      "epoch": 0.0009742510691691258,
      "grad_norm": 9632.010797336141,
      "learning_rate": 1.931020155147365e-07,
      "loss": 1.7189,
      "step": 268448
    },
    {
      "epoch": 0.0009743672035199625,
      "grad_norm": 9341.882465541943,
      "learning_rate": 1.930904958049224e-07,
      "loss": 1.6905,
      "step": 268480
    },
    {
      "epoch": 0.0009744833378707992,
      "grad_norm": 9555.07844028504,
      "learning_rate": 1.9307897815652457e-07,
      "loss": 1.6852,
      "step": 268512
    },
    {
      "epoch": 0.0009745994722216359,
      "grad_norm": 8956.269312610022,
      "learning_rate": 1.9306746256892842e-07,
      "loss": 1.7128,
      "step": 268544
    },
    {
      "epoch": 0.0009747156065724726,
      "grad_norm": 12965.22518122998,
      "learning_rate": 1.9305594904151936e-07,
      "loss": 1.7074,
      "step": 268576
    },
    {
      "epoch": 0.0009748317409233094,
      "grad_norm": 10910.952295743942,
      "learning_rate": 1.930444375736832e-07,
      "loss": 1.6866,
      "step": 268608
    },
    {
      "epoch": 0.000974947875274146,
      "grad_norm": 8270.712544877859,
      "learning_rate": 1.9303292816480602e-07,
      "loss": 1.7122,
      "step": 268640
    },
    {
      "epoch": 0.0009750640096249828,
      "grad_norm": 9442.01345053056,
      "learning_rate": 1.9302142081427403e-07,
      "loss": 1.7276,
      "step": 268672
    },
    {
      "epoch": 0.0009751801439758194,
      "grad_norm": 11138.534912635501,
      "learning_rate": 1.9300991552147384e-07,
      "loss": 1.7332,
      "step": 268704
    },
    {
      "epoch": 0.0009752962783266562,
      "grad_norm": 9504.183499912026,
      "learning_rate": 1.9299841228579223e-07,
      "loss": 1.7256,
      "step": 268736
    },
    {
      "epoch": 0.0009754124126774928,
      "grad_norm": 9553.308746188412,
      "learning_rate": 1.9298691110661628e-07,
      "loss": 1.7122,
      "step": 268768
    },
    {
      "epoch": 0.0009755285470283296,
      "grad_norm": 8613.975272776212,
      "learning_rate": 1.9297541198333323e-07,
      "loss": 1.7117,
      "step": 268800
    },
    {
      "epoch": 0.0009756446813791662,
      "grad_norm": 8556.03868621455,
      "learning_rate": 1.9296391491533076e-07,
      "loss": 1.7218,
      "step": 268832
    },
    {
      "epoch": 0.000975760815730003,
      "grad_norm": 9695.307318491767,
      "learning_rate": 1.9295241990199663e-07,
      "loss": 1.7375,
      "step": 268864
    },
    {
      "epoch": 0.0009758769500808397,
      "grad_norm": 8286.732287216717,
      "learning_rate": 1.9294092694271898e-07,
      "loss": 1.721,
      "step": 268896
    },
    {
      "epoch": 0.0009759930844316764,
      "grad_norm": 10356.989137775514,
      "learning_rate": 1.9292943603688607e-07,
      "loss": 1.6936,
      "step": 268928
    },
    {
      "epoch": 0.0009761092187825131,
      "grad_norm": 9690.229305852365,
      "learning_rate": 1.9291794718388654e-07,
      "loss": 1.7232,
      "step": 268960
    },
    {
      "epoch": 0.0009762253531333498,
      "grad_norm": 8960.365505937802,
      "learning_rate": 1.9290646038310924e-07,
      "loss": 1.7198,
      "step": 268992
    },
    {
      "epoch": 0.0009763414874841865,
      "grad_norm": 10214.39024122341,
      "learning_rate": 1.9289497563394322e-07,
      "loss": 1.7281,
      "step": 269024
    },
    {
      "epoch": 0.0009764576218350232,
      "grad_norm": 10800.974955993555,
      "learning_rate": 1.928834929357779e-07,
      "loss": 1.7376,
      "step": 269056
    },
    {
      "epoch": 0.00097657375618586,
      "grad_norm": 10248.887842102673,
      "learning_rate": 1.9287201228800282e-07,
      "loss": 1.7049,
      "step": 269088
    },
    {
      "epoch": 0.0009766898905366967,
      "grad_norm": 9481.296957695187,
      "learning_rate": 1.9286053369000794e-07,
      "loss": 1.7132,
      "step": 269120
    },
    {
      "epoch": 0.0009768060248875334,
      "grad_norm": 11007.650067112418,
      "learning_rate": 1.9284905714118332e-07,
      "loss": 1.7163,
      "step": 269152
    },
    {
      "epoch": 0.00097692215923837,
      "grad_norm": 11528.170540029325,
      "learning_rate": 1.928375826409193e-07,
      "loss": 1.7382,
      "step": 269184
    },
    {
      "epoch": 0.0009770382935892067,
      "grad_norm": 9597.384018575061,
      "learning_rate": 1.928261101886066e-07,
      "loss": 1.7436,
      "step": 269216
    },
    {
      "epoch": 0.0009771544279400435,
      "grad_norm": 9405.444380782867,
      "learning_rate": 1.9281463978363598e-07,
      "loss": 1.7097,
      "step": 269248
    },
    {
      "epoch": 0.0009772705622908802,
      "grad_norm": 18743.93768662284,
      "learning_rate": 1.9280317142539868e-07,
      "loss": 1.7245,
      "step": 269280
    },
    {
      "epoch": 0.0009773866966417168,
      "grad_norm": 9389.639183696037,
      "learning_rate": 1.9279206340457402e-07,
      "loss": 1.7257,
      "step": 269312
    },
    {
      "epoch": 0.0009775028309925535,
      "grad_norm": 9793.872165798368,
      "learning_rate": 1.9278059907406452e-07,
      "loss": 1.7077,
      "step": 269344
    },
    {
      "epoch": 0.0009776189653433903,
      "grad_norm": 10213.215556327008,
      "learning_rate": 1.927691367884822e-07,
      "loss": 1.7125,
      "step": 269376
    },
    {
      "epoch": 0.000977735099694227,
      "grad_norm": 9523.325679614238,
      "learning_rate": 1.9275767654721926e-07,
      "loss": 1.6992,
      "step": 269408
    },
    {
      "epoch": 0.0009778512340450636,
      "grad_norm": 10747.43727592769,
      "learning_rate": 1.9274621834966803e-07,
      "loss": 1.7093,
      "step": 269440
    },
    {
      "epoch": 0.0009779673683959003,
      "grad_norm": 11006.570219646082,
      "learning_rate": 1.9273476219522116e-07,
      "loss": 1.7073,
      "step": 269472
    },
    {
      "epoch": 0.000978083502746737,
      "grad_norm": 9315.468640921938,
      "learning_rate": 1.927233080832716e-07,
      "loss": 1.7034,
      "step": 269504
    },
    {
      "epoch": 0.0009781996370975738,
      "grad_norm": 8486.133395133498,
      "learning_rate": 1.9271185601321246e-07,
      "loss": 1.7024,
      "step": 269536
    },
    {
      "epoch": 0.0009783157714484106,
      "grad_norm": 10196.202430316887,
      "learning_rate": 1.9270040598443716e-07,
      "loss": 1.7037,
      "step": 269568
    },
    {
      "epoch": 0.0009784319057992471,
      "grad_norm": 8726.812934857719,
      "learning_rate": 1.9268895799633935e-07,
      "loss": 1.7112,
      "step": 269600
    },
    {
      "epoch": 0.0009785480401500839,
      "grad_norm": 9807.1928705415,
      "learning_rate": 1.926775120483129e-07,
      "loss": 1.7111,
      "step": 269632
    },
    {
      "epoch": 0.0009786641745009206,
      "grad_norm": 9334.957525345255,
      "learning_rate": 1.92666068139752e-07,
      "loss": 1.714,
      "step": 269664
    },
    {
      "epoch": 0.0009787803088517574,
      "grad_norm": 10467.111731514095,
      "learning_rate": 1.9265462627005106e-07,
      "loss": 1.7099,
      "step": 269696
    },
    {
      "epoch": 0.000978896443202594,
      "grad_norm": 10302.181322419054,
      "learning_rate": 1.9264318643860475e-07,
      "loss": 1.6822,
      "step": 269728
    },
    {
      "epoch": 0.0009790125775534307,
      "grad_norm": 8567.28159920053,
      "learning_rate": 1.9263174864480803e-07,
      "loss": 1.6918,
      "step": 269760
    },
    {
      "epoch": 0.0009791287119042674,
      "grad_norm": 13110.715617387177,
      "learning_rate": 1.9262031288805592e-07,
      "loss": 1.6971,
      "step": 269792
    },
    {
      "epoch": 0.0009792448462551042,
      "grad_norm": 8471.09272762375,
      "learning_rate": 1.92608879167744e-07,
      "loss": 1.7142,
      "step": 269824
    },
    {
      "epoch": 0.000979360980605941,
      "grad_norm": 9919.728121274293,
      "learning_rate": 1.9259744748326784e-07,
      "loss": 1.6862,
      "step": 269856
    },
    {
      "epoch": 0.0009794771149567775,
      "grad_norm": 9521.061390412311,
      "learning_rate": 1.9258601783402338e-07,
      "loss": 1.6901,
      "step": 269888
    },
    {
      "epoch": 0.0009795932493076142,
      "grad_norm": 8493.173965014492,
      "learning_rate": 1.9257459021940685e-07,
      "loss": 1.7099,
      "step": 269920
    },
    {
      "epoch": 0.000979709383658451,
      "grad_norm": 9561.745447354264,
      "learning_rate": 1.9256316463881453e-07,
      "loss": 1.7158,
      "step": 269952
    },
    {
      "epoch": 0.0009798255180092877,
      "grad_norm": 8288.031249941087,
      "learning_rate": 1.9255174109164322e-07,
      "loss": 1.726,
      "step": 269984
    },
    {
      "epoch": 0.0009799416523601243,
      "grad_norm": 8217.51008517787,
      "learning_rate": 1.9254031957728982e-07,
      "loss": 1.7245,
      "step": 270016
    },
    {
      "epoch": 0.000980057786710961,
      "grad_norm": 9465.64841941639,
      "learning_rate": 1.9252890009515147e-07,
      "loss": 1.717,
      "step": 270048
    },
    {
      "epoch": 0.0009801739210617978,
      "grad_norm": 11782.74280462745,
      "learning_rate": 1.9251748264462561e-07,
      "loss": 1.725,
      "step": 270080
    },
    {
      "epoch": 0.0009802900554126345,
      "grad_norm": 7909.456744935142,
      "learning_rate": 1.9250606722510988e-07,
      "loss": 1.7255,
      "step": 270112
    },
    {
      "epoch": 0.0009804061897634713,
      "grad_norm": 10464.645048925453,
      "learning_rate": 1.9249465383600226e-07,
      "loss": 1.7301,
      "step": 270144
    },
    {
      "epoch": 0.0009805223241143078,
      "grad_norm": 8738.512230351344,
      "learning_rate": 1.9248324247670087e-07,
      "loss": 1.71,
      "step": 270176
    },
    {
      "epoch": 0.0009806384584651446,
      "grad_norm": 10704.333514983546,
      "learning_rate": 1.924718331466042e-07,
      "loss": 1.7052,
      "step": 270208
    },
    {
      "epoch": 0.0009807545928159813,
      "grad_norm": 8446.050674723661,
      "learning_rate": 1.9246042584511082e-07,
      "loss": 1.7147,
      "step": 270240
    },
    {
      "epoch": 0.000980870727166818,
      "grad_norm": 8594.274605805891,
      "learning_rate": 1.9244902057161973e-07,
      "loss": 1.6988,
      "step": 270272
    },
    {
      "epoch": 0.0009809868615176546,
      "grad_norm": 9515.44008441018,
      "learning_rate": 1.924376173255301e-07,
      "loss": 1.7059,
      "step": 270304
    },
    {
      "epoch": 0.0009811029958684914,
      "grad_norm": 10704.897944399096,
      "learning_rate": 1.9242657236367094e-07,
      "loss": 1.6974,
      "step": 270336
    },
    {
      "epoch": 0.0009812191302193281,
      "grad_norm": 8996.354150432275,
      "learning_rate": 1.9241517310727298e-07,
      "loss": 1.6797,
      "step": 270368
    },
    {
      "epoch": 0.0009813352645701649,
      "grad_norm": 9115.518635821003,
      "learning_rate": 1.9240377587649422e-07,
      "loss": 1.6976,
      "step": 270400
    },
    {
      "epoch": 0.0009814513989210016,
      "grad_norm": 12109.192210878478,
      "learning_rate": 1.9239238067073487e-07,
      "loss": 1.7128,
      "step": 270432
    },
    {
      "epoch": 0.0009815675332718382,
      "grad_norm": 10164.038960964288,
      "learning_rate": 1.9238098748939522e-07,
      "loss": 1.7172,
      "step": 270464
    },
    {
      "epoch": 0.000981683667622675,
      "grad_norm": 10288.747834406284,
      "learning_rate": 1.9236959633187602e-07,
      "loss": 1.6853,
      "step": 270496
    },
    {
      "epoch": 0.0009817998019735117,
      "grad_norm": 8263.905614175419,
      "learning_rate": 1.923582071975781e-07,
      "loss": 1.7015,
      "step": 270528
    },
    {
      "epoch": 0.0009819159363243484,
      "grad_norm": 8515.079095346091,
      "learning_rate": 1.9234682008590264e-07,
      "loss": 1.7104,
      "step": 270560
    },
    {
      "epoch": 0.000982032070675185,
      "grad_norm": 8853.016209179785,
      "learning_rate": 1.923354349962511e-07,
      "loss": 1.723,
      "step": 270592
    },
    {
      "epoch": 0.0009821482050260217,
      "grad_norm": 8699.717236784194,
      "learning_rate": 1.9232405192802507e-07,
      "loss": 1.7281,
      "step": 270624
    },
    {
      "epoch": 0.0009822643393768585,
      "grad_norm": 8941.30057653807,
      "learning_rate": 1.923126708806265e-07,
      "loss": 1.7013,
      "step": 270656
    },
    {
      "epoch": 0.0009823804737276952,
      "grad_norm": 10644.512389019987,
      "learning_rate": 1.9230129185345745e-07,
      "loss": 1.6907,
      "step": 270688
    },
    {
      "epoch": 0.000982496608078532,
      "grad_norm": 10811.841471275835,
      "learning_rate": 1.922899148459203e-07,
      "loss": 1.7075,
      "step": 270720
    },
    {
      "epoch": 0.0009826127424293685,
      "grad_norm": 9560.966792118881,
      "learning_rate": 1.9227853985741783e-07,
      "loss": 1.7109,
      "step": 270752
    },
    {
      "epoch": 0.0009827288767802053,
      "grad_norm": 8859.553261875004,
      "learning_rate": 1.9226716688735278e-07,
      "loss": 1.6972,
      "step": 270784
    },
    {
      "epoch": 0.000982845011131042,
      "grad_norm": 11424.04411756187,
      "learning_rate": 1.922557959351284e-07,
      "loss": 1.6998,
      "step": 270816
    },
    {
      "epoch": 0.0009829611454818788,
      "grad_norm": 9273.038768386554,
      "learning_rate": 1.92244427000148e-07,
      "loss": 1.7062,
      "step": 270848
    },
    {
      "epoch": 0.0009830772798327153,
      "grad_norm": 10309.959650745486,
      "learning_rate": 1.9223306008181518e-07,
      "loss": 1.7111,
      "step": 270880
    },
    {
      "epoch": 0.000983193414183552,
      "grad_norm": 10162.68606225736,
      "learning_rate": 1.9222169517953387e-07,
      "loss": 1.711,
      "step": 270912
    },
    {
      "epoch": 0.0009833095485343888,
      "grad_norm": 8838.898347644914,
      "learning_rate": 1.922103322927082e-07,
      "loss": 1.7255,
      "step": 270944
    },
    {
      "epoch": 0.0009834256828852256,
      "grad_norm": 10851.364338183472,
      "learning_rate": 1.921989714207425e-07,
      "loss": 1.7116,
      "step": 270976
    },
    {
      "epoch": 0.0009835418172360623,
      "grad_norm": 10105.245568515393,
      "learning_rate": 1.921876125630414e-07,
      "loss": 1.7145,
      "step": 271008
    },
    {
      "epoch": 0.0009836579515868989,
      "grad_norm": 8794.505557448923,
      "learning_rate": 1.9217625571900975e-07,
      "loss": 1.7185,
      "step": 271040
    },
    {
      "epoch": 0.0009837740859377356,
      "grad_norm": 9108.40128672425,
      "learning_rate": 1.921649008880527e-07,
      "loss": 1.7245,
      "step": 271072
    },
    {
      "epoch": 0.0009838902202885724,
      "grad_norm": 9271.183527468324,
      "learning_rate": 1.9215354806957557e-07,
      "loss": 1.7026,
      "step": 271104
    },
    {
      "epoch": 0.0009840063546394091,
      "grad_norm": 11637.259643060303,
      "learning_rate": 1.9214219726298393e-07,
      "loss": 1.6931,
      "step": 271136
    },
    {
      "epoch": 0.0009841224889902457,
      "grad_norm": 9584.832392900775,
      "learning_rate": 1.9213084846768368e-07,
      "loss": 1.728,
      "step": 271168
    },
    {
      "epoch": 0.0009842386233410824,
      "grad_norm": 9002.42778365925,
      "learning_rate": 1.9211950168308085e-07,
      "loss": 1.7317,
      "step": 271200
    },
    {
      "epoch": 0.0009843547576919192,
      "grad_norm": 9468.507590956455,
      "learning_rate": 1.9210815690858183e-07,
      "loss": 1.7336,
      "step": 271232
    },
    {
      "epoch": 0.000984470892042756,
      "grad_norm": 10568.081566679924,
      "learning_rate": 1.9209681414359317e-07,
      "loss": 1.7101,
      "step": 271264
    },
    {
      "epoch": 0.0009845870263935927,
      "grad_norm": 8822.339712343886,
      "learning_rate": 1.9208547338752171e-07,
      "loss": 1.6924,
      "step": 271296
    },
    {
      "epoch": 0.0009847031607444292,
      "grad_norm": 18108.63087038885,
      "learning_rate": 1.9207413463977448e-07,
      "loss": 1.715,
      "step": 271328
    },
    {
      "epoch": 0.000984819295095266,
      "grad_norm": 9518.885018740377,
      "learning_rate": 1.9206315214249978e-07,
      "loss": 1.7012,
      "step": 271360
    },
    {
      "epoch": 0.0009849354294461027,
      "grad_norm": 8118.61761631868,
      "learning_rate": 1.920518173469092e-07,
      "loss": 1.7117,
      "step": 271392
    },
    {
      "epoch": 0.0009850515637969395,
      "grad_norm": 10811.36846102287,
      "learning_rate": 1.92040484557884e-07,
      "loss": 1.6913,
      "step": 271424
    },
    {
      "epoch": 0.000985167698147776,
      "grad_norm": 8724.998567335126,
      "learning_rate": 1.9202915377483237e-07,
      "loss": 1.6894,
      "step": 271456
    },
    {
      "epoch": 0.0009852838324986128,
      "grad_norm": 9326.163412679407,
      "learning_rate": 1.9201782499716249e-07,
      "loss": 1.709,
      "step": 271488
    },
    {
      "epoch": 0.0009853999668494495,
      "grad_norm": 8044.791979908492,
      "learning_rate": 1.9200649822428295e-07,
      "loss": 1.7057,
      "step": 271520
    },
    {
      "epoch": 0.0009855161012002863,
      "grad_norm": 11576.591035360972,
      "learning_rate": 1.9199517345560244e-07,
      "loss": 1.7037,
      "step": 271552
    },
    {
      "epoch": 0.000985632235551123,
      "grad_norm": 10055.001342615524,
      "learning_rate": 1.9198385069053008e-07,
      "loss": 1.7123,
      "step": 271584
    },
    {
      "epoch": 0.0009857483699019596,
      "grad_norm": 8978.18177583858,
      "learning_rate": 1.9197252992847507e-07,
      "loss": 1.6822,
      "step": 271616
    },
    {
      "epoch": 0.0009858645042527963,
      "grad_norm": 9826.432109367062,
      "learning_rate": 1.9196121116884692e-07,
      "loss": 1.6955,
      "step": 271648
    },
    {
      "epoch": 0.000985980638603633,
      "grad_norm": 7603.6200588930005,
      "learning_rate": 1.9194989441105544e-07,
      "loss": 1.7049,
      "step": 271680
    },
    {
      "epoch": 0.0009860967729544699,
      "grad_norm": 8789.512728246089,
      "learning_rate": 1.9193857965451055e-07,
      "loss": 1.7194,
      "step": 271712
    },
    {
      "epoch": 0.0009862129073053064,
      "grad_norm": 10550.9969197228,
      "learning_rate": 1.9192726689862256e-07,
      "loss": 1.7075,
      "step": 271744
    },
    {
      "epoch": 0.0009863290416561431,
      "grad_norm": 10978.240113970909,
      "learning_rate": 1.9191595614280188e-07,
      "loss": 1.7189,
      "step": 271776
    },
    {
      "epoch": 0.00098644517600698,
      "grad_norm": 11186.066869101043,
      "learning_rate": 1.9190464738645927e-07,
      "loss": 1.7399,
      "step": 271808
    },
    {
      "epoch": 0.0009865613103578167,
      "grad_norm": 11365.147777305845,
      "learning_rate": 1.918933406290057e-07,
      "loss": 1.7341,
      "step": 271840
    },
    {
      "epoch": 0.0009866774447086534,
      "grad_norm": 8868.056495083914,
      "learning_rate": 1.918820358698524e-07,
      "loss": 1.7267,
      "step": 271872
    },
    {
      "epoch": 0.00098679357905949,
      "grad_norm": 8617.016421012553,
      "learning_rate": 1.9187073310841076e-07,
      "loss": 1.7192,
      "step": 271904
    },
    {
      "epoch": 0.0009869097134103267,
      "grad_norm": 10442.32081483805,
      "learning_rate": 1.9185943234409255e-07,
      "loss": 1.702,
      "step": 271936
    },
    {
      "epoch": 0.0009870258477611635,
      "grad_norm": 11343.826691200813,
      "learning_rate": 1.9184813357630964e-07,
      "loss": 1.7043,
      "step": 271968
    },
    {
      "epoch": 0.0009871419821120002,
      "grad_norm": 10340.490510609252,
      "learning_rate": 1.918368368044743e-07,
      "loss": 1.7156,
      "step": 272000
    },
    {
      "epoch": 0.0009872581164628367,
      "grad_norm": 8569.226802926854,
      "learning_rate": 1.9182554202799883e-07,
      "loss": 1.7082,
      "step": 272032
    },
    {
      "epoch": 0.0009873742508136735,
      "grad_norm": 9705.031890725553,
      "learning_rate": 1.9181424924629596e-07,
      "loss": 1.6914,
      "step": 272064
    },
    {
      "epoch": 0.0009874903851645103,
      "grad_norm": 7877.793726672462,
      "learning_rate": 1.9180295845877866e-07,
      "loss": 1.7038,
      "step": 272096
    },
    {
      "epoch": 0.000987606519515347,
      "grad_norm": 12306.510959650584,
      "learning_rate": 1.9179166966485997e-07,
      "loss": 1.7086,
      "step": 272128
    },
    {
      "epoch": 0.0009877226538661838,
      "grad_norm": 8510.03290240408,
      "learning_rate": 1.9178038286395335e-07,
      "loss": 1.7269,
      "step": 272160
    },
    {
      "epoch": 0.0009878387882170203,
      "grad_norm": 10023.697321846865,
      "learning_rate": 1.9176909805547239e-07,
      "loss": 1.7327,
      "step": 272192
    },
    {
      "epoch": 0.000987954922567857,
      "grad_norm": 9803.230079927738,
      "learning_rate": 1.9175781523883098e-07,
      "loss": 1.7241,
      "step": 272224
    },
    {
      "epoch": 0.0009880710569186938,
      "grad_norm": 8342.646462604058,
      "learning_rate": 1.917465344134433e-07,
      "loss": 1.6836,
      "step": 272256
    },
    {
      "epoch": 0.0009881871912695306,
      "grad_norm": 11179.84168045326,
      "learning_rate": 1.917352555787236e-07,
      "loss": 1.6924,
      "step": 272288
    },
    {
      "epoch": 0.000988303325620367,
      "grad_norm": 10316.30554026004,
      "learning_rate": 1.9172397873408653e-07,
      "loss": 1.7142,
      "step": 272320
    },
    {
      "epoch": 0.0009884194599712039,
      "grad_norm": 11836.290297217283,
      "learning_rate": 1.9171305618806136e-07,
      "loss": 1.7166,
      "step": 272352
    },
    {
      "epoch": 0.0009885355943220406,
      "grad_norm": 9540.61308302564,
      "learning_rate": 1.9170178325968967e-07,
      "loss": 1.7129,
      "step": 272384
    },
    {
      "epoch": 0.0009886517286728774,
      "grad_norm": 10003.717109154977,
      "learning_rate": 1.9169051231966406e-07,
      "loss": 1.7125,
      "step": 272416
    },
    {
      "epoch": 0.0009887678630237141,
      "grad_norm": 8661.176017146863,
      "learning_rate": 1.9167924336740016e-07,
      "loss": 1.7097,
      "step": 272448
    },
    {
      "epoch": 0.0009888839973745506,
      "grad_norm": 8859.853497660106,
      "learning_rate": 1.9166797640231372e-07,
      "loss": 1.7074,
      "step": 272480
    },
    {
      "epoch": 0.0009890001317253874,
      "grad_norm": 13286.086105396125,
      "learning_rate": 1.916567114238208e-07,
      "loss": 1.6996,
      "step": 272512
    },
    {
      "epoch": 0.0009891162660762242,
      "grad_norm": 9744.73344940743,
      "learning_rate": 1.9164544843133764e-07,
      "loss": 1.6981,
      "step": 272544
    },
    {
      "epoch": 0.000989232400427061,
      "grad_norm": 8320.249755866707,
      "learning_rate": 1.916341874242808e-07,
      "loss": 1.7072,
      "step": 272576
    },
    {
      "epoch": 0.0009893485347778974,
      "grad_norm": 11534.590759970637,
      "learning_rate": 1.9162292840206703e-07,
      "loss": 1.6974,
      "step": 272608
    },
    {
      "epoch": 0.0009894646691287342,
      "grad_norm": 8252.890523931601,
      "learning_rate": 1.916116713641133e-07,
      "loss": 1.7111,
      "step": 272640
    },
    {
      "epoch": 0.000989580803479571,
      "grad_norm": 8477.53560889012,
      "learning_rate": 1.9160041630983687e-07,
      "loss": 1.7061,
      "step": 272672
    },
    {
      "epoch": 0.0009896969378304077,
      "grad_norm": 9285.423092137482,
      "learning_rate": 1.9158916323865515e-07,
      "loss": 1.7026,
      "step": 272704
    },
    {
      "epoch": 0.0009898130721812445,
      "grad_norm": 10555.23926777598,
      "learning_rate": 1.9157791214998592e-07,
      "loss": 1.7296,
      "step": 272736
    },
    {
      "epoch": 0.000989929206532081,
      "grad_norm": 9822.994248191333,
      "learning_rate": 1.9156666304324713e-07,
      "loss": 1.7279,
      "step": 272768
    },
    {
      "epoch": 0.0009900453408829178,
      "grad_norm": 8646.276886614261,
      "learning_rate": 1.9155541591785697e-07,
      "loss": 1.7307,
      "step": 272800
    },
    {
      "epoch": 0.0009901614752337545,
      "grad_norm": 10070.989822256797,
      "learning_rate": 1.915441707732338e-07,
      "loss": 1.7227,
      "step": 272832
    },
    {
      "epoch": 0.0009902776095845913,
      "grad_norm": 9975.728745309787,
      "learning_rate": 1.9153292760879638e-07,
      "loss": 1.6874,
      "step": 272864
    },
    {
      "epoch": 0.0009903937439354278,
      "grad_norm": 9145.12143167055,
      "learning_rate": 1.9152168642396359e-07,
      "loss": 1.6988,
      "step": 272896
    },
    {
      "epoch": 0.0009905098782862646,
      "grad_norm": 9471.464934211603,
      "learning_rate": 1.9151044721815456e-07,
      "loss": 1.7127,
      "step": 272928
    },
    {
      "epoch": 0.0009906260126371013,
      "grad_norm": 9649.40226128023,
      "learning_rate": 1.9149920999078868e-07,
      "loss": 1.7327,
      "step": 272960
    },
    {
      "epoch": 0.000990742146987938,
      "grad_norm": 9127.981814179955,
      "learning_rate": 1.9148797474128555e-07,
      "loss": 1.7184,
      "step": 272992
    },
    {
      "epoch": 0.0009908582813387748,
      "grad_norm": 9785.562835115821,
      "learning_rate": 1.914767414690651e-07,
      "loss": 1.7009,
      "step": 273024
    },
    {
      "epoch": 0.0009909744156896114,
      "grad_norm": 10209.096140207515,
      "learning_rate": 1.9146551017354741e-07,
      "loss": 1.7193,
      "step": 273056
    },
    {
      "epoch": 0.0009910905500404481,
      "grad_norm": 9432.202818005982,
      "learning_rate": 1.9145428085415273e-07,
      "loss": 1.7257,
      "step": 273088
    },
    {
      "epoch": 0.0009912066843912849,
      "grad_norm": 9748.501833615255,
      "learning_rate": 1.9144305351030178e-07,
      "loss": 1.6983,
      "step": 273120
    },
    {
      "epoch": 0.0009913228187421216,
      "grad_norm": 12203.731232700924,
      "learning_rate": 1.914318281414153e-07,
      "loss": 1.7124,
      "step": 273152
    },
    {
      "epoch": 0.0009914389530929582,
      "grad_norm": 9831.262787658561,
      "learning_rate": 1.914206047469143e-07,
      "loss": 1.6828,
      "step": 273184
    },
    {
      "epoch": 0.000991555087443795,
      "grad_norm": 9225.693903441626,
      "learning_rate": 1.9140938332622013e-07,
      "loss": 1.6825,
      "step": 273216
    },
    {
      "epoch": 0.0009916712217946317,
      "grad_norm": 7921.229197542512,
      "learning_rate": 1.9139816387875432e-07,
      "loss": 1.6915,
      "step": 273248
    },
    {
      "epoch": 0.0009917873561454684,
      "grad_norm": 7934.150364090663,
      "learning_rate": 1.9138694640393859e-07,
      "loss": 1.7164,
      "step": 273280
    },
    {
      "epoch": 0.0009919034904963052,
      "grad_norm": 9109.099187076623,
      "learning_rate": 1.91375730901195e-07,
      "loss": 1.6897,
      "step": 273312
    },
    {
      "epoch": 0.0009920196248471417,
      "grad_norm": 10059.632995293616,
      "learning_rate": 1.9136451736994573e-07,
      "loss": 1.6957,
      "step": 273344
    },
    {
      "epoch": 0.0009921357591979785,
      "grad_norm": 8472.391397946627,
      "learning_rate": 1.9135365614104624e-07,
      "loss": 1.7096,
      "step": 273376
    },
    {
      "epoch": 0.0009922518935488152,
      "grad_norm": 9635.889372548856,
      "learning_rate": 1.9134244648948896e-07,
      "loss": 1.7032,
      "step": 273408
    },
    {
      "epoch": 0.000992368027899652,
      "grad_norm": 8514.1343658648,
      "learning_rate": 1.913312388077122e-07,
      "loss": 1.7105,
      "step": 273440
    },
    {
      "epoch": 0.0009924841622504885,
      "grad_norm": 9267.831785266713,
      "learning_rate": 1.9132003309513912e-07,
      "loss": 1.7162,
      "step": 273472
    },
    {
      "epoch": 0.0009926002966013253,
      "grad_norm": 8765.199598411893,
      "learning_rate": 1.9130882935119317e-07,
      "loss": 1.7016,
      "step": 273504
    },
    {
      "epoch": 0.000992716430952162,
      "grad_norm": 10241.177276075246,
      "learning_rate": 1.9129762757529797e-07,
      "loss": 1.7251,
      "step": 273536
    },
    {
      "epoch": 0.0009928325653029988,
      "grad_norm": 9933.304988773878,
      "learning_rate": 1.912864277668774e-07,
      "loss": 1.7457,
      "step": 273568
    },
    {
      "epoch": 0.0009929486996538355,
      "grad_norm": 9963.7611372413,
      "learning_rate": 1.9127522992535563e-07,
      "loss": 1.7465,
      "step": 273600
    },
    {
      "epoch": 0.000993064834004672,
      "grad_norm": 11376.527413934358,
      "learning_rate": 1.9126403405015695e-07,
      "loss": 1.7171,
      "step": 273632
    },
    {
      "epoch": 0.0009931809683555088,
      "grad_norm": 12041.017232775643,
      "learning_rate": 1.9125284014070604e-07,
      "loss": 1.7372,
      "step": 273664
    },
    {
      "epoch": 0.0009932971027063456,
      "grad_norm": 9548.964760642904,
      "learning_rate": 1.912416481964277e-07,
      "loss": 1.7379,
      "step": 273696
    },
    {
      "epoch": 0.0009934132370571823,
      "grad_norm": 9608.64007027009,
      "learning_rate": 1.912304582167469e-07,
      "loss": 1.7223,
      "step": 273728
    },
    {
      "epoch": 0.0009935293714080189,
      "grad_norm": 8381.147534794982,
      "learning_rate": 1.9121927020108908e-07,
      "loss": 1.7286,
      "step": 273760
    },
    {
      "epoch": 0.0009936455057588556,
      "grad_norm": 8344.15280301122,
      "learning_rate": 1.912080841488797e-07,
      "loss": 1.7078,
      "step": 273792
    },
    {
      "epoch": 0.0009937616401096924,
      "grad_norm": 8713.121369520799,
      "learning_rate": 1.9119690005954459e-07,
      "loss": 1.7047,
      "step": 273824
    },
    {
      "epoch": 0.0009938777744605291,
      "grad_norm": 9629.359480256202,
      "learning_rate": 1.9118571793250972e-07,
      "loss": 1.701,
      "step": 273856
    },
    {
      "epoch": 0.0009939939088113659,
      "grad_norm": 10268.503883234403,
      "learning_rate": 1.911745377672013e-07,
      "loss": 1.7147,
      "step": 273888
    },
    {
      "epoch": 0.0009941100431622024,
      "grad_norm": 11041.545543989754,
      "learning_rate": 1.911633595630459e-07,
      "loss": 1.7272,
      "step": 273920
    },
    {
      "epoch": 0.0009942261775130392,
      "grad_norm": 11264.154295818218,
      "learning_rate": 1.9115218331947011e-07,
      "loss": 1.7154,
      "step": 273952
    },
    {
      "epoch": 0.000994342311863876,
      "grad_norm": 9704.378393282075,
      "learning_rate": 1.9114100903590097e-07,
      "loss": 1.7188,
      "step": 273984
    },
    {
      "epoch": 0.0009944584462147127,
      "grad_norm": 10473.946152238897,
      "learning_rate": 1.9112983671176566e-07,
      "loss": 1.7171,
      "step": 274016
    },
    {
      "epoch": 0.0009945745805655492,
      "grad_norm": 9139.498673340897,
      "learning_rate": 1.911186663464915e-07,
      "loss": 1.724,
      "step": 274048
    },
    {
      "epoch": 0.000994690714916386,
      "grad_norm": 12668.985121153155,
      "learning_rate": 1.9110749793950628e-07,
      "loss": 1.7237,
      "step": 274080
    },
    {
      "epoch": 0.0009948068492672227,
      "grad_norm": 10120.420939862135,
      "learning_rate": 1.9109633149023775e-07,
      "loss": 1.7264,
      "step": 274112
    },
    {
      "epoch": 0.0009949229836180595,
      "grad_norm": 9413.408309427568,
      "learning_rate": 1.9108516699811412e-07,
      "loss": 1.7252,
      "step": 274144
    },
    {
      "epoch": 0.0009950391179688962,
      "grad_norm": 10653.131182896417,
      "learning_rate": 1.9107400446256374e-07,
      "loss": 1.7094,
      "step": 274176
    },
    {
      "epoch": 0.0009951552523197328,
      "grad_norm": 7559.614672719767,
      "learning_rate": 1.9106284388301515e-07,
      "loss": 1.7009,
      "step": 274208
    },
    {
      "epoch": 0.0009952713866705695,
      "grad_norm": 10103.108432556784,
      "learning_rate": 1.9105168525889716e-07,
      "loss": 1.6981,
      "step": 274240
    },
    {
      "epoch": 0.0009953875210214063,
      "grad_norm": 9577.722902652802,
      "learning_rate": 1.9104052858963888e-07,
      "loss": 1.6842,
      "step": 274272
    },
    {
      "epoch": 0.000995503655372243,
      "grad_norm": 9030.4187056858,
      "learning_rate": 1.9102937387466955e-07,
      "loss": 1.6963,
      "step": 274304
    },
    {
      "epoch": 0.0009956197897230796,
      "grad_norm": 8603.496498517332,
      "learning_rate": 1.9101822111341872e-07,
      "loss": 1.7126,
      "step": 274336
    },
    {
      "epoch": 0.0009957359240739163,
      "grad_norm": 9856.904382208442,
      "learning_rate": 1.9100707030531612e-07,
      "loss": 1.7121,
      "step": 274368
    },
    {
      "epoch": 0.000995852058424753,
      "grad_norm": 10280.781876880767,
      "learning_rate": 1.9099626982197691e-07,
      "loss": 1.7127,
      "step": 274400
    },
    {
      "epoch": 0.0009959681927755898,
      "grad_norm": 8549.461035644294,
      "learning_rate": 1.9098512285746932e-07,
      "loss": 1.7011,
      "step": 274432
    },
    {
      "epoch": 0.0009960843271264266,
      "grad_norm": 10040.04661343761,
      "learning_rate": 1.909739778444184e-07,
      "loss": 1.7146,
      "step": 274464
    },
    {
      "epoch": 0.0009962004614772631,
      "grad_norm": 8999.044838203663,
      "learning_rate": 1.909628347822549e-07,
      "loss": 1.7117,
      "step": 274496
    },
    {
      "epoch": 0.0009963165958280999,
      "grad_norm": 9330.636634228127,
      "learning_rate": 1.909516936704096e-07,
      "loss": 1.7275,
      "step": 274528
    },
    {
      "epoch": 0.0009964327301789366,
      "grad_norm": 11065.131178616908,
      "learning_rate": 1.9094055450831377e-07,
      "loss": 1.7148,
      "step": 274560
    },
    {
      "epoch": 0.0009965488645297734,
      "grad_norm": 8099.795429515489,
      "learning_rate": 1.909294172953987e-07,
      "loss": 1.696,
      "step": 274592
    },
    {
      "epoch": 0.00099666499888061,
      "grad_norm": 8479.893041778298,
      "learning_rate": 1.9091828203109602e-07,
      "loss": 1.7146,
      "step": 274624
    },
    {
      "epoch": 0.0009967811332314467,
      "grad_norm": 8131.680023217835,
      "learning_rate": 1.9090714871483758e-07,
      "loss": 1.7165,
      "step": 274656
    },
    {
      "epoch": 0.0009968972675822834,
      "grad_norm": 10581.324680776032,
      "learning_rate": 1.9089601734605544e-07,
      "loss": 1.7156,
      "step": 274688
    },
    {
      "epoch": 0.0009970134019331202,
      "grad_norm": 9248.214097867762,
      "learning_rate": 1.908848879241819e-07,
      "loss": 1.7282,
      "step": 274720
    },
    {
      "epoch": 0.000997129536283957,
      "grad_norm": 11245.024499750989,
      "learning_rate": 1.9087376044864947e-07,
      "loss": 1.694,
      "step": 274752
    },
    {
      "epoch": 0.0009972456706347935,
      "grad_norm": 8152.817672436936,
      "learning_rate": 1.9086263491889096e-07,
      "loss": 1.6973,
      "step": 274784
    },
    {
      "epoch": 0.0009973618049856302,
      "grad_norm": 9699.705253253833,
      "learning_rate": 1.9085151133433933e-07,
      "loss": 1.7041,
      "step": 274816
    },
    {
      "epoch": 0.000997477939336467,
      "grad_norm": 8668.588004975205,
      "learning_rate": 1.9084038969442783e-07,
      "loss": 1.7143,
      "step": 274848
    },
    {
      "epoch": 0.0009975940736873037,
      "grad_norm": 9650.230256320312,
      "learning_rate": 1.908292699985899e-07,
      "loss": 1.6803,
      "step": 274880
    },
    {
      "epoch": 0.0009977102080381403,
      "grad_norm": 9557.678379188117,
      "learning_rate": 1.9081815224625924e-07,
      "loss": 1.6876,
      "step": 274912
    },
    {
      "epoch": 0.000997826342388977,
      "grad_norm": 8912.642817930044,
      "learning_rate": 1.9080703643686978e-07,
      "loss": 1.7152,
      "step": 274944
    },
    {
      "epoch": 0.0009979424767398138,
      "grad_norm": 8285.260647680314,
      "learning_rate": 1.9079592256985567e-07,
      "loss": 1.7002,
      "step": 274976
    },
    {
      "epoch": 0.0009980586110906505,
      "grad_norm": 8390.579241029787,
      "learning_rate": 1.9078481064465123e-07,
      "loss": 1.7095,
      "step": 275008
    },
    {
      "epoch": 0.0009981747454414873,
      "grad_norm": 9328.816002044417,
      "learning_rate": 1.9077370066069119e-07,
      "loss": 1.6996,
      "step": 275040
    },
    {
      "epoch": 0.0009982908797923238,
      "grad_norm": 9026.12375275234,
      "learning_rate": 1.9076259261741026e-07,
      "loss": 1.6796,
      "step": 275072
    },
    {
      "epoch": 0.0009984070141431606,
      "grad_norm": 9401.329905922885,
      "learning_rate": 1.9075148651424363e-07,
      "loss": 1.6978,
      "step": 275104
    },
    {
      "epoch": 0.0009985231484939973,
      "grad_norm": 10163.440362396977,
      "learning_rate": 1.9074038235062652e-07,
      "loss": 1.6912,
      "step": 275136
    },
    {
      "epoch": 0.000998639282844834,
      "grad_norm": 9956.675951340387,
      "learning_rate": 1.907292801259945e-07,
      "loss": 1.7047,
      "step": 275168
    },
    {
      "epoch": 0.0009987554171956706,
      "grad_norm": 8649.813408392114,
      "learning_rate": 1.9071817983978337e-07,
      "loss": 1.6859,
      "step": 275200
    },
    {
      "epoch": 0.0009988715515465074,
      "grad_norm": 8449.713486266857,
      "learning_rate": 1.9070708149142903e-07,
      "loss": 1.7062,
      "step": 275232
    },
    {
      "epoch": 0.0009989876858973441,
      "grad_norm": 9472.65179345256,
      "learning_rate": 1.9069598508036776e-07,
      "loss": 1.724,
      "step": 275264
    },
    {
      "epoch": 0.000999103820248181,
      "grad_norm": 8992.637655326718,
      "learning_rate": 1.9068489060603602e-07,
      "loss": 1.7326,
      "step": 275296
    },
    {
      "epoch": 0.0009992199545990176,
      "grad_norm": 9096.10587009628,
      "learning_rate": 1.906737980678705e-07,
      "loss": 1.7468,
      "step": 275328
    },
    {
      "epoch": 0.0009993360889498542,
      "grad_norm": 11102.127723999576,
      "learning_rate": 1.9066270746530806e-07,
      "loss": 1.7279,
      "step": 275360
    },
    {
      "epoch": 0.000999452223300691,
      "grad_norm": 10079.851387793373,
      "learning_rate": 1.9065196528936142e-07,
      "loss": 1.7015,
      "step": 275392
    },
    {
      "epoch": 0.0009995683576515277,
      "grad_norm": 8519.311239765806,
      "learning_rate": 1.9064087849587297e-07,
      "loss": 1.7151,
      "step": 275424
    },
    {
      "epoch": 0.0009996844920023644,
      "grad_norm": 10100.030098965051,
      "learning_rate": 1.9062979363631727e-07,
      "loss": 1.7101,
      "step": 275456
    },
    {
      "epoch": 0.000999800626353201,
      "grad_norm": 9426.495637298094,
      "learning_rate": 1.906187107101322e-07,
      "loss": 1.7043,
      "step": 275488
    },
    {
      "epoch": 0.0009999167607040377,
      "grad_norm": 9847.549542906601,
      "learning_rate": 1.9060762971675583e-07,
      "loss": 1.6895,
      "step": 275520
    },
    {
      "epoch": 0.0010000328950548745,
      "grad_norm": 10179.173836810136,
      "learning_rate": 1.9059655065562638e-07,
      "loss": 1.6928,
      "step": 275552
    },
    {
      "epoch": 0.0010001490294057112,
      "grad_norm": 8780.015034155693,
      "learning_rate": 1.9058547352618238e-07,
      "loss": 1.7088,
      "step": 275584
    },
    {
      "epoch": 0.001000265163756548,
      "grad_norm": 9161.834205005021,
      "learning_rate": 1.9057439832786254e-07,
      "loss": 1.7047,
      "step": 275616
    },
    {
      "epoch": 0.0010003812981073845,
      "grad_norm": 8278.36626394363,
      "learning_rate": 1.9056332506010585e-07,
      "loss": 1.7144,
      "step": 275648
    },
    {
      "epoch": 0.0010004974324582213,
      "grad_norm": 11236.328759875265,
      "learning_rate": 1.905522537223515e-07,
      "loss": 1.7051,
      "step": 275680
    },
    {
      "epoch": 0.001000613566809058,
      "grad_norm": 11275.987938979004,
      "learning_rate": 1.9054118431403889e-07,
      "loss": 1.6998,
      "step": 275712
    },
    {
      "epoch": 0.0010007297011598948,
      "grad_norm": 9783.392663079612,
      "learning_rate": 1.905301168346077e-07,
      "loss": 1.6866,
      "step": 275744
    },
    {
      "epoch": 0.0010008458355107313,
      "grad_norm": 8778.876693518368,
      "learning_rate": 1.905190512834978e-07,
      "loss": 1.7028,
      "step": 275776
    },
    {
      "epoch": 0.001000961969861568,
      "grad_norm": 10835.242867605692,
      "learning_rate": 1.9050798766014927e-07,
      "loss": 1.6997,
      "step": 275808
    },
    {
      "epoch": 0.0010010781042124048,
      "grad_norm": 8978.716500703205,
      "learning_rate": 1.904969259640024e-07,
      "loss": 1.691,
      "step": 275840
    },
    {
      "epoch": 0.0010011942385632416,
      "grad_norm": 9235.731914688733,
      "learning_rate": 1.9048586619449787e-07,
      "loss": 1.7061,
      "step": 275872
    },
    {
      "epoch": 0.0010013103729140784,
      "grad_norm": 8497.47868488059,
      "learning_rate": 1.9047480835107637e-07,
      "loss": 1.7249,
      "step": 275904
    },
    {
      "epoch": 0.0010014265072649149,
      "grad_norm": 10099.961188044239,
      "learning_rate": 1.904637524331789e-07,
      "loss": 1.7217,
      "step": 275936
    },
    {
      "epoch": 0.0010015426416157516,
      "grad_norm": 9982.032258012394,
      "learning_rate": 1.9045269844024676e-07,
      "loss": 1.716,
      "step": 275968
    },
    {
      "epoch": 0.0010016587759665884,
      "grad_norm": 9515.572289673386,
      "learning_rate": 1.904416463717214e-07,
      "loss": 1.6888,
      "step": 276000
    },
    {
      "epoch": 0.0010017749103174252,
      "grad_norm": 10615.958929837661,
      "learning_rate": 1.904305962270445e-07,
      "loss": 1.6873,
      "step": 276032
    },
    {
      "epoch": 0.0010018910446682617,
      "grad_norm": 9355.027311558208,
      "learning_rate": 1.9041954800565795e-07,
      "loss": 1.6961,
      "step": 276064
    },
    {
      "epoch": 0.0010020071790190984,
      "grad_norm": 10393.1793018306,
      "learning_rate": 1.9040850170700399e-07,
      "loss": 1.7168,
      "step": 276096
    },
    {
      "epoch": 0.0010021233133699352,
      "grad_norm": 11816.37947934984,
      "learning_rate": 1.903974573305249e-07,
      "loss": 1.7127,
      "step": 276128
    },
    {
      "epoch": 0.001002239447720772,
      "grad_norm": 8990.202889812888,
      "learning_rate": 1.903864148756633e-07,
      "loss": 1.7017,
      "step": 276160
    },
    {
      "epoch": 0.0010023555820716087,
      "grad_norm": 9162.507080488396,
      "learning_rate": 1.9037537434186204e-07,
      "loss": 1.7215,
      "step": 276192
    },
    {
      "epoch": 0.0010024717164224452,
      "grad_norm": 8940.958561586112,
      "learning_rate": 1.9036433572856416e-07,
      "loss": 1.7285,
      "step": 276224
    },
    {
      "epoch": 0.001002587850773282,
      "grad_norm": 10128.273890451423,
      "learning_rate": 1.9035329903521294e-07,
      "loss": 1.7348,
      "step": 276256
    },
    {
      "epoch": 0.0010027039851241188,
      "grad_norm": 9557.033430934518,
      "learning_rate": 1.9034226426125188e-07,
      "loss": 1.733,
      "step": 276288
    },
    {
      "epoch": 0.0010028201194749555,
      "grad_norm": 10885.875803076204,
      "learning_rate": 1.9033123140612472e-07,
      "loss": 1.711,
      "step": 276320
    },
    {
      "epoch": 0.001002936253825792,
      "grad_norm": 11178.012345672194,
      "learning_rate": 1.903202004692754e-07,
      "loss": 1.6873,
      "step": 276352
    },
    {
      "epoch": 0.0010030523881766288,
      "grad_norm": 19331.91433873014,
      "learning_rate": 1.9030917145014811e-07,
      "loss": 1.699,
      "step": 276384
    },
    {
      "epoch": 0.0010031685225274656,
      "grad_norm": 20821.705981979478,
      "learning_rate": 1.9029814434818723e-07,
      "loss": 1.7186,
      "step": 276416
    },
    {
      "epoch": 0.0010032846568783023,
      "grad_norm": 9738.377482928046,
      "learning_rate": 1.9028746367087394e-07,
      "loss": 1.6998,
      "step": 276448
    },
    {
      "epoch": 0.001003400791229139,
      "grad_norm": 11265.740810084351,
      "learning_rate": 1.902764403417117e-07,
      "loss": 1.7046,
      "step": 276480
    },
    {
      "epoch": 0.0010035169255799756,
      "grad_norm": 9487.349682603672,
      "learning_rate": 1.9026541892806785e-07,
      "loss": 1.7201,
      "step": 276512
    },
    {
      "epoch": 0.0010036330599308124,
      "grad_norm": 8547.708815817254,
      "learning_rate": 1.902543994293876e-07,
      "loss": 1.7215,
      "step": 276544
    },
    {
      "epoch": 0.001003749194281649,
      "grad_norm": 8747.093688763143,
      "learning_rate": 1.9024338184511656e-07,
      "loss": 1.7223,
      "step": 276576
    },
    {
      "epoch": 0.0010038653286324859,
      "grad_norm": 8537.024540201346,
      "learning_rate": 1.9023236617470037e-07,
      "loss": 1.7228,
      "step": 276608
    },
    {
      "epoch": 0.0010039814629833224,
      "grad_norm": 10881.311134233778,
      "learning_rate": 1.9022135241758512e-07,
      "loss": 1.6742,
      "step": 276640
    },
    {
      "epoch": 0.0010040975973341592,
      "grad_norm": 8518.306756627164,
      "learning_rate": 1.9021034057321697e-07,
      "loss": 1.6934,
      "step": 276672
    },
    {
      "epoch": 0.001004213731684996,
      "grad_norm": 8839.478491404343,
      "learning_rate": 1.901993306410423e-07,
      "loss": 1.7037,
      "step": 276704
    },
    {
      "epoch": 0.0010043298660358327,
      "grad_norm": 9596.807802597696,
      "learning_rate": 1.9018832262050776e-07,
      "loss": 1.6953,
      "step": 276736
    },
    {
      "epoch": 0.0010044460003866694,
      "grad_norm": 11883.658190978063,
      "learning_rate": 1.901773165110603e-07,
      "loss": 1.6914,
      "step": 276768
    },
    {
      "epoch": 0.001004562134737506,
      "grad_norm": 11019.987114330033,
      "learning_rate": 1.9016631231214694e-07,
      "loss": 1.6832,
      "step": 276800
    },
    {
      "epoch": 0.0010046782690883427,
      "grad_norm": 10332.396430644732,
      "learning_rate": 1.9015531002321504e-07,
      "loss": 1.7076,
      "step": 276832
    },
    {
      "epoch": 0.0010047944034391795,
      "grad_norm": 11332.709120064805,
      "learning_rate": 1.9014430964371213e-07,
      "loss": 1.7126,
      "step": 276864
    },
    {
      "epoch": 0.0010049105377900162,
      "grad_norm": 9963.699212641859,
      "learning_rate": 1.9013331117308596e-07,
      "loss": 1.7138,
      "step": 276896
    },
    {
      "epoch": 0.0010050266721408528,
      "grad_norm": 9318.72201538387,
      "learning_rate": 1.9012231461078454e-07,
      "loss": 1.7061,
      "step": 276928
    },
    {
      "epoch": 0.0010051428064916895,
      "grad_norm": 9877.89532238523,
      "learning_rate": 1.901113199562561e-07,
      "loss": 1.692,
      "step": 276960
    },
    {
      "epoch": 0.0010052589408425263,
      "grad_norm": 8569.067860625215,
      "learning_rate": 1.9010032720894904e-07,
      "loss": 1.7037,
      "step": 276992
    },
    {
      "epoch": 0.001005375075193363,
      "grad_norm": 13556.937412262403,
      "learning_rate": 1.9008933636831203e-07,
      "loss": 1.7268,
      "step": 277024
    },
    {
      "epoch": 0.0010054912095441998,
      "grad_norm": 10303.263075356272,
      "learning_rate": 1.9007834743379393e-07,
      "loss": 1.7374,
      "step": 277056
    },
    {
      "epoch": 0.0010056073438950363,
      "grad_norm": 9458.116937318971,
      "learning_rate": 1.9006736040484394e-07,
      "loss": 1.738,
      "step": 277088
    },
    {
      "epoch": 0.001005723478245873,
      "grad_norm": 10869.288293168049,
      "learning_rate": 1.900563752809113e-07,
      "loss": 1.7461,
      "step": 277120
    },
    {
      "epoch": 0.0010058396125967098,
      "grad_norm": 9057.13884181975,
      "learning_rate": 1.9004539206144557e-07,
      "loss": 1.7459,
      "step": 277152
    },
    {
      "epoch": 0.0010059557469475466,
      "grad_norm": 9441.099829998622,
      "learning_rate": 1.9003441074589658e-07,
      "loss": 1.7342,
      "step": 277184
    },
    {
      "epoch": 0.001006071881298383,
      "grad_norm": 10252.061841405366,
      "learning_rate": 1.9002343133371426e-07,
      "loss": 1.7147,
      "step": 277216
    },
    {
      "epoch": 0.0010061880156492199,
      "grad_norm": 10518.50578742057,
      "learning_rate": 1.9001245382434887e-07,
      "loss": 1.6942,
      "step": 277248
    },
    {
      "epoch": 0.0010063041500000566,
      "grad_norm": 9140.963844146852,
      "learning_rate": 1.9000147821725082e-07,
      "loss": 1.6941,
      "step": 277280
    },
    {
      "epoch": 0.0010064202843508934,
      "grad_norm": 10771.68844703559,
      "learning_rate": 1.8999050451187082e-07,
      "loss": 1.7025,
      "step": 277312
    },
    {
      "epoch": 0.0010065364187017301,
      "grad_norm": 10094.574186165557,
      "learning_rate": 1.8997953270765974e-07,
      "loss": 1.6993,
      "step": 277344
    },
    {
      "epoch": 0.0010066525530525667,
      "grad_norm": 9754.871603460499,
      "learning_rate": 1.8996856280406865e-07,
      "loss": 1.6945,
      "step": 277376
    },
    {
      "epoch": 0.0010067686874034034,
      "grad_norm": 10847.48689789483,
      "learning_rate": 1.8995759480054894e-07,
      "loss": 1.6851,
      "step": 277408
    },
    {
      "epoch": 0.0010068848217542402,
      "grad_norm": 20508.457572426065,
      "learning_rate": 1.8994662869655212e-07,
      "loss": 1.7003,
      "step": 277440
    },
    {
      "epoch": 0.001007000956105077,
      "grad_norm": 9222.282147060998,
      "learning_rate": 1.8993600709419815e-07,
      "loss": 1.7302,
      "step": 277472
    },
    {
      "epoch": 0.0010071170904559135,
      "grad_norm": 9449.671105387743,
      "learning_rate": 1.8992504472828515e-07,
      "loss": 1.7049,
      "step": 277504
    },
    {
      "epoch": 0.0010072332248067502,
      "grad_norm": 9923.084802620604,
      "learning_rate": 1.8991408426026813e-07,
      "loss": 1.7072,
      "step": 277536
    },
    {
      "epoch": 0.001007349359157587,
      "grad_norm": 12591.365295312498,
      "learning_rate": 1.8990312568959957e-07,
      "loss": 1.6888,
      "step": 277568
    },
    {
      "epoch": 0.0010074654935084237,
      "grad_norm": 10840.688907998421,
      "learning_rate": 1.8989216901573207e-07,
      "loss": 1.6914,
      "step": 277600
    },
    {
      "epoch": 0.0010075816278592605,
      "grad_norm": 8923.139806144472,
      "learning_rate": 1.8988121423811855e-07,
      "loss": 1.6974,
      "step": 277632
    },
    {
      "epoch": 0.001007697762210097,
      "grad_norm": 11466.808797568747,
      "learning_rate": 1.8987026135621204e-07,
      "loss": 1.721,
      "step": 277664
    },
    {
      "epoch": 0.0010078138965609338,
      "grad_norm": 8154.389002249035,
      "learning_rate": 1.8985931036946594e-07,
      "loss": 1.7116,
      "step": 277696
    },
    {
      "epoch": 0.0010079300309117705,
      "grad_norm": 10629.928503992865,
      "learning_rate": 1.8984836127733373e-07,
      "loss": 1.6988,
      "step": 277728
    },
    {
      "epoch": 0.0010080461652626073,
      "grad_norm": 8331.939630122148,
      "learning_rate": 1.8983741407926916e-07,
      "loss": 1.7074,
      "step": 277760
    },
    {
      "epoch": 0.0010081622996134438,
      "grad_norm": 10441.24168861156,
      "learning_rate": 1.8982646877472627e-07,
      "loss": 1.7075,
      "step": 277792
    },
    {
      "epoch": 0.0010082784339642806,
      "grad_norm": 9497.036169247753,
      "learning_rate": 1.8981552536315916e-07,
      "loss": 1.71,
      "step": 277824
    },
    {
      "epoch": 0.0010083945683151173,
      "grad_norm": 10263.398462497693,
      "learning_rate": 1.8980458384402232e-07,
      "loss": 1.7198,
      "step": 277856
    },
    {
      "epoch": 0.001008510702665954,
      "grad_norm": 9449.598933288122,
      "learning_rate": 1.8979364421677036e-07,
      "loss": 1.7087,
      "step": 277888
    },
    {
      "epoch": 0.0010086268370167908,
      "grad_norm": 9762.459833464105,
      "learning_rate": 1.8978270648085813e-07,
      "loss": 1.7096,
      "step": 277920
    },
    {
      "epoch": 0.0010087429713676274,
      "grad_norm": 11958.862487711782,
      "learning_rate": 1.8977177063574074e-07,
      "loss": 1.7239,
      "step": 277952
    },
    {
      "epoch": 0.0010088591057184641,
      "grad_norm": 10479.1194286543,
      "learning_rate": 1.8976083668087345e-07,
      "loss": 1.7264,
      "step": 277984
    },
    {
      "epoch": 0.0010089752400693009,
      "grad_norm": 11437.53609830369,
      "learning_rate": 1.8974990461571178e-07,
      "loss": 1.7176,
      "step": 278016
    },
    {
      "epoch": 0.0010090913744201376,
      "grad_norm": 8490.56440997888,
      "learning_rate": 1.8973897443971147e-07,
      "loss": 1.7128,
      "step": 278048
    },
    {
      "epoch": 0.0010092075087709742,
      "grad_norm": 9959.062204846397,
      "learning_rate": 1.8972804615232852e-07,
      "loss": 1.7323,
      "step": 278080
    },
    {
      "epoch": 0.001009323643121811,
      "grad_norm": 8230.289180824693,
      "learning_rate": 1.8971711975301905e-07,
      "loss": 1.7016,
      "step": 278112
    },
    {
      "epoch": 0.0010094397774726477,
      "grad_norm": 10477.291062101884,
      "learning_rate": 1.8970619524123944e-07,
      "loss": 1.7014,
      "step": 278144
    },
    {
      "epoch": 0.0010095559118234844,
      "grad_norm": 9955.419629528431,
      "learning_rate": 1.8969527261644635e-07,
      "loss": 1.7091,
      "step": 278176
    },
    {
      "epoch": 0.0010096720461743212,
      "grad_norm": 8243.094321915769,
      "learning_rate": 1.896843518780966e-07,
      "loss": 1.6952,
      "step": 278208
    },
    {
      "epoch": 0.0010097881805251577,
      "grad_norm": 11671.819224096987,
      "learning_rate": 1.8967343302564722e-07,
      "loss": 1.7059,
      "step": 278240
    },
    {
      "epoch": 0.0010099043148759945,
      "grad_norm": 10154.02107541638,
      "learning_rate": 1.896625160585555e-07,
      "loss": 1.7141,
      "step": 278272
    },
    {
      "epoch": 0.0010100204492268312,
      "grad_norm": 9712.022858292705,
      "learning_rate": 1.8965160097627894e-07,
      "loss": 1.7384,
      "step": 278304
    },
    {
      "epoch": 0.001010136583577668,
      "grad_norm": 12975.684953018857,
      "learning_rate": 1.8964068777827523e-07,
      "loss": 1.6982,
      "step": 278336
    },
    {
      "epoch": 0.0010102527179285045,
      "grad_norm": 11112.034737166727,
      "learning_rate": 1.896297764640023e-07,
      "loss": 1.6911,
      "step": 278368
    },
    {
      "epoch": 0.0010103688522793413,
      "grad_norm": 9674.029356994943,
      "learning_rate": 1.8961886703291826e-07,
      "loss": 1.713,
      "step": 278400
    },
    {
      "epoch": 0.001010484986630178,
      "grad_norm": 8834.959875404076,
      "learning_rate": 1.8960795948448153e-07,
      "loss": 1.7038,
      "step": 278432
    },
    {
      "epoch": 0.0010106011209810148,
      "grad_norm": 8347.398636701137,
      "learning_rate": 1.8959739459173994e-07,
      "loss": 1.7162,
      "step": 278464
    },
    {
      "epoch": 0.0010107172553318515,
      "grad_norm": 9136.19592609528,
      "learning_rate": 1.89586490748183e-07,
      "loss": 1.7079,
      "step": 278496
    },
    {
      "epoch": 0.001010833389682688,
      "grad_norm": 8473.772123440658,
      "learning_rate": 1.8957558878566662e-07,
      "loss": 1.6924,
      "step": 278528
    },
    {
      "epoch": 0.0010109495240335248,
      "grad_norm": 11971.240871354983,
      "learning_rate": 1.8956468870365007e-07,
      "loss": 1.6926,
      "step": 278560
    },
    {
      "epoch": 0.0010110656583843616,
      "grad_norm": 9814.626635792112,
      "learning_rate": 1.8955379050159273e-07,
      "loss": 1.7254,
      "step": 278592
    },
    {
      "epoch": 0.0010111817927351983,
      "grad_norm": 8360.52773453925,
      "learning_rate": 1.8954289417895435e-07,
      "loss": 1.722,
      "step": 278624
    },
    {
      "epoch": 0.0010112979270860349,
      "grad_norm": 11418.105797372871,
      "learning_rate": 1.895319997351947e-07,
      "loss": 1.696,
      "step": 278656
    },
    {
      "epoch": 0.0010114140614368716,
      "grad_norm": 9564.894562931679,
      "learning_rate": 1.8952110716977397e-07,
      "loss": 1.708,
      "step": 278688
    },
    {
      "epoch": 0.0010115301957877084,
      "grad_norm": 9170.076662711168,
      "learning_rate": 1.895102164821524e-07,
      "loss": 1.725,
      "step": 278720
    },
    {
      "epoch": 0.0010116463301385451,
      "grad_norm": 9133.809719936145,
      "learning_rate": 1.8949932767179056e-07,
      "loss": 1.7306,
      "step": 278752
    },
    {
      "epoch": 0.0010117624644893819,
      "grad_norm": 9122.54218954344,
      "learning_rate": 1.8948844073814923e-07,
      "loss": 1.7351,
      "step": 278784
    },
    {
      "epoch": 0.0010118785988402184,
      "grad_norm": 10550.784994492114,
      "learning_rate": 1.8947755568068928e-07,
      "loss": 1.7296,
      "step": 278816
    },
    {
      "epoch": 0.0010119947331910552,
      "grad_norm": 9675.722401970821,
      "learning_rate": 1.8946667249887195e-07,
      "loss": 1.7462,
      "step": 278848
    },
    {
      "epoch": 0.001012110867541892,
      "grad_norm": 9362.234562325386,
      "learning_rate": 1.894557911921587e-07,
      "loss": 1.742,
      "step": 278880
    },
    {
      "epoch": 0.0010122270018927287,
      "grad_norm": 10130.524369448996,
      "learning_rate": 1.8944491176001104e-07,
      "loss": 1.745,
      "step": 278912
    },
    {
      "epoch": 0.0010123431362435652,
      "grad_norm": 13198.280797134148,
      "learning_rate": 1.8943403420189085e-07,
      "loss": 1.7323,
      "step": 278944
    },
    {
      "epoch": 0.001012459270594402,
      "grad_norm": 9234.963562461955,
      "learning_rate": 1.8942315851726013e-07,
      "loss": 1.707,
      "step": 278976
    },
    {
      "epoch": 0.0010125754049452387,
      "grad_norm": 9615.397547683611,
      "learning_rate": 1.8941228470558122e-07,
      "loss": 1.7106,
      "step": 279008
    },
    {
      "epoch": 0.0010126915392960755,
      "grad_norm": 9977.027713703115,
      "learning_rate": 1.8940141276631656e-07,
      "loss": 1.7247,
      "step": 279040
    },
    {
      "epoch": 0.0010128076736469122,
      "grad_norm": 9749.913640643183,
      "learning_rate": 1.893905426989288e-07,
      "loss": 1.7282,
      "step": 279072
    },
    {
      "epoch": 0.0010129238079977488,
      "grad_norm": 11087.74188011247,
      "learning_rate": 1.8937967450288095e-07,
      "loss": 1.7087,
      "step": 279104
    },
    {
      "epoch": 0.0010130399423485855,
      "grad_norm": 11698.294918491327,
      "learning_rate": 1.8936880817763604e-07,
      "loss": 1.6779,
      "step": 279136
    },
    {
      "epoch": 0.0010131560766994223,
      "grad_norm": 11736.531003665435,
      "learning_rate": 1.8935794372265746e-07,
      "loss": 1.6844,
      "step": 279168
    },
    {
      "epoch": 0.001013272211050259,
      "grad_norm": 10354.51360518687,
      "learning_rate": 1.8934708113740877e-07,
      "loss": 1.6994,
      "step": 279200
    },
    {
      "epoch": 0.0010133883454010956,
      "grad_norm": 8767.09324690915,
      "learning_rate": 1.893362204213537e-07,
      "loss": 1.7075,
      "step": 279232
    },
    {
      "epoch": 0.0010135044797519323,
      "grad_norm": 10227.196292239629,
      "learning_rate": 1.893253615739563e-07,
      "loss": 1.6866,
      "step": 279264
    },
    {
      "epoch": 0.001013620614102769,
      "grad_norm": 10225.496760549093,
      "learning_rate": 1.8931450459468073e-07,
      "loss": 1.6792,
      "step": 279296
    },
    {
      "epoch": 0.0010137367484536058,
      "grad_norm": 10464.577774568834,
      "learning_rate": 1.8930364948299143e-07,
      "loss": 1.6944,
      "step": 279328
    },
    {
      "epoch": 0.0010138528828044426,
      "grad_norm": 10001.332311247337,
      "learning_rate": 1.8929279623835306e-07,
      "loss": 1.7105,
      "step": 279360
    },
    {
      "epoch": 0.0010139690171552791,
      "grad_norm": 8575.210901196542,
      "learning_rate": 1.892819448602304e-07,
      "loss": 1.7161,
      "step": 279392
    },
    {
      "epoch": 0.0010140851515061159,
      "grad_norm": 9358.166914519104,
      "learning_rate": 1.8927109534808856e-07,
      "loss": 1.7197,
      "step": 279424
    },
    {
      "epoch": 0.0010142012858569526,
      "grad_norm": 9937.332841361409,
      "learning_rate": 1.892602477013928e-07,
      "loss": 1.7013,
      "step": 279456
    },
    {
      "epoch": 0.0010143174202077894,
      "grad_norm": 9276.602610869995,
      "learning_rate": 1.8924974082206603e-07,
      "loss": 1.6991,
      "step": 279488
    },
    {
      "epoch": 0.001014433554558626,
      "grad_norm": 9993.03637539662,
      "learning_rate": 1.892388968464054e-07,
      "loss": 1.6891,
      "step": 279520
    },
    {
      "epoch": 0.0010145496889094627,
      "grad_norm": 8186.637527092548,
      "learning_rate": 1.892280547346047e-07,
      "loss": 1.7127,
      "step": 279552
    },
    {
      "epoch": 0.0010146658232602994,
      "grad_norm": 11628.672667161974,
      "learning_rate": 1.8921721448612997e-07,
      "loss": 1.7014,
      "step": 279584
    },
    {
      "epoch": 0.0010147819576111362,
      "grad_norm": 9592.45182422096,
      "learning_rate": 1.8920637610044765e-07,
      "loss": 1.6923,
      "step": 279616
    },
    {
      "epoch": 0.001014898091961973,
      "grad_norm": 8672.567901146696,
      "learning_rate": 1.8919553957702425e-07,
      "loss": 1.7258,
      "step": 279648
    },
    {
      "epoch": 0.0010150142263128095,
      "grad_norm": 8545.40859175265,
      "learning_rate": 1.891847049153265e-07,
      "loss": 1.707,
      "step": 279680
    },
    {
      "epoch": 0.0010151303606636462,
      "grad_norm": 9656.18641079386,
      "learning_rate": 1.8917387211482144e-07,
      "loss": 1.7262,
      "step": 279712
    },
    {
      "epoch": 0.001015246495014483,
      "grad_norm": 11512.08790793399,
      "learning_rate": 1.8916304117497626e-07,
      "loss": 1.7258,
      "step": 279744
    },
    {
      "epoch": 0.0010153626293653197,
      "grad_norm": 10749.609667332112,
      "learning_rate": 1.8915221209525833e-07,
      "loss": 1.699,
      "step": 279776
    },
    {
      "epoch": 0.0010154787637161563,
      "grad_norm": 10798.179105756673,
      "learning_rate": 1.8914138487513536e-07,
      "loss": 1.7081,
      "step": 279808
    },
    {
      "epoch": 0.001015594898066993,
      "grad_norm": 12429.008005468497,
      "learning_rate": 1.8913055951407509e-07,
      "loss": 1.7056,
      "step": 279840
    },
    {
      "epoch": 0.0010157110324178298,
      "grad_norm": 10733.287474022114,
      "learning_rate": 1.891197360115456e-07,
      "loss": 1.7004,
      "step": 279872
    },
    {
      "epoch": 0.0010158271667686665,
      "grad_norm": 10162.936977075082,
      "learning_rate": 1.8910891436701518e-07,
      "loss": 1.6867,
      "step": 279904
    },
    {
      "epoch": 0.0010159433011195033,
      "grad_norm": 9357.720021458219,
      "learning_rate": 1.8909809457995228e-07,
      "loss": 1.6972,
      "step": 279936
    },
    {
      "epoch": 0.0010160594354703398,
      "grad_norm": 9812.012433746708,
      "learning_rate": 1.8908727664982565e-07,
      "loss": 1.7156,
      "step": 279968
    },
    {
      "epoch": 0.0010161755698211766,
      "grad_norm": 10240.815201926065,
      "learning_rate": 1.8907646057610408e-07,
      "loss": 1.718,
      "step": 280000
    },
    {
      "epoch": 0.0010162917041720133,
      "grad_norm": 10173.532031698725,
      "learning_rate": 1.890656463582568e-07,
      "loss": 1.726,
      "step": 280032
    },
    {
      "epoch": 0.00101640783852285,
      "grad_norm": 9742.283099971997,
      "learning_rate": 1.8905483399575306e-07,
      "loss": 1.7271,
      "step": 280064
    },
    {
      "epoch": 0.0010165239728736866,
      "grad_norm": 9291.654750366051,
      "learning_rate": 1.8904402348806243e-07,
      "loss": 1.6902,
      "step": 280096
    },
    {
      "epoch": 0.0010166401072245234,
      "grad_norm": 8966.49942842802,
      "learning_rate": 1.8903321483465468e-07,
      "loss": 1.6849,
      "step": 280128
    },
    {
      "epoch": 0.0010167562415753601,
      "grad_norm": 9953.54067656329,
      "learning_rate": 1.8902240803499975e-07,
      "loss": 1.6954,
      "step": 280160
    },
    {
      "epoch": 0.001016872375926197,
      "grad_norm": 9499.49956576661,
      "learning_rate": 1.8901160308856784e-07,
      "loss": 1.7027,
      "step": 280192
    },
    {
      "epoch": 0.0010169885102770337,
      "grad_norm": 9071.511891630855,
      "learning_rate": 1.8900079999482926e-07,
      "loss": 1.6843,
      "step": 280224
    },
    {
      "epoch": 0.0010171046446278702,
      "grad_norm": 11244.42617477655,
      "learning_rate": 1.8898999875325472e-07,
      "loss": 1.6949,
      "step": 280256
    },
    {
      "epoch": 0.001017220778978707,
      "grad_norm": 9203.857452177319,
      "learning_rate": 1.8897919936331495e-07,
      "loss": 1.6991,
      "step": 280288
    },
    {
      "epoch": 0.0010173369133295437,
      "grad_norm": 9153.347256605093,
      "learning_rate": 1.8896840182448104e-07,
      "loss": 1.6989,
      "step": 280320
    },
    {
      "epoch": 0.0010174530476803805,
      "grad_norm": 11458.756651574377,
      "learning_rate": 1.8895760613622417e-07,
      "loss": 1.7115,
      "step": 280352
    },
    {
      "epoch": 0.001017569182031217,
      "grad_norm": 8762.159208779534,
      "learning_rate": 1.8894681229801582e-07,
      "loss": 1.6884,
      "step": 280384
    },
    {
      "epoch": 0.0010176853163820537,
      "grad_norm": 10180.324945697952,
      "learning_rate": 1.8893602030932763e-07,
      "loss": 1.6834,
      "step": 280416
    },
    {
      "epoch": 0.0010178014507328905,
      "grad_norm": 9098.110792906404,
      "learning_rate": 1.8892523016963147e-07,
      "loss": 1.6882,
      "step": 280448
    },
    {
      "epoch": 0.0010179175850837273,
      "grad_norm": 9231.648606830742,
      "learning_rate": 1.8891477898452602e-07,
      "loss": 1.7049,
      "step": 280480
    },
    {
      "epoch": 0.001018033719434564,
      "grad_norm": 9269.589419170625,
      "learning_rate": 1.8890399248349036e-07,
      "loss": 1.7093,
      "step": 280512
    },
    {
      "epoch": 0.0010181498537854005,
      "grad_norm": 11497.610882265932,
      "learning_rate": 1.888932078298801e-07,
      "loss": 1.7068,
      "step": 280544
    },
    {
      "epoch": 0.0010182659881362373,
      "grad_norm": 10588.510943470757,
      "learning_rate": 1.888824250231679e-07,
      "loss": 1.7308,
      "step": 280576
    },
    {
      "epoch": 0.001018382122487074,
      "grad_norm": 8714.316266925363,
      "learning_rate": 1.8887164406282677e-07,
      "loss": 1.7501,
      "step": 280608
    },
    {
      "epoch": 0.0010184982568379108,
      "grad_norm": 8517.944470351986,
      "learning_rate": 1.8886086494832972e-07,
      "loss": 1.7465,
      "step": 280640
    },
    {
      "epoch": 0.0010186143911887473,
      "grad_norm": 9496.376361539174,
      "learning_rate": 1.888500876791502e-07,
      "loss": 1.7295,
      "step": 280672
    },
    {
      "epoch": 0.001018730525539584,
      "grad_norm": 7661.8839719745165,
      "learning_rate": 1.888393122547617e-07,
      "loss": 1.7113,
      "step": 280704
    },
    {
      "epoch": 0.0010188466598904209,
      "grad_norm": 8342.709631768326,
      "learning_rate": 1.8882853867463802e-07,
      "loss": 1.6844,
      "step": 280736
    },
    {
      "epoch": 0.0010189627942412576,
      "grad_norm": 9870.270310381575,
      "learning_rate": 1.8881776693825306e-07,
      "loss": 1.6848,
      "step": 280768
    },
    {
      "epoch": 0.0010190789285920944,
      "grad_norm": 10740.428110648105,
      "learning_rate": 1.8880699704508105e-07,
      "loss": 1.7044,
      "step": 280800
    },
    {
      "epoch": 0.001019195062942931,
      "grad_norm": 13564.65981880858,
      "learning_rate": 1.8879622899459633e-07,
      "loss": 1.7067,
      "step": 280832
    },
    {
      "epoch": 0.0010193111972937677,
      "grad_norm": 11208.499810411739,
      "learning_rate": 1.8878546278627354e-07,
      "loss": 1.6803,
      "step": 280864
    },
    {
      "epoch": 0.0010194273316446044,
      "grad_norm": 8691.041249470629,
      "learning_rate": 1.8877469841958755e-07,
      "loss": 1.6984,
      "step": 280896
    },
    {
      "epoch": 0.0010195434659954412,
      "grad_norm": 9992.127501188124,
      "learning_rate": 1.8876393589401327e-07,
      "loss": 1.7114,
      "step": 280928
    },
    {
      "epoch": 0.0010196596003462777,
      "grad_norm": 9601.200966545799,
      "learning_rate": 1.8875317520902592e-07,
      "loss": 1.718,
      "step": 280960
    },
    {
      "epoch": 0.0010197757346971145,
      "grad_norm": 10621.321198419715,
      "learning_rate": 1.8874241636410104e-07,
      "loss": 1.7227,
      "step": 280992
    },
    {
      "epoch": 0.0010198918690479512,
      "grad_norm": 9134.769838370314,
      "learning_rate": 1.8873165935871422e-07,
      "loss": 1.6835,
      "step": 281024
    },
    {
      "epoch": 0.001020008003398788,
      "grad_norm": 9634.342323168717,
      "learning_rate": 1.8872090419234129e-07,
      "loss": 1.6908,
      "step": 281056
    },
    {
      "epoch": 0.0010201241377496247,
      "grad_norm": 8875.97521402578,
      "learning_rate": 1.8871015086445834e-07,
      "loss": 1.6983,
      "step": 281088
    },
    {
      "epoch": 0.0010202402721004613,
      "grad_norm": 10056.82335531454,
      "learning_rate": 1.8869939937454167e-07,
      "loss": 1.7045,
      "step": 281120
    },
    {
      "epoch": 0.001020356406451298,
      "grad_norm": 9506.20618333097,
      "learning_rate": 1.8868864972206774e-07,
      "loss": 1.7029,
      "step": 281152
    },
    {
      "epoch": 0.0010204725408021348,
      "grad_norm": 11461.811026186046,
      "learning_rate": 1.8867790190651322e-07,
      "loss": 1.7076,
      "step": 281184
    },
    {
      "epoch": 0.0010205886751529715,
      "grad_norm": 10210.898883056281,
      "learning_rate": 1.8866715592735503e-07,
      "loss": 1.729,
      "step": 281216
    },
    {
      "epoch": 0.001020704809503808,
      "grad_norm": 8882.067101750583,
      "learning_rate": 1.8865641178407026e-07,
      "loss": 1.7218,
      "step": 281248
    },
    {
      "epoch": 0.0010208209438546448,
      "grad_norm": 9979.87775476233,
      "learning_rate": 1.8864566947613625e-07,
      "loss": 1.6985,
      "step": 281280
    },
    {
      "epoch": 0.0010209370782054816,
      "grad_norm": 9955.637397977087,
      "learning_rate": 1.8863492900303053e-07,
      "loss": 1.7089,
      "step": 281312
    },
    {
      "epoch": 0.0010210532125563183,
      "grad_norm": 8980.120823240632,
      "learning_rate": 1.8862419036423082e-07,
      "loss": 1.6858,
      "step": 281344
    },
    {
      "epoch": 0.001021169346907155,
      "grad_norm": 8569.889380849674,
      "learning_rate": 1.8861345355921504e-07,
      "loss": 1.6946,
      "step": 281376
    },
    {
      "epoch": 0.0010212854812579916,
      "grad_norm": 12865.723143298243,
      "learning_rate": 1.886027185874614e-07,
      "loss": 1.7115,
      "step": 281408
    },
    {
      "epoch": 0.0010214016156088284,
      "grad_norm": 8732.576481199578,
      "learning_rate": 1.8859198544844817e-07,
      "loss": 1.7162,
      "step": 281440
    },
    {
      "epoch": 0.0010215177499596651,
      "grad_norm": 16809.929684564417,
      "learning_rate": 1.8858125414165399e-07,
      "loss": 1.713,
      "step": 281472
    },
    {
      "epoch": 0.0010216338843105019,
      "grad_norm": 10424.712753836433,
      "learning_rate": 1.885708599349336e-07,
      "loss": 1.7113,
      "step": 281504
    },
    {
      "epoch": 0.0010217500186613384,
      "grad_norm": 7719.478479793826,
      "learning_rate": 1.885601322337976e-07,
      "loss": 1.7304,
      "step": 281536
    },
    {
      "epoch": 0.0010218661530121752,
      "grad_norm": 10592.466568273889,
      "learning_rate": 1.8854940636333382e-07,
      "loss": 1.7244,
      "step": 281568
    },
    {
      "epoch": 0.001021982287363012,
      "grad_norm": 9558.189159040536,
      "learning_rate": 1.8853868232302166e-07,
      "loss": 1.7184,
      "step": 281600
    },
    {
      "epoch": 0.0010220984217138487,
      "grad_norm": 8449.447674256586,
      "learning_rate": 1.8852796011234073e-07,
      "loss": 1.6986,
      "step": 281632
    },
    {
      "epoch": 0.0010222145560646854,
      "grad_norm": 8980.183071630556,
      "learning_rate": 1.885172397307708e-07,
      "loss": 1.6786,
      "step": 281664
    },
    {
      "epoch": 0.001022330690415522,
      "grad_norm": 10851.55325287583,
      "learning_rate": 1.8850652117779194e-07,
      "loss": 1.689,
      "step": 281696
    },
    {
      "epoch": 0.0010224468247663587,
      "grad_norm": 11946.470441096819,
      "learning_rate": 1.8849580445288427e-07,
      "loss": 1.7073,
      "step": 281728
    },
    {
      "epoch": 0.0010225629591171955,
      "grad_norm": 9394.195867662118,
      "learning_rate": 1.8848508955552832e-07,
      "loss": 1.7142,
      "step": 281760
    },
    {
      "epoch": 0.0010226790934680322,
      "grad_norm": 11209.676890972372,
      "learning_rate": 1.8847437648520462e-07,
      "loss": 1.7084,
      "step": 281792
    },
    {
      "epoch": 0.0010227952278188688,
      "grad_norm": 10925.320681792366,
      "learning_rate": 1.884636652413941e-07,
      "loss": 1.7252,
      "step": 281824
    },
    {
      "epoch": 0.0010229113621697055,
      "grad_norm": 8512.417752906633,
      "learning_rate": 1.8845295582357778e-07,
      "loss": 1.7259,
      "step": 281856
    },
    {
      "epoch": 0.0010230274965205423,
      "grad_norm": 9151.224836053369,
      "learning_rate": 1.8844224823123683e-07,
      "loss": 1.7002,
      "step": 281888
    },
    {
      "epoch": 0.001023143630871379,
      "grad_norm": 9230.058613031664,
      "learning_rate": 1.8843154246385277e-07,
      "loss": 1.7072,
      "step": 281920
    },
    {
      "epoch": 0.0010232597652222158,
      "grad_norm": 9293.6089868253,
      "learning_rate": 1.8842083852090733e-07,
      "loss": 1.6999,
      "step": 281952
    },
    {
      "epoch": 0.0010233758995730523,
      "grad_norm": 9429.882714010817,
      "learning_rate": 1.8841013640188224e-07,
      "loss": 1.6935,
      "step": 281984
    },
    {
      "epoch": 0.001023492033923889,
      "grad_norm": 9293.258847142912,
      "learning_rate": 1.8839943610625969e-07,
      "loss": 1.703,
      "step": 282016
    },
    {
      "epoch": 0.0010236081682747258,
      "grad_norm": 10794.915284521692,
      "learning_rate": 1.883887376335219e-07,
      "loss": 1.7109,
      "step": 282048
    },
    {
      "epoch": 0.0010237243026255626,
      "grad_norm": 10674.802480608249,
      "learning_rate": 1.883780409831513e-07,
      "loss": 1.7186,
      "step": 282080
    },
    {
      "epoch": 0.0010238404369763991,
      "grad_norm": 10578.935485198877,
      "learning_rate": 1.8836734615463075e-07,
      "loss": 1.6927,
      "step": 282112
    },
    {
      "epoch": 0.0010239565713272359,
      "grad_norm": 9962.875488532414,
      "learning_rate": 1.88356653147443e-07,
      "loss": 1.686,
      "step": 282144
    },
    {
      "epoch": 0.0010240727056780726,
      "grad_norm": 9311.572692085907,
      "learning_rate": 1.883459619610712e-07,
      "loss": 1.706,
      "step": 282176
    },
    {
      "epoch": 0.0010241888400289094,
      "grad_norm": 10529.381558287267,
      "learning_rate": 1.8833527259499864e-07,
      "loss": 1.7075,
      "step": 282208
    },
    {
      "epoch": 0.0010243049743797461,
      "grad_norm": 8169.879925678222,
      "learning_rate": 1.8832458504870886e-07,
      "loss": 1.7168,
      "step": 282240
    },
    {
      "epoch": 0.0010244211087305827,
      "grad_norm": 9068.443857685837,
      "learning_rate": 1.8831389932168562e-07,
      "loss": 1.6962,
      "step": 282272
    },
    {
      "epoch": 0.0010245372430814194,
      "grad_norm": 8403.62314719074,
      "learning_rate": 1.8830321541341277e-07,
      "loss": 1.7014,
      "step": 282304
    },
    {
      "epoch": 0.0010246533774322562,
      "grad_norm": 9239.554318255832,
      "learning_rate": 1.8829253332337442e-07,
      "loss": 1.715,
      "step": 282336
    },
    {
      "epoch": 0.001024769511783093,
      "grad_norm": 9591.940575295492,
      "learning_rate": 1.8828185305105498e-07,
      "loss": 1.7372,
      "step": 282368
    },
    {
      "epoch": 0.0010248856461339295,
      "grad_norm": 11320.159362835842,
      "learning_rate": 1.8827117459593895e-07,
      "loss": 1.7429,
      "step": 282400
    },
    {
      "epoch": 0.0010250017804847662,
      "grad_norm": 13584.401495833374,
      "learning_rate": 1.8826049795751108e-07,
      "loss": 1.7173,
      "step": 282432
    },
    {
      "epoch": 0.001025117914835603,
      "grad_norm": 8775.457822814717,
      "learning_rate": 1.8824982313525631e-07,
      "loss": 1.7104,
      "step": 282464
    },
    {
      "epoch": 0.0010252340491864397,
      "grad_norm": 8397.075205093735,
      "learning_rate": 1.8823948363263792e-07,
      "loss": 1.7039,
      "step": 282496
    },
    {
      "epoch": 0.0010253501835372765,
      "grad_norm": 8550.145495838067,
      "learning_rate": 1.8822881238446957e-07,
      "loss": 1.6973,
      "step": 282528
    },
    {
      "epoch": 0.001025466317888113,
      "grad_norm": 11156.826789011291,
      "learning_rate": 1.8821814295094646e-07,
      "loss": 1.7079,
      "step": 282560
    },
    {
      "epoch": 0.0010255824522389498,
      "grad_norm": 9992.713345233115,
      "learning_rate": 1.882074753315543e-07,
      "loss": 1.6967,
      "step": 282592
    },
    {
      "epoch": 0.0010256985865897865,
      "grad_norm": 9443.322932103933,
      "learning_rate": 1.8819680952577918e-07,
      "loss": 1.6814,
      "step": 282624
    },
    {
      "epoch": 0.0010258147209406233,
      "grad_norm": 9557.009992670302,
      "learning_rate": 1.8818614553310714e-07,
      "loss": 1.6902,
      "step": 282656
    },
    {
      "epoch": 0.0010259308552914598,
      "grad_norm": 11303.705764040393,
      "learning_rate": 1.881754833530246e-07,
      "loss": 1.7151,
      "step": 282688
    },
    {
      "epoch": 0.0010260469896422966,
      "grad_norm": 9246.501176120619,
      "learning_rate": 1.8816482298501814e-07,
      "loss": 1.7137,
      "step": 282720
    },
    {
      "epoch": 0.0010261631239931333,
      "grad_norm": 9629.884942199466,
      "learning_rate": 1.8815416442857455e-07,
      "loss": 1.6922,
      "step": 282752
    },
    {
      "epoch": 0.00102627925834397,
      "grad_norm": 8222.84683062989,
      "learning_rate": 1.8814350768318073e-07,
      "loss": 1.7063,
      "step": 282784
    },
    {
      "epoch": 0.0010263953926948068,
      "grad_norm": 9801.934502943794,
      "learning_rate": 1.88132852748324e-07,
      "loss": 1.7136,
      "step": 282816
    },
    {
      "epoch": 0.0010265115270456434,
      "grad_norm": 9335.740998978068,
      "learning_rate": 1.881221996234916e-07,
      "loss": 1.7114,
      "step": 282848
    },
    {
      "epoch": 0.0010266276613964801,
      "grad_norm": 10518.202508033395,
      "learning_rate": 1.8811154830817122e-07,
      "loss": 1.7206,
      "step": 282880
    },
    {
      "epoch": 0.0010267437957473169,
      "grad_norm": 9697.858732730643,
      "learning_rate": 1.8810089880185062e-07,
      "loss": 1.6993,
      "step": 282912
    },
    {
      "epoch": 0.0010268599300981536,
      "grad_norm": 10831.16041797923,
      "learning_rate": 1.8809025110401776e-07,
      "loss": 1.7033,
      "step": 282944
    },
    {
      "epoch": 0.0010269760644489902,
      "grad_norm": 8525.556521424276,
      "learning_rate": 1.8807960521416092e-07,
      "loss": 1.7151,
      "step": 282976
    },
    {
      "epoch": 0.001027092198799827,
      "grad_norm": 10532.25028187234,
      "learning_rate": 1.8806896113176842e-07,
      "loss": 1.7229,
      "step": 283008
    },
    {
      "epoch": 0.0010272083331506637,
      "grad_norm": 9712.826365173012,
      "learning_rate": 1.8805831885632894e-07,
      "loss": 1.6889,
      "step": 283040
    },
    {
      "epoch": 0.0010273244675015004,
      "grad_norm": 10695.859385762324,
      "learning_rate": 1.8804767838733127e-07,
      "loss": 1.6878,
      "step": 283072
    },
    {
      "epoch": 0.0010274406018523372,
      "grad_norm": 9601.091812913779,
      "learning_rate": 1.8803703972426432e-07,
      "loss": 1.7021,
      "step": 283104
    },
    {
      "epoch": 0.0010275567362031737,
      "grad_norm": 9306.799449864599,
      "learning_rate": 1.8802640286661743e-07,
      "loss": 1.7156,
      "step": 283136
    },
    {
      "epoch": 0.0010276728705540105,
      "grad_norm": 9403.053546587938,
      "learning_rate": 1.8801576781387998e-07,
      "loss": 1.7285,
      "step": 283168
    },
    {
      "epoch": 0.0010277890049048472,
      "grad_norm": 9169.131365620191,
      "learning_rate": 1.8800513456554154e-07,
      "loss": 1.7266,
      "step": 283200
    },
    {
      "epoch": 0.001027905139255684,
      "grad_norm": 8483.427962799002,
      "learning_rate": 1.8799450312109192e-07,
      "loss": 1.7066,
      "step": 283232
    },
    {
      "epoch": 0.0010280212736065205,
      "grad_norm": 10334.29862158047,
      "learning_rate": 1.8798387348002123e-07,
      "loss": 1.705,
      "step": 283264
    },
    {
      "epoch": 0.0010281374079573573,
      "grad_norm": 10747.122219459496,
      "learning_rate": 1.8797324564181963e-07,
      "loss": 1.7136,
      "step": 283296
    },
    {
      "epoch": 0.001028253542308194,
      "grad_norm": 10389.890278535187,
      "learning_rate": 1.8796261960597756e-07,
      "loss": 1.7265,
      "step": 283328
    },
    {
      "epoch": 0.0010283696766590308,
      "grad_norm": 8366.680106230906,
      "learning_rate": 1.879519953719856e-07,
      "loss": 1.6992,
      "step": 283360
    },
    {
      "epoch": 0.0010284858110098675,
      "grad_norm": 10321.71284235325,
      "learning_rate": 1.8794137293933463e-07,
      "loss": 1.6843,
      "step": 283392
    },
    {
      "epoch": 0.001028601945360704,
      "grad_norm": 9973.41847111611,
      "learning_rate": 1.8793075230751567e-07,
      "loss": 1.7154,
      "step": 283424
    },
    {
      "epoch": 0.0010287180797115408,
      "grad_norm": 9487.20823003269,
      "learning_rate": 1.8792013347601995e-07,
      "loss": 1.7198,
      "step": 283456
    },
    {
      "epoch": 0.0010288342140623776,
      "grad_norm": 9274.775900257644,
      "learning_rate": 1.8790951644433888e-07,
      "loss": 1.7234,
      "step": 283488
    },
    {
      "epoch": 0.0010289503484132143,
      "grad_norm": 16089.634675778068,
      "learning_rate": 1.8789890121196408e-07,
      "loss": 1.7314,
      "step": 283520
    },
    {
      "epoch": 0.0010290664827640509,
      "grad_norm": 9763.000768206464,
      "learning_rate": 1.8788861942096385e-07,
      "loss": 1.718,
      "step": 283552
    },
    {
      "epoch": 0.0010291826171148876,
      "grad_norm": 9377.16087096729,
      "learning_rate": 1.878780077294885e-07,
      "loss": 1.733,
      "step": 283584
    },
    {
      "epoch": 0.0010292987514657244,
      "grad_norm": 11744.362051639926,
      "learning_rate": 1.8786739783581138e-07,
      "loss": 1.7184,
      "step": 283616
    },
    {
      "epoch": 0.0010294148858165611,
      "grad_norm": 8195.73328018915,
      "learning_rate": 1.8785678973942494e-07,
      "loss": 1.7114,
      "step": 283648
    },
    {
      "epoch": 0.001029531020167398,
      "grad_norm": 10763.75454941258,
      "learning_rate": 1.8784618343982186e-07,
      "loss": 1.6955,
      "step": 283680
    },
    {
      "epoch": 0.0010296471545182344,
      "grad_norm": 9299.49331953091,
      "learning_rate": 1.878355789364949e-07,
      "loss": 1.7035,
      "step": 283712
    },
    {
      "epoch": 0.0010297632888690712,
      "grad_norm": 8278.235198398268,
      "learning_rate": 1.8782497622893707e-07,
      "loss": 1.7202,
      "step": 283744
    },
    {
      "epoch": 0.001029879423219908,
      "grad_norm": 8671.560182573838,
      "learning_rate": 1.878143753166417e-07,
      "loss": 1.7247,
      "step": 283776
    },
    {
      "epoch": 0.0010299955575707447,
      "grad_norm": 10696.957324398372,
      "learning_rate": 1.8780377619910218e-07,
      "loss": 1.7143,
      "step": 283808
    },
    {
      "epoch": 0.0010301116919215812,
      "grad_norm": 8151.413129022476,
      "learning_rate": 1.8779317887581209e-07,
      "loss": 1.7119,
      "step": 283840
    },
    {
      "epoch": 0.001030227826272418,
      "grad_norm": 10284.406059661394,
      "learning_rate": 1.877825833462653e-07,
      "loss": 1.6971,
      "step": 283872
    },
    {
      "epoch": 0.0010303439606232547,
      "grad_norm": 10265.374420838238,
      "learning_rate": 1.8777198960995585e-07,
      "loss": 1.7032,
      "step": 283904
    },
    {
      "epoch": 0.0010304600949740915,
      "grad_norm": 10387.504608903912,
      "learning_rate": 1.8776139766637797e-07,
      "loss": 1.7192,
      "step": 283936
    },
    {
      "epoch": 0.0010305762293249282,
      "grad_norm": 9096.305623713399,
      "learning_rate": 1.877508075150261e-07,
      "loss": 1.7174,
      "step": 283968
    },
    {
      "epoch": 0.0010306923636757648,
      "grad_norm": 10743.213671895388,
      "learning_rate": 1.8774021915539482e-07,
      "loss": 1.7078,
      "step": 284000
    },
    {
      "epoch": 0.0010308084980266015,
      "grad_norm": 9798.0528677896,
      "learning_rate": 1.8772963258697897e-07,
      "loss": 1.7083,
      "step": 284032
    },
    {
      "epoch": 0.0010309246323774383,
      "grad_norm": 9212.73205949245,
      "learning_rate": 1.8771904780927365e-07,
      "loss": 1.7133,
      "step": 284064
    },
    {
      "epoch": 0.001031040766728275,
      "grad_norm": 10508.150931538812,
      "learning_rate": 1.8770846482177402e-07,
      "loss": 1.7285,
      "step": 284096
    },
    {
      "epoch": 0.0010311569010791116,
      "grad_norm": 9557.61643925932,
      "learning_rate": 1.876978836239755e-07,
      "loss": 1.747,
      "step": 284128
    },
    {
      "epoch": 0.0010312730354299483,
      "grad_norm": 10392.079291460395,
      "learning_rate": 1.8768730421537374e-07,
      "loss": 1.7286,
      "step": 284160
    },
    {
      "epoch": 0.001031389169780785,
      "grad_norm": 8175.393201553061,
      "learning_rate": 1.876767265954646e-07,
      "loss": 1.7098,
      "step": 284192
    },
    {
      "epoch": 0.0010315053041316218,
      "grad_norm": 10536.56870143217,
      "learning_rate": 1.8766615076374406e-07,
      "loss": 1.6976,
      "step": 284224
    },
    {
      "epoch": 0.0010316214384824586,
      "grad_norm": 9163.018280021055,
      "learning_rate": 1.8765557671970832e-07,
      "loss": 1.6982,
      "step": 284256
    },
    {
      "epoch": 0.0010317375728332951,
      "grad_norm": 10899.397598032654,
      "learning_rate": 1.8764500446285385e-07,
      "loss": 1.6922,
      "step": 284288
    },
    {
      "epoch": 0.001031853707184132,
      "grad_norm": 9537.739564488013,
      "learning_rate": 1.8763443399267725e-07,
      "loss": 1.6882,
      "step": 284320
    },
    {
      "epoch": 0.0010319698415349686,
      "grad_norm": 9527.352832765248,
      "learning_rate": 1.8762386530867535e-07,
      "loss": 1.7084,
      "step": 284352
    },
    {
      "epoch": 0.0010320859758858054,
      "grad_norm": 9220.78478221892,
      "learning_rate": 1.876132984103452e-07,
      "loss": 1.6924,
      "step": 284384
    },
    {
      "epoch": 0.001032202110236642,
      "grad_norm": 10556.80671415367,
      "learning_rate": 1.876027332971839e-07,
      "loss": 1.6993,
      "step": 284416
    },
    {
      "epoch": 0.0010323182445874787,
      "grad_norm": 13110.5703918632,
      "learning_rate": 1.8759216996868894e-07,
      "loss": 1.7142,
      "step": 284448
    },
    {
      "epoch": 0.0010324343789383154,
      "grad_norm": 9088.90884540053,
      "learning_rate": 1.87581608424358e-07,
      "loss": 1.704,
      "step": 284480
    },
    {
      "epoch": 0.0010325505132891522,
      "grad_norm": 8939.264511132893,
      "learning_rate": 1.8757104866368877e-07,
      "loss": 1.694,
      "step": 284512
    },
    {
      "epoch": 0.001032666647639989,
      "grad_norm": 10404.63194928105,
      "learning_rate": 1.8756082059599028e-07,
      "loss": 1.6874,
      "step": 284544
    },
    {
      "epoch": 0.0010327827819908255,
      "grad_norm": 10149.81320025152,
      "learning_rate": 1.8755026434543831e-07,
      "loss": 1.7051,
      "step": 284576
    },
    {
      "epoch": 0.0010328989163416622,
      "grad_norm": 14767.941495008708,
      "learning_rate": 1.875397098770584e-07,
      "loss": 1.6922,
      "step": 284608
    },
    {
      "epoch": 0.001033015050692499,
      "grad_norm": 7721.819733715622,
      "learning_rate": 1.8752915719034913e-07,
      "loss": 1.6811,
      "step": 284640
    },
    {
      "epoch": 0.0010331311850433358,
      "grad_norm": 9595.528125121618,
      "learning_rate": 1.875186062848093e-07,
      "loss": 1.7102,
      "step": 284672
    },
    {
      "epoch": 0.0010332473193941723,
      "grad_norm": 9648.125621072726,
      "learning_rate": 1.8750805715993786e-07,
      "loss": 1.7169,
      "step": 284704
    },
    {
      "epoch": 0.001033363453745009,
      "grad_norm": 10793.641090938683,
      "learning_rate": 1.8749750981523405e-07,
      "loss": 1.723,
      "step": 284736
    },
    {
      "epoch": 0.0010334795880958458,
      "grad_norm": 8867.097834128142,
      "learning_rate": 1.8748696425019725e-07,
      "loss": 1.7185,
      "step": 284768
    },
    {
      "epoch": 0.0010335957224466826,
      "grad_norm": 9496.129316726894,
      "learning_rate": 1.87476420464327e-07,
      "loss": 1.6811,
      "step": 284800
    },
    {
      "epoch": 0.0010337118567975193,
      "grad_norm": 9921.018697694304,
      "learning_rate": 1.8746587845712304e-07,
      "loss": 1.6879,
      "step": 284832
    },
    {
      "epoch": 0.0010338279911483558,
      "grad_norm": 8824.312211158442,
      "learning_rate": 1.8745533822808547e-07,
      "loss": 1.6968,
      "step": 284864
    },
    {
      "epoch": 0.0010339441254991926,
      "grad_norm": 11574.77222238088,
      "learning_rate": 1.8744479977671437e-07,
      "loss": 1.707,
      "step": 284896
    },
    {
      "epoch": 0.0010340602598500294,
      "grad_norm": 9947.013018992184,
      "learning_rate": 1.8743426310251013e-07,
      "loss": 1.7154,
      "step": 284928
    },
    {
      "epoch": 0.0010341763942008661,
      "grad_norm": 9751.684572421322,
      "learning_rate": 1.8742372820497333e-07,
      "loss": 1.7014,
      "step": 284960
    },
    {
      "epoch": 0.0010342925285517026,
      "grad_norm": 10622.786263499798,
      "learning_rate": 1.874131950836047e-07,
      "loss": 1.7277,
      "step": 284992
    },
    {
      "epoch": 0.0010344086629025394,
      "grad_norm": 8690.791103231051,
      "learning_rate": 1.8740266373790524e-07,
      "loss": 1.7195,
      "step": 285024
    },
    {
      "epoch": 0.0010345247972533762,
      "grad_norm": 10515.971091630103,
      "learning_rate": 1.8739213416737604e-07,
      "loss": 1.7216,
      "step": 285056
    },
    {
      "epoch": 0.001034640931604213,
      "grad_norm": 8637.218533764211,
      "learning_rate": 1.873816063715185e-07,
      "loss": 1.7243,
      "step": 285088
    },
    {
      "epoch": 0.0010347570659550497,
      "grad_norm": 9748.575280521765,
      "learning_rate": 1.8737108034983415e-07,
      "loss": 1.6799,
      "step": 285120
    },
    {
      "epoch": 0.0010348732003058862,
      "grad_norm": 9814.65027395271,
      "learning_rate": 1.8736055610182475e-07,
      "loss": 1.6791,
      "step": 285152
    },
    {
      "epoch": 0.001034989334656723,
      "grad_norm": 9330.284025687535,
      "learning_rate": 1.873500336269922e-07,
      "loss": 1.6913,
      "step": 285184
    },
    {
      "epoch": 0.0010351054690075597,
      "grad_norm": 9764.9487453852,
      "learning_rate": 1.8733951292483865e-07,
      "loss": 1.7014,
      "step": 285216
    },
    {
      "epoch": 0.0010352216033583965,
      "grad_norm": 10664.268376217846,
      "learning_rate": 1.8732899399486643e-07,
      "loss": 1.6917,
      "step": 285248
    },
    {
      "epoch": 0.001035337737709233,
      "grad_norm": 9142.575457714309,
      "learning_rate": 1.8731847683657812e-07,
      "loss": 1.6934,
      "step": 285280
    },
    {
      "epoch": 0.0010354538720600698,
      "grad_norm": 10109.019833791997,
      "learning_rate": 1.8730796144947633e-07,
      "loss": 1.7265,
      "step": 285312
    },
    {
      "epoch": 0.0010355700064109065,
      "grad_norm": 8377.401267696325,
      "learning_rate": 1.8729744783306407e-07,
      "loss": 1.7317,
      "step": 285344
    },
    {
      "epoch": 0.0010356861407617433,
      "grad_norm": 7985.392914565945,
      "learning_rate": 1.872869359868444e-07,
      "loss": 1.7132,
      "step": 285376
    },
    {
      "epoch": 0.00103580227511258,
      "grad_norm": 8932.914641929587,
      "learning_rate": 1.872764259103206e-07,
      "loss": 1.6965,
      "step": 285408
    },
    {
      "epoch": 0.0010359184094634166,
      "grad_norm": 12435.196178589222,
      "learning_rate": 1.8726591760299626e-07,
      "loss": 1.6729,
      "step": 285440
    },
    {
      "epoch": 0.0010360345438142533,
      "grad_norm": 10820.649148734099,
      "learning_rate": 1.8725541106437503e-07,
      "loss": 1.6828,
      "step": 285472
    },
    {
      "epoch": 0.00103615067816509,
      "grad_norm": 8750.671059981629,
      "learning_rate": 1.8724490629396078e-07,
      "loss": 1.7076,
      "step": 285504
    },
    {
      "epoch": 0.0010362668125159268,
      "grad_norm": 18744.7989586445,
      "learning_rate": 1.8723440329125763e-07,
      "loss": 1.7209,
      "step": 285536
    },
    {
      "epoch": 0.0010363829468667634,
      "grad_norm": 21028.783702344746,
      "learning_rate": 1.872239020557698e-07,
      "loss": 1.6811,
      "step": 285568
    },
    {
      "epoch": 0.0010364990812176,
      "grad_norm": 9622.644958637931,
      "learning_rate": 1.8721373066866345e-07,
      "loss": 1.6872,
      "step": 285600
    },
    {
      "epoch": 0.0010366152155684369,
      "grad_norm": 10088.681975362291,
      "learning_rate": 1.87203232910933e-07,
      "loss": 1.708,
      "step": 285632
    },
    {
      "epoch": 0.0010367313499192736,
      "grad_norm": 8787.627097231652,
      "learning_rate": 1.8719273691894736e-07,
      "loss": 1.6962,
      "step": 285664
    },
    {
      "epoch": 0.0010368474842701104,
      "grad_norm": 9887.14741469955,
      "learning_rate": 1.8718224269221162e-07,
      "loss": 1.7067,
      "step": 285696
    },
    {
      "epoch": 0.001036963618620947,
      "grad_norm": 9856.937861222419,
      "learning_rate": 1.8717175023023107e-07,
      "loss": 1.6984,
      "step": 285728
    },
    {
      "epoch": 0.0010370797529717837,
      "grad_norm": 8594.51964917179,
      "learning_rate": 1.8716125953251104e-07,
      "loss": 1.6869,
      "step": 285760
    },
    {
      "epoch": 0.0010371958873226204,
      "grad_norm": 8006.486870032324,
      "learning_rate": 1.8715077059855728e-07,
      "loss": 1.6898,
      "step": 285792
    },
    {
      "epoch": 0.0010373120216734572,
      "grad_norm": 9114.532571668171,
      "learning_rate": 1.8714028342787553e-07,
      "loss": 1.7199,
      "step": 285824
    },
    {
      "epoch": 0.0010374281560242937,
      "grad_norm": 10112.552595660503,
      "learning_rate": 1.8712979801997186e-07,
      "loss": 1.734,
      "step": 285856
    },
    {
      "epoch": 0.0010375442903751305,
      "grad_norm": 10245.556500259027,
      "learning_rate": 1.8711931437435248e-07,
      "loss": 1.7234,
      "step": 285888
    },
    {
      "epoch": 0.0010376604247259672,
      "grad_norm": 9960.193170817522,
      "learning_rate": 1.8710883249052378e-07,
      "loss": 1.7366,
      "step": 285920
    },
    {
      "epoch": 0.001037776559076804,
      "grad_norm": 10230.640253669366,
      "learning_rate": 1.8709835236799238e-07,
      "loss": 1.7412,
      "step": 285952
    },
    {
      "epoch": 0.0010378926934276407,
      "grad_norm": 8642.618584665182,
      "learning_rate": 1.8708787400626506e-07,
      "loss": 1.7155,
      "step": 285984
    },
    {
      "epoch": 0.0010380088277784773,
      "grad_norm": 10423.735702712343,
      "learning_rate": 1.8707739740484886e-07,
      "loss": 1.7089,
      "step": 286016
    },
    {
      "epoch": 0.001038124962129314,
      "grad_norm": 9266.583836560267,
      "learning_rate": 1.870669225632509e-07,
      "loss": 1.6909,
      "step": 286048
    },
    {
      "epoch": 0.0010382410964801508,
      "grad_norm": 9711.676065437934,
      "learning_rate": 1.8705644948097857e-07,
      "loss": 1.6871,
      "step": 286080
    },
    {
      "epoch": 0.0010383572308309875,
      "grad_norm": 11461.488036027433,
      "learning_rate": 1.870459781575395e-07,
      "loss": 1.7155,
      "step": 286112
    },
    {
      "epoch": 0.001038473365181824,
      "grad_norm": 9568.537401295978,
      "learning_rate": 1.8703550859244136e-07,
      "loss": 1.6952,
      "step": 286144
    },
    {
      "epoch": 0.0010385894995326608,
      "grad_norm": 9761.83958073477,
      "learning_rate": 1.8702504078519217e-07,
      "loss": 1.7004,
      "step": 286176
    },
    {
      "epoch": 0.0010387056338834976,
      "grad_norm": 11753.496671203851,
      "learning_rate": 1.870145747353e-07,
      "loss": 1.688,
      "step": 286208
    },
    {
      "epoch": 0.0010388217682343343,
      "grad_norm": 9246.565849005781,
      "learning_rate": 1.8700411044227333e-07,
      "loss": 1.7142,
      "step": 286240
    },
    {
      "epoch": 0.001038937902585171,
      "grad_norm": 8430.160852557916,
      "learning_rate": 1.8699364790562058e-07,
      "loss": 1.7081,
      "step": 286272
    },
    {
      "epoch": 0.0010390540369360076,
      "grad_norm": 9715.249559326821,
      "learning_rate": 1.869831871248505e-07,
      "loss": 1.6984,
      "step": 286304
    },
    {
      "epoch": 0.0010391701712868444,
      "grad_norm": 9148.21928027526,
      "learning_rate": 1.86972728099472e-07,
      "loss": 1.7012,
      "step": 286336
    },
    {
      "epoch": 0.0010392863056376811,
      "grad_norm": 11193.807573832953,
      "learning_rate": 1.8696227082899422e-07,
      "loss": 1.6838,
      "step": 286368
    },
    {
      "epoch": 0.0010394024399885179,
      "grad_norm": 9649.045755928406,
      "learning_rate": 1.8695181531292643e-07,
      "loss": 1.687,
      "step": 286400
    },
    {
      "epoch": 0.0010395185743393544,
      "grad_norm": 9913.665517859677,
      "learning_rate": 1.869413615507782e-07,
      "loss": 1.7001,
      "step": 286432
    },
    {
      "epoch": 0.0010396347086901912,
      "grad_norm": 8956.400616319035,
      "learning_rate": 1.869309095420591e-07,
      "loss": 1.7157,
      "step": 286464
    },
    {
      "epoch": 0.001039750843041028,
      "grad_norm": 10446.575706900323,
      "learning_rate": 1.8692045928627906e-07,
      "loss": 1.7187,
      "step": 286496
    },
    {
      "epoch": 0.0010398669773918647,
      "grad_norm": 10742.784369054421,
      "learning_rate": 1.8691001078294822e-07,
      "loss": 1.7047,
      "step": 286528
    },
    {
      "epoch": 0.0010399831117427014,
      "grad_norm": 12109.828570215186,
      "learning_rate": 1.868995640315767e-07,
      "loss": 1.6982,
      "step": 286560
    },
    {
      "epoch": 0.001040099246093538,
      "grad_norm": 8700.615840272458,
      "learning_rate": 1.8688944541141534e-07,
      "loss": 1.7037,
      "step": 286592
    },
    {
      "epoch": 0.0010402153804443747,
      "grad_norm": 8992.220193033532,
      "learning_rate": 1.8687900210778346e-07,
      "loss": 1.7079,
      "step": 286624
    },
    {
      "epoch": 0.0010403315147952115,
      "grad_norm": 8681.537306260914,
      "learning_rate": 1.868685605546582e-07,
      "loss": 1.7104,
      "step": 286656
    },
    {
      "epoch": 0.0010404476491460482,
      "grad_norm": 10688.289292492042,
      "learning_rate": 1.8685812075155055e-07,
      "loss": 1.7057,
      "step": 286688
    },
    {
      "epoch": 0.0010405637834968848,
      "grad_norm": 10131.381741894835,
      "learning_rate": 1.8684768269797176e-07,
      "loss": 1.7076,
      "step": 286720
    },
    {
      "epoch": 0.0010406799178477215,
      "grad_norm": 10728.582012549468,
      "learning_rate": 1.8683724639343324e-07,
      "loss": 1.7112,
      "step": 286752
    },
    {
      "epoch": 0.0010407960521985583,
      "grad_norm": 9717.31835436094,
      "learning_rate": 1.8682681183744656e-07,
      "loss": 1.7292,
      "step": 286784
    },
    {
      "epoch": 0.001040912186549395,
      "grad_norm": 10983.171673064206,
      "learning_rate": 1.868163790295235e-07,
      "loss": 1.7179,
      "step": 286816
    },
    {
      "epoch": 0.0010410283209002318,
      "grad_norm": 9596.75017909709,
      "learning_rate": 1.8680594796917606e-07,
      "loss": 1.7056,
      "step": 286848
    },
    {
      "epoch": 0.0010411444552510683,
      "grad_norm": 8956.274671982766,
      "learning_rate": 1.8679551865591635e-07,
      "loss": 1.715,
      "step": 286880
    },
    {
      "epoch": 0.001041260589601905,
      "grad_norm": 8537.38180006025,
      "learning_rate": 1.8678509108925684e-07,
      "loss": 1.7043,
      "step": 286912
    },
    {
      "epoch": 0.0010413767239527418,
      "grad_norm": 8812.023831107132,
      "learning_rate": 1.8677466526870998e-07,
      "loss": 1.719,
      "step": 286944
    },
    {
      "epoch": 0.0010414928583035786,
      "grad_norm": 12717.32173061608,
      "learning_rate": 1.8676424119378856e-07,
      "loss": 1.7148,
      "step": 286976
    },
    {
      "epoch": 0.0010416089926544151,
      "grad_norm": 9994.672580930303,
      "learning_rate": 1.8675381886400546e-07,
      "loss": 1.6981,
      "step": 287008
    },
    {
      "epoch": 0.0010417251270052519,
      "grad_norm": 12547.422524168061,
      "learning_rate": 1.8674339827887385e-07,
      "loss": 1.7018,
      "step": 287040
    },
    {
      "epoch": 0.0010418412613560886,
      "grad_norm": 10837.954419538772,
      "learning_rate": 1.8673297943790702e-07,
      "loss": 1.7133,
      "step": 287072
    },
    {
      "epoch": 0.0010419573957069254,
      "grad_norm": 10036.388394238238,
      "learning_rate": 1.8672256234061847e-07,
      "loss": 1.7386,
      "step": 287104
    },
    {
      "epoch": 0.0010420735300577621,
      "grad_norm": 8095.078751933178,
      "learning_rate": 1.8671214698652187e-07,
      "loss": 1.6991,
      "step": 287136
    },
    {
      "epoch": 0.0010421896644085987,
      "grad_norm": 9239.708220501338,
      "learning_rate": 1.8670173337513113e-07,
      "loss": 1.6859,
      "step": 287168
    },
    {
      "epoch": 0.0010423057987594354,
      "grad_norm": 10094.93199580859,
      "learning_rate": 1.8669132150596032e-07,
      "loss": 1.7023,
      "step": 287200
    },
    {
      "epoch": 0.0010424219331102722,
      "grad_norm": 8996.262779621324,
      "learning_rate": 1.8668091137852364e-07,
      "loss": 1.6982,
      "step": 287232
    },
    {
      "epoch": 0.001042538067461109,
      "grad_norm": 9330.873699713227,
      "learning_rate": 1.8667050299233564e-07,
      "loss": 1.7055,
      "step": 287264
    },
    {
      "epoch": 0.0010426542018119455,
      "grad_norm": 10452.715819345707,
      "learning_rate": 1.8666009634691085e-07,
      "loss": 1.7039,
      "step": 287296
    },
    {
      "epoch": 0.0010427703361627822,
      "grad_norm": 10889.271600984153,
      "learning_rate": 1.866496914417642e-07,
      "loss": 1.6748,
      "step": 287328
    },
    {
      "epoch": 0.001042886470513619,
      "grad_norm": 10439.259360701793,
      "learning_rate": 1.8663928827641064e-07,
      "loss": 1.6908,
      "step": 287360
    },
    {
      "epoch": 0.0010430026048644557,
      "grad_norm": 9282.414879760547,
      "learning_rate": 1.8662888685036537e-07,
      "loss": 1.6945,
      "step": 287392
    },
    {
      "epoch": 0.0010431187392152925,
      "grad_norm": 10351.72835810523,
      "learning_rate": 1.866184871631438e-07,
      "loss": 1.701,
      "step": 287424
    },
    {
      "epoch": 0.001043234873566129,
      "grad_norm": 9150.979182579316,
      "learning_rate": 1.8660808921426154e-07,
      "loss": 1.6896,
      "step": 287456
    },
    {
      "epoch": 0.0010433510079169658,
      "grad_norm": 10271.87052099081,
      "learning_rate": 1.8659769300323435e-07,
      "loss": 1.6884,
      "step": 287488
    },
    {
      "epoch": 0.0010434671422678025,
      "grad_norm": 9452.553729019477,
      "learning_rate": 1.8658729852957818e-07,
      "loss": 1.7112,
      "step": 287520
    },
    {
      "epoch": 0.0010435832766186393,
      "grad_norm": 8991.581395950325,
      "learning_rate": 1.8657690579280917e-07,
      "loss": 1.7097,
      "step": 287552
    },
    {
      "epoch": 0.0010436994109694758,
      "grad_norm": 19845.378706389052,
      "learning_rate": 1.8656683948492647e-07,
      "loss": 1.7184,
      "step": 287584
    },
    {
      "epoch": 0.0010438155453203126,
      "grad_norm": 10329.078371277856,
      "learning_rate": 1.8655645016624084e-07,
      "loss": 1.7351,
      "step": 287616
    },
    {
      "epoch": 0.0010439316796711493,
      "grad_norm": 12604.562348610125,
      "learning_rate": 1.8654606258300706e-07,
      "loss": 1.7254,
      "step": 287648
    },
    {
      "epoch": 0.001044047814021986,
      "grad_norm": 13324.247220762605,
      "learning_rate": 1.8653567673474207e-07,
      "loss": 1.7261,
      "step": 287680
    },
    {
      "epoch": 0.0010441639483728228,
      "grad_norm": 9252.320681861389,
      "learning_rate": 1.8652529262096286e-07,
      "loss": 1.7399,
      "step": 287712
    },
    {
      "epoch": 0.0010442800827236594,
      "grad_norm": 9381.514589873002,
      "learning_rate": 1.8651491024118683e-07,
      "loss": 1.7179,
      "step": 287744
    },
    {
      "epoch": 0.0010443962170744961,
      "grad_norm": 11228.44120971384,
      "learning_rate": 1.8650452959493135e-07,
      "loss": 1.6899,
      "step": 287776
    },
    {
      "epoch": 0.0010445123514253329,
      "grad_norm": 9702.311477168725,
      "learning_rate": 1.864941506817141e-07,
      "loss": 1.7037,
      "step": 287808
    },
    {
      "epoch": 0.0010446284857761696,
      "grad_norm": 9928.110595677306,
      "learning_rate": 1.8648377350105293e-07,
      "loss": 1.7132,
      "step": 287840
    },
    {
      "epoch": 0.0010447446201270062,
      "grad_norm": 10562.183107672388,
      "learning_rate": 1.8647339805246585e-07,
      "loss": 1.7158,
      "step": 287872
    },
    {
      "epoch": 0.001044860754477843,
      "grad_norm": 8538.574822533325,
      "learning_rate": 1.864630243354711e-07,
      "loss": 1.7011,
      "step": 287904
    },
    {
      "epoch": 0.0010449768888286797,
      "grad_norm": 9784.789420319683,
      "learning_rate": 1.8645265234958703e-07,
      "loss": 1.6991,
      "step": 287936
    },
    {
      "epoch": 0.0010450930231795164,
      "grad_norm": 10047.114212548795,
      "learning_rate": 1.864422820943323e-07,
      "loss": 1.6853,
      "step": 287968
    },
    {
      "epoch": 0.0010452091575303532,
      "grad_norm": 9349.021767008568,
      "learning_rate": 1.8643191356922564e-07,
      "loss": 1.7051,
      "step": 288000
    },
    {
      "epoch": 0.0010453252918811897,
      "grad_norm": 10819.591119815943,
      "learning_rate": 1.8642154677378598e-07,
      "loss": 1.6918,
      "step": 288032
    },
    {
      "epoch": 0.0010454414262320265,
      "grad_norm": 10926.27676749953,
      "learning_rate": 1.8641118170753253e-07,
      "loss": 1.7012,
      "step": 288064
    },
    {
      "epoch": 0.0010455575605828632,
      "grad_norm": 9115.378543977205,
      "learning_rate": 1.8640081836998463e-07,
      "loss": 1.6848,
      "step": 288096
    },
    {
      "epoch": 0.0010456736949337,
      "grad_norm": 9844.466262830098,
      "learning_rate": 1.8639045676066177e-07,
      "loss": 1.697,
      "step": 288128
    },
    {
      "epoch": 0.0010457898292845365,
      "grad_norm": 9610.550140340561,
      "learning_rate": 1.863800968790837e-07,
      "loss": 1.708,
      "step": 288160
    },
    {
      "epoch": 0.0010459059636353733,
      "grad_norm": 10195.115497138813,
      "learning_rate": 1.863697387247703e-07,
      "loss": 1.7151,
      "step": 288192
    },
    {
      "epoch": 0.00104602209798621,
      "grad_norm": 10829.31115076116,
      "learning_rate": 1.8635938229724164e-07,
      "loss": 1.7099,
      "step": 288224
    },
    {
      "epoch": 0.0010461382323370468,
      "grad_norm": 12359.05773107319,
      "learning_rate": 1.86349027596018e-07,
      "loss": 1.7147,
      "step": 288256
    },
    {
      "epoch": 0.0010462543666878836,
      "grad_norm": 10781.536439673151,
      "learning_rate": 1.8633867462061987e-07,
      "loss": 1.7077,
      "step": 288288
    },
    {
      "epoch": 0.00104637050103872,
      "grad_norm": 9918.387570568111,
      "learning_rate": 1.8632832337056784e-07,
      "loss": 1.6893,
      "step": 288320
    },
    {
      "epoch": 0.0010464866353895568,
      "grad_norm": 8485.901955596706,
      "learning_rate": 1.8631797384538282e-07,
      "loss": 1.7075,
      "step": 288352
    },
    {
      "epoch": 0.0010466027697403936,
      "grad_norm": 11918.62190020306,
      "learning_rate": 1.8630762604458575e-07,
      "loss": 1.7076,
      "step": 288384
    },
    {
      "epoch": 0.0010467189040912303,
      "grad_norm": 10535.717156416074,
      "learning_rate": 1.8629727996769787e-07,
      "loss": 1.6967,
      "step": 288416
    },
    {
      "epoch": 0.0010468350384420669,
      "grad_norm": 7677.111826722338,
      "learning_rate": 1.862869356142406e-07,
      "loss": 1.7352,
      "step": 288448
    },
    {
      "epoch": 0.0010469511727929036,
      "grad_norm": 11787.809126381373,
      "learning_rate": 1.8627659298373543e-07,
      "loss": 1.7417,
      "step": 288480
    },
    {
      "epoch": 0.0010470673071437404,
      "grad_norm": 9775.69353038443,
      "learning_rate": 1.8626625207570423e-07,
      "loss": 1.7383,
      "step": 288512
    },
    {
      "epoch": 0.0010471834414945771,
      "grad_norm": 10465.081366143313,
      "learning_rate": 1.8625591288966885e-07,
      "loss": 1.7378,
      "step": 288544
    },
    {
      "epoch": 0.001047299575845414,
      "grad_norm": 10291.094402443308,
      "learning_rate": 1.862455754251515e-07,
      "loss": 1.7198,
      "step": 288576
    },
    {
      "epoch": 0.0010474157101962504,
      "grad_norm": 9970.259775953684,
      "learning_rate": 1.8623556264761193e-07,
      "loss": 1.7127,
      "step": 288608
    },
    {
      "epoch": 0.0010475318445470872,
      "grad_norm": 8934.033803383554,
      "learning_rate": 1.8622522857093734e-07,
      "loss": 1.7015,
      "step": 288640
    },
    {
      "epoch": 0.001047647978897924,
      "grad_norm": 11501.205849822878,
      "learning_rate": 1.862148962143632e-07,
      "loss": 1.7102,
      "step": 288672
    },
    {
      "epoch": 0.0010477641132487607,
      "grad_norm": 11908.017467236097,
      "learning_rate": 1.8620456557741233e-07,
      "loss": 1.7083,
      "step": 288704
    },
    {
      "epoch": 0.0010478802475995972,
      "grad_norm": 10121.743525697537,
      "learning_rate": 1.8619423665960784e-07,
      "loss": 1.6988,
      "step": 288736
    },
    {
      "epoch": 0.001047996381950434,
      "grad_norm": 8368.685798857548,
      "learning_rate": 1.8618390946047293e-07,
      "loss": 1.7214,
      "step": 288768
    },
    {
      "epoch": 0.0010481125163012707,
      "grad_norm": 8541.165494240233,
      "learning_rate": 1.8617358397953104e-07,
      "loss": 1.7261,
      "step": 288800
    },
    {
      "epoch": 0.0010482286506521075,
      "grad_norm": 9241.932914709994,
      "learning_rate": 1.8616326021630579e-07,
      "loss": 1.7437,
      "step": 288832
    },
    {
      "epoch": 0.0010483447850029443,
      "grad_norm": 9220.49770890921,
      "learning_rate": 1.8615293817032096e-07,
      "loss": 1.7522,
      "step": 288864
    },
    {
      "epoch": 0.0010484609193537808,
      "grad_norm": 7837.559951923813,
      "learning_rate": 1.8614261784110056e-07,
      "loss": 1.7016,
      "step": 288896
    },
    {
      "epoch": 0.0010485770537046175,
      "grad_norm": 9754.534330248676,
      "learning_rate": 1.861322992281687e-07,
      "loss": 1.6953,
      "step": 288928
    },
    {
      "epoch": 0.0010486931880554543,
      "grad_norm": 8670.780126378479,
      "learning_rate": 1.8612198233104974e-07,
      "loss": 1.7028,
      "step": 288960
    },
    {
      "epoch": 0.001048809322406291,
      "grad_norm": 13158.659354204743,
      "learning_rate": 1.861116671492683e-07,
      "loss": 1.689,
      "step": 288992
    },
    {
      "epoch": 0.0010489254567571276,
      "grad_norm": 9994.043025722873,
      "learning_rate": 1.86101353682349e-07,
      "loss": 1.6875,
      "step": 289024
    },
    {
      "epoch": 0.0010490415911079643,
      "grad_norm": 9350.7636051822,
      "learning_rate": 1.860910419298168e-07,
      "loss": 1.695,
      "step": 289056
    },
    {
      "epoch": 0.001049157725458801,
      "grad_norm": 8983.015863283334,
      "learning_rate": 1.8608073189119673e-07,
      "loss": 1.6945,
      "step": 289088
    },
    {
      "epoch": 0.0010492738598096379,
      "grad_norm": 10621.196166157557,
      "learning_rate": 1.860704235660141e-07,
      "loss": 1.703,
      "step": 289120
    },
    {
      "epoch": 0.0010493899941604746,
      "grad_norm": 11075.756046428614,
      "learning_rate": 1.860601169537944e-07,
      "loss": 1.6987,
      "step": 289152
    },
    {
      "epoch": 0.0010495061285113111,
      "grad_norm": 7830.284286026913,
      "learning_rate": 1.8604981205406318e-07,
      "loss": 1.6947,
      "step": 289184
    },
    {
      "epoch": 0.001049622262862148,
      "grad_norm": 10912.316894225534,
      "learning_rate": 1.8603950886634636e-07,
      "loss": 1.6809,
      "step": 289216
    },
    {
      "epoch": 0.0010497383972129847,
      "grad_norm": 9749.978256386012,
      "learning_rate": 1.8602920739016989e-07,
      "loss": 1.6893,
      "step": 289248
    },
    {
      "epoch": 0.0010498545315638214,
      "grad_norm": 8765.949691847427,
      "learning_rate": 1.8601890762506e-07,
      "loss": 1.7006,
      "step": 289280
    },
    {
      "epoch": 0.001049970665914658,
      "grad_norm": 9967.699634318844,
      "learning_rate": 1.8600860957054302e-07,
      "loss": 1.7143,
      "step": 289312
    },
    {
      "epoch": 0.0010500868002654947,
      "grad_norm": 9425.60045832625,
      "learning_rate": 1.8599831322614548e-07,
      "loss": 1.7009,
      "step": 289344
    },
    {
      "epoch": 0.0010502029346163315,
      "grad_norm": 10745.421536635964,
      "learning_rate": 1.8598801859139422e-07,
      "loss": 1.7213,
      "step": 289376
    },
    {
      "epoch": 0.0010503190689671682,
      "grad_norm": 9225.728914291813,
      "learning_rate": 1.8597772566581613e-07,
      "loss": 1.7482,
      "step": 289408
    },
    {
      "epoch": 0.001050435203318005,
      "grad_norm": 9244.821902016285,
      "learning_rate": 1.8596743444893826e-07,
      "loss": 1.75,
      "step": 289440
    },
    {
      "epoch": 0.0010505513376688415,
      "grad_norm": 8590.23445547326,
      "learning_rate": 1.8595714494028796e-07,
      "loss": 1.7376,
      "step": 289472
    },
    {
      "epoch": 0.0010506674720196783,
      "grad_norm": 8022.769970527635,
      "learning_rate": 1.8594685713939267e-07,
      "loss": 1.6971,
      "step": 289504
    },
    {
      "epoch": 0.001050783606370515,
      "grad_norm": 12060.080928418349,
      "learning_rate": 1.859365710457801e-07,
      "loss": 1.6803,
      "step": 289536
    },
    {
      "epoch": 0.0010508997407213518,
      "grad_norm": 10856.513805084945,
      "learning_rate": 1.8592628665897803e-07,
      "loss": 1.6824,
      "step": 289568
    },
    {
      "epoch": 0.0010510158750721883,
      "grad_norm": 15006.358918805054,
      "learning_rate": 1.859160039785145e-07,
      "loss": 1.7008,
      "step": 289600
    },
    {
      "epoch": 0.001051132009423025,
      "grad_norm": 9055.051407915915,
      "learning_rate": 1.859060442585573e-07,
      "loss": 1.7103,
      "step": 289632
    },
    {
      "epoch": 0.0010512481437738618,
      "grad_norm": 10805.89450253888,
      "learning_rate": 1.8589576493606922e-07,
      "loss": 1.6729,
      "step": 289664
    },
    {
      "epoch": 0.0010513642781246986,
      "grad_norm": 8457.763534173795,
      "learning_rate": 1.858854873185196e-07,
      "loss": 1.6925,
      "step": 289696
    },
    {
      "epoch": 0.0010514804124755353,
      "grad_norm": 9526.550057602175,
      "learning_rate": 1.8587521140543712e-07,
      "loss": 1.7094,
      "step": 289728
    },
    {
      "epoch": 0.0010515965468263719,
      "grad_norm": 8658.015015001994,
      "learning_rate": 1.858649371963508e-07,
      "loss": 1.7149,
      "step": 289760
    },
    {
      "epoch": 0.0010517126811772086,
      "grad_norm": 8979.992427613734,
      "learning_rate": 1.8585466469078972e-07,
      "loss": 1.6991,
      "step": 289792
    },
    {
      "epoch": 0.0010518288155280454,
      "grad_norm": 12274.384057866204,
      "learning_rate": 1.858443938882831e-07,
      "loss": 1.6859,
      "step": 289824
    },
    {
      "epoch": 0.0010519449498788821,
      "grad_norm": 11145.022925054933,
      "learning_rate": 1.8583412478836053e-07,
      "loss": 1.6783,
      "step": 289856
    },
    {
      "epoch": 0.0010520610842297187,
      "grad_norm": 12823.224243535633,
      "learning_rate": 1.858238573905516e-07,
      "loss": 1.6816,
      "step": 289888
    },
    {
      "epoch": 0.0010521772185805554,
      "grad_norm": 9671.054957966064,
      "learning_rate": 1.858135916943862e-07,
      "loss": 1.7016,
      "step": 289920
    },
    {
      "epoch": 0.0010522933529313922,
      "grad_norm": 10236.762574173536,
      "learning_rate": 1.858033276993943e-07,
      "loss": 1.7031,
      "step": 289952
    },
    {
      "epoch": 0.001052409487282229,
      "grad_norm": 11274.222101768264,
      "learning_rate": 1.857930654051061e-07,
      "loss": 1.6968,
      "step": 289984
    },
    {
      "epoch": 0.0010525256216330657,
      "grad_norm": 10875.922213771117,
      "learning_rate": 1.8578280481105204e-07,
      "loss": 1.7118,
      "step": 290016
    },
    {
      "epoch": 0.0010526417559839022,
      "grad_norm": 9180.750078288811,
      "learning_rate": 1.857725459167627e-07,
      "loss": 1.7182,
      "step": 290048
    },
    {
      "epoch": 0.001052757890334739,
      "grad_norm": 9511.351323550192,
      "learning_rate": 1.8576228872176872e-07,
      "loss": 1.7005,
      "step": 290080
    },
    {
      "epoch": 0.0010528740246855757,
      "grad_norm": 10164.822280787796,
      "learning_rate": 1.8575203322560113e-07,
      "loss": 1.6995,
      "step": 290112
    },
    {
      "epoch": 0.0010529901590364125,
      "grad_norm": 10542.583364621785,
      "learning_rate": 1.85741779427791e-07,
      "loss": 1.6931,
      "step": 290144
    },
    {
      "epoch": 0.001053106293387249,
      "grad_norm": 9234.384440773516,
      "learning_rate": 1.857315273278696e-07,
      "loss": 1.6883,
      "step": 290176
    },
    {
      "epoch": 0.0010532224277380858,
      "grad_norm": 8807.7656644577,
      "learning_rate": 1.8572127692536845e-07,
      "loss": 1.7058,
      "step": 290208
    },
    {
      "epoch": 0.0010533385620889225,
      "grad_norm": 9854.62307752052,
      "learning_rate": 1.857110282198192e-07,
      "loss": 1.7278,
      "step": 290240
    },
    {
      "epoch": 0.0010534546964397593,
      "grad_norm": 9726.657390902592,
      "learning_rate": 1.8570078121075368e-07,
      "loss": 1.7113,
      "step": 290272
    },
    {
      "epoch": 0.001053570830790596,
      "grad_norm": 11229.31431566505,
      "learning_rate": 1.8569053589770386e-07,
      "loss": 1.7009,
      "step": 290304
    },
    {
      "epoch": 0.0010536869651414326,
      "grad_norm": 11380.725108709023,
      "learning_rate": 1.85680292280202e-07,
      "loss": 1.7213,
      "step": 290336
    },
    {
      "epoch": 0.0010538030994922693,
      "grad_norm": 10873.707923243112,
      "learning_rate": 1.8567005035778046e-07,
      "loss": 1.7113,
      "step": 290368
    },
    {
      "epoch": 0.001053919233843106,
      "grad_norm": 11473.880424686324,
      "learning_rate": 1.8565981012997176e-07,
      "loss": 1.7046,
      "step": 290400
    },
    {
      "epoch": 0.0010540353681939428,
      "grad_norm": 9128.825773340184,
      "learning_rate": 1.8564957159630868e-07,
      "loss": 1.7028,
      "step": 290432
    },
    {
      "epoch": 0.0010541515025447794,
      "grad_norm": 10604.48716346057,
      "learning_rate": 1.8563933475632412e-07,
      "loss": 1.6934,
      "step": 290464
    },
    {
      "epoch": 0.0010542676368956161,
      "grad_norm": 9956.113096987197,
      "learning_rate": 1.8562909960955118e-07,
      "loss": 1.6925,
      "step": 290496
    },
    {
      "epoch": 0.0010543837712464529,
      "grad_norm": 9966.180010415224,
      "learning_rate": 1.8561886615552317e-07,
      "loss": 1.6874,
      "step": 290528
    },
    {
      "epoch": 0.0010544999055972896,
      "grad_norm": 9736.764349618408,
      "learning_rate": 1.8560863439377352e-07,
      "loss": 1.7103,
      "step": 290560
    },
    {
      "epoch": 0.0010546160399481264,
      "grad_norm": 9198.196453653292,
      "learning_rate": 1.855984043238358e-07,
      "loss": 1.7146,
      "step": 290592
    },
    {
      "epoch": 0.001054732174298963,
      "grad_norm": 8961.084532577515,
      "learning_rate": 1.8558849555647818e-07,
      "loss": 1.7125,
      "step": 290624
    },
    {
      "epoch": 0.0010548483086497997,
      "grad_norm": 10364.859092143994,
      "learning_rate": 1.855782688159332e-07,
      "loss": 1.7089,
      "step": 290656
    },
    {
      "epoch": 0.0010549644430006364,
      "grad_norm": 9633.86070067447,
      "learning_rate": 1.8556804376581676e-07,
      "loss": 1.6975,
      "step": 290688
    },
    {
      "epoch": 0.0010550805773514732,
      "grad_norm": 9342.966124309774,
      "learning_rate": 1.8555782040566325e-07,
      "loss": 1.7038,
      "step": 290720
    },
    {
      "epoch": 0.0010551967117023097,
      "grad_norm": 8154.21007823566,
      "learning_rate": 1.8554759873500716e-07,
      "loss": 1.7001,
      "step": 290752
    },
    {
      "epoch": 0.0010553128460531465,
      "grad_norm": 9052.70611474823,
      "learning_rate": 1.855373787533832e-07,
      "loss": 1.6809,
      "step": 290784
    },
    {
      "epoch": 0.0010554289804039832,
      "grad_norm": 7494.926550674129,
      "learning_rate": 1.8552716046032632e-07,
      "loss": 1.6953,
      "step": 290816
    },
    {
      "epoch": 0.00105554511475482,
      "grad_norm": 10161.967132401089,
      "learning_rate": 1.8551694385537152e-07,
      "loss": 1.6935,
      "step": 290848
    },
    {
      "epoch": 0.0010556612491056565,
      "grad_norm": 9867.816678475538,
      "learning_rate": 1.855067289380541e-07,
      "loss": 1.71,
      "step": 290880
    },
    {
      "epoch": 0.0010557773834564933,
      "grad_norm": 13539.41918990619,
      "learning_rate": 1.8549651570790942e-07,
      "loss": 1.6873,
      "step": 290912
    },
    {
      "epoch": 0.00105589351780733,
      "grad_norm": 11025.811716150425,
      "learning_rate": 1.8548630416447317e-07,
      "loss": 1.6862,
      "step": 290944
    },
    {
      "epoch": 0.0010560096521581668,
      "grad_norm": 9402.040416845697,
      "learning_rate": 1.8547609430728112e-07,
      "loss": 1.7114,
      "step": 290976
    },
    {
      "epoch": 0.0010561257865090035,
      "grad_norm": 9641.958929595168,
      "learning_rate": 1.8546588613586918e-07,
      "loss": 1.7044,
      "step": 291008
    },
    {
      "epoch": 0.00105624192085984,
      "grad_norm": 9614.907591859632,
      "learning_rate": 1.8545567964977352e-07,
      "loss": 1.7182,
      "step": 291040
    },
    {
      "epoch": 0.0010563580552106768,
      "grad_norm": 10521.675341883534,
      "learning_rate": 1.8544547484853048e-07,
      "loss": 1.712,
      "step": 291072
    },
    {
      "epoch": 0.0010564741895615136,
      "grad_norm": 9004.400701879054,
      "learning_rate": 1.8543527173167656e-07,
      "loss": 1.6986,
      "step": 291104
    },
    {
      "epoch": 0.0010565903239123503,
      "grad_norm": 9001.54608942264,
      "learning_rate": 1.8542507029874844e-07,
      "loss": 1.7159,
      "step": 291136
    },
    {
      "epoch": 0.0010567064582631869,
      "grad_norm": 9176.631626037955,
      "learning_rate": 1.8541487054928295e-07,
      "loss": 1.7383,
      "step": 291168
    },
    {
      "epoch": 0.0010568225926140236,
      "grad_norm": 11795.221744418373,
      "learning_rate": 1.8540467248281713e-07,
      "loss": 1.7527,
      "step": 291200
    },
    {
      "epoch": 0.0010569387269648604,
      "grad_norm": 10875.456772016521,
      "learning_rate": 1.8539447609888824e-07,
      "loss": 1.7178,
      "step": 291232
    },
    {
      "epoch": 0.0010570548613156971,
      "grad_norm": 11900.15142760797,
      "learning_rate": 1.8538428139703365e-07,
      "loss": 1.6903,
      "step": 291264
    },
    {
      "epoch": 0.0010571709956665339,
      "grad_norm": 9478.110993230666,
      "learning_rate": 1.8537408837679094e-07,
      "loss": 1.6981,
      "step": 291296
    },
    {
      "epoch": 0.0010572871300173704,
      "grad_norm": 9863.8083922996,
      "learning_rate": 1.853638970376978e-07,
      "loss": 1.7011,
      "step": 291328
    },
    {
      "epoch": 0.0010574032643682072,
      "grad_norm": 9306.79923496795,
      "learning_rate": 1.8535370737929225e-07,
      "loss": 1.7058,
      "step": 291360
    },
    {
      "epoch": 0.001057519398719044,
      "grad_norm": 7142.274287648158,
      "learning_rate": 1.8534351940111232e-07,
      "loss": 1.7081,
      "step": 291392
    },
    {
      "epoch": 0.0010576355330698807,
      "grad_norm": 11423.058259503014,
      "learning_rate": 1.853333331026963e-07,
      "loss": 1.6731,
      "step": 291424
    },
    {
      "epoch": 0.0010577516674207172,
      "grad_norm": 8888.077632424236,
      "learning_rate": 1.8532314848358268e-07,
      "loss": 1.6863,
      "step": 291456
    },
    {
      "epoch": 0.001057867801771554,
      "grad_norm": 9109.160444300012,
      "learning_rate": 1.8531296554331009e-07,
      "loss": 1.7107,
      "step": 291488
    },
    {
      "epoch": 0.0010579839361223907,
      "grad_norm": 9781.779388229936,
      "learning_rate": 1.8530278428141734e-07,
      "loss": 1.7095,
      "step": 291520
    },
    {
      "epoch": 0.0010581000704732275,
      "grad_norm": 9849.943959231445,
      "learning_rate": 1.8529260469744344e-07,
      "loss": 1.6754,
      "step": 291552
    },
    {
      "epoch": 0.0010582162048240642,
      "grad_norm": 8667.178202852414,
      "learning_rate": 1.852824267909275e-07,
      "loss": 1.6904,
      "step": 291584
    },
    {
      "epoch": 0.0010583323391749008,
      "grad_norm": 8254.126119705223,
      "learning_rate": 1.8527225056140896e-07,
      "loss": 1.6966,
      "step": 291616
    },
    {
      "epoch": 0.0010584484735257375,
      "grad_norm": 8546.137841153746,
      "learning_rate": 1.8526239393783523e-07,
      "loss": 1.7026,
      "step": 291648
    },
    {
      "epoch": 0.0010585646078765743,
      "grad_norm": 8110.162883691054,
      "learning_rate": 1.852522210085597e-07,
      "loss": 1.7071,
      "step": 291680
    },
    {
      "epoch": 0.001058680742227411,
      "grad_norm": 9192.410130101898,
      "learning_rate": 1.8524204975491497e-07,
      "loss": 1.7047,
      "step": 291712
    },
    {
      "epoch": 0.0010587968765782476,
      "grad_norm": 11926.437523418299,
      "learning_rate": 1.852318801764411e-07,
      "loss": 1.6992,
      "step": 291744
    },
    {
      "epoch": 0.0010589130109290843,
      "grad_norm": 11419.227294348773,
      "learning_rate": 1.8522171227267836e-07,
      "loss": 1.7169,
      "step": 291776
    },
    {
      "epoch": 0.001059029145279921,
      "grad_norm": 11048.663267563184,
      "learning_rate": 1.852115460431671e-07,
      "loss": 1.7264,
      "step": 291808
    },
    {
      "epoch": 0.0010591452796307578,
      "grad_norm": 11385.732124022592,
      "learning_rate": 1.8520138148744792e-07,
      "loss": 1.7061,
      "step": 291840
    },
    {
      "epoch": 0.0010592614139815946,
      "grad_norm": 10384.793016714391,
      "learning_rate": 1.851912186050616e-07,
      "loss": 1.6892,
      "step": 291872
    },
    {
      "epoch": 0.0010593775483324311,
      "grad_norm": 8034.692028945477,
      "learning_rate": 1.8518105739554902e-07,
      "loss": 1.7085,
      "step": 291904
    },
    {
      "epoch": 0.0010594936826832679,
      "grad_norm": 9749.021079062246,
      "learning_rate": 1.8517089785845136e-07,
      "loss": 1.7184,
      "step": 291936
    },
    {
      "epoch": 0.0010596098170341046,
      "grad_norm": 13874.356777883435,
      "learning_rate": 1.8516073999330983e-07,
      "loss": 1.7325,
      "step": 291968
    },
    {
      "epoch": 0.0010597259513849414,
      "grad_norm": 10024.767528476657,
      "learning_rate": 1.851505837996659e-07,
      "loss": 1.7278,
      "step": 292000
    },
    {
      "epoch": 0.001059842085735778,
      "grad_norm": 9274.109984251858,
      "learning_rate": 1.8514042927706124e-07,
      "loss": 1.7074,
      "step": 292032
    },
    {
      "epoch": 0.0010599582200866147,
      "grad_norm": 10560.851291444265,
      "learning_rate": 1.8513027642503768e-07,
      "loss": 1.7046,
      "step": 292064
    },
    {
      "epoch": 0.0010600743544374514,
      "grad_norm": 10765.901727212635,
      "learning_rate": 1.8512012524313716e-07,
      "loss": 1.7148,
      "step": 292096
    },
    {
      "epoch": 0.0010601904887882882,
      "grad_norm": 10400.276534785024,
      "learning_rate": 1.8510997573090185e-07,
      "loss": 1.705,
      "step": 292128
    },
    {
      "epoch": 0.001060306623139125,
      "grad_norm": 8824.52083685001,
      "learning_rate": 1.850998278878741e-07,
      "loss": 1.6933,
      "step": 292160
    },
    {
      "epoch": 0.0010604227574899615,
      "grad_norm": 10001.666261178685,
      "learning_rate": 1.8508968171359644e-07,
      "loss": 1.6745,
      "step": 292192
    },
    {
      "epoch": 0.0010605388918407982,
      "grad_norm": 12205.861051150796,
      "learning_rate": 1.8507953720761156e-07,
      "loss": 1.699,
      "step": 292224
    },
    {
      "epoch": 0.001060655026191635,
      "grad_norm": 9571.815815194106,
      "learning_rate": 1.8506939436946227e-07,
      "loss": 1.7088,
      "step": 292256
    },
    {
      "epoch": 0.0010607711605424717,
      "grad_norm": 11209.613909497508,
      "learning_rate": 1.8505925319869168e-07,
      "loss": 1.7097,
      "step": 292288
    },
    {
      "epoch": 0.0010608872948933083,
      "grad_norm": 10536.348513598059,
      "learning_rate": 1.8504911369484298e-07,
      "loss": 1.7159,
      "step": 292320
    },
    {
      "epoch": 0.001061003429244145,
      "grad_norm": 10474.308950952325,
      "learning_rate": 1.850389758574596e-07,
      "loss": 1.7063,
      "step": 292352
    },
    {
      "epoch": 0.0010611195635949818,
      "grad_norm": 9209.769378220064,
      "learning_rate": 1.85028839686085e-07,
      "loss": 1.7183,
      "step": 292384
    },
    {
      "epoch": 0.0010612356979458185,
      "grad_norm": 10699.953457842701,
      "learning_rate": 1.8501870518026302e-07,
      "loss": 1.6981,
      "step": 292416
    },
    {
      "epoch": 0.0010613518322966553,
      "grad_norm": 9824.00570032408,
      "learning_rate": 1.8500857233953757e-07,
      "loss": 1.6943,
      "step": 292448
    },
    {
      "epoch": 0.0010614679666474918,
      "grad_norm": 10729.972972938935,
      "learning_rate": 1.8499844116345273e-07,
      "loss": 1.6924,
      "step": 292480
    },
    {
      "epoch": 0.0010615841009983286,
      "grad_norm": 10986.063899322633,
      "learning_rate": 1.849883116515527e-07,
      "loss": 1.6726,
      "step": 292512
    },
    {
      "epoch": 0.0010617002353491653,
      "grad_norm": 8761.180970622625,
      "learning_rate": 1.84978183803382e-07,
      "loss": 1.7159,
      "step": 292544
    },
    {
      "epoch": 0.001061816369700002,
      "grad_norm": 8955.290950047352,
      "learning_rate": 1.8496805761848523e-07,
      "loss": 1.7174,
      "step": 292576
    },
    {
      "epoch": 0.0010619325040508386,
      "grad_norm": 11576.820461594798,
      "learning_rate": 1.8495793309640723e-07,
      "loss": 1.6969,
      "step": 292608
    },
    {
      "epoch": 0.0010620486384016754,
      "grad_norm": 21004.599877169763,
      "learning_rate": 1.8494781023669284e-07,
      "loss": 1.7182,
      "step": 292640
    },
    {
      "epoch": 0.0010621647727525121,
      "grad_norm": 20864.24846477821,
      "learning_rate": 1.8493768903888728e-07,
      "loss": 1.6795,
      "step": 292672
    },
    {
      "epoch": 0.001062280907103349,
      "grad_norm": 9358.638148790667,
      "learning_rate": 1.8492788571290237e-07,
      "loss": 1.6943,
      "step": 292704
    },
    {
      "epoch": 0.0010623970414541857,
      "grad_norm": 9565.261104643198,
      "learning_rate": 1.849177677856512e-07,
      "loss": 1.7035,
      "step": 292736
    },
    {
      "epoch": 0.0010625131758050222,
      "grad_norm": 12046.439639993221,
      "learning_rate": 1.8490765151895955e-07,
      "loss": 1.717,
      "step": 292768
    },
    {
      "epoch": 0.001062629310155859,
      "grad_norm": 9540.941253356505,
      "learning_rate": 1.8489753691237315e-07,
      "loss": 1.6895,
      "step": 292800
    },
    {
      "epoch": 0.0010627454445066957,
      "grad_norm": 10478.039893033429,
      "learning_rate": 1.8488742396543807e-07,
      "loss": 1.6975,
      "step": 292832
    },
    {
      "epoch": 0.0010628615788575325,
      "grad_norm": 10467.67022789694,
      "learning_rate": 1.848773126777005e-07,
      "loss": 1.7316,
      "step": 292864
    },
    {
      "epoch": 0.001062977713208369,
      "grad_norm": 8743.00428914455,
      "learning_rate": 1.8486720304870673e-07,
      "loss": 1.7313,
      "step": 292896
    },
    {
      "epoch": 0.0010630938475592057,
      "grad_norm": 8449.366248423607,
      "learning_rate": 1.8485709507800334e-07,
      "loss": 1.7421,
      "step": 292928
    },
    {
      "epoch": 0.0010632099819100425,
      "grad_norm": 10245.896544470865,
      "learning_rate": 1.8484698876513706e-07,
      "loss": 1.7362,
      "step": 292960
    },
    {
      "epoch": 0.0010633261162608793,
      "grad_norm": 10259.093332258948,
      "learning_rate": 1.8483688410965467e-07,
      "loss": 1.716,
      "step": 292992
    },
    {
      "epoch": 0.001063442250611716,
      "grad_norm": 8962.632648948633,
      "learning_rate": 1.8482678111110324e-07,
      "loss": 1.6825,
      "step": 293024
    },
    {
      "epoch": 0.0010635583849625525,
      "grad_norm": 10386.439235849792,
      "learning_rate": 1.8481667976903003e-07,
      "loss": 1.6937,
      "step": 293056
    },
    {
      "epoch": 0.0010636745193133893,
      "grad_norm": 9878.769356554489,
      "learning_rate": 1.848065800829824e-07,
      "loss": 1.7032,
      "step": 293088
    },
    {
      "epoch": 0.001063790653664226,
      "grad_norm": 12101.662034613262,
      "learning_rate": 1.8479648205250793e-07,
      "loss": 1.6856,
      "step": 293120
    },
    {
      "epoch": 0.0010639067880150628,
      "grad_norm": 11885.536756915944,
      "learning_rate": 1.8478638567715434e-07,
      "loss": 1.6931,
      "step": 293152
    },
    {
      "epoch": 0.0010640229223658993,
      "grad_norm": 8920.59930722146,
      "learning_rate": 1.8477629095646958e-07,
      "loss": 1.7036,
      "step": 293184
    },
    {
      "epoch": 0.001064139056716736,
      "grad_norm": 9723.612703105775,
      "learning_rate": 1.847661978900017e-07,
      "loss": 1.7147,
      "step": 293216
    },
    {
      "epoch": 0.0010642551910675728,
      "grad_norm": 12198.691077324649,
      "learning_rate": 1.8475610647729894e-07,
      "loss": 1.7191,
      "step": 293248
    },
    {
      "epoch": 0.0010643713254184096,
      "grad_norm": 9171.141477482506,
      "learning_rate": 1.8474601671790977e-07,
      "loss": 1.7013,
      "step": 293280
    },
    {
      "epoch": 0.0010644874597692464,
      "grad_norm": 8649.519639841279,
      "learning_rate": 1.8473592861138277e-07,
      "loss": 1.6865,
      "step": 293312
    },
    {
      "epoch": 0.001064603594120083,
      "grad_norm": 9871.038648490847,
      "learning_rate": 1.847258421572667e-07,
      "loss": 1.6981,
      "step": 293344
    },
    {
      "epoch": 0.0010647197284709196,
      "grad_norm": 9052.788189281799,
      "learning_rate": 1.8471575735511053e-07,
      "loss": 1.7088,
      "step": 293376
    },
    {
      "epoch": 0.0010648358628217564,
      "grad_norm": 11120.056654531936,
      "learning_rate": 1.847056742044634e-07,
      "loss": 1.7138,
      "step": 293408
    },
    {
      "epoch": 0.0010649519971725932,
      "grad_norm": 9829.663880316559,
      "learning_rate": 1.8469559270487454e-07,
      "loss": 1.7022,
      "step": 293440
    },
    {
      "epoch": 0.0010650681315234297,
      "grad_norm": 10288.688351777402,
      "learning_rate": 1.846855128558935e-07,
      "loss": 1.7093,
      "step": 293472
    },
    {
      "epoch": 0.0010651842658742664,
      "grad_norm": 9554.438340373546,
      "learning_rate": 1.846754346570698e-07,
      "loss": 1.7299,
      "step": 293504
    },
    {
      "epoch": 0.0010653004002251032,
      "grad_norm": 8182.953012207757,
      "learning_rate": 1.8466535810795336e-07,
      "loss": 1.7372,
      "step": 293536
    },
    {
      "epoch": 0.00106541653457594,
      "grad_norm": 14724.244360917133,
      "learning_rate": 1.8465528320809408e-07,
      "loss": 1.7354,
      "step": 293568
    },
    {
      "epoch": 0.0010655326689267767,
      "grad_norm": 9948.46540929806,
      "learning_rate": 1.846452099570422e-07,
      "loss": 1.7124,
      "step": 293600
    },
    {
      "epoch": 0.0010656488032776132,
      "grad_norm": 9564.14763583248,
      "learning_rate": 1.8463513835434793e-07,
      "loss": 1.6939,
      "step": 293632
    },
    {
      "epoch": 0.00106576493762845,
      "grad_norm": 8092.738967741392,
      "learning_rate": 1.8462506839956186e-07,
      "loss": 1.7078,
      "step": 293664
    },
    {
      "epoch": 0.0010658810719792868,
      "grad_norm": 21884.969453942584,
      "learning_rate": 1.846150000922346e-07,
      "loss": 1.7216,
      "step": 293696
    },
    {
      "epoch": 0.0010659972063301235,
      "grad_norm": 10872.349700041845,
      "learning_rate": 1.8460524799012606e-07,
      "loss": 1.7269,
      "step": 293728
    },
    {
      "epoch": 0.00106611334068096,
      "grad_norm": 9515.978457310632,
      "learning_rate": 1.845951829249209e-07,
      "loss": 1.7247,
      "step": 293760
    },
    {
      "epoch": 0.0010662294750317968,
      "grad_norm": 9551.8398227776,
      "learning_rate": 1.8458511950584163e-07,
      "loss": 1.7321,
      "step": 293792
    },
    {
      "epoch": 0.0010663456093826336,
      "grad_norm": 9365.843261554188,
      "learning_rate": 1.845750577324396e-07,
      "loss": 1.7395,
      "step": 293824
    },
    {
      "epoch": 0.0010664617437334703,
      "grad_norm": 8711.140912647435,
      "learning_rate": 1.8456499760426632e-07,
      "loss": 1.7378,
      "step": 293856
    },
    {
      "epoch": 0.001066577878084307,
      "grad_norm": 10874.465504106398,
      "learning_rate": 1.8455493912087345e-07,
      "loss": 1.7209,
      "step": 293888
    },
    {
      "epoch": 0.0010666940124351436,
      "grad_norm": 8975.3203842537,
      "learning_rate": 1.8454488228181285e-07,
      "loss": 1.6777,
      "step": 293920
    },
    {
      "epoch": 0.0010668101467859804,
      "grad_norm": 9297.720795980056,
      "learning_rate": 1.845348270866366e-07,
      "loss": 1.6781,
      "step": 293952
    },
    {
      "epoch": 0.0010669262811368171,
      "grad_norm": 9177.93745892834,
      "learning_rate": 1.8452477353489688e-07,
      "loss": 1.6866,
      "step": 293984
    },
    {
      "epoch": 0.0010670424154876539,
      "grad_norm": 8280.214489975486,
      "learning_rate": 1.8451472162614605e-07,
      "loss": 1.6964,
      "step": 294016
    },
    {
      "epoch": 0.0010671585498384904,
      "grad_norm": 10040.15756848467,
      "learning_rate": 1.845046713599367e-07,
      "loss": 1.6914,
      "step": 294048
    },
    {
      "epoch": 0.0010672746841893272,
      "grad_norm": 9868.680560237017,
      "learning_rate": 1.8449462273582146e-07,
      "loss": 1.6836,
      "step": 294080
    },
    {
      "epoch": 0.001067390818540164,
      "grad_norm": 7877.4043948498675,
      "learning_rate": 1.8448457575335324e-07,
      "loss": 1.7311,
      "step": 294112
    },
    {
      "epoch": 0.0010675069528910007,
      "grad_norm": 10413.32838241453,
      "learning_rate": 1.8447453041208516e-07,
      "loss": 1.7329,
      "step": 294144
    },
    {
      "epoch": 0.0010676230872418374,
      "grad_norm": 9319.101458831748,
      "learning_rate": 1.8446448671157038e-07,
      "loss": 1.7134,
      "step": 294176
    },
    {
      "epoch": 0.001067739221592674,
      "grad_norm": 9317.143339028331,
      "learning_rate": 1.8445444465136232e-07,
      "loss": 1.6954,
      "step": 294208
    },
    {
      "epoch": 0.0010678553559435107,
      "grad_norm": 10518.35310302901,
      "learning_rate": 1.8444440423101454e-07,
      "loss": 1.6748,
      "step": 294240
    },
    {
      "epoch": 0.0010679714902943475,
      "grad_norm": 10851.17523589035,
      "learning_rate": 1.8443436545008075e-07,
      "loss": 1.6752,
      "step": 294272
    },
    {
      "epoch": 0.0010680876246451842,
      "grad_norm": 8603.88482024254,
      "learning_rate": 1.8442432830811491e-07,
      "loss": 1.6955,
      "step": 294304
    },
    {
      "epoch": 0.0010682037589960208,
      "grad_norm": 8929.485091538034,
      "learning_rate": 1.8441429280467106e-07,
      "loss": 1.7161,
      "step": 294336
    },
    {
      "epoch": 0.0010683198933468575,
      "grad_norm": 10112.338206369484,
      "learning_rate": 1.8440425893930342e-07,
      "loss": 1.6927,
      "step": 294368
    },
    {
      "epoch": 0.0010684360276976943,
      "grad_norm": 10214.786928761656,
      "learning_rate": 1.843942267115665e-07,
      "loss": 1.6875,
      "step": 294400
    },
    {
      "epoch": 0.001068552162048531,
      "grad_norm": 9939.217273004951,
      "learning_rate": 1.8438419612101476e-07,
      "loss": 1.7017,
      "step": 294432
    },
    {
      "epoch": 0.0010686682963993678,
      "grad_norm": 12972.633348707579,
      "learning_rate": 1.8437416716720304e-07,
      "loss": 1.6978,
      "step": 294464
    },
    {
      "epoch": 0.0010687844307502043,
      "grad_norm": 10899.721464331096,
      "learning_rate": 1.8436413984968622e-07,
      "loss": 1.6975,
      "step": 294496
    },
    {
      "epoch": 0.001068900565101041,
      "grad_norm": 9560.844732553709,
      "learning_rate": 1.8435411416801944e-07,
      "loss": 1.705,
      "step": 294528
    },
    {
      "epoch": 0.0010690166994518778,
      "grad_norm": 10980.066484316021,
      "learning_rate": 1.843440901217579e-07,
      "loss": 1.6814,
      "step": 294560
    },
    {
      "epoch": 0.0010691328338027146,
      "grad_norm": 8332.716723854232,
      "learning_rate": 1.8433406771045708e-07,
      "loss": 1.6946,
      "step": 294592
    },
    {
      "epoch": 0.0010692489681535511,
      "grad_norm": 9944.88330751045,
      "learning_rate": 1.843240469336726e-07,
      "loss": 1.7152,
      "step": 294624
    },
    {
      "epoch": 0.0010693651025043879,
      "grad_norm": 9593.394602537728,
      "learning_rate": 1.8431402779096017e-07,
      "loss": 1.7328,
      "step": 294656
    },
    {
      "epoch": 0.0010694812368552246,
      "grad_norm": 9444.121981423154,
      "learning_rate": 1.8430401028187575e-07,
      "loss": 1.7176,
      "step": 294688
    },
    {
      "epoch": 0.0010695973712060614,
      "grad_norm": 17359.651609407374,
      "learning_rate": 1.8429399440597545e-07,
      "loss": 1.7289,
      "step": 294720
    },
    {
      "epoch": 0.0010697135055568981,
      "grad_norm": 10418.097139113264,
      "learning_rate": 1.8428429308320438e-07,
      "loss": 1.7406,
      "step": 294752
    },
    {
      "epoch": 0.0010698296399077347,
      "grad_norm": 9834.158428660787,
      "learning_rate": 1.8427428042133876e-07,
      "loss": 1.7004,
      "step": 294784
    },
    {
      "epoch": 0.0010699457742585714,
      "grad_norm": 9140.118598792906,
      "learning_rate": 1.842642693913405e-07,
      "loss": 1.6944,
      "step": 294816
    },
    {
      "epoch": 0.0010700619086094082,
      "grad_norm": 9386.401120770408,
      "learning_rate": 1.842542599927663e-07,
      "loss": 1.6982,
      "step": 294848
    },
    {
      "epoch": 0.001070178042960245,
      "grad_norm": 10764.437003392235,
      "learning_rate": 1.8424425222517315e-07,
      "loss": 1.6743,
      "step": 294880
    },
    {
      "epoch": 0.0010702941773110815,
      "grad_norm": 10081.983931746767,
      "learning_rate": 1.842342460881181e-07,
      "loss": 1.6875,
      "step": 294912
    },
    {
      "epoch": 0.0010704103116619182,
      "grad_norm": 9500.850909260707,
      "learning_rate": 1.8422424158115854e-07,
      "loss": 1.6949,
      "step": 294944
    },
    {
      "epoch": 0.001070526446012755,
      "grad_norm": 11497.109027925237,
      "learning_rate": 1.8421423870385188e-07,
      "loss": 1.7084,
      "step": 294976
    },
    {
      "epoch": 0.0010706425803635917,
      "grad_norm": 9701.588838947979,
      "learning_rate": 1.8420423745575567e-07,
      "loss": 1.6909,
      "step": 295008
    },
    {
      "epoch": 0.0010707587147144285,
      "grad_norm": 9706.673786627425,
      "learning_rate": 1.8419423783642778e-07,
      "loss": 1.6871,
      "step": 295040
    },
    {
      "epoch": 0.001070874849065265,
      "grad_norm": 11836.332202164655,
      "learning_rate": 1.8418423984542612e-07,
      "loss": 1.6903,
      "step": 295072
    },
    {
      "epoch": 0.0010709909834161018,
      "grad_norm": 8138.211597150814,
      "learning_rate": 1.841742434823089e-07,
      "loss": 1.7006,
      "step": 295104
    },
    {
      "epoch": 0.0010711071177669385,
      "grad_norm": 10969.28055981795,
      "learning_rate": 1.8416424874663427e-07,
      "loss": 1.6954,
      "step": 295136
    },
    {
      "epoch": 0.0010712232521177753,
      "grad_norm": 9811.868527451843,
      "learning_rate": 1.841542556379608e-07,
      "loss": 1.6884,
      "step": 295168
    },
    {
      "epoch": 0.0010713393864686118,
      "grad_norm": 10374.25602151788,
      "learning_rate": 1.8414426415584704e-07,
      "loss": 1.6797,
      "step": 295200
    },
    {
      "epoch": 0.0010714555208194486,
      "grad_norm": 8656.54087958926,
      "learning_rate": 1.8413427429985187e-07,
      "loss": 1.6882,
      "step": 295232
    },
    {
      "epoch": 0.0010715716551702853,
      "grad_norm": 12496.936264541002,
      "learning_rate": 1.8412428606953417e-07,
      "loss": 1.7108,
      "step": 295264
    },
    {
      "epoch": 0.001071687789521122,
      "grad_norm": 8800.73996888898,
      "learning_rate": 1.8411429946445316e-07,
      "loss": 1.7123,
      "step": 295296
    },
    {
      "epoch": 0.0010718039238719588,
      "grad_norm": 12484.412361020442,
      "learning_rate": 1.84104314484168e-07,
      "loss": 1.7007,
      "step": 295328
    },
    {
      "epoch": 0.0010719200582227954,
      "grad_norm": 9026.269882958297,
      "learning_rate": 1.840943311282383e-07,
      "loss": 1.7066,
      "step": 295360
    },
    {
      "epoch": 0.0010720361925736321,
      "grad_norm": 10776.88637779948,
      "learning_rate": 1.8408434939622358e-07,
      "loss": 1.7115,
      "step": 295392
    },
    {
      "epoch": 0.0010721523269244689,
      "grad_norm": 10963.4076819208,
      "learning_rate": 1.840743692876837e-07,
      "loss": 1.7104,
      "step": 295424
    },
    {
      "epoch": 0.0010722684612753056,
      "grad_norm": 13310.779391155125,
      "learning_rate": 1.8406439080217862e-07,
      "loss": 1.7063,
      "step": 295456
    },
    {
      "epoch": 0.0010723845956261422,
      "grad_norm": 8986.814341022073,
      "learning_rate": 1.8405441393926849e-07,
      "loss": 1.7075,
      "step": 295488
    },
    {
      "epoch": 0.001072500729976979,
      "grad_norm": 9841.258659338246,
      "learning_rate": 1.8404443869851356e-07,
      "loss": 1.7115,
      "step": 295520
    },
    {
      "epoch": 0.0010726168643278157,
      "grad_norm": 9160.113536414274,
      "learning_rate": 1.840344650794743e-07,
      "loss": 1.6994,
      "step": 295552
    },
    {
      "epoch": 0.0010727329986786524,
      "grad_norm": 9459.433598265807,
      "learning_rate": 1.8402449308171136e-07,
      "loss": 1.7167,
      "step": 295584
    },
    {
      "epoch": 0.0010728491330294892,
      "grad_norm": 10754.013576335117,
      "learning_rate": 1.8401452270478555e-07,
      "loss": 1.7117,
      "step": 295616
    },
    {
      "epoch": 0.0010729652673803257,
      "grad_norm": 8704.774551934128,
      "learning_rate": 1.8400455394825786e-07,
      "loss": 1.6833,
      "step": 295648
    },
    {
      "epoch": 0.0010730814017311625,
      "grad_norm": 9295.208873392787,
      "learning_rate": 1.8399458681168934e-07,
      "loss": 1.6974,
      "step": 295680
    },
    {
      "epoch": 0.0010731975360819992,
      "grad_norm": 8898.989043706031,
      "learning_rate": 1.8398462129464135e-07,
      "loss": 1.7021,
      "step": 295712
    },
    {
      "epoch": 0.001073313670432836,
      "grad_norm": 9502.550604969174,
      "learning_rate": 1.8397496874398355e-07,
      "loss": 1.6976,
      "step": 295744
    },
    {
      "epoch": 0.0010734298047836725,
      "grad_norm": 9919.005998586754,
      "learning_rate": 1.8396500641408515e-07,
      "loss": 1.7036,
      "step": 295776
    },
    {
      "epoch": 0.0010735459391345093,
      "grad_norm": 8469.596212335036,
      "learning_rate": 1.839550457024059e-07,
      "loss": 1.6875,
      "step": 295808
    },
    {
      "epoch": 0.001073662073485346,
      "grad_norm": 9824.734907365186,
      "learning_rate": 1.8394508660850767e-07,
      "loss": 1.6983,
      "step": 295840
    },
    {
      "epoch": 0.0010737782078361828,
      "grad_norm": 9768.369567128384,
      "learning_rate": 1.8393512913195265e-07,
      "loss": 1.7216,
      "step": 295872
    },
    {
      "epoch": 0.0010738943421870195,
      "grad_norm": 10705.630668017648,
      "learning_rate": 1.839251732723031e-07,
      "loss": 1.7326,
      "step": 295904
    },
    {
      "epoch": 0.001074010476537856,
      "grad_norm": 10500.20533132567,
      "learning_rate": 1.839152190291215e-07,
      "loss": 1.7062,
      "step": 295936
    },
    {
      "epoch": 0.0010741266108886928,
      "grad_norm": 10331.494180417467,
      "learning_rate": 1.839052664019704e-07,
      "loss": 1.6803,
      "step": 295968
    },
    {
      "epoch": 0.0010742427452395296,
      "grad_norm": 10033.356367636905,
      "learning_rate": 1.8389531539041268e-07,
      "loss": 1.7053,
      "step": 296000
    },
    {
      "epoch": 0.0010743588795903663,
      "grad_norm": 9594.938978440667,
      "learning_rate": 1.8388536599401125e-07,
      "loss": 1.6947,
      "step": 296032
    },
    {
      "epoch": 0.0010744750139412029,
      "grad_norm": 8970.004459307698,
      "learning_rate": 1.8387541821232925e-07,
      "loss": 1.7004,
      "step": 296064
    },
    {
      "epoch": 0.0010745911482920396,
      "grad_norm": 8902.924912634051,
      "learning_rate": 1.8386547204492991e-07,
      "loss": 1.7096,
      "step": 296096
    },
    {
      "epoch": 0.0010747072826428764,
      "grad_norm": 10619.536336394352,
      "learning_rate": 1.838555274913767e-07,
      "loss": 1.6897,
      "step": 296128
    },
    {
      "epoch": 0.0010748234169937131,
      "grad_norm": 9232.980775459246,
      "learning_rate": 1.838455845512333e-07,
      "loss": 1.6875,
      "step": 296160
    },
    {
      "epoch": 0.00107493955134455,
      "grad_norm": 9213.955502388753,
      "learning_rate": 1.8383564322406344e-07,
      "loss": 1.6828,
      "step": 296192
    },
    {
      "epoch": 0.0010750556856953864,
      "grad_norm": 10316.200463348898,
      "learning_rate": 1.8382570350943103e-07,
      "loss": 1.6985,
      "step": 296224
    },
    {
      "epoch": 0.0010751718200462232,
      "grad_norm": 9635.20191796726,
      "learning_rate": 1.838157654069002e-07,
      "loss": 1.6884,
      "step": 296256
    },
    {
      "epoch": 0.00107528795439706,
      "grad_norm": 10892.360625686242,
      "learning_rate": 1.8380582891603528e-07,
      "loss": 1.6906,
      "step": 296288
    },
    {
      "epoch": 0.0010754040887478967,
      "grad_norm": 9614.908631911174,
      "learning_rate": 1.8379589403640066e-07,
      "loss": 1.7124,
      "step": 296320
    },
    {
      "epoch": 0.0010755202230987332,
      "grad_norm": 9247.422992380094,
      "learning_rate": 1.8378596076756092e-07,
      "loss": 1.7151,
      "step": 296352
    },
    {
      "epoch": 0.00107563635744957,
      "grad_norm": 9225.044606938223,
      "learning_rate": 1.8377602910908087e-07,
      "loss": 1.7246,
      "step": 296384
    },
    {
      "epoch": 0.0010757524918004067,
      "grad_norm": 9662.52906852031,
      "learning_rate": 1.8376609906052545e-07,
      "loss": 1.7309,
      "step": 296416
    },
    {
      "epoch": 0.0010758686261512435,
      "grad_norm": 8967.265804022985,
      "learning_rate": 1.8375617062145976e-07,
      "loss": 1.7091,
      "step": 296448
    },
    {
      "epoch": 0.0010759847605020802,
      "grad_norm": 9242.295602284099,
      "learning_rate": 1.8374624379144902e-07,
      "loss": 1.7228,
      "step": 296480
    },
    {
      "epoch": 0.0010761008948529168,
      "grad_norm": 11305.694317466752,
      "learning_rate": 1.837363185700587e-07,
      "loss": 1.7184,
      "step": 296512
    },
    {
      "epoch": 0.0010762170292037535,
      "grad_norm": 10690.932980802003,
      "learning_rate": 1.8372639495685438e-07,
      "loss": 1.6978,
      "step": 296544
    },
    {
      "epoch": 0.0010763331635545903,
      "grad_norm": 8127.391586480868,
      "learning_rate": 1.8371647295140177e-07,
      "loss": 1.6824,
      "step": 296576
    },
    {
      "epoch": 0.001076449297905427,
      "grad_norm": 9528.188915003731,
      "learning_rate": 1.8370655255326687e-07,
      "loss": 1.6878,
      "step": 296608
    },
    {
      "epoch": 0.0010765654322562636,
      "grad_norm": 7874.410327129264,
      "learning_rate": 1.8369663376201572e-07,
      "loss": 1.7084,
      "step": 296640
    },
    {
      "epoch": 0.0010766815666071003,
      "grad_norm": 8882.719403425957,
      "learning_rate": 1.8368671657721457e-07,
      "loss": 1.7075,
      "step": 296672
    },
    {
      "epoch": 0.001076797700957937,
      "grad_norm": 12862.70889043206,
      "learning_rate": 1.8367680099842982e-07,
      "loss": 1.7137,
      "step": 296704
    },
    {
      "epoch": 0.0010769138353087738,
      "grad_norm": 9176.383383446879,
      "learning_rate": 1.8366688702522804e-07,
      "loss": 1.7108,
      "step": 296736
    },
    {
      "epoch": 0.0010770299696596106,
      "grad_norm": 21452.6455245035,
      "learning_rate": 1.8365697465717604e-07,
      "loss": 1.7004,
      "step": 296768
    },
    {
      "epoch": 0.0010771461040104471,
      "grad_norm": 10563.594653336524,
      "learning_rate": 1.8364737358090902e-07,
      "loss": 1.6866,
      "step": 296800
    },
    {
      "epoch": 0.0010772622383612839,
      "grad_norm": 8747.611559734463,
      "learning_rate": 1.8363746437173002e-07,
      "loss": 1.6987,
      "step": 296832
    },
    {
      "epoch": 0.0010773783727121206,
      "grad_norm": 8622.967006779047,
      "learning_rate": 1.8362755676641546e-07,
      "loss": 1.7194,
      "step": 296864
    },
    {
      "epoch": 0.0010774945070629574,
      "grad_norm": 9870.542639591808,
      "learning_rate": 1.8361765076453273e-07,
      "loss": 1.6898,
      "step": 296896
    },
    {
      "epoch": 0.001077610641413794,
      "grad_norm": 9798.237188392613,
      "learning_rate": 1.836077463656494e-07,
      "loss": 1.6858,
      "step": 296928
    },
    {
      "epoch": 0.0010777267757646307,
      "grad_norm": 11163.504825994389,
      "learning_rate": 1.8359784356933318e-07,
      "loss": 1.7046,
      "step": 296960
    },
    {
      "epoch": 0.0010778429101154674,
      "grad_norm": 8989.103181074295,
      "learning_rate": 1.8358794237515194e-07,
      "loss": 1.7068,
      "step": 296992
    },
    {
      "epoch": 0.0010779590444663042,
      "grad_norm": 9207.546035725263,
      "learning_rate": 1.8357804278267371e-07,
      "loss": 1.7173,
      "step": 297024
    },
    {
      "epoch": 0.001078075178817141,
      "grad_norm": 8485.727075507437,
      "learning_rate": 1.8356814479146672e-07,
      "loss": 1.7089,
      "step": 297056
    },
    {
      "epoch": 0.0010781913131679775,
      "grad_norm": 10934.036765989036,
      "learning_rate": 1.835582484010993e-07,
      "loss": 1.6984,
      "step": 297088
    },
    {
      "epoch": 0.0010783074475188142,
      "grad_norm": 11448.839766544032,
      "learning_rate": 1.8354835361114008e-07,
      "loss": 1.6937,
      "step": 297120
    },
    {
      "epoch": 0.001078423581869651,
      "grad_norm": 11042.89128806401,
      "learning_rate": 1.835384604211576e-07,
      "loss": 1.6924,
      "step": 297152
    },
    {
      "epoch": 0.0010785397162204878,
      "grad_norm": 9373.899508742345,
      "learning_rate": 1.8352856883072084e-07,
      "loss": 1.7076,
      "step": 297184
    },
    {
      "epoch": 0.0010786558505713243,
      "grad_norm": 9714.02779489538,
      "learning_rate": 1.8351867883939873e-07,
      "loss": 1.6917,
      "step": 297216
    },
    {
      "epoch": 0.001078771984922161,
      "grad_norm": 9379.389105906632,
      "learning_rate": 1.8350879044676052e-07,
      "loss": 1.7139,
      "step": 297248
    },
    {
      "epoch": 0.0010788881192729978,
      "grad_norm": 8586.447460970106,
      "learning_rate": 1.834989036523755e-07,
      "loss": 1.7309,
      "step": 297280
    },
    {
      "epoch": 0.0010790042536238346,
      "grad_norm": 8861.498857416842,
      "learning_rate": 1.8348901845581323e-07,
      "loss": 1.7204,
      "step": 297312
    },
    {
      "epoch": 0.0010791203879746713,
      "grad_norm": 11978.432952602774,
      "learning_rate": 1.8347913485664336e-07,
      "loss": 1.7263,
      "step": 297344
    },
    {
      "epoch": 0.0010792365223255078,
      "grad_norm": 9169.632708020534,
      "learning_rate": 1.8346925285443572e-07,
      "loss": 1.6975,
      "step": 297376
    },
    {
      "epoch": 0.0010793526566763446,
      "grad_norm": 9274.480039333743,
      "learning_rate": 1.8345937244876025e-07,
      "loss": 1.6755,
      "step": 297408
    },
    {
      "epoch": 0.0010794687910271814,
      "grad_norm": 9327.96655225564,
      "learning_rate": 1.8344949363918716e-07,
      "loss": 1.6795,
      "step": 297440
    },
    {
      "epoch": 0.001079584925378018,
      "grad_norm": 9736.79742009661,
      "learning_rate": 1.8343961642528674e-07,
      "loss": 1.7085,
      "step": 297472
    },
    {
      "epoch": 0.0010797010597288546,
      "grad_norm": 12914.696744407125,
      "learning_rate": 1.8342974080662953e-07,
      "loss": 1.7001,
      "step": 297504
    },
    {
      "epoch": 0.0010798171940796914,
      "grad_norm": 10756.285976116478,
      "learning_rate": 1.8341986678278608e-07,
      "loss": 1.6772,
      "step": 297536
    },
    {
      "epoch": 0.0010799333284305282,
      "grad_norm": 8882.450337603921,
      "learning_rate": 1.8340999435332725e-07,
      "loss": 1.7015,
      "step": 297568
    },
    {
      "epoch": 0.001080049462781365,
      "grad_norm": 8063.678068970759,
      "learning_rate": 1.8340012351782397e-07,
      "loss": 1.7228,
      "step": 297600
    },
    {
      "epoch": 0.0010801655971322017,
      "grad_norm": 10615.213233845094,
      "learning_rate": 1.833902542758474e-07,
      "loss": 1.7414,
      "step": 297632
    },
    {
      "epoch": 0.0010802817314830382,
      "grad_norm": 9315.73797398789,
      "learning_rate": 1.8338038662696885e-07,
      "loss": 1.7336,
      "step": 297664
    },
    {
      "epoch": 0.001080397865833875,
      "grad_norm": 10040.102589117305,
      "learning_rate": 1.8337052057075967e-07,
      "loss": 1.7131,
      "step": 297696
    },
    {
      "epoch": 0.0010805140001847117,
      "grad_norm": 9072.786782461053,
      "learning_rate": 1.8336065610679155e-07,
      "loss": 1.6798,
      "step": 297728
    },
    {
      "epoch": 0.0010806301345355485,
      "grad_norm": 9100.625692775197,
      "learning_rate": 1.8335079323463623e-07,
      "loss": 1.6854,
      "step": 297760
    },
    {
      "epoch": 0.001080746268886385,
      "grad_norm": 9516.168556724917,
      "learning_rate": 1.8334124009480567e-07,
      "loss": 1.7008,
      "step": 297792
    },
    {
      "epoch": 0.0010808624032372218,
      "grad_norm": 9733.259680086625,
      "learning_rate": 1.83331380355281e-07,
      "loss": 1.69,
      "step": 297824
    },
    {
      "epoch": 0.0010809785375880585,
      "grad_norm": 8962.122962780639,
      "learning_rate": 1.8332152220629878e-07,
      "loss": 1.6834,
      "step": 297856
    },
    {
      "epoch": 0.0010810946719388953,
      "grad_norm": 9682.114231922695,
      "learning_rate": 1.8331166564743143e-07,
      "loss": 1.7069,
      "step": 297888
    },
    {
      "epoch": 0.001081210806289732,
      "grad_norm": 9702.863494865833,
      "learning_rate": 1.8330181067825153e-07,
      "loss": 1.6986,
      "step": 297920
    },
    {
      "epoch": 0.0010813269406405685,
      "grad_norm": 8025.964614923243,
      "learning_rate": 1.8329195729833178e-07,
      "loss": 1.7017,
      "step": 297952
    },
    {
      "epoch": 0.0010814430749914053,
      "grad_norm": 9857.910731995902,
      "learning_rate": 1.8328210550724512e-07,
      "loss": 1.7053,
      "step": 297984
    },
    {
      "epoch": 0.001081559209342242,
      "grad_norm": 10628.47326759587,
      "learning_rate": 1.8327225530456454e-07,
      "loss": 1.6813,
      "step": 298016
    },
    {
      "epoch": 0.0010816753436930788,
      "grad_norm": 7793.715288615565,
      "learning_rate": 1.832624066898633e-07,
      "loss": 1.6962,
      "step": 298048
    },
    {
      "epoch": 0.0010817914780439153,
      "grad_norm": 10020.901057290208,
      "learning_rate": 1.832525596627148e-07,
      "loss": 1.6896,
      "step": 298080
    },
    {
      "epoch": 0.001081907612394752,
      "grad_norm": 9249.675453765933,
      "learning_rate": 1.8324271422269247e-07,
      "loss": 1.7166,
      "step": 298112
    },
    {
      "epoch": 0.0010820237467455889,
      "grad_norm": 11095.593359527917,
      "learning_rate": 1.8323287036937012e-07,
      "loss": 1.7177,
      "step": 298144
    },
    {
      "epoch": 0.0010821398810964256,
      "grad_norm": 10084.877391421276,
      "learning_rate": 1.8322302810232155e-07,
      "loss": 1.7133,
      "step": 298176
    },
    {
      "epoch": 0.0010822560154472624,
      "grad_norm": 9282.057530526301,
      "learning_rate": 1.832131874211208e-07,
      "loss": 1.7391,
      "step": 298208
    },
    {
      "epoch": 0.001082372149798099,
      "grad_norm": 10355.862494258989,
      "learning_rate": 1.8320334832534201e-07,
      "loss": 1.7493,
      "step": 298240
    },
    {
      "epoch": 0.0010824882841489357,
      "grad_norm": 10657.656590451768,
      "learning_rate": 1.8319351081455952e-07,
      "loss": 1.7355,
      "step": 298272
    },
    {
      "epoch": 0.0010826044184997724,
      "grad_norm": 9639.79667835375,
      "learning_rate": 1.8318367488834792e-07,
      "loss": 1.7213,
      "step": 298304
    },
    {
      "epoch": 0.0010827205528506092,
      "grad_norm": 13405.324613749568,
      "learning_rate": 1.831738405462817e-07,
      "loss": 1.689,
      "step": 298336
    },
    {
      "epoch": 0.0010828366872014457,
      "grad_norm": 9013.55113149085,
      "learning_rate": 1.831640077879358e-07,
      "loss": 1.6971,
      "step": 298368
    },
    {
      "epoch": 0.0010829528215522825,
      "grad_norm": 8626.950214299373,
      "learning_rate": 1.8315417661288517e-07,
      "loss": 1.7065,
      "step": 298400
    },
    {
      "epoch": 0.0010830689559031192,
      "grad_norm": 7883.658161031591,
      "learning_rate": 1.8314434702070498e-07,
      "loss": 1.715,
      "step": 298432
    },
    {
      "epoch": 0.001083185090253956,
      "grad_norm": 9322.18804787803,
      "learning_rate": 1.8313451901097043e-07,
      "loss": 1.7132,
      "step": 298464
    },
    {
      "epoch": 0.0010833012246047927,
      "grad_norm": 8803.480902461253,
      "learning_rate": 1.8312469258325703e-07,
      "loss": 1.7084,
      "step": 298496
    },
    {
      "epoch": 0.0010834173589556293,
      "grad_norm": 9118.068874493107,
      "learning_rate": 1.8311486773714038e-07,
      "loss": 1.7295,
      "step": 298528
    },
    {
      "epoch": 0.001083533493306466,
      "grad_norm": 11541.430067370335,
      "learning_rate": 1.831050444721963e-07,
      "loss": 1.7079,
      "step": 298560
    },
    {
      "epoch": 0.0010836496276573028,
      "grad_norm": 9085.95311456096,
      "learning_rate": 1.8309522278800068e-07,
      "loss": 1.7082,
      "step": 298592
    },
    {
      "epoch": 0.0010837657620081395,
      "grad_norm": 10031.57654608686,
      "learning_rate": 1.8308540268412956e-07,
      "loss": 1.7105,
      "step": 298624
    },
    {
      "epoch": 0.001083881896358976,
      "grad_norm": 9137.432899890428,
      "learning_rate": 1.8307558416015934e-07,
      "loss": 1.6903,
      "step": 298656
    },
    {
      "epoch": 0.0010839980307098128,
      "grad_norm": 8950.117876318725,
      "learning_rate": 1.830657672156663e-07,
      "loss": 1.6994,
      "step": 298688
    },
    {
      "epoch": 0.0010841141650606496,
      "grad_norm": 9885.52153404159,
      "learning_rate": 1.83055951850227e-07,
      "loss": 1.7112,
      "step": 298720
    },
    {
      "epoch": 0.0010842302994114863,
      "grad_norm": 10401.243579495675,
      "learning_rate": 1.8304613806341826e-07,
      "loss": 1.7256,
      "step": 298752
    },
    {
      "epoch": 0.001084346433762323,
      "grad_norm": 9704.686702825598,
      "learning_rate": 1.8303632585481693e-07,
      "loss": 1.7061,
      "step": 298784
    },
    {
      "epoch": 0.0010844625681131596,
      "grad_norm": 11689.44703568137,
      "learning_rate": 1.8302682178233482e-07,
      "loss": 1.7232,
      "step": 298816
    },
    {
      "epoch": 0.0010845787024639964,
      "grad_norm": 11682.589610184892,
      "learning_rate": 1.830170126795934e-07,
      "loss": 1.712,
      "step": 298848
    },
    {
      "epoch": 0.0010846948368148331,
      "grad_norm": 9430.668905226183,
      "learning_rate": 1.830072051538042e-07,
      "loss": 1.7127,
      "step": 298880
    },
    {
      "epoch": 0.0010848109711656699,
      "grad_norm": 8591.62301314484,
      "learning_rate": 1.829973992045448e-07,
      "loss": 1.7041,
      "step": 298912
    },
    {
      "epoch": 0.0010849271055165064,
      "grad_norm": 10219.921330421286,
      "learning_rate": 1.8298759483139278e-07,
      "loss": 1.6883,
      "step": 298944
    },
    {
      "epoch": 0.0010850432398673432,
      "grad_norm": 8070.018463423736,
      "learning_rate": 1.8297779203392602e-07,
      "loss": 1.6883,
      "step": 298976
    },
    {
      "epoch": 0.00108515937421818,
      "grad_norm": 9118.968801350293,
      "learning_rate": 1.8296799081172246e-07,
      "loss": 1.7024,
      "step": 299008
    },
    {
      "epoch": 0.0010852755085690167,
      "grad_norm": 10857.498422749137,
      "learning_rate": 1.829581911643603e-07,
      "loss": 1.7295,
      "step": 299040
    },
    {
      "epoch": 0.0010853916429198534,
      "grad_norm": 9537.617941603658,
      "learning_rate": 1.8294839309141783e-07,
      "loss": 1.717,
      "step": 299072
    },
    {
      "epoch": 0.00108550777727069,
      "grad_norm": 8380.941116604985,
      "learning_rate": 1.8293859659247354e-07,
      "loss": 1.7023,
      "step": 299104
    },
    {
      "epoch": 0.0010856239116215267,
      "grad_norm": 8851.081289876396,
      "learning_rate": 1.82928801667106e-07,
      "loss": 1.7029,
      "step": 299136
    },
    {
      "epoch": 0.0010857400459723635,
      "grad_norm": 8767.383646219663,
      "learning_rate": 1.8291900831489405e-07,
      "loss": 1.6926,
      "step": 299168
    },
    {
      "epoch": 0.0010858561803232002,
      "grad_norm": 8578.468627907898,
      "learning_rate": 1.829092165354166e-07,
      "loss": 1.6946,
      "step": 299200
    },
    {
      "epoch": 0.0010859723146740368,
      "grad_norm": 10738.368777426113,
      "learning_rate": 1.8289942632825273e-07,
      "loss": 1.7023,
      "step": 299232
    },
    {
      "epoch": 0.0010860884490248735,
      "grad_norm": 9762.559705323189,
      "learning_rate": 1.8288963769298174e-07,
      "loss": 1.6908,
      "step": 299264
    },
    {
      "epoch": 0.0010862045833757103,
      "grad_norm": 10854.010687298958,
      "learning_rate": 1.82879850629183e-07,
      "loss": 1.6889,
      "step": 299296
    },
    {
      "epoch": 0.001086320717726547,
      "grad_norm": 10857.932031468976,
      "learning_rate": 1.828700651364361e-07,
      "loss": 1.6888,
      "step": 299328
    },
    {
      "epoch": 0.0010864368520773838,
      "grad_norm": 10377.87107262371,
      "learning_rate": 1.828602812143208e-07,
      "loss": 1.7191,
      "step": 299360
    },
    {
      "epoch": 0.0010865529864282203,
      "grad_norm": 11872.821905511764,
      "learning_rate": 1.828504988624169e-07,
      "loss": 1.718,
      "step": 299392
    },
    {
      "epoch": 0.001086669120779057,
      "grad_norm": 9387.606723760855,
      "learning_rate": 1.8284071808030452e-07,
      "loss": 1.694,
      "step": 299424
    },
    {
      "epoch": 0.0010867852551298938,
      "grad_norm": 8344.75619775677,
      "learning_rate": 1.8283093886756383e-07,
      "loss": 1.7122,
      "step": 299456
    },
    {
      "epoch": 0.0010869013894807306,
      "grad_norm": 8253.838016341246,
      "learning_rate": 1.8282116122377517e-07,
      "loss": 1.7,
      "step": 299488
    },
    {
      "epoch": 0.0010870175238315671,
      "grad_norm": 10137.837244698694,
      "learning_rate": 1.828113851485191e-07,
      "loss": 1.6883,
      "step": 299520
    },
    {
      "epoch": 0.0010871336581824039,
      "grad_norm": 10290.55693342202,
      "learning_rate": 1.828016106413763e-07,
      "loss": 1.7011,
      "step": 299552
    },
    {
      "epoch": 0.0010872497925332406,
      "grad_norm": 8773.16772893349,
      "learning_rate": 1.8279183770192747e-07,
      "loss": 1.6772,
      "step": 299584
    },
    {
      "epoch": 0.0010873659268840774,
      "grad_norm": 9345.039325759952,
      "learning_rate": 1.8278206632975377e-07,
      "loss": 1.6872,
      "step": 299616
    },
    {
      "epoch": 0.0010874820612349141,
      "grad_norm": 12679.083799707294,
      "learning_rate": 1.827722965244362e-07,
      "loss": 1.6965,
      "step": 299648
    },
    {
      "epoch": 0.0010875981955857507,
      "grad_norm": 10152.707028177263,
      "learning_rate": 1.8276252828555613e-07,
      "loss": 1.6971,
      "step": 299680
    },
    {
      "epoch": 0.0010877143299365874,
      "grad_norm": 8097.076262454245,
      "learning_rate": 1.8275276161269504e-07,
      "loss": 1.687,
      "step": 299712
    },
    {
      "epoch": 0.0010878304642874242,
      "grad_norm": 10485.126990170409,
      "learning_rate": 1.8274299650543446e-07,
      "loss": 1.677,
      "step": 299744
    },
    {
      "epoch": 0.001087946598638261,
      "grad_norm": 10450.046698460252,
      "learning_rate": 1.8273323296335621e-07,
      "loss": 1.7026,
      "step": 299776
    },
    {
      "epoch": 0.0010880627329890975,
      "grad_norm": 8549.17516489164,
      "learning_rate": 1.8272377602415203e-07,
      "loss": 1.7034,
      "step": 299808
    },
    {
      "epoch": 0.0010881788673399342,
      "grad_norm": 10849.128259911024,
      "learning_rate": 1.8271401556230484e-07,
      "loss": 1.7082,
      "step": 299840
    },
    {
      "epoch": 0.001088295001690771,
      "grad_norm": 9584.03025871684,
      "learning_rate": 1.8270425666439927e-07,
      "loss": 1.7133,
      "step": 299872
    },
    {
      "epoch": 0.0010884111360416077,
      "grad_norm": 10123.266468882463,
      "learning_rate": 1.826944993300177e-07,
      "loss": 1.7005,
      "step": 299904
    },
    {
      "epoch": 0.0010885272703924445,
      "grad_norm": 10929.685814331535,
      "learning_rate": 1.8268474355874268e-07,
      "loss": 1.7109,
      "step": 299936
    },
    {
      "epoch": 0.001088643404743281,
      "grad_norm": 9328.051029019942,
      "learning_rate": 1.826749893501569e-07,
      "loss": 1.718,
      "step": 299968
    },
    {
      "epoch": 0.0010887595390941178,
      "grad_norm": 10432.30693566864,
      "learning_rate": 1.826652367038432e-07,
      "loss": 1.7338,
      "step": 300000
    },
    {
      "epoch": 0.0010888756734449545,
      "grad_norm": 11796.011359777507,
      "learning_rate": 1.8265548561938467e-07,
      "loss": 1.7053,
      "step": 300032
    },
    {
      "epoch": 0.0010889918077957913,
      "grad_norm": 13220.167775032207,
      "learning_rate": 1.8264573609636437e-07,
      "loss": 1.6842,
      "step": 300064
    },
    {
      "epoch": 0.0010891079421466278,
      "grad_norm": 10708.805722395005,
      "learning_rate": 1.826359881343657e-07,
      "loss": 1.6996,
      "step": 300096
    },
    {
      "epoch": 0.0010892240764974646,
      "grad_norm": 11766.449761929041,
      "learning_rate": 1.8262624173297208e-07,
      "loss": 1.6954,
      "step": 300128
    },
    {
      "epoch": 0.0010893402108483013,
      "grad_norm": 8866.280505375407,
      "learning_rate": 1.8261649689176722e-07,
      "loss": 1.6977,
      "step": 300160
    },
    {
      "epoch": 0.001089456345199138,
      "grad_norm": 10076.016474778115,
      "learning_rate": 1.8260675361033485e-07,
      "loss": 1.6949,
      "step": 300192
    },
    {
      "epoch": 0.0010895724795499748,
      "grad_norm": 8768.230152088847,
      "learning_rate": 1.8259701188825893e-07,
      "loss": 1.6918,
      "step": 300224
    },
    {
      "epoch": 0.0010896886139008114,
      "grad_norm": 8890.676689656419,
      "learning_rate": 1.8258727172512357e-07,
      "loss": 1.694,
      "step": 300256
    },
    {
      "epoch": 0.0010898047482516481,
      "grad_norm": 10468.657602577323,
      "learning_rate": 1.82577533120513e-07,
      "loss": 1.7064,
      "step": 300288
    },
    {
      "epoch": 0.0010899208826024849,
      "grad_norm": 9082.600288463651,
      "learning_rate": 1.8256779607401167e-07,
      "loss": 1.6965,
      "step": 300320
    },
    {
      "epoch": 0.0010900370169533216,
      "grad_norm": 8430.722626204708,
      "learning_rate": 1.825580605852041e-07,
      "loss": 1.6813,
      "step": 300352
    },
    {
      "epoch": 0.0010901531513041582,
      "grad_norm": 9178.950484668712,
      "learning_rate": 1.825483266536751e-07,
      "loss": 1.6836,
      "step": 300384
    },
    {
      "epoch": 0.001090269285654995,
      "grad_norm": 9869.314261892769,
      "learning_rate": 1.8253859427900942e-07,
      "loss": 1.6949,
      "step": 300416
    },
    {
      "epoch": 0.0010903854200058317,
      "grad_norm": 8627.749069137326,
      "learning_rate": 1.8252886346079218e-07,
      "loss": 1.6959,
      "step": 300448
    },
    {
      "epoch": 0.0010905015543566684,
      "grad_norm": 9878.083214875242,
      "learning_rate": 1.8251913419860853e-07,
      "loss": 1.6959,
      "step": 300480
    },
    {
      "epoch": 0.0010906176887075052,
      "grad_norm": 8626.593881712526,
      "learning_rate": 1.8250940649204387e-07,
      "loss": 1.6951,
      "step": 300512
    },
    {
      "epoch": 0.0010907338230583417,
      "grad_norm": 9380.096801206264,
      "learning_rate": 1.8249968034068362e-07,
      "loss": 1.6899,
      "step": 300544
    },
    {
      "epoch": 0.0010908499574091785,
      "grad_norm": 8610.473854556438,
      "learning_rate": 1.8248995574411345e-07,
      "loss": 1.6912,
      "step": 300576
    },
    {
      "epoch": 0.0010909660917600152,
      "grad_norm": 10708.428456127443,
      "learning_rate": 1.8248023270191918e-07,
      "loss": 1.7154,
      "step": 300608
    },
    {
      "epoch": 0.001091082226110852,
      "grad_norm": 10593.704545625198,
      "learning_rate": 1.824705112136868e-07,
      "loss": 1.7224,
      "step": 300640
    },
    {
      "epoch": 0.0010911983604616885,
      "grad_norm": 10591.49696690699,
      "learning_rate": 1.8246079127900237e-07,
      "loss": 1.6847,
      "step": 300672
    },
    {
      "epoch": 0.0010913144948125253,
      "grad_norm": 8920.57352416312,
      "learning_rate": 1.8245107289745218e-07,
      "loss": 1.6989,
      "step": 300704
    },
    {
      "epoch": 0.001091430629163362,
      "grad_norm": 8500.631505952955,
      "learning_rate": 1.8244135606862262e-07,
      "loss": 1.7153,
      "step": 300736
    },
    {
      "epoch": 0.0010915467635141988,
      "grad_norm": 11914.689253186589,
      "learning_rate": 1.8243164079210034e-07,
      "loss": 1.7276,
      "step": 300768
    },
    {
      "epoch": 0.0010916628978650355,
      "grad_norm": 22233.56345707993,
      "learning_rate": 1.8242192706747201e-07,
      "loss": 1.7329,
      "step": 300800
    },
    {
      "epoch": 0.001091779032215872,
      "grad_norm": 8586.991673455845,
      "learning_rate": 1.824125183762551e-07,
      "loss": 1.7243,
      "step": 300832
    },
    {
      "epoch": 0.0010918951665667088,
      "grad_norm": 8014.51096449434,
      "learning_rate": 1.8240280770571094e-07,
      "loss": 1.7007,
      "step": 300864
    },
    {
      "epoch": 0.0010920113009175456,
      "grad_norm": 9961.35432559248,
      "learning_rate": 1.823930985858347e-07,
      "loss": 1.6958,
      "step": 300896
    },
    {
      "epoch": 0.0010921274352683823,
      "grad_norm": 9521.25642969456,
      "learning_rate": 1.8238339101621376e-07,
      "loss": 1.703,
      "step": 300928
    },
    {
      "epoch": 0.0010922435696192189,
      "grad_norm": 9985.114721424085,
      "learning_rate": 1.8237368499643567e-07,
      "loss": 1.6863,
      "step": 300960
    },
    {
      "epoch": 0.0010923597039700556,
      "grad_norm": 10458.593978159779,
      "learning_rate": 1.8236398052608802e-07,
      "loss": 1.6816,
      "step": 300992
    },
    {
      "epoch": 0.0010924758383208924,
      "grad_norm": 8995.14624672662,
      "learning_rate": 1.8235427760475865e-07,
      "loss": 1.6954,
      "step": 301024
    },
    {
      "epoch": 0.0010925919726717291,
      "grad_norm": 11791.044907047042,
      "learning_rate": 1.8234457623203552e-07,
      "loss": 1.7039,
      "step": 301056
    },
    {
      "epoch": 0.001092708107022566,
      "grad_norm": 10140.021498991015,
      "learning_rate": 1.823348764075067e-07,
      "loss": 1.7024,
      "step": 301088
    },
    {
      "epoch": 0.0010928242413734024,
      "grad_norm": 8862.573441162562,
      "learning_rate": 1.8232517813076047e-07,
      "loss": 1.7199,
      "step": 301120
    },
    {
      "epoch": 0.0010929403757242392,
      "grad_norm": 8723.260285008124,
      "learning_rate": 1.8231548140138528e-07,
      "loss": 1.7049,
      "step": 301152
    },
    {
      "epoch": 0.001093056510075076,
      "grad_norm": 9570.598100432386,
      "learning_rate": 1.8230578621896965e-07,
      "loss": 1.699,
      "step": 301184
    },
    {
      "epoch": 0.0010931726444259127,
      "grad_norm": 9480.040928181692,
      "learning_rate": 1.8229609258310232e-07,
      "loss": 1.7062,
      "step": 301216
    },
    {
      "epoch": 0.0010932887787767492,
      "grad_norm": 10944.274667605889,
      "learning_rate": 1.8228640049337217e-07,
      "loss": 1.7109,
      "step": 301248
    },
    {
      "epoch": 0.001093404913127586,
      "grad_norm": 9842.54479288766,
      "learning_rate": 1.8227670994936823e-07,
      "loss": 1.685,
      "step": 301280
    },
    {
      "epoch": 0.0010935210474784227,
      "grad_norm": 10897.755915783762,
      "learning_rate": 1.8226702095067968e-07,
      "loss": 1.6775,
      "step": 301312
    },
    {
      "epoch": 0.0010936371818292595,
      "grad_norm": 11706.372623490166,
      "learning_rate": 1.8225733349689584e-07,
      "loss": 1.6994,
      "step": 301344
    },
    {
      "epoch": 0.0010937533161800963,
      "grad_norm": 8998.356294346208,
      "learning_rate": 1.822476475876062e-07,
      "loss": 1.7045,
      "step": 301376
    },
    {
      "epoch": 0.0010938694505309328,
      "grad_norm": 9215.554676740841,
      "learning_rate": 1.8223796322240046e-07,
      "loss": 1.7092,
      "step": 301408
    },
    {
      "epoch": 0.0010939855848817695,
      "grad_norm": 10677.574069047707,
      "learning_rate": 1.822282804008683e-07,
      "loss": 1.7008,
      "step": 301440
    },
    {
      "epoch": 0.0010941017192326063,
      "grad_norm": 9107.034424004336,
      "learning_rate": 1.8221859912259973e-07,
      "loss": 1.6715,
      "step": 301472
    },
    {
      "epoch": 0.001094217853583443,
      "grad_norm": 9098.3462233529,
      "learning_rate": 1.8220891938718486e-07,
      "loss": 1.6882,
      "step": 301504
    },
    {
      "epoch": 0.0010943339879342796,
      "grad_norm": 9296.659400021063,
      "learning_rate": 1.8219924119421394e-07,
      "loss": 1.691,
      "step": 301536
    },
    {
      "epoch": 0.0010944501222851163,
      "grad_norm": 9520.207560762528,
      "learning_rate": 1.8218956454327732e-07,
      "loss": 1.7072,
      "step": 301568
    },
    {
      "epoch": 0.001094566256635953,
      "grad_norm": 8486.847117746378,
      "learning_rate": 1.8217988943396558e-07,
      "loss": 1.7019,
      "step": 301600
    },
    {
      "epoch": 0.0010946823909867899,
      "grad_norm": 8422.610165500955,
      "learning_rate": 1.8217021586586946e-07,
      "loss": 1.6995,
      "step": 301632
    },
    {
      "epoch": 0.0010947985253376266,
      "grad_norm": 8069.109368449531,
      "learning_rate": 1.821605438385798e-07,
      "loss": 1.7354,
      "step": 301664
    },
    {
      "epoch": 0.0010949146596884631,
      "grad_norm": 10166.274342157012,
      "learning_rate": 1.8215087335168757e-07,
      "loss": 1.7386,
      "step": 301696
    },
    {
      "epoch": 0.0010950307940393,
      "grad_norm": 11655.14581633366,
      "learning_rate": 1.8214120440478396e-07,
      "loss": 1.7475,
      "step": 301728
    },
    {
      "epoch": 0.0010951469283901367,
      "grad_norm": 11367.144760228928,
      "learning_rate": 1.8213153699746032e-07,
      "loss": 1.748,
      "step": 301760
    },
    {
      "epoch": 0.0010952630627409734,
      "grad_norm": 9992.091973155571,
      "learning_rate": 1.8212187112930808e-07,
      "loss": 1.6895,
      "step": 301792
    },
    {
      "epoch": 0.00109537919709181,
      "grad_norm": 9284.463366291022,
      "learning_rate": 1.821125087869245e-07,
      "loss": 1.6924,
      "step": 301824
    },
    {
      "epoch": 0.0010954953314426467,
      "grad_norm": 8876.498859347643,
      "learning_rate": 1.8210284594782266e-07,
      "loss": 1.6916,
      "step": 301856
    },
    {
      "epoch": 0.0010956114657934835,
      "grad_norm": 8559.140844734358,
      "learning_rate": 1.820931846466803e-07,
      "loss": 1.6946,
      "step": 301888
    },
    {
      "epoch": 0.0010957276001443202,
      "grad_norm": 10828.173991952659,
      "learning_rate": 1.8208352488308944e-07,
      "loss": 1.6872,
      "step": 301920
    },
    {
      "epoch": 0.001095843734495157,
      "grad_norm": 10203.11364241328,
      "learning_rate": 1.8207386665664237e-07,
      "loss": 1.6912,
      "step": 301952
    },
    {
      "epoch": 0.0010959598688459935,
      "grad_norm": 10387.808431040688,
      "learning_rate": 1.820642099669314e-07,
      "loss": 1.7089,
      "step": 301984
    },
    {
      "epoch": 0.0010960760031968303,
      "grad_norm": 8528.80835756086,
      "learning_rate": 1.820545548135491e-07,
      "loss": 1.7047,
      "step": 302016
    },
    {
      "epoch": 0.001096192137547667,
      "grad_norm": 10010.921436111663,
      "learning_rate": 1.8204490119608807e-07,
      "loss": 1.7042,
      "step": 302048
    },
    {
      "epoch": 0.0010963082718985038,
      "grad_norm": 9550.707827171764,
      "learning_rate": 1.8203524911414124e-07,
      "loss": 1.6837,
      "step": 302080
    },
    {
      "epoch": 0.0010964244062493403,
      "grad_norm": 9031.771697734614,
      "learning_rate": 1.820255985673015e-07,
      "loss": 1.6747,
      "step": 302112
    },
    {
      "epoch": 0.001096540540600177,
      "grad_norm": 10238.985887283956,
      "learning_rate": 1.82015949555162e-07,
      "loss": 1.6845,
      "step": 302144
    },
    {
      "epoch": 0.0010966566749510138,
      "grad_norm": 10802.420654649586,
      "learning_rate": 1.8200630207731603e-07,
      "loss": 1.6956,
      "step": 302176
    },
    {
      "epoch": 0.0010967728093018506,
      "grad_norm": 8893.371913959294,
      "learning_rate": 1.8199665613335703e-07,
      "loss": 1.7082,
      "step": 302208
    },
    {
      "epoch": 0.0010968889436526873,
      "grad_norm": 10405.10663088082,
      "learning_rate": 1.8198701172287852e-07,
      "loss": 1.6831,
      "step": 302240
    },
    {
      "epoch": 0.0010970050780035239,
      "grad_norm": 9025.870595128206,
      "learning_rate": 1.8197736884547433e-07,
      "loss": 1.7126,
      "step": 302272
    },
    {
      "epoch": 0.0010971212123543606,
      "grad_norm": 9191.974107883463,
      "learning_rate": 1.8196772750073826e-07,
      "loss": 1.7181,
      "step": 302304
    },
    {
      "epoch": 0.0010972373467051974,
      "grad_norm": 8777.857483463717,
      "learning_rate": 1.8195808768826437e-07,
      "loss": 1.7002,
      "step": 302336
    },
    {
      "epoch": 0.0010973534810560341,
      "grad_norm": 9153.782169136428,
      "learning_rate": 1.819484494076468e-07,
      "loss": 1.6945,
      "step": 302368
    },
    {
      "epoch": 0.0010974696154068707,
      "grad_norm": 9351.730535040026,
      "learning_rate": 1.8193881265847998e-07,
      "loss": 1.6997,
      "step": 302400
    },
    {
      "epoch": 0.0010975857497577074,
      "grad_norm": 8815.157287309172,
      "learning_rate": 1.8192917744035826e-07,
      "loss": 1.6876,
      "step": 302432
    },
    {
      "epoch": 0.0010977018841085442,
      "grad_norm": 9323.577639511563,
      "learning_rate": 1.8191954375287638e-07,
      "loss": 1.7017,
      "step": 302464
    },
    {
      "epoch": 0.001097818018459381,
      "grad_norm": 10225.134717938929,
      "learning_rate": 1.8190991159562914e-07,
      "loss": 1.7177,
      "step": 302496
    },
    {
      "epoch": 0.0010979341528102177,
      "grad_norm": 9449.737985785638,
      "learning_rate": 1.8190028096821135e-07,
      "loss": 1.7236,
      "step": 302528
    },
    {
      "epoch": 0.0010980502871610542,
      "grad_norm": 8892.421717395098,
      "learning_rate": 1.8189065187021817e-07,
      "loss": 1.7077,
      "step": 302560
    },
    {
      "epoch": 0.001098166421511891,
      "grad_norm": 10200.422540267633,
      "learning_rate": 1.8188102430124488e-07,
      "loss": 1.7287,
      "step": 302592
    },
    {
      "epoch": 0.0010982825558627277,
      "grad_norm": 9232.33469930548,
      "learning_rate": 1.8187139826088677e-07,
      "loss": 1.7329,
      "step": 302624
    },
    {
      "epoch": 0.0010983986902135645,
      "grad_norm": 10608.488865055193,
      "learning_rate": 1.8186177374873945e-07,
      "loss": 1.7029,
      "step": 302656
    },
    {
      "epoch": 0.001098514824564401,
      "grad_norm": 8910.272274178831,
      "learning_rate": 1.8185215076439855e-07,
      "loss": 1.6988,
      "step": 302688
    },
    {
      "epoch": 0.0010986309589152378,
      "grad_norm": 11198.246291272577,
      "learning_rate": 1.818425293074599e-07,
      "loss": 1.7078,
      "step": 302720
    },
    {
      "epoch": 0.0010987470932660745,
      "grad_norm": 9363.676628333553,
      "learning_rate": 1.8183290937751954e-07,
      "loss": 1.6859,
      "step": 302752
    },
    {
      "epoch": 0.0010988632276169113,
      "grad_norm": 10384.259049157046,
      "learning_rate": 1.8182329097417354e-07,
      "loss": 1.695,
      "step": 302784
    },
    {
      "epoch": 0.001098979361967748,
      "grad_norm": 9414.595689672498,
      "learning_rate": 1.8181367409701825e-07,
      "loss": 1.704,
      "step": 302816
    },
    {
      "epoch": 0.0010990954963185846,
      "grad_norm": 10672.25618133298,
      "learning_rate": 1.818043592022889e-07,
      "loss": 1.7218,
      "step": 302848
    },
    {
      "epoch": 0.0010992116306694213,
      "grad_norm": 9186.229476776638,
      "learning_rate": 1.8179474532864221e-07,
      "loss": 1.7117,
      "step": 302880
    },
    {
      "epoch": 0.001099327765020258,
      "grad_norm": 10191.900509718489,
      "learning_rate": 1.817851329799886e-07,
      "loss": 1.7014,
      "step": 302912
    },
    {
      "epoch": 0.0010994438993710948,
      "grad_norm": 9435.191254023419,
      "learning_rate": 1.8177552215592483e-07,
      "loss": 1.6898,
      "step": 302944
    },
    {
      "epoch": 0.0010995600337219314,
      "grad_norm": 11254.395941142287,
      "learning_rate": 1.8176591285604796e-07,
      "loss": 1.6979,
      "step": 302976
    },
    {
      "epoch": 0.0010996761680727681,
      "grad_norm": 8387.443591464566,
      "learning_rate": 1.8175630507995522e-07,
      "loss": 1.7072,
      "step": 303008
    },
    {
      "epoch": 0.0010997923024236049,
      "grad_norm": 8524.551131877854,
      "learning_rate": 1.8174669882724386e-07,
      "loss": 1.6982,
      "step": 303040
    },
    {
      "epoch": 0.0010999084367744416,
      "grad_norm": 9928.989676699237,
      "learning_rate": 1.8173709409751136e-07,
      "loss": 1.7059,
      "step": 303072
    },
    {
      "epoch": 0.0011000245711252784,
      "grad_norm": 11000.776336241002,
      "learning_rate": 1.8172749089035537e-07,
      "loss": 1.6779,
      "step": 303104
    },
    {
      "epoch": 0.001100140705476115,
      "grad_norm": 10388.571990413311,
      "learning_rate": 1.8171788920537363e-07,
      "loss": 1.6898,
      "step": 303136
    },
    {
      "epoch": 0.0011002568398269517,
      "grad_norm": 9407.608410217763,
      "learning_rate": 1.817082890421641e-07,
      "loss": 1.7056,
      "step": 303168
    },
    {
      "epoch": 0.0011003729741777884,
      "grad_norm": 8127.694014909764,
      "learning_rate": 1.8169869040032482e-07,
      "loss": 1.7113,
      "step": 303200
    },
    {
      "epoch": 0.0011004891085286252,
      "grad_norm": 10421.748605680335,
      "learning_rate": 1.81689093279454e-07,
      "loss": 1.6947,
      "step": 303232
    },
    {
      "epoch": 0.0011006052428794617,
      "grad_norm": 11060.164555737858,
      "learning_rate": 1.8167949767914998e-07,
      "loss": 1.7123,
      "step": 303264
    },
    {
      "epoch": 0.0011007213772302985,
      "grad_norm": 9498.988893561253,
      "learning_rate": 1.8166990359901133e-07,
      "loss": 1.7185,
      "step": 303296
    },
    {
      "epoch": 0.0011008375115811352,
      "grad_norm": 8503.544319870392,
      "learning_rate": 1.8166031103863666e-07,
      "loss": 1.7335,
      "step": 303328
    },
    {
      "epoch": 0.001100953645931972,
      "grad_norm": 9572.144587290772,
      "learning_rate": 1.8165071999762482e-07,
      "loss": 1.744,
      "step": 303360
    },
    {
      "epoch": 0.0011010697802828087,
      "grad_norm": 9350.275717859875,
      "learning_rate": 1.816411304755747e-07,
      "loss": 1.7498,
      "step": 303392
    },
    {
      "epoch": 0.0011011859146336453,
      "grad_norm": 11145.81751151525,
      "learning_rate": 1.8163154247208548e-07,
      "loss": 1.7264,
      "step": 303424
    },
    {
      "epoch": 0.001101302048984482,
      "grad_norm": 7796.230114613088,
      "learning_rate": 1.8162195598675636e-07,
      "loss": 1.7134,
      "step": 303456
    },
    {
      "epoch": 0.0011014181833353188,
      "grad_norm": 11129.480491020235,
      "learning_rate": 1.8161237101918677e-07,
      "loss": 1.7238,
      "step": 303488
    },
    {
      "epoch": 0.0011015343176861555,
      "grad_norm": 8961.246899846024,
      "learning_rate": 1.816027875689763e-07,
      "loss": 1.7284,
      "step": 303520
    },
    {
      "epoch": 0.001101650452036992,
      "grad_norm": 9909.685565142821,
      "learning_rate": 1.8159320563572454e-07,
      "loss": 1.7162,
      "step": 303552
    },
    {
      "epoch": 0.0011017665863878288,
      "grad_norm": 9978.880899179027,
      "learning_rate": 1.815836252190314e-07,
      "loss": 1.6885,
      "step": 303584
    },
    {
      "epoch": 0.0011018827207386656,
      "grad_norm": 11523.697844008233,
      "learning_rate": 1.8157404631849687e-07,
      "loss": 1.7044,
      "step": 303616
    },
    {
      "epoch": 0.0011019988550895023,
      "grad_norm": 9879.484804381249,
      "learning_rate": 1.8156446893372108e-07,
      "loss": 1.714,
      "step": 303648
    },
    {
      "epoch": 0.001102114989440339,
      "grad_norm": 9691.220356590804,
      "learning_rate": 1.8155489306430433e-07,
      "loss": 1.708,
      "step": 303680
    },
    {
      "epoch": 0.0011022311237911756,
      "grad_norm": 8603.758016122954,
      "learning_rate": 1.8154531870984706e-07,
      "loss": 1.717,
      "step": 303712
    },
    {
      "epoch": 0.0011023472581420124,
      "grad_norm": 10834.038397569024,
      "learning_rate": 1.8153574586994984e-07,
      "loss": 1.7322,
      "step": 303744
    },
    {
      "epoch": 0.0011024633924928491,
      "grad_norm": 11221.583488973383,
      "learning_rate": 1.8152617454421338e-07,
      "loss": 1.6859,
      "step": 303776
    },
    {
      "epoch": 0.0011025795268436859,
      "grad_norm": 9239.660166910902,
      "learning_rate": 1.8151660473223858e-07,
      "loss": 1.6821,
      "step": 303808
    },
    {
      "epoch": 0.0011026956611945224,
      "grad_norm": 19276.092135077586,
      "learning_rate": 1.8150703643362648e-07,
      "loss": 1.6924,
      "step": 303840
    },
    {
      "epoch": 0.0011028117955453592,
      "grad_norm": 9241.613495488762,
      "learning_rate": 1.8149776858713236e-07,
      "loss": 1.7055,
      "step": 303872
    },
    {
      "epoch": 0.001102927929896196,
      "grad_norm": 11231.214181912836,
      "learning_rate": 1.8148820326678768e-07,
      "loss": 1.6966,
      "step": 303904
    },
    {
      "epoch": 0.0011030440642470327,
      "grad_norm": 9015.139710509206,
      "learning_rate": 1.8147863945862204e-07,
      "loss": 1.6945,
      "step": 303936
    },
    {
      "epoch": 0.0011031601985978694,
      "grad_norm": 9704.018136833834,
      "learning_rate": 1.8146907716223715e-07,
      "loss": 1.6931,
      "step": 303968
    },
    {
      "epoch": 0.001103276332948706,
      "grad_norm": 9399.200497914702,
      "learning_rate": 1.814595163772347e-07,
      "loss": 1.6914,
      "step": 304000
    },
    {
      "epoch": 0.0011033924672995427,
      "grad_norm": 8610.514502629909,
      "learning_rate": 1.814499571032166e-07,
      "loss": 1.7046,
      "step": 304032
    },
    {
      "epoch": 0.0011035086016503795,
      "grad_norm": 9386.208712787075,
      "learning_rate": 1.814403993397849e-07,
      "loss": 1.7017,
      "step": 304064
    },
    {
      "epoch": 0.0011036247360012162,
      "grad_norm": 8598.331931252713,
      "learning_rate": 1.8143084308654182e-07,
      "loss": 1.7047,
      "step": 304096
    },
    {
      "epoch": 0.0011037408703520528,
      "grad_norm": 9171.226090332742,
      "learning_rate": 1.8142128834308967e-07,
      "loss": 1.6714,
      "step": 304128
    },
    {
      "epoch": 0.0011038570047028895,
      "grad_norm": 7234.521684258055,
      "learning_rate": 1.8141173510903098e-07,
      "loss": 1.6859,
      "step": 304160
    },
    {
      "epoch": 0.0011039731390537263,
      "grad_norm": 10070.222837653593,
      "learning_rate": 1.8140218338396837e-07,
      "loss": 1.7046,
      "step": 304192
    },
    {
      "epoch": 0.001104089273404563,
      "grad_norm": 11055.267341860168,
      "learning_rate": 1.8139263316750458e-07,
      "loss": 1.7172,
      "step": 304224
    },
    {
      "epoch": 0.0011042054077553998,
      "grad_norm": 9366.214176496285,
      "learning_rate": 1.813830844592426e-07,
      "loss": 1.6958,
      "step": 304256
    },
    {
      "epoch": 0.0011043215421062363,
      "grad_norm": 8268.715861607532,
      "learning_rate": 1.813735372587855e-07,
      "loss": 1.7172,
      "step": 304288
    },
    {
      "epoch": 0.001104437676457073,
      "grad_norm": 8702.85079729625,
      "learning_rate": 1.8136399156573638e-07,
      "loss": 1.7145,
      "step": 304320
    },
    {
      "epoch": 0.0011045538108079098,
      "grad_norm": 11348.556912665152,
      "learning_rate": 1.8135444737969878e-07,
      "loss": 1.7262,
      "step": 304352
    },
    {
      "epoch": 0.0011046699451587466,
      "grad_norm": 9591.235374027685,
      "learning_rate": 1.8134490470027613e-07,
      "loss": 1.7285,
      "step": 304384
    },
    {
      "epoch": 0.0011047860795095831,
      "grad_norm": 9479.630372540903,
      "learning_rate": 1.8133536352707212e-07,
      "loss": 1.711,
      "step": 304416
    },
    {
      "epoch": 0.0011049022138604199,
      "grad_norm": 10213.954180433746,
      "learning_rate": 1.8132582385969052e-07,
      "loss": 1.6984,
      "step": 304448
    },
    {
      "epoch": 0.0011050183482112566,
      "grad_norm": 9634.002698774793,
      "learning_rate": 1.8131628569773527e-07,
      "loss": 1.6984,
      "step": 304480
    },
    {
      "epoch": 0.0011051344825620934,
      "grad_norm": 8617.428734837324,
      "learning_rate": 1.813067490408105e-07,
      "loss": 1.682,
      "step": 304512
    },
    {
      "epoch": 0.0011052506169129301,
      "grad_norm": 10910.704285242085,
      "learning_rate": 1.8129721388852045e-07,
      "loss": 1.6981,
      "step": 304544
    },
    {
      "epoch": 0.0011053667512637667,
      "grad_norm": 9776.829956586133,
      "learning_rate": 1.812876802404695e-07,
      "loss": 1.6881,
      "step": 304576
    },
    {
      "epoch": 0.0011054828856146034,
      "grad_norm": 10347.863547612134,
      "learning_rate": 1.812781480962622e-07,
      "loss": 1.6877,
      "step": 304608
    },
    {
      "epoch": 0.0011055990199654402,
      "grad_norm": 14245.094594280517,
      "learning_rate": 1.812686174555032e-07,
      "loss": 1.7122,
      "step": 304640
    },
    {
      "epoch": 0.001105715154316277,
      "grad_norm": 9232.835750732274,
      "learning_rate": 1.8125908831779734e-07,
      "loss": 1.706,
      "step": 304672
    },
    {
      "epoch": 0.0011058312886671135,
      "grad_norm": 9541.568843748915,
      "learning_rate": 1.8124956068274961e-07,
      "loss": 1.6945,
      "step": 304704
    },
    {
      "epoch": 0.0011059474230179502,
      "grad_norm": 9039.289795111119,
      "learning_rate": 1.812400345499651e-07,
      "loss": 1.6925,
      "step": 304736
    },
    {
      "epoch": 0.001106063557368787,
      "grad_norm": 9359.835468639392,
      "learning_rate": 1.8123050991904905e-07,
      "loss": 1.6975,
      "step": 304768
    },
    {
      "epoch": 0.0011061796917196237,
      "grad_norm": 9173.488540353665,
      "learning_rate": 1.812209867896069e-07,
      "loss": 1.6725,
      "step": 304800
    },
    {
      "epoch": 0.0011062958260704605,
      "grad_norm": 10084.866880628619,
      "learning_rate": 1.8121146516124422e-07,
      "loss": 1.6823,
      "step": 304832
    },
    {
      "epoch": 0.001106411960421297,
      "grad_norm": 21098.40448943948,
      "learning_rate": 1.8120194503356667e-07,
      "loss": 1.684,
      "step": 304864
    },
    {
      "epoch": 0.0011065280947721338,
      "grad_norm": 12692.9375638581,
      "learning_rate": 1.8119272384058036e-07,
      "loss": 1.7044,
      "step": 304896
    },
    {
      "epoch": 0.0011066442291229705,
      "grad_norm": 9319.110901797445,
      "learning_rate": 1.8118320666622494e-07,
      "loss": 1.6932,
      "step": 304928
    },
    {
      "epoch": 0.0011067603634738073,
      "grad_norm": 9608.541824855633,
      "learning_rate": 1.8117369099138493e-07,
      "loss": 1.6879,
      "step": 304960
    },
    {
      "epoch": 0.0011068764978246438,
      "grad_norm": 10283.324365204086,
      "learning_rate": 1.811641768156666e-07,
      "loss": 1.7106,
      "step": 304992
    },
    {
      "epoch": 0.0011069926321754806,
      "grad_norm": 9005.9624693866,
      "learning_rate": 1.8115466413867638e-07,
      "loss": 1.7191,
      "step": 305024
    },
    {
      "epoch": 0.0011071087665263173,
      "grad_norm": 11210.330949619642,
      "learning_rate": 1.811451529600208e-07,
      "loss": 1.699,
      "step": 305056
    },
    {
      "epoch": 0.001107224900877154,
      "grad_norm": 9540.650082672564,
      "learning_rate": 1.811356432793066e-07,
      "loss": 1.7056,
      "step": 305088
    },
    {
      "epoch": 0.0011073410352279908,
      "grad_norm": 10195.528431621384,
      "learning_rate": 1.8112613509614056e-07,
      "loss": 1.7066,
      "step": 305120
    },
    {
      "epoch": 0.0011074571695788274,
      "grad_norm": 8499.78611495607,
      "learning_rate": 1.8111662841012977e-07,
      "loss": 1.6971,
      "step": 305152
    },
    {
      "epoch": 0.0011075733039296641,
      "grad_norm": 7582.849200663297,
      "learning_rate": 1.811071232208813e-07,
      "loss": 1.7125,
      "step": 305184
    },
    {
      "epoch": 0.001107689438280501,
      "grad_norm": 8878.179092584245,
      "learning_rate": 1.8109761952800248e-07,
      "loss": 1.7165,
      "step": 305216
    },
    {
      "epoch": 0.0011078055726313376,
      "grad_norm": 9792.196178590379,
      "learning_rate": 1.810881173311007e-07,
      "loss": 1.7289,
      "step": 305248
    },
    {
      "epoch": 0.0011079217069821742,
      "grad_norm": 9117.843166012453,
      "learning_rate": 1.8107861662978351e-07,
      "loss": 1.6925,
      "step": 305280
    },
    {
      "epoch": 0.001108037841333011,
      "grad_norm": 8422.61491462123,
      "learning_rate": 1.8106911742365871e-07,
      "loss": 1.6845,
      "step": 305312
    },
    {
      "epoch": 0.0011081539756838477,
      "grad_norm": 9204.248366922744,
      "learning_rate": 1.810596197123341e-07,
      "loss": 1.6914,
      "step": 305344
    },
    {
      "epoch": 0.0011082701100346844,
      "grad_norm": 9168.677440067351,
      "learning_rate": 1.8105012349541768e-07,
      "loss": 1.696,
      "step": 305376
    },
    {
      "epoch": 0.0011083862443855212,
      "grad_norm": 8874.775715475856,
      "learning_rate": 1.8104062877251762e-07,
      "loss": 1.7006,
      "step": 305408
    },
    {
      "epoch": 0.0011085023787363577,
      "grad_norm": 8076.710221371075,
      "learning_rate": 1.810311355432422e-07,
      "loss": 1.6964,
      "step": 305440
    },
    {
      "epoch": 0.0011086185130871945,
      "grad_norm": 10532.35719105652,
      "learning_rate": 1.8102164380719984e-07,
      "loss": 1.6979,
      "step": 305472
    },
    {
      "epoch": 0.0011087346474380312,
      "grad_norm": 12454.518216294036,
      "learning_rate": 1.8101215356399914e-07,
      "loss": 1.7065,
      "step": 305504
    },
    {
      "epoch": 0.001108850781788868,
      "grad_norm": 9154.881539375592,
      "learning_rate": 1.8100266481324882e-07,
      "loss": 1.7204,
      "step": 305536
    },
    {
      "epoch": 0.0011089669161397045,
      "grad_norm": 9056.730425490206,
      "learning_rate": 1.8099317755455776e-07,
      "loss": 1.7162,
      "step": 305568
    },
    {
      "epoch": 0.0011090830504905413,
      "grad_norm": 9710.84054034459,
      "learning_rate": 1.809836917875349e-07,
      "loss": 1.6902,
      "step": 305600
    },
    {
      "epoch": 0.001109199184841378,
      "grad_norm": 13433.943129252855,
      "learning_rate": 1.8097420751178954e-07,
      "loss": 1.6798,
      "step": 305632
    },
    {
      "epoch": 0.0011093153191922148,
      "grad_norm": 9082.763015734805,
      "learning_rate": 1.8096472472693083e-07,
      "loss": 1.6894,
      "step": 305664
    },
    {
      "epoch": 0.0011094314535430516,
      "grad_norm": 9210.146795789957,
      "learning_rate": 1.8095524343256827e-07,
      "loss": 1.7086,
      "step": 305696
    },
    {
      "epoch": 0.001109547587893888,
      "grad_norm": 9458.554329283095,
      "learning_rate": 1.809457636283114e-07,
      "loss": 1.7116,
      "step": 305728
    },
    {
      "epoch": 0.0011096637222447248,
      "grad_norm": 9556.868524783627,
      "learning_rate": 1.8093628531377002e-07,
      "loss": 1.7029,
      "step": 305760
    },
    {
      "epoch": 0.0011097798565955616,
      "grad_norm": 8857.756375064737,
      "learning_rate": 1.8092680848855397e-07,
      "loss": 1.7021,
      "step": 305792
    },
    {
      "epoch": 0.0011098959909463984,
      "grad_norm": 11164.306158467707,
      "learning_rate": 1.809173331522732e-07,
      "loss": 1.683,
      "step": 305824
    },
    {
      "epoch": 0.001110012125297235,
      "grad_norm": 11202.60987448907,
      "learning_rate": 1.8090785930453792e-07,
      "loss": 1.6849,
      "step": 305856
    },
    {
      "epoch": 0.0011101282596480716,
      "grad_norm": 19858.223082642617,
      "learning_rate": 1.8089838694495849e-07,
      "loss": 1.6899,
      "step": 305888
    },
    {
      "epoch": 0.0011102443939989084,
      "grad_norm": 20072.87642566456,
      "learning_rate": 1.8088891607314522e-07,
      "loss": 1.7024,
      "step": 305920
    },
    {
      "epoch": 0.0011103605283497452,
      "grad_norm": 23058.5373343584,
      "learning_rate": 1.808794466887088e-07,
      "loss": 1.6944,
      "step": 305952
    },
    {
      "epoch": 0.001110476662700582,
      "grad_norm": 9267.600120851137,
      "learning_rate": 1.8087027464055092e-07,
      "loss": 1.7059,
      "step": 305984
    },
    {
      "epoch": 0.0011105927970514184,
      "grad_norm": 8383.18567133044,
      "learning_rate": 1.808608081832501e-07,
      "loss": 1.7196,
      "step": 306016
    },
    {
      "epoch": 0.0011107089314022552,
      "grad_norm": 8449.983550279847,
      "learning_rate": 1.8085134321217086e-07,
      "loss": 1.7399,
      "step": 306048
    },
    {
      "epoch": 0.001110825065753092,
      "grad_norm": 8635.53078855029,
      "learning_rate": 1.8084187972692432e-07,
      "loss": 1.7419,
      "step": 306080
    },
    {
      "epoch": 0.0011109412001039287,
      "grad_norm": 8891.044932964855,
      "learning_rate": 1.8083241772712182e-07,
      "loss": 1.7308,
      "step": 306112
    },
    {
      "epoch": 0.0011110573344547652,
      "grad_norm": 8675.564419678987,
      "learning_rate": 1.8082295721237472e-07,
      "loss": 1.7186,
      "step": 306144
    },
    {
      "epoch": 0.001111173468805602,
      "grad_norm": 9925.466235900458,
      "learning_rate": 1.8081349818229467e-07,
      "loss": 1.6768,
      "step": 306176
    },
    {
      "epoch": 0.0011112896031564388,
      "grad_norm": 8704.250455955413,
      "learning_rate": 1.8080404063649335e-07,
      "loss": 1.6799,
      "step": 306208
    },
    {
      "epoch": 0.0011114057375072755,
      "grad_norm": 9277.902995828314,
      "learning_rate": 1.8079458457458263e-07,
      "loss": 1.7007,
      "step": 306240
    },
    {
      "epoch": 0.0011115218718581123,
      "grad_norm": 9555.22160915172,
      "learning_rate": 1.807851299961745e-07,
      "loss": 1.7085,
      "step": 306272
    },
    {
      "epoch": 0.0011116380062089488,
      "grad_norm": 10905.343644287419,
      "learning_rate": 1.8077567690088113e-07,
      "loss": 1.6733,
      "step": 306304
    },
    {
      "epoch": 0.0011117541405597856,
      "grad_norm": 11197.166427270786,
      "learning_rate": 1.8076622528831478e-07,
      "loss": 1.6859,
      "step": 306336
    },
    {
      "epoch": 0.0011118702749106223,
      "grad_norm": 9940.924705478861,
      "learning_rate": 1.8075677515808789e-07,
      "loss": 1.6957,
      "step": 306368
    },
    {
      "epoch": 0.001111986409261459,
      "grad_norm": 9797.76443889115,
      "learning_rate": 1.8074732650981305e-07,
      "loss": 1.7118,
      "step": 306400
    },
    {
      "epoch": 0.0011121025436122956,
      "grad_norm": 10414.417122431769,
      "learning_rate": 1.8073787934310295e-07,
      "loss": 1.7093,
      "step": 306432
    },
    {
      "epoch": 0.0011122186779631324,
      "grad_norm": 9653.625018613475,
      "learning_rate": 1.807284336575704e-07,
      "loss": 1.6939,
      "step": 306464
    },
    {
      "epoch": 0.001112334812313969,
      "grad_norm": 10231.210290087874,
      "learning_rate": 1.807189894528285e-07,
      "loss": 1.6981,
      "step": 306496
    },
    {
      "epoch": 0.0011124509466648059,
      "grad_norm": 9883.560289693183,
      "learning_rate": 1.8070954672849032e-07,
      "loss": 1.6815,
      "step": 306528
    },
    {
      "epoch": 0.0011125670810156426,
      "grad_norm": 9743.2663927453,
      "learning_rate": 1.8070010548416915e-07,
      "loss": 1.6931,
      "step": 306560
    },
    {
      "epoch": 0.0011126832153664792,
      "grad_norm": 9796.863171444214,
      "learning_rate": 1.8069066571947838e-07,
      "loss": 1.7214,
      "step": 306592
    },
    {
      "epoch": 0.001112799349717316,
      "grad_norm": 11359.376919532162,
      "learning_rate": 1.806812274340316e-07,
      "loss": 1.7229,
      "step": 306624
    },
    {
      "epoch": 0.0011129154840681527,
      "grad_norm": 9131.955321835516,
      "learning_rate": 1.8067179062744254e-07,
      "loss": 1.6994,
      "step": 306656
    },
    {
      "epoch": 0.0011130316184189894,
      "grad_norm": 8426.471622215315,
      "learning_rate": 1.8066235529932498e-07,
      "loss": 1.7025,
      "step": 306688
    },
    {
      "epoch": 0.001113147752769826,
      "grad_norm": 10372.487647618578,
      "learning_rate": 1.8065292144929298e-07,
      "loss": 1.7116,
      "step": 306720
    },
    {
      "epoch": 0.0011132638871206627,
      "grad_norm": 11795.578493656001,
      "learning_rate": 1.8064348907696063e-07,
      "loss": 1.7136,
      "step": 306752
    },
    {
      "epoch": 0.0011133800214714995,
      "grad_norm": 12091.85097493349,
      "learning_rate": 1.8063405818194215e-07,
      "loss": 1.7029,
      "step": 306784
    },
    {
      "epoch": 0.0011134961558223362,
      "grad_norm": 10513.420756347574,
      "learning_rate": 1.8062462876385203e-07,
      "loss": 1.7119,
      "step": 306816
    },
    {
      "epoch": 0.001113612290173173,
      "grad_norm": 9956.642807693766,
      "learning_rate": 1.806152008223048e-07,
      "loss": 1.687,
      "step": 306848
    },
    {
      "epoch": 0.0011137284245240095,
      "grad_norm": 12067.986410333748,
      "learning_rate": 1.8060577435691506e-07,
      "loss": 1.6969,
      "step": 306880
    },
    {
      "epoch": 0.0011138445588748463,
      "grad_norm": 8394.545610097071,
      "learning_rate": 1.8059634936729774e-07,
      "loss": 1.707,
      "step": 306912
    },
    {
      "epoch": 0.001113960693225683,
      "grad_norm": 9803.780291295801,
      "learning_rate": 1.805869258530678e-07,
      "loss": 1.722,
      "step": 306944
    },
    {
      "epoch": 0.0011140768275765198,
      "grad_norm": 17872.893889910498,
      "learning_rate": 1.8057750381384033e-07,
      "loss": 1.7098,
      "step": 306976
    },
    {
      "epoch": 0.0011141929619273563,
      "grad_norm": 11166.998522432068,
      "learning_rate": 1.805683776195576e-07,
      "loss": 1.7079,
      "step": 307008
    },
    {
      "epoch": 0.001114309096278193,
      "grad_norm": 8923.549966241015,
      "learning_rate": 1.8055895848311697e-07,
      "loss": 1.7021,
      "step": 307040
    },
    {
      "epoch": 0.0011144252306290298,
      "grad_norm": 10558.435300744139,
      "learning_rate": 1.8054954082053703e-07,
      "loss": 1.7063,
      "step": 307072
    },
    {
      "epoch": 0.0011145413649798666,
      "grad_norm": 10208.865167098642,
      "learning_rate": 1.8054012463143344e-07,
      "loss": 1.7077,
      "step": 307104
    },
    {
      "epoch": 0.0011146574993307033,
      "grad_norm": 10641.22060667854,
      "learning_rate": 1.8053070991542198e-07,
      "loss": 1.7181,
      "step": 307136
    },
    {
      "epoch": 0.0011147736336815399,
      "grad_norm": 10946.157682036195,
      "learning_rate": 1.8052129667211863e-07,
      "loss": 1.7208,
      "step": 307168
    },
    {
      "epoch": 0.0011148897680323766,
      "grad_norm": 10141.864325655319,
      "learning_rate": 1.8051188490113945e-07,
      "loss": 1.6787,
      "step": 307200
    },
    {
      "epoch": 0.0011150059023832134,
      "grad_norm": 9402.676959249424,
      "learning_rate": 1.8050247460210067e-07,
      "loss": 1.6796,
      "step": 307232
    },
    {
      "epoch": 0.0011151220367340501,
      "grad_norm": 9130.515867134782,
      "learning_rate": 1.804930657746187e-07,
      "loss": 1.6976,
      "step": 307264
    },
    {
      "epoch": 0.0011152381710848867,
      "grad_norm": 10438.205401313005,
      "learning_rate": 1.8048365841831002e-07,
      "loss": 1.7143,
      "step": 307296
    },
    {
      "epoch": 0.0011153543054357234,
      "grad_norm": 9382.407793311906,
      "learning_rate": 1.8047425253279126e-07,
      "loss": 1.6914,
      "step": 307328
    },
    {
      "epoch": 0.0011154704397865602,
      "grad_norm": 9811.594773531977,
      "learning_rate": 1.8046484811767921e-07,
      "loss": 1.6928,
      "step": 307360
    },
    {
      "epoch": 0.001115586574137397,
      "grad_norm": 10851.737372421065,
      "learning_rate": 1.8045544517259086e-07,
      "loss": 1.6913,
      "step": 307392
    },
    {
      "epoch": 0.0011157027084882337,
      "grad_norm": 9126.5938881929,
      "learning_rate": 1.8044604369714324e-07,
      "loss": 1.6992,
      "step": 307424
    },
    {
      "epoch": 0.0011158188428390702,
      "grad_norm": 9257.617404062452,
      "learning_rate": 1.8043664369095357e-07,
      "loss": 1.7041,
      "step": 307456
    },
    {
      "epoch": 0.001115934977189907,
      "grad_norm": 8376.579134706482,
      "learning_rate": 1.8042724515363917e-07,
      "loss": 1.7103,
      "step": 307488
    },
    {
      "epoch": 0.0011160511115407437,
      "grad_norm": 10337.555223552617,
      "learning_rate": 1.8041784808481754e-07,
      "loss": 1.7003,
      "step": 307520
    },
    {
      "epoch": 0.0011161672458915805,
      "grad_norm": 10969.044443341452,
      "learning_rate": 1.804084524841063e-07,
      "loss": 1.6959,
      "step": 307552
    },
    {
      "epoch": 0.001116283380242417,
      "grad_norm": 11888.767471861833,
      "learning_rate": 1.8039905835112327e-07,
      "loss": 1.6825,
      "step": 307584
    },
    {
      "epoch": 0.0011163995145932538,
      "grad_norm": 8694.290195294841,
      "learning_rate": 1.8038966568548629e-07,
      "loss": 1.7029,
      "step": 307616
    },
    {
      "epoch": 0.0011165156489440905,
      "grad_norm": 9669.280428242839,
      "learning_rate": 1.8038027448681344e-07,
      "loss": 1.7132,
      "step": 307648
    },
    {
      "epoch": 0.0011166317832949273,
      "grad_norm": 13634.040486957636,
      "learning_rate": 1.8037088475472287e-07,
      "loss": 1.6956,
      "step": 307680
    },
    {
      "epoch": 0.001116747917645764,
      "grad_norm": 12477.24488819547,
      "learning_rate": 1.8036149648883297e-07,
      "loss": 1.7157,
      "step": 307712
    },
    {
      "epoch": 0.0011168640519966006,
      "grad_norm": 10671.64373468305,
      "learning_rate": 1.8035210968876214e-07,
      "loss": 1.7131,
      "step": 307744
    },
    {
      "epoch": 0.0011169801863474373,
      "grad_norm": 10178.894438985011,
      "learning_rate": 1.8034272435412898e-07,
      "loss": 1.7158,
      "step": 307776
    },
    {
      "epoch": 0.001117096320698274,
      "grad_norm": 9545.330586208107,
      "learning_rate": 1.8033334048455226e-07,
      "loss": 1.7293,
      "step": 307808
    },
    {
      "epoch": 0.0011172124550491108,
      "grad_norm": 9231.389494545228,
      "learning_rate": 1.8032395807965088e-07,
      "loss": 1.7255,
      "step": 307840
    },
    {
      "epoch": 0.0011173285893999474,
      "grad_norm": 9432.260386566944,
      "learning_rate": 1.8031457713904383e-07,
      "loss": 1.7103,
      "step": 307872
    },
    {
      "epoch": 0.0011174447237507841,
      "grad_norm": 9269.52425963706,
      "learning_rate": 1.8030519766235028e-07,
      "loss": 1.7074,
      "step": 307904
    },
    {
      "epoch": 0.0011175608581016209,
      "grad_norm": 9479.766452819395,
      "learning_rate": 1.8029581964918944e-07,
      "loss": 1.6784,
      "step": 307936
    },
    {
      "epoch": 0.0011176769924524576,
      "grad_norm": 10015.926716984304,
      "learning_rate": 1.8028644309918088e-07,
      "loss": 1.6939,
      "step": 307968
    },
    {
      "epoch": 0.0011177931268032944,
      "grad_norm": 19298.124053907417,
      "learning_rate": 1.802770680119441e-07,
      "loss": 1.6961,
      "step": 308000
    },
    {
      "epoch": 0.001117909261154131,
      "grad_norm": 9813.497541651499,
      "learning_rate": 1.8026798729074323e-07,
      "loss": 1.6849,
      "step": 308032
    },
    {
      "epoch": 0.0011180253955049677,
      "grad_norm": 9830.858355199713,
      "learning_rate": 1.8025861508222719e-07,
      "loss": 1.6995,
      "step": 308064
    },
    {
      "epoch": 0.0011181415298558044,
      "grad_norm": 10449.04876053318,
      "learning_rate": 1.8024924433535435e-07,
      "loss": 1.705,
      "step": 308096
    },
    {
      "epoch": 0.0011182576642066412,
      "grad_norm": 9138.95530134599,
      "learning_rate": 1.8023987504974484e-07,
      "loss": 1.7184,
      "step": 308128
    },
    {
      "epoch": 0.0011183737985574777,
      "grad_norm": 9427.197144432697,
      "learning_rate": 1.80230507225019e-07,
      "loss": 1.7357,
      "step": 308160
    },
    {
      "epoch": 0.0011184899329083145,
      "grad_norm": 11337.683008445772,
      "learning_rate": 1.8022114086079706e-07,
      "loss": 1.731,
      "step": 308192
    },
    {
      "epoch": 0.0011186060672591512,
      "grad_norm": 9487.434848261146,
      "learning_rate": 1.8021177595669968e-07,
      "loss": 1.7028,
      "step": 308224
    },
    {
      "epoch": 0.001118722201609988,
      "grad_norm": 8258.351651510124,
      "learning_rate": 1.802024125123475e-07,
      "loss": 1.7157,
      "step": 308256
    },
    {
      "epoch": 0.0011188383359608247,
      "grad_norm": 8483.421361691286,
      "learning_rate": 1.8019305052736134e-07,
      "loss": 1.7106,
      "step": 308288
    },
    {
      "epoch": 0.0011189544703116613,
      "grad_norm": 8927.032877725947,
      "learning_rate": 1.8018369000136217e-07,
      "loss": 1.7111,
      "step": 308320
    },
    {
      "epoch": 0.001119070604662498,
      "grad_norm": 9464.419475065546,
      "learning_rate": 1.80174330933971e-07,
      "loss": 1.6977,
      "step": 308352
    },
    {
      "epoch": 0.0011191867390133348,
      "grad_norm": 9217.13762509815,
      "learning_rate": 1.8016497332480914e-07,
      "loss": 1.7011,
      "step": 308384
    },
    {
      "epoch": 0.0011193028733641715,
      "grad_norm": 9718.117924783584,
      "learning_rate": 1.801556171734979e-07,
      "loss": 1.7177,
      "step": 308416
    },
    {
      "epoch": 0.001119419007715008,
      "grad_norm": 9682.497198553687,
      "learning_rate": 1.8014626247965882e-07,
      "loss": 1.7152,
      "step": 308448
    },
    {
      "epoch": 0.0011195351420658448,
      "grad_norm": 10217.645129872148,
      "learning_rate": 1.8013690924291352e-07,
      "loss": 1.7098,
      "step": 308480
    },
    {
      "epoch": 0.0011196512764166816,
      "grad_norm": 9699.248424491456,
      "learning_rate": 1.8012755746288378e-07,
      "loss": 1.7091,
      "step": 308512
    },
    {
      "epoch": 0.0011197674107675183,
      "grad_norm": 8803.902770930628,
      "learning_rate": 1.801182071391915e-07,
      "loss": 1.7044,
      "step": 308544
    },
    {
      "epoch": 0.001119883545118355,
      "grad_norm": 8966.790730244573,
      "learning_rate": 1.8010885827145874e-07,
      "loss": 1.6986,
      "step": 308576
    },
    {
      "epoch": 0.0011199996794691916,
      "grad_norm": 11496.643162245231,
      "learning_rate": 1.800995108593077e-07,
      "loss": 1.7043,
      "step": 308608
    },
    {
      "epoch": 0.0011201158138200284,
      "grad_norm": 10902.490724600502,
      "learning_rate": 1.8009016490236067e-07,
      "loss": 1.7265,
      "step": 308640
    },
    {
      "epoch": 0.0011202319481708651,
      "grad_norm": 9727.355858608238,
      "learning_rate": 1.8008082040024013e-07,
      "loss": 1.7382,
      "step": 308672
    },
    {
      "epoch": 0.0011203480825217019,
      "grad_norm": 11505.838344075584,
      "learning_rate": 1.800714773525687e-07,
      "loss": 1.7001,
      "step": 308704
    },
    {
      "epoch": 0.0011204642168725384,
      "grad_norm": 9282.128635178462,
      "learning_rate": 1.8006213575896913e-07,
      "loss": 1.7175,
      "step": 308736
    },
    {
      "epoch": 0.0011205803512233752,
      "grad_norm": 10685.798238784037,
      "learning_rate": 1.8005279561906425e-07,
      "loss": 1.7378,
      "step": 308768
    },
    {
      "epoch": 0.001120696485574212,
      "grad_norm": 9431.410286908316,
      "learning_rate": 1.8004345693247708e-07,
      "loss": 1.719,
      "step": 308800
    },
    {
      "epoch": 0.0011208126199250487,
      "grad_norm": 8183.452816507223,
      "learning_rate": 1.8003411969883076e-07,
      "loss": 1.6956,
      "step": 308832
    },
    {
      "epoch": 0.0011209287542758854,
      "grad_norm": 9833.008288413062,
      "learning_rate": 1.8002478391774864e-07,
      "loss": 1.6915,
      "step": 308864
    },
    {
      "epoch": 0.001121044888626722,
      "grad_norm": 9996.820894664463,
      "learning_rate": 1.8001544958885402e-07,
      "loss": 1.677,
      "step": 308896
    },
    {
      "epoch": 0.0011211610229775587,
      "grad_norm": 9214.533520477311,
      "learning_rate": 1.8000611671177057e-07,
      "loss": 1.6802,
      "step": 308928
    },
    {
      "epoch": 0.0011212771573283955,
      "grad_norm": 10188.053788629113,
      "learning_rate": 1.7999678528612194e-07,
      "loss": 1.695,
      "step": 308960
    },
    {
      "epoch": 0.0011213932916792322,
      "grad_norm": 8723.639836673681,
      "learning_rate": 1.7998745531153196e-07,
      "loss": 1.7019,
      "step": 308992
    },
    {
      "epoch": 0.0011215094260300688,
      "grad_norm": 23805.206489337583,
      "learning_rate": 1.7997812678762458e-07,
      "loss": 1.6992,
      "step": 309024
    },
    {
      "epoch": 0.0011216255603809055,
      "grad_norm": 21459.793475241087,
      "learning_rate": 1.7996879971402395e-07,
      "loss": 1.69,
      "step": 309056
    },
    {
      "epoch": 0.0011217416947317423,
      "grad_norm": 19494.3700590709,
      "learning_rate": 1.7995947409035428e-07,
      "loss": 1.6846,
      "step": 309088
    },
    {
      "epoch": 0.001121857829082579,
      "grad_norm": 8709.347277494451,
      "learning_rate": 1.7995044127474324e-07,
      "loss": 1.7024,
      "step": 309120
    },
    {
      "epoch": 0.0011219739634334158,
      "grad_norm": 9519.43632785051,
      "learning_rate": 1.7994111850452757e-07,
      "loss": 1.7014,
      "step": 309152
    },
    {
      "epoch": 0.0011220900977842523,
      "grad_norm": 11142.88759702798,
      "learning_rate": 1.7993179718312808e-07,
      "loss": 1.6963,
      "step": 309184
    },
    {
      "epoch": 0.001122206232135089,
      "grad_norm": 9885.049924001396,
      "learning_rate": 1.7992247731016967e-07,
      "loss": 1.7045,
      "step": 309216
    },
    {
      "epoch": 0.0011223223664859258,
      "grad_norm": 10027.206988987511,
      "learning_rate": 1.7991315888527714e-07,
      "loss": 1.6798,
      "step": 309248
    },
    {
      "epoch": 0.0011224385008367626,
      "grad_norm": 9317.805320997,
      "learning_rate": 1.7990384190807562e-07,
      "loss": 1.6889,
      "step": 309280
    },
    {
      "epoch": 0.0011225546351875991,
      "grad_norm": 8769.337945363948,
      "learning_rate": 1.7989452637819027e-07,
      "loss": 1.7121,
      "step": 309312
    },
    {
      "epoch": 0.0011226707695384359,
      "grad_norm": 7991.311657043542,
      "learning_rate": 1.7988521229524636e-07,
      "loss": 1.7126,
      "step": 309344
    },
    {
      "epoch": 0.0011227869038892726,
      "grad_norm": 9941.409558005344,
      "learning_rate": 1.7987589965886944e-07,
      "loss": 1.6755,
      "step": 309376
    },
    {
      "epoch": 0.0011229030382401094,
      "grad_norm": 11233.643932402345,
      "learning_rate": 1.7986658846868508e-07,
      "loss": 1.6915,
      "step": 309408
    },
    {
      "epoch": 0.0011230191725909461,
      "grad_norm": 9470.642111282636,
      "learning_rate": 1.7985727872431897e-07,
      "loss": 1.695,
      "step": 309440
    },
    {
      "epoch": 0.0011231353069417827,
      "grad_norm": 8764.583389984946,
      "learning_rate": 1.7984797042539703e-07,
      "loss": 1.7021,
      "step": 309472
    },
    {
      "epoch": 0.0011232514412926194,
      "grad_norm": 9430.498184083384,
      "learning_rate": 1.7983866357154522e-07,
      "loss": 1.7222,
      "step": 309504
    },
    {
      "epoch": 0.0011233675756434562,
      "grad_norm": 13467.477714850691,
      "learning_rate": 1.798293581623897e-07,
      "loss": 1.7177,
      "step": 309536
    },
    {
      "epoch": 0.001123483709994293,
      "grad_norm": 11341.072436061768,
      "learning_rate": 1.7982005419755674e-07,
      "loss": 1.7124,
      "step": 309568
    },
    {
      "epoch": 0.0011235998443451295,
      "grad_norm": 11504.414630914516,
      "learning_rate": 1.7981075167667275e-07,
      "loss": 1.694,
      "step": 309600
    },
    {
      "epoch": 0.0011237159786959662,
      "grad_norm": 8913.711684814582,
      "learning_rate": 1.7980145059936425e-07,
      "loss": 1.7053,
      "step": 309632
    },
    {
      "epoch": 0.001123832113046803,
      "grad_norm": 8723.532770615355,
      "learning_rate": 1.7979215096525796e-07,
      "loss": 1.7116,
      "step": 309664
    },
    {
      "epoch": 0.0011239482473976397,
      "grad_norm": 10099.613061894996,
      "learning_rate": 1.7978285277398068e-07,
      "loss": 1.6929,
      "step": 309696
    },
    {
      "epoch": 0.0011240643817484765,
      "grad_norm": 10286.055026102087,
      "learning_rate": 1.7977355602515935e-07,
      "loss": 1.6674,
      "step": 309728
    },
    {
      "epoch": 0.001124180516099313,
      "grad_norm": 8309.50973283021,
      "learning_rate": 1.79764260718421e-07,
      "loss": 1.6959,
      "step": 309760
    },
    {
      "epoch": 0.0011242966504501498,
      "grad_norm": 12397.343747755,
      "learning_rate": 1.79754966853393e-07,
      "loss": 1.6995,
      "step": 309792
    },
    {
      "epoch": 0.0011244127848009865,
      "grad_norm": 8836.821374227273,
      "learning_rate": 1.7974567442970255e-07,
      "loss": 1.7032,
      "step": 309824
    },
    {
      "epoch": 0.0011245289191518233,
      "grad_norm": 10999.326706667094,
      "learning_rate": 1.7973638344697728e-07,
      "loss": 1.713,
      "step": 309856
    },
    {
      "epoch": 0.0011246450535026598,
      "grad_norm": 11406.775004355964,
      "learning_rate": 1.7972709390484466e-07,
      "loss": 1.7239,
      "step": 309888
    },
    {
      "epoch": 0.0011247611878534966,
      "grad_norm": 9282.703700969885,
      "learning_rate": 1.7971780580293255e-07,
      "loss": 1.6991,
      "step": 309920
    },
    {
      "epoch": 0.0011248773222043333,
      "grad_norm": 10388.508458869348,
      "learning_rate": 1.7970851914086882e-07,
      "loss": 1.6796,
      "step": 309952
    },
    {
      "epoch": 0.00112499345655517,
      "grad_norm": 9571.62514936727,
      "learning_rate": 1.7969923391828147e-07,
      "loss": 1.6802,
      "step": 309984
    },
    {
      "epoch": 0.0011251095909060069,
      "grad_norm": 10449.628892932036,
      "learning_rate": 1.7968995013479872e-07,
      "loss": 1.6939,
      "step": 310016
    },
    {
      "epoch": 0.0011252257252568434,
      "grad_norm": 11682.275805681014,
      "learning_rate": 1.7968066779004882e-07,
      "loss": 1.6955,
      "step": 310048
    },
    {
      "epoch": 0.0011253418596076801,
      "grad_norm": 11920.64377456184,
      "learning_rate": 1.796713868836602e-07,
      "loss": 1.6721,
      "step": 310080
    },
    {
      "epoch": 0.001125457993958517,
      "grad_norm": 8272.497446358022,
      "learning_rate": 1.7966210741526148e-07,
      "loss": 1.6906,
      "step": 310112
    },
    {
      "epoch": 0.0011255741283093537,
      "grad_norm": 12023.359929736778,
      "learning_rate": 1.7965311930118605e-07,
      "loss": 1.6975,
      "step": 310144
    },
    {
      "epoch": 0.0011256902626601902,
      "grad_norm": 10574.016644586862,
      "learning_rate": 1.796438426627449e-07,
      "loss": 1.712,
      "step": 310176
    },
    {
      "epoch": 0.001125806397011027,
      "grad_norm": 10341.247023449347,
      "learning_rate": 1.796345674611917e-07,
      "loss": 1.7015,
      "step": 310208
    },
    {
      "epoch": 0.0011259225313618637,
      "grad_norm": 11519.843228099939,
      "learning_rate": 1.796252936961556e-07,
      "loss": 1.693,
      "step": 310240
    },
    {
      "epoch": 0.0011260386657127005,
      "grad_norm": 10995.951163951211,
      "learning_rate": 1.796160213672657e-07,
      "loss": 1.6811,
      "step": 310272
    },
    {
      "epoch": 0.0011261548000635372,
      "grad_norm": 8814.538445091723,
      "learning_rate": 1.7960675047415154e-07,
      "loss": 1.6815,
      "step": 310304
    },
    {
      "epoch": 0.0011262709344143737,
      "grad_norm": 11475.893864967557,
      "learning_rate": 1.7959748101644249e-07,
      "loss": 1.6948,
      "step": 310336
    },
    {
      "epoch": 0.0011263870687652105,
      "grad_norm": 12756.670882326627,
      "learning_rate": 1.7958821299376823e-07,
      "loss": 1.717,
      "step": 310368
    },
    {
      "epoch": 0.0011265032031160473,
      "grad_norm": 13443.178790747372,
      "learning_rate": 1.7957894640575852e-07,
      "loss": 1.7136,
      "step": 310400
    },
    {
      "epoch": 0.001126619337466884,
      "grad_norm": 9132.128119994813,
      "learning_rate": 1.7956968125204328e-07,
      "loss": 1.7259,
      "step": 310432
    },
    {
      "epoch": 0.0011267354718177205,
      "grad_norm": 7935.992313504342,
      "learning_rate": 1.7956041753225253e-07,
      "loss": 1.7189,
      "step": 310464
    },
    {
      "epoch": 0.0011268516061685573,
      "grad_norm": 9305.998925424396,
      "learning_rate": 1.7955115524601643e-07,
      "loss": 1.7202,
      "step": 310496
    },
    {
      "epoch": 0.001126967740519394,
      "grad_norm": 8346.208360686906,
      "learning_rate": 1.7954189439296529e-07,
      "loss": 1.7167,
      "step": 310528
    },
    {
      "epoch": 0.0011270838748702308,
      "grad_norm": 10637.955630665132,
      "learning_rate": 1.7953263497272953e-07,
      "loss": 1.7064,
      "step": 310560
    },
    {
      "epoch": 0.0011272000092210676,
      "grad_norm": 8909.984287303767,
      "learning_rate": 1.7952337698493976e-07,
      "loss": 1.7082,
      "step": 310592
    },
    {
      "epoch": 0.001127316143571904,
      "grad_norm": 9565.14547720002,
      "learning_rate": 1.7951412042922665e-07,
      "loss": 1.6754,
      "step": 310624
    },
    {
      "epoch": 0.0011274322779227409,
      "grad_norm": 12282.096563697909,
      "learning_rate": 1.7950486530522102e-07,
      "loss": 1.6852,
      "step": 310656
    },
    {
      "epoch": 0.0011275484122735776,
      "grad_norm": 12229.096450678602,
      "learning_rate": 1.794956116125539e-07,
      "loss": 1.6999,
      "step": 310688
    },
    {
      "epoch": 0.0011276645466244144,
      "grad_norm": 10682.584893180114,
      "learning_rate": 1.7948635935085635e-07,
      "loss": 1.6974,
      "step": 310720
    },
    {
      "epoch": 0.001127780680975251,
      "grad_norm": 8357.696333320564,
      "learning_rate": 1.7947710851975957e-07,
      "loss": 1.6865,
      "step": 310752
    },
    {
      "epoch": 0.0011278968153260877,
      "grad_norm": 7962.039814017511,
      "learning_rate": 1.7946785911889502e-07,
      "loss": 1.7091,
      "step": 310784
    },
    {
      "epoch": 0.0011280129496769244,
      "grad_norm": 12911.40309958604,
      "learning_rate": 1.7945861114789413e-07,
      "loss": 1.7041,
      "step": 310816
    },
    {
      "epoch": 0.0011281290840277612,
      "grad_norm": 10989.912283544396,
      "learning_rate": 1.794493646063885e-07,
      "loss": 1.6992,
      "step": 310848
    },
    {
      "epoch": 0.001128245218378598,
      "grad_norm": 8152.542670848157,
      "learning_rate": 1.7944011949401e-07,
      "loss": 1.7012,
      "step": 310880
    },
    {
      "epoch": 0.0011283613527294345,
      "grad_norm": 9964.46325699483,
      "learning_rate": 1.7943087581039047e-07,
      "loss": 1.7189,
      "step": 310912
    },
    {
      "epoch": 0.0011284774870802712,
      "grad_norm": 9586.004172751022,
      "learning_rate": 1.7942163355516196e-07,
      "loss": 1.7022,
      "step": 310944
    },
    {
      "epoch": 0.001128593621431108,
      "grad_norm": 9859.469762619083,
      "learning_rate": 1.7941239272795662e-07,
      "loss": 1.6996,
      "step": 310976
    },
    {
      "epoch": 0.0011287097557819447,
      "grad_norm": 9286.785019585626,
      "learning_rate": 1.7940315332840672e-07,
      "loss": 1.6813,
      "step": 311008
    },
    {
      "epoch": 0.0011288258901327813,
      "grad_norm": 11797.437348848265,
      "learning_rate": 1.7939391535614475e-07,
      "loss": 1.6955,
      "step": 311040
    },
    {
      "epoch": 0.001128942024483618,
      "grad_norm": 8623.525960997624,
      "learning_rate": 1.7938467881080327e-07,
      "loss": 1.6964,
      "step": 311072
    },
    {
      "epoch": 0.0011290581588344548,
      "grad_norm": 9356.295313851524,
      "learning_rate": 1.7937544369201488e-07,
      "loss": 1.6831,
      "step": 311104
    },
    {
      "epoch": 0.0011291742931852915,
      "grad_norm": 11535.488372843172,
      "learning_rate": 1.7936620999941254e-07,
      "loss": 1.7023,
      "step": 311136
    },
    {
      "epoch": 0.0011292904275361283,
      "grad_norm": 7836.20418314888,
      "learning_rate": 1.793572662193875e-07,
      "loss": 1.7001,
      "step": 311168
    },
    {
      "epoch": 0.0011294065618869648,
      "grad_norm": 10616.289370585186,
      "learning_rate": 1.7934803533351633e-07,
      "loss": 1.7042,
      "step": 311200
    },
    {
      "epoch": 0.0011295226962378016,
      "grad_norm": 10535.84851827322,
      "learning_rate": 1.7933880587274185e-07,
      "loss": 1.6976,
      "step": 311232
    },
    {
      "epoch": 0.0011296388305886383,
      "grad_norm": 9359.02131635568,
      "learning_rate": 1.7932957783669743e-07,
      "loss": 1.7103,
      "step": 311264
    },
    {
      "epoch": 0.001129754964939475,
      "grad_norm": 9096.703358909754,
      "learning_rate": 1.7932035122501658e-07,
      "loss": 1.7008,
      "step": 311296
    },
    {
      "epoch": 0.0011298710992903116,
      "grad_norm": 8869.972378761955,
      "learning_rate": 1.7931112603733291e-07,
      "loss": 1.6995,
      "step": 311328
    },
    {
      "epoch": 0.0011299872336411484,
      "grad_norm": 12000.253330659316,
      "learning_rate": 1.7930190227328016e-07,
      "loss": 1.7114,
      "step": 311360
    },
    {
      "epoch": 0.0011301033679919851,
      "grad_norm": 11229.493488132044,
      "learning_rate": 1.7929267993249224e-07,
      "loss": 1.7255,
      "step": 311392
    },
    {
      "epoch": 0.0011302195023428219,
      "grad_norm": 9271.298290962275,
      "learning_rate": 1.792834590146031e-07,
      "loss": 1.7148,
      "step": 311424
    },
    {
      "epoch": 0.0011303356366936586,
      "grad_norm": 9092.874133078056,
      "learning_rate": 1.792742395192469e-07,
      "loss": 1.6959,
      "step": 311456
    },
    {
      "epoch": 0.0011304517710444952,
      "grad_norm": 9162.373273339173,
      "learning_rate": 1.7926502144605799e-07,
      "loss": 1.7104,
      "step": 311488
    },
    {
      "epoch": 0.001130567905395332,
      "grad_norm": 13202.067262364633,
      "learning_rate": 1.7925580479467065e-07,
      "loss": 1.7205,
      "step": 311520
    },
    {
      "epoch": 0.0011306840397461687,
      "grad_norm": 9177.011387156495,
      "learning_rate": 1.7924658956471955e-07,
      "loss": 1.707,
      "step": 311552
    },
    {
      "epoch": 0.0011308001740970054,
      "grad_norm": 9272.8461650132,
      "learning_rate": 1.7923737575583925e-07,
      "loss": 1.7025,
      "step": 311584
    },
    {
      "epoch": 0.001130916308447842,
      "grad_norm": 12550.135298075475,
      "learning_rate": 1.7922816336766463e-07,
      "loss": 1.7052,
      "step": 311616
    },
    {
      "epoch": 0.0011310324427986787,
      "grad_norm": 10829.337006483822,
      "learning_rate": 1.7921895239983054e-07,
      "loss": 1.699,
      "step": 311648
    },
    {
      "epoch": 0.0011311485771495155,
      "grad_norm": 10537.104346071552,
      "learning_rate": 1.7920974285197215e-07,
      "loss": 1.7204,
      "step": 311680
    },
    {
      "epoch": 0.0011312647115003522,
      "grad_norm": 11513.906635021842,
      "learning_rate": 1.7920053472372459e-07,
      "loss": 1.6856,
      "step": 311712
    },
    {
      "epoch": 0.001131380845851189,
      "grad_norm": 8362.942663919202,
      "learning_rate": 1.7919132801472318e-07,
      "loss": 1.6964,
      "step": 311744
    },
    {
      "epoch": 0.0011314969802020255,
      "grad_norm": 17726.973684190994,
      "learning_rate": 1.791821227246034e-07,
      "loss": 1.6732,
      "step": 311776
    },
    {
      "epoch": 0.0011316131145528623,
      "grad_norm": 10761.78981396682,
      "learning_rate": 1.7917291885300083e-07,
      "loss": 1.6906,
      "step": 311808
    },
    {
      "epoch": 0.001131729248903699,
      "grad_norm": 8860.168282826236,
      "learning_rate": 1.791637163995512e-07,
      "loss": 1.6887,
      "step": 311840
    },
    {
      "epoch": 0.0011318453832545358,
      "grad_norm": 10138.4338040942,
      "learning_rate": 1.791545153638903e-07,
      "loss": 1.6968,
      "step": 311872
    },
    {
      "epoch": 0.0011319615176053723,
      "grad_norm": 9739.279850173729,
      "learning_rate": 1.7914531574565423e-07,
      "loss": 1.6957,
      "step": 311904
    },
    {
      "epoch": 0.001132077651956209,
      "grad_norm": 10485.850084757078,
      "learning_rate": 1.79136117544479e-07,
      "loss": 1.7035,
      "step": 311936
    },
    {
      "epoch": 0.0011321937863070458,
      "grad_norm": 11500.689892349938,
      "learning_rate": 1.7912692076000093e-07,
      "loss": 1.6939,
      "step": 311968
    },
    {
      "epoch": 0.0011323099206578826,
      "grad_norm": 9908.80901016868,
      "learning_rate": 1.791177253918563e-07,
      "loss": 1.6967,
      "step": 312000
    },
    {
      "epoch": 0.0011324260550087193,
      "grad_norm": 9760.548959971462,
      "learning_rate": 1.7910853143968169e-07,
      "loss": 1.7096,
      "step": 312032
    },
    {
      "epoch": 0.0011325421893595559,
      "grad_norm": 9067.164054984336,
      "learning_rate": 1.790993389031137e-07,
      "loss": 1.7158,
      "step": 312064
    },
    {
      "epoch": 0.0011326583237103926,
      "grad_norm": 11119.796580873232,
      "learning_rate": 1.7909014778178912e-07,
      "loss": 1.6916,
      "step": 312096
    },
    {
      "epoch": 0.0011327744580612294,
      "grad_norm": 9833.414462942157,
      "learning_rate": 1.7908095807534485e-07,
      "loss": 1.6912,
      "step": 312128
    },
    {
      "epoch": 0.0011328905924120661,
      "grad_norm": 26382.761947908333,
      "learning_rate": 1.7907176978341785e-07,
      "loss": 1.7051,
      "step": 312160
    },
    {
      "epoch": 0.0011330067267629027,
      "grad_norm": 11794.366621400235,
      "learning_rate": 1.790628699741737e-07,
      "loss": 1.7223,
      "step": 312192
    },
    {
      "epoch": 0.0011331228611137394,
      "grad_norm": 13947.195560398513,
      "learning_rate": 1.7905368446601747e-07,
      "loss": 1.7272,
      "step": 312224
    },
    {
      "epoch": 0.0011332389954645762,
      "grad_norm": 8640.815702235524,
      "learning_rate": 1.790445003713017e-07,
      "loss": 1.7273,
      "step": 312256
    },
    {
      "epoch": 0.001133355129815413,
      "grad_norm": 9117.962053002853,
      "learning_rate": 1.7903531768966405e-07,
      "loss": 1.7202,
      "step": 312288
    },
    {
      "epoch": 0.0011334712641662497,
      "grad_norm": 10363.354476230175,
      "learning_rate": 1.790261364207421e-07,
      "loss": 1.6769,
      "step": 312320
    },
    {
      "epoch": 0.0011335873985170862,
      "grad_norm": 11732.63448676383,
      "learning_rate": 1.7901695656417367e-07,
      "loss": 1.6793,
      "step": 312352
    },
    {
      "epoch": 0.001133703532867923,
      "grad_norm": 9182.421902744396,
      "learning_rate": 1.7900777811959668e-07,
      "loss": 1.6941,
      "step": 312384
    },
    {
      "epoch": 0.0011338196672187597,
      "grad_norm": 8651.726532895038,
      "learning_rate": 1.7899860108664928e-07,
      "loss": 1.7046,
      "step": 312416
    },
    {
      "epoch": 0.0011339358015695965,
      "grad_norm": 9650.09284929425,
      "learning_rate": 1.7898942546496955e-07,
      "loss": 1.6948,
      "step": 312448
    },
    {
      "epoch": 0.001134051935920433,
      "grad_norm": 13725.515509444445,
      "learning_rate": 1.789802512541959e-07,
      "loss": 1.694,
      "step": 312480
    },
    {
      "epoch": 0.0011341680702712698,
      "grad_norm": 10699.418301945205,
      "learning_rate": 1.789710784539667e-07,
      "loss": 1.7005,
      "step": 312512
    },
    {
      "epoch": 0.0011342842046221065,
      "grad_norm": 8575.479228591252,
      "learning_rate": 1.7896190706392062e-07,
      "loss": 1.728,
      "step": 312544
    },
    {
      "epoch": 0.0011344003389729433,
      "grad_norm": 9922.988864248513,
      "learning_rate": 1.789527370836963e-07,
      "loss": 1.7307,
      "step": 312576
    },
    {
      "epoch": 0.0011345164733237798,
      "grad_norm": 8623.546138335436,
      "learning_rate": 1.7894356851293262e-07,
      "loss": 1.7072,
      "step": 312608
    },
    {
      "epoch": 0.0011346326076746166,
      "grad_norm": 17968.75332347795,
      "learning_rate": 1.7893440135126856e-07,
      "loss": 1.6916,
      "step": 312640
    },
    {
      "epoch": 0.0011347487420254533,
      "grad_norm": 9800.410603643095,
      "learning_rate": 1.789252355983432e-07,
      "loss": 1.6664,
      "step": 312672
    },
    {
      "epoch": 0.00113486487637629,
      "grad_norm": 9313.869872399979,
      "learning_rate": 1.7891607125379575e-07,
      "loss": 1.679,
      "step": 312704
    },
    {
      "epoch": 0.0011349810107271268,
      "grad_norm": 8544.598644757985,
      "learning_rate": 1.789069083172656e-07,
      "loss": 1.7003,
      "step": 312736
    },
    {
      "epoch": 0.0011350971450779634,
      "grad_norm": 9573.22432621319,
      "learning_rate": 1.7889774678839224e-07,
      "loss": 1.7062,
      "step": 312768
    },
    {
      "epoch": 0.0011352132794288001,
      "grad_norm": 8528.605044202715,
      "learning_rate": 1.7888858666681527e-07,
      "loss": 1.6767,
      "step": 312800
    },
    {
      "epoch": 0.0011353294137796369,
      "grad_norm": 9770.134901832214,
      "learning_rate": 1.7887942795217444e-07,
      "loss": 1.6922,
      "step": 312832
    },
    {
      "epoch": 0.0011354455481304736,
      "grad_norm": 8764.276581669475,
      "learning_rate": 1.7887027064410967e-07,
      "loss": 1.685,
      "step": 312864
    },
    {
      "epoch": 0.0011355616824813102,
      "grad_norm": 9442.272184172622,
      "learning_rate": 1.7886111474226092e-07,
      "loss": 1.6952,
      "step": 312896
    },
    {
      "epoch": 0.001135677816832147,
      "grad_norm": 10639.218204360695,
      "learning_rate": 1.788519602462683e-07,
      "loss": 1.7083,
      "step": 312928
    },
    {
      "epoch": 0.0011357939511829837,
      "grad_norm": 8657.341855327188,
      "learning_rate": 1.7884280715577214e-07,
      "loss": 1.7078,
      "step": 312960
    },
    {
      "epoch": 0.0011359100855338204,
      "grad_norm": 10411.478281204836,
      "learning_rate": 1.7883365547041276e-07,
      "loss": 1.7041,
      "step": 312992
    },
    {
      "epoch": 0.0011360262198846572,
      "grad_norm": 9466.18339142022,
      "learning_rate": 1.7882450518983077e-07,
      "loss": 1.6985,
      "step": 313024
    },
    {
      "epoch": 0.0011361423542354937,
      "grad_norm": 8976.762668133762,
      "learning_rate": 1.7881535631366675e-07,
      "loss": 1.7221,
      "step": 313056
    },
    {
      "epoch": 0.0011362584885863305,
      "grad_norm": 10437.450646589903,
      "learning_rate": 1.7880620884156147e-07,
      "loss": 1.7487,
      "step": 313088
    },
    {
      "epoch": 0.0011363746229371672,
      "grad_norm": 9030.62312357237,
      "learning_rate": 1.787970627731559e-07,
      "loss": 1.7495,
      "step": 313120
    },
    {
      "epoch": 0.001136490757288004,
      "grad_norm": 12998.91441621184,
      "learning_rate": 1.7878791810809098e-07,
      "loss": 1.7232,
      "step": 313152
    },
    {
      "epoch": 0.0011366068916388405,
      "grad_norm": 21558.299376342282,
      "learning_rate": 1.78778774846008e-07,
      "loss": 1.7107,
      "step": 313184
    },
    {
      "epoch": 0.0011367230259896773,
      "grad_norm": 8607.158997021026,
      "learning_rate": 1.787699186484287e-07,
      "loss": 1.7065,
      "step": 313216
    },
    {
      "epoch": 0.001136839160340514,
      "grad_norm": 11651.935461544575,
      "learning_rate": 1.7876077814741813e-07,
      "loss": 1.7089,
      "step": 313248
    },
    {
      "epoch": 0.0011369552946913508,
      "grad_norm": 11293.31855567707,
      "learning_rate": 1.7875163904832484e-07,
      "loss": 1.7165,
      "step": 313280
    },
    {
      "epoch": 0.0011370714290421875,
      "grad_norm": 10984.570633393005,
      "learning_rate": 1.787425013507906e-07,
      "loss": 1.7228,
      "step": 313312
    },
    {
      "epoch": 0.001137187563393024,
      "grad_norm": 9528.840643016338,
      "learning_rate": 1.7873336505445717e-07,
      "loss": 1.6883,
      "step": 313344
    },
    {
      "epoch": 0.0011373036977438608,
      "grad_norm": 9788.899223099603,
      "learning_rate": 1.7872423015896645e-07,
      "loss": 1.6942,
      "step": 313376
    },
    {
      "epoch": 0.0011374198320946976,
      "grad_norm": 8374.486252899338,
      "learning_rate": 1.7871509666396057e-07,
      "loss": 1.7089,
      "step": 313408
    },
    {
      "epoch": 0.0011375359664455343,
      "grad_norm": 8455.48390099585,
      "learning_rate": 1.7870596456908164e-07,
      "loss": 1.7238,
      "step": 313440
    },
    {
      "epoch": 0.0011376521007963709,
      "grad_norm": 10074.200514184737,
      "learning_rate": 1.78696833873972e-07,
      "loss": 1.6987,
      "step": 313472
    },
    {
      "epoch": 0.0011377682351472076,
      "grad_norm": 9831.084172155175,
      "learning_rate": 1.786877045782741e-07,
      "loss": 1.6921,
      "step": 313504
    },
    {
      "epoch": 0.0011378843694980444,
      "grad_norm": 11611.545633549395,
      "learning_rate": 1.7867857668163053e-07,
      "loss": 1.7061,
      "step": 313536
    },
    {
      "epoch": 0.0011380005038488811,
      "grad_norm": 14017.364231552237,
      "learning_rate": 1.7866945018368393e-07,
      "loss": 1.7112,
      "step": 313568
    },
    {
      "epoch": 0.001138116638199718,
      "grad_norm": 9719.458215353365,
      "learning_rate": 1.7866032508407715e-07,
      "loss": 1.7235,
      "step": 313600
    },
    {
      "epoch": 0.0011382327725505544,
      "grad_norm": 9499.798945240895,
      "learning_rate": 1.786512013824532e-07,
      "loss": 1.7111,
      "step": 313632
    },
    {
      "epoch": 0.0011383489069013912,
      "grad_norm": 9161.09578598543,
      "learning_rate": 1.786420790784551e-07,
      "loss": 1.7138,
      "step": 313664
    },
    {
      "epoch": 0.001138465041252228,
      "grad_norm": 9972.77975290741,
      "learning_rate": 1.7863295817172607e-07,
      "loss": 1.6862,
      "step": 313696
    },
    {
      "epoch": 0.0011385811756030647,
      "grad_norm": 11746.418517999433,
      "learning_rate": 1.7862383866190942e-07,
      "loss": 1.6757,
      "step": 313728
    },
    {
      "epoch": 0.0011386973099539012,
      "grad_norm": 8261.26733619001,
      "learning_rate": 1.7861472054864868e-07,
      "loss": 1.685,
      "step": 313760
    },
    {
      "epoch": 0.001138813444304738,
      "grad_norm": 10653.544198997815,
      "learning_rate": 1.7860560383158737e-07,
      "loss": 1.6933,
      "step": 313792
    },
    {
      "epoch": 0.0011389295786555747,
      "grad_norm": 10224.327068320927,
      "learning_rate": 1.7859648851036924e-07,
      "loss": 1.6882,
      "step": 313824
    },
    {
      "epoch": 0.0011390457130064115,
      "grad_norm": 9806.439822891894,
      "learning_rate": 1.7858737458463817e-07,
      "loss": 1.6962,
      "step": 313856
    },
    {
      "epoch": 0.0011391618473572483,
      "grad_norm": 8926.170735539401,
      "learning_rate": 1.7857826205403805e-07,
      "loss": 1.7029,
      "step": 313888
    },
    {
      "epoch": 0.0011392779817080848,
      "grad_norm": 12160.432064692439,
      "learning_rate": 1.7856915091821305e-07,
      "loss": 1.7097,
      "step": 313920
    },
    {
      "epoch": 0.0011393941160589215,
      "grad_norm": 9521.862422866652,
      "learning_rate": 1.7856004117680735e-07,
      "loss": 1.7218,
      "step": 313952
    },
    {
      "epoch": 0.0011395102504097583,
      "grad_norm": 9130.388600711363,
      "learning_rate": 1.7855093282946534e-07,
      "loss": 1.7145,
      "step": 313984
    },
    {
      "epoch": 0.001139626384760595,
      "grad_norm": 8515.204401539635,
      "learning_rate": 1.7854182587583147e-07,
      "loss": 1.7138,
      "step": 314016
    },
    {
      "epoch": 0.0011397425191114316,
      "grad_norm": 10908.498155108246,
      "learning_rate": 1.7853272031555034e-07,
      "loss": 1.698,
      "step": 314048
    },
    {
      "epoch": 0.0011398586534622683,
      "grad_norm": 7095.204436800958,
      "learning_rate": 1.7852361614826674e-07,
      "loss": 1.6836,
      "step": 314080
    },
    {
      "epoch": 0.001139974787813105,
      "grad_norm": 9046.353298429152,
      "learning_rate": 1.785145133736255e-07,
      "loss": 1.6981,
      "step": 314112
    },
    {
      "epoch": 0.0011400909221639418,
      "grad_norm": 9093.532426950485,
      "learning_rate": 1.7850541199127157e-07,
      "loss": 1.7095,
      "step": 314144
    },
    {
      "epoch": 0.0011402070565147786,
      "grad_norm": 8956.200310399494,
      "learning_rate": 1.784963120008501e-07,
      "loss": 1.6906,
      "step": 314176
    },
    {
      "epoch": 0.0011403231908656151,
      "grad_norm": 7943.193941985805,
      "learning_rate": 1.7848721340200637e-07,
      "loss": 1.7053,
      "step": 314208
    },
    {
      "epoch": 0.001140439325216452,
      "grad_norm": 10523.857467677904,
      "learning_rate": 1.784784004610688e-07,
      "loss": 1.6982,
      "step": 314240
    },
    {
      "epoch": 0.0011405554595672886,
      "grad_norm": 10837.596043403722,
      "learning_rate": 1.7846930460085738e-07,
      "loss": 1.6989,
      "step": 314272
    },
    {
      "epoch": 0.0011406715939181254,
      "grad_norm": 7997.79707169418,
      "learning_rate": 1.7846021013117125e-07,
      "loss": 1.7084,
      "step": 314304
    },
    {
      "epoch": 0.001140787728268962,
      "grad_norm": 9364.99588894731,
      "learning_rate": 1.7845111705165605e-07,
      "loss": 1.7017,
      "step": 314336
    },
    {
      "epoch": 0.0011409038626197987,
      "grad_norm": 8423.557680695254,
      "learning_rate": 1.784420253619578e-07,
      "loss": 1.6863,
      "step": 314368
    },
    {
      "epoch": 0.0011410199969706354,
      "grad_norm": 10122.184546825849,
      "learning_rate": 1.7843293506172236e-07,
      "loss": 1.6786,
      "step": 314400
    },
    {
      "epoch": 0.0011411361313214722,
      "grad_norm": 10861.465094544106,
      "learning_rate": 1.784238461505959e-07,
      "loss": 1.6805,
      "step": 314432
    },
    {
      "epoch": 0.001141252265672309,
      "grad_norm": 9286.158516846457,
      "learning_rate": 1.784147586282247e-07,
      "loss": 1.6931,
      "step": 314464
    },
    {
      "epoch": 0.0011413684000231455,
      "grad_norm": 9851.56921510477,
      "learning_rate": 1.7840567249425507e-07,
      "loss": 1.6895,
      "step": 314496
    },
    {
      "epoch": 0.0011414845343739822,
      "grad_norm": 9215.810653436842,
      "learning_rate": 1.7839658774833356e-07,
      "loss": 1.6748,
      "step": 314528
    },
    {
      "epoch": 0.001141600668724819,
      "grad_norm": 9694.770342818854,
      "learning_rate": 1.783875043901068e-07,
      "loss": 1.6911,
      "step": 314560
    },
    {
      "epoch": 0.0011417168030756558,
      "grad_norm": 9249.09065800525,
      "learning_rate": 1.783784224192215e-07,
      "loss": 1.6948,
      "step": 314592
    },
    {
      "epoch": 0.0011418329374264923,
      "grad_norm": 8568.80213332062,
      "learning_rate": 1.7836934183532456e-07,
      "loss": 1.6939,
      "step": 314624
    },
    {
      "epoch": 0.001141949071777329,
      "grad_norm": 10078.81620032829,
      "learning_rate": 1.7836026263806302e-07,
      "loss": 1.699,
      "step": 314656
    },
    {
      "epoch": 0.0011420652061281658,
      "grad_norm": 10263.345458475029,
      "learning_rate": 1.7835118482708393e-07,
      "loss": 1.7086,
      "step": 314688
    },
    {
      "epoch": 0.0011421813404790026,
      "grad_norm": 9995.575621243632,
      "learning_rate": 1.783421084020346e-07,
      "loss": 1.6992,
      "step": 314720
    },
    {
      "epoch": 0.0011422974748298393,
      "grad_norm": 10522.279981068741,
      "learning_rate": 1.7833303336256242e-07,
      "loss": 1.7042,
      "step": 314752
    },
    {
      "epoch": 0.0011424136091806758,
      "grad_norm": 9518.861906761753,
      "learning_rate": 1.7832395970831488e-07,
      "loss": 1.7024,
      "step": 314784
    },
    {
      "epoch": 0.0011425297435315126,
      "grad_norm": 8586.72976167295,
      "learning_rate": 1.783148874389396e-07,
      "loss": 1.7149,
      "step": 314816
    },
    {
      "epoch": 0.0011426458778823494,
      "grad_norm": 8390.438844303675,
      "learning_rate": 1.7830581655408433e-07,
      "loss": 1.7043,
      "step": 314848
    },
    {
      "epoch": 0.0011427620122331861,
      "grad_norm": 9043.543995580494,
      "learning_rate": 1.78296747053397e-07,
      "loss": 1.7016,
      "step": 314880
    },
    {
      "epoch": 0.0011428781465840226,
      "grad_norm": 10030.559306439496,
      "learning_rate": 1.7828767893652556e-07,
      "loss": 1.7113,
      "step": 314912
    },
    {
      "epoch": 0.0011429942809348594,
      "grad_norm": 9134.918937790308,
      "learning_rate": 1.782786122031182e-07,
      "loss": 1.7067,
      "step": 314944
    },
    {
      "epoch": 0.0011431104152856962,
      "grad_norm": 11963.815612086306,
      "learning_rate": 1.7826954685282315e-07,
      "loss": 1.6944,
      "step": 314976
    },
    {
      "epoch": 0.001143226549636533,
      "grad_norm": 7911.625623094156,
      "learning_rate": 1.7826048288528875e-07,
      "loss": 1.6916,
      "step": 315008
    },
    {
      "epoch": 0.0011433426839873697,
      "grad_norm": 10863.701395012658,
      "learning_rate": 1.782514203001636e-07,
      "loss": 1.6951,
      "step": 315040
    },
    {
      "epoch": 0.0011434588183382062,
      "grad_norm": 10201.739067433553,
      "learning_rate": 1.7824235909709625e-07,
      "loss": 1.6736,
      "step": 315072
    },
    {
      "epoch": 0.001143574952689043,
      "grad_norm": 9622.32840844668,
      "learning_rate": 1.7823329927573551e-07,
      "loss": 1.6851,
      "step": 315104
    },
    {
      "epoch": 0.0011436910870398797,
      "grad_norm": 9996.546203564509,
      "learning_rate": 1.7822424083573026e-07,
      "loss": 1.6984,
      "step": 315136
    },
    {
      "epoch": 0.0011438072213907165,
      "grad_norm": 9673.400849752894,
      "learning_rate": 1.782151837767295e-07,
      "loss": 1.7073,
      "step": 315168
    },
    {
      "epoch": 0.001143923355741553,
      "grad_norm": 11186.950075869652,
      "learning_rate": 1.7820612809838236e-07,
      "loss": 1.6971,
      "step": 315200
    },
    {
      "epoch": 0.0011440394900923898,
      "grad_norm": 21957.675104618887,
      "learning_rate": 1.7819707380033813e-07,
      "loss": 1.6862,
      "step": 315232
    },
    {
      "epoch": 0.0011441556244432265,
      "grad_norm": 18324.536119640245,
      "learning_rate": 1.7818802088224612e-07,
      "loss": 1.7012,
      "step": 315264
    },
    {
      "epoch": 0.0011442717587940633,
      "grad_norm": 10037.244641832738,
      "learning_rate": 1.781792521834546e-07,
      "loss": 1.7155,
      "step": 315296
    },
    {
      "epoch": 0.0011443878931449,
      "grad_norm": 8498.151093031942,
      "learning_rate": 1.7817020198111946e-07,
      "loss": 1.6976,
      "step": 315328
    },
    {
      "epoch": 0.0011445040274957366,
      "grad_norm": 9726.85828004089,
      "learning_rate": 1.7816115315769647e-07,
      "loss": 1.6957,
      "step": 315360
    },
    {
      "epoch": 0.0011446201618465733,
      "grad_norm": 8981.305027667193,
      "learning_rate": 1.7815210571283543e-07,
      "loss": 1.687,
      "step": 315392
    },
    {
      "epoch": 0.00114473629619741,
      "grad_norm": 10563.941499270053,
      "learning_rate": 1.781430596461864e-07,
      "loss": 1.675,
      "step": 315424
    },
    {
      "epoch": 0.0011448524305482468,
      "grad_norm": 11555.382814948192,
      "learning_rate": 1.7813401495739948e-07,
      "loss": 1.6903,
      "step": 315456
    },
    {
      "epoch": 0.0011449685648990834,
      "grad_norm": 10757.00106907125,
      "learning_rate": 1.7812497164612493e-07,
      "loss": 1.7015,
      "step": 315488
    },
    {
      "epoch": 0.00114508469924992,
      "grad_norm": 13866.482466725296,
      "learning_rate": 1.7811592971201313e-07,
      "loss": 1.6939,
      "step": 315520
    },
    {
      "epoch": 0.0011452008336007569,
      "grad_norm": 12466.319103889487,
      "learning_rate": 1.7810688915471458e-07,
      "loss": 1.679,
      "step": 315552
    },
    {
      "epoch": 0.0011453169679515936,
      "grad_norm": 12881.802203108073,
      "learning_rate": 1.7809784997387986e-07,
      "loss": 1.6939,
      "step": 315584
    },
    {
      "epoch": 0.0011454331023024304,
      "grad_norm": 9380.227715786008,
      "learning_rate": 1.7808881216915977e-07,
      "loss": 1.7104,
      "step": 315616
    },
    {
      "epoch": 0.001145549236653267,
      "grad_norm": 9579.003497232894,
      "learning_rate": 1.7807977574020515e-07,
      "loss": 1.708,
      "step": 315648
    },
    {
      "epoch": 0.0011456653710041037,
      "grad_norm": 9113.709014446315,
      "learning_rate": 1.7807074068666705e-07,
      "loss": 1.7243,
      "step": 315680
    },
    {
      "epoch": 0.0011457815053549404,
      "grad_norm": 9094.155265883688,
      "learning_rate": 1.7806170700819652e-07,
      "loss": 1.7267,
      "step": 315712
    },
    {
      "epoch": 0.0011458976397057772,
      "grad_norm": 9315.487534208823,
      "learning_rate": 1.7805267470444482e-07,
      "loss": 1.7033,
      "step": 315744
    },
    {
      "epoch": 0.0011460137740566137,
      "grad_norm": 10359.094941161606,
      "learning_rate": 1.7804364377506334e-07,
      "loss": 1.7094,
      "step": 315776
    },
    {
      "epoch": 0.0011461299084074505,
      "grad_norm": 9576.522542133966,
      "learning_rate": 1.7803461421970359e-07,
      "loss": 1.7186,
      "step": 315808
    },
    {
      "epoch": 0.0011462460427582872,
      "grad_norm": 10221.719816156183,
      "learning_rate": 1.7802558603801713e-07,
      "loss": 1.7189,
      "step": 315840
    },
    {
      "epoch": 0.001146362177109124,
      "grad_norm": 8524.190166813501,
      "learning_rate": 1.7801655922965574e-07,
      "loss": 1.6883,
      "step": 315872
    },
    {
      "epoch": 0.0011464783114599607,
      "grad_norm": 8658.872443915547,
      "learning_rate": 1.7800753379427125e-07,
      "loss": 1.6793,
      "step": 315904
    },
    {
      "epoch": 0.0011465944458107973,
      "grad_norm": 10598.393085746537,
      "learning_rate": 1.7799850973151565e-07,
      "loss": 1.6907,
      "step": 315936
    },
    {
      "epoch": 0.001146710580161634,
      "grad_norm": 10125.230861565577,
      "learning_rate": 1.7798948704104111e-07,
      "loss": 1.6909,
      "step": 315968
    },
    {
      "epoch": 0.0011468267145124708,
      "grad_norm": 8285.053047506695,
      "learning_rate": 1.7798046572249978e-07,
      "loss": 1.708,
      "step": 316000
    },
    {
      "epoch": 0.0011469428488633075,
      "grad_norm": 9005.157411172779,
      "learning_rate": 1.7797144577554408e-07,
      "loss": 1.7151,
      "step": 316032
    },
    {
      "epoch": 0.001147058983214144,
      "grad_norm": 10849.096275727301,
      "learning_rate": 1.7796242719982646e-07,
      "loss": 1.7108,
      "step": 316064
    },
    {
      "epoch": 0.0011471751175649808,
      "grad_norm": 10155.467295993818,
      "learning_rate": 1.779534099949995e-07,
      "loss": 1.672,
      "step": 316096
    },
    {
      "epoch": 0.0011472912519158176,
      "grad_norm": 10766.167749018217,
      "learning_rate": 1.7794439416071597e-07,
      "loss": 1.6803,
      "step": 316128
    },
    {
      "epoch": 0.0011474073862666543,
      "grad_norm": 9615.143680673731,
      "learning_rate": 1.7793537969662865e-07,
      "loss": 1.6886,
      "step": 316160
    },
    {
      "epoch": 0.001147523520617491,
      "grad_norm": 11091.3142593653,
      "learning_rate": 1.7792636660239065e-07,
      "loss": 1.693,
      "step": 316192
    },
    {
      "epoch": 0.0011476396549683276,
      "grad_norm": 8635.209319987558,
      "learning_rate": 1.7791735487765491e-07,
      "loss": 1.6771,
      "step": 316224
    },
    {
      "epoch": 0.0011477557893191644,
      "grad_norm": 10744.04169761082,
      "learning_rate": 1.7790834452207472e-07,
      "loss": 1.6891,
      "step": 316256
    },
    {
      "epoch": 0.0011478719236700011,
      "grad_norm": 9465.192866497755,
      "learning_rate": 1.7789933553530343e-07,
      "loss": 1.6979,
      "step": 316288
    },
    {
      "epoch": 0.0011479880580208379,
      "grad_norm": 8920.863635321415,
      "learning_rate": 1.7789060938435605e-07,
      "loss": 1.7102,
      "step": 316320
    },
    {
      "epoch": 0.0011481041923716744,
      "grad_norm": 9712.767885623542,
      "learning_rate": 1.7788160309141467e-07,
      "loss": 1.7225,
      "step": 316352
    },
    {
      "epoch": 0.0011482203267225112,
      "grad_norm": 11922.349936149332,
      "learning_rate": 1.7787259816625374e-07,
      "loss": 1.714,
      "step": 316384
    },
    {
      "epoch": 0.001148336461073348,
      "grad_norm": 9836.67443804053,
      "learning_rate": 1.7786359460852712e-07,
      "loss": 1.6948,
      "step": 316416
    },
    {
      "epoch": 0.0011484525954241847,
      "grad_norm": 10604.993823666282,
      "learning_rate": 1.7785459241788874e-07,
      "loss": 1.6794,
      "step": 316448
    },
    {
      "epoch": 0.0011485687297750214,
      "grad_norm": 10945.71806689721,
      "learning_rate": 1.7784559159399267e-07,
      "loss": 1.6993,
      "step": 316480
    },
    {
      "epoch": 0.001148684864125858,
      "grad_norm": 11399.377000520686,
      "learning_rate": 1.7783659213649312e-07,
      "loss": 1.7124,
      "step": 316512
    },
    {
      "epoch": 0.0011488009984766947,
      "grad_norm": 11145.615460798923,
      "learning_rate": 1.778275940450444e-07,
      "loss": 1.7247,
      "step": 316544
    },
    {
      "epoch": 0.0011489171328275315,
      "grad_norm": 9364.434526440986,
      "learning_rate": 1.7781859731930096e-07,
      "loss": 1.7108,
      "step": 316576
    },
    {
      "epoch": 0.0011490332671783682,
      "grad_norm": 8759.272915031246,
      "learning_rate": 1.7780960195891735e-07,
      "loss": 1.7309,
      "step": 316608
    },
    {
      "epoch": 0.0011491494015292048,
      "grad_norm": 9326.27578404156,
      "learning_rate": 1.7780060796354825e-07,
      "loss": 1.715,
      "step": 316640
    },
    {
      "epoch": 0.0011492655358800415,
      "grad_norm": 10533.356160312818,
      "learning_rate": 1.7779161533284852e-07,
      "loss": 1.7137,
      "step": 316672
    },
    {
      "epoch": 0.0011493816702308783,
      "grad_norm": 10629.640351394773,
      "learning_rate": 1.77782624066473e-07,
      "loss": 1.7056,
      "step": 316704
    },
    {
      "epoch": 0.001149497804581715,
      "grad_norm": 9779.500805255859,
      "learning_rate": 1.7777363416407686e-07,
      "loss": 1.7002,
      "step": 316736
    },
    {
      "epoch": 0.0011496139389325518,
      "grad_norm": 9275.338700015218,
      "learning_rate": 1.7776464562531518e-07,
      "loss": 1.6806,
      "step": 316768
    },
    {
      "epoch": 0.0011497300732833883,
      "grad_norm": 11404.40336010613,
      "learning_rate": 1.7775565844984323e-07,
      "loss": 1.6791,
      "step": 316800
    },
    {
      "epoch": 0.001149846207634225,
      "grad_norm": 10621.481252631387,
      "learning_rate": 1.7774667263731655e-07,
      "loss": 1.6926,
      "step": 316832
    },
    {
      "epoch": 0.0011499623419850618,
      "grad_norm": 9433.677967791777,
      "learning_rate": 1.777376881873906e-07,
      "loss": 1.7027,
      "step": 316864
    },
    {
      "epoch": 0.0011500784763358986,
      "grad_norm": 11063.36314146833,
      "learning_rate": 1.7772870509972103e-07,
      "loss": 1.7107,
      "step": 316896
    },
    {
      "epoch": 0.0011501946106867351,
      "grad_norm": 9765.971124266138,
      "learning_rate": 1.7771972337396368e-07,
      "loss": 1.7162,
      "step": 316928
    },
    {
      "epoch": 0.0011503107450375719,
      "grad_norm": 9959.436931875216,
      "learning_rate": 1.7771074300977438e-07,
      "loss": 1.7001,
      "step": 316960
    },
    {
      "epoch": 0.0011504268793884086,
      "grad_norm": 9552.252613912595,
      "learning_rate": 1.7770176400680925e-07,
      "loss": 1.6937,
      "step": 316992
    },
    {
      "epoch": 0.0011505430137392454,
      "grad_norm": 13306.157822602285,
      "learning_rate": 1.7769278636472436e-07,
      "loss": 1.6889,
      "step": 317024
    },
    {
      "epoch": 0.0011506591480900821,
      "grad_norm": 10417.504883608166,
      "learning_rate": 1.77683810083176e-07,
      "loss": 1.6984,
      "step": 317056
    },
    {
      "epoch": 0.0011507752824409187,
      "grad_norm": 10473.126562779617,
      "learning_rate": 1.7767483516182063e-07,
      "loss": 1.7005,
      "step": 317088
    },
    {
      "epoch": 0.0011508914167917554,
      "grad_norm": 10397.612225891096,
      "learning_rate": 1.7766586160031467e-07,
      "loss": 1.6667,
      "step": 317120
    },
    {
      "epoch": 0.0011510075511425922,
      "grad_norm": 9254.027015305282,
      "learning_rate": 1.776568893983148e-07,
      "loss": 1.6764,
      "step": 317152
    },
    {
      "epoch": 0.001151123685493429,
      "grad_norm": 9037.226897671653,
      "learning_rate": 1.7764791855547775e-07,
      "loss": 1.6936,
      "step": 317184
    },
    {
      "epoch": 0.0011512398198442655,
      "grad_norm": 11899.555958101966,
      "learning_rate": 1.7763894907146046e-07,
      "loss": 1.7022,
      "step": 317216
    },
    {
      "epoch": 0.0011513559541951022,
      "grad_norm": 10308.218080735389,
      "learning_rate": 1.7762998094591985e-07,
      "loss": 1.6756,
      "step": 317248
    },
    {
      "epoch": 0.001151472088545939,
      "grad_norm": 12466.461406509869,
      "learning_rate": 1.776210141785131e-07,
      "loss": 1.6884,
      "step": 317280
    },
    {
      "epoch": 0.0011515882228967757,
      "grad_norm": 8010.185765636151,
      "learning_rate": 1.776123289173988e-07,
      "loss": 1.7,
      "step": 317312
    },
    {
      "epoch": 0.0011517043572476125,
      "grad_norm": 9637.614435118267,
      "learning_rate": 1.7760336482281646e-07,
      "loss": 1.7172,
      "step": 317344
    },
    {
      "epoch": 0.001151820491598449,
      "grad_norm": 9472.112752707286,
      "learning_rate": 1.7759440208535077e-07,
      "loss": 1.7194,
      "step": 317376
    },
    {
      "epoch": 0.0011519366259492858,
      "grad_norm": 9030.16035295055,
      "learning_rate": 1.775854407046593e-07,
      "loss": 1.7366,
      "step": 317408
    },
    {
      "epoch": 0.0011520527603001225,
      "grad_norm": 10234.984123094671,
      "learning_rate": 1.7757648068039975e-07,
      "loss": 1.728,
      "step": 317440
    },
    {
      "epoch": 0.0011521688946509593,
      "grad_norm": 8805.265129455218,
      "learning_rate": 1.7756752201223002e-07,
      "loss": 1.7169,
      "step": 317472
    },
    {
      "epoch": 0.0011522850290017958,
      "grad_norm": 10727.000699170296,
      "learning_rate": 1.7755856469980807e-07,
      "loss": 1.707,
      "step": 317504
    },
    {
      "epoch": 0.0011524011633526326,
      "grad_norm": 11248.988398962816,
      "learning_rate": 1.7754960874279192e-07,
      "loss": 1.7201,
      "step": 317536
    },
    {
      "epoch": 0.0011525172977034693,
      "grad_norm": 10105.26733936317,
      "learning_rate": 1.7754065414083982e-07,
      "loss": 1.7097,
      "step": 317568
    },
    {
      "epoch": 0.001152633432054306,
      "grad_norm": 7963.280605378665,
      "learning_rate": 1.7753170089361007e-07,
      "loss": 1.674,
      "step": 317600
    },
    {
      "epoch": 0.0011527495664051428,
      "grad_norm": 11074.751283888952,
      "learning_rate": 1.775227490007612e-07,
      "loss": 1.6977,
      "step": 317632
    },
    {
      "epoch": 0.0011528657007559794,
      "grad_norm": 9786.570696623,
      "learning_rate": 1.7751379846195164e-07,
      "loss": 1.6985,
      "step": 317664
    },
    {
      "epoch": 0.0011529818351068161,
      "grad_norm": 10097.090670089083,
      "learning_rate": 1.775048492768402e-07,
      "loss": 1.7027,
      "step": 317696
    },
    {
      "epoch": 0.0011530979694576529,
      "grad_norm": 8118.706916744809,
      "learning_rate": 1.774959014450856e-07,
      "loss": 1.6925,
      "step": 317728
    },
    {
      "epoch": 0.0011532141038084896,
      "grad_norm": 10048.513720944009,
      "learning_rate": 1.774869549663468e-07,
      "loss": 1.6973,
      "step": 317760
    },
    {
      "epoch": 0.0011533302381593262,
      "grad_norm": 8145.441301734363,
      "learning_rate": 1.774780098402829e-07,
      "loss": 1.6881,
      "step": 317792
    },
    {
      "epoch": 0.001153446372510163,
      "grad_norm": 10521.058501880882,
      "learning_rate": 1.7746906606655295e-07,
      "loss": 1.6929,
      "step": 317824
    },
    {
      "epoch": 0.0011535625068609997,
      "grad_norm": 9463.52545302225,
      "learning_rate": 1.7746012364481638e-07,
      "loss": 1.6918,
      "step": 317856
    },
    {
      "epoch": 0.0011536786412118364,
      "grad_norm": 12314.075523562457,
      "learning_rate": 1.7745118257473247e-07,
      "loss": 1.6974,
      "step": 317888
    },
    {
      "epoch": 0.0011537947755626732,
      "grad_norm": 9649.591079418858,
      "learning_rate": 1.7744224285596085e-07,
      "loss": 1.6991,
      "step": 317920
    },
    {
      "epoch": 0.0011539109099135097,
      "grad_norm": 8572.804908546561,
      "learning_rate": 1.7743330448816112e-07,
      "loss": 1.6946,
      "step": 317952
    },
    {
      "epoch": 0.0011540270442643465,
      "grad_norm": 12890.134832498845,
      "learning_rate": 1.77424367470993e-07,
      "loss": 1.7177,
      "step": 317984
    },
    {
      "epoch": 0.0011541431786151832,
      "grad_norm": 11175.156374744829,
      "learning_rate": 1.774154318041165e-07,
      "loss": 1.7288,
      "step": 318016
    },
    {
      "epoch": 0.00115425931296602,
      "grad_norm": 9573.232682850658,
      "learning_rate": 1.7740649748719154e-07,
      "loss": 1.7089,
      "step": 318048
    },
    {
      "epoch": 0.0011543754473168565,
      "grad_norm": 8613.659152764289,
      "learning_rate": 1.7739756451987827e-07,
      "loss": 1.7136,
      "step": 318080
    },
    {
      "epoch": 0.0011544915816676933,
      "grad_norm": 9830.56702332068,
      "learning_rate": 1.7738863290183695e-07,
      "loss": 1.7123,
      "step": 318112
    },
    {
      "epoch": 0.00115460771601853,
      "grad_norm": 9142.905008803275,
      "learning_rate": 1.7737970263272792e-07,
      "loss": 1.6833,
      "step": 318144
    },
    {
      "epoch": 0.0011547238503693668,
      "grad_norm": 9500.330520566113,
      "learning_rate": 1.773707737122117e-07,
      "loss": 1.7062,
      "step": 318176
    },
    {
      "epoch": 0.0011548399847202036,
      "grad_norm": 8889.040555650536,
      "learning_rate": 1.773618461399489e-07,
      "loss": 1.7001,
      "step": 318208
    },
    {
      "epoch": 0.00115495611907104,
      "grad_norm": 8602.33317187843,
      "learning_rate": 1.7735291991560022e-07,
      "loss": 1.7175,
      "step": 318240
    },
    {
      "epoch": 0.0011550722534218768,
      "grad_norm": 8631.545632156503,
      "learning_rate": 1.773439950388265e-07,
      "loss": 1.7061,
      "step": 318272
    },
    {
      "epoch": 0.0011551883877727136,
      "grad_norm": 9803.73724658102,
      "learning_rate": 1.7733507150928874e-07,
      "loss": 1.714,
      "step": 318304
    },
    {
      "epoch": 0.0011553045221235504,
      "grad_norm": 9585.41788343106,
      "learning_rate": 1.773264281244713e-07,
      "loss": 1.7272,
      "step": 318336
    },
    {
      "epoch": 0.0011554206564743869,
      "grad_norm": 10909.994133820604,
      "learning_rate": 1.77317507246314e-07,
      "loss": 1.7347,
      "step": 318368
    },
    {
      "epoch": 0.0011555367908252236,
      "grad_norm": 10465.08308614891,
      "learning_rate": 1.7730858771438682e-07,
      "loss": 1.7344,
      "step": 318400
    },
    {
      "epoch": 0.0011556529251760604,
      "grad_norm": 9464.651076505674,
      "learning_rate": 1.772996695283512e-07,
      "loss": 1.7341,
      "step": 318432
    },
    {
      "epoch": 0.0011557690595268972,
      "grad_norm": 9882.74051060737,
      "learning_rate": 1.7729075268786876e-07,
      "loss": 1.7109,
      "step": 318464
    },
    {
      "epoch": 0.001155885193877734,
      "grad_norm": 9462.952816114006,
      "learning_rate": 1.7728183719260113e-07,
      "loss": 1.6975,
      "step": 318496
    },
    {
      "epoch": 0.0011560013282285704,
      "grad_norm": 8037.538802394673,
      "learning_rate": 1.772729230422101e-07,
      "loss": 1.7178,
      "step": 318528
    },
    {
      "epoch": 0.0011561174625794072,
      "grad_norm": 10852.339102700395,
      "learning_rate": 1.772640102363576e-07,
      "loss": 1.7004,
      "step": 318560
    },
    {
      "epoch": 0.001156233596930244,
      "grad_norm": 10415.873079103834,
      "learning_rate": 1.772550987747057e-07,
      "loss": 1.6852,
      "step": 318592
    },
    {
      "epoch": 0.0011563497312810807,
      "grad_norm": 12553.873027874704,
      "learning_rate": 1.7724618865691647e-07,
      "loss": 1.6694,
      "step": 318624
    },
    {
      "epoch": 0.0011564658656319172,
      "grad_norm": 8687.589193786733,
      "learning_rate": 1.7723727988265226e-07,
      "loss": 1.6907,
      "step": 318656
    },
    {
      "epoch": 0.001156581999982754,
      "grad_norm": 8928.183017837391,
      "learning_rate": 1.7722837245157542e-07,
      "loss": 1.6982,
      "step": 318688
    },
    {
      "epoch": 0.0011566981343335907,
      "grad_norm": 11529.387668042047,
      "learning_rate": 1.7721946636334848e-07,
      "loss": 1.707,
      "step": 318720
    },
    {
      "epoch": 0.0011568142686844275,
      "grad_norm": 9859.977180500977,
      "learning_rate": 1.772105616176341e-07,
      "loss": 1.6907,
      "step": 318752
    },
    {
      "epoch": 0.0011569304030352643,
      "grad_norm": 10835.809337562194,
      "learning_rate": 1.7720165821409494e-07,
      "loss": 1.6886,
      "step": 318784
    },
    {
      "epoch": 0.0011570465373861008,
      "grad_norm": 10086.610332515082,
      "learning_rate": 1.7719275615239394e-07,
      "loss": 1.6845,
      "step": 318816
    },
    {
      "epoch": 0.0011571626717369375,
      "grad_norm": 10708.836164588569,
      "learning_rate": 1.7718385543219405e-07,
      "loss": 1.6711,
      "step": 318848
    },
    {
      "epoch": 0.0011572788060877743,
      "grad_norm": 8171.1058003185835,
      "learning_rate": 1.771749560531584e-07,
      "loss": 1.6805,
      "step": 318880
    },
    {
      "epoch": 0.001157394940438611,
      "grad_norm": 10082.087085519546,
      "learning_rate": 1.771660580149502e-07,
      "loss": 1.6888,
      "step": 318912
    },
    {
      "epoch": 0.0011575110747894476,
      "grad_norm": 11279.472505396696,
      "learning_rate": 1.7715716131723277e-07,
      "loss": 1.6948,
      "step": 318944
    },
    {
      "epoch": 0.0011576272091402843,
      "grad_norm": 10533.017041664749,
      "learning_rate": 1.7714826595966962e-07,
      "loss": 1.6803,
      "step": 318976
    },
    {
      "epoch": 0.001157743343491121,
      "grad_norm": 13154.514054118456,
      "learning_rate": 1.7713937194192425e-07,
      "loss": 1.7008,
      "step": 319008
    },
    {
      "epoch": 0.0011578594778419579,
      "grad_norm": 8440.995438927805,
      "learning_rate": 1.771304792636604e-07,
      "loss": 1.7167,
      "step": 319040
    },
    {
      "epoch": 0.0011579756121927946,
      "grad_norm": 7984.434356922223,
      "learning_rate": 1.7712158792454194e-07,
      "loss": 1.721,
      "step": 319072
    },
    {
      "epoch": 0.0011580917465436311,
      "grad_norm": 9519.180006702258,
      "learning_rate": 1.7711269792423269e-07,
      "loss": 1.7138,
      "step": 319104
    },
    {
      "epoch": 0.001158207880894468,
      "grad_norm": 9119.224747751314,
      "learning_rate": 1.7710380926239677e-07,
      "loss": 1.6984,
      "step": 319136
    },
    {
      "epoch": 0.0011583240152453047,
      "grad_norm": 8705.660342558742,
      "learning_rate": 1.7709492193869831e-07,
      "loss": 1.6866,
      "step": 319168
    },
    {
      "epoch": 0.0011584401495961414,
      "grad_norm": 10523.36695169374,
      "learning_rate": 1.7708603595280163e-07,
      "loss": 1.6931,
      "step": 319200
    },
    {
      "epoch": 0.001158556283946978,
      "grad_norm": 8867.748756025961,
      "learning_rate": 1.7707715130437113e-07,
      "loss": 1.7032,
      "step": 319232
    },
    {
      "epoch": 0.0011586724182978147,
      "grad_norm": 11823.376167575825,
      "learning_rate": 1.7706826799307132e-07,
      "loss": 1.7252,
      "step": 319264
    },
    {
      "epoch": 0.0011587885526486515,
      "grad_norm": 10075.418700977147,
      "learning_rate": 1.7705938601856682e-07,
      "loss": 1.7037,
      "step": 319296
    },
    {
      "epoch": 0.0011589046869994882,
      "grad_norm": 15940.399744046572,
      "learning_rate": 1.7705050538052245e-07,
      "loss": 1.6872,
      "step": 319328
    },
    {
      "epoch": 0.001159020821350325,
      "grad_norm": 23299.44325515097,
      "learning_rate": 1.7704162607860301e-07,
      "loss": 1.6854,
      "step": 319360
    },
    {
      "epoch": 0.0011591369557011615,
      "grad_norm": 8894.014279277946,
      "learning_rate": 1.7703302552869889e-07,
      "loss": 1.6962,
      "step": 319392
    },
    {
      "epoch": 0.0011592530900519983,
      "grad_norm": 9148.477141032818,
      "learning_rate": 1.7702414885629654e-07,
      "loss": 1.6938,
      "step": 319424
    },
    {
      "epoch": 0.001159369224402835,
      "grad_norm": 9223.216792421177,
      "learning_rate": 1.7701527351902496e-07,
      "loss": 1.6982,
      "step": 319456
    },
    {
      "epoch": 0.0011594853587536718,
      "grad_norm": 9987.59630742052,
      "learning_rate": 1.7700639951654947e-07,
      "loss": 1.6958,
      "step": 319488
    },
    {
      "epoch": 0.0011596014931045083,
      "grad_norm": 9180.790162072108,
      "learning_rate": 1.7699752684853554e-07,
      "loss": 1.6729,
      "step": 319520
    },
    {
      "epoch": 0.001159717627455345,
      "grad_norm": 10405.352853219347,
      "learning_rate": 1.769886555146487e-07,
      "loss": 1.6986,
      "step": 319552
    },
    {
      "epoch": 0.0011598337618061818,
      "grad_norm": 8909.2485653954,
      "learning_rate": 1.769797855145547e-07,
      "loss": 1.7173,
      "step": 319584
    },
    {
      "epoch": 0.0011599498961570186,
      "grad_norm": 9323.076745366843,
      "learning_rate": 1.7697091684791933e-07,
      "loss": 1.7082,
      "step": 319616
    },
    {
      "epoch": 0.0011600660305078553,
      "grad_norm": 9458.891901274694,
      "learning_rate": 1.769620495144085e-07,
      "loss": 1.6849,
      "step": 319648
    },
    {
      "epoch": 0.0011601821648586919,
      "grad_norm": 10907.760906803926,
      "learning_rate": 1.7695318351368828e-07,
      "loss": 1.6805,
      "step": 319680
    },
    {
      "epoch": 0.0011602982992095286,
      "grad_norm": 9917.90522237433,
      "learning_rate": 1.769443188454248e-07,
      "loss": 1.6784,
      "step": 319712
    },
    {
      "epoch": 0.0011604144335603654,
      "grad_norm": 9640.412024389829,
      "learning_rate": 1.7693545550928438e-07,
      "loss": 1.6873,
      "step": 319744
    },
    {
      "epoch": 0.0011605305679112021,
      "grad_norm": 10804.74136664085,
      "learning_rate": 1.769265935049334e-07,
      "loss": 1.6972,
      "step": 319776
    },
    {
      "epoch": 0.0011606467022620387,
      "grad_norm": 9541.23933249764,
      "learning_rate": 1.7691773283203835e-07,
      "loss": 1.7041,
      "step": 319808
    },
    {
      "epoch": 0.0011607628366128754,
      "grad_norm": 10352.13195433675,
      "learning_rate": 1.7690887349026587e-07,
      "loss": 1.6828,
      "step": 319840
    },
    {
      "epoch": 0.0011608789709637122,
      "grad_norm": 8350.112574091441,
      "learning_rate": 1.7690001547928272e-07,
      "loss": 1.6709,
      "step": 319872
    },
    {
      "epoch": 0.001160995105314549,
      "grad_norm": 10507.182876489778,
      "learning_rate": 1.768911587987558e-07,
      "loss": 1.6837,
      "step": 319904
    },
    {
      "epoch": 0.0011611112396653857,
      "grad_norm": 9932.136728821246,
      "learning_rate": 1.7688230344835198e-07,
      "loss": 1.6937,
      "step": 319936
    },
    {
      "epoch": 0.0011612273740162222,
      "grad_norm": 11389.726247807714,
      "learning_rate": 1.7687344942773847e-07,
      "loss": 1.6978,
      "step": 319968
    },
    {
      "epoch": 0.001161343508367059,
      "grad_norm": 10942.68513665636,
      "learning_rate": 1.7686459673658245e-07,
      "loss": 1.6802,
      "step": 320000
    },
    {
      "epoch": 0.0011614596427178957,
      "grad_norm": 10333.606146936316,
      "learning_rate": 1.768557453745512e-07,
      "loss": 1.713,
      "step": 320032
    },
    {
      "epoch": 0.0011615757770687325,
      "grad_norm": 9919.671970382891,
      "learning_rate": 1.7684689534131223e-07,
      "loss": 1.7207,
      "step": 320064
    },
    {
      "epoch": 0.001161691911419569,
      "grad_norm": 9693.529181882108,
      "learning_rate": 1.7683804663653305e-07,
      "loss": 1.7216,
      "step": 320096
    },
    {
      "epoch": 0.0011618080457704058,
      "grad_norm": 10096.027535620136,
      "learning_rate": 1.7682919925988142e-07,
      "loss": 1.7356,
      "step": 320128
    },
    {
      "epoch": 0.0011619241801212425,
      "grad_norm": 10318.852649398576,
      "learning_rate": 1.7682035321102505e-07,
      "loss": 1.7393,
      "step": 320160
    },
    {
      "epoch": 0.0011620403144720793,
      "grad_norm": 8438.510058061198,
      "learning_rate": 1.768115084896319e-07,
      "loss": 1.6991,
      "step": 320192
    },
    {
      "epoch": 0.001162156448822916,
      "grad_norm": 11552.62498309367,
      "learning_rate": 1.7680266509536993e-07,
      "loss": 1.6709,
      "step": 320224
    },
    {
      "epoch": 0.0011622725831737526,
      "grad_norm": 8859.406526398934,
      "learning_rate": 1.767938230279074e-07,
      "loss": 1.68,
      "step": 320256
    },
    {
      "epoch": 0.0011623887175245893,
      "grad_norm": 10660.600358328793,
      "learning_rate": 1.767849822869125e-07,
      "loss": 1.6929,
      "step": 320288
    },
    {
      "epoch": 0.001162504851875426,
      "grad_norm": 11540.084228462112,
      "learning_rate": 1.7677614287205357e-07,
      "loss": 1.6991,
      "step": 320320
    },
    {
      "epoch": 0.0011626209862262628,
      "grad_norm": 9124.772435518598,
      "learning_rate": 1.7676730478299916e-07,
      "loss": 1.6797,
      "step": 320352
    },
    {
      "epoch": 0.0011627371205770994,
      "grad_norm": 8258.88297531815,
      "learning_rate": 1.767584680194179e-07,
      "loss": 1.6853,
      "step": 320384
    },
    {
      "epoch": 0.0011628532549279361,
      "grad_norm": 23399.37811139433,
      "learning_rate": 1.7674963258097848e-07,
      "loss": 1.7066,
      "step": 320416
    },
    {
      "epoch": 0.0011629693892787729,
      "grad_norm": 10964.928818738406,
      "learning_rate": 1.7674107451335063e-07,
      "loss": 1.709,
      "step": 320448
    },
    {
      "epoch": 0.0011630855236296096,
      "grad_norm": 8648.899583183978,
      "learning_rate": 1.7673224168281652e-07,
      "loss": 1.7001,
      "step": 320480
    },
    {
      "epoch": 0.0011632016579804464,
      "grad_norm": 10325.633539885095,
      "learning_rate": 1.767234101764415e-07,
      "loss": 1.6986,
      "step": 320512
    },
    {
      "epoch": 0.001163317792331283,
      "grad_norm": 7897.829448652332,
      "learning_rate": 1.767145799938947e-07,
      "loss": 1.6772,
      "step": 320544
    },
    {
      "epoch": 0.0011634339266821197,
      "grad_norm": 10331.83178337704,
      "learning_rate": 1.7670575113484544e-07,
      "loss": 1.6782,
      "step": 320576
    },
    {
      "epoch": 0.0011635500610329564,
      "grad_norm": 9736.581124809672,
      "learning_rate": 1.7669692359896315e-07,
      "loss": 1.6942,
      "step": 320608
    },
    {
      "epoch": 0.0011636661953837932,
      "grad_norm": 10167.059948677395,
      "learning_rate": 1.7668809738591732e-07,
      "loss": 1.7111,
      "step": 320640
    },
    {
      "epoch": 0.0011637823297346297,
      "grad_norm": 10561.757240156583,
      "learning_rate": 1.7667927249537763e-07,
      "loss": 1.6953,
      "step": 320672
    },
    {
      "epoch": 0.0011638984640854665,
      "grad_norm": 8900.154380683518,
      "learning_rate": 1.7667044892701384e-07,
      "loss": 1.7084,
      "step": 320704
    },
    {
      "epoch": 0.0011640145984363032,
      "grad_norm": 9012.784142538863,
      "learning_rate": 1.766616266804958e-07,
      "loss": 1.6945,
      "step": 320736
    },
    {
      "epoch": 0.00116413073278714,
      "grad_norm": 9836.614864881109,
      "learning_rate": 1.7665280575549354e-07,
      "loss": 1.7004,
      "step": 320768
    },
    {
      "epoch": 0.0011642468671379767,
      "grad_norm": 8823.694691001043,
      "learning_rate": 1.7664398615167714e-07,
      "loss": 1.7026,
      "step": 320800
    },
    {
      "epoch": 0.0011643630014888133,
      "grad_norm": 10435.841125659206,
      "learning_rate": 1.7663516786871685e-07,
      "loss": 1.6959,
      "step": 320832
    },
    {
      "epoch": 0.00116447913583965,
      "grad_norm": 9941.256258642567,
      "learning_rate": 1.76626350906283e-07,
      "loss": 1.7029,
      "step": 320864
    },
    {
      "epoch": 0.0011645952701904868,
      "grad_norm": 9448.251478448274,
      "learning_rate": 1.7661753526404606e-07,
      "loss": 1.687,
      "step": 320896
    },
    {
      "epoch": 0.0011647114045413235,
      "grad_norm": 9128.267524563464,
      "learning_rate": 1.7660872094167655e-07,
      "loss": 1.6945,
      "step": 320928
    },
    {
      "epoch": 0.00116482753889216,
      "grad_norm": 8691.760696199592,
      "learning_rate": 1.765999079388452e-07,
      "loss": 1.7178,
      "step": 320960
    },
    {
      "epoch": 0.0011649436732429968,
      "grad_norm": 8989.411771634448,
      "learning_rate": 1.7659109625522278e-07,
      "loss": 1.7182,
      "step": 320992
    },
    {
      "epoch": 0.0011650598075938336,
      "grad_norm": 8595.999534667275,
      "learning_rate": 1.7658228589048025e-07,
      "loss": 1.695,
      "step": 321024
    },
    {
      "epoch": 0.0011651759419446703,
      "grad_norm": 10109.620764400612,
      "learning_rate": 1.7657347684428857e-07,
      "loss": 1.7134,
      "step": 321056
    },
    {
      "epoch": 0.001165292076295507,
      "grad_norm": 10182.389110616428,
      "learning_rate": 1.7656466911631894e-07,
      "loss": 1.694,
      "step": 321088
    },
    {
      "epoch": 0.0011654082106463436,
      "grad_norm": 10241.218872770956,
      "learning_rate": 1.7655586270624258e-07,
      "loss": 1.6901,
      "step": 321120
    },
    {
      "epoch": 0.0011655243449971804,
      "grad_norm": 8671.942804239428,
      "learning_rate": 1.7654705761373092e-07,
      "loss": 1.702,
      "step": 321152
    },
    {
      "epoch": 0.0011656404793480171,
      "grad_norm": 9213.898089299664,
      "learning_rate": 1.7653825383845542e-07,
      "loss": 1.7053,
      "step": 321184
    },
    {
      "epoch": 0.0011657566136988539,
      "grad_norm": 9432.324209864713,
      "learning_rate": 1.7652945138008767e-07,
      "loss": 1.7001,
      "step": 321216
    },
    {
      "epoch": 0.0011658727480496904,
      "grad_norm": 10153.345064558775,
      "learning_rate": 1.765206502382994e-07,
      "loss": 1.6918,
      "step": 321248
    },
    {
      "epoch": 0.0011659888824005272,
      "grad_norm": 10361.113067619714,
      "learning_rate": 1.765118504127624e-07,
      "loss": 1.6988,
      "step": 321280
    },
    {
      "epoch": 0.001166105016751364,
      "grad_norm": 15136.064085488011,
      "learning_rate": 1.7650305190314872e-07,
      "loss": 1.7105,
      "step": 321312
    },
    {
      "epoch": 0.0011662211511022007,
      "grad_norm": 9233.80593255024,
      "learning_rate": 1.764942547091303e-07,
      "loss": 1.704,
      "step": 321344
    },
    {
      "epoch": 0.0011663372854530374,
      "grad_norm": 9067.645559901424,
      "learning_rate": 1.7648545883037938e-07,
      "loss": 1.674,
      "step": 321376
    },
    {
      "epoch": 0.001166453419803874,
      "grad_norm": 9587.040836462522,
      "learning_rate": 1.7647666426656825e-07,
      "loss": 1.7001,
      "step": 321408
    },
    {
      "epoch": 0.0011665695541547107,
      "grad_norm": 9181.6290493572,
      "learning_rate": 1.7646814578651113e-07,
      "loss": 1.7078,
      "step": 321440
    },
    {
      "epoch": 0.0011666856885055475,
      "grad_norm": 10050.253529140446,
      "learning_rate": 1.7645935381053045e-07,
      "loss": 1.7015,
      "step": 321472
    },
    {
      "epoch": 0.0011668018228563842,
      "grad_norm": 8808.05699345775,
      "learning_rate": 1.7645056314851738e-07,
      "loss": 1.7058,
      "step": 321504
    },
    {
      "epoch": 0.0011669179572072208,
      "grad_norm": 10124.118924627466,
      "learning_rate": 1.7644177380014457e-07,
      "loss": 1.7125,
      "step": 321536
    },
    {
      "epoch": 0.0011670340915580575,
      "grad_norm": 9433.210694138024,
      "learning_rate": 1.7643298576508495e-07,
      "loss": 1.6799,
      "step": 321568
    },
    {
      "epoch": 0.0011671502259088943,
      "grad_norm": 12874.991572812776,
      "learning_rate": 1.7642419904301149e-07,
      "loss": 1.6762,
      "step": 321600
    },
    {
      "epoch": 0.001167266360259731,
      "grad_norm": 10506.824163371157,
      "learning_rate": 1.764154136335972e-07,
      "loss": 1.6866,
      "step": 321632
    },
    {
      "epoch": 0.0011673824946105678,
      "grad_norm": 8485.677226951304,
      "learning_rate": 1.7640662953651538e-07,
      "loss": 1.6992,
      "step": 321664
    },
    {
      "epoch": 0.0011674986289614043,
      "grad_norm": 8779.339952411001,
      "learning_rate": 1.7639784675143928e-07,
      "loss": 1.691,
      "step": 321696
    },
    {
      "epoch": 0.001167614763312241,
      "grad_norm": 7869.090417576863,
      "learning_rate": 1.7638906527804238e-07,
      "loss": 1.6997,
      "step": 321728
    },
    {
      "epoch": 0.0011677308976630778,
      "grad_norm": 8188.449792237844,
      "learning_rate": 1.7638028511599817e-07,
      "loss": 1.7101,
      "step": 321760
    },
    {
      "epoch": 0.0011678470320139146,
      "grad_norm": 9609.993028093204,
      "learning_rate": 1.7637150626498028e-07,
      "loss": 1.7242,
      "step": 321792
    },
    {
      "epoch": 0.0011679631663647511,
      "grad_norm": 10184.5070572905,
      "learning_rate": 1.7636272872466257e-07,
      "loss": 1.7212,
      "step": 321824
    },
    {
      "epoch": 0.0011680793007155879,
      "grad_norm": 11088.06709936407,
      "learning_rate": 1.7635395249471886e-07,
      "loss": 1.7201,
      "step": 321856
    },
    {
      "epoch": 0.0011681954350664246,
      "grad_norm": 9384.665897089784,
      "learning_rate": 1.7634517757482318e-07,
      "loss": 1.7146,
      "step": 321888
    },
    {
      "epoch": 0.0011683115694172614,
      "grad_norm": 10672.035981948338,
      "learning_rate": 1.7633640396464956e-07,
      "loss": 1.6964,
      "step": 321920
    },
    {
      "epoch": 0.0011684277037680981,
      "grad_norm": 10149.026357242354,
      "learning_rate": 1.7632763166387234e-07,
      "loss": 1.6984,
      "step": 321952
    },
    {
      "epoch": 0.0011685438381189347,
      "grad_norm": 7324.738493625558,
      "learning_rate": 1.7631886067216578e-07,
      "loss": 1.6935,
      "step": 321984
    },
    {
      "epoch": 0.0011686599724697714,
      "grad_norm": 9484.984976266436,
      "learning_rate": 1.763100909892043e-07,
      "loss": 1.6901,
      "step": 322016
    },
    {
      "epoch": 0.0011687761068206082,
      "grad_norm": 12121.820820322333,
      "learning_rate": 1.763013226146626e-07,
      "loss": 1.6661,
      "step": 322048
    },
    {
      "epoch": 0.001168892241171445,
      "grad_norm": 9698.205607224461,
      "learning_rate": 1.7629255554821518e-07,
      "loss": 1.6909,
      "step": 322080
    },
    {
      "epoch": 0.0011690083755222815,
      "grad_norm": 9568.393595583326,
      "learning_rate": 1.7628378978953695e-07,
      "loss": 1.6933,
      "step": 322112
    },
    {
      "epoch": 0.0011691245098731182,
      "grad_norm": 10471.33630440738,
      "learning_rate": 1.7627502533830278e-07,
      "loss": 1.6987,
      "step": 322144
    },
    {
      "epoch": 0.001169240644223955,
      "grad_norm": 9210.77814302353,
      "learning_rate": 1.762662621941877e-07,
      "loss": 1.7083,
      "step": 322176
    },
    {
      "epoch": 0.0011693567785747917,
      "grad_norm": 9672.963558289672,
      "learning_rate": 1.762575003568668e-07,
      "loss": 1.7153,
      "step": 322208
    },
    {
      "epoch": 0.0011694729129256285,
      "grad_norm": 10499.23225764627,
      "learning_rate": 1.7624873982601537e-07,
      "loss": 1.696,
      "step": 322240
    },
    {
      "epoch": 0.001169589047276465,
      "grad_norm": 9430.66222489174,
      "learning_rate": 1.7623998060130872e-07,
      "loss": 1.6928,
      "step": 322272
    },
    {
      "epoch": 0.0011697051816273018,
      "grad_norm": 11123.8408834359,
      "learning_rate": 1.7623122268242233e-07,
      "loss": 1.7106,
      "step": 322304
    },
    {
      "epoch": 0.0011698213159781385,
      "grad_norm": 8927.51118733547,
      "learning_rate": 1.762224660690318e-07,
      "loss": 1.7222,
      "step": 322336
    },
    {
      "epoch": 0.0011699374503289753,
      "grad_norm": 11245.34926091671,
      "learning_rate": 1.762137107608128e-07,
      "loss": 1.6912,
      "step": 322368
    },
    {
      "epoch": 0.0011700535846798118,
      "grad_norm": 9131.835412445846,
      "learning_rate": 1.7620495675744117e-07,
      "loss": 1.674,
      "step": 322400
    },
    {
      "epoch": 0.0011701697190306486,
      "grad_norm": 11947.619679249921,
      "learning_rate": 1.7619620405859279e-07,
      "loss": 1.6879,
      "step": 322432
    },
    {
      "epoch": 0.0011702858533814853,
      "grad_norm": 21076.253177450682,
      "learning_rate": 1.761874526639437e-07,
      "loss": 1.7033,
      "step": 322464
    },
    {
      "epoch": 0.001170401987732322,
      "grad_norm": 20777.279513930593,
      "learning_rate": 1.7617870257317005e-07,
      "loss": 1.7029,
      "step": 322496
    },
    {
      "epoch": 0.0011705181220831589,
      "grad_norm": 9734.232378569972,
      "learning_rate": 1.7617022716582051e-07,
      "loss": 1.6928,
      "step": 322528
    },
    {
      "epoch": 0.0011706342564339954,
      "grad_norm": 10900.250822802198,
      "learning_rate": 1.7616147964110566e-07,
      "loss": 1.6985,
      "step": 322560
    },
    {
      "epoch": 0.0011707503907848321,
      "grad_norm": 9076.33185819029,
      "learning_rate": 1.761527334193055e-07,
      "loss": 1.6775,
      "step": 322592
    },
    {
      "epoch": 0.001170866525135669,
      "grad_norm": 9441.941325807951,
      "learning_rate": 1.7614398850009654e-07,
      "loss": 1.6858,
      "step": 322624
    },
    {
      "epoch": 0.0011709826594865057,
      "grad_norm": 8835.751467758699,
      "learning_rate": 1.7613524488315556e-07,
      "loss": 1.6988,
      "step": 322656
    },
    {
      "epoch": 0.0011710987938373422,
      "grad_norm": 12095.917823794935,
      "learning_rate": 1.7612650256815933e-07,
      "loss": 1.7125,
      "step": 322688
    },
    {
      "epoch": 0.001171214928188179,
      "grad_norm": 11101.036348017242,
      "learning_rate": 1.7611776155478482e-07,
      "loss": 1.7004,
      "step": 322720
    },
    {
      "epoch": 0.0011713310625390157,
      "grad_norm": 9125.041260180691,
      "learning_rate": 1.7610902184270903e-07,
      "loss": 1.7042,
      "step": 322752
    },
    {
      "epoch": 0.0011714471968898525,
      "grad_norm": 9002.408566600385,
      "learning_rate": 1.7610028343160912e-07,
      "loss": 1.7246,
      "step": 322784
    },
    {
      "epoch": 0.0011715633312406892,
      "grad_norm": 8216.718444732056,
      "learning_rate": 1.7609154632116237e-07,
      "loss": 1.7276,
      "step": 322816
    },
    {
      "epoch": 0.0011716794655915257,
      "grad_norm": 10778.919611909163,
      "learning_rate": 1.7608281051104616e-07,
      "loss": 1.7185,
      "step": 322848
    },
    {
      "epoch": 0.0011717955999423625,
      "grad_norm": 8547.49202982957,
      "learning_rate": 1.7607407600093796e-07,
      "loss": 1.7184,
      "step": 322880
    },
    {
      "epoch": 0.0011719117342931993,
      "grad_norm": 8963.2650301104,
      "learning_rate": 1.7606534279051537e-07,
      "loss": 1.7064,
      "step": 322912
    },
    {
      "epoch": 0.001172027868644036,
      "grad_norm": 9470.446029622892,
      "learning_rate": 1.760566108794561e-07,
      "loss": 1.6811,
      "step": 322944
    },
    {
      "epoch": 0.0011721440029948725,
      "grad_norm": 9905.375712207993,
      "learning_rate": 1.7604788026743803e-07,
      "loss": 1.6962,
      "step": 322976
    },
    {
      "epoch": 0.0011722601373457093,
      "grad_norm": 9621.949490617793,
      "learning_rate": 1.7603915095413904e-07,
      "loss": 1.699,
      "step": 323008
    },
    {
      "epoch": 0.001172376271696546,
      "grad_norm": 8485.084796276346,
      "learning_rate": 1.7603042293923713e-07,
      "loss": 1.7306,
      "step": 323040
    },
    {
      "epoch": 0.0011724924060473828,
      "grad_norm": 13713.974916121146,
      "learning_rate": 1.760216962224106e-07,
      "loss": 1.7001,
      "step": 323072
    },
    {
      "epoch": 0.0011726085403982196,
      "grad_norm": 7811.964925676511,
      "learning_rate": 1.760129708033376e-07,
      "loss": 1.707,
      "step": 323104
    },
    {
      "epoch": 0.001172724674749056,
      "grad_norm": 11415.122250768933,
      "learning_rate": 1.7600424668169655e-07,
      "loss": 1.6976,
      "step": 323136
    },
    {
      "epoch": 0.0011728408090998929,
      "grad_norm": 8084.245295635209,
      "learning_rate": 1.7599552385716592e-07,
      "loss": 1.7074,
      "step": 323168
    },
    {
      "epoch": 0.0011729569434507296,
      "grad_norm": 10694.825851784592,
      "learning_rate": 1.7598680232942437e-07,
      "loss": 1.706,
      "step": 323200
    },
    {
      "epoch": 0.0011730730778015664,
      "grad_norm": 12862.445179669377,
      "learning_rate": 1.759780820981506e-07,
      "loss": 1.7058,
      "step": 323232
    },
    {
      "epoch": 0.001173189212152403,
      "grad_norm": 9642.019809147874,
      "learning_rate": 1.759693631630234e-07,
      "loss": 1.7,
      "step": 323264
    },
    {
      "epoch": 0.0011733053465032397,
      "grad_norm": 9001.623075868041,
      "learning_rate": 1.7596064552372175e-07,
      "loss": 1.6849,
      "step": 323296
    },
    {
      "epoch": 0.0011734214808540764,
      "grad_norm": 9855.66720217358,
      "learning_rate": 1.7595192917992467e-07,
      "loss": 1.707,
      "step": 323328
    },
    {
      "epoch": 0.0011735376152049132,
      "grad_norm": 8493.597353300896,
      "learning_rate": 1.7594321413131137e-07,
      "loss": 1.7184,
      "step": 323360
    },
    {
      "epoch": 0.00117365374955575,
      "grad_norm": 10301.545709261305,
      "learning_rate": 1.759345003775611e-07,
      "loss": 1.7188,
      "step": 323392
    },
    {
      "epoch": 0.0011737698839065864,
      "grad_norm": 12470.295746292468,
      "learning_rate": 1.7592578791835323e-07,
      "loss": 1.7055,
      "step": 323424
    },
    {
      "epoch": 0.0011738860182574232,
      "grad_norm": 7815.246125362912,
      "learning_rate": 1.7591707675336726e-07,
      "loss": 1.7081,
      "step": 323456
    },
    {
      "epoch": 0.00117400215260826,
      "grad_norm": 9559.07819823648,
      "learning_rate": 1.759083668822828e-07,
      "loss": 1.697,
      "step": 323488
    },
    {
      "epoch": 0.0011741182869590967,
      "grad_norm": 9139.784023706468,
      "learning_rate": 1.7589965830477962e-07,
      "loss": 1.6922,
      "step": 323520
    },
    {
      "epoch": 0.0011742344213099332,
      "grad_norm": 15388.89781628301,
      "learning_rate": 1.7589095102053746e-07,
      "loss": 1.7097,
      "step": 323552
    },
    {
      "epoch": 0.00117435055566077,
      "grad_norm": 17567.065776617335,
      "learning_rate": 1.7588224502923632e-07,
      "loss": 1.7293,
      "step": 323584
    },
    {
      "epoch": 0.0011744666900116068,
      "grad_norm": 9078.207091711447,
      "learning_rate": 1.758738123328271e-07,
      "loss": 1.6925,
      "step": 323616
    },
    {
      "epoch": 0.0011745828243624435,
      "grad_norm": 9323.797187841443,
      "learning_rate": 1.7586510888606866e-07,
      "loss": 1.6963,
      "step": 323648
    },
    {
      "epoch": 0.0011746989587132803,
      "grad_norm": 11569.231089402612,
      "learning_rate": 1.758564067313017e-07,
      "loss": 1.7043,
      "step": 323680
    },
    {
      "epoch": 0.0011748150930641168,
      "grad_norm": 10540.496003509512,
      "learning_rate": 1.7584770586820656e-07,
      "loss": 1.6931,
      "step": 323712
    },
    {
      "epoch": 0.0011749312274149536,
      "grad_norm": 8654.336369705074,
      "learning_rate": 1.758390062964638e-07,
      "loss": 1.6828,
      "step": 323744
    },
    {
      "epoch": 0.0011750473617657903,
      "grad_norm": 8521.635406422878,
      "learning_rate": 1.75830308015754e-07,
      "loss": 1.6777,
      "step": 323776
    },
    {
      "epoch": 0.001175163496116627,
      "grad_norm": 11387.091990495202,
      "learning_rate": 1.7582161102575786e-07,
      "loss": 1.6848,
      "step": 323808
    },
    {
      "epoch": 0.0011752796304674636,
      "grad_norm": 8595.683102581203,
      "learning_rate": 1.7581291532615623e-07,
      "loss": 1.6967,
      "step": 323840
    },
    {
      "epoch": 0.0011753957648183004,
      "grad_norm": 8052.674710926799,
      "learning_rate": 1.7580422091662995e-07,
      "loss": 1.6977,
      "step": 323872
    },
    {
      "epoch": 0.0011755118991691371,
      "grad_norm": 7947.080721875172,
      "learning_rate": 1.757955277968602e-07,
      "loss": 1.7113,
      "step": 323904
    },
    {
      "epoch": 0.0011756280335199739,
      "grad_norm": 9249.670480617135,
      "learning_rate": 1.7578683596652804e-07,
      "loss": 1.7327,
      "step": 323936
    },
    {
      "epoch": 0.0011757441678708106,
      "grad_norm": 9714.616616212912,
      "learning_rate": 1.7577814542531475e-07,
      "loss": 1.7035,
      "step": 323968
    },
    {
      "epoch": 0.0011758603022216472,
      "grad_norm": 10625.529069180508,
      "learning_rate": 1.7576945617290172e-07,
      "loss": 1.6741,
      "step": 324000
    },
    {
      "epoch": 0.001175976436572484,
      "grad_norm": 9416.813049009734,
      "learning_rate": 1.7576076820897045e-07,
      "loss": 1.6841,
      "step": 324032
    },
    {
      "epoch": 0.0011760925709233207,
      "grad_norm": 9371.276540578663,
      "learning_rate": 1.757520815332025e-07,
      "loss": 1.6954,
      "step": 324064
    },
    {
      "epoch": 0.0011762087052741574,
      "grad_norm": 13298.123627038516,
      "learning_rate": 1.7574339614527957e-07,
      "loss": 1.6724,
      "step": 324096
    },
    {
      "epoch": 0.001176324839624994,
      "grad_norm": 10759.726204694987,
      "learning_rate": 1.7573471204488355e-07,
      "loss": 1.696,
      "step": 324128
    },
    {
      "epoch": 0.0011764409739758307,
      "grad_norm": 9748.325804978002,
      "learning_rate": 1.7572602923169627e-07,
      "loss": 1.682,
      "step": 324160
    },
    {
      "epoch": 0.0011765571083266675,
      "grad_norm": 9412.324048820248,
      "learning_rate": 1.757173477053998e-07,
      "loss": 1.695,
      "step": 324192
    },
    {
      "epoch": 0.0011766732426775042,
      "grad_norm": 10767.697432599041,
      "learning_rate": 1.7570866746567633e-07,
      "loss": 1.6912,
      "step": 324224
    },
    {
      "epoch": 0.001176789377028341,
      "grad_norm": 10065.55830542946,
      "learning_rate": 1.7569998851220802e-07,
      "loss": 1.6937,
      "step": 324256
    },
    {
      "epoch": 0.0011769055113791775,
      "grad_norm": 11737.869483002443,
      "learning_rate": 1.7569131084467734e-07,
      "loss": 1.6864,
      "step": 324288
    },
    {
      "epoch": 0.0011770216457300143,
      "grad_norm": 9600.37478435087,
      "learning_rate": 1.756826344627667e-07,
      "loss": 1.6788,
      "step": 324320
    },
    {
      "epoch": 0.001177137780080851,
      "grad_norm": 10102.507015587764,
      "learning_rate": 1.7567395936615868e-07,
      "loss": 1.6818,
      "step": 324352
    },
    {
      "epoch": 0.0011772539144316878,
      "grad_norm": 9208.232729465519,
      "learning_rate": 1.75665285554536e-07,
      "loss": 1.7013,
      "step": 324384
    },
    {
      "epoch": 0.0011773700487825243,
      "grad_norm": 8991.060004248664,
      "learning_rate": 1.7565661302758148e-07,
      "loss": 1.7094,
      "step": 324416
    },
    {
      "epoch": 0.001177486183133361,
      "grad_norm": 9870.185611223327,
      "learning_rate": 1.7564794178497805e-07,
      "loss": 1.7061,
      "step": 324448
    },
    {
      "epoch": 0.0011776023174841978,
      "grad_norm": 9073.277357162626,
      "learning_rate": 1.7563927182640863e-07,
      "loss": 1.7279,
      "step": 324480
    },
    {
      "epoch": 0.0011777184518350346,
      "grad_norm": 8961.397882027111,
      "learning_rate": 1.756306031515564e-07,
      "loss": 1.7293,
      "step": 324512
    },
    {
      "epoch": 0.0011778345861858713,
      "grad_norm": 11945.572568947879,
      "learning_rate": 1.7562193576010467e-07,
      "loss": 1.7148,
      "step": 324544
    },
    {
      "epoch": 0.0011779507205367079,
      "grad_norm": 10924.44909366143,
      "learning_rate": 1.7561326965173676e-07,
      "loss": 1.7081,
      "step": 324576
    },
    {
      "epoch": 0.0011780668548875446,
      "grad_norm": 19008.998290283474,
      "learning_rate": 1.7560460482613607e-07,
      "loss": 1.6894,
      "step": 324608
    },
    {
      "epoch": 0.0011781829892383814,
      "grad_norm": 11409.928132990146,
      "learning_rate": 1.7559621199930068e-07,
      "loss": 1.67,
      "step": 324640
    },
    {
      "epoch": 0.0011782991235892181,
      "grad_norm": 9502.027152139695,
      "learning_rate": 1.7558754969822342e-07,
      "loss": 1.6852,
      "step": 324672
    },
    {
      "epoch": 0.0011784152579400547,
      "grad_norm": 9902.287412512323,
      "learning_rate": 1.7557888867897436e-07,
      "loss": 1.677,
      "step": 324704
    },
    {
      "epoch": 0.0011785313922908914,
      "grad_norm": 8984.31255021774,
      "learning_rate": 1.7557022894123737e-07,
      "loss": 1.6929,
      "step": 324736
    },
    {
      "epoch": 0.0011786475266417282,
      "grad_norm": 9716.230544815206,
      "learning_rate": 1.7556157048469644e-07,
      "loss": 1.6792,
      "step": 324768
    },
    {
      "epoch": 0.001178763660992565,
      "grad_norm": 10197.960580429795,
      "learning_rate": 1.7555291330903567e-07,
      "loss": 1.6817,
      "step": 324800
    },
    {
      "epoch": 0.0011788797953434017,
      "grad_norm": 9036.79722025453,
      "learning_rate": 1.7554425741393934e-07,
      "loss": 1.7024,
      "step": 324832
    },
    {
      "epoch": 0.0011789959296942382,
      "grad_norm": 8324.853632346938,
      "learning_rate": 1.7553560279909179e-07,
      "loss": 1.6919,
      "step": 324864
    },
    {
      "epoch": 0.001179112064045075,
      "grad_norm": 8763.558181469441,
      "learning_rate": 1.7552694946417737e-07,
      "loss": 1.6927,
      "step": 324896
    },
    {
      "epoch": 0.0011792281983959117,
      "grad_norm": 11506.772962042834,
      "learning_rate": 1.755182974088807e-07,
      "loss": 1.7011,
      "step": 324928
    },
    {
      "epoch": 0.0011793443327467485,
      "grad_norm": 9539.234350827115,
      "learning_rate": 1.7550964663288645e-07,
      "loss": 1.7108,
      "step": 324960
    },
    {
      "epoch": 0.001179460467097585,
      "grad_norm": 13881.228187736127,
      "learning_rate": 1.7550099713587932e-07,
      "loss": 1.6831,
      "step": 324992
    },
    {
      "epoch": 0.0011795766014484218,
      "grad_norm": 9092.61040625848,
      "learning_rate": 1.7549234891754422e-07,
      "loss": 1.6925,
      "step": 325024
    },
    {
      "epoch": 0.0011796927357992585,
      "grad_norm": 9272.18830697479,
      "learning_rate": 1.754837019775661e-07,
      "loss": 1.7005,
      "step": 325056
    },
    {
      "epoch": 0.0011798088701500953,
      "grad_norm": 9367.539271334816,
      "learning_rate": 1.7547505631563014e-07,
      "loss": 1.7006,
      "step": 325088
    },
    {
      "epoch": 0.001179925004500932,
      "grad_norm": 12909.000890851314,
      "learning_rate": 1.7546641193142144e-07,
      "loss": 1.6687,
      "step": 325120
    },
    {
      "epoch": 0.0011800411388517686,
      "grad_norm": 10251.168713858922,
      "learning_rate": 1.7545776882462537e-07,
      "loss": 1.6805,
      "step": 325152
    },
    {
      "epoch": 0.0011801572732026053,
      "grad_norm": 9837.686110056571,
      "learning_rate": 1.7544912699492734e-07,
      "loss": 1.6909,
      "step": 325184
    },
    {
      "epoch": 0.001180273407553442,
      "grad_norm": 10101.818054192028,
      "learning_rate": 1.7544048644201284e-07,
      "loss": 1.703,
      "step": 325216
    },
    {
      "epoch": 0.0011803895419042788,
      "grad_norm": 11032.502526625589,
      "learning_rate": 1.754318471655676e-07,
      "loss": 1.6994,
      "step": 325248
    },
    {
      "epoch": 0.0011805056762551154,
      "grad_norm": 9271.540109388516,
      "learning_rate": 1.754232091652772e-07,
      "loss": 1.7095,
      "step": 325280
    },
    {
      "epoch": 0.0011806218106059521,
      "grad_norm": 11028.03554582592,
      "learning_rate": 1.7541457244082765e-07,
      "loss": 1.7027,
      "step": 325312
    },
    {
      "epoch": 0.0011807379449567889,
      "grad_norm": 9197.413875650047,
      "learning_rate": 1.7540593699190484e-07,
      "loss": 1.6918,
      "step": 325344
    },
    {
      "epoch": 0.0011808540793076256,
      "grad_norm": 10554.845143345306,
      "learning_rate": 1.7539730281819484e-07,
      "loss": 1.7039,
      "step": 325376
    },
    {
      "epoch": 0.0011809702136584624,
      "grad_norm": 9595.437457458624,
      "learning_rate": 1.7538866991938378e-07,
      "loss": 1.718,
      "step": 325408
    },
    {
      "epoch": 0.001181086348009299,
      "grad_norm": 8992.208516265624,
      "learning_rate": 1.7538003829515806e-07,
      "loss": 1.7165,
      "step": 325440
    },
    {
      "epoch": 0.0011812024823601357,
      "grad_norm": 10259.783233577598,
      "learning_rate": 1.7537140794520397e-07,
      "loss": 1.6814,
      "step": 325472
    },
    {
      "epoch": 0.0011813186167109724,
      "grad_norm": 9244.85478523054,
      "learning_rate": 1.7536277886920803e-07,
      "loss": 1.6938,
      "step": 325504
    },
    {
      "epoch": 0.0011814347510618092,
      "grad_norm": 8928.879212980764,
      "learning_rate": 1.7535415106685688e-07,
      "loss": 1.7086,
      "step": 325536
    },
    {
      "epoch": 0.0011815508854126457,
      "grad_norm": 10975.09052354467,
      "learning_rate": 1.7534552453783726e-07,
      "loss": 1.7207,
      "step": 325568
    },
    {
      "epoch": 0.0011816670197634825,
      "grad_norm": 10321.522174563208,
      "learning_rate": 1.7533689928183587e-07,
      "loss": 1.7095,
      "step": 325600
    },
    {
      "epoch": 0.0011817831541143192,
      "grad_norm": 8968.894580716178,
      "learning_rate": 1.753282752985398e-07,
      "loss": 1.6877,
      "step": 325632
    },
    {
      "epoch": 0.001181899288465156,
      "grad_norm": 10587.210208548804,
      "learning_rate": 1.7531992202809496e-07,
      "loss": 1.6853,
      "step": 325664
    },
    {
      "epoch": 0.0011820154228159927,
      "grad_norm": 8610.00336817588,
      "learning_rate": 1.7531130054952282e-07,
      "loss": 1.6966,
      "step": 325696
    },
    {
      "epoch": 0.0011821315571668293,
      "grad_norm": 10266.179230853122,
      "learning_rate": 1.7530268034272715e-07,
      "loss": 1.6966,
      "step": 325728
    },
    {
      "epoch": 0.001182247691517666,
      "grad_norm": 9866.173929137882,
      "learning_rate": 1.7529406140739528e-07,
      "loss": 1.6987,
      "step": 325760
    },
    {
      "epoch": 0.0011823638258685028,
      "grad_norm": 10997.305670026637,
      "learning_rate": 1.752854437432147e-07,
      "loss": 1.6809,
      "step": 325792
    },
    {
      "epoch": 0.0011824799602193395,
      "grad_norm": 10488.278409729597,
      "learning_rate": 1.7527682734987296e-07,
      "loss": 1.6714,
      "step": 325824
    },
    {
      "epoch": 0.001182596094570176,
      "grad_norm": 9994.674782102717,
      "learning_rate": 1.752682122270578e-07,
      "loss": 1.6826,
      "step": 325856
    },
    {
      "epoch": 0.0011827122289210128,
      "grad_norm": 9810.768369500933,
      "learning_rate": 1.7525959837445693e-07,
      "loss": 1.6983,
      "step": 325888
    },
    {
      "epoch": 0.0011828283632718496,
      "grad_norm": 9982.649748438538,
      "learning_rate": 1.7525098579175826e-07,
      "loss": 1.6948,
      "step": 325920
    },
    {
      "epoch": 0.0011829444976226863,
      "grad_norm": 11585.50818911281,
      "learning_rate": 1.7524237447864988e-07,
      "loss": 1.6972,
      "step": 325952
    },
    {
      "epoch": 0.001183060631973523,
      "grad_norm": 9497.131777542101,
      "learning_rate": 1.752337644348198e-07,
      "loss": 1.7048,
      "step": 325984
    },
    {
      "epoch": 0.0011831767663243596,
      "grad_norm": 9578.382326885892,
      "learning_rate": 1.7522515565995626e-07,
      "loss": 1.6735,
      "step": 326016
    },
    {
      "epoch": 0.0011832929006751964,
      "grad_norm": 10445.435366704443,
      "learning_rate": 1.7521654815374765e-07,
      "loss": 1.6888,
      "step": 326048
    },
    {
      "epoch": 0.0011834090350260331,
      "grad_norm": 10924.387763165494,
      "learning_rate": 1.7520794191588231e-07,
      "loss": 1.7026,
      "step": 326080
    },
    {
      "epoch": 0.00118352516937687,
      "grad_norm": 8711.554511107648,
      "learning_rate": 1.7519933694604888e-07,
      "loss": 1.7188,
      "step": 326112
    },
    {
      "epoch": 0.0011836413037277064,
      "grad_norm": 9687.316449874032,
      "learning_rate": 1.751907332439359e-07,
      "loss": 1.6978,
      "step": 326144
    },
    {
      "epoch": 0.0011837574380785432,
      "grad_norm": 8140.963825002541,
      "learning_rate": 1.7518213080923223e-07,
      "loss": 1.6915,
      "step": 326176
    },
    {
      "epoch": 0.00118387357242938,
      "grad_norm": 9169.61591343934,
      "learning_rate": 1.7517352964162665e-07,
      "loss": 1.7182,
      "step": 326208
    },
    {
      "epoch": 0.0011839897067802167,
      "grad_norm": 10897.513661381663,
      "learning_rate": 1.7516492974080813e-07,
      "loss": 1.7156,
      "step": 326240
    },
    {
      "epoch": 0.0011841058411310534,
      "grad_norm": 8337.333146756222,
      "learning_rate": 1.751563311064658e-07,
      "loss": 1.7287,
      "step": 326272
    },
    {
      "epoch": 0.00118422197548189,
      "grad_norm": 8818.67699828041,
      "learning_rate": 1.751477337382888e-07,
      "loss": 1.7283,
      "step": 326304
    },
    {
      "epoch": 0.0011843381098327267,
      "grad_norm": 9217.354718139039,
      "learning_rate": 1.7513913763596643e-07,
      "loss": 1.7015,
      "step": 326336
    },
    {
      "epoch": 0.0011844542441835635,
      "grad_norm": 10458.972416064591,
      "learning_rate": 1.751305427991881e-07,
      "loss": 1.6758,
      "step": 326368
    },
    {
      "epoch": 0.0011845703785344002,
      "grad_norm": 11650.516211739288,
      "learning_rate": 1.7512194922764328e-07,
      "loss": 1.6876,
      "step": 326400
    },
    {
      "epoch": 0.0011846865128852368,
      "grad_norm": 9631.287245223248,
      "learning_rate": 1.751133569210216e-07,
      "loss": 1.7001,
      "step": 326432
    },
    {
      "epoch": 0.0011848026472360735,
      "grad_norm": 11274.82381237064,
      "learning_rate": 1.751047658790128e-07,
      "loss": 1.7154,
      "step": 326464
    },
    {
      "epoch": 0.0011849187815869103,
      "grad_norm": 10792.010563375112,
      "learning_rate": 1.7509617610130665e-07,
      "loss": 1.6729,
      "step": 326496
    },
    {
      "epoch": 0.001185034915937747,
      "grad_norm": 9655.138528265661,
      "learning_rate": 1.7508758758759308e-07,
      "loss": 1.6883,
      "step": 326528
    },
    {
      "epoch": 0.0011851510502885838,
      "grad_norm": 8374.950507316446,
      "learning_rate": 1.7507900033756217e-07,
      "loss": 1.7069,
      "step": 326560
    },
    {
      "epoch": 0.0011852671846394203,
      "grad_norm": 8106.36453658482,
      "learning_rate": 1.7507041435090403e-07,
      "loss": 1.7185,
      "step": 326592
    },
    {
      "epoch": 0.001185383318990257,
      "grad_norm": 9342.170839799495,
      "learning_rate": 1.7506182962730892e-07,
      "loss": 1.7146,
      "step": 326624
    },
    {
      "epoch": 0.0011854994533410938,
      "grad_norm": 17727.461408786086,
      "learning_rate": 1.7505324616646716e-07,
      "loss": 1.7079,
      "step": 326656
    },
    {
      "epoch": 0.0011856155876919306,
      "grad_norm": 13188.433417203121,
      "learning_rate": 1.7504493214266301e-07,
      "loss": 1.6865,
      "step": 326688
    },
    {
      "epoch": 0.0011857317220427671,
      "grad_norm": 11380.78977927279,
      "learning_rate": 1.7503635116696249e-07,
      "loss": 1.6696,
      "step": 326720
    },
    {
      "epoch": 0.0011858478563936039,
      "grad_norm": 10473.622868902621,
      "learning_rate": 1.750277714530967e-07,
      "loss": 1.6752,
      "step": 326752
    },
    {
      "epoch": 0.0011859639907444406,
      "grad_norm": 8989.161696176123,
      "learning_rate": 1.7501919300075643e-07,
      "loss": 1.6948,
      "step": 326784
    },
    {
      "epoch": 0.0011860801250952774,
      "grad_norm": 9906.713481271172,
      "learning_rate": 1.750106158096326e-07,
      "loss": 1.6977,
      "step": 326816
    },
    {
      "epoch": 0.0011861962594461142,
      "grad_norm": 9674.14575040091,
      "learning_rate": 1.7500203987941608e-07,
      "loss": 1.6824,
      "step": 326848
    },
    {
      "epoch": 0.0011863123937969507,
      "grad_norm": 10059.164478225814,
      "learning_rate": 1.749934652097981e-07,
      "loss": 1.6844,
      "step": 326880
    },
    {
      "epoch": 0.0011864285281477874,
      "grad_norm": 11551.38571773967,
      "learning_rate": 1.7498489180046978e-07,
      "loss": 1.6914,
      "step": 326912
    },
    {
      "epoch": 0.0011865446624986242,
      "grad_norm": 9702.504006698477,
      "learning_rate": 1.7497631965112242e-07,
      "loss": 1.6947,
      "step": 326944
    },
    {
      "epoch": 0.001186660796849461,
      "grad_norm": 8917.230511767653,
      "learning_rate": 1.7496774876144746e-07,
      "loss": 1.6972,
      "step": 326976
    },
    {
      "epoch": 0.0011867769312002975,
      "grad_norm": 9861.51773308754,
      "learning_rate": 1.749591791311364e-07,
      "loss": 1.6993,
      "step": 327008
    },
    {
      "epoch": 0.0011868930655511342,
      "grad_norm": 10483.86569925426,
      "learning_rate": 1.7495061075988086e-07,
      "loss": 1.6896,
      "step": 327040
    },
    {
      "epoch": 0.001187009199901971,
      "grad_norm": 9986.833331942613,
      "learning_rate": 1.7494204364737262e-07,
      "loss": 1.7035,
      "step": 327072
    },
    {
      "epoch": 0.0011871253342528078,
      "grad_norm": 8777.463984545879,
      "learning_rate": 1.7493347779330343e-07,
      "loss": 1.7165,
      "step": 327104
    },
    {
      "epoch": 0.0011872414686036445,
      "grad_norm": 9431.659875122725,
      "learning_rate": 1.7492491319736534e-07,
      "loss": 1.726,
      "step": 327136
    },
    {
      "epoch": 0.001187357602954481,
      "grad_norm": 7788.899023610462,
      "learning_rate": 1.7491634985925026e-07,
      "loss": 1.7293,
      "step": 327168
    },
    {
      "epoch": 0.0011874737373053178,
      "grad_norm": 8166.876636756551,
      "learning_rate": 1.7490778777865043e-07,
      "loss": 1.7252,
      "step": 327200
    },
    {
      "epoch": 0.0011875898716561546,
      "grad_norm": 10964.804421420384,
      "learning_rate": 1.748992269552581e-07,
      "loss": 1.705,
      "step": 327232
    },
    {
      "epoch": 0.0011877060060069913,
      "grad_norm": 9480.12594853043,
      "learning_rate": 1.7489066738876558e-07,
      "loss": 1.7046,
      "step": 327264
    },
    {
      "epoch": 0.0011878221403578278,
      "grad_norm": 12798.539447921392,
      "learning_rate": 1.748821090788654e-07,
      "loss": 1.6943,
      "step": 327296
    },
    {
      "epoch": 0.0011879382747086646,
      "grad_norm": 9602.985577412892,
      "learning_rate": 1.748735520252501e-07,
      "loss": 1.6985,
      "step": 327328
    },
    {
      "epoch": 0.0011880544090595014,
      "grad_norm": 8343.003655758519,
      "learning_rate": 1.7486499622761237e-07,
      "loss": 1.6963,
      "step": 327360
    },
    {
      "epoch": 0.001188170543410338,
      "grad_norm": 9250.015783770317,
      "learning_rate": 1.7485644168564495e-07,
      "loss": 1.6781,
      "step": 327392
    },
    {
      "epoch": 0.0011882866777611749,
      "grad_norm": 9936.108996986697,
      "learning_rate": 1.748478883990408e-07,
      "loss": 1.682,
      "step": 327424
    },
    {
      "epoch": 0.0011884028121120114,
      "grad_norm": 9703.508025451414,
      "learning_rate": 1.7483933636749284e-07,
      "loss": 1.7018,
      "step": 327456
    },
    {
      "epoch": 0.0011885189464628482,
      "grad_norm": 13714.77626503619,
      "learning_rate": 1.7483078559069423e-07,
      "loss": 1.6943,
      "step": 327488
    },
    {
      "epoch": 0.001188635080813685,
      "grad_norm": 9895.728169265767,
      "learning_rate": 1.7482223606833813e-07,
      "loss": 1.6769,
      "step": 327520
    },
    {
      "epoch": 0.0011887512151645217,
      "grad_norm": 9958.414934114766,
      "learning_rate": 1.7481368780011785e-07,
      "loss": 1.6903,
      "step": 327552
    },
    {
      "epoch": 0.0011888673495153582,
      "grad_norm": 7455.076659565614,
      "learning_rate": 1.7480514078572686e-07,
      "loss": 1.6841,
      "step": 327584
    },
    {
      "epoch": 0.001188983483866195,
      "grad_norm": 8410.06884632938,
      "learning_rate": 1.7479659502485858e-07,
      "loss": 1.6959,
      "step": 327616
    },
    {
      "epoch": 0.0011890996182170317,
      "grad_norm": 9907.47626795038,
      "learning_rate": 1.7478805051720672e-07,
      "loss": 1.6997,
      "step": 327648
    },
    {
      "epoch": 0.0011892157525678685,
      "grad_norm": 17673.775827479538,
      "learning_rate": 1.7477950726246498e-07,
      "loss": 1.699,
      "step": 327680
    },
    {
      "epoch": 0.0011893318869187052,
      "grad_norm": 20880.14022941417,
      "learning_rate": 1.7477096526032721e-07,
      "loss": 1.7102,
      "step": 327712
    },
    {
      "epoch": 0.0011894480212695418,
      "grad_norm": 18766.373331040817,
      "learning_rate": 1.747624245104873e-07,
      "loss": 1.6978,
      "step": 327744
    },
    {
      "epoch": 0.0011895641556203785,
      "grad_norm": 16898.118001718416,
      "learning_rate": 1.7475388501263933e-07,
      "loss": 1.6874,
      "step": 327776
    },
    {
      "epoch": 0.0011896802899712153,
      "grad_norm": 10333.271698740917,
      "learning_rate": 1.747456135677266e-07,
      "loss": 1.699,
      "step": 327808
    },
    {
      "epoch": 0.001189796424322052,
      "grad_norm": 10086.11223415643,
      "learning_rate": 1.7473707653384397e-07,
      "loss": 1.6997,
      "step": 327840
    },
    {
      "epoch": 0.0011899125586728886,
      "grad_norm": 11006.729214439683,
      "learning_rate": 1.7472854075104561e-07,
      "loss": 1.6872,
      "step": 327872
    },
    {
      "epoch": 0.0011900286930237253,
      "grad_norm": 14064.485202096805,
      "learning_rate": 1.7472000621902595e-07,
      "loss": 1.7258,
      "step": 327904
    },
    {
      "epoch": 0.001190144827374562,
      "grad_norm": 8618.114062833005,
      "learning_rate": 1.7471147293747955e-07,
      "loss": 1.7257,
      "step": 327936
    },
    {
      "epoch": 0.0011902609617253988,
      "grad_norm": 9074.766443275552,
      "learning_rate": 1.7470294090610102e-07,
      "loss": 1.7312,
      "step": 327968
    },
    {
      "epoch": 0.0011903770960762356,
      "grad_norm": 10875.802499126214,
      "learning_rate": 1.746944101245852e-07,
      "loss": 1.7286,
      "step": 328000
    },
    {
      "epoch": 0.001190493230427072,
      "grad_norm": 9523.717131456604,
      "learning_rate": 1.7468588059262694e-07,
      "loss": 1.7335,
      "step": 328032
    },
    {
      "epoch": 0.0011906093647779089,
      "grad_norm": 8415.859552060027,
      "learning_rate": 1.746773523099212e-07,
      "loss": 1.7084,
      "step": 328064
    },
    {
      "epoch": 0.0011907254991287456,
      "grad_norm": 8663.279286736633,
      "learning_rate": 1.7466882527616312e-07,
      "loss": 1.6882,
      "step": 328096
    },
    {
      "epoch": 0.0011908416334795824,
      "grad_norm": 8504.789356591968,
      "learning_rate": 1.746602994910478e-07,
      "loss": 1.6946,
      "step": 328128
    },
    {
      "epoch": 0.001190957767830419,
      "grad_norm": 7275.528846757465,
      "learning_rate": 1.7465177495427062e-07,
      "loss": 1.7087,
      "step": 328160
    },
    {
      "epoch": 0.0011910739021812557,
      "grad_norm": 7669.589689155477,
      "learning_rate": 1.746432516655269e-07,
      "loss": 1.6999,
      "step": 328192
    },
    {
      "epoch": 0.0011911900365320924,
      "grad_norm": 8095.442050932117,
      "learning_rate": 1.746347296245122e-07,
      "loss": 1.7019,
      "step": 328224
    },
    {
      "epoch": 0.0011913061708829292,
      "grad_norm": 9623.860348113953,
      "learning_rate": 1.746262088309221e-07,
      "loss": 1.7157,
      "step": 328256
    },
    {
      "epoch": 0.001191422305233766,
      "grad_norm": 9293.74111969986,
      "learning_rate": 1.746176892844523e-07,
      "loss": 1.7313,
      "step": 328288
    },
    {
      "epoch": 0.0011915384395846025,
      "grad_norm": 9119.697144094205,
      "learning_rate": 1.746091709847986e-07,
      "loss": 1.7269,
      "step": 328320
    },
    {
      "epoch": 0.0011916545739354392,
      "grad_norm": 9376.315161085404,
      "learning_rate": 1.7460065393165697e-07,
      "loss": 1.7173,
      "step": 328352
    },
    {
      "epoch": 0.001191770708286276,
      "grad_norm": 10062.758071224807,
      "learning_rate": 1.7459213812472336e-07,
      "loss": 1.7077,
      "step": 328384
    },
    {
      "epoch": 0.0011918868426371127,
      "grad_norm": 9619.834510011075,
      "learning_rate": 1.7458362356369392e-07,
      "loss": 1.6688,
      "step": 328416
    },
    {
      "epoch": 0.0011920029769879493,
      "grad_norm": 9245.473270741742,
      "learning_rate": 1.745751102482649e-07,
      "loss": 1.6764,
      "step": 328448
    },
    {
      "epoch": 0.001192119111338786,
      "grad_norm": 10063.492236793349,
      "learning_rate": 1.7456659817813262e-07,
      "loss": 1.6844,
      "step": 328480
    },
    {
      "epoch": 0.0011922352456896228,
      "grad_norm": 9714.818166080104,
      "learning_rate": 1.7455808735299346e-07,
      "loss": 1.6931,
      "step": 328512
    },
    {
      "epoch": 0.0011923513800404595,
      "grad_norm": 9112.761820655689,
      "learning_rate": 1.7454957777254403e-07,
      "loss": 1.6628,
      "step": 328544
    },
    {
      "epoch": 0.0011924675143912963,
      "grad_norm": 10028.471269341106,
      "learning_rate": 1.7454106943648096e-07,
      "loss": 1.6844,
      "step": 328576
    },
    {
      "epoch": 0.0011925836487421328,
      "grad_norm": 9495.339277772016,
      "learning_rate": 1.7453256234450097e-07,
      "loss": 1.6895,
      "step": 328608
    },
    {
      "epoch": 0.0011926997830929696,
      "grad_norm": 11441.391523761433,
      "learning_rate": 1.745240564963009e-07,
      "loss": 1.6865,
      "step": 328640
    },
    {
      "epoch": 0.0011928159174438063,
      "grad_norm": 10798.032783799094,
      "learning_rate": 1.745155518915777e-07,
      "loss": 1.6949,
      "step": 328672
    },
    {
      "epoch": 0.001192932051794643,
      "grad_norm": 9096.535384419718,
      "learning_rate": 1.745070485300285e-07,
      "loss": 1.7003,
      "step": 328704
    },
    {
      "epoch": 0.0011930481861454796,
      "grad_norm": 8461.789881579429,
      "learning_rate": 1.7449854641135035e-07,
      "loss": 1.6947,
      "step": 328736
    },
    {
      "epoch": 0.0011931643204963164,
      "grad_norm": 8634.844758303418,
      "learning_rate": 1.7449004553524057e-07,
      "loss": 1.6861,
      "step": 328768
    },
    {
      "epoch": 0.0011932804548471531,
      "grad_norm": 9343.618785031847,
      "learning_rate": 1.744815459013965e-07,
      "loss": 1.7109,
      "step": 328800
    },
    {
      "epoch": 0.0011933965891979899,
      "grad_norm": 12022.155214436387,
      "learning_rate": 1.744733130654657e-07,
      "loss": 1.7389,
      "step": 328832
    },
    {
      "epoch": 0.0011935127235488266,
      "grad_norm": 10975.199314818843,
      "learning_rate": 1.7446481587644827e-07,
      "loss": 1.7199,
      "step": 328864
    },
    {
      "epoch": 0.0011936288578996632,
      "grad_norm": 8911.18443305939,
      "learning_rate": 1.7445631992879876e-07,
      "loss": 1.6986,
      "step": 328896
    },
    {
      "epoch": 0.0011937449922505,
      "grad_norm": 10497.055968222709,
      "learning_rate": 1.744478252222149e-07,
      "loss": 1.708,
      "step": 328928
    },
    {
      "epoch": 0.0011938611266013367,
      "grad_norm": 7988.806669334287,
      "learning_rate": 1.7443933175639452e-07,
      "loss": 1.704,
      "step": 328960
    },
    {
      "epoch": 0.0011939772609521734,
      "grad_norm": 10111.863725347568,
      "learning_rate": 1.744308395310357e-07,
      "loss": 1.6991,
      "step": 328992
    },
    {
      "epoch": 0.00119409339530301,
      "grad_norm": 8748.050068443825,
      "learning_rate": 1.7442234854583647e-07,
      "loss": 1.6879,
      "step": 329024
    },
    {
      "epoch": 0.0011942095296538467,
      "grad_norm": 8475.870102827203,
      "learning_rate": 1.7441385880049503e-07,
      "loss": 1.6888,
      "step": 329056
    },
    {
      "epoch": 0.0011943256640046835,
      "grad_norm": 10600.880906792605,
      "learning_rate": 1.7440537029470965e-07,
      "loss": 1.6738,
      "step": 329088
    },
    {
      "epoch": 0.0011944417983555202,
      "grad_norm": 15164.153256941187,
      "learning_rate": 1.7439688302817874e-07,
      "loss": 1.6651,
      "step": 329120
    },
    {
      "epoch": 0.001194557932706357,
      "grad_norm": 8767.70734000628,
      "learning_rate": 1.7438839700060076e-07,
      "loss": 1.6816,
      "step": 329152
    },
    {
      "epoch": 0.0011946740670571935,
      "grad_norm": 9351.409091682386,
      "learning_rate": 1.7437991221167436e-07,
      "loss": 1.7006,
      "step": 329184
    },
    {
      "epoch": 0.0011947902014080303,
      "grad_norm": 9118.404685031259,
      "learning_rate": 1.743714286610982e-07,
      "loss": 1.7027,
      "step": 329216
    },
    {
      "epoch": 0.001194906335758867,
      "grad_norm": 7931.176709669253,
      "learning_rate": 1.7436294634857108e-07,
      "loss": 1.6861,
      "step": 329248
    },
    {
      "epoch": 0.0011950224701097038,
      "grad_norm": 9598.541555882332,
      "learning_rate": 1.7435446527379193e-07,
      "loss": 1.6882,
      "step": 329280
    },
    {
      "epoch": 0.0011951386044605403,
      "grad_norm": 9449.46411178962,
      "learning_rate": 1.7434598543645974e-07,
      "loss": 1.7048,
      "step": 329312
    },
    {
      "epoch": 0.001195254738811377,
      "grad_norm": 9089.292601737498,
      "learning_rate": 1.7433750683627363e-07,
      "loss": 1.7114,
      "step": 329344
    },
    {
      "epoch": 0.0011953708731622138,
      "grad_norm": 8838.598531441508,
      "learning_rate": 1.7432902947293282e-07,
      "loss": 1.7066,
      "step": 329376
    },
    {
      "epoch": 0.0011954870075130506,
      "grad_norm": 9706.342668585321,
      "learning_rate": 1.7432055334613658e-07,
      "loss": 1.6915,
      "step": 329408
    },
    {
      "epoch": 0.0011956031418638873,
      "grad_norm": 11834.198747697286,
      "learning_rate": 1.7431207845558437e-07,
      "loss": 1.661,
      "step": 329440
    },
    {
      "epoch": 0.0011957192762147239,
      "grad_norm": 9660.810939046472,
      "learning_rate": 1.743036048009757e-07,
      "loss": 1.6821,
      "step": 329472
    },
    {
      "epoch": 0.0011958354105655606,
      "grad_norm": 9567.219449767,
      "learning_rate": 1.7429513238201018e-07,
      "loss": 1.6844,
      "step": 329504
    },
    {
      "epoch": 0.0011959515449163974,
      "grad_norm": 10837.966783488495,
      "learning_rate": 1.7428666119838752e-07,
      "loss": 1.6989,
      "step": 329536
    },
    {
      "epoch": 0.0011960676792672341,
      "grad_norm": 10177.527990627194,
      "learning_rate": 1.742781912498076e-07,
      "loss": 1.6732,
      "step": 329568
    },
    {
      "epoch": 0.0011961838136180707,
      "grad_norm": 9801.200130596253,
      "learning_rate": 1.7426972253597027e-07,
      "loss": 1.6748,
      "step": 329600
    },
    {
      "epoch": 0.0011962999479689074,
      "grad_norm": 8936.831877125138,
      "learning_rate": 1.7426125505657563e-07,
      "loss": 1.6895,
      "step": 329632
    },
    {
      "epoch": 0.0011964160823197442,
      "grad_norm": 11796.661222566323,
      "learning_rate": 1.7425278881132375e-07,
      "loss": 1.7066,
      "step": 329664
    },
    {
      "epoch": 0.001196532216670581,
      "grad_norm": 10320.376155935402,
      "learning_rate": 1.7424432379991493e-07,
      "loss": 1.7085,
      "step": 329696
    },
    {
      "epoch": 0.0011966483510214177,
      "grad_norm": 8681.907624479772,
      "learning_rate": 1.7423586002204945e-07,
      "loss": 1.7136,
      "step": 329728
    },
    {
      "epoch": 0.0011967644853722542,
      "grad_norm": 9250.008432428589,
      "learning_rate": 1.7422739747742774e-07,
      "loss": 1.7117,
      "step": 329760
    },
    {
      "epoch": 0.001196880619723091,
      "grad_norm": 9600.848712483705,
      "learning_rate": 1.742189361657504e-07,
      "loss": 1.6979,
      "step": 329792
    },
    {
      "epoch": 0.0011969967540739277,
      "grad_norm": 9125.14679333982,
      "learning_rate": 1.7421047608671804e-07,
      "loss": 1.6981,
      "step": 329824
    },
    {
      "epoch": 0.0011971128884247645,
      "grad_norm": 11701.441278748529,
      "learning_rate": 1.7420228156033964e-07,
      "loss": 1.7057,
      "step": 329856
    },
    {
      "epoch": 0.001197229022775601,
      "grad_norm": 9678.560430146625,
      "learning_rate": 1.7419382390720265e-07,
      "loss": 1.7192,
      "step": 329888
    },
    {
      "epoch": 0.0011973451571264378,
      "grad_norm": 9657.11023029146,
      "learning_rate": 1.7418536748582247e-07,
      "loss": 1.6957,
      "step": 329920
    },
    {
      "epoch": 0.0011974612914772745,
      "grad_norm": 9511.155976010486,
      "learning_rate": 1.7417691229590025e-07,
      "loss": 1.6855,
      "step": 329952
    },
    {
      "epoch": 0.0011975774258281113,
      "grad_norm": 9430.935054383526,
      "learning_rate": 1.74168458337137e-07,
      "loss": 1.6841,
      "step": 329984
    },
    {
      "epoch": 0.001197693560178948,
      "grad_norm": 10021.988824579681,
      "learning_rate": 1.741600056092341e-07,
      "loss": 1.688,
      "step": 330016
    },
    {
      "epoch": 0.0011978096945297846,
      "grad_norm": 10162.056484786925,
      "learning_rate": 1.741515541118928e-07,
      "loss": 1.6946,
      "step": 330048
    },
    {
      "epoch": 0.0011979258288806213,
      "grad_norm": 9992.309843074323,
      "learning_rate": 1.7414310384481465e-07,
      "loss": 1.7217,
      "step": 330080
    },
    {
      "epoch": 0.001198041963231458,
      "grad_norm": 11340.450961050889,
      "learning_rate": 1.7413465480770108e-07,
      "loss": 1.6825,
      "step": 330112
    },
    {
      "epoch": 0.0011981580975822948,
      "grad_norm": 10779.585891860595,
      "learning_rate": 1.7412620700025384e-07,
      "loss": 1.6685,
      "step": 330144
    },
    {
      "epoch": 0.0011982742319331314,
      "grad_norm": 10325.349388761622,
      "learning_rate": 1.7411776042217464e-07,
      "loss": 1.6775,
      "step": 330176
    },
    {
      "epoch": 0.0011983903662839681,
      "grad_norm": 10127.37458574531,
      "learning_rate": 1.7410931507316532e-07,
      "loss": 1.6909,
      "step": 330208
    },
    {
      "epoch": 0.0011985065006348049,
      "grad_norm": 8439.338718169807,
      "learning_rate": 1.741008709529279e-07,
      "loss": 1.6822,
      "step": 330240
    },
    {
      "epoch": 0.0011986226349856416,
      "grad_norm": 8932.145095104535,
      "learning_rate": 1.740924280611644e-07,
      "loss": 1.6732,
      "step": 330272
    },
    {
      "epoch": 0.0011987387693364784,
      "grad_norm": 13375.253567689848,
      "learning_rate": 1.7408398639757698e-07,
      "loss": 1.6905,
      "step": 330304
    },
    {
      "epoch": 0.001198854903687315,
      "grad_norm": 9076.925470664612,
      "learning_rate": 1.740755459618679e-07,
      "loss": 1.7048,
      "step": 330336
    },
    {
      "epoch": 0.0011989710380381517,
      "grad_norm": 10932.986051395106,
      "learning_rate": 1.740671067537395e-07,
      "loss": 1.7058,
      "step": 330368
    },
    {
      "epoch": 0.0011990871723889884,
      "grad_norm": 11477.849101639209,
      "learning_rate": 1.740586687728943e-07,
      "loss": 1.715,
      "step": 330400
    },
    {
      "epoch": 0.0011992033067398252,
      "grad_norm": 10310.300674568129,
      "learning_rate": 1.740502320190348e-07,
      "loss": 1.72,
      "step": 330432
    },
    {
      "epoch": 0.0011993194410906617,
      "grad_norm": 10640.10507466914,
      "learning_rate": 1.7404179649186369e-07,
      "loss": 1.6912,
      "step": 330464
    },
    {
      "epoch": 0.0011994355754414985,
      "grad_norm": 12629.55945391604,
      "learning_rate": 1.7403336219108375e-07,
      "loss": 1.6885,
      "step": 330496
    },
    {
      "epoch": 0.0011995517097923352,
      "grad_norm": 10710.858415645313,
      "learning_rate": 1.7402492911639784e-07,
      "loss": 1.6898,
      "step": 330528
    },
    {
      "epoch": 0.001199667844143172,
      "grad_norm": 9385.573184414472,
      "learning_rate": 1.740164972675089e-07,
      "loss": 1.7039,
      "step": 330560
    },
    {
      "epoch": 0.0011997839784940087,
      "grad_norm": 9974.837843293495,
      "learning_rate": 1.7400806664412004e-07,
      "loss": 1.7035,
      "step": 330592
    },
    {
      "epoch": 0.0011999001128448453,
      "grad_norm": 8505.419331226416,
      "learning_rate": 1.739996372459344e-07,
      "loss": 1.7169,
      "step": 330624
    },
    {
      "epoch": 0.001200016247195682,
      "grad_norm": 11010.300086736965,
      "learning_rate": 1.7399120907265526e-07,
      "loss": 1.7109,
      "step": 330656
    },
    {
      "epoch": 0.0012001323815465188,
      "grad_norm": 8204.963254031063,
      "learning_rate": 1.73982782123986e-07,
      "loss": 1.7121,
      "step": 330688
    },
    {
      "epoch": 0.0012002485158973555,
      "grad_norm": 10413.422876268878,
      "learning_rate": 1.7397435639963003e-07,
      "loss": 1.7041,
      "step": 330720
    },
    {
      "epoch": 0.001200364650248192,
      "grad_norm": 10836.368210798302,
      "learning_rate": 1.73965931899291e-07,
      "loss": 1.6872,
      "step": 330752
    },
    {
      "epoch": 0.0012004807845990288,
      "grad_norm": 8459.13707182949,
      "learning_rate": 1.7395750862267252e-07,
      "loss": 1.6892,
      "step": 330784
    },
    {
      "epoch": 0.0012005969189498656,
      "grad_norm": 9589.465782826486,
      "learning_rate": 1.739490865694784e-07,
      "loss": 1.6742,
      "step": 330816
    },
    {
      "epoch": 0.0012007130533007023,
      "grad_norm": 7300.573539113211,
      "learning_rate": 1.7394066573941247e-07,
      "loss": 1.681,
      "step": 330848
    },
    {
      "epoch": 0.001200829187651539,
      "grad_norm": 8447.609484345261,
      "learning_rate": 1.7393250922639808e-07,
      "loss": 1.6861,
      "step": 330880
    },
    {
      "epoch": 0.0012009453220023756,
      "grad_norm": 8925.659303379218,
      "learning_rate": 1.7392409080350088e-07,
      "loss": 1.7094,
      "step": 330912
    },
    {
      "epoch": 0.0012010614563532124,
      "grad_norm": 10417.31289728786,
      "learning_rate": 1.7391567360285332e-07,
      "loss": 1.7,
      "step": 330944
    },
    {
      "epoch": 0.0012011775907040491,
      "grad_norm": 7865.0702476201695,
      "learning_rate": 1.7390725762415973e-07,
      "loss": 1.7209,
      "step": 330976
    },
    {
      "epoch": 0.001201293725054886,
      "grad_norm": 9917.271197259859,
      "learning_rate": 1.7389884286712443e-07,
      "loss": 1.6928,
      "step": 331008
    },
    {
      "epoch": 0.0012014098594057224,
      "grad_norm": 8911.409091720569,
      "learning_rate": 1.7389042933145193e-07,
      "loss": 1.6903,
      "step": 331040
    },
    {
      "epoch": 0.0012015259937565592,
      "grad_norm": 9815.525660910882,
      "learning_rate": 1.7388201701684677e-07,
      "loss": 1.6917,
      "step": 331072
    },
    {
      "epoch": 0.001201642128107396,
      "grad_norm": 9384.271628634799,
      "learning_rate": 1.7387360592301366e-07,
      "loss": 1.6879,
      "step": 331104
    },
    {
      "epoch": 0.0012017582624582327,
      "grad_norm": 9711.506371310272,
      "learning_rate": 1.7386519604965736e-07,
      "loss": 1.6936,
      "step": 331136
    },
    {
      "epoch": 0.0012018743968090695,
      "grad_norm": 10660.09343298641,
      "learning_rate": 1.7385678739648272e-07,
      "loss": 1.6876,
      "step": 331168
    },
    {
      "epoch": 0.001201990531159906,
      "grad_norm": 9458.065764203588,
      "learning_rate": 1.7384837996319468e-07,
      "loss": 1.6816,
      "step": 331200
    },
    {
      "epoch": 0.0012021066655107427,
      "grad_norm": 8850.431401914824,
      "learning_rate": 1.738399737494984e-07,
      "loss": 1.6961,
      "step": 331232
    },
    {
      "epoch": 0.0012022227998615795,
      "grad_norm": 9842.72340361142,
      "learning_rate": 1.7383156875509897e-07,
      "loss": 1.6928,
      "step": 331264
    },
    {
      "epoch": 0.0012023389342124163,
      "grad_norm": 9171.125994118716,
      "learning_rate": 1.7382316497970173e-07,
      "loss": 1.6758,
      "step": 331296
    },
    {
      "epoch": 0.0012024550685632528,
      "grad_norm": 9799.604277724688,
      "learning_rate": 1.7381476242301194e-07,
      "loss": 1.7027,
      "step": 331328
    },
    {
      "epoch": 0.0012025712029140895,
      "grad_norm": 9266.45692808206,
      "learning_rate": 1.7380636108473516e-07,
      "loss": 1.704,
      "step": 331360
    },
    {
      "epoch": 0.0012026873372649263,
      "grad_norm": 10349.824732815527,
      "learning_rate": 1.7379796096457697e-07,
      "loss": 1.7137,
      "step": 331392
    },
    {
      "epoch": 0.001202803471615763,
      "grad_norm": 10268.825638796288,
      "learning_rate": 1.7378956206224294e-07,
      "loss": 1.7138,
      "step": 331424
    },
    {
      "epoch": 0.0012029196059665998,
      "grad_norm": 9290.751315152074,
      "learning_rate": 1.7378116437743894e-07,
      "loss": 1.7193,
      "step": 331456
    },
    {
      "epoch": 0.0012030357403174363,
      "grad_norm": 10042.534142336783,
      "learning_rate": 1.7377276790987078e-07,
      "loss": 1.7155,
      "step": 331488
    },
    {
      "epoch": 0.001203151874668273,
      "grad_norm": 10963.639541685052,
      "learning_rate": 1.7376437265924447e-07,
      "loss": 1.7188,
      "step": 331520
    },
    {
      "epoch": 0.0012032680090191099,
      "grad_norm": 8994.024683088212,
      "learning_rate": 1.7375597862526602e-07,
      "loss": 1.7174,
      "step": 331552
    },
    {
      "epoch": 0.0012033841433699466,
      "grad_norm": 13387.580513296643,
      "learning_rate": 1.7374758580764163e-07,
      "loss": 1.707,
      "step": 331584
    },
    {
      "epoch": 0.0012035002777207831,
      "grad_norm": 9351.912959389645,
      "learning_rate": 1.7373919420607756e-07,
      "loss": 1.6812,
      "step": 331616
    },
    {
      "epoch": 0.00120361641207162,
      "grad_norm": 10633.061459429264,
      "learning_rate": 1.7373080382028016e-07,
      "loss": 1.6696,
      "step": 331648
    },
    {
      "epoch": 0.0012037325464224567,
      "grad_norm": 9386.934856490696,
      "learning_rate": 1.7372241464995593e-07,
      "loss": 1.6875,
      "step": 331680
    },
    {
      "epoch": 0.0012038486807732934,
      "grad_norm": 8168.758534808089,
      "learning_rate": 1.7371402669481142e-07,
      "loss": 1.7033,
      "step": 331712
    },
    {
      "epoch": 0.0012039648151241302,
      "grad_norm": 9933.150557602557,
      "learning_rate": 1.7370563995455324e-07,
      "loss": 1.6926,
      "step": 331744
    },
    {
      "epoch": 0.0012040809494749667,
      "grad_norm": 13389.973412968377,
      "learning_rate": 1.7369725442888822e-07,
      "loss": 1.6901,
      "step": 331776
    },
    {
      "epoch": 0.0012041970838258035,
      "grad_norm": 10670.927232438613,
      "learning_rate": 1.736888701175232e-07,
      "loss": 1.7046,
      "step": 331808
    },
    {
      "epoch": 0.0012043132181766402,
      "grad_norm": 11838.848254792356,
      "learning_rate": 1.7368048702016507e-07,
      "loss": 1.6822,
      "step": 331840
    },
    {
      "epoch": 0.001204429352527477,
      "grad_norm": 18773.180337918242,
      "learning_rate": 1.736723670520161e-07,
      "loss": 1.683,
      "step": 331872
    },
    {
      "epoch": 0.0012045454868783135,
      "grad_norm": 10056.79034284796,
      "learning_rate": 1.7366398634387827e-07,
      "loss": 1.6774,
      "step": 331904
    },
    {
      "epoch": 0.0012046616212291503,
      "grad_norm": 11597.767543799107,
      "learning_rate": 1.736556068488779e-07,
      "loss": 1.6927,
      "step": 331936
    },
    {
      "epoch": 0.001204777755579987,
      "grad_norm": 8345.23360967205,
      "learning_rate": 1.736472285667225e-07,
      "loss": 1.6767,
      "step": 331968
    },
    {
      "epoch": 0.0012048938899308238,
      "grad_norm": 9619.11700729334,
      "learning_rate": 1.7363885149711945e-07,
      "loss": 1.6923,
      "step": 332000
    },
    {
      "epoch": 0.0012050100242816605,
      "grad_norm": 10106.34513560664,
      "learning_rate": 1.7363047563977632e-07,
      "loss": 1.7065,
      "step": 332032
    },
    {
      "epoch": 0.001205126158632497,
      "grad_norm": 7566.887471080829,
      "learning_rate": 1.7362210099440078e-07,
      "loss": 1.7139,
      "step": 332064
    },
    {
      "epoch": 0.0012052422929833338,
      "grad_norm": 10539.440023075229,
      "learning_rate": 1.7361372756070052e-07,
      "loss": 1.7176,
      "step": 332096
    },
    {
      "epoch": 0.0012053584273341706,
      "grad_norm": 9052.450055095582,
      "learning_rate": 1.7360535533838346e-07,
      "loss": 1.697,
      "step": 332128
    },
    {
      "epoch": 0.0012054745616850073,
      "grad_norm": 9060.304189153916,
      "learning_rate": 1.7359698432715746e-07,
      "loss": 1.6956,
      "step": 332160
    },
    {
      "epoch": 0.0012055906960358439,
      "grad_norm": 11618.578742686215,
      "learning_rate": 1.7358861452673062e-07,
      "loss": 1.6803,
      "step": 332192
    },
    {
      "epoch": 0.0012057068303866806,
      "grad_norm": 9225.3377173955,
      "learning_rate": 1.7358024593681104e-07,
      "loss": 1.6884,
      "step": 332224
    },
    {
      "epoch": 0.0012058229647375174,
      "grad_norm": 8953.792604254357,
      "learning_rate": 1.7357187855710704e-07,
      "loss": 1.7042,
      "step": 332256
    },
    {
      "epoch": 0.0012059390990883541,
      "grad_norm": 9658.087595378289,
      "learning_rate": 1.7356351238732687e-07,
      "loss": 1.7025,
      "step": 332288
    },
    {
      "epoch": 0.0012060552334391909,
      "grad_norm": 9691.923028996876,
      "learning_rate": 1.73555147427179e-07,
      "loss": 1.6849,
      "step": 332320
    },
    {
      "epoch": 0.0012061713677900274,
      "grad_norm": 8131.211102904659,
      "learning_rate": 1.7354678367637197e-07,
      "loss": 1.7081,
      "step": 332352
    },
    {
      "epoch": 0.0012062875021408642,
      "grad_norm": 11696.142953982737,
      "learning_rate": 1.7353842113461441e-07,
      "loss": 1.7115,
      "step": 332384
    },
    {
      "epoch": 0.001206403636491701,
      "grad_norm": 8341.915007958305,
      "learning_rate": 1.73530059801615e-07,
      "loss": 1.7124,
      "step": 332416
    },
    {
      "epoch": 0.0012065197708425377,
      "grad_norm": 10213.152108923083,
      "learning_rate": 1.7352169967708267e-07,
      "loss": 1.7196,
      "step": 332448
    },
    {
      "epoch": 0.0012066359051933742,
      "grad_norm": 8677.887991902177,
      "learning_rate": 1.735133407607263e-07,
      "loss": 1.7052,
      "step": 332480
    },
    {
      "epoch": 0.001206752039544211,
      "grad_norm": 9787.091907201035,
      "learning_rate": 1.7350498305225487e-07,
      "loss": 1.6796,
      "step": 332512
    },
    {
      "epoch": 0.0012068681738950477,
      "grad_norm": 9476.312785044614,
      "learning_rate": 1.7349662655137758e-07,
      "loss": 1.6826,
      "step": 332544
    },
    {
      "epoch": 0.0012069843082458845,
      "grad_norm": 11655.001329901253,
      "learning_rate": 1.734882712578036e-07,
      "loss": 1.6985,
      "step": 332576
    },
    {
      "epoch": 0.0012071004425967212,
      "grad_norm": 12005.005789253082,
      "learning_rate": 1.7347991717124227e-07,
      "loss": 1.7115,
      "step": 332608
    },
    {
      "epoch": 0.0012072165769475578,
      "grad_norm": 12718.789093306013,
      "learning_rate": 1.7347156429140298e-07,
      "loss": 1.6953,
      "step": 332640
    },
    {
      "epoch": 0.0012073327112983945,
      "grad_norm": 10113.216698953898,
      "learning_rate": 1.7346321261799532e-07,
      "loss": 1.6781,
      "step": 332672
    },
    {
      "epoch": 0.0012074488456492313,
      "grad_norm": 9157.688572997009,
      "learning_rate": 1.734548621507288e-07,
      "loss": 1.7064,
      "step": 332704
    },
    {
      "epoch": 0.001207564980000068,
      "grad_norm": 10635.940578999114,
      "learning_rate": 1.7344651288931321e-07,
      "loss": 1.7107,
      "step": 332736
    },
    {
      "epoch": 0.0012076811143509046,
      "grad_norm": 7941.4073060132105,
      "learning_rate": 1.7343816483345832e-07,
      "loss": 1.7047,
      "step": 332768
    },
    {
      "epoch": 0.0012077972487017413,
      "grad_norm": 10545.59642694523,
      "learning_rate": 1.734298179828741e-07,
      "loss": 1.7008,
      "step": 332800
    },
    {
      "epoch": 0.001207913383052578,
      "grad_norm": 8789.426033592865,
      "learning_rate": 1.7342147233727046e-07,
      "loss": 1.7129,
      "step": 332832
    },
    {
      "epoch": 0.0012080295174034148,
      "grad_norm": 10125.935413580317,
      "learning_rate": 1.7341312789635756e-07,
      "loss": 1.6771,
      "step": 332864
    },
    {
      "epoch": 0.0012081456517542516,
      "grad_norm": 16652.12250735623,
      "learning_rate": 1.734047846598456e-07,
      "loss": 1.6876,
      "step": 332896
    },
    {
      "epoch": 0.0012082617861050881,
      "grad_norm": 10900.032660501527,
      "learning_rate": 1.7339670329773397e-07,
      "loss": 1.6978,
      "step": 332928
    },
    {
      "epoch": 0.0012083779204559249,
      "grad_norm": 11321.499194011365,
      "learning_rate": 1.733883624315398e-07,
      "loss": 1.7091,
      "step": 332960
    },
    {
      "epoch": 0.0012084940548067616,
      "grad_norm": 8311.414320078142,
      "learning_rate": 1.733800227688868e-07,
      "loss": 1.6949,
      "step": 332992
    },
    {
      "epoch": 0.0012086101891575984,
      "grad_norm": 9934.438283063617,
      "learning_rate": 1.7337168430948553e-07,
      "loss": 1.6974,
      "step": 333024
    },
    {
      "epoch": 0.001208726323508435,
      "grad_norm": 10992.231620558221,
      "learning_rate": 1.7336334705304674e-07,
      "loss": 1.7076,
      "step": 333056
    },
    {
      "epoch": 0.0012088424578592717,
      "grad_norm": 10520.879621020287,
      "learning_rate": 1.7335501099928113e-07,
      "loss": 1.7202,
      "step": 333088
    },
    {
      "epoch": 0.0012089585922101084,
      "grad_norm": 8688.002532227993,
      "learning_rate": 1.7334667614789962e-07,
      "loss": 1.7348,
      "step": 333120
    },
    {
      "epoch": 0.0012090747265609452,
      "grad_norm": 9085.607409524142,
      "learning_rate": 1.7333834249861315e-07,
      "loss": 1.7306,
      "step": 333152
    },
    {
      "epoch": 0.001209190860911782,
      "grad_norm": 8724.25423746924,
      "learning_rate": 1.7333001005113285e-07,
      "loss": 1.722,
      "step": 333184
    },
    {
      "epoch": 0.0012093069952626185,
      "grad_norm": 10543.137483690516,
      "learning_rate": 1.7332167880516987e-07,
      "loss": 1.7,
      "step": 333216
    },
    {
      "epoch": 0.0012094231296134552,
      "grad_norm": 10873.397629076204,
      "learning_rate": 1.7331334876043545e-07,
      "loss": 1.7141,
      "step": 333248
    },
    {
      "epoch": 0.001209539263964292,
      "grad_norm": 9350.904341292344,
      "learning_rate": 1.7330501991664102e-07,
      "loss": 1.7252,
      "step": 333280
    },
    {
      "epoch": 0.0012096553983151287,
      "grad_norm": 8248.89398646873,
      "learning_rate": 1.7329669227349798e-07,
      "loss": 1.744,
      "step": 333312
    },
    {
      "epoch": 0.0012097715326659653,
      "grad_norm": 9236.224120277722,
      "learning_rate": 1.7328836583071794e-07,
      "loss": 1.688,
      "step": 333344
    },
    {
      "epoch": 0.001209887667016802,
      "grad_norm": 10533.733621086116,
      "learning_rate": 1.7328004058801248e-07,
      "loss": 1.6803,
      "step": 333376
    },
    {
      "epoch": 0.0012100038013676388,
      "grad_norm": 9547.519049470391,
      "learning_rate": 1.732717165450934e-07,
      "loss": 1.6754,
      "step": 333408
    },
    {
      "epoch": 0.0012101199357184755,
      "grad_norm": 8579.895453908513,
      "learning_rate": 1.732633937016726e-07,
      "loss": 1.6917,
      "step": 333440
    },
    {
      "epoch": 0.0012102360700693123,
      "grad_norm": 9583.419431497297,
      "learning_rate": 1.7325507205746194e-07,
      "loss": 1.6903,
      "step": 333472
    },
    {
      "epoch": 0.0012103522044201488,
      "grad_norm": 12647.516436043876,
      "learning_rate": 1.7324675161217354e-07,
      "loss": 1.6808,
      "step": 333504
    },
    {
      "epoch": 0.0012104683387709856,
      "grad_norm": 10867.588140889404,
      "learning_rate": 1.7323843236551945e-07,
      "loss": 1.678,
      "step": 333536
    },
    {
      "epoch": 0.0012105844731218223,
      "grad_norm": 8862.475049330182,
      "learning_rate": 1.7323011431721203e-07,
      "loss": 1.6765,
      "step": 333568
    },
    {
      "epoch": 0.001210700607472659,
      "grad_norm": 9220.814497646074,
      "learning_rate": 1.7322179746696349e-07,
      "loss": 1.7023,
      "step": 333600
    },
    {
      "epoch": 0.0012108167418234956,
      "grad_norm": 10525.932167746474,
      "learning_rate": 1.7321348181448634e-07,
      "loss": 1.7019,
      "step": 333632
    },
    {
      "epoch": 0.0012109328761743324,
      "grad_norm": 10929.672273220272,
      "learning_rate": 1.7320516735949306e-07,
      "loss": 1.6987,
      "step": 333664
    },
    {
      "epoch": 0.0012110490105251691,
      "grad_norm": 8315.623007327833,
      "learning_rate": 1.7319685410169632e-07,
      "loss": 1.6849,
      "step": 333696
    },
    {
      "epoch": 0.0012111651448760059,
      "grad_norm": 10525.148740041634,
      "learning_rate": 1.731885420408088e-07,
      "loss": 1.6828,
      "step": 333728
    },
    {
      "epoch": 0.0012112812792268426,
      "grad_norm": 8777.995670994604,
      "learning_rate": 1.7318023117654338e-07,
      "loss": 1.6953,
      "step": 333760
    },
    {
      "epoch": 0.0012113974135776792,
      "grad_norm": 8376.153293726184,
      "learning_rate": 1.7317192150861294e-07,
      "loss": 1.7002,
      "step": 333792
    },
    {
      "epoch": 0.001211513547928516,
      "grad_norm": 9929.169350957813,
      "learning_rate": 1.7316361303673042e-07,
      "loss": 1.6941,
      "step": 333824
    },
    {
      "epoch": 0.0012116296822793527,
      "grad_norm": 9737.10234104582,
      "learning_rate": 1.7315530576060902e-07,
      "loss": 1.7074,
      "step": 333856
    },
    {
      "epoch": 0.0012117458166301894,
      "grad_norm": 12972.88772787308,
      "learning_rate": 1.731469996799619e-07,
      "loss": 1.6719,
      "step": 333888
    },
    {
      "epoch": 0.001211861950981026,
      "grad_norm": 17971.35453993382,
      "learning_rate": 1.7313869479450237e-07,
      "loss": 1.6683,
      "step": 333920
    },
    {
      "epoch": 0.0012119780853318627,
      "grad_norm": 22697.282656741096,
      "learning_rate": 1.7313039110394384e-07,
      "loss": 1.6815,
      "step": 333952
    },
    {
      "epoch": 0.0012120942196826995,
      "grad_norm": 20688.4087353281,
      "learning_rate": 1.731220886079998e-07,
      "loss": 1.6933,
      "step": 333984
    },
    {
      "epoch": 0.0012122103540335362,
      "grad_norm": 12525.686887352726,
      "learning_rate": 1.731140467039839e-07,
      "loss": 1.6851,
      "step": 334016
    },
    {
      "epoch": 0.0012123264883843728,
      "grad_norm": 8831.9930932944,
      "learning_rate": 1.7310574655910024e-07,
      "loss": 1.6849,
      "step": 334048
    },
    {
      "epoch": 0.0012124426227352095,
      "grad_norm": 10936.807578082373,
      "learning_rate": 1.7309744760798096e-07,
      "loss": 1.7025,
      "step": 334080
    },
    {
      "epoch": 0.0012125587570860463,
      "grad_norm": 8579.742653483261,
      "learning_rate": 1.7308914985034006e-07,
      "loss": 1.7204,
      "step": 334112
    },
    {
      "epoch": 0.001212674891436883,
      "grad_norm": 7503.531435264332,
      "learning_rate": 1.730808532858915e-07,
      "loss": 1.7215,
      "step": 334144
    },
    {
      "epoch": 0.0012127910257877198,
      "grad_norm": 8630.63230592058,
      "learning_rate": 1.730725579143493e-07,
      "loss": 1.7274,
      "step": 334176
    },
    {
      "epoch": 0.0012129071601385563,
      "grad_norm": 9503.537446656377,
      "learning_rate": 1.730642637354276e-07,
      "loss": 1.7314,
      "step": 334208
    },
    {
      "epoch": 0.001213023294489393,
      "grad_norm": 9376.517050589733,
      "learning_rate": 1.7305597074884074e-07,
      "loss": 1.6839,
      "step": 334240
    },
    {
      "epoch": 0.0012131394288402298,
      "grad_norm": 10465.418481838173,
      "learning_rate": 1.73047678954303e-07,
      "loss": 1.677,
      "step": 334272
    },
    {
      "epoch": 0.0012132555631910666,
      "grad_norm": 9094.996756459015,
      "learning_rate": 1.730393883515289e-07,
      "loss": 1.6805,
      "step": 334304
    },
    {
      "epoch": 0.0012133716975419031,
      "grad_norm": 9053.534116575693,
      "learning_rate": 1.730310989402329e-07,
      "loss": 1.689,
      "step": 334336
    },
    {
      "epoch": 0.0012134878318927399,
      "grad_norm": 10248.853594427037,
      "learning_rate": 1.730228107201297e-07,
      "loss": 1.6644,
      "step": 334368
    },
    {
      "epoch": 0.0012136039662435766,
      "grad_norm": 10316.006785573574,
      "learning_rate": 1.7301452369093402e-07,
      "loss": 1.6914,
      "step": 334400
    },
    {
      "epoch": 0.0012137201005944134,
      "grad_norm": 9554.186726247295,
      "learning_rate": 1.730062378523607e-07,
      "loss": 1.6953,
      "step": 334432
    },
    {
      "epoch": 0.0012138362349452501,
      "grad_norm": 7972.8699976859025,
      "learning_rate": 1.7299795320412463e-07,
      "loss": 1.7004,
      "step": 334464
    },
    {
      "epoch": 0.0012139523692960867,
      "grad_norm": 8109.560037387972,
      "learning_rate": 1.7298966974594084e-07,
      "loss": 1.6974,
      "step": 334496
    },
    {
      "epoch": 0.0012140685036469234,
      "grad_norm": 10929.933211140862,
      "learning_rate": 1.7298138747752454e-07,
      "loss": 1.6822,
      "step": 334528
    },
    {
      "epoch": 0.0012141846379977602,
      "grad_norm": 8013.634256690281,
      "learning_rate": 1.729731063985908e-07,
      "loss": 1.677,
      "step": 334560
    },
    {
      "epoch": 0.001214300772348597,
      "grad_norm": 9818.999745391584,
      "learning_rate": 1.7296482650885503e-07,
      "loss": 1.67,
      "step": 334592
    },
    {
      "epoch": 0.0012144169066994335,
      "grad_norm": 11475.729170732464,
      "learning_rate": 1.729565478080326e-07,
      "loss": 1.6726,
      "step": 334624
    },
    {
      "epoch": 0.0012145330410502702,
      "grad_norm": 10592.704470530649,
      "learning_rate": 1.72948270295839e-07,
      "loss": 1.6939,
      "step": 334656
    },
    {
      "epoch": 0.001214649175401107,
      "grad_norm": 11059.932007024274,
      "learning_rate": 1.7293999397198985e-07,
      "loss": 1.6937,
      "step": 334688
    },
    {
      "epoch": 0.0012147653097519437,
      "grad_norm": 8566.580881541948,
      "learning_rate": 1.729317188362008e-07,
      "loss": 1.6844,
      "step": 334720
    },
    {
      "epoch": 0.0012148814441027805,
      "grad_norm": 8750.117713493915,
      "learning_rate": 1.729234448881877e-07,
      "loss": 1.7037,
      "step": 334752
    },
    {
      "epoch": 0.001214997578453617,
      "grad_norm": 10254.548454222642,
      "learning_rate": 1.7291517212766637e-07,
      "loss": 1.7109,
      "step": 334784
    },
    {
      "epoch": 0.0012151137128044538,
      "grad_norm": 8394.559190332748,
      "learning_rate": 1.7290690055435282e-07,
      "loss": 1.7037,
      "step": 334816
    },
    {
      "epoch": 0.0012152298471552905,
      "grad_norm": 8848.191905694632,
      "learning_rate": 1.728986301679631e-07,
      "loss": 1.6981,
      "step": 334848
    },
    {
      "epoch": 0.0012153459815061273,
      "grad_norm": 8986.482070309827,
      "learning_rate": 1.728903609682134e-07,
      "loss": 1.6998,
      "step": 334880
    },
    {
      "epoch": 0.0012154621158569638,
      "grad_norm": 11398.650797353168,
      "learning_rate": 1.7288209295481994e-07,
      "loss": 1.6781,
      "step": 334912
    },
    {
      "epoch": 0.0012155782502078006,
      "grad_norm": 10630.761967046388,
      "learning_rate": 1.7287382612749913e-07,
      "loss": 1.6998,
      "step": 334944
    },
    {
      "epoch": 0.0012156943845586373,
      "grad_norm": 8476.24633903475,
      "learning_rate": 1.7286556048596734e-07,
      "loss": 1.6978,
      "step": 334976
    },
    {
      "epoch": 0.001215810518909474,
      "grad_norm": 18089.632389852482,
      "learning_rate": 1.7285729602994122e-07,
      "loss": 1.7124,
      "step": 335008
    },
    {
      "epoch": 0.0012159266532603108,
      "grad_norm": 9172.865855336597,
      "learning_rate": 1.7284929096841238e-07,
      "loss": 1.7022,
      "step": 335040
    },
    {
      "epoch": 0.0012160427876111474,
      "grad_norm": 10278.439570284976,
      "learning_rate": 1.7284102884552242e-07,
      "loss": 1.6954,
      "step": 335072
    },
    {
      "epoch": 0.0012161589219619841,
      "grad_norm": 9183.359515994132,
      "learning_rate": 1.7283276790729717e-07,
      "loss": 1.6946,
      "step": 335104
    },
    {
      "epoch": 0.001216275056312821,
      "grad_norm": 9482.436290321175,
      "learning_rate": 1.7282450815345347e-07,
      "loss": 1.6887,
      "step": 335136
    },
    {
      "epoch": 0.0012163911906636576,
      "grad_norm": 12113.81822548118,
      "learning_rate": 1.728162495837084e-07,
      "loss": 1.6945,
      "step": 335168
    },
    {
      "epoch": 0.0012165073250144942,
      "grad_norm": 8346.433489820667,
      "learning_rate": 1.7280799219777906e-07,
      "loss": 1.695,
      "step": 335200
    },
    {
      "epoch": 0.001216623459365331,
      "grad_norm": 9761.386172055689,
      "learning_rate": 1.7279973599538262e-07,
      "loss": 1.7125,
      "step": 335232
    },
    {
      "epoch": 0.0012167395937161677,
      "grad_norm": 9280.212066542445,
      "learning_rate": 1.7279148097623644e-07,
      "loss": 1.6801,
      "step": 335264
    },
    {
      "epoch": 0.0012168557280670044,
      "grad_norm": 9554.73662640682,
      "learning_rate": 1.7278322714005782e-07,
      "loss": 1.7016,
      "step": 335296
    },
    {
      "epoch": 0.0012169718624178412,
      "grad_norm": 12302.014306608491,
      "learning_rate": 1.727749744865644e-07,
      "loss": 1.7193,
      "step": 335328
    },
    {
      "epoch": 0.0012170879967686777,
      "grad_norm": 8950.480210580883,
      "learning_rate": 1.727667230154736e-07,
      "loss": 1.7139,
      "step": 335360
    },
    {
      "epoch": 0.0012172041311195145,
      "grad_norm": 10501.83793438082,
      "learning_rate": 1.7275847272650319e-07,
      "loss": 1.6701,
      "step": 335392
    },
    {
      "epoch": 0.0012173202654703512,
      "grad_norm": 13089.27056791172,
      "learning_rate": 1.7275022361937092e-07,
      "loss": 1.6868,
      "step": 335424
    },
    {
      "epoch": 0.001217436399821188,
      "grad_norm": 9368.306997531625,
      "learning_rate": 1.7274197569379467e-07,
      "loss": 1.6849,
      "step": 335456
    },
    {
      "epoch": 0.0012175525341720245,
      "grad_norm": 10497.200960256025,
      "learning_rate": 1.727337289494924e-07,
      "loss": 1.6974,
      "step": 335488
    },
    {
      "epoch": 0.0012176686685228613,
      "grad_norm": 11151.978837856535,
      "learning_rate": 1.7272548338618216e-07,
      "loss": 1.6933,
      "step": 335520
    },
    {
      "epoch": 0.001217784802873698,
      "grad_norm": 8784.470615808332,
      "learning_rate": 1.727172390035821e-07,
      "loss": 1.6939,
      "step": 335552
    },
    {
      "epoch": 0.0012179009372245348,
      "grad_norm": 9348.800350847161,
      "learning_rate": 1.727089958014104e-07,
      "loss": 1.6881,
      "step": 335584
    },
    {
      "epoch": 0.0012180170715753716,
      "grad_norm": 13295.74638747295,
      "learning_rate": 1.7270075377938554e-07,
      "loss": 1.6687,
      "step": 335616
    },
    {
      "epoch": 0.001218133205926208,
      "grad_norm": 8430.321108949529,
      "learning_rate": 1.726925129372258e-07,
      "loss": 1.6753,
      "step": 335648
    },
    {
      "epoch": 0.0012182493402770448,
      "grad_norm": 11919.890267951296,
      "learning_rate": 1.7268427327464985e-07,
      "loss": 1.6863,
      "step": 335680
    },
    {
      "epoch": 0.0012183654746278816,
      "grad_norm": 11342.241224731557,
      "learning_rate": 1.7267603479137618e-07,
      "loss": 1.693,
      "step": 335712
    },
    {
      "epoch": 0.0012184816089787184,
      "grad_norm": 10545.563806644006,
      "learning_rate": 1.726677974871236e-07,
      "loss": 1.6799,
      "step": 335744
    },
    {
      "epoch": 0.001218597743329555,
      "grad_norm": 11371.768903737007,
      "learning_rate": 1.7265956136161087e-07,
      "loss": 1.698,
      "step": 335776
    },
    {
      "epoch": 0.0012187138776803916,
      "grad_norm": 9393.42897987737,
      "learning_rate": 1.726513264145569e-07,
      "loss": 1.71,
      "step": 335808
    },
    {
      "epoch": 0.0012188300120312284,
      "grad_norm": 10213.94791449418,
      "learning_rate": 1.7264309264568065e-07,
      "loss": 1.7333,
      "step": 335840
    },
    {
      "epoch": 0.0012189461463820652,
      "grad_norm": 10347.368554371686,
      "learning_rate": 1.7263486005470126e-07,
      "loss": 1.7321,
      "step": 335872
    },
    {
      "epoch": 0.001219062280732902,
      "grad_norm": 11552.771961741475,
      "learning_rate": 1.7262662864133793e-07,
      "loss": 1.7124,
      "step": 335904
    },
    {
      "epoch": 0.0012191784150837384,
      "grad_norm": 9909.000151377535,
      "learning_rate": 1.726183984053099e-07,
      "loss": 1.7001,
      "step": 335936
    },
    {
      "epoch": 0.0012192945494345752,
      "grad_norm": 9171.74301864155,
      "learning_rate": 1.726101693463365e-07,
      "loss": 1.6905,
      "step": 335968
    },
    {
      "epoch": 0.001219410683785412,
      "grad_norm": 10447.611210224086,
      "learning_rate": 1.726019414641373e-07,
      "loss": 1.6737,
      "step": 336000
    },
    {
      "epoch": 0.0012195268181362487,
      "grad_norm": 17567.117919567798,
      "learning_rate": 1.7259371475843178e-07,
      "loss": 1.6991,
      "step": 336032
    },
    {
      "epoch": 0.0012196429524870852,
      "grad_norm": 9361.789145243552,
      "learning_rate": 1.72585746258935e-07,
      "loss": 1.6919,
      "step": 336064
    },
    {
      "epoch": 0.001219759086837922,
      "grad_norm": 9020.92412117517,
      "learning_rate": 1.7257752186863224e-07,
      "loss": 1.6761,
      "step": 336096
    },
    {
      "epoch": 0.0012198752211887588,
      "grad_norm": 8259.489814752482,
      "learning_rate": 1.7256929865399116e-07,
      "loss": 1.6832,
      "step": 336128
    },
    {
      "epoch": 0.0012199913555395955,
      "grad_norm": 9660.040786663378,
      "learning_rate": 1.7256107661473172e-07,
      "loss": 1.6924,
      "step": 336160
    },
    {
      "epoch": 0.0012201074898904323,
      "grad_norm": 8629.974275743816,
      "learning_rate": 1.725528557505739e-07,
      "loss": 1.71,
      "step": 336192
    },
    {
      "epoch": 0.0012202236242412688,
      "grad_norm": 8912.013801605111,
      "learning_rate": 1.7254463606123783e-07,
      "loss": 1.7148,
      "step": 336224
    },
    {
      "epoch": 0.0012203397585921056,
      "grad_norm": 10103.23888661453,
      "learning_rate": 1.7253641754644373e-07,
      "loss": 1.7096,
      "step": 336256
    },
    {
      "epoch": 0.0012204558929429423,
      "grad_norm": 11321.5654394611,
      "learning_rate": 1.7252820020591192e-07,
      "loss": 1.6829,
      "step": 336288
    },
    {
      "epoch": 0.001220572027293779,
      "grad_norm": 8949.454285038837,
      "learning_rate": 1.7251998403936274e-07,
      "loss": 1.6969,
      "step": 336320
    },
    {
      "epoch": 0.0012206881616446156,
      "grad_norm": 11409.299890878492,
      "learning_rate": 1.7251176904651668e-07,
      "loss": 1.6995,
      "step": 336352
    },
    {
      "epoch": 0.0012208042959954524,
      "grad_norm": 9841.00970429356,
      "learning_rate": 1.725035552270944e-07,
      "loss": 1.7053,
      "step": 336384
    },
    {
      "epoch": 0.001220920430346289,
      "grad_norm": 10828.451782226302,
      "learning_rate": 1.724953425808165e-07,
      "loss": 1.7014,
      "step": 336416
    },
    {
      "epoch": 0.0012210365646971259,
      "grad_norm": 7886.008622871268,
      "learning_rate": 1.7248713110740378e-07,
      "loss": 1.6735,
      "step": 336448
    },
    {
      "epoch": 0.0012211526990479626,
      "grad_norm": 9969.473005129208,
      "learning_rate": 1.7247892080657709e-07,
      "loss": 1.6945,
      "step": 336480
    },
    {
      "epoch": 0.0012212688333987992,
      "grad_norm": 9935.520318533901,
      "learning_rate": 1.724707116780574e-07,
      "loss": 1.6869,
      "step": 336512
    },
    {
      "epoch": 0.001221384967749636,
      "grad_norm": 8109.052719029517,
      "learning_rate": 1.7246250372156574e-07,
      "loss": 1.6933,
      "step": 336544
    },
    {
      "epoch": 0.0012215011021004727,
      "grad_norm": 9996.214683568976,
      "learning_rate": 1.7245429693682324e-07,
      "loss": 1.7024,
      "step": 336576
    },
    {
      "epoch": 0.0012216172364513094,
      "grad_norm": 10205.603754800595,
      "learning_rate": 1.7244609132355115e-07,
      "loss": 1.6875,
      "step": 336608
    },
    {
      "epoch": 0.001221733370802146,
      "grad_norm": 10034.932585722736,
      "learning_rate": 1.7243788688147077e-07,
      "loss": 1.6783,
      "step": 336640
    },
    {
      "epoch": 0.0012218495051529827,
      "grad_norm": 9587.055856726818,
      "learning_rate": 1.7242968361030357e-07,
      "loss": 1.6856,
      "step": 336672
    },
    {
      "epoch": 0.0012219656395038195,
      "grad_norm": 10244.096055777689,
      "learning_rate": 1.7242148150977106e-07,
      "loss": 1.6978,
      "step": 336704
    },
    {
      "epoch": 0.0012220817738546562,
      "grad_norm": 9402.44393761537,
      "learning_rate": 1.724132805795948e-07,
      "loss": 1.7177,
      "step": 336736
    },
    {
      "epoch": 0.001222197908205493,
      "grad_norm": 9999.777597526858,
      "learning_rate": 1.7240508081949648e-07,
      "loss": 1.6938,
      "step": 336768
    },
    {
      "epoch": 0.0012223140425563295,
      "grad_norm": 8809.431082652272,
      "learning_rate": 1.723968822291979e-07,
      "loss": 1.7093,
      "step": 336800
    },
    {
      "epoch": 0.0012224301769071663,
      "grad_norm": 10345.655513306056,
      "learning_rate": 1.72388684808421e-07,
      "loss": 1.7047,
      "step": 336832
    },
    {
      "epoch": 0.001222546311258003,
      "grad_norm": 9747.407860554518,
      "learning_rate": 1.7238048855688767e-07,
      "loss": 1.7055,
      "step": 336864
    },
    {
      "epoch": 0.0012226624456088398,
      "grad_norm": 14236.286594473997,
      "learning_rate": 1.7237229347432003e-07,
      "loss": 1.7058,
      "step": 336896
    },
    {
      "epoch": 0.0012227785799596763,
      "grad_norm": 8648.813213383672,
      "learning_rate": 1.7236409956044026e-07,
      "loss": 1.7133,
      "step": 336928
    },
    {
      "epoch": 0.001222894714310513,
      "grad_norm": 9451.250076048142,
      "learning_rate": 1.7235590681497051e-07,
      "loss": 1.6981,
      "step": 336960
    },
    {
      "epoch": 0.0012230108486613498,
      "grad_norm": 9159.211101399509,
      "learning_rate": 1.7234771523763324e-07,
      "loss": 1.6715,
      "step": 336992
    },
    {
      "epoch": 0.0012231269830121866,
      "grad_norm": 11857.897790080668,
      "learning_rate": 1.7233952482815085e-07,
      "loss": 1.6786,
      "step": 337024
    },
    {
      "epoch": 0.0012232431173630233,
      "grad_norm": 20096.409231502028,
      "learning_rate": 1.7233133558624583e-07,
      "loss": 1.7018,
      "step": 337056
    },
    {
      "epoch": 0.0012233592517138599,
      "grad_norm": 25393.07653672552,
      "learning_rate": 1.723231475116408e-07,
      "loss": 1.7167,
      "step": 337088
    },
    {
      "epoch": 0.0012234753860646966,
      "grad_norm": 20562.74495294828,
      "learning_rate": 1.7231496060405853e-07,
      "loss": 1.7009,
      "step": 337120
    },
    {
      "epoch": 0.0012235915204155334,
      "grad_norm": 9338.527078720712,
      "learning_rate": 1.72307030649965e-07,
      "loss": 1.6854,
      "step": 337152
    },
    {
      "epoch": 0.0012237076547663701,
      "grad_norm": 9160.301741755016,
      "learning_rate": 1.7229884603914875e-07,
      "loss": 1.6875,
      "step": 337184
    },
    {
      "epoch": 0.0012238237891172067,
      "grad_norm": 9993.491281829389,
      "learning_rate": 1.7229066259453262e-07,
      "loss": 1.6891,
      "step": 337216
    },
    {
      "epoch": 0.0012239399234680434,
      "grad_norm": 10861.123698770769,
      "learning_rate": 1.722824803158396e-07,
      "loss": 1.6886,
      "step": 337248
    },
    {
      "epoch": 0.0012240560578188802,
      "grad_norm": 9082.60997731379,
      "learning_rate": 1.7227429920279292e-07,
      "loss": 1.6946,
      "step": 337280
    },
    {
      "epoch": 0.001224172192169717,
      "grad_norm": 9351.773949363831,
      "learning_rate": 1.722661192551158e-07,
      "loss": 1.6725,
      "step": 337312
    },
    {
      "epoch": 0.0012242883265205537,
      "grad_norm": 11624.071231715676,
      "learning_rate": 1.722579404725317e-07,
      "loss": 1.6719,
      "step": 337344
    },
    {
      "epoch": 0.0012244044608713902,
      "grad_norm": 10175.274541750705,
      "learning_rate": 1.7224976285476398e-07,
      "loss": 1.6906,
      "step": 337376
    },
    {
      "epoch": 0.001224520595222227,
      "grad_norm": 10652.439908302698,
      "learning_rate": 1.7224158640153618e-07,
      "loss": 1.7025,
      "step": 337408
    },
    {
      "epoch": 0.0012246367295730637,
      "grad_norm": 10016.533931455531,
      "learning_rate": 1.7223341111257194e-07,
      "loss": 1.6991,
      "step": 337440
    },
    {
      "epoch": 0.0012247528639239005,
      "grad_norm": 9037.688199976807,
      "learning_rate": 1.72225236987595e-07,
      "loss": 1.6992,
      "step": 337472
    },
    {
      "epoch": 0.001224868998274737,
      "grad_norm": 9476.169690333749,
      "learning_rate": 1.7221706402632913e-07,
      "loss": 1.703,
      "step": 337504
    },
    {
      "epoch": 0.0012249851326255738,
      "grad_norm": 10531.054173253502,
      "learning_rate": 1.722088922284983e-07,
      "loss": 1.6942,
      "step": 337536
    },
    {
      "epoch": 0.0012251012669764105,
      "grad_norm": 9778.265490361775,
      "learning_rate": 1.722007215938265e-07,
      "loss": 1.71,
      "step": 337568
    },
    {
      "epoch": 0.0012252174013272473,
      "grad_norm": 7894.186468534931,
      "learning_rate": 1.7219255212203776e-07,
      "loss": 1.7196,
      "step": 337600
    },
    {
      "epoch": 0.001225333535678084,
      "grad_norm": 9895.819117182771,
      "learning_rate": 1.721843838128563e-07,
      "loss": 1.7218,
      "step": 337632
    },
    {
      "epoch": 0.0012254496700289206,
      "grad_norm": 8444.111794617596,
      "learning_rate": 1.721762166660064e-07,
      "loss": 1.7001,
      "step": 337664
    },
    {
      "epoch": 0.0012255658043797573,
      "grad_norm": 7674.221393731092,
      "learning_rate": 1.721680506812124e-07,
      "loss": 1.7041,
      "step": 337696
    },
    {
      "epoch": 0.001225681938730594,
      "grad_norm": 8750.31930845955,
      "learning_rate": 1.721598858581988e-07,
      "loss": 1.7053,
      "step": 337728
    },
    {
      "epoch": 0.0012257980730814308,
      "grad_norm": 8678.264573058372,
      "learning_rate": 1.7215172219669012e-07,
      "loss": 1.7091,
      "step": 337760
    },
    {
      "epoch": 0.0012259142074322674,
      "grad_norm": 10153.420310417569,
      "learning_rate": 1.7214355969641098e-07,
      "loss": 1.6839,
      "step": 337792
    },
    {
      "epoch": 0.0012260303417831041,
      "grad_norm": 9132.519258123686,
      "learning_rate": 1.7213539835708613e-07,
      "loss": 1.6951,
      "step": 337824
    },
    {
      "epoch": 0.0012261464761339409,
      "grad_norm": 8604.685932676452,
      "learning_rate": 1.7212723817844036e-07,
      "loss": 1.7037,
      "step": 337856
    },
    {
      "epoch": 0.0012262626104847776,
      "grad_norm": 7754.9630560048445,
      "learning_rate": 1.7211907916019866e-07,
      "loss": 1.7056,
      "step": 337888
    },
    {
      "epoch": 0.0012263787448356144,
      "grad_norm": 8769.672627869299,
      "learning_rate": 1.7211092130208594e-07,
      "loss": 1.7114,
      "step": 337920
    },
    {
      "epoch": 0.001226494879186451,
      "grad_norm": 8314.313802112596,
      "learning_rate": 1.7210276460382735e-07,
      "loss": 1.725,
      "step": 337952
    },
    {
      "epoch": 0.0012266110135372877,
      "grad_norm": 10764.437003392235,
      "learning_rate": 1.720946090651481e-07,
      "loss": 1.73,
      "step": 337984
    },
    {
      "epoch": 0.0012267271478881244,
      "grad_norm": 8166.281528333443,
      "learning_rate": 1.7208645468577341e-07,
      "loss": 1.7014,
      "step": 338016
    },
    {
      "epoch": 0.0012268432822389612,
      "grad_norm": 8862.852588190779,
      "learning_rate": 1.720783014654287e-07,
      "loss": 1.6966,
      "step": 338048
    },
    {
      "epoch": 0.0012269594165897977,
      "grad_norm": 9372.328632735836,
      "learning_rate": 1.7207014940383936e-07,
      "loss": 1.699,
      "step": 338080
    },
    {
      "epoch": 0.0012270755509406345,
      "grad_norm": 12794.01062997839,
      "learning_rate": 1.72061998500731e-07,
      "loss": 1.7025,
      "step": 338112
    },
    {
      "epoch": 0.0012271916852914712,
      "grad_norm": 19627.951090218256,
      "learning_rate": 1.7205384875582925e-07,
      "loss": 1.6801,
      "step": 338144
    },
    {
      "epoch": 0.001227307819642308,
      "grad_norm": 17197.202098015827,
      "learning_rate": 1.7204570016885986e-07,
      "loss": 1.7086,
      "step": 338176
    },
    {
      "epoch": 0.0012274239539931447,
      "grad_norm": 26580.071030755356,
      "learning_rate": 1.720375527395486e-07,
      "loss": 1.7063,
      "step": 338208
    },
    {
      "epoch": 0.0012275400883439813,
      "grad_norm": 9277.509579623187,
      "learning_rate": 1.720296610211029e-07,
      "loss": 1.7087,
      "step": 338240
    },
    {
      "epoch": 0.001227656222694818,
      "grad_norm": 10167.063686237045,
      "learning_rate": 1.7202151587013023e-07,
      "loss": 1.6958,
      "step": 338272
    },
    {
      "epoch": 0.0012277723570456548,
      "grad_norm": 8400.858289484473,
      "learning_rate": 1.7201337187600234e-07,
      "loss": 1.6839,
      "step": 338304
    },
    {
      "epoch": 0.0012278884913964915,
      "grad_norm": 11361.649880189056,
      "learning_rate": 1.720052290384454e-07,
      "loss": 1.6737,
      "step": 338336
    },
    {
      "epoch": 0.001228004625747328,
      "grad_norm": 11754.116555488125,
      "learning_rate": 1.7199708735718561e-07,
      "loss": 1.6773,
      "step": 338368
    },
    {
      "epoch": 0.0012281207600981648,
      "grad_norm": 8783.790525735458,
      "learning_rate": 1.719889468319494e-07,
      "loss": 1.6789,
      "step": 338400
    },
    {
      "epoch": 0.0012282368944490016,
      "grad_norm": 9976.165195103778,
      "learning_rate": 1.7198080746246315e-07,
      "loss": 1.7014,
      "step": 338432
    },
    {
      "epoch": 0.0012283530287998383,
      "grad_norm": 9610.23724993301,
      "learning_rate": 1.7197266924845352e-07,
      "loss": 1.7037,
      "step": 338464
    },
    {
      "epoch": 0.001228469163150675,
      "grad_norm": 11203.066544477899,
      "learning_rate": 1.719645321896471e-07,
      "loss": 1.6968,
      "step": 338496
    },
    {
      "epoch": 0.0012285852975015116,
      "grad_norm": 9381.788422257241,
      "learning_rate": 1.719563962857706e-07,
      "loss": 1.7214,
      "step": 338528
    },
    {
      "epoch": 0.0012287014318523484,
      "grad_norm": 10012.52056177664,
      "learning_rate": 1.7194826153655082e-07,
      "loss": 1.7397,
      "step": 338560
    },
    {
      "epoch": 0.0012288175662031851,
      "grad_norm": 9505.880285381254,
      "learning_rate": 1.7194012794171471e-07,
      "loss": 1.7134,
      "step": 338592
    },
    {
      "epoch": 0.0012289337005540219,
      "grad_norm": 8952.633802406977,
      "learning_rate": 1.7193199550098922e-07,
      "loss": 1.6857,
      "step": 338624
    },
    {
      "epoch": 0.0012290498349048584,
      "grad_norm": 9119.613807612688,
      "learning_rate": 1.7192386421410153e-07,
      "loss": 1.6864,
      "step": 338656
    },
    {
      "epoch": 0.0012291659692556952,
      "grad_norm": 10165.593538992202,
      "learning_rate": 1.7191573408077873e-07,
      "loss": 1.6708,
      "step": 338688
    },
    {
      "epoch": 0.001229282103606532,
      "grad_norm": 9491.23953970186,
      "learning_rate": 1.7190760510074817e-07,
      "loss": 1.685,
      "step": 338720
    },
    {
      "epoch": 0.0012293982379573687,
      "grad_norm": 8820.478445073148,
      "learning_rate": 1.7189947727373713e-07,
      "loss": 1.6911,
      "step": 338752
    },
    {
      "epoch": 0.0012295143723082054,
      "grad_norm": 9016.472924597512,
      "learning_rate": 1.718913505994731e-07,
      "loss": 1.6865,
      "step": 338784
    },
    {
      "epoch": 0.001229630506659042,
      "grad_norm": 10533.485463036439,
      "learning_rate": 1.7188322507768362e-07,
      "loss": 1.6834,
      "step": 338816
    },
    {
      "epoch": 0.0012297466410098787,
      "grad_norm": 9000.721526633295,
      "learning_rate": 1.7187510070809634e-07,
      "loss": 1.6879,
      "step": 338848
    },
    {
      "epoch": 0.0012298627753607155,
      "grad_norm": 12216.98178765934,
      "learning_rate": 1.7186697749043891e-07,
      "loss": 1.6853,
      "step": 338880
    },
    {
      "epoch": 0.0012299789097115522,
      "grad_norm": 12058.53208313516,
      "learning_rate": 1.7185885542443924e-07,
      "loss": 1.6896,
      "step": 338912
    },
    {
      "epoch": 0.0012300950440623888,
      "grad_norm": 9854.09579819478,
      "learning_rate": 1.7185073450982517e-07,
      "loss": 1.687,
      "step": 338944
    },
    {
      "epoch": 0.0012302111784132255,
      "grad_norm": 8133.743295678811,
      "learning_rate": 1.718426147463247e-07,
      "loss": 1.6842,
      "step": 338976
    },
    {
      "epoch": 0.0012303273127640623,
      "grad_norm": 9366.533830612048,
      "learning_rate": 1.718344961336659e-07,
      "loss": 1.6924,
      "step": 339008
    },
    {
      "epoch": 0.001230443447114899,
      "grad_norm": 12260.22789347735,
      "learning_rate": 1.7182637867157697e-07,
      "loss": 1.6765,
      "step": 339040
    },
    {
      "epoch": 0.0012305595814657358,
      "grad_norm": 10272.182046673433,
      "learning_rate": 1.7181826235978617e-07,
      "loss": 1.6909,
      "step": 339072
    },
    {
      "epoch": 0.0012306757158165723,
      "grad_norm": 9093.766656342134,
      "learning_rate": 1.7181014719802183e-07,
      "loss": 1.7107,
      "step": 339104
    },
    {
      "epoch": 0.001230791850167409,
      "grad_norm": 10184.40572640348,
      "learning_rate": 1.7180203318601239e-07,
      "loss": 1.6988,
      "step": 339136
    },
    {
      "epoch": 0.0012309079845182458,
      "grad_norm": 11191.792349753458,
      "learning_rate": 1.717939203234864e-07,
      "loss": 1.6662,
      "step": 339168
    },
    {
      "epoch": 0.0012310241188690826,
      "grad_norm": 11284.647978559191,
      "learning_rate": 1.7178580861017243e-07,
      "loss": 1.6867,
      "step": 339200
    },
    {
      "epoch": 0.0012311402532199191,
      "grad_norm": 10379.706739595296,
      "learning_rate": 1.7177769804579926e-07,
      "loss": 1.6818,
      "step": 339232
    },
    {
      "epoch": 0.0012312563875707559,
      "grad_norm": 18048.468079036513,
      "learning_rate": 1.7176958863009568e-07,
      "loss": 1.7019,
      "step": 339264
    },
    {
      "epoch": 0.0012313725219215926,
      "grad_norm": 17636.186662654713,
      "learning_rate": 1.7176173372876352e-07,
      "loss": 1.6995,
      "step": 339296
    },
    {
      "epoch": 0.0012314886562724294,
      "grad_norm": 9476.244403770937,
      "learning_rate": 1.7175362657371096e-07,
      "loss": 1.7019,
      "step": 339328
    },
    {
      "epoch": 0.0012316047906232662,
      "grad_norm": 9549.81947473354,
      "learning_rate": 1.7174552056652334e-07,
      "loss": 1.7025,
      "step": 339360
    },
    {
      "epoch": 0.0012317209249741027,
      "grad_norm": 10814.738092066771,
      "learning_rate": 1.7173741570692983e-07,
      "loss": 1.6891,
      "step": 339392
    },
    {
      "epoch": 0.0012318370593249394,
      "grad_norm": 8218.591241812674,
      "learning_rate": 1.7172931199465973e-07,
      "loss": 1.6961,
      "step": 339424
    },
    {
      "epoch": 0.0012319531936757762,
      "grad_norm": 10327.074319476935,
      "learning_rate": 1.717212094294423e-07,
      "loss": 1.7032,
      "step": 339456
    },
    {
      "epoch": 0.001232069328026613,
      "grad_norm": 9629.99937694702,
      "learning_rate": 1.71713108011007e-07,
      "loss": 1.6919,
      "step": 339488
    },
    {
      "epoch": 0.0012321854623774495,
      "grad_norm": 9463.778737903798,
      "learning_rate": 1.7170500773908338e-07,
      "loss": 1.6681,
      "step": 339520
    },
    {
      "epoch": 0.0012323015967282862,
      "grad_norm": 8743.562546239376,
      "learning_rate": 1.7169690861340095e-07,
      "loss": 1.6869,
      "step": 339552
    },
    {
      "epoch": 0.001232417731079123,
      "grad_norm": 9237.909720277634,
      "learning_rate": 1.7168881063368946e-07,
      "loss": 1.6955,
      "step": 339584
    },
    {
      "epoch": 0.0012325338654299597,
      "grad_norm": 9036.399061573145,
      "learning_rate": 1.716807137996787e-07,
      "loss": 1.7095,
      "step": 339616
    },
    {
      "epoch": 0.0012326499997807965,
      "grad_norm": 9581.081567338835,
      "learning_rate": 1.7167261811109853e-07,
      "loss": 1.7065,
      "step": 339648
    },
    {
      "epoch": 0.001232766134131633,
      "grad_norm": 9865.407036711664,
      "learning_rate": 1.7166452356767887e-07,
      "loss": 1.7015,
      "step": 339680
    },
    {
      "epoch": 0.0012328822684824698,
      "grad_norm": 10049.369532463219,
      "learning_rate": 1.7165643016914982e-07,
      "loss": 1.6855,
      "step": 339712
    },
    {
      "epoch": 0.0012329984028333065,
      "grad_norm": 12117.32808832046,
      "learning_rate": 1.7164833791524147e-07,
      "loss": 1.6952,
      "step": 339744
    },
    {
      "epoch": 0.0012331145371841433,
      "grad_norm": 9096.093337251987,
      "learning_rate": 1.7164024680568412e-07,
      "loss": 1.6805,
      "step": 339776
    },
    {
      "epoch": 0.0012332306715349798,
      "grad_norm": 9807.105383343242,
      "learning_rate": 1.71632156840208e-07,
      "loss": 1.6944,
      "step": 339808
    },
    {
      "epoch": 0.0012333468058858166,
      "grad_norm": 10201.384807956221,
      "learning_rate": 1.716240680185436e-07,
      "loss": 1.6831,
      "step": 339840
    },
    {
      "epoch": 0.0012334629402366533,
      "grad_norm": 8670.97180251441,
      "learning_rate": 1.7161598034042132e-07,
      "loss": 1.6701,
      "step": 339872
    },
    {
      "epoch": 0.00123357907458749,
      "grad_norm": 11087.630946239145,
      "learning_rate": 1.7160789380557176e-07,
      "loss": 1.6781,
      "step": 339904
    },
    {
      "epoch": 0.0012336952089383269,
      "grad_norm": 9181.693090056975,
      "learning_rate": 1.7159980841372567e-07,
      "loss": 1.6886,
      "step": 339936
    },
    {
      "epoch": 0.0012338113432891634,
      "grad_norm": 8110.586168705688,
      "learning_rate": 1.7159172416461373e-07,
      "loss": 1.6883,
      "step": 339968
    },
    {
      "epoch": 0.0012339274776400001,
      "grad_norm": 11623.733479394648,
      "learning_rate": 1.715836410579668e-07,
      "loss": 1.692,
      "step": 340000
    },
    {
      "epoch": 0.001234043611990837,
      "grad_norm": 10526.37981454213,
      "learning_rate": 1.7157555909351587e-07,
      "loss": 1.6966,
      "step": 340032
    },
    {
      "epoch": 0.0012341597463416737,
      "grad_norm": 14666.633083294882,
      "learning_rate": 1.715674782709919e-07,
      "loss": 1.669,
      "step": 340064
    },
    {
      "epoch": 0.0012342758806925102,
      "grad_norm": 9637.196687834072,
      "learning_rate": 1.7155939859012602e-07,
      "loss": 1.6836,
      "step": 340096
    },
    {
      "epoch": 0.001234392015043347,
      "grad_norm": 8986.900689336675,
      "learning_rate": 1.7155132005064947e-07,
      "loss": 1.6931,
      "step": 340128
    },
    {
      "epoch": 0.0012345081493941837,
      "grad_norm": 8261.564984916598,
      "learning_rate": 1.715432426522935e-07,
      "loss": 1.7237,
      "step": 340160
    },
    {
      "epoch": 0.0012346242837450205,
      "grad_norm": 10438.957802386214,
      "learning_rate": 1.715351663947895e-07,
      "loss": 1.7092,
      "step": 340192
    },
    {
      "epoch": 0.0012347404180958572,
      "grad_norm": 9342.66214737534,
      "learning_rate": 1.7152709127786898e-07,
      "loss": 1.7108,
      "step": 340224
    },
    {
      "epoch": 0.0012348565524466937,
      "grad_norm": 9815.639968947517,
      "learning_rate": 1.7151901730126344e-07,
      "loss": 1.7121,
      "step": 340256
    },
    {
      "epoch": 0.0012349726867975305,
      "grad_norm": 9392.101149370144,
      "learning_rate": 1.7151094446470455e-07,
      "loss": 1.7192,
      "step": 340288
    },
    {
      "epoch": 0.0012350888211483673,
      "grad_norm": 10757.986986420834,
      "learning_rate": 1.715031249911986e-07,
      "loss": 1.7184,
      "step": 340320
    },
    {
      "epoch": 0.001235204955499204,
      "grad_norm": 9133.632793144248,
      "learning_rate": 1.7149505439832264e-07,
      "loss": 1.7237,
      "step": 340352
    },
    {
      "epoch": 0.0012353210898500405,
      "grad_norm": 9312.453167667476,
      "learning_rate": 1.714869849446972e-07,
      "loss": 1.6852,
      "step": 340384
    },
    {
      "epoch": 0.0012354372242008773,
      "grad_norm": 10374.87002328222,
      "learning_rate": 1.7147891663005422e-07,
      "loss": 1.6624,
      "step": 340416
    },
    {
      "epoch": 0.001235553358551714,
      "grad_norm": 12491.041109531263,
      "learning_rate": 1.7147084945412583e-07,
      "loss": 1.6694,
      "step": 340448
    },
    {
      "epoch": 0.0012356694929025508,
      "grad_norm": 9067.45929133404,
      "learning_rate": 1.7146278341664416e-07,
      "loss": 1.6911,
      "step": 340480
    },
    {
      "epoch": 0.0012357856272533876,
      "grad_norm": 9454.427957311855,
      "learning_rate": 1.7145471851734152e-07,
      "loss": 1.6975,
      "step": 340512
    },
    {
      "epoch": 0.001235901761604224,
      "grad_norm": 10086.510992409616,
      "learning_rate": 1.7144665475595023e-07,
      "loss": 1.6648,
      "step": 340544
    },
    {
      "epoch": 0.0012360178959550609,
      "grad_norm": 10122.499493702137,
      "learning_rate": 1.7143859213220274e-07,
      "loss": 1.6962,
      "step": 340576
    },
    {
      "epoch": 0.0012361340303058976,
      "grad_norm": 11085.214837791824,
      "learning_rate": 1.7143053064583153e-07,
      "loss": 1.6993,
      "step": 340608
    },
    {
      "epoch": 0.0012362501646567344,
      "grad_norm": 9575.100626103102,
      "learning_rate": 1.7142247029656929e-07,
      "loss": 1.6984,
      "step": 340640
    },
    {
      "epoch": 0.001236366299007571,
      "grad_norm": 9471.001742160119,
      "learning_rate": 1.7141441108414868e-07,
      "loss": 1.6989,
      "step": 340672
    },
    {
      "epoch": 0.0012364824333584077,
      "grad_norm": 10108.831386465994,
      "learning_rate": 1.7140635300830246e-07,
      "loss": 1.7109,
      "step": 340704
    },
    {
      "epoch": 0.0012365985677092444,
      "grad_norm": 14884.98317096798,
      "learning_rate": 1.7139829606876356e-07,
      "loss": 1.693,
      "step": 340736
    },
    {
      "epoch": 0.0012367147020600812,
      "grad_norm": 13973.741374449435,
      "learning_rate": 1.713902402652649e-07,
      "loss": 1.674,
      "step": 340768
    },
    {
      "epoch": 0.001236830836410918,
      "grad_norm": 10452.300033963817,
      "learning_rate": 1.7138218559753958e-07,
      "loss": 1.6779,
      "step": 340800
    },
    {
      "epoch": 0.0012369469707617545,
      "grad_norm": 11288.026576864531,
      "learning_rate": 1.7137413206532073e-07,
      "loss": 1.6864,
      "step": 340832
    },
    {
      "epoch": 0.0012370631051125912,
      "grad_norm": 9807.753871299992,
      "learning_rate": 1.7136607966834155e-07,
      "loss": 1.6902,
      "step": 340864
    },
    {
      "epoch": 0.001237179239463428,
      "grad_norm": 10000.795168385363,
      "learning_rate": 1.7135802840633538e-07,
      "loss": 1.6763,
      "step": 340896
    },
    {
      "epoch": 0.0012372953738142647,
      "grad_norm": 12239.117778663624,
      "learning_rate": 1.7134997827903558e-07,
      "loss": 1.6815,
      "step": 340928
    },
    {
      "epoch": 0.0012374115081651013,
      "grad_norm": 9350.948401098147,
      "learning_rate": 1.713419292861757e-07,
      "loss": 1.6958,
      "step": 340960
    },
    {
      "epoch": 0.001237527642515938,
      "grad_norm": 9313.728147202923,
      "learning_rate": 1.7133388142748928e-07,
      "loss": 1.6925,
      "step": 340992
    },
    {
      "epoch": 0.0012376437768667748,
      "grad_norm": 8540.618947125553,
      "learning_rate": 1.7132583470271004e-07,
      "loss": 1.7021,
      "step": 341024
    },
    {
      "epoch": 0.0012377599112176115,
      "grad_norm": 8572.57720875117,
      "learning_rate": 1.7131778911157166e-07,
      "loss": 1.7078,
      "step": 341056
    },
    {
      "epoch": 0.0012378760455684483,
      "grad_norm": 10500.475608276036,
      "learning_rate": 1.7130974465380802e-07,
      "loss": 1.6872,
      "step": 341088
    },
    {
      "epoch": 0.0012379921799192848,
      "grad_norm": 9910.873220862024,
      "learning_rate": 1.7130170132915307e-07,
      "loss": 1.7027,
      "step": 341120
    },
    {
      "epoch": 0.0012381083142701216,
      "grad_norm": 9195.551533214308,
      "learning_rate": 1.7129365913734076e-07,
      "loss": 1.7098,
      "step": 341152
    },
    {
      "epoch": 0.0012382244486209583,
      "grad_norm": 9970.492264677809,
      "learning_rate": 1.7128561807810529e-07,
      "loss": 1.7332,
      "step": 341184
    },
    {
      "epoch": 0.001238340582971795,
      "grad_norm": 9831.126893698403,
      "learning_rate": 1.7127757815118075e-07,
      "loss": 1.7138,
      "step": 341216
    },
    {
      "epoch": 0.0012384567173226316,
      "grad_norm": 8875.8411432382,
      "learning_rate": 1.712695393563015e-07,
      "loss": 1.7138,
      "step": 341248
    },
    {
      "epoch": 0.0012385728516734684,
      "grad_norm": 9529.137841378935,
      "learning_rate": 1.7126150169320184e-07,
      "loss": 1.71,
      "step": 341280
    },
    {
      "epoch": 0.0012386889860243051,
      "grad_norm": 8854.024056890743,
      "learning_rate": 1.712537162861036e-07,
      "loss": 1.692,
      "step": 341312
    },
    {
      "epoch": 0.0012388051203751419,
      "grad_norm": 9619.902286406032,
      "learning_rate": 1.7124568085041912e-07,
      "loss": 1.6881,
      "step": 341344
    },
    {
      "epoch": 0.0012389212547259786,
      "grad_norm": 7988.1793920767705,
      "learning_rate": 1.7123764654572623e-07,
      "loss": 1.6888,
      "step": 341376
    },
    {
      "epoch": 0.0012390373890768152,
      "grad_norm": 14351.152148869442,
      "learning_rate": 1.712296133717596e-07,
      "loss": 1.6874,
      "step": 341408
    },
    {
      "epoch": 0.001239153523427652,
      "grad_norm": 9000.981613135315,
      "learning_rate": 1.71221581328254e-07,
      "loss": 1.6816,
      "step": 341440
    },
    {
      "epoch": 0.0012392696577784887,
      "grad_norm": 9102.996869163473,
      "learning_rate": 1.7121355041494434e-07,
      "loss": 1.6883,
      "step": 341472
    },
    {
      "epoch": 0.0012393857921293254,
      "grad_norm": 8554.828110488252,
      "learning_rate": 1.7120552063156556e-07,
      "loss": 1.6965,
      "step": 341504
    },
    {
      "epoch": 0.001239501926480162,
      "grad_norm": 9172.694696761688,
      "learning_rate": 1.711974919778528e-07,
      "loss": 1.6855,
      "step": 341536
    },
    {
      "epoch": 0.0012396180608309987,
      "grad_norm": 8580.317942827061,
      "learning_rate": 1.7118946445354115e-07,
      "loss": 1.6672,
      "step": 341568
    },
    {
      "epoch": 0.0012397341951818355,
      "grad_norm": 8737.770196108388,
      "learning_rate": 1.7118143805836584e-07,
      "loss": 1.6853,
      "step": 341600
    },
    {
      "epoch": 0.0012398503295326722,
      "grad_norm": 9913.424837058079,
      "learning_rate": 1.7117341279206216e-07,
      "loss": 1.6772,
      "step": 341632
    },
    {
      "epoch": 0.001239966463883509,
      "grad_norm": 9060.11611404622,
      "learning_rate": 1.7116538865436563e-07,
      "loss": 1.6911,
      "step": 341664
    },
    {
      "epoch": 0.0012400825982343455,
      "grad_norm": 9767.685908136073,
      "learning_rate": 1.7115736564501162e-07,
      "loss": 1.693,
      "step": 341696
    },
    {
      "epoch": 0.0012401987325851823,
      "grad_norm": 12483.152486451489,
      "learning_rate": 1.711493437637358e-07,
      "loss": 1.6989,
      "step": 341728
    },
    {
      "epoch": 0.001240314866936019,
      "grad_norm": 12429.099082395313,
      "learning_rate": 1.7114132301027379e-07,
      "loss": 1.6884,
      "step": 341760
    },
    {
      "epoch": 0.0012404310012868558,
      "grad_norm": 8703.194815698429,
      "learning_rate": 1.7113330338436136e-07,
      "loss": 1.6858,
      "step": 341792
    },
    {
      "epoch": 0.0012405471356376923,
      "grad_norm": 8777.16366487489,
      "learning_rate": 1.7112528488573438e-07,
      "loss": 1.694,
      "step": 341824
    },
    {
      "epoch": 0.001240663269988529,
      "grad_norm": 10259.87251382784,
      "learning_rate": 1.711172675141287e-07,
      "loss": 1.7043,
      "step": 341856
    },
    {
      "epoch": 0.0012407794043393658,
      "grad_norm": 8158.01262563377,
      "learning_rate": 1.7110925126928044e-07,
      "loss": 1.6918,
      "step": 341888
    },
    {
      "epoch": 0.0012408955386902026,
      "grad_norm": 8376.885340029432,
      "learning_rate": 1.711012361509256e-07,
      "loss": 1.6775,
      "step": 341920
    },
    {
      "epoch": 0.0012410116730410393,
      "grad_norm": 8538.73187305937,
      "learning_rate": 1.7109322215880046e-07,
      "loss": 1.7036,
      "step": 341952
    },
    {
      "epoch": 0.0012411278073918759,
      "grad_norm": 9704.964502768673,
      "learning_rate": 1.7108520929264122e-07,
      "loss": 1.7334,
      "step": 341984
    },
    {
      "epoch": 0.0012412439417427126,
      "grad_norm": 10224.891979869519,
      "learning_rate": 1.7107719755218426e-07,
      "loss": 1.7235,
      "step": 342016
    },
    {
      "epoch": 0.0012413600760935494,
      "grad_norm": 9448.96904429261,
      "learning_rate": 1.710691869371661e-07,
      "loss": 1.7202,
      "step": 342048
    },
    {
      "epoch": 0.0012414762104443861,
      "grad_norm": 9819.037631051222,
      "learning_rate": 1.7106117744732313e-07,
      "loss": 1.7208,
      "step": 342080
    },
    {
      "epoch": 0.0012415923447952227,
      "grad_norm": 10177.085437393163,
      "learning_rate": 1.710531690823921e-07,
      "loss": 1.6741,
      "step": 342112
    },
    {
      "epoch": 0.0012417084791460594,
      "grad_norm": 8342.7591359214,
      "learning_rate": 1.7104516184210966e-07,
      "loss": 1.6718,
      "step": 342144
    },
    {
      "epoch": 0.0012418246134968962,
      "grad_norm": 10220.318977409657,
      "learning_rate": 1.7103715572621262e-07,
      "loss": 1.6833,
      "step": 342176
    },
    {
      "epoch": 0.001241940747847733,
      "grad_norm": 9805.56821403023,
      "learning_rate": 1.7102915073443781e-07,
      "loss": 1.6896,
      "step": 342208
    },
    {
      "epoch": 0.0012420568821985697,
      "grad_norm": 8639.058629272058,
      "learning_rate": 1.7102114686652228e-07,
      "loss": 1.6843,
      "step": 342240
    },
    {
      "epoch": 0.0012421730165494062,
      "grad_norm": 10607.100640608629,
      "learning_rate": 1.7101314412220302e-07,
      "loss": 1.6787,
      "step": 342272
    },
    {
      "epoch": 0.001242289150900243,
      "grad_norm": 9386.578290303662,
      "learning_rate": 1.710051425012172e-07,
      "loss": 1.7,
      "step": 342304
    },
    {
      "epoch": 0.0012424052852510797,
      "grad_norm": 18575.610245695832,
      "learning_rate": 1.70997142003302e-07,
      "loss": 1.7243,
      "step": 342336
    },
    {
      "epoch": 0.0012425214196019165,
      "grad_norm": 9000.49287539299,
      "learning_rate": 1.7098939259167384e-07,
      "loss": 1.7312,
      "step": 342368
    },
    {
      "epoch": 0.001242637553952753,
      "grad_norm": 9642.29381423321,
      "learning_rate": 1.709813943040364e-07,
      "loss": 1.6869,
      "step": 342400
    },
    {
      "epoch": 0.0012427536883035898,
      "grad_norm": 10276.107434237927,
      "learning_rate": 1.7097339713869001e-07,
      "loss": 1.6845,
      "step": 342432
    },
    {
      "epoch": 0.0012428698226544265,
      "grad_norm": 9348.430991348227,
      "learning_rate": 1.709654010953722e-07,
      "loss": 1.6621,
      "step": 342464
    },
    {
      "epoch": 0.0012429859570052633,
      "grad_norm": 12703.769676753433,
      "learning_rate": 1.7095740617382064e-07,
      "loss": 1.6746,
      "step": 342496
    },
    {
      "epoch": 0.0012431020913561,
      "grad_norm": 11237.702078272052,
      "learning_rate": 1.7094941237377308e-07,
      "loss": 1.7039,
      "step": 342528
    },
    {
      "epoch": 0.0012432182257069366,
      "grad_norm": 9800.568554935984,
      "learning_rate": 1.709414196949673e-07,
      "loss": 1.6953,
      "step": 342560
    },
    {
      "epoch": 0.0012433343600577733,
      "grad_norm": 8855.593260758988,
      "learning_rate": 1.7093342813714128e-07,
      "loss": 1.6753,
      "step": 342592
    },
    {
      "epoch": 0.00124345049440861,
      "grad_norm": 9773.147701738677,
      "learning_rate": 1.7092543770003297e-07,
      "loss": 1.6947,
      "step": 342624
    },
    {
      "epoch": 0.0012435666287594468,
      "grad_norm": 8373.504881469886,
      "learning_rate": 1.709174483833804e-07,
      "loss": 1.6906,
      "step": 342656
    },
    {
      "epoch": 0.0012436827631102834,
      "grad_norm": 8208.093810380093,
      "learning_rate": 1.7090946018692187e-07,
      "loss": 1.709,
      "step": 342688
    },
    {
      "epoch": 0.0012437988974611201,
      "grad_norm": 9498.66369548896,
      "learning_rate": 1.709014731103955e-07,
      "loss": 1.706,
      "step": 342720
    },
    {
      "epoch": 0.0012439150318119569,
      "grad_norm": 9852.543224974961,
      "learning_rate": 1.708934871535397e-07,
      "loss": 1.7117,
      "step": 342752
    },
    {
      "epoch": 0.0012440311661627936,
      "grad_norm": 12718.021072478217,
      "learning_rate": 1.7088550231609282e-07,
      "loss": 1.7081,
      "step": 342784
    },
    {
      "epoch": 0.0012441473005136304,
      "grad_norm": 10007.929056503148,
      "learning_rate": 1.7087751859779345e-07,
      "loss": 1.7058,
      "step": 342816
    },
    {
      "epoch": 0.001244263434864467,
      "grad_norm": 10482.632875380115,
      "learning_rate": 1.7086953599838013e-07,
      "loss": 1.7238,
      "step": 342848
    },
    {
      "epoch": 0.0012443795692153037,
      "grad_norm": 11367.750700996217,
      "learning_rate": 1.7086155451759161e-07,
      "loss": 1.7469,
      "step": 342880
    },
    {
      "epoch": 0.0012444957035661404,
      "grad_norm": 9749.785638669191,
      "learning_rate": 1.7085357415516656e-07,
      "loss": 1.7385,
      "step": 342912
    },
    {
      "epoch": 0.0012446118379169772,
      "grad_norm": 9520.933357607331,
      "learning_rate": 1.7084559491084389e-07,
      "loss": 1.7041,
      "step": 342944
    },
    {
      "epoch": 0.0012447279722678137,
      "grad_norm": 9761.521807587176,
      "learning_rate": 1.7083761678436248e-07,
      "loss": 1.7151,
      "step": 342976
    },
    {
      "epoch": 0.0012448441066186505,
      "grad_norm": 9015.813662670718,
      "learning_rate": 1.708296397754614e-07,
      "loss": 1.7021,
      "step": 343008
    },
    {
      "epoch": 0.0012449602409694872,
      "grad_norm": 7903.028913018096,
      "learning_rate": 1.708216638838798e-07,
      "loss": 1.7018,
      "step": 343040
    },
    {
      "epoch": 0.001245076375320324,
      "grad_norm": 10678.369725758703,
      "learning_rate": 1.7081368910935674e-07,
      "loss": 1.7154,
      "step": 343072
    },
    {
      "epoch": 0.0012451925096711607,
      "grad_norm": 8995.283208437631,
      "learning_rate": 1.7080571545163161e-07,
      "loss": 1.7039,
      "step": 343104
    },
    {
      "epoch": 0.0012453086440219973,
      "grad_norm": 8144.382849547288,
      "learning_rate": 1.7079774291044372e-07,
      "loss": 1.6825,
      "step": 343136
    },
    {
      "epoch": 0.001245424778372834,
      "grad_norm": 10906.980700450515,
      "learning_rate": 1.707897714855325e-07,
      "loss": 1.6857,
      "step": 343168
    },
    {
      "epoch": 0.0012455409127236708,
      "grad_norm": 11781.201806267474,
      "learning_rate": 1.7078180117663754e-07,
      "loss": 1.7023,
      "step": 343200
    },
    {
      "epoch": 0.0012456570470745075,
      "grad_norm": 12009.306391295044,
      "learning_rate": 1.7077383198349844e-07,
      "loss": 1.6998,
      "step": 343232
    },
    {
      "epoch": 0.001245773181425344,
      "grad_norm": 9495.906275864352,
      "learning_rate": 1.7076586390585485e-07,
      "loss": 1.6767,
      "step": 343264
    },
    {
      "epoch": 0.0012458893157761808,
      "grad_norm": 10703.952167307176,
      "learning_rate": 1.7075789694344662e-07,
      "loss": 1.6754,
      "step": 343296
    },
    {
      "epoch": 0.0012460054501270176,
      "grad_norm": 9968.89803338363,
      "learning_rate": 1.707499310960136e-07,
      "loss": 1.6809,
      "step": 343328
    },
    {
      "epoch": 0.0012461215844778543,
      "grad_norm": 8237.50678300176,
      "learning_rate": 1.7074221524432262e-07,
      "loss": 1.6886,
      "step": 343360
    },
    {
      "epoch": 0.001246237718828691,
      "grad_norm": 9558.327468757283,
      "learning_rate": 1.7073425159123713e-07,
      "loss": 1.6946,
      "step": 343392
    },
    {
      "epoch": 0.0012463538531795276,
      "grad_norm": 8530.21218962342,
      "learning_rate": 1.7072628905235517e-07,
      "loss": 1.7064,
      "step": 343424
    },
    {
      "epoch": 0.0012464699875303644,
      "grad_norm": 10646.191055959873,
      "learning_rate": 1.707183276274168e-07,
      "loss": 1.7021,
      "step": 343456
    },
    {
      "epoch": 0.0012465861218812011,
      "grad_norm": 9099.86329567648,
      "learning_rate": 1.7071036731616239e-07,
      "loss": 1.6699,
      "step": 343488
    },
    {
      "epoch": 0.001246702256232038,
      "grad_norm": 10939.459218809676,
      "learning_rate": 1.7070240811833232e-07,
      "loss": 1.6694,
      "step": 343520
    },
    {
      "epoch": 0.0012468183905828744,
      "grad_norm": 11263.265956195832,
      "learning_rate": 1.7069445003366704e-07,
      "loss": 1.6817,
      "step": 343552
    },
    {
      "epoch": 0.0012469345249337112,
      "grad_norm": 10190.372515271461,
      "learning_rate": 1.7068649306190708e-07,
      "loss": 1.696,
      "step": 343584
    },
    {
      "epoch": 0.001247050659284548,
      "grad_norm": 10947.561737665606,
      "learning_rate": 1.7067853720279314e-07,
      "loss": 1.6826,
      "step": 343616
    },
    {
      "epoch": 0.0012471667936353847,
      "grad_norm": 10017.930125529923,
      "learning_rate": 1.7067058245606586e-07,
      "loss": 1.6909,
      "step": 343648
    },
    {
      "epoch": 0.0012472829279862215,
      "grad_norm": 9231.360463117015,
      "learning_rate": 1.7066262882146608e-07,
      "loss": 1.6929,
      "step": 343680
    },
    {
      "epoch": 0.001247399062337058,
      "grad_norm": 9554.10173695047,
      "learning_rate": 1.7065467629873465e-07,
      "loss": 1.7034,
      "step": 343712
    },
    {
      "epoch": 0.0012475151966878947,
      "grad_norm": 12660.749108958758,
      "learning_rate": 1.7064672488761259e-07,
      "loss": 1.7121,
      "step": 343744
    },
    {
      "epoch": 0.0012476313310387315,
      "grad_norm": 9225.139348541028,
      "learning_rate": 1.7063877458784092e-07,
      "loss": 1.7122,
      "step": 343776
    },
    {
      "epoch": 0.0012477474653895683,
      "grad_norm": 10444.582710668723,
      "learning_rate": 1.706308253991608e-07,
      "loss": 1.699,
      "step": 343808
    },
    {
      "epoch": 0.0012478635997404048,
      "grad_norm": 10293.587712746223,
      "learning_rate": 1.7062287732131343e-07,
      "loss": 1.6895,
      "step": 343840
    },
    {
      "epoch": 0.0012479797340912415,
      "grad_norm": 7103.64652836837,
      "learning_rate": 1.7061493035404014e-07,
      "loss": 1.6818,
      "step": 343872
    },
    {
      "epoch": 0.0012480958684420783,
      "grad_norm": 9864.128141908945,
      "learning_rate": 1.7060698449708235e-07,
      "loss": 1.6902,
      "step": 343904
    },
    {
      "epoch": 0.001248212002792915,
      "grad_norm": 9369.841620859981,
      "learning_rate": 1.7059903975018147e-07,
      "loss": 1.7027,
      "step": 343936
    },
    {
      "epoch": 0.0012483281371437518,
      "grad_norm": 9501.738998730705,
      "learning_rate": 1.7059109611307908e-07,
      "loss": 1.688,
      "step": 343968
    },
    {
      "epoch": 0.0012484442714945883,
      "grad_norm": 9445.813252441529,
      "learning_rate": 1.7058315358551683e-07,
      "loss": 1.6972,
      "step": 344000
    },
    {
      "epoch": 0.001248560405845425,
      "grad_norm": 10883.742371078066,
      "learning_rate": 1.705752121672365e-07,
      "loss": 1.6828,
      "step": 344032
    },
    {
      "epoch": 0.0012486765401962619,
      "grad_norm": 12676.127326593087,
      "learning_rate": 1.7056727185797981e-07,
      "loss": 1.6956,
      "step": 344064
    },
    {
      "epoch": 0.0012487926745470986,
      "grad_norm": 8452.622314997872,
      "learning_rate": 1.7055933265748876e-07,
      "loss": 1.7005,
      "step": 344096
    },
    {
      "epoch": 0.0012489088088979351,
      "grad_norm": 10621.420055717597,
      "learning_rate": 1.7055139456550524e-07,
      "loss": 1.7105,
      "step": 344128
    },
    {
      "epoch": 0.001249024943248772,
      "grad_norm": 9375.677895491077,
      "learning_rate": 1.7054345758177138e-07,
      "loss": 1.6743,
      "step": 344160
    },
    {
      "epoch": 0.0012491410775996086,
      "grad_norm": 9649.838236986152,
      "learning_rate": 1.705355217060293e-07,
      "loss": 1.6652,
      "step": 344192
    },
    {
      "epoch": 0.0012492572119504454,
      "grad_norm": 13615.004957766267,
      "learning_rate": 1.7052758693802124e-07,
      "loss": 1.6715,
      "step": 344224
    },
    {
      "epoch": 0.0012493733463012822,
      "grad_norm": 9772.314157864554,
      "learning_rate": 1.7051965327748948e-07,
      "loss": 1.6833,
      "step": 344256
    },
    {
      "epoch": 0.0012494894806521187,
      "grad_norm": 13227.157971385992,
      "learning_rate": 1.7051172072417648e-07,
      "loss": 1.6799,
      "step": 344288
    },
    {
      "epoch": 0.0012496056150029554,
      "grad_norm": 10399.7073035735,
      "learning_rate": 1.7050378927782475e-07,
      "loss": 1.6745,
      "step": 344320
    },
    {
      "epoch": 0.0012497217493537922,
      "grad_norm": 18667.58045382422,
      "learning_rate": 1.7049585893817677e-07,
      "loss": 1.6806,
      "step": 344352
    },
    {
      "epoch": 0.001249837883704629,
      "grad_norm": 18958.006857262186,
      "learning_rate": 1.7048792970497524e-07,
      "loss": 1.6921,
      "step": 344384
    },
    {
      "epoch": 0.0012499540180554655,
      "grad_norm": 9510.767582061922,
      "learning_rate": 1.7048024931519052e-07,
      "loss": 1.6916,
      "step": 344416
    },
    {
      "epoch": 0.0012500701524063022,
      "grad_norm": 15484.264012215757,
      "learning_rate": 1.7047232225955373e-07,
      "loss": 1.6974,
      "step": 344448
    },
    {
      "epoch": 0.001250186286757139,
      "grad_norm": 8515.53004809448,
      "learning_rate": 1.7046439630959985e-07,
      "loss": 1.7113,
      "step": 344480
    },
    {
      "epoch": 0.0012503024211079758,
      "grad_norm": 18557.160881988388,
      "learning_rate": 1.704564714650719e-07,
      "loss": 1.6836,
      "step": 344512
    },
    {
      "epoch": 0.0012504185554588125,
      "grad_norm": 10395.358002493229,
      "learning_rate": 1.7044854772571294e-07,
      "loss": 1.6922,
      "step": 344544
    },
    {
      "epoch": 0.001250534689809649,
      "grad_norm": 9819.254350509513,
      "learning_rate": 1.7044062509126616e-07,
      "loss": 1.691,
      "step": 344576
    },
    {
      "epoch": 0.0012506508241604858,
      "grad_norm": 10255.532555650145,
      "learning_rate": 1.7043270356147472e-07,
      "loss": 1.7076,
      "step": 344608
    },
    {
      "epoch": 0.0012507669585113226,
      "grad_norm": 10040.050199077692,
      "learning_rate": 1.7042478313608204e-07,
      "loss": 1.6897,
      "step": 344640
    },
    {
      "epoch": 0.0012508830928621593,
      "grad_norm": 9012.617377876419,
      "learning_rate": 1.704168638148314e-07,
      "loss": 1.7121,
      "step": 344672
    },
    {
      "epoch": 0.0012509992272129958,
      "grad_norm": 8908.304664749629,
      "learning_rate": 1.704089455974664e-07,
      "loss": 1.7188,
      "step": 344704
    },
    {
      "epoch": 0.0012511153615638326,
      "grad_norm": 8543.473649517508,
      "learning_rate": 1.7040102848373055e-07,
      "loss": 1.7083,
      "step": 344736
    },
    {
      "epoch": 0.0012512314959146694,
      "grad_norm": 8171.852421574927,
      "learning_rate": 1.7039311247336756e-07,
      "loss": 1.6819,
      "step": 344768
    },
    {
      "epoch": 0.0012513476302655061,
      "grad_norm": 7483.497177122472,
      "learning_rate": 1.703851975661211e-07,
      "loss": 1.686,
      "step": 344800
    },
    {
      "epoch": 0.0012514637646163429,
      "grad_norm": 9305.750695134702,
      "learning_rate": 1.70377283761735e-07,
      "loss": 1.6797,
      "step": 344832
    },
    {
      "epoch": 0.0012515798989671794,
      "grad_norm": 12919.39472266406,
      "learning_rate": 1.7036937105995318e-07,
      "loss": 1.6665,
      "step": 344864
    },
    {
      "epoch": 0.0012516960333180162,
      "grad_norm": 9203.531713423929,
      "learning_rate": 1.7036145946051966e-07,
      "loss": 1.6666,
      "step": 344896
    },
    {
      "epoch": 0.001251812167668853,
      "grad_norm": 13233.680818275769,
      "learning_rate": 1.7035354896317844e-07,
      "loss": 1.6882,
      "step": 344928
    },
    {
      "epoch": 0.0012519283020196897,
      "grad_norm": 9461.513198215178,
      "learning_rate": 1.7034563956767372e-07,
      "loss": 1.7025,
      "step": 344960
    },
    {
      "epoch": 0.0012520444363705262,
      "grad_norm": 9672.346767977253,
      "learning_rate": 1.7033773127374975e-07,
      "loss": 1.6935,
      "step": 344992
    },
    {
      "epoch": 0.001252160570721363,
      "grad_norm": 10366.603300985333,
      "learning_rate": 1.703298240811508e-07,
      "loss": 1.7068,
      "step": 345024
    },
    {
      "epoch": 0.0012522767050721997,
      "grad_norm": 8485.97419274888,
      "learning_rate": 1.703219179896213e-07,
      "loss": 1.7001,
      "step": 345056
    },
    {
      "epoch": 0.0012523928394230365,
      "grad_norm": 10445.648663438762,
      "learning_rate": 1.7031401299890573e-07,
      "loss": 1.7029,
      "step": 345088
    },
    {
      "epoch": 0.0012525089737738732,
      "grad_norm": 9791.219331625658,
      "learning_rate": 1.703061091087487e-07,
      "loss": 1.6882,
      "step": 345120
    },
    {
      "epoch": 0.0012526251081247098,
      "grad_norm": 11014.258940119395,
      "learning_rate": 1.7029820631889476e-07,
      "loss": 1.693,
      "step": 345152
    },
    {
      "epoch": 0.0012527412424755465,
      "grad_norm": 9473.944479465772,
      "learning_rate": 1.7029030462908877e-07,
      "loss": 1.6759,
      "step": 345184
    },
    {
      "epoch": 0.0012528573768263833,
      "grad_norm": 10112.764013858921,
      "learning_rate": 1.7028240403907543e-07,
      "loss": 1.6849,
      "step": 345216
    },
    {
      "epoch": 0.00125297351117722,
      "grad_norm": 8483.404387390714,
      "learning_rate": 1.7027450454859974e-07,
      "loss": 1.6886,
      "step": 345248
    },
    {
      "epoch": 0.0012530896455280566,
      "grad_norm": 9407.780184506862,
      "learning_rate": 1.702666061574066e-07,
      "loss": 1.6934,
      "step": 345280
    },
    {
      "epoch": 0.0012532057798788933,
      "grad_norm": 9848.120226723473,
      "learning_rate": 1.7025870886524113e-07,
      "loss": 1.6824,
      "step": 345312
    },
    {
      "epoch": 0.00125332191422973,
      "grad_norm": 9623.537811013162,
      "learning_rate": 1.7025081267184847e-07,
      "loss": 1.6703,
      "step": 345344
    },
    {
      "epoch": 0.0012534380485805668,
      "grad_norm": 10023.248974259794,
      "learning_rate": 1.7024291757697384e-07,
      "loss": 1.6813,
      "step": 345376
    },
    {
      "epoch": 0.0012535541829314036,
      "grad_norm": 15840.752759891178,
      "learning_rate": 1.7023502358036256e-07,
      "loss": 1.6943,
      "step": 345408
    },
    {
      "epoch": 0.0012536703172822401,
      "grad_norm": 17687.471498210252,
      "learning_rate": 1.7022713068176e-07,
      "loss": 1.7081,
      "step": 345440
    },
    {
      "epoch": 0.0012537864516330769,
      "grad_norm": 18609.02275779145,
      "learning_rate": 1.702192388809117e-07,
      "loss": 1.7085,
      "step": 345472
    },
    {
      "epoch": 0.0012539025859839136,
      "grad_norm": 11475.817530790562,
      "learning_rate": 1.702115947454328e-07,
      "loss": 1.7244,
      "step": 345504
    },
    {
      "epoch": 0.0012540187203347504,
      "grad_norm": 8601.212937719889,
      "learning_rate": 1.7020370510504463e-07,
      "loss": 1.6983,
      "step": 345536
    },
    {
      "epoch": 0.001254134854685587,
      "grad_norm": 9181.537561868383,
      "learning_rate": 1.7019581656165555e-07,
      "loss": 1.7151,
      "step": 345568
    },
    {
      "epoch": 0.0012542509890364237,
      "grad_norm": 10277.467976111626,
      "learning_rate": 1.701879291150114e-07,
      "loss": 1.7207,
      "step": 345600
    },
    {
      "epoch": 0.0012543671233872604,
      "grad_norm": 11106.067710940719,
      "learning_rate": 1.7018004276485805e-07,
      "loss": 1.7075,
      "step": 345632
    },
    {
      "epoch": 0.0012544832577380972,
      "grad_norm": 11006.67815464775,
      "learning_rate": 1.7017215751094148e-07,
      "loss": 1.6729,
      "step": 345664
    },
    {
      "epoch": 0.001254599392088934,
      "grad_norm": 9324.230370384465,
      "learning_rate": 1.701642733530077e-07,
      "loss": 1.6804,
      "step": 345696
    },
    {
      "epoch": 0.0012547155264397705,
      "grad_norm": 9469.699256048209,
      "learning_rate": 1.701563902908029e-07,
      "loss": 1.6831,
      "step": 345728
    },
    {
      "epoch": 0.0012548316607906072,
      "grad_norm": 9520.707746801181,
      "learning_rate": 1.7014850832407326e-07,
      "loss": 1.6973,
      "step": 345760
    },
    {
      "epoch": 0.001254947795141444,
      "grad_norm": 8668.354053682855,
      "learning_rate": 1.7014062745256514e-07,
      "loss": 1.6875,
      "step": 345792
    },
    {
      "epoch": 0.0012550639294922807,
      "grad_norm": 10016.467041826674,
      "learning_rate": 1.7013274767602484e-07,
      "loss": 1.69,
      "step": 345824
    },
    {
      "epoch": 0.0012551800638431173,
      "grad_norm": 9760.890533142967,
      "learning_rate": 1.7012486899419888e-07,
      "loss": 1.6968,
      "step": 345856
    },
    {
      "epoch": 0.001255296198193954,
      "grad_norm": 12373.06283827897,
      "learning_rate": 1.701169914068338e-07,
      "loss": 1.68,
      "step": 345888
    },
    {
      "epoch": 0.0012554123325447908,
      "grad_norm": 10787.826472464229,
      "learning_rate": 1.7010911491367622e-07,
      "loss": 1.6698,
      "step": 345920
    },
    {
      "epoch": 0.0012555284668956275,
      "grad_norm": 10809.514882731786,
      "learning_rate": 1.7010123951447287e-07,
      "loss": 1.6896,
      "step": 345952
    },
    {
      "epoch": 0.0012556446012464643,
      "grad_norm": 10281.40710214317,
      "learning_rate": 1.7009336520897053e-07,
      "loss": 1.6928,
      "step": 345984
    },
    {
      "epoch": 0.0012557607355973008,
      "grad_norm": 8915.243126241707,
      "learning_rate": 1.700854919969161e-07,
      "loss": 1.6661,
      "step": 346016
    },
    {
      "epoch": 0.0012558768699481376,
      "grad_norm": 9906.766980200957,
      "learning_rate": 1.700776198780565e-07,
      "loss": 1.6932,
      "step": 346048
    },
    {
      "epoch": 0.0012559930042989743,
      "grad_norm": 10076.218338245753,
      "learning_rate": 1.7006974885213878e-07,
      "loss": 1.7003,
      "step": 346080
    },
    {
      "epoch": 0.001256109138649811,
      "grad_norm": 10346.822507417433,
      "learning_rate": 1.7006187891891008e-07,
      "loss": 1.7169,
      "step": 346112
    },
    {
      "epoch": 0.0012562252730006476,
      "grad_norm": 7560.044444313802,
      "learning_rate": 1.7005401007811757e-07,
      "loss": 1.7231,
      "step": 346144
    },
    {
      "epoch": 0.0012563414073514844,
      "grad_norm": 9850.60810305638,
      "learning_rate": 1.700461423295086e-07,
      "loss": 1.7075,
      "step": 346176
    },
    {
      "epoch": 0.0012564575417023211,
      "grad_norm": 10302.450388135825,
      "learning_rate": 1.7003827567283048e-07,
      "loss": 1.6926,
      "step": 346208
    },
    {
      "epoch": 0.0012565736760531579,
      "grad_norm": 10648.781902170784,
      "learning_rate": 1.7003041010783063e-07,
      "loss": 1.6744,
      "step": 346240
    },
    {
      "epoch": 0.0012566898104039946,
      "grad_norm": 10062.183957769803,
      "learning_rate": 1.7002254563425669e-07,
      "loss": 1.683,
      "step": 346272
    },
    {
      "epoch": 0.0012568059447548312,
      "grad_norm": 10370.540005226343,
      "learning_rate": 1.7001468225185618e-07,
      "loss": 1.7026,
      "step": 346304
    },
    {
      "epoch": 0.001256922079105668,
      "grad_norm": 8134.836199948958,
      "learning_rate": 1.7000681996037683e-07,
      "loss": 1.6857,
      "step": 346336
    },
    {
      "epoch": 0.0012570382134565047,
      "grad_norm": 9812.270073739308,
      "learning_rate": 1.6999895875956639e-07,
      "loss": 1.7098,
      "step": 346368
    },
    {
      "epoch": 0.0012571543478073414,
      "grad_norm": 10838.360392605517,
      "learning_rate": 1.6999109864917276e-07,
      "loss": 1.7057,
      "step": 346400
    },
    {
      "epoch": 0.001257270482158178,
      "grad_norm": 8676.200781448064,
      "learning_rate": 1.6998323962894387e-07,
      "loss": 1.7131,
      "step": 346432
    },
    {
      "epoch": 0.0012573866165090147,
      "grad_norm": 10528.692606397055,
      "learning_rate": 1.6997538169862768e-07,
      "loss": 1.7088,
      "step": 346464
    },
    {
      "epoch": 0.0012575027508598515,
      "grad_norm": 9385.23947483494,
      "learning_rate": 1.6996752485797238e-07,
      "loss": 1.6827,
      "step": 346496
    },
    {
      "epoch": 0.0012576188852106882,
      "grad_norm": 19590.880735689247,
      "learning_rate": 1.6995966910672614e-07,
      "loss": 1.677,
      "step": 346528
    },
    {
      "epoch": 0.001257735019561525,
      "grad_norm": 24873.87802494818,
      "learning_rate": 1.6995181444463716e-07,
      "loss": 1.6798,
      "step": 346560
    },
    {
      "epoch": 0.0012578511539123615,
      "grad_norm": 11028.410583579123,
      "learning_rate": 1.6994420627913585e-07,
      "loss": 1.6933,
      "step": 346592
    },
    {
      "epoch": 0.0012579672882631983,
      "grad_norm": 9591.109216352403,
      "learning_rate": 1.6993635376058998e-07,
      "loss": 1.6791,
      "step": 346624
    },
    {
      "epoch": 0.001258083422614035,
      "grad_norm": 9593.160688740703,
      "learning_rate": 1.6992850233045456e-07,
      "loss": 1.7054,
      "step": 346656
    },
    {
      "epoch": 0.0012581995569648718,
      "grad_norm": 9104.520196034495,
      "learning_rate": 1.6992065198847814e-07,
      "loss": 1.7057,
      "step": 346688
    },
    {
      "epoch": 0.0012583156913157083,
      "grad_norm": 9911.557496175867,
      "learning_rate": 1.6991280273440945e-07,
      "loss": 1.7018,
      "step": 346720
    },
    {
      "epoch": 0.001258431825666545,
      "grad_norm": 9660.771397771505,
      "learning_rate": 1.699049545679972e-07,
      "loss": 1.7027,
      "step": 346752
    },
    {
      "epoch": 0.0012585479600173818,
      "grad_norm": 9955.714941680482,
      "learning_rate": 1.6989710748899026e-07,
      "loss": 1.6759,
      "step": 346784
    },
    {
      "epoch": 0.0012586640943682186,
      "grad_norm": 9952.8564743997,
      "learning_rate": 1.698892614971375e-07,
      "loss": 1.6733,
      "step": 346816
    },
    {
      "epoch": 0.0012587802287190553,
      "grad_norm": 11346.657657654081,
      "learning_rate": 1.6988141659218796e-07,
      "loss": 1.6897,
      "step": 346848
    },
    {
      "epoch": 0.0012588963630698919,
      "grad_norm": 9505.196999536622,
      "learning_rate": 1.6987357277389063e-07,
      "loss": 1.6816,
      "step": 346880
    },
    {
      "epoch": 0.0012590124974207286,
      "grad_norm": 8952.568346569604,
      "learning_rate": 1.6986573004199478e-07,
      "loss": 1.6736,
      "step": 346912
    },
    {
      "epoch": 0.0012591286317715654,
      "grad_norm": 11131.101472900156,
      "learning_rate": 1.6985788839624959e-07,
      "loss": 1.6998,
      "step": 346944
    },
    {
      "epoch": 0.0012592447661224021,
      "grad_norm": 9929.688816876387,
      "learning_rate": 1.6985004783640437e-07,
      "loss": 1.696,
      "step": 346976
    },
    {
      "epoch": 0.0012593609004732387,
      "grad_norm": 8714.054165542007,
      "learning_rate": 1.6984220836220855e-07,
      "loss": 1.7109,
      "step": 347008
    },
    {
      "epoch": 0.0012594770348240754,
      "grad_norm": 11250.91391843347,
      "learning_rate": 1.6983436997341159e-07,
      "loss": 1.6845,
      "step": 347040
    },
    {
      "epoch": 0.0012595931691749122,
      "grad_norm": 8743.906220906078,
      "learning_rate": 1.69826532669763e-07,
      "loss": 1.6708,
      "step": 347072
    },
    {
      "epoch": 0.001259709303525749,
      "grad_norm": 11235.049799622608,
      "learning_rate": 1.6981869645101257e-07,
      "loss": 1.6775,
      "step": 347104
    },
    {
      "epoch": 0.0012598254378765857,
      "grad_norm": 10440.387732263587,
      "learning_rate": 1.698108613169099e-07,
      "loss": 1.6918,
      "step": 347136
    },
    {
      "epoch": 0.0012599415722274222,
      "grad_norm": 10301.68762873346,
      "learning_rate": 1.698030272672048e-07,
      "loss": 1.6998,
      "step": 347168
    },
    {
      "epoch": 0.001260057706578259,
      "grad_norm": 13702.508237545415,
      "learning_rate": 1.6979519430164723e-07,
      "loss": 1.6963,
      "step": 347200
    },
    {
      "epoch": 0.0012601738409290957,
      "grad_norm": 9334.735882712483,
      "learning_rate": 1.6978736241998707e-07,
      "loss": 1.7217,
      "step": 347232
    },
    {
      "epoch": 0.0012602899752799325,
      "grad_norm": 10769.852738083284,
      "learning_rate": 1.697795316219744e-07,
      "loss": 1.715,
      "step": 347264
    },
    {
      "epoch": 0.001260406109630769,
      "grad_norm": 10628.560579871575,
      "learning_rate": 1.697717019073594e-07,
      "loss": 1.7123,
      "step": 347296
    },
    {
      "epoch": 0.0012605222439816058,
      "grad_norm": 8796.837840951714,
      "learning_rate": 1.6976387327589218e-07,
      "loss": 1.7139,
      "step": 347328
    },
    {
      "epoch": 0.0012606383783324425,
      "grad_norm": 9850.383748869888,
      "learning_rate": 1.697560457273231e-07,
      "loss": 1.6883,
      "step": 347360
    },
    {
      "epoch": 0.0012607545126832793,
      "grad_norm": 10570.55760118642,
      "learning_rate": 1.6974821926140254e-07,
      "loss": 1.6807,
      "step": 347392
    },
    {
      "epoch": 0.001260870647034116,
      "grad_norm": 9518.829024622724,
      "learning_rate": 1.697403938778809e-07,
      "loss": 1.6802,
      "step": 347424
    },
    {
      "epoch": 0.0012609867813849526,
      "grad_norm": 7858.284800132915,
      "learning_rate": 1.697325695765087e-07,
      "loss": 1.689,
      "step": 347456
    },
    {
      "epoch": 0.0012611029157357893,
      "grad_norm": 10262.56088897893,
      "learning_rate": 1.697247463570366e-07,
      "loss": 1.6705,
      "step": 347488
    },
    {
      "epoch": 0.001261219050086626,
      "grad_norm": 10021.58071363994,
      "learning_rate": 1.6971692421921528e-07,
      "loss": 1.6924,
      "step": 347520
    },
    {
      "epoch": 0.0012613351844374628,
      "grad_norm": 9261.156299296541,
      "learning_rate": 1.697091031627955e-07,
      "loss": 1.6947,
      "step": 347552
    },
    {
      "epoch": 0.0012614513187882994,
      "grad_norm": 17524.595972518167,
      "learning_rate": 1.6970128318752813e-07,
      "loss": 1.7042,
      "step": 347584
    },
    {
      "epoch": 0.0012615674531391361,
      "grad_norm": 17562.33834089299,
      "learning_rate": 1.6969346429316403e-07,
      "loss": 1.7177,
      "step": 347616
    },
    {
      "epoch": 0.0012616835874899729,
      "grad_norm": 22615.96816410918,
      "learning_rate": 1.6968564647945433e-07,
      "loss": 1.6919,
      "step": 347648
    },
    {
      "epoch": 0.0012617997218408096,
      "grad_norm": 16403.80224216325,
      "learning_rate": 1.6967782974615003e-07,
      "loss": 1.6884,
      "step": 347680
    },
    {
      "epoch": 0.0012619158561916464,
      "grad_norm": 23576.389884797885,
      "learning_rate": 1.6967001409300235e-07,
      "loss": 1.7069,
      "step": 347712
    },
    {
      "epoch": 0.001262031990542483,
      "grad_norm": 16044.315878216808,
      "learning_rate": 1.6966219951976253e-07,
      "loss": 1.7114,
      "step": 347744
    },
    {
      "epoch": 0.0012621481248933197,
      "grad_norm": 10878.92770451206,
      "learning_rate": 1.6965463018151628e-07,
      "loss": 1.7001,
      "step": 347776
    },
    {
      "epoch": 0.0012622642592441564,
      "grad_norm": 9295.99440619453,
      "learning_rate": 1.6964681773361842e-07,
      "loss": 1.7086,
      "step": 347808
    },
    {
      "epoch": 0.0012623803935949932,
      "grad_norm": 10124.22737792865,
      "learning_rate": 1.6963900636489046e-07,
      "loss": 1.6989,
      "step": 347840
    },
    {
      "epoch": 0.0012624965279458297,
      "grad_norm": 9128.772973406667,
      "learning_rate": 1.6963119607508393e-07,
      "loss": 1.7042,
      "step": 347872
    },
    {
      "epoch": 0.0012626126622966665,
      "grad_norm": 9229.453071553049,
      "learning_rate": 1.6962338686395048e-07,
      "loss": 1.7,
      "step": 347904
    },
    {
      "epoch": 0.0012627287966475032,
      "grad_norm": 10430.292805094208,
      "learning_rate": 1.6961557873124186e-07,
      "loss": 1.6867,
      "step": 347936
    },
    {
      "epoch": 0.00126284493099834,
      "grad_norm": 8867.435480453185,
      "learning_rate": 1.6960777167670988e-07,
      "loss": 1.6869,
      "step": 347968
    },
    {
      "epoch": 0.0012629610653491768,
      "grad_norm": 15091.373429877083,
      "learning_rate": 1.6959996570010642e-07,
      "loss": 1.7017,
      "step": 348000
    },
    {
      "epoch": 0.0012630771997000133,
      "grad_norm": 8206.94547319525,
      "learning_rate": 1.6959216080118345e-07,
      "loss": 1.6963,
      "step": 348032
    },
    {
      "epoch": 0.00126319333405085,
      "grad_norm": 10549.253433300386,
      "learning_rate": 1.69584356979693e-07,
      "loss": 1.6903,
      "step": 348064
    },
    {
      "epoch": 0.0012633094684016868,
      "grad_norm": 9224.02254984234,
      "learning_rate": 1.6957655423538729e-07,
      "loss": 1.7179,
      "step": 348096
    },
    {
      "epoch": 0.0012634256027525236,
      "grad_norm": 10173.264176261226,
      "learning_rate": 1.6956875256801842e-07,
      "loss": 1.7299,
      "step": 348128
    },
    {
      "epoch": 0.00126354173710336,
      "grad_norm": 10734.122786702228,
      "learning_rate": 1.6956095197733878e-07,
      "loss": 1.7139,
      "step": 348160
    },
    {
      "epoch": 0.0012636578714541968,
      "grad_norm": 9286.928232736593,
      "learning_rate": 1.6955315246310066e-07,
      "loss": 1.7018,
      "step": 348192
    },
    {
      "epoch": 0.0012637740058050336,
      "grad_norm": 8287.699680852342,
      "learning_rate": 1.6954535402505652e-07,
      "loss": 1.6991,
      "step": 348224
    },
    {
      "epoch": 0.0012638901401558704,
      "grad_norm": 11620.282612742256,
      "learning_rate": 1.6953755666295895e-07,
      "loss": 1.6899,
      "step": 348256
    },
    {
      "epoch": 0.001264006274506707,
      "grad_norm": 9753.19127260406,
      "learning_rate": 1.6952976037656053e-07,
      "loss": 1.6966,
      "step": 348288
    },
    {
      "epoch": 0.0012641224088575436,
      "grad_norm": 9789.11303438672,
      "learning_rate": 1.695219651656139e-07,
      "loss": 1.6866,
      "step": 348320
    },
    {
      "epoch": 0.0012642385432083804,
      "grad_norm": 10169.233402769356,
      "learning_rate": 1.695141710298719e-07,
      "loss": 1.6678,
      "step": 348352
    },
    {
      "epoch": 0.0012643546775592172,
      "grad_norm": 10632.505913471197,
      "learning_rate": 1.6950637796908732e-07,
      "loss": 1.6868,
      "step": 348384
    },
    {
      "epoch": 0.001264470811910054,
      "grad_norm": 8626.039067845682,
      "learning_rate": 1.6949858598301316e-07,
      "loss": 1.6857,
      "step": 348416
    },
    {
      "epoch": 0.0012645869462608904,
      "grad_norm": 8999.87355466731,
      "learning_rate": 1.6949079507140233e-07,
      "loss": 1.6955,
      "step": 348448
    },
    {
      "epoch": 0.0012647030806117272,
      "grad_norm": 11001.638423434939,
      "learning_rate": 1.6948300523400802e-07,
      "loss": 1.6998,
      "step": 348480
    },
    {
      "epoch": 0.001264819214962564,
      "grad_norm": 8345.011084474363,
      "learning_rate": 1.6947521647058336e-07,
      "loss": 1.6813,
      "step": 348512
    },
    {
      "epoch": 0.0012649353493134007,
      "grad_norm": 11873.852112941277,
      "learning_rate": 1.6946742878088156e-07,
      "loss": 1.6632,
      "step": 348544
    },
    {
      "epoch": 0.0012650514836642375,
      "grad_norm": 7990.406873244941,
      "learning_rate": 1.6945964216465593e-07,
      "loss": 1.6845,
      "step": 348576
    },
    {
      "epoch": 0.001265167618015074,
      "grad_norm": 9345.634488893731,
      "learning_rate": 1.6945185662165996e-07,
      "loss": 1.675,
      "step": 348608
    },
    {
      "epoch": 0.0012652837523659108,
      "grad_norm": 9903.500795173391,
      "learning_rate": 1.6944407215164708e-07,
      "loss": 1.6742,
      "step": 348640
    },
    {
      "epoch": 0.0012653998867167475,
      "grad_norm": 9926.623595160643,
      "learning_rate": 1.6943628875437085e-07,
      "loss": 1.6945,
      "step": 348672
    },
    {
      "epoch": 0.0012655160210675843,
      "grad_norm": 9968.442004646464,
      "learning_rate": 1.6942850642958494e-07,
      "loss": 1.689,
      "step": 348704
    },
    {
      "epoch": 0.0012656321554184208,
      "grad_norm": 7345.762588050338,
      "learning_rate": 1.6942072517704304e-07,
      "loss": 1.687,
      "step": 348736
    },
    {
      "epoch": 0.0012657482897692576,
      "grad_norm": 22294.206960553678,
      "learning_rate": 1.6941294499649897e-07,
      "loss": 1.688,
      "step": 348768
    },
    {
      "epoch": 0.0012658644241200943,
      "grad_norm": 15971.734032346018,
      "learning_rate": 1.694051658877066e-07,
      "loss": 1.6794,
      "step": 348800
    },
    {
      "epoch": 0.001265980558470931,
      "grad_norm": 21851.883214039015,
      "learning_rate": 1.693973878504199e-07,
      "loss": 1.6871,
      "step": 348832
    },
    {
      "epoch": 0.0012660966928217678,
      "grad_norm": 10551.31650553617,
      "learning_rate": 1.6938985389836834e-07,
      "loss": 1.7029,
      "step": 348864
    },
    {
      "epoch": 0.0012662128271726044,
      "grad_norm": 10249.239581549453,
      "learning_rate": 1.693820779698897e-07,
      "loss": 1.672,
      "step": 348896
    },
    {
      "epoch": 0.001266328961523441,
      "grad_norm": 10285.44155590804,
      "learning_rate": 1.6937430311218677e-07,
      "loss": 1.6808,
      "step": 348928
    },
    {
      "epoch": 0.0012664450958742779,
      "grad_norm": 9181.861249223928,
      "learning_rate": 1.693665293250138e-07,
      "loss": 1.701,
      "step": 348960
    },
    {
      "epoch": 0.0012665612302251146,
      "grad_norm": 10521.775515567702,
      "learning_rate": 1.6935875660812512e-07,
      "loss": 1.712,
      "step": 348992
    },
    {
      "epoch": 0.0012666773645759511,
      "grad_norm": 9658.378538864585,
      "learning_rate": 1.6935098496127523e-07,
      "loss": 1.7212,
      "step": 349024
    },
    {
      "epoch": 0.001266793498926788,
      "grad_norm": 11077.408541712271,
      "learning_rate": 1.6934321438421856e-07,
      "loss": 1.697,
      "step": 349056
    },
    {
      "epoch": 0.0012669096332776247,
      "grad_norm": 9513.87471012731,
      "learning_rate": 1.6933544487670978e-07,
      "loss": 1.6957,
      "step": 349088
    },
    {
      "epoch": 0.0012670257676284614,
      "grad_norm": 13226.013004681341,
      "learning_rate": 1.6932767643850349e-07,
      "loss": 1.684,
      "step": 349120
    },
    {
      "epoch": 0.0012671419019792982,
      "grad_norm": 12366.786809838683,
      "learning_rate": 1.6931990906935447e-07,
      "loss": 1.6797,
      "step": 349152
    },
    {
      "epoch": 0.0012672580363301347,
      "grad_norm": 10519.416713867742,
      "learning_rate": 1.6931214276901752e-07,
      "loss": 1.6659,
      "step": 349184
    },
    {
      "epoch": 0.0012673741706809715,
      "grad_norm": 8628.228555155456,
      "learning_rate": 1.6930437753724757e-07,
      "loss": 1.6734,
      "step": 349216
    },
    {
      "epoch": 0.0012674903050318082,
      "grad_norm": 9521.020113412218,
      "learning_rate": 1.692966133737996e-07,
      "loss": 1.6925,
      "step": 349248
    },
    {
      "epoch": 0.001267606439382645,
      "grad_norm": 8430.786677410359,
      "learning_rate": 1.6928885027842864e-07,
      "loss": 1.6864,
      "step": 349280
    },
    {
      "epoch": 0.0012677225737334815,
      "grad_norm": 8294.805482951364,
      "learning_rate": 1.6928108825088986e-07,
      "loss": 1.6914,
      "step": 349312
    },
    {
      "epoch": 0.0012678387080843183,
      "grad_norm": 10541.423623021703,
      "learning_rate": 1.6927332729093846e-07,
      "loss": 1.6964,
      "step": 349344
    },
    {
      "epoch": 0.001267954842435155,
      "grad_norm": 10056.498595435689,
      "learning_rate": 1.6926556739832976e-07,
      "loss": 1.6982,
      "step": 349376
    },
    {
      "epoch": 0.0012680709767859918,
      "grad_norm": 10734.318236385578,
      "learning_rate": 1.692578085728191e-07,
      "loss": 1.692,
      "step": 349408
    },
    {
      "epoch": 0.0012681871111368285,
      "grad_norm": 9293.803527081902,
      "learning_rate": 1.6925005081416196e-07,
      "loss": 1.6788,
      "step": 349440
    },
    {
      "epoch": 0.001268303245487665,
      "grad_norm": 10680.199810864962,
      "learning_rate": 1.6924229412211386e-07,
      "loss": 1.6696,
      "step": 349472
    },
    {
      "epoch": 0.0012684193798385018,
      "grad_norm": 9923.12350018884,
      "learning_rate": 1.692345384964304e-07,
      "loss": 1.6682,
      "step": 349504
    },
    {
      "epoch": 0.0012685355141893386,
      "grad_norm": 10749.771532455934,
      "learning_rate": 1.6922678393686732e-07,
      "loss": 1.6901,
      "step": 349536
    },
    {
      "epoch": 0.0012686516485401753,
      "grad_norm": 9870.666238912143,
      "learning_rate": 1.692190304431803e-07,
      "loss": 1.7028,
      "step": 349568
    },
    {
      "epoch": 0.0012687677828910119,
      "grad_norm": 11690.212316292635,
      "learning_rate": 1.6921127801512528e-07,
      "loss": 1.6832,
      "step": 349600
    },
    {
      "epoch": 0.0012688839172418486,
      "grad_norm": 9810.739421674596,
      "learning_rate": 1.692035266524581e-07,
      "loss": 1.687,
      "step": 349632
    },
    {
      "epoch": 0.0012690000515926854,
      "grad_norm": 11214.402703666388,
      "learning_rate": 1.691957763549348e-07,
      "loss": 1.6733,
      "step": 349664
    },
    {
      "epoch": 0.0012691161859435221,
      "grad_norm": 9436.848838462975,
      "learning_rate": 1.691880271223115e-07,
      "loss": 1.6658,
      "step": 349696
    },
    {
      "epoch": 0.0012692323202943589,
      "grad_norm": 10220.981166209045,
      "learning_rate": 1.6918027895434425e-07,
      "loss": 1.6913,
      "step": 349728
    },
    {
      "epoch": 0.0012693484546451954,
      "grad_norm": 9995.634447097393,
      "learning_rate": 1.6917253185078943e-07,
      "loss": 1.6731,
      "step": 349760
    },
    {
      "epoch": 0.0012694645889960322,
      "grad_norm": 8137.4835176484385,
      "learning_rate": 1.6916478581140323e-07,
      "loss": 1.6796,
      "step": 349792
    },
    {
      "epoch": 0.001269580723346869,
      "grad_norm": 10800.187776145376,
      "learning_rate": 1.6915704083594208e-07,
      "loss": 1.7016,
      "step": 349824
    },
    {
      "epoch": 0.0012696968576977057,
      "grad_norm": 18507.84222971441,
      "learning_rate": 1.6914929692416251e-07,
      "loss": 1.7067,
      "step": 349856
    },
    {
      "epoch": 0.0012698129920485422,
      "grad_norm": 8818.720655514608,
      "learning_rate": 1.691417960237371e-07,
      "loss": 1.7257,
      "step": 349888
    },
    {
      "epoch": 0.001269929126399379,
      "grad_norm": 10561.777312554928,
      "learning_rate": 1.6913405420536914e-07,
      "loss": 1.7186,
      "step": 349920
    },
    {
      "epoch": 0.0012700452607502157,
      "grad_norm": 8554.98018700219,
      "learning_rate": 1.6912631344996018e-07,
      "loss": 1.7082,
      "step": 349952
    },
    {
      "epoch": 0.0012701613951010525,
      "grad_norm": 8966.025206299611,
      "learning_rate": 1.6911857375726702e-07,
      "loss": 1.6914,
      "step": 349984
    },
    {
      "epoch": 0.0012702775294518892,
      "grad_norm": 9266.111050489304,
      "learning_rate": 1.6911083512704652e-07,
      "loss": 1.6904,
      "step": 350016
    },
    {
      "epoch": 0.0012703936638027258,
      "grad_norm": 9024.348840775161,
      "learning_rate": 1.6910309755905563e-07,
      "loss": 1.6682,
      "step": 350048
    },
    {
      "epoch": 0.0012705097981535625,
      "grad_norm": 10165.873105641247,
      "learning_rate": 1.690953610530513e-07,
      "loss": 1.6921,
      "step": 350080
    },
    {
      "epoch": 0.0012706259325043993,
      "grad_norm": 8645.020994769186,
      "learning_rate": 1.6908762560879066e-07,
      "loss": 1.6991,
      "step": 350112
    },
    {
      "epoch": 0.001270742066855236,
      "grad_norm": 10342.759593067993,
      "learning_rate": 1.690798912260309e-07,
      "loss": 1.6977,
      "step": 350144
    },
    {
      "epoch": 0.0012708582012060726,
      "grad_norm": 9365.914157197898,
      "learning_rate": 1.6907215790452923e-07,
      "loss": 1.6809,
      "step": 350176
    },
    {
      "epoch": 0.0012709743355569093,
      "grad_norm": 8797.86280865984,
      "learning_rate": 1.69064425644043e-07,
      "loss": 1.6801,
      "step": 350208
    },
    {
      "epoch": 0.001271090469907746,
      "grad_norm": 10367.64312657414,
      "learning_rate": 1.6905669444432964e-07,
      "loss": 1.6917,
      "step": 350240
    },
    {
      "epoch": 0.0012712066042585828,
      "grad_norm": 11820.99623551247,
      "learning_rate": 1.6904896430514653e-07,
      "loss": 1.6916,
      "step": 350272
    },
    {
      "epoch": 0.0012713227386094196,
      "grad_norm": 9423.429736566193,
      "learning_rate": 1.6904123522625136e-07,
      "loss": 1.6817,
      "step": 350304
    },
    {
      "epoch": 0.0012714388729602561,
      "grad_norm": 9179.888016746174,
      "learning_rate": 1.6903350720740167e-07,
      "loss": 1.666,
      "step": 350336
    },
    {
      "epoch": 0.0012715550073110929,
      "grad_norm": 10064.002583465488,
      "learning_rate": 1.6902578024835519e-07,
      "loss": 1.6903,
      "step": 350368
    },
    {
      "epoch": 0.0012716711416619296,
      "grad_norm": 10642.310275499394,
      "learning_rate": 1.6901805434886973e-07,
      "loss": 1.6966,
      "step": 350400
    },
    {
      "epoch": 0.0012717872760127664,
      "grad_norm": 10978.260153594467,
      "learning_rate": 1.6901032950870316e-07,
      "loss": 1.7019,
      "step": 350432
    },
    {
      "epoch": 0.001271903410363603,
      "grad_norm": 8643.518033763798,
      "learning_rate": 1.690026057276134e-07,
      "loss": 1.7087,
      "step": 350464
    },
    {
      "epoch": 0.0012720195447144397,
      "grad_norm": 11855.758432086915,
      "learning_rate": 1.689948830053585e-07,
      "loss": 1.6856,
      "step": 350496
    },
    {
      "epoch": 0.0012721356790652764,
      "grad_norm": 11974.211623317837,
      "learning_rate": 1.6898716134169654e-07,
      "loss": 1.673,
      "step": 350528
    },
    {
      "epoch": 0.0012722518134161132,
      "grad_norm": 9716.547123335531,
      "learning_rate": 1.6897944073638572e-07,
      "loss": 1.6818,
      "step": 350560
    },
    {
      "epoch": 0.00127236794776695,
      "grad_norm": 9874.027344503356,
      "learning_rate": 1.6897172118918429e-07,
      "loss": 1.6805,
      "step": 350592
    },
    {
      "epoch": 0.0012724840821177865,
      "grad_norm": 9443.562992853915,
      "learning_rate": 1.6896400269985055e-07,
      "loss": 1.6743,
      "step": 350624
    },
    {
      "epoch": 0.0012726002164686232,
      "grad_norm": 9631.234188825438,
      "learning_rate": 1.6895628526814294e-07,
      "loss": 1.7011,
      "step": 350656
    },
    {
      "epoch": 0.00127271635081946,
      "grad_norm": 10906.820985053344,
      "learning_rate": 1.6894856889381993e-07,
      "loss": 1.6914,
      "step": 350688
    },
    {
      "epoch": 0.0012728324851702967,
      "grad_norm": 14004.118965504398,
      "learning_rate": 1.6894085357664013e-07,
      "loss": 1.6922,
      "step": 350720
    },
    {
      "epoch": 0.0012729486195211333,
      "grad_norm": 9069.52369201382,
      "learning_rate": 1.6893313931636212e-07,
      "loss": 1.7087,
      "step": 350752
    },
    {
      "epoch": 0.00127306475387197,
      "grad_norm": 11792.445378291985,
      "learning_rate": 1.6892542611274463e-07,
      "loss": 1.6935,
      "step": 350784
    },
    {
      "epoch": 0.0012731808882228068,
      "grad_norm": 8981.896124983856,
      "learning_rate": 1.6891771396554646e-07,
      "loss": 1.6898,
      "step": 350816
    },
    {
      "epoch": 0.0012732970225736435,
      "grad_norm": 10302.381860521382,
      "learning_rate": 1.689100028745265e-07,
      "loss": 1.7,
      "step": 350848
    },
    {
      "epoch": 0.0012734131569244803,
      "grad_norm": 24084.010629461198,
      "learning_rate": 1.6890229283944373e-07,
      "loss": 1.7024,
      "step": 350880
    },
    {
      "epoch": 0.0012735292912753168,
      "grad_norm": 17800.193482094513,
      "learning_rate": 1.6889458386005709e-07,
      "loss": 1.6715,
      "step": 350912
    },
    {
      "epoch": 0.0012736454256261536,
      "grad_norm": 8043.796118748908,
      "learning_rate": 1.6888711679277484e-07,
      "loss": 1.6957,
      "step": 350944
    },
    {
      "epoch": 0.0012737615599769903,
      "grad_norm": 9527.911418563881,
      "learning_rate": 1.6887940989108614e-07,
      "loss": 1.7088,
      "step": 350976
    },
    {
      "epoch": 0.001273877694327827,
      "grad_norm": 11362.5889655483,
      "learning_rate": 1.688717040443787e-07,
      "loss": 1.721,
      "step": 351008
    },
    {
      "epoch": 0.0012739938286786636,
      "grad_norm": 10025.919010245396,
      "learning_rate": 1.688639992524118e-07,
      "loss": 1.7052,
      "step": 351040
    },
    {
      "epoch": 0.0012741099630295004,
      "grad_norm": 9224.696417768988,
      "learning_rate": 1.688562955149449e-07,
      "loss": 1.6776,
      "step": 351072
    },
    {
      "epoch": 0.0012742260973803371,
      "grad_norm": 8216.632887016433,
      "learning_rate": 1.6884859283173747e-07,
      "loss": 1.6934,
      "step": 351104
    },
    {
      "epoch": 0.0012743422317311739,
      "grad_norm": 9423.008649046227,
      "learning_rate": 1.6884089120254903e-07,
      "loss": 1.7102,
      "step": 351136
    },
    {
      "epoch": 0.0012744583660820106,
      "grad_norm": 9102.26499284656,
      "learning_rate": 1.688331906271393e-07,
      "loss": 1.6899,
      "step": 351168
    },
    {
      "epoch": 0.0012745745004328472,
      "grad_norm": 11152.451927715269,
      "learning_rate": 1.688254911052679e-07,
      "loss": 1.6662,
      "step": 351200
    },
    {
      "epoch": 0.001274690634783684,
      "grad_norm": 9044.69833659476,
      "learning_rate": 1.688177926366947e-07,
      "loss": 1.6858,
      "step": 351232
    },
    {
      "epoch": 0.0012748067691345207,
      "grad_norm": 9375.49497360006,
      "learning_rate": 1.6881009522117956e-07,
      "loss": 1.6822,
      "step": 351264
    },
    {
      "epoch": 0.0012749229034853574,
      "grad_norm": 8048.735552867916,
      "learning_rate": 1.688023988584824e-07,
      "loss": 1.6902,
      "step": 351296
    },
    {
      "epoch": 0.001275039037836194,
      "grad_norm": 9484.847494820357,
      "learning_rate": 1.6879470354836323e-07,
      "loss": 1.687,
      "step": 351328
    },
    {
      "epoch": 0.0012751551721870307,
      "grad_norm": 8301.27243258526,
      "learning_rate": 1.687870092905822e-07,
      "loss": 1.6622,
      "step": 351360
    },
    {
      "epoch": 0.0012752713065378675,
      "grad_norm": 8904.301881674946,
      "learning_rate": 1.6877931608489945e-07,
      "loss": 1.6766,
      "step": 351392
    },
    {
      "epoch": 0.0012753874408887042,
      "grad_norm": 10643.327205343263,
      "learning_rate": 1.6877162393107522e-07,
      "loss": 1.6785,
      "step": 351424
    },
    {
      "epoch": 0.001275503575239541,
      "grad_norm": 10120.018675872094,
      "learning_rate": 1.687639328288699e-07,
      "loss": 1.6869,
      "step": 351456
    },
    {
      "epoch": 0.0012756197095903775,
      "grad_norm": 10817.61729772319,
      "learning_rate": 1.687562427780438e-07,
      "loss": 1.6816,
      "step": 351488
    },
    {
      "epoch": 0.0012757358439412143,
      "grad_norm": 9676.439737837465,
      "learning_rate": 1.6874855377835747e-07,
      "loss": 1.711,
      "step": 351520
    },
    {
      "epoch": 0.001275851978292051,
      "grad_norm": 9980.822411004016,
      "learning_rate": 1.6874086582957144e-07,
      "loss": 1.7127,
      "step": 351552
    },
    {
      "epoch": 0.0012759681126428878,
      "grad_norm": 9620.882288023276,
      "learning_rate": 1.6873317893144636e-07,
      "loss": 1.7124,
      "step": 351584
    },
    {
      "epoch": 0.0012760842469937243,
      "grad_norm": 13544.913731729708,
      "learning_rate": 1.6872549308374296e-07,
      "loss": 1.6975,
      "step": 351616
    },
    {
      "epoch": 0.001276200381344561,
      "grad_norm": 11921.132496537399,
      "learning_rate": 1.6871780828622197e-07,
      "loss": 1.6885,
      "step": 351648
    },
    {
      "epoch": 0.0012763165156953978,
      "grad_norm": 8958.248043004838,
      "learning_rate": 1.687101245386443e-07,
      "loss": 1.6888,
      "step": 351680
    },
    {
      "epoch": 0.0012764326500462346,
      "grad_norm": 8895.6519716095,
      "learning_rate": 1.6870244184077087e-07,
      "loss": 1.7199,
      "step": 351712
    },
    {
      "epoch": 0.0012765487843970713,
      "grad_norm": 9384.331409322669,
      "learning_rate": 1.6869476019236268e-07,
      "loss": 1.7091,
      "step": 351744
    },
    {
      "epoch": 0.0012766649187479079,
      "grad_norm": 9495.375506002909,
      "learning_rate": 1.6868707959318085e-07,
      "loss": 1.6692,
      "step": 351776
    },
    {
      "epoch": 0.0012767810530987446,
      "grad_norm": 9471.10658793364,
      "learning_rate": 1.6867940004298657e-07,
      "loss": 1.6856,
      "step": 351808
    },
    {
      "epoch": 0.0012768971874495814,
      "grad_norm": 9602.17412881062,
      "learning_rate": 1.6867172154154102e-07,
      "loss": 1.6965,
      "step": 351840
    },
    {
      "epoch": 0.0012770133218004181,
      "grad_norm": 12478.035262011404,
      "learning_rate": 1.6866404408860553e-07,
      "loss": 1.6948,
      "step": 351872
    },
    {
      "epoch": 0.0012771294561512547,
      "grad_norm": 11112.353306118375,
      "learning_rate": 1.6865636768394155e-07,
      "loss": 1.6965,
      "step": 351904
    },
    {
      "epoch": 0.0012772455905020914,
      "grad_norm": 18525.421668615265,
      "learning_rate": 1.686486923273105e-07,
      "loss": 1.6768,
      "step": 351936
    },
    {
      "epoch": 0.0012773617248529282,
      "grad_norm": 20562.227116730326,
      "learning_rate": 1.6864101801847397e-07,
      "loss": 1.6779,
      "step": 351968
    },
    {
      "epoch": 0.001277477859203765,
      "grad_norm": 10898.850214586859,
      "learning_rate": 1.6863358453075436e-07,
      "loss": 1.7055,
      "step": 352000
    },
    {
      "epoch": 0.0012775939935546017,
      "grad_norm": 12762.77861595977,
      "learning_rate": 1.6862591228406673e-07,
      "loss": 1.6939,
      "step": 352032
    },
    {
      "epoch": 0.0012777101279054382,
      "grad_norm": 8637.260676858145,
      "learning_rate": 1.6861824108446605e-07,
      "loss": 1.6922,
      "step": 352064
    },
    {
      "epoch": 0.001277826262256275,
      "grad_norm": 8070.395529340554,
      "learning_rate": 1.6861057093171433e-07,
      "loss": 1.7061,
      "step": 352096
    },
    {
      "epoch": 0.0012779423966071117,
      "grad_norm": 12796.293525861307,
      "learning_rate": 1.686029018255734e-07,
      "loss": 1.6968,
      "step": 352128
    },
    {
      "epoch": 0.0012780585309579485,
      "grad_norm": 8661.483360256487,
      "learning_rate": 1.6859523376580527e-07,
      "loss": 1.6823,
      "step": 352160
    },
    {
      "epoch": 0.001278174665308785,
      "grad_norm": 11299.285818139128,
      "learning_rate": 1.6858756675217202e-07,
      "loss": 1.6895,
      "step": 352192
    },
    {
      "epoch": 0.0012782907996596218,
      "grad_norm": 9586.331728038624,
      "learning_rate": 1.6857990078443586e-07,
      "loss": 1.6654,
      "step": 352224
    },
    {
      "epoch": 0.0012784069340104585,
      "grad_norm": 9730.487552019169,
      "learning_rate": 1.6857223586235894e-07,
      "loss": 1.679,
      "step": 352256
    },
    {
      "epoch": 0.0012785230683612953,
      "grad_norm": 9247.234181094367,
      "learning_rate": 1.6856457198570358e-07,
      "loss": 1.6913,
      "step": 352288
    },
    {
      "epoch": 0.001278639202712132,
      "grad_norm": 11307.37431944304,
      "learning_rate": 1.6855690915423219e-07,
      "loss": 1.6715,
      "step": 352320
    },
    {
      "epoch": 0.0012787553370629686,
      "grad_norm": 15086.637133569562,
      "learning_rate": 1.6854924736770723e-07,
      "loss": 1.6683,
      "step": 352352
    },
    {
      "epoch": 0.0012788714714138053,
      "grad_norm": 9431.666660776344,
      "learning_rate": 1.6854158662589118e-07,
      "loss": 1.6999,
      "step": 352384
    },
    {
      "epoch": 0.001278987605764642,
      "grad_norm": 8755.120101974615,
      "learning_rate": 1.6853392692854668e-07,
      "loss": 1.6985,
      "step": 352416
    },
    {
      "epoch": 0.0012791037401154789,
      "grad_norm": 10519.96159688808,
      "learning_rate": 1.6852626827543638e-07,
      "loss": 1.6979,
      "step": 352448
    },
    {
      "epoch": 0.0012792198744663154,
      "grad_norm": 9362.621427783994,
      "learning_rate": 1.685186106663231e-07,
      "loss": 1.6993,
      "step": 352480
    },
    {
      "epoch": 0.0012793360088171521,
      "grad_norm": 8283.161353010093,
      "learning_rate": 1.6851095410096965e-07,
      "loss": 1.7051,
      "step": 352512
    },
    {
      "epoch": 0.001279452143167989,
      "grad_norm": 9138.449759122168,
      "learning_rate": 1.6850329857913888e-07,
      "loss": 1.7098,
      "step": 352544
    },
    {
      "epoch": 0.0012795682775188257,
      "grad_norm": 11229.001914685026,
      "learning_rate": 1.6849564410059385e-07,
      "loss": 1.7345,
      "step": 352576
    },
    {
      "epoch": 0.0012796844118696624,
      "grad_norm": 8583.936043564165,
      "learning_rate": 1.6848799066509756e-07,
      "loss": 1.7275,
      "step": 352608
    },
    {
      "epoch": 0.001279800546220499,
      "grad_norm": 9909.279893110295,
      "learning_rate": 1.684803382724132e-07,
      "loss": 1.7142,
      "step": 352640
    },
    {
      "epoch": 0.0012799166805713357,
      "grad_norm": 8732.972918771706,
      "learning_rate": 1.6847268692230392e-07,
      "loss": 1.7081,
      "step": 352672
    },
    {
      "epoch": 0.0012800328149221725,
      "grad_norm": 9318.136401663156,
      "learning_rate": 1.68465036614533e-07,
      "loss": 1.6967,
      "step": 352704
    },
    {
      "epoch": 0.0012801489492730092,
      "grad_norm": 9303.668308790893,
      "learning_rate": 1.684573873488639e-07,
      "loss": 1.701,
      "step": 352736
    },
    {
      "epoch": 0.0012802650836238457,
      "grad_norm": 7859.280755896178,
      "learning_rate": 1.6844973912505995e-07,
      "loss": 1.694,
      "step": 352768
    },
    {
      "epoch": 0.0012803812179746825,
      "grad_norm": 8564.324258223762,
      "learning_rate": 1.6844209194288474e-07,
      "loss": 1.6883,
      "step": 352800
    },
    {
      "epoch": 0.0012804973523255193,
      "grad_norm": 8978.999498830592,
      "learning_rate": 1.6843444580210175e-07,
      "loss": 1.6902,
      "step": 352832
    },
    {
      "epoch": 0.001280613486676356,
      "grad_norm": 9807.888049932055,
      "learning_rate": 1.6842680070247472e-07,
      "loss": 1.7116,
      "step": 352864
    },
    {
      "epoch": 0.0012807296210271928,
      "grad_norm": 11088.100107773198,
      "learning_rate": 1.6841915664376738e-07,
      "loss": 1.698,
      "step": 352896
    },
    {
      "epoch": 0.0012808457553780293,
      "grad_norm": 8451.657233939388,
      "learning_rate": 1.6841151362574352e-07,
      "loss": 1.6842,
      "step": 352928
    },
    {
      "epoch": 0.001280961889728866,
      "grad_norm": 9887.115251679834,
      "learning_rate": 1.6840387164816707e-07,
      "loss": 1.7009,
      "step": 352960
    },
    {
      "epoch": 0.0012810780240797028,
      "grad_norm": 10023.931963057212,
      "learning_rate": 1.6839623071080192e-07,
      "loss": 1.7015,
      "step": 352992
    },
    {
      "epoch": 0.0012811941584305396,
      "grad_norm": 7776.615716364027,
      "learning_rate": 1.6838882954446608e-07,
      "loss": 1.6961,
      "step": 353024
    },
    {
      "epoch": 0.001281310292781376,
      "grad_norm": 9325.46835284963,
      "learning_rate": 1.6838119065432753e-07,
      "loss": 1.697,
      "step": 353056
    },
    {
      "epoch": 0.0012814264271322129,
      "grad_norm": 9118.620948367137,
      "learning_rate": 1.6837355280369997e-07,
      "loss": 1.6817,
      "step": 353088
    },
    {
      "epoch": 0.0012815425614830496,
      "grad_norm": 8745.891149562747,
      "learning_rate": 1.6836591599234772e-07,
      "loss": 1.6736,
      "step": 353120
    },
    {
      "epoch": 0.0012816586958338864,
      "grad_norm": 9989.46104652298,
      "learning_rate": 1.6835828022003509e-07,
      "loss": 1.7093,
      "step": 353152
    },
    {
      "epoch": 0.0012817748301847231,
      "grad_norm": 9376.998880238816,
      "learning_rate": 1.683506454865265e-07,
      "loss": 1.6856,
      "step": 353184
    },
    {
      "epoch": 0.0012818909645355597,
      "grad_norm": 8281.64766215033,
      "learning_rate": 1.6834301179158638e-07,
      "loss": 1.6739,
      "step": 353216
    },
    {
      "epoch": 0.0012820070988863964,
      "grad_norm": 8236.445106962105,
      "learning_rate": 1.6833537913497936e-07,
      "loss": 1.6877,
      "step": 353248
    },
    {
      "epoch": 0.0012821232332372332,
      "grad_norm": 9875.9528147921,
      "learning_rate": 1.6832774751647006e-07,
      "loss": 1.6905,
      "step": 353280
    },
    {
      "epoch": 0.00128223936758807,
      "grad_norm": 9728.215252552753,
      "learning_rate": 1.6832011693582316e-07,
      "loss": 1.6854,
      "step": 353312
    },
    {
      "epoch": 0.0012823555019389065,
      "grad_norm": 9631.613779632155,
      "learning_rate": 1.6831248739280345e-07,
      "loss": 1.6979,
      "step": 353344
    },
    {
      "epoch": 0.0012824716362897432,
      "grad_norm": 13609.727991403795,
      "learning_rate": 1.6830485888717578e-07,
      "loss": 1.6939,
      "step": 353376
    },
    {
      "epoch": 0.00128258777064058,
      "grad_norm": 8225.301818170565,
      "learning_rate": 1.682972314187051e-07,
      "loss": 1.6916,
      "step": 353408
    },
    {
      "epoch": 0.0012827039049914167,
      "grad_norm": 8490.93398867286,
      "learning_rate": 1.6828960498715636e-07,
      "loss": 1.7027,
      "step": 353440
    },
    {
      "epoch": 0.0012828200393422535,
      "grad_norm": 10796.097813562084,
      "learning_rate": 1.6828197959229472e-07,
      "loss": 1.6887,
      "step": 353472
    },
    {
      "epoch": 0.00128293617369309,
      "grad_norm": 8681.675414342557,
      "learning_rate": 1.6827435523388528e-07,
      "loss": 1.6969,
      "step": 353504
    },
    {
      "epoch": 0.0012830523080439268,
      "grad_norm": 9427.427644909294,
      "learning_rate": 1.682667319116933e-07,
      "loss": 1.693,
      "step": 353536
    },
    {
      "epoch": 0.0012831684423947635,
      "grad_norm": 10069.315567604384,
      "learning_rate": 1.6825910962548406e-07,
      "loss": 1.6869,
      "step": 353568
    },
    {
      "epoch": 0.0012832845767456003,
      "grad_norm": 9092.864235212137,
      "learning_rate": 1.6825148837502292e-07,
      "loss": 1.6907,
      "step": 353600
    },
    {
      "epoch": 0.0012834007110964368,
      "grad_norm": 7591.256549478486,
      "learning_rate": 1.6824386816007536e-07,
      "loss": 1.6847,
      "step": 353632
    },
    {
      "epoch": 0.0012835168454472736,
      "grad_norm": 11670.390053464367,
      "learning_rate": 1.6823624898040694e-07,
      "loss": 1.676,
      "step": 353664
    },
    {
      "epoch": 0.0012836329797981103,
      "grad_norm": 9653.135241982265,
      "learning_rate": 1.6822863083578318e-07,
      "loss": 1.6914,
      "step": 353696
    },
    {
      "epoch": 0.001283749114148947,
      "grad_norm": 10232.40519135164,
      "learning_rate": 1.6822101372596979e-07,
      "loss": 1.7151,
      "step": 353728
    },
    {
      "epoch": 0.0012838652484997838,
      "grad_norm": 8149.25149937097,
      "learning_rate": 1.6821339765073254e-07,
      "loss": 1.6794,
      "step": 353760
    },
    {
      "epoch": 0.0012839813828506204,
      "grad_norm": 8529.743724168975,
      "learning_rate": 1.6820578260983727e-07,
      "loss": 1.6782,
      "step": 353792
    },
    {
      "epoch": 0.0012840975172014571,
      "grad_norm": 12050.56977906024,
      "learning_rate": 1.6819816860304977e-07,
      "loss": 1.6843,
      "step": 353824
    },
    {
      "epoch": 0.0012842136515522939,
      "grad_norm": 8805.04332754814,
      "learning_rate": 1.6819055563013613e-07,
      "loss": 1.6875,
      "step": 353856
    },
    {
      "epoch": 0.0012843297859031306,
      "grad_norm": 10948.061928944318,
      "learning_rate": 1.6818294369086238e-07,
      "loss": 1.6935,
      "step": 353888
    },
    {
      "epoch": 0.0012844459202539672,
      "grad_norm": 8767.346919108426,
      "learning_rate": 1.6817533278499456e-07,
      "loss": 1.675,
      "step": 353920
    },
    {
      "epoch": 0.001284562054604804,
      "grad_norm": 9488.955474655786,
      "learning_rate": 1.6816772291229895e-07,
      "loss": 1.6707,
      "step": 353952
    },
    {
      "epoch": 0.0012846781889556407,
      "grad_norm": 10048.727481626716,
      "learning_rate": 1.6816011407254176e-07,
      "loss": 1.6692,
      "step": 353984
    },
    {
      "epoch": 0.0012847943233064774,
      "grad_norm": 16469.085949135126,
      "learning_rate": 1.6815274399383033e-07,
      "loss": 1.692,
      "step": 354016
    },
    {
      "epoch": 0.0012849104576573142,
      "grad_norm": 10610.735318534715,
      "learning_rate": 1.6814513718698792e-07,
      "loss": 1.6615,
      "step": 354048
    },
    {
      "epoch": 0.0012850265920081507,
      "grad_norm": 11260.729283665423,
      "learning_rate": 1.681375314123905e-07,
      "loss": 1.6759,
      "step": 354080
    },
    {
      "epoch": 0.0012851427263589875,
      "grad_norm": 10310.68824084988,
      "learning_rate": 1.6812992666980468e-07,
      "loss": 1.6858,
      "step": 354112
    },
    {
      "epoch": 0.0012852588607098242,
      "grad_norm": 11921.864954779516,
      "learning_rate": 1.68122322958997e-07,
      "loss": 1.6924,
      "step": 354144
    },
    {
      "epoch": 0.001285374995060661,
      "grad_norm": 9811.530359734918,
      "learning_rate": 1.6811472027973423e-07,
      "loss": 1.6998,
      "step": 354176
    },
    {
      "epoch": 0.0012854911294114975,
      "grad_norm": 9738.604006735257,
      "learning_rate": 1.681071186317831e-07,
      "loss": 1.6918,
      "step": 354208
    },
    {
      "epoch": 0.0012856072637623343,
      "grad_norm": 7964.675636835439,
      "learning_rate": 1.6809951801491053e-07,
      "loss": 1.7052,
      "step": 354240
    },
    {
      "epoch": 0.001285723398113171,
      "grad_norm": 10306.373173915255,
      "learning_rate": 1.6809191842888342e-07,
      "loss": 1.7066,
      "step": 354272
    },
    {
      "epoch": 0.0012858395324640078,
      "grad_norm": 12069.656664545186,
      "learning_rate": 1.6808431987346877e-07,
      "loss": 1.704,
      "step": 354304
    },
    {
      "epoch": 0.0012859556668148445,
      "grad_norm": 12387.817886940378,
      "learning_rate": 1.6807672234843367e-07,
      "loss": 1.6909,
      "step": 354336
    },
    {
      "epoch": 0.001286071801165681,
      "grad_norm": 8944.062387975611,
      "learning_rate": 1.6806912585354528e-07,
      "loss": 1.7003,
      "step": 354368
    },
    {
      "epoch": 0.0012861879355165178,
      "grad_norm": 8777.142131696399,
      "learning_rate": 1.6806153038857078e-07,
      "loss": 1.691,
      "step": 354400
    },
    {
      "epoch": 0.0012863040698673546,
      "grad_norm": 8597.203847763527,
      "learning_rate": 1.6805393595327754e-07,
      "loss": 1.6953,
      "step": 354432
    },
    {
      "epoch": 0.0012864202042181913,
      "grad_norm": 9307.713467871688,
      "learning_rate": 1.6804634254743286e-07,
      "loss": 1.6855,
      "step": 354464
    },
    {
      "epoch": 0.0012865363385690279,
      "grad_norm": 12584.719941262101,
      "learning_rate": 1.6803875017080425e-07,
      "loss": 1.6658,
      "step": 354496
    },
    {
      "epoch": 0.0012866524729198646,
      "grad_norm": 9742.067337069684,
      "learning_rate": 1.6803115882315916e-07,
      "loss": 1.6706,
      "step": 354528
    },
    {
      "epoch": 0.0012867686072707014,
      "grad_norm": 10957.088116831042,
      "learning_rate": 1.6802356850426528e-07,
      "loss": 1.6767,
      "step": 354560
    },
    {
      "epoch": 0.0012868847416215381,
      "grad_norm": 11515.96457097711,
      "learning_rate": 1.6801597921389018e-07,
      "loss": 1.6931,
      "step": 354592
    },
    {
      "epoch": 0.0012870008759723749,
      "grad_norm": 10923.597392800597,
      "learning_rate": 1.6800839095180165e-07,
      "loss": 1.6805,
      "step": 354624
    },
    {
      "epoch": 0.0012871170103232114,
      "grad_norm": 10040.09442186676,
      "learning_rate": 1.6800080371776748e-07,
      "loss": 1.6918,
      "step": 354656
    },
    {
      "epoch": 0.0012872331446740482,
      "grad_norm": 10470.403812652117,
      "learning_rate": 1.679932175115556e-07,
      "loss": 1.6917,
      "step": 354688
    },
    {
      "epoch": 0.001287349279024885,
      "grad_norm": 10706.459358723592,
      "learning_rate": 1.6798563233293392e-07,
      "loss": 1.686,
      "step": 354720
    },
    {
      "epoch": 0.0012874654133757217,
      "grad_norm": 10984.928766268811,
      "learning_rate": 1.679780481816705e-07,
      "loss": 1.7006,
      "step": 354752
    },
    {
      "epoch": 0.0012875815477265582,
      "grad_norm": 12820.27597206862,
      "learning_rate": 1.6797046505753345e-07,
      "loss": 1.6787,
      "step": 354784
    },
    {
      "epoch": 0.001287697682077395,
      "grad_norm": 9367.763874052334,
      "learning_rate": 1.6796288296029094e-07,
      "loss": 1.688,
      "step": 354816
    },
    {
      "epoch": 0.0012878138164282317,
      "grad_norm": 11718.156510304852,
      "learning_rate": 1.679553018897112e-07,
      "loss": 1.6744,
      "step": 354848
    },
    {
      "epoch": 0.0012879299507790685,
      "grad_norm": 10123.515792450764,
      "learning_rate": 1.6794772184556261e-07,
      "loss": 1.6839,
      "step": 354880
    },
    {
      "epoch": 0.0012880460851299052,
      "grad_norm": 14376.03422366544,
      "learning_rate": 1.6794014282761357e-07,
      "loss": 1.6715,
      "step": 354912
    },
    {
      "epoch": 0.0012881622194807418,
      "grad_norm": 8756.348097237798,
      "learning_rate": 1.6793256483563247e-07,
      "loss": 1.6855,
      "step": 354944
    },
    {
      "epoch": 0.0012882783538315785,
      "grad_norm": 11391.13760780722,
      "learning_rate": 1.6792498786938794e-07,
      "loss": 1.705,
      "step": 354976
    },
    {
      "epoch": 0.0012883944881824153,
      "grad_norm": 9546.623067870649,
      "learning_rate": 1.6791741192864856e-07,
      "loss": 1.6919,
      "step": 355008
    },
    {
      "epoch": 0.001288510622533252,
      "grad_norm": 20435.240541769996,
      "learning_rate": 1.6790983701318308e-07,
      "loss": 1.6971,
      "step": 355040
    },
    {
      "epoch": 0.0012886267568840886,
      "grad_norm": 20121.089433726,
      "learning_rate": 1.6790226312276013e-07,
      "loss": 1.6783,
      "step": 355072
    },
    {
      "epoch": 0.0012887428912349253,
      "grad_norm": 12926.07442342802,
      "learning_rate": 1.678949268936891e-07,
      "loss": 1.6812,
      "step": 355104
    },
    {
      "epoch": 0.001288859025585762,
      "grad_norm": 9863.590421342524,
      "learning_rate": 1.6788735502064337e-07,
      "loss": 1.705,
      "step": 355136
    },
    {
      "epoch": 0.0012889751599365988,
      "grad_norm": 9422.844156622776,
      "learning_rate": 1.6787978417195423e-07,
      "loss": 1.7132,
      "step": 355168
    },
    {
      "epoch": 0.0012890912942874356,
      "grad_norm": 10581.54412172439,
      "learning_rate": 1.6787221434739073e-07,
      "loss": 1.6877,
      "step": 355200
    },
    {
      "epoch": 0.0012892074286382721,
      "grad_norm": 12233.02611784999,
      "learning_rate": 1.6786464554672198e-07,
      "loss": 1.7133,
      "step": 355232
    },
    {
      "epoch": 0.0012893235629891089,
      "grad_norm": 11730.18635828093,
      "learning_rate": 1.678570777697172e-07,
      "loss": 1.7077,
      "step": 355264
    },
    {
      "epoch": 0.0012894396973399456,
      "grad_norm": 9806.365891603271,
      "learning_rate": 1.678495110161456e-07,
      "loss": 1.7031,
      "step": 355296
    },
    {
      "epoch": 0.0012895558316907824,
      "grad_norm": 9826.356394920753,
      "learning_rate": 1.678419452857766e-07,
      "loss": 1.708,
      "step": 355328
    },
    {
      "epoch": 0.001289671966041619,
      "grad_norm": 13276.83832845757,
      "learning_rate": 1.6783438057837958e-07,
      "loss": 1.6782,
      "step": 355360
    },
    {
      "epoch": 0.0012897881003924557,
      "grad_norm": 8069.640140675419,
      "learning_rate": 1.6782681689372408e-07,
      "loss": 1.6711,
      "step": 355392
    },
    {
      "epoch": 0.0012899042347432924,
      "grad_norm": 9151.232048199849,
      "learning_rate": 1.678192542315796e-07,
      "loss": 1.6767,
      "step": 355424
    },
    {
      "epoch": 0.0012900203690941292,
      "grad_norm": 9345.253768625013,
      "learning_rate": 1.6781169259171584e-07,
      "loss": 1.6808,
      "step": 355456
    },
    {
      "epoch": 0.0012901365034449657,
      "grad_norm": 9799.756119414402,
      "learning_rate": 1.6780413197390245e-07,
      "loss": 1.6851,
      "step": 355488
    },
    {
      "epoch": 0.0012902526377958025,
      "grad_norm": 9657.54813604364,
      "learning_rate": 1.6779657237790924e-07,
      "loss": 1.7145,
      "step": 355520
    },
    {
      "epoch": 0.0012903687721466392,
      "grad_norm": 9200.065217160147,
      "learning_rate": 1.6778901380350608e-07,
      "loss": 1.6862,
      "step": 355552
    },
    {
      "epoch": 0.001290484906497476,
      "grad_norm": 14281.37766463726,
      "learning_rate": 1.677814562504629e-07,
      "loss": 1.6865,
      "step": 355584
    },
    {
      "epoch": 0.0012906010408483127,
      "grad_norm": 8964.2808969822,
      "learning_rate": 1.6777389971854965e-07,
      "loss": 1.6885,
      "step": 355616
    },
    {
      "epoch": 0.0012907171751991493,
      "grad_norm": 11243.15791937479,
      "learning_rate": 1.6776634420753644e-07,
      "loss": 1.6566,
      "step": 355648
    },
    {
      "epoch": 0.001290833309549986,
      "grad_norm": 8754.857965723944,
      "learning_rate": 1.6775878971719343e-07,
      "loss": 1.67,
      "step": 355680
    },
    {
      "epoch": 0.0012909494439008228,
      "grad_norm": 9845.02087351774,
      "learning_rate": 1.677512362472908e-07,
      "loss": 1.6817,
      "step": 355712
    },
    {
      "epoch": 0.0012910655782516595,
      "grad_norm": 11160.642454625988,
      "learning_rate": 1.6774368379759887e-07,
      "loss": 1.678,
      "step": 355744
    },
    {
      "epoch": 0.001291181712602496,
      "grad_norm": 10024.879251143127,
      "learning_rate": 1.6773613236788802e-07,
      "loss": 1.6749,
      "step": 355776
    },
    {
      "epoch": 0.0012912978469533328,
      "grad_norm": 9299.603002279184,
      "learning_rate": 1.677285819579286e-07,
      "loss": 1.7014,
      "step": 355808
    },
    {
      "epoch": 0.0012914139813041696,
      "grad_norm": 8890.064791664907,
      "learning_rate": 1.677210325674912e-07,
      "loss": 1.7046,
      "step": 355840
    },
    {
      "epoch": 0.0012915301156550063,
      "grad_norm": 9902.214297822482,
      "learning_rate": 1.677134841963464e-07,
      "loss": 1.7134,
      "step": 355872
    },
    {
      "epoch": 0.001291646250005843,
      "grad_norm": 10906.315784901884,
      "learning_rate": 1.677059368442648e-07,
      "loss": 1.7032,
      "step": 355904
    },
    {
      "epoch": 0.0012917623843566796,
      "grad_norm": 8769.84378424154,
      "learning_rate": 1.6769839051101716e-07,
      "loss": 1.675,
      "step": 355936
    },
    {
      "epoch": 0.0012918785187075164,
      "grad_norm": 10022.373571165665,
      "learning_rate": 1.6769084519637425e-07,
      "loss": 1.6893,
      "step": 355968
    },
    {
      "epoch": 0.0012919946530583531,
      "grad_norm": 8246.5569785214,
      "learning_rate": 1.6768330090010696e-07,
      "loss": 1.7079,
      "step": 356000
    },
    {
      "epoch": 0.00129211078740919,
      "grad_norm": 11809.103945685294,
      "learning_rate": 1.676757576219862e-07,
      "loss": 1.715,
      "step": 356032
    },
    {
      "epoch": 0.0012922269217600264,
      "grad_norm": 10588.971243704462,
      "learning_rate": 1.6766821536178302e-07,
      "loss": 1.7134,
      "step": 356064
    },
    {
      "epoch": 0.0012923430561108632,
      "grad_norm": 11191.032838840212,
      "learning_rate": 1.6766090976769491e-07,
      "loss": 1.7173,
      "step": 356096
    },
    {
      "epoch": 0.0012924591904617,
      "grad_norm": 11181.02589210847,
      "learning_rate": 1.6765336951084802e-07,
      "loss": 1.7012,
      "step": 356128
    },
    {
      "epoch": 0.0012925753248125367,
      "grad_norm": 10037.711292919317,
      "learning_rate": 1.6764583027123936e-07,
      "loss": 1.6887,
      "step": 356160
    },
    {
      "epoch": 0.0012926914591633734,
      "grad_norm": 9980.780330214668,
      "learning_rate": 1.6763829204864016e-07,
      "loss": 1.6808,
      "step": 356192
    },
    {
      "epoch": 0.00129280759351421,
      "grad_norm": 8968.696895313165,
      "learning_rate": 1.676307548428218e-07,
      "loss": 1.6696,
      "step": 356224
    },
    {
      "epoch": 0.0012929237278650467,
      "grad_norm": 11305.932601957256,
      "learning_rate": 1.6762321865355576e-07,
      "loss": 1.6653,
      "step": 356256
    },
    {
      "epoch": 0.0012930398622158835,
      "grad_norm": 9239.199965364967,
      "learning_rate": 1.676156834806135e-07,
      "loss": 1.6837,
      "step": 356288
    },
    {
      "epoch": 0.0012931559965667202,
      "grad_norm": 9891.959563200813,
      "learning_rate": 1.6760814932376668e-07,
      "loss": 1.6751,
      "step": 356320
    },
    {
      "epoch": 0.0012932721309175568,
      "grad_norm": 10050.163182754795,
      "learning_rate": 1.676006161827869e-07,
      "loss": 1.6786,
      "step": 356352
    },
    {
      "epoch": 0.0012933882652683935,
      "grad_norm": 8854.28800073727,
      "learning_rate": 1.675930840574459e-07,
      "loss": 1.7251,
      "step": 356384
    },
    {
      "epoch": 0.0012935043996192303,
      "grad_norm": 7492.45794115656,
      "learning_rate": 1.6758555294751548e-07,
      "loss": 1.7155,
      "step": 356416
    },
    {
      "epoch": 0.001293620533970067,
      "grad_norm": 9786.068056170467,
      "learning_rate": 1.675780228527675e-07,
      "loss": 1.6954,
      "step": 356448
    },
    {
      "epoch": 0.0012937366683209038,
      "grad_norm": 10485.277678726492,
      "learning_rate": 1.67570493772974e-07,
      "loss": 1.6799,
      "step": 356480
    },
    {
      "epoch": 0.0012938528026717403,
      "grad_norm": 9207.905190650043,
      "learning_rate": 1.6756296570790687e-07,
      "loss": 1.6648,
      "step": 356512
    },
    {
      "epoch": 0.001293968937022577,
      "grad_norm": 10772.599686241014,
      "learning_rate": 1.6755543865733827e-07,
      "loss": 1.6622,
      "step": 356544
    },
    {
      "epoch": 0.0012940850713734138,
      "grad_norm": 8285.414896068874,
      "learning_rate": 1.6754791262104033e-07,
      "loss": 1.6861,
      "step": 356576
    },
    {
      "epoch": 0.0012942012057242506,
      "grad_norm": 12228.22979829869,
      "learning_rate": 1.675403875987853e-07,
      "loss": 1.6746,
      "step": 356608
    },
    {
      "epoch": 0.0012943173400750871,
      "grad_norm": 11098.344741446806,
      "learning_rate": 1.6753286359034546e-07,
      "loss": 1.6786,
      "step": 356640
    },
    {
      "epoch": 0.001294433474425924,
      "grad_norm": 8309.98062572952,
      "learning_rate": 1.6752534059549323e-07,
      "loss": 1.6945,
      "step": 356672
    },
    {
      "epoch": 0.0012945496087767606,
      "grad_norm": 9702.834018986412,
      "learning_rate": 1.67517818614001e-07,
      "loss": 1.6858,
      "step": 356704
    },
    {
      "epoch": 0.0012946657431275974,
      "grad_norm": 10116.556726475665,
      "learning_rate": 1.6751029764564132e-07,
      "loss": 1.6888,
      "step": 356736
    },
    {
      "epoch": 0.0012947818774784342,
      "grad_norm": 14380.233238720435,
      "learning_rate": 1.675027776901868e-07,
      "loss": 1.6889,
      "step": 356768
    },
    {
      "epoch": 0.0012948980118292707,
      "grad_norm": 10342.558000804249,
      "learning_rate": 1.6749525874741008e-07,
      "loss": 1.684,
      "step": 356800
    },
    {
      "epoch": 0.0012950141461801074,
      "grad_norm": 9306.588848767307,
      "learning_rate": 1.674877408170839e-07,
      "loss": 1.6916,
      "step": 356832
    },
    {
      "epoch": 0.0012951302805309442,
      "grad_norm": 9302.49923407683,
      "learning_rate": 1.6748022389898103e-07,
      "loss": 1.7138,
      "step": 356864
    },
    {
      "epoch": 0.001295246414881781,
      "grad_norm": 10844.394127843198,
      "learning_rate": 1.674727079928744e-07,
      "loss": 1.7036,
      "step": 356896
    },
    {
      "epoch": 0.0012953625492326175,
      "grad_norm": 9835.672219019907,
      "learning_rate": 1.6746519309853691e-07,
      "loss": 1.7182,
      "step": 356928
    },
    {
      "epoch": 0.0012954786835834542,
      "grad_norm": 10878.67455161703,
      "learning_rate": 1.6745767921574158e-07,
      "loss": 1.7316,
      "step": 356960
    },
    {
      "epoch": 0.001295594817934291,
      "grad_norm": 10918.50081284056,
      "learning_rate": 1.6745016634426156e-07,
      "loss": 1.7194,
      "step": 356992
    },
    {
      "epoch": 0.0012957109522851278,
      "grad_norm": 9582.32998805614,
      "learning_rate": 1.6744265448386994e-07,
      "loss": 1.6904,
      "step": 357024
    },
    {
      "epoch": 0.0012958270866359645,
      "grad_norm": 8696.76986012623,
      "learning_rate": 1.6743514363433996e-07,
      "loss": 1.6761,
      "step": 357056
    },
    {
      "epoch": 0.001295943220986801,
      "grad_norm": 19500.63834852593,
      "learning_rate": 1.6742763379544496e-07,
      "loss": 1.6609,
      "step": 357088
    },
    {
      "epoch": 0.0012960593553376378,
      "grad_norm": 20380.382332036854,
      "learning_rate": 1.6742012496695833e-07,
      "loss": 1.6848,
      "step": 357120
    },
    {
      "epoch": 0.0012961754896884746,
      "grad_norm": 9152.51659381178,
      "learning_rate": 1.6741285175268685e-07,
      "loss": 1.6948,
      "step": 357152
    },
    {
      "epoch": 0.0012962916240393113,
      "grad_norm": 10039.688242171665,
      "learning_rate": 1.674053449127796e-07,
      "loss": 1.6684,
      "step": 357184
    },
    {
      "epoch": 0.0012964077583901478,
      "grad_norm": 8985.482513476947,
      "learning_rate": 1.673978390826083e-07,
      "loss": 1.6744,
      "step": 357216
    },
    {
      "epoch": 0.0012965238927409846,
      "grad_norm": 8208.58940378918,
      "learning_rate": 1.673903342619466e-07,
      "loss": 1.6963,
      "step": 357248
    },
    {
      "epoch": 0.0012966400270918214,
      "grad_norm": 11259.427871788157,
      "learning_rate": 1.6738283045056827e-07,
      "loss": 1.7002,
      "step": 357280
    },
    {
      "epoch": 0.001296756161442658,
      "grad_norm": 9877.45918746314,
      "learning_rate": 1.6737532764824707e-07,
      "loss": 1.69,
      "step": 357312
    },
    {
      "epoch": 0.0012968722957934949,
      "grad_norm": 12358.27140015949,
      "learning_rate": 1.6736782585475685e-07,
      "loss": 1.6786,
      "step": 357344
    },
    {
      "epoch": 0.0012969884301443314,
      "grad_norm": 9294.173873992244,
      "learning_rate": 1.6736032506987164e-07,
      "loss": 1.6718,
      "step": 357376
    },
    {
      "epoch": 0.0012971045644951682,
      "grad_norm": 9373.078683122212,
      "learning_rate": 1.6735282529336534e-07,
      "loss": 1.6815,
      "step": 357408
    },
    {
      "epoch": 0.001297220698846005,
      "grad_norm": 10284.480152151591,
      "learning_rate": 1.673453265250121e-07,
      "loss": 1.7016,
      "step": 357440
    },
    {
      "epoch": 0.0012973368331968417,
      "grad_norm": 8571.448769023822,
      "learning_rate": 1.6733782876458604e-07,
      "loss": 1.6962,
      "step": 357472
    },
    {
      "epoch": 0.0012974529675476782,
      "grad_norm": 9061.97351574148,
      "learning_rate": 1.6733033201186144e-07,
      "loss": 1.7112,
      "step": 357504
    },
    {
      "epoch": 0.001297569101898515,
      "grad_norm": 10192.626550600193,
      "learning_rate": 1.6732283626661252e-07,
      "loss": 1.7111,
      "step": 357536
    },
    {
      "epoch": 0.0012976852362493517,
      "grad_norm": 9155.833113376411,
      "learning_rate": 1.673153415286137e-07,
      "loss": 1.6993,
      "step": 357568
    },
    {
      "epoch": 0.0012978013706001885,
      "grad_norm": 10220.562215455664,
      "learning_rate": 1.673078477976394e-07,
      "loss": 1.7035,
      "step": 357600
    },
    {
      "epoch": 0.0012979175049510252,
      "grad_norm": 10485.60670633798,
      "learning_rate": 1.673003550734641e-07,
      "loss": 1.6905,
      "step": 357632
    },
    {
      "epoch": 0.0012980336393018618,
      "grad_norm": 7422.243730840426,
      "learning_rate": 1.672928633558624e-07,
      "loss": 1.6978,
      "step": 357664
    },
    {
      "epoch": 0.0012981497736526985,
      "grad_norm": 9131.183274910212,
      "learning_rate": 1.67285372644609e-07,
      "loss": 1.6967,
      "step": 357696
    },
    {
      "epoch": 0.0012982659080035353,
      "grad_norm": 9107.483077118508,
      "learning_rate": 1.672778829394785e-07,
      "loss": 1.7118,
      "step": 357728
    },
    {
      "epoch": 0.001298382042354372,
      "grad_norm": 10517.23575850613,
      "learning_rate": 1.6727039424024582e-07,
      "loss": 1.6952,
      "step": 357760
    },
    {
      "epoch": 0.0012984981767052086,
      "grad_norm": 8611.539002988955,
      "learning_rate": 1.6726290654668572e-07,
      "loss": 1.7165,
      "step": 357792
    },
    {
      "epoch": 0.0012986143110560453,
      "grad_norm": 8249.370400218431,
      "learning_rate": 1.6725541985857314e-07,
      "loss": 1.7263,
      "step": 357824
    },
    {
      "epoch": 0.001298730445406882,
      "grad_norm": 12540.05310993538,
      "learning_rate": 1.6724793417568317e-07,
      "loss": 1.7229,
      "step": 357856
    },
    {
      "epoch": 0.0012988465797577188,
      "grad_norm": 8995.090661021712,
      "learning_rate": 1.672404494977908e-07,
      "loss": 1.7217,
      "step": 357888
    },
    {
      "epoch": 0.0012989627141085556,
      "grad_norm": 9110.087815164023,
      "learning_rate": 1.6723296582467118e-07,
      "loss": 1.684,
      "step": 357920
    },
    {
      "epoch": 0.001299078848459392,
      "grad_norm": 10265.506904191338,
      "learning_rate": 1.672254831560995e-07,
      "loss": 1.6858,
      "step": 357952
    },
    {
      "epoch": 0.0012991949828102289,
      "grad_norm": 8284.991852741921,
      "learning_rate": 1.6721800149185113e-07,
      "loss": 1.7004,
      "step": 357984
    },
    {
      "epoch": 0.0012993111171610656,
      "grad_norm": 10295.445206497872,
      "learning_rate": 1.6721052083170137e-07,
      "loss": 1.7092,
      "step": 358016
    },
    {
      "epoch": 0.0012994272515119024,
      "grad_norm": 12486.765073468789,
      "learning_rate": 1.6720304117542561e-07,
      "loss": 1.6756,
      "step": 358048
    },
    {
      "epoch": 0.001299543385862739,
      "grad_norm": 11439.432940491412,
      "learning_rate": 1.671955625227994e-07,
      "loss": 1.6825,
      "step": 358080
    },
    {
      "epoch": 0.0012996595202135757,
      "grad_norm": 7625.021180298451,
      "learning_rate": 1.6718808487359827e-07,
      "loss": 1.6982,
      "step": 358112
    },
    {
      "epoch": 0.0012997756545644124,
      "grad_norm": 20561.05483675388,
      "learning_rate": 1.6718060822759786e-07,
      "loss": 1.6973,
      "step": 358144
    },
    {
      "epoch": 0.0012998917889152492,
      "grad_norm": 10657.13976637259,
      "learning_rate": 1.6717336618323884e-07,
      "loss": 1.6861,
      "step": 358176
    },
    {
      "epoch": 0.001300007923266086,
      "grad_norm": 10868.126793518742,
      "learning_rate": 1.6716589151163448e-07,
      "loss": 1.673,
      "step": 358208
    },
    {
      "epoch": 0.0013001240576169225,
      "grad_norm": 7999.162581170606,
      "learning_rate": 1.6715841784256514e-07,
      "loss": 1.6678,
      "step": 358240
    },
    {
      "epoch": 0.0013002401919677592,
      "grad_norm": 8723.605218027693,
      "learning_rate": 1.6715094517580682e-07,
      "loss": 1.6778,
      "step": 358272
    },
    {
      "epoch": 0.001300356326318596,
      "grad_norm": 12426.191693354807,
      "learning_rate": 1.6714347351113539e-07,
      "loss": 1.6776,
      "step": 358304
    },
    {
      "epoch": 0.0013004724606694327,
      "grad_norm": 12901.019649624599,
      "learning_rate": 1.6713600284832698e-07,
      "loss": 1.6653,
      "step": 358336
    },
    {
      "epoch": 0.0013005885950202693,
      "grad_norm": 10144.314466734557,
      "learning_rate": 1.6712853318715771e-07,
      "loss": 1.6785,
      "step": 358368
    },
    {
      "epoch": 0.001300704729371106,
      "grad_norm": 10115.522131852611,
      "learning_rate": 1.6712106452740374e-07,
      "loss": 1.6862,
      "step": 358400
    },
    {
      "epoch": 0.0013008208637219428,
      "grad_norm": 9164.328235064479,
      "learning_rate": 1.6711359686884136e-07,
      "loss": 1.693,
      "step": 358432
    },
    {
      "epoch": 0.0013009369980727795,
      "grad_norm": 9804.235819277299,
      "learning_rate": 1.6710613021124688e-07,
      "loss": 1.6936,
      "step": 358464
    },
    {
      "epoch": 0.0013010531324236163,
      "grad_norm": 9673.742605630976,
      "learning_rate": 1.6709866455439673e-07,
      "loss": 1.6786,
      "step": 358496
    },
    {
      "epoch": 0.0013011692667744528,
      "grad_norm": 9865.858502938303,
      "learning_rate": 1.6709119989806737e-07,
      "loss": 1.6837,
      "step": 358528
    },
    {
      "epoch": 0.0013012854011252896,
      "grad_norm": 8745.989938251701,
      "learning_rate": 1.6708373624203533e-07,
      "loss": 1.7064,
      "step": 358560
    },
    {
      "epoch": 0.0013014015354761263,
      "grad_norm": 10110.006528187803,
      "learning_rate": 1.6707627358607728e-07,
      "loss": 1.7045,
      "step": 358592
    },
    {
      "epoch": 0.001301517669826963,
      "grad_norm": 8838.33400590858,
      "learning_rate": 1.6706881192996984e-07,
      "loss": 1.6818,
      "step": 358624
    },
    {
      "epoch": 0.0013016338041777996,
      "grad_norm": 10900.489530291747,
      "learning_rate": 1.6706135127348978e-07,
      "loss": 1.7056,
      "step": 358656
    },
    {
      "epoch": 0.0013017499385286364,
      "grad_norm": 10485.125845692077,
      "learning_rate": 1.670538916164139e-07,
      "loss": 1.7082,
      "step": 358688
    },
    {
      "epoch": 0.0013018660728794731,
      "grad_norm": 9342.409111144727,
      "learning_rate": 1.6704643295851914e-07,
      "loss": 1.7047,
      "step": 358720
    },
    {
      "epoch": 0.0013019822072303099,
      "grad_norm": 9325.19855016503,
      "learning_rate": 1.6703897529958243e-07,
      "loss": 1.7167,
      "step": 358752
    },
    {
      "epoch": 0.0013020983415811466,
      "grad_norm": 9683.55905646266,
      "learning_rate": 1.6703151863938082e-07,
      "loss": 1.6733,
      "step": 358784
    },
    {
      "epoch": 0.0013022144759319832,
      "grad_norm": 11561.548598695592,
      "learning_rate": 1.670240629776914e-07,
      "loss": 1.6647,
      "step": 358816
    },
    {
      "epoch": 0.00130233061028282,
      "grad_norm": 8803.418426952112,
      "learning_rate": 1.670166083142913e-07,
      "loss": 1.6773,
      "step": 358848
    },
    {
      "epoch": 0.0013024467446336567,
      "grad_norm": 14113.34659108179,
      "learning_rate": 1.6700915464895785e-07,
      "loss": 1.6771,
      "step": 358880
    },
    {
      "epoch": 0.0013025628789844934,
      "grad_norm": 12967.868136282077,
      "learning_rate": 1.6700170198146828e-07,
      "loss": 1.665,
      "step": 358912
    },
    {
      "epoch": 0.00130267901333533,
      "grad_norm": 9196.328397790066,
      "learning_rate": 1.6699425031160002e-07,
      "loss": 1.6844,
      "step": 358944
    },
    {
      "epoch": 0.0013027951476861667,
      "grad_norm": 8820.477084602624,
      "learning_rate": 1.6698679963913048e-07,
      "loss": 1.6874,
      "step": 358976
    },
    {
      "epoch": 0.0013029112820370035,
      "grad_norm": 9858.709246143737,
      "learning_rate": 1.669793499638372e-07,
      "loss": 1.6987,
      "step": 359008
    },
    {
      "epoch": 0.0013030274163878402,
      "grad_norm": 9470.633769711507,
      "learning_rate": 1.6697190128549778e-07,
      "loss": 1.7052,
      "step": 359040
    },
    {
      "epoch": 0.001303143550738677,
      "grad_norm": 10193.645079165744,
      "learning_rate": 1.6696445360388988e-07,
      "loss": 1.6677,
      "step": 359072
    },
    {
      "epoch": 0.0013032596850895135,
      "grad_norm": 11849.023926045555,
      "learning_rate": 1.6695700691879117e-07,
      "loss": 1.6841,
      "step": 359104
    },
    {
      "epoch": 0.0013033758194403503,
      "grad_norm": 9413.024062436047,
      "learning_rate": 1.669495612299795e-07,
      "loss": 1.693,
      "step": 359136
    },
    {
      "epoch": 0.001303491953791187,
      "grad_norm": 22391.19755618265,
      "learning_rate": 1.6694211653723272e-07,
      "loss": 1.6767,
      "step": 359168
    },
    {
      "epoch": 0.0013036080881420238,
      "grad_norm": 26286.8395970303,
      "learning_rate": 1.669346728403288e-07,
      "loss": 1.6563,
      "step": 359200
    },
    {
      "epoch": 0.0013037242224928603,
      "grad_norm": 24120.163846873013,
      "learning_rate": 1.6692723013904566e-07,
      "loss": 1.6865,
      "step": 359232
    },
    {
      "epoch": 0.001303840356843697,
      "grad_norm": 8290.265978845311,
      "learning_rate": 1.6692002097140545e-07,
      "loss": 1.6885,
      "step": 359264
    },
    {
      "epoch": 0.0013039564911945338,
      "grad_norm": 10922.198496639767,
      "learning_rate": 1.6691258022960235e-07,
      "loss": 1.6958,
      "step": 359296
    },
    {
      "epoch": 0.0013040726255453706,
      "grad_norm": 8575.339760032835,
      "learning_rate": 1.6690514048276145e-07,
      "loss": 1.6798,
      "step": 359328
    },
    {
      "epoch": 0.0013041887598962073,
      "grad_norm": 11619.603435573867,
      "learning_rate": 1.6689770173066101e-07,
      "loss": 1.6601,
      "step": 359360
    },
    {
      "epoch": 0.0013043048942470439,
      "grad_norm": 9454.094351126394,
      "learning_rate": 1.6689026397307937e-07,
      "loss": 1.6681,
      "step": 359392
    },
    {
      "epoch": 0.0013044210285978806,
      "grad_norm": 10925.913600244146,
      "learning_rate": 1.6688282720979498e-07,
      "loss": 1.6806,
      "step": 359424
    },
    {
      "epoch": 0.0013045371629487174,
      "grad_norm": 8809.987911455952,
      "learning_rate": 1.6687539144058628e-07,
      "loss": 1.6864,
      "step": 359456
    },
    {
      "epoch": 0.0013046532972995541,
      "grad_norm": 7967.900978300369,
      "learning_rate": 1.668679566652319e-07,
      "loss": 1.6845,
      "step": 359488
    },
    {
      "epoch": 0.0013047694316503907,
      "grad_norm": 11632.484515356124,
      "learning_rate": 1.6686052288351038e-07,
      "loss": 1.7149,
      "step": 359520
    },
    {
      "epoch": 0.0013048855660012274,
      "grad_norm": 8868.630108421481,
      "learning_rate": 1.6685309009520047e-07,
      "loss": 1.7128,
      "step": 359552
    },
    {
      "epoch": 0.0013050017003520642,
      "grad_norm": 14167.706095201156,
      "learning_rate": 1.6684565830008094e-07,
      "loss": 1.713,
      "step": 359584
    },
    {
      "epoch": 0.001305117834702901,
      "grad_norm": 8946.355459068236,
      "learning_rate": 1.6683822749793057e-07,
      "loss": 1.7198,
      "step": 359616
    },
    {
      "epoch": 0.0013052339690537377,
      "grad_norm": 11091.781101338054,
      "learning_rate": 1.6683079768852828e-07,
      "loss": 1.69,
      "step": 359648
    },
    {
      "epoch": 0.0013053501034045742,
      "grad_norm": 10825.163462969047,
      "learning_rate": 1.6682336887165304e-07,
      "loss": 1.6871,
      "step": 359680
    },
    {
      "epoch": 0.001305466237755411,
      "grad_norm": 10591.642176735391,
      "learning_rate": 1.6681594104708392e-07,
      "loss": 1.6767,
      "step": 359712
    },
    {
      "epoch": 0.0013055823721062477,
      "grad_norm": 11369.927000645166,
      "learning_rate": 1.6680851421459998e-07,
      "loss": 1.6756,
      "step": 359744
    },
    {
      "epoch": 0.0013056985064570845,
      "grad_norm": 9057.464104262297,
      "learning_rate": 1.668010883739804e-07,
      "loss": 1.667,
      "step": 359776
    },
    {
      "epoch": 0.001305814640807921,
      "grad_norm": 8693.52667218546,
      "learning_rate": 1.667936635250045e-07,
      "loss": 1.6882,
      "step": 359808
    },
    {
      "epoch": 0.0013059307751587578,
      "grad_norm": 9427.525656289672,
      "learning_rate": 1.6678623966745147e-07,
      "loss": 1.6973,
      "step": 359840
    },
    {
      "epoch": 0.0013060469095095945,
      "grad_norm": 8707.881028126189,
      "learning_rate": 1.667788168011008e-07,
      "loss": 1.6931,
      "step": 359872
    },
    {
      "epoch": 0.0013061630438604313,
      "grad_norm": 9356.522537780796,
      "learning_rate": 1.6677139492573187e-07,
      "loss": 1.7025,
      "step": 359904
    },
    {
      "epoch": 0.001306279178211268,
      "grad_norm": 10099.49464082238,
      "learning_rate": 1.6676397404112425e-07,
      "loss": 1.6725,
      "step": 359936
    },
    {
      "epoch": 0.0013063953125621046,
      "grad_norm": 11949.053686380357,
      "learning_rate": 1.6675655414705753e-07,
      "loss": 1.6686,
      "step": 359968
    },
    {
      "epoch": 0.0013065114469129413,
      "grad_norm": 9319.61791062273,
      "learning_rate": 1.6674913524331132e-07,
      "loss": 1.6903,
      "step": 360000
    },
    {
      "epoch": 0.001306627581263778,
      "grad_norm": 12601.685918955447,
      "learning_rate": 1.6674171732966536e-07,
      "loss": 1.6709,
      "step": 360032
    },
    {
      "epoch": 0.0013067437156146148,
      "grad_norm": 15834.754308166577,
      "learning_rate": 1.6673430040589945e-07,
      "loss": 1.669,
      "step": 360064
    },
    {
      "epoch": 0.0013068598499654514,
      "grad_norm": 8945.84909329461,
      "learning_rate": 1.6672688447179344e-07,
      "loss": 1.704,
      "step": 360096
    },
    {
      "epoch": 0.0013069759843162881,
      "grad_norm": 8965.038984856676,
      "learning_rate": 1.6671946952712732e-07,
      "loss": 1.6876,
      "step": 360128
    },
    {
      "epoch": 0.0013070921186671249,
      "grad_norm": 10787.320705346625,
      "learning_rate": 1.6671205557168104e-07,
      "loss": 1.7061,
      "step": 360160
    },
    {
      "epoch": 0.0013072082530179616,
      "grad_norm": 9529.070468833777,
      "learning_rate": 1.6670464260523467e-07,
      "loss": 1.6977,
      "step": 360192
    },
    {
      "epoch": 0.0013073243873687984,
      "grad_norm": 10541.099183671502,
      "learning_rate": 1.6669723062756834e-07,
      "loss": 1.6895,
      "step": 360224
    },
    {
      "epoch": 0.001307440521719635,
      "grad_norm": 19632.297471258935,
      "learning_rate": 1.6668981963846228e-07,
      "loss": 1.6747,
      "step": 360256
    },
    {
      "epoch": 0.0013075566560704717,
      "grad_norm": 18635.79609246678,
      "learning_rate": 1.6668240963769676e-07,
      "loss": 1.6807,
      "step": 360288
    },
    {
      "epoch": 0.0013076727904213084,
      "grad_norm": 9172.245090489023,
      "learning_rate": 1.6667523214174252e-07,
      "loss": 1.6814,
      "step": 360320
    },
    {
      "epoch": 0.0013077889247721452,
      "grad_norm": 10200.202547008563,
      "learning_rate": 1.6666782408613056e-07,
      "loss": 1.6823,
      "step": 360352
    },
    {
      "epoch": 0.0013079050591229817,
      "grad_norm": 8952.76158511998,
      "learning_rate": 1.666604170182072e-07,
      "loss": 1.7141,
      "step": 360384
    },
    {
      "epoch": 0.0013080211934738185,
      "grad_norm": 9519.831301026295,
      "learning_rate": 1.6665301093775305e-07,
      "loss": 1.7112,
      "step": 360416
    },
    {
      "epoch": 0.0013081373278246552,
      "grad_norm": 10222.726446501443,
      "learning_rate": 1.6664560584454862e-07,
      "loss": 1.7095,
      "step": 360448
    },
    {
      "epoch": 0.001308253462175492,
      "grad_norm": 12878.621199491816,
      "learning_rate": 1.6663820173837466e-07,
      "loss": 1.6952,
      "step": 360480
    },
    {
      "epoch": 0.0013083695965263287,
      "grad_norm": 12551.03087399597,
      "learning_rate": 1.6663079861901196e-07,
      "loss": 1.6884,
      "step": 360512
    },
    {
      "epoch": 0.0013084857308771653,
      "grad_norm": 9086.840374959824,
      "learning_rate": 1.6662339648624125e-07,
      "loss": 1.674,
      "step": 360544
    },
    {
      "epoch": 0.001308601865228002,
      "grad_norm": 10792.169754039269,
      "learning_rate": 1.6661599533984346e-07,
      "loss": 1.6776,
      "step": 360576
    },
    {
      "epoch": 0.0013087179995788388,
      "grad_norm": 9586.08512376142,
      "learning_rate": 1.6660859517959954e-07,
      "loss": 1.6742,
      "step": 360608
    },
    {
      "epoch": 0.0013088341339296755,
      "grad_norm": 8844.23902888202,
      "learning_rate": 1.666011960052905e-07,
      "loss": 1.6785,
      "step": 360640
    },
    {
      "epoch": 0.001308950268280512,
      "grad_norm": 9193.125475049277,
      "learning_rate": 1.6659379781669744e-07,
      "loss": 1.6984,
      "step": 360672
    },
    {
      "epoch": 0.0013090664026313488,
      "grad_norm": 10045.374856121598,
      "learning_rate": 1.6658640061360152e-07,
      "loss": 1.6988,
      "step": 360704
    },
    {
      "epoch": 0.0013091825369821856,
      "grad_norm": 11606.095984438523,
      "learning_rate": 1.6657900439578397e-07,
      "loss": 1.7139,
      "step": 360736
    },
    {
      "epoch": 0.0013092986713330223,
      "grad_norm": 12502.79216815188,
      "learning_rate": 1.6657160916302608e-07,
      "loss": 1.7052,
      "step": 360768
    },
    {
      "epoch": 0.001309414805683859,
      "grad_norm": 10350.864698178602,
      "learning_rate": 1.6656421491510916e-07,
      "loss": 1.6801,
      "step": 360800
    },
    {
      "epoch": 0.0013095309400346956,
      "grad_norm": 9882.829756704301,
      "learning_rate": 1.6655682165181474e-07,
      "loss": 1.6716,
      "step": 360832
    },
    {
      "epoch": 0.0013096470743855324,
      "grad_norm": 10805.791039993324,
      "learning_rate": 1.6654942937292424e-07,
      "loss": 1.6874,
      "step": 360864
    },
    {
      "epoch": 0.0013097632087363691,
      "grad_norm": 9104.782040224796,
      "learning_rate": 1.6654203807821927e-07,
      "loss": 1.6766,
      "step": 360896
    },
    {
      "epoch": 0.001309879343087206,
      "grad_norm": 10760.464859846901,
      "learning_rate": 1.6653464776748143e-07,
      "loss": 1.6879,
      "step": 360928
    },
    {
      "epoch": 0.0013099954774380424,
      "grad_norm": 12024.949396982924,
      "learning_rate": 1.6652725844049247e-07,
      "loss": 1.6992,
      "step": 360960
    },
    {
      "epoch": 0.0013101116117888792,
      "grad_norm": 10315.062384687744,
      "learning_rate": 1.6651987009703413e-07,
      "loss": 1.7016,
      "step": 360992
    },
    {
      "epoch": 0.001310227746139716,
      "grad_norm": 8862.421113894328,
      "learning_rate": 1.6651248273688823e-07,
      "loss": 1.6959,
      "step": 361024
    },
    {
      "epoch": 0.0013103438804905527,
      "grad_norm": 12707.597884730221,
      "learning_rate": 1.6650509635983667e-07,
      "loss": 1.6703,
      "step": 361056
    },
    {
      "epoch": 0.0013104600148413895,
      "grad_norm": 13679.08301020211,
      "learning_rate": 1.6649771096566144e-07,
      "loss": 1.6676,
      "step": 361088
    },
    {
      "epoch": 0.001310576149192226,
      "grad_norm": 9547.529942346346,
      "learning_rate": 1.6649032655414457e-07,
      "loss": 1.6721,
      "step": 361120
    },
    {
      "epoch": 0.0013106922835430627,
      "grad_norm": 9049.359977368566,
      "learning_rate": 1.6648294312506823e-07,
      "loss": 1.6967,
      "step": 361152
    },
    {
      "epoch": 0.0013108084178938995,
      "grad_norm": 9521.83448711434,
      "learning_rate": 1.664755606782145e-07,
      "loss": 1.6747,
      "step": 361184
    },
    {
      "epoch": 0.0013109245522447363,
      "grad_norm": 11279.704251442055,
      "learning_rate": 1.6646817921336566e-07,
      "loss": 1.6903,
      "step": 361216
    },
    {
      "epoch": 0.0013110406865955728,
      "grad_norm": 11748.20343712178,
      "learning_rate": 1.6646079873030408e-07,
      "loss": 1.7148,
      "step": 361248
    },
    {
      "epoch": 0.0013111568209464095,
      "grad_norm": 8566.446637900688,
      "learning_rate": 1.6645341922881203e-07,
      "loss": 1.7227,
      "step": 361280
    },
    {
      "epoch": 0.0013112729552972463,
      "grad_norm": 22852.4095884876,
      "learning_rate": 1.6644604070867204e-07,
      "loss": 1.7261,
      "step": 361312
    },
    {
      "epoch": 0.001311389089648083,
      "grad_norm": 9930.23967485176,
      "learning_rate": 1.6643889370291154e-07,
      "loss": 1.6976,
      "step": 361344
    },
    {
      "epoch": 0.0013115052239989198,
      "grad_norm": 11093.602300425233,
      "learning_rate": 1.6643151711417285e-07,
      "loss": 1.6957,
      "step": 361376
    },
    {
      "epoch": 0.0013116213583497563,
      "grad_norm": 9607.107785384735,
      "learning_rate": 1.6642414150614072e-07,
      "loss": 1.6792,
      "step": 361408
    },
    {
      "epoch": 0.001311737492700593,
      "grad_norm": 9377.398679804544,
      "learning_rate": 1.6641676687859787e-07,
      "loss": 1.6876,
      "step": 361440
    },
    {
      "epoch": 0.0013118536270514299,
      "grad_norm": 12385.019176408246,
      "learning_rate": 1.664093932313271e-07,
      "loss": 1.6756,
      "step": 361472
    },
    {
      "epoch": 0.0013119697614022666,
      "grad_norm": 9466.441570093803,
      "learning_rate": 1.664020205641112e-07,
      "loss": 1.6749,
      "step": 361504
    },
    {
      "epoch": 0.0013120858957531031,
      "grad_norm": 9390.620852744509,
      "learning_rate": 1.6639464887673317e-07,
      "loss": 1.6822,
      "step": 361536
    },
    {
      "epoch": 0.00131220203010394,
      "grad_norm": 12438.454244800678,
      "learning_rate": 1.663872781689759e-07,
      "loss": 1.6804,
      "step": 361568
    },
    {
      "epoch": 0.0013123181644547767,
      "grad_norm": 10473.171821372931,
      "learning_rate": 1.6637990844062254e-07,
      "loss": 1.6924,
      "step": 361600
    },
    {
      "epoch": 0.0013124342988056134,
      "grad_norm": 12570.049482798388,
      "learning_rate": 1.6637253969145608e-07,
      "loss": 1.6814,
      "step": 361632
    },
    {
      "epoch": 0.0013125504331564502,
      "grad_norm": 10606.588141339325,
      "learning_rate": 1.6636517192125985e-07,
      "loss": 1.6859,
      "step": 361664
    },
    {
      "epoch": 0.0013126665675072867,
      "grad_norm": 9493.794394234583,
      "learning_rate": 1.6635780512981698e-07,
      "loss": 1.6762,
      "step": 361696
    },
    {
      "epoch": 0.0013127827018581235,
      "grad_norm": 9495.605931166268,
      "learning_rate": 1.6635043931691085e-07,
      "loss": 1.6907,
      "step": 361728
    },
    {
      "epoch": 0.0013128988362089602,
      "grad_norm": 9057.638102728548,
      "learning_rate": 1.6634307448232486e-07,
      "loss": 1.6855,
      "step": 361760
    },
    {
      "epoch": 0.001313014970559797,
      "grad_norm": 11123.082666239607,
      "learning_rate": 1.663357106258424e-07,
      "loss": 1.704,
      "step": 361792
    },
    {
      "epoch": 0.0013131311049106335,
      "grad_norm": 10703.289774644056,
      "learning_rate": 1.6632834774724709e-07,
      "loss": 1.7132,
      "step": 361824
    },
    {
      "epoch": 0.0013132472392614703,
      "grad_norm": 9401.355434191391,
      "learning_rate": 1.6632098584632242e-07,
      "loss": 1.6952,
      "step": 361856
    },
    {
      "epoch": 0.001313363373612307,
      "grad_norm": 9143.630351233583,
      "learning_rate": 1.6631362492285206e-07,
      "loss": 1.6822,
      "step": 361888
    },
    {
      "epoch": 0.0013134795079631438,
      "grad_norm": 9994.549914828582,
      "learning_rate": 1.6630626497661976e-07,
      "loss": 1.6784,
      "step": 361920
    },
    {
      "epoch": 0.0013135956423139805,
      "grad_norm": 11073.377804446121,
      "learning_rate": 1.6629890600740934e-07,
      "loss": 1.6653,
      "step": 361952
    },
    {
      "epoch": 0.001313711776664817,
      "grad_norm": 8060.164638517007,
      "learning_rate": 1.662915480150046e-07,
      "loss": 1.6814,
      "step": 361984
    },
    {
      "epoch": 0.0013138279110156538,
      "grad_norm": 8820.040816232087,
      "learning_rate": 1.6628419099918943e-07,
      "loss": 1.685,
      "step": 362016
    },
    {
      "epoch": 0.0013139440453664906,
      "grad_norm": 9840.611159882297,
      "learning_rate": 1.6627683495974792e-07,
      "loss": 1.6716,
      "step": 362048
    },
    {
      "epoch": 0.0013140601797173273,
      "grad_norm": 10269.354215334088,
      "learning_rate": 1.6626947989646405e-07,
      "loss": 1.6925,
      "step": 362080
    },
    {
      "epoch": 0.0013141763140681639,
      "grad_norm": 11753.996171515457,
      "learning_rate": 1.6626212580912202e-07,
      "loss": 1.6994,
      "step": 362112
    },
    {
      "epoch": 0.0013142924484190006,
      "grad_norm": 10419.29517769796,
      "learning_rate": 1.6625477269750593e-07,
      "loss": 1.7043,
      "step": 362144
    },
    {
      "epoch": 0.0013144085827698374,
      "grad_norm": 10760.619684757938,
      "learning_rate": 1.6624742056140005e-07,
      "loss": 1.7094,
      "step": 362176
    },
    {
      "epoch": 0.0013145247171206741,
      "grad_norm": 10082.3074739863,
      "learning_rate": 1.6624006940058874e-07,
      "loss": 1.6831,
      "step": 362208
    },
    {
      "epoch": 0.0013146408514715109,
      "grad_norm": 8460.11193779373,
      "learning_rate": 1.6623271921485638e-07,
      "loss": 1.6986,
      "step": 362240
    },
    {
      "epoch": 0.0013147569858223474,
      "grad_norm": 11140.807690647927,
      "learning_rate": 1.6622537000398745e-07,
      "loss": 1.7106,
      "step": 362272
    },
    {
      "epoch": 0.0013148731201731842,
      "grad_norm": 10371.354009964176,
      "learning_rate": 1.6621802176776642e-07,
      "loss": 1.6821,
      "step": 362304
    },
    {
      "epoch": 0.001314989254524021,
      "grad_norm": 9106.372164588925,
      "learning_rate": 1.6621067450597792e-07,
      "loss": 1.6887,
      "step": 362336
    },
    {
      "epoch": 0.0013151053888748577,
      "grad_norm": 8873.597692029993,
      "learning_rate": 1.6620355777514892e-07,
      "loss": 1.7149,
      "step": 362368
    },
    {
      "epoch": 0.0013152215232256942,
      "grad_norm": 9394.319453797598,
      "learning_rate": 1.6619621243114516e-07,
      "loss": 1.7046,
      "step": 362400
    },
    {
      "epoch": 0.001315337657576531,
      "grad_norm": 10202.032346547427,
      "learning_rate": 1.6618886806093484e-07,
      "loss": 1.6999,
      "step": 362432
    },
    {
      "epoch": 0.0013154537919273677,
      "grad_norm": 10534.201441020577,
      "learning_rate": 1.6618152466430281e-07,
      "loss": 1.7001,
      "step": 362464
    },
    {
      "epoch": 0.0013155699262782045,
      "grad_norm": 8895.301906062548,
      "learning_rate": 1.6617418224103396e-07,
      "loss": 1.684,
      "step": 362496
    },
    {
      "epoch": 0.0013156860606290412,
      "grad_norm": 9720.776306447959,
      "learning_rate": 1.661668407909133e-07,
      "loss": 1.7065,
      "step": 362528
    },
    {
      "epoch": 0.0013158021949798778,
      "grad_norm": 8191.647209200357,
      "learning_rate": 1.6615950031372585e-07,
      "loss": 1.7053,
      "step": 362560
    },
    {
      "epoch": 0.0013159183293307145,
      "grad_norm": 9238.03377348232,
      "learning_rate": 1.6615216080925675e-07,
      "loss": 1.687,
      "step": 362592
    },
    {
      "epoch": 0.0013160344636815513,
      "grad_norm": 9040.692893799678,
      "learning_rate": 1.661448222772912e-07,
      "loss": 1.6765,
      "step": 362624
    },
    {
      "epoch": 0.001316150598032388,
      "grad_norm": 12038.574500330178,
      "learning_rate": 1.6613748471761445e-07,
      "loss": 1.7007,
      "step": 362656
    },
    {
      "epoch": 0.0013162667323832246,
      "grad_norm": 8560.197077170595,
      "learning_rate": 1.661301481300118e-07,
      "loss": 1.6936,
      "step": 362688
    },
    {
      "epoch": 0.0013163828667340613,
      "grad_norm": 9580.507710972315,
      "learning_rate": 1.6612281251426863e-07,
      "loss": 1.7015,
      "step": 362720
    },
    {
      "epoch": 0.001316499001084898,
      "grad_norm": 12728.997603896389,
      "learning_rate": 1.6611547787017046e-07,
      "loss": 1.6984,
      "step": 362752
    },
    {
      "epoch": 0.0013166151354357348,
      "grad_norm": 9434.971860053425,
      "learning_rate": 1.6610814419750274e-07,
      "loss": 1.6814,
      "step": 362784
    },
    {
      "epoch": 0.0013167312697865716,
      "grad_norm": 8818.678812611332,
      "learning_rate": 1.6610081149605107e-07,
      "loss": 1.6889,
      "step": 362816
    },
    {
      "epoch": 0.0013168474041374081,
      "grad_norm": 11644.060116643162,
      "learning_rate": 1.6609347976560113e-07,
      "loss": 1.703,
      "step": 362848
    },
    {
      "epoch": 0.0013169635384882449,
      "grad_norm": 9534.84032378099,
      "learning_rate": 1.6608614900593858e-07,
      "loss": 1.706,
      "step": 362880
    },
    {
      "epoch": 0.0013170796728390816,
      "grad_norm": 9944.289416544552,
      "learning_rate": 1.6607881921684924e-07,
      "loss": 1.7026,
      "step": 362912
    },
    {
      "epoch": 0.0013171958071899184,
      "grad_norm": 8105.477530657895,
      "learning_rate": 1.66071490398119e-07,
      "loss": 1.7129,
      "step": 362944
    },
    {
      "epoch": 0.001317311941540755,
      "grad_norm": 9509.760459654071,
      "learning_rate": 1.660641625495337e-07,
      "loss": 1.6858,
      "step": 362976
    },
    {
      "epoch": 0.0013174280758915917,
      "grad_norm": 9710.870609785716,
      "learning_rate": 1.6605683567087937e-07,
      "loss": 1.6949,
      "step": 363008
    },
    {
      "epoch": 0.0013175442102424284,
      "grad_norm": 10024.272342669068,
      "learning_rate": 1.6604950976194206e-07,
      "loss": 1.7019,
      "step": 363040
    },
    {
      "epoch": 0.0013176603445932652,
      "grad_norm": 8368.36567078662,
      "learning_rate": 1.6604218482250783e-07,
      "loss": 1.684,
      "step": 363072
    },
    {
      "epoch": 0.001317776478944102,
      "grad_norm": 10218.644332787006,
      "learning_rate": 1.6603486085236294e-07,
      "loss": 1.695,
      "step": 363104
    },
    {
      "epoch": 0.0013178926132949385,
      "grad_norm": 9252.581477620179,
      "learning_rate": 1.6602753785129357e-07,
      "loss": 1.7002,
      "step": 363136
    },
    {
      "epoch": 0.0013180087476457752,
      "grad_norm": 9281.291289470448,
      "learning_rate": 1.6602021581908606e-07,
      "loss": 1.6711,
      "step": 363168
    },
    {
      "epoch": 0.001318124881996612,
      "grad_norm": 9441.636722517977,
      "learning_rate": 1.6601289475552682e-07,
      "loss": 1.6672,
      "step": 363200
    },
    {
      "epoch": 0.0013182410163474487,
      "grad_norm": 10045.537118541746,
      "learning_rate": 1.6600557466040223e-07,
      "loss": 1.6922,
      "step": 363232
    },
    {
      "epoch": 0.0013183571506982853,
      "grad_norm": 8784.419844246971,
      "learning_rate": 1.6599825553349886e-07,
      "loss": 1.6826,
      "step": 363264
    },
    {
      "epoch": 0.001318473285049122,
      "grad_norm": 11479.768464564082,
      "learning_rate": 1.6599093737460324e-07,
      "loss": 1.6816,
      "step": 363296
    },
    {
      "epoch": 0.0013185894193999588,
      "grad_norm": 10175.2491861379,
      "learning_rate": 1.6598362018350205e-07,
      "loss": 1.6718,
      "step": 363328
    },
    {
      "epoch": 0.0013187055537507955,
      "grad_norm": 23500.245785948708,
      "learning_rate": 1.6597630395998195e-07,
      "loss": 1.6771,
      "step": 363360
    },
    {
      "epoch": 0.0013188216881016323,
      "grad_norm": 18589.66938920647,
      "learning_rate": 1.6596898870382975e-07,
      "loss": 1.6914,
      "step": 363392
    },
    {
      "epoch": 0.0013189378224524688,
      "grad_norm": 11766.270097188828,
      "learning_rate": 1.6596190297172605e-07,
      "loss": 1.7264,
      "step": 363424
    },
    {
      "epoch": 0.0013190539568033056,
      "grad_norm": 9614.557296100533,
      "learning_rate": 1.659545896194565e-07,
      "loss": 1.6883,
      "step": 363456
    },
    {
      "epoch": 0.0013191700911541423,
      "grad_norm": 8183.092813844897,
      "learning_rate": 1.6594727723392228e-07,
      "loss": 1.6751,
      "step": 363488
    },
    {
      "epoch": 0.001319286225504979,
      "grad_norm": 8422.786712246725,
      "learning_rate": 1.659399658149103e-07,
      "loss": 1.683,
      "step": 363520
    },
    {
      "epoch": 0.0013194023598558156,
      "grad_norm": 8960.162498526464,
      "learning_rate": 1.6593265536220773e-07,
      "loss": 1.6802,
      "step": 363552
    },
    {
      "epoch": 0.0013195184942066524,
      "grad_norm": 9457.129374181153,
      "learning_rate": 1.6592534587560173e-07,
      "loss": 1.6854,
      "step": 363584
    },
    {
      "epoch": 0.0013196346285574891,
      "grad_norm": 9612.987048779376,
      "learning_rate": 1.6591803735487956e-07,
      "loss": 1.6802,
      "step": 363616
    },
    {
      "epoch": 0.0013197507629083259,
      "grad_norm": 10086.817535774106,
      "learning_rate": 1.6591072979982844e-07,
      "loss": 1.6658,
      "step": 363648
    },
    {
      "epoch": 0.0013198668972591626,
      "grad_norm": 8355.745568170443,
      "learning_rate": 1.6590342321023577e-07,
      "loss": 1.6702,
      "step": 363680
    },
    {
      "epoch": 0.0013199830316099992,
      "grad_norm": 11678.893783231355,
      "learning_rate": 1.6589611758588903e-07,
      "loss": 1.6731,
      "step": 363712
    },
    {
      "epoch": 0.001320099165960836,
      "grad_norm": 9888.680700679944,
      "learning_rate": 1.6588881292657566e-07,
      "loss": 1.6664,
      "step": 363744
    },
    {
      "epoch": 0.0013202153003116727,
      "grad_norm": 11464.400900177907,
      "learning_rate": 1.658815092320832e-07,
      "loss": 1.6775,
      "step": 363776
    },
    {
      "epoch": 0.0013203314346625094,
      "grad_norm": 9633.487738093614,
      "learning_rate": 1.6587420650219932e-07,
      "loss": 1.6897,
      "step": 363808
    },
    {
      "epoch": 0.001320447569013346,
      "grad_norm": 9192.870715940695,
      "learning_rate": 1.6586690473671168e-07,
      "loss": 1.6907,
      "step": 363840
    },
    {
      "epoch": 0.0013205637033641827,
      "grad_norm": 9524.588180073719,
      "learning_rate": 1.6585960393540804e-07,
      "loss": 1.7005,
      "step": 363872
    },
    {
      "epoch": 0.0013206798377150195,
      "grad_norm": 10236.726820619959,
      "learning_rate": 1.6585230409807624e-07,
      "loss": 1.7003,
      "step": 363904
    },
    {
      "epoch": 0.0013207959720658562,
      "grad_norm": 9039.104380412919,
      "learning_rate": 1.6584500522450418e-07,
      "loss": 1.6949,
      "step": 363936
    },
    {
      "epoch": 0.001320912106416693,
      "grad_norm": 11275.844802053634,
      "learning_rate": 1.6583770731447978e-07,
      "loss": 1.7091,
      "step": 363968
    },
    {
      "epoch": 0.0013210282407675295,
      "grad_norm": 9685.419763747981,
      "learning_rate": 1.6583041036779102e-07,
      "loss": 1.7222,
      "step": 364000
    },
    {
      "epoch": 0.0013211443751183663,
      "grad_norm": 10506.850146452076,
      "learning_rate": 1.6582311438422606e-07,
      "loss": 1.6921,
      "step": 364032
    },
    {
      "epoch": 0.001321260509469203,
      "grad_norm": 8655.865756814856,
      "learning_rate": 1.6581581936357302e-07,
      "loss": 1.6652,
      "step": 364064
    },
    {
      "epoch": 0.0013213766438200398,
      "grad_norm": 9797.200212305554,
      "learning_rate": 1.658085253056201e-07,
      "loss": 1.6825,
      "step": 364096
    },
    {
      "epoch": 0.0013214927781708763,
      "grad_norm": 9646.880946710186,
      "learning_rate": 1.6580123221015558e-07,
      "loss": 1.6824,
      "step": 364128
    },
    {
      "epoch": 0.001321608912521713,
      "grad_norm": 9723.296971706664,
      "learning_rate": 1.657939400769678e-07,
      "loss": 1.6904,
      "step": 364160
    },
    {
      "epoch": 0.0013217250468725498,
      "grad_norm": 10581.99243998974,
      "learning_rate": 1.657866489058452e-07,
      "loss": 1.6737,
      "step": 364192
    },
    {
      "epoch": 0.0013218411812233866,
      "grad_norm": 13538.59505266333,
      "learning_rate": 1.657793586965762e-07,
      "loss": 1.664,
      "step": 364224
    },
    {
      "epoch": 0.0013219573155742233,
      "grad_norm": 10351.326678257237,
      "learning_rate": 1.657720694489494e-07,
      "loss": 1.6739,
      "step": 364256
    },
    {
      "epoch": 0.0013220734499250599,
      "grad_norm": 10752.701986012631,
      "learning_rate": 1.6576478116275336e-07,
      "loss": 1.6962,
      "step": 364288
    },
    {
      "epoch": 0.0013221895842758966,
      "grad_norm": 17464.48355949869,
      "learning_rate": 1.6575749383777674e-07,
      "loss": 1.6741,
      "step": 364320
    },
    {
      "epoch": 0.0013223057186267334,
      "grad_norm": 9525.609271852378,
      "learning_rate": 1.657502074738083e-07,
      "loss": 1.665,
      "step": 364352
    },
    {
      "epoch": 0.0013224218529775701,
      "grad_norm": 9493.55297030569,
      "learning_rate": 1.6574292207063684e-07,
      "loss": 1.6902,
      "step": 364384
    },
    {
      "epoch": 0.0013225379873284067,
      "grad_norm": 8630.954640131067,
      "learning_rate": 1.6573563762805122e-07,
      "loss": 1.6906,
      "step": 364416
    },
    {
      "epoch": 0.0013226541216792434,
      "grad_norm": 18510.59329141019,
      "learning_rate": 1.6572835414584039e-07,
      "loss": 1.6981,
      "step": 364448
    },
    {
      "epoch": 0.0013227702560300802,
      "grad_norm": 32422.47220678892,
      "learning_rate": 1.6572107162379326e-07,
      "loss": 1.6818,
      "step": 364480
    },
    {
      "epoch": 0.001322886390380917,
      "grad_norm": 9940.255529914712,
      "learning_rate": 1.65714017595986e-07,
      "loss": 1.682,
      "step": 364512
    },
    {
      "epoch": 0.0013230025247317537,
      "grad_norm": 11927.218619611196,
      "learning_rate": 1.657067369636449e-07,
      "loss": 1.697,
      "step": 364544
    },
    {
      "epoch": 0.0013231186590825902,
      "grad_norm": 10217.120337942584,
      "learning_rate": 1.6569945729084154e-07,
      "loss": 1.6804,
      "step": 364576
    },
    {
      "epoch": 0.001323234793433427,
      "grad_norm": 8922.077784910867,
      "learning_rate": 1.6569217857736514e-07,
      "loss": 1.66,
      "step": 364608
    },
    {
      "epoch": 0.0013233509277842637,
      "grad_norm": 9777.003017285,
      "learning_rate": 1.65684900823005e-07,
      "loss": 1.676,
      "step": 364640
    },
    {
      "epoch": 0.0013234670621351005,
      "grad_norm": 9003.753883797579,
      "learning_rate": 1.6567762402755054e-07,
      "loss": 1.6849,
      "step": 364672
    },
    {
      "epoch": 0.001323583196485937,
      "grad_norm": 10530.99501471727,
      "learning_rate": 1.656703481907912e-07,
      "loss": 1.7012,
      "step": 364704
    },
    {
      "epoch": 0.0013236993308367738,
      "grad_norm": 8083.524849964896,
      "learning_rate": 1.6566307331251645e-07,
      "loss": 1.6984,
      "step": 364736
    },
    {
      "epoch": 0.0013238154651876105,
      "grad_norm": 7688.709774728137,
      "learning_rate": 1.6565579939251594e-07,
      "loss": 1.6891,
      "step": 364768
    },
    {
      "epoch": 0.0013239315995384473,
      "grad_norm": 8968.056199645494,
      "learning_rate": 1.656485264305792e-07,
      "loss": 1.6871,
      "step": 364800
    },
    {
      "epoch": 0.001324047733889284,
      "grad_norm": 11262.426381557396,
      "learning_rate": 1.65641254426496e-07,
      "loss": 1.698,
      "step": 364832
    },
    {
      "epoch": 0.0013241638682401206,
      "grad_norm": 9531.107385818292,
      "learning_rate": 1.6563398338005614e-07,
      "loss": 1.712,
      "step": 364864
    },
    {
      "epoch": 0.0013242800025909573,
      "grad_norm": 13060.548227390764,
      "learning_rate": 1.656267132910494e-07,
      "loss": 1.6878,
      "step": 364896
    },
    {
      "epoch": 0.001324396136941794,
      "grad_norm": 9771.86614726174,
      "learning_rate": 1.6561944415926574e-07,
      "loss": 1.69,
      "step": 364928
    },
    {
      "epoch": 0.0013245122712926308,
      "grad_norm": 9701.091278820131,
      "learning_rate": 1.6561217598449503e-07,
      "loss": 1.6914,
      "step": 364960
    },
    {
      "epoch": 0.0013246284056434674,
      "grad_norm": 9119.815568310578,
      "learning_rate": 1.6560490876652735e-07,
      "loss": 1.6919,
      "step": 364992
    },
    {
      "epoch": 0.0013247445399943041,
      "grad_norm": 10082.80258658276,
      "learning_rate": 1.6559764250515276e-07,
      "loss": 1.7039,
      "step": 365024
    },
    {
      "epoch": 0.001324860674345141,
      "grad_norm": 10680.837982106086,
      "learning_rate": 1.655903772001615e-07,
      "loss": 1.6892,
      "step": 365056
    },
    {
      "epoch": 0.0013249768086959776,
      "grad_norm": 8617.643761493046,
      "learning_rate": 1.6558311285134373e-07,
      "loss": 1.6842,
      "step": 365088
    },
    {
      "epoch": 0.0013250929430468144,
      "grad_norm": 11500.193911408625,
      "learning_rate": 1.655758494584897e-07,
      "loss": 1.68,
      "step": 365120
    },
    {
      "epoch": 0.001325209077397651,
      "grad_norm": 8160.172424648881,
      "learning_rate": 1.655685870213898e-07,
      "loss": 1.6941,
      "step": 365152
    },
    {
      "epoch": 0.0013253252117484877,
      "grad_norm": 9577.035867114626,
      "learning_rate": 1.6556132553983445e-07,
      "loss": 1.6845,
      "step": 365184
    },
    {
      "epoch": 0.0013254413460993244,
      "grad_norm": 8577.347958430973,
      "learning_rate": 1.655540650136141e-07,
      "loss": 1.6755,
      "step": 365216
    },
    {
      "epoch": 0.0013255574804501612,
      "grad_norm": 9570.968811985545,
      "learning_rate": 1.6554680544251934e-07,
      "loss": 1.6901,
      "step": 365248
    },
    {
      "epoch": 0.0013256736148009977,
      "grad_norm": 8173.406389015537,
      "learning_rate": 1.6553954682634073e-07,
      "loss": 1.6822,
      "step": 365280
    },
    {
      "epoch": 0.0013257897491518345,
      "grad_norm": 11676.768045996288,
      "learning_rate": 1.6553228916486895e-07,
      "loss": 1.6822,
      "step": 365312
    },
    {
      "epoch": 0.0013259058835026712,
      "grad_norm": 8433.922574935106,
      "learning_rate": 1.6552503245789477e-07,
      "loss": 1.6697,
      "step": 365344
    },
    {
      "epoch": 0.001326022017853508,
      "grad_norm": 11973.113212527476,
      "learning_rate": 1.655177767052089e-07,
      "loss": 1.6682,
      "step": 365376
    },
    {
      "epoch": 0.0013261381522043448,
      "grad_norm": 10939.391025098244,
      "learning_rate": 1.655105219066023e-07,
      "loss": 1.6764,
      "step": 365408
    },
    {
      "epoch": 0.0013262542865551813,
      "grad_norm": 11358.378053225733,
      "learning_rate": 1.6550326806186585e-07,
      "loss": 1.6846,
      "step": 365440
    },
    {
      "epoch": 0.001326370420906018,
      "grad_norm": 11117.28168213795,
      "learning_rate": 1.6549601517079058e-07,
      "loss": 1.664,
      "step": 365472
    },
    {
      "epoch": 0.0013264865552568548,
      "grad_norm": 18801.082096517745,
      "learning_rate": 1.654887632331675e-07,
      "loss": 1.681,
      "step": 365504
    },
    {
      "epoch": 0.0013266026896076916,
      "grad_norm": 17115.081536469523,
      "learning_rate": 1.6548151224878774e-07,
      "loss": 1.6943,
      "step": 365536
    },
    {
      "epoch": 0.001326718823958528,
      "grad_norm": 10735.815385894077,
      "learning_rate": 1.6547448876649829e-07,
      "loss": 1.7035,
      "step": 365568
    },
    {
      "epoch": 0.0013268349583093648,
      "grad_norm": 12338.888766821752,
      "learning_rate": 1.6546723965820614e-07,
      "loss": 1.7095,
      "step": 365600
    },
    {
      "epoch": 0.0013269510926602016,
      "grad_norm": 8717.447332791866,
      "learning_rate": 1.654599915025376e-07,
      "loss": 1.6943,
      "step": 365632
    },
    {
      "epoch": 0.0013270672270110384,
      "grad_norm": 9080.282814978837,
      "learning_rate": 1.6545274429928405e-07,
      "loss": 1.6934,
      "step": 365664
    },
    {
      "epoch": 0.0013271833613618751,
      "grad_norm": 9108.15985806134,
      "learning_rate": 1.6544549804823688e-07,
      "loss": 1.6994,
      "step": 365696
    },
    {
      "epoch": 0.0013272994957127116,
      "grad_norm": 9213.581713969872,
      "learning_rate": 1.6543825274918764e-07,
      "loss": 1.7024,
      "step": 365728
    },
    {
      "epoch": 0.0013274156300635484,
      "grad_norm": 10182.164799294893,
      "learning_rate": 1.6543100840192782e-07,
      "loss": 1.6921,
      "step": 365760
    },
    {
      "epoch": 0.0013275317644143852,
      "grad_norm": 10242.415925942472,
      "learning_rate": 1.6542376500624915e-07,
      "loss": 1.717,
      "step": 365792
    },
    {
      "epoch": 0.001327647898765222,
      "grad_norm": 8335.771469996043,
      "learning_rate": 1.6541652256194325e-07,
      "loss": 1.6888,
      "step": 365824
    },
    {
      "epoch": 0.0013277640331160584,
      "grad_norm": 11628.890488778368,
      "learning_rate": 1.654092810688019e-07,
      "loss": 1.6925,
      "step": 365856
    },
    {
      "epoch": 0.0013278801674668952,
      "grad_norm": 9524.07727814091,
      "learning_rate": 1.654020405266169e-07,
      "loss": 1.6899,
      "step": 365888
    },
    {
      "epoch": 0.001327996301817732,
      "grad_norm": 8607.03421626753,
      "learning_rate": 1.653948009351802e-07,
      "loss": 1.6783,
      "step": 365920
    },
    {
      "epoch": 0.0013281124361685687,
      "grad_norm": 13193.745639506624,
      "learning_rate": 1.6538756229428368e-07,
      "loss": 1.6778,
      "step": 365952
    },
    {
      "epoch": 0.0013282285705194055,
      "grad_norm": 10455.251312139751,
      "learning_rate": 1.653803246037194e-07,
      "loss": 1.6726,
      "step": 365984
    },
    {
      "epoch": 0.001328344704870242,
      "grad_norm": 11095.62904931487,
      "learning_rate": 1.6537308786327936e-07,
      "loss": 1.6888,
      "step": 366016
    },
    {
      "epoch": 0.0013284608392210788,
      "grad_norm": 15137.735101394792,
      "learning_rate": 1.6536585207275578e-07,
      "loss": 1.6829,
      "step": 366048
    },
    {
      "epoch": 0.0013285769735719155,
      "grad_norm": 11511.911048996166,
      "learning_rate": 1.6535861723194084e-07,
      "loss": 1.7051,
      "step": 366080
    },
    {
      "epoch": 0.0013286931079227523,
      "grad_norm": 8237.059669566563,
      "learning_rate": 1.6535138334062683e-07,
      "loss": 1.6909,
      "step": 366112
    },
    {
      "epoch": 0.0013288092422735888,
      "grad_norm": 7899.849618821868,
      "learning_rate": 1.6534415039860602e-07,
      "loss": 1.7042,
      "step": 366144
    },
    {
      "epoch": 0.0013289253766244256,
      "grad_norm": 10725.607115683475,
      "learning_rate": 1.6533691840567085e-07,
      "loss": 1.6981,
      "step": 366176
    },
    {
      "epoch": 0.0013290415109752623,
      "grad_norm": 11209.660833406155,
      "learning_rate": 1.6532968736161379e-07,
      "loss": 1.6545,
      "step": 366208
    },
    {
      "epoch": 0.001329157645326099,
      "grad_norm": 9345.45750618984,
      "learning_rate": 1.653224572662273e-07,
      "loss": 1.6731,
      "step": 366240
    },
    {
      "epoch": 0.0013292737796769358,
      "grad_norm": 9984.717121681515,
      "learning_rate": 1.65315228119304e-07,
      "loss": 1.6811,
      "step": 366272
    },
    {
      "epoch": 0.0013293899140277724,
      "grad_norm": 10530.151755791556,
      "learning_rate": 1.6530799992063657e-07,
      "loss": 1.6835,
      "step": 366304
    },
    {
      "epoch": 0.0013295060483786091,
      "grad_norm": 8824.230504695579,
      "learning_rate": 1.6530077267001767e-07,
      "loss": 1.6724,
      "step": 366336
    },
    {
      "epoch": 0.0013296221827294459,
      "grad_norm": 9740.625031280077,
      "learning_rate": 1.652935463672401e-07,
      "loss": 1.6832,
      "step": 366368
    },
    {
      "epoch": 0.0013297383170802826,
      "grad_norm": 8419.594289513005,
      "learning_rate": 1.652863210120967e-07,
      "loss": 1.6864,
      "step": 366400
    },
    {
      "epoch": 0.0013298544514311192,
      "grad_norm": 9116.091706427706,
      "learning_rate": 1.6527909660438039e-07,
      "loss": 1.6799,
      "step": 366432
    },
    {
      "epoch": 0.001329970585781956,
      "grad_norm": 9808.588889335713,
      "learning_rate": 1.6527187314388407e-07,
      "loss": 1.6849,
      "step": 366464
    },
    {
      "epoch": 0.0013300867201327927,
      "grad_norm": 13997.147137899208,
      "learning_rate": 1.6526465063040082e-07,
      "loss": 1.676,
      "step": 366496
    },
    {
      "epoch": 0.0013302028544836294,
      "grad_norm": 11577.632227705284,
      "learning_rate": 1.6525742906372375e-07,
      "loss": 1.6899,
      "step": 366528
    },
    {
      "epoch": 0.0013303189888344662,
      "grad_norm": 22121.087495871445,
      "learning_rate": 1.6525020844364596e-07,
      "loss": 1.7048,
      "step": 366560
    },
    {
      "epoch": 0.0013304351231853027,
      "grad_norm": 20341.165748304593,
      "learning_rate": 1.652429887699607e-07,
      "loss": 1.7002,
      "step": 366592
    },
    {
      "epoch": 0.0013305512575361395,
      "grad_norm": 11347.848959164023,
      "learning_rate": 1.6523599561337551e-07,
      "loss": 1.7058,
      "step": 366624
    },
    {
      "epoch": 0.0013306673918869762,
      "grad_norm": 8607.560165343022,
      "learning_rate": 1.6522877780229652e-07,
      "loss": 1.7282,
      "step": 366656
    },
    {
      "epoch": 0.001330783526237813,
      "grad_norm": 10998.942494621926,
      "learning_rate": 1.6522156093699652e-07,
      "loss": 1.7149,
      "step": 366688
    },
    {
      "epoch": 0.0013308996605886495,
      "grad_norm": 10938.108611638485,
      "learning_rate": 1.65214345017269e-07,
      "loss": 1.7101,
      "step": 366720
    },
    {
      "epoch": 0.0013310157949394863,
      "grad_norm": 10266.95495266245,
      "learning_rate": 1.6520713004290754e-07,
      "loss": 1.6852,
      "step": 366752
    },
    {
      "epoch": 0.001331131929290323,
      "grad_norm": 9206.09298236771,
      "learning_rate": 1.6519991601370564e-07,
      "loss": 1.662,
      "step": 366784
    },
    {
      "epoch": 0.0013312480636411598,
      "grad_norm": 9346.835400283884,
      "learning_rate": 1.6519270292945705e-07,
      "loss": 1.6721,
      "step": 366816
    },
    {
      "epoch": 0.0013313641979919965,
      "grad_norm": 9192.019364644528,
      "learning_rate": 1.6518549078995546e-07,
      "loss": 1.6895,
      "step": 366848
    },
    {
      "epoch": 0.001331480332342833,
      "grad_norm": 10031.568172524174,
      "learning_rate": 1.651782795949946e-07,
      "loss": 1.6828,
      "step": 366880
    },
    {
      "epoch": 0.0013315964666936698,
      "grad_norm": 9325.769887789425,
      "learning_rate": 1.651710693443684e-07,
      "loss": 1.6807,
      "step": 366912
    },
    {
      "epoch": 0.0013317126010445066,
      "grad_norm": 8518.414171663644,
      "learning_rate": 1.6516386003787077e-07,
      "loss": 1.6941,
      "step": 366944
    },
    {
      "epoch": 0.0013318287353953433,
      "grad_norm": 8784.09335105223,
      "learning_rate": 1.651566516752956e-07,
      "loss": 1.6788,
      "step": 366976
    },
    {
      "epoch": 0.0013319448697461799,
      "grad_norm": 14217.421707187277,
      "learning_rate": 1.65149444256437e-07,
      "loss": 1.6867,
      "step": 367008
    },
    {
      "epoch": 0.0013320610040970166,
      "grad_norm": 8772.481632924631,
      "learning_rate": 1.6514223778108902e-07,
      "loss": 1.6819,
      "step": 367040
    },
    {
      "epoch": 0.0013321771384478534,
      "grad_norm": 9182.007841425535,
      "learning_rate": 1.6513503224904588e-07,
      "loss": 1.659,
      "step": 367072
    },
    {
      "epoch": 0.0013322932727986901,
      "grad_norm": 12659.894944271851,
      "learning_rate": 1.6512782766010178e-07,
      "loss": 1.6738,
      "step": 367104
    },
    {
      "epoch": 0.0013324094071495269,
      "grad_norm": 9502.062723430108,
      "learning_rate": 1.6512062401405098e-07,
      "loss": 1.6824,
      "step": 367136
    },
    {
      "epoch": 0.0013325255415003634,
      "grad_norm": 9796.681070648365,
      "learning_rate": 1.6511342131068787e-07,
      "loss": 1.683,
      "step": 367168
    },
    {
      "epoch": 0.0013326416758512002,
      "grad_norm": 9219.974294975014,
      "learning_rate": 1.6510621954980683e-07,
      "loss": 1.6936,
      "step": 367200
    },
    {
      "epoch": 0.001332757810202037,
      "grad_norm": 11954.15392238196,
      "learning_rate": 1.6509901873120236e-07,
      "loss": 1.7094,
      "step": 367232
    },
    {
      "epoch": 0.0013328739445528737,
      "grad_norm": 9700.190101229975,
      "learning_rate": 1.6509181885466898e-07,
      "loss": 1.7071,
      "step": 367264
    },
    {
      "epoch": 0.0013329900789037102,
      "grad_norm": 8119.759479196413,
      "learning_rate": 1.6508461992000133e-07,
      "loss": 1.694,
      "step": 367296
    },
    {
      "epoch": 0.001333106213254547,
      "grad_norm": 8288.989202550574,
      "learning_rate": 1.6507742192699406e-07,
      "loss": 1.6928,
      "step": 367328
    },
    {
      "epoch": 0.0013332223476053837,
      "grad_norm": 10342.009669305091,
      "learning_rate": 1.6507022487544185e-07,
      "loss": 1.6893,
      "step": 367360
    },
    {
      "epoch": 0.0013333384819562205,
      "grad_norm": 12467.755692184539,
      "learning_rate": 1.6506302876513955e-07,
      "loss": 1.7014,
      "step": 367392
    },
    {
      "epoch": 0.0013334546163070572,
      "grad_norm": 9098.065508667214,
      "learning_rate": 1.6505583359588198e-07,
      "loss": 1.7287,
      "step": 367424
    },
    {
      "epoch": 0.0013335707506578938,
      "grad_norm": 10484.97095847194,
      "learning_rate": 1.6504863936746407e-07,
      "loss": 1.7033,
      "step": 367456
    },
    {
      "epoch": 0.0013336868850087305,
      "grad_norm": 9876.771132308371,
      "learning_rate": 1.650414460796808e-07,
      "loss": 1.7074,
      "step": 367488
    },
    {
      "epoch": 0.0013338030193595673,
      "grad_norm": 8963.393219088404,
      "learning_rate": 1.650342537323272e-07,
      "loss": 1.7246,
      "step": 367520
    },
    {
      "epoch": 0.001333919153710404,
      "grad_norm": 8661.15742842722,
      "learning_rate": 1.6502706232519834e-07,
      "loss": 1.7082,
      "step": 367552
    },
    {
      "epoch": 0.0013340352880612406,
      "grad_norm": 9060.53309689888,
      "learning_rate": 1.6501987185808947e-07,
      "loss": 1.6982,
      "step": 367584
    },
    {
      "epoch": 0.0013341514224120773,
      "grad_norm": 20085.7139280634,
      "learning_rate": 1.6501268233079576e-07,
      "loss": 1.6838,
      "step": 367616
    },
    {
      "epoch": 0.001334267556762914,
      "grad_norm": 9025.949700724019,
      "learning_rate": 1.65005718372257e-07,
      "loss": 1.6811,
      "step": 367648
    },
    {
      "epoch": 0.0013343836911137508,
      "grad_norm": 9840.597542832447,
      "learning_rate": 1.6499853069462623e-07,
      "loss": 1.6908,
      "step": 367680
    },
    {
      "epoch": 0.0013344998254645876,
      "grad_norm": 7887.632851496069,
      "learning_rate": 1.6499134395620305e-07,
      "loss": 1.7003,
      "step": 367712
    },
    {
      "epoch": 0.0013346159598154241,
      "grad_norm": 11311.091194044897,
      "learning_rate": 1.6498415815678297e-07,
      "loss": 1.6969,
      "step": 367744
    },
    {
      "epoch": 0.0013347320941662609,
      "grad_norm": 7868.734205702973,
      "learning_rate": 1.6497697329616157e-07,
      "loss": 1.7177,
      "step": 367776
    },
    {
      "epoch": 0.0013348482285170976,
      "grad_norm": 9039.18868040711,
      "learning_rate": 1.6496978937413435e-07,
      "loss": 1.7303,
      "step": 367808
    },
    {
      "epoch": 0.0013349643628679344,
      "grad_norm": 9714.59047000953,
      "learning_rate": 1.6496260639049703e-07,
      "loss": 1.6969,
      "step": 367840
    },
    {
      "epoch": 0.001335080497218771,
      "grad_norm": 12879.367996916619,
      "learning_rate": 1.6495542434504534e-07,
      "loss": 1.6992,
      "step": 367872
    },
    {
      "epoch": 0.0013351966315696077,
      "grad_norm": 13518.899807306807,
      "learning_rate": 1.6494824323757504e-07,
      "loss": 1.6616,
      "step": 367904
    },
    {
      "epoch": 0.0013353127659204444,
      "grad_norm": 9008.332365093996,
      "learning_rate": 1.64941063067882e-07,
      "loss": 1.6614,
      "step": 367936
    },
    {
      "epoch": 0.0013354289002712812,
      "grad_norm": 9428.767257706599,
      "learning_rate": 1.649338838357621e-07,
      "loss": 1.676,
      "step": 367968
    },
    {
      "epoch": 0.001335545034622118,
      "grad_norm": 8627.12223165987,
      "learning_rate": 1.649267055410114e-07,
      "loss": 1.6808,
      "step": 368000
    },
    {
      "epoch": 0.0013356611689729545,
      "grad_norm": 9124.088557220386,
      "learning_rate": 1.649195281834258e-07,
      "loss": 1.657,
      "step": 368032
    },
    {
      "epoch": 0.0013357773033237912,
      "grad_norm": 9145.782415955455,
      "learning_rate": 1.6491235176280157e-07,
      "loss": 1.6704,
      "step": 368064
    },
    {
      "epoch": 0.001335893437674628,
      "grad_norm": 10740.220668124095,
      "learning_rate": 1.649051762789347e-07,
      "loss": 1.6821,
      "step": 368096
    },
    {
      "epoch": 0.0013360095720254647,
      "grad_norm": 10711.65860172924,
      "learning_rate": 1.6489800173162148e-07,
      "loss": 1.6828,
      "step": 368128
    },
    {
      "epoch": 0.0013361257063763013,
      "grad_norm": 10613.082681294818,
      "learning_rate": 1.6489082812065826e-07,
      "loss": 1.6882,
      "step": 368160
    },
    {
      "epoch": 0.001336241840727138,
      "grad_norm": 8754.941690268415,
      "learning_rate": 1.648836554458413e-07,
      "loss": 1.673,
      "step": 368192
    },
    {
      "epoch": 0.0013363579750779748,
      "grad_norm": 8598.989242928497,
      "learning_rate": 1.64876483706967e-07,
      "loss": 1.6743,
      "step": 368224
    },
    {
      "epoch": 0.0013364741094288115,
      "grad_norm": 8824.261442183137,
      "learning_rate": 1.6486931290383187e-07,
      "loss": 1.6861,
      "step": 368256
    },
    {
      "epoch": 0.0013365902437796483,
      "grad_norm": 9228.758963154254,
      "learning_rate": 1.6486214303623247e-07,
      "loss": 1.7198,
      "step": 368288
    },
    {
      "epoch": 0.0013367063781304848,
      "grad_norm": 8945.667778315938,
      "learning_rate": 1.6485497410396531e-07,
      "loss": 1.7101,
      "step": 368320
    },
    {
      "epoch": 0.0013368225124813216,
      "grad_norm": 9676.264981902883,
      "learning_rate": 1.6484780610682714e-07,
      "loss": 1.7152,
      "step": 368352
    },
    {
      "epoch": 0.0013369386468321583,
      "grad_norm": 9779.671364621616,
      "learning_rate": 1.6484063904461463e-07,
      "loss": 1.7062,
      "step": 368384
    },
    {
      "epoch": 0.001337054781182995,
      "grad_norm": 10365.181522771321,
      "learning_rate": 1.6483347291712455e-07,
      "loss": 1.7072,
      "step": 368416
    },
    {
      "epoch": 0.0013371709155338316,
      "grad_norm": 10464.0806571815,
      "learning_rate": 1.6482630772415372e-07,
      "loss": 1.6797,
      "step": 368448
    },
    {
      "epoch": 0.0013372870498846684,
      "grad_norm": 12582.183594273292,
      "learning_rate": 1.6481914346549914e-07,
      "loss": 1.6713,
      "step": 368480
    },
    {
      "epoch": 0.0013374031842355051,
      "grad_norm": 11329.914033213137,
      "learning_rate": 1.6481198014095765e-07,
      "loss": 1.6777,
      "step": 368512
    },
    {
      "epoch": 0.0013375193185863419,
      "grad_norm": 9389.956123433165,
      "learning_rate": 1.648048177503264e-07,
      "loss": 1.6651,
      "step": 368544
    },
    {
      "epoch": 0.0013376354529371786,
      "grad_norm": 10901.61345856658,
      "learning_rate": 1.6479765629340235e-07,
      "loss": 1.674,
      "step": 368576
    },
    {
      "epoch": 0.0013377515872880152,
      "grad_norm": 9584.756021933996,
      "learning_rate": 1.6479049576998276e-07,
      "loss": 1.6606,
      "step": 368608
    },
    {
      "epoch": 0.001337867721638852,
      "grad_norm": 13664.808231365707,
      "learning_rate": 1.6478333617986477e-07,
      "loss": 1.6826,
      "step": 368640
    },
    {
      "epoch": 0.0013379838559896887,
      "grad_norm": 19361.411105598683,
      "learning_rate": 1.6477617752284572e-07,
      "loss": 1.6995,
      "step": 368672
    },
    {
      "epoch": 0.0013380999903405254,
      "grad_norm": 19936.256418896704,
      "learning_rate": 1.6476901979872293e-07,
      "loss": 1.6933,
      "step": 368704
    },
    {
      "epoch": 0.001338216124691362,
      "grad_norm": 22315.1449916867,
      "learning_rate": 1.6476186300729373e-07,
      "loss": 1.6795,
      "step": 368736
    },
    {
      "epoch": 0.0013383322590421987,
      "grad_norm": 26722.63879185587,
      "learning_rate": 1.6475470714835561e-07,
      "loss": 1.6768,
      "step": 368768
    },
    {
      "epoch": 0.0013384483933930355,
      "grad_norm": 18887.22997159721,
      "learning_rate": 1.6474755222170614e-07,
      "loss": 1.6728,
      "step": 368800
    },
    {
      "epoch": 0.0013385645277438722,
      "grad_norm": 9163.43832848784,
      "learning_rate": 1.647406217753662e-07,
      "loss": 1.6863,
      "step": 368832
    },
    {
      "epoch": 0.001338680662094709,
      "grad_norm": 9843.946972632471,
      "learning_rate": 1.6473346868356848e-07,
      "loss": 1.6949,
      "step": 368864
    },
    {
      "epoch": 0.0013387967964455455,
      "grad_norm": 10079.048566209014,
      "learning_rate": 1.6472631652345855e-07,
      "loss": 1.6685,
      "step": 368896
    },
    {
      "epoch": 0.0013389129307963823,
      "grad_norm": 10290.039455706668,
      "learning_rate": 1.647191652948343e-07,
      "loss": 1.6778,
      "step": 368928
    },
    {
      "epoch": 0.001339029065147219,
      "grad_norm": 8793.924948508487,
      "learning_rate": 1.647120149974935e-07,
      "loss": 1.6863,
      "step": 368960
    },
    {
      "epoch": 0.0013391451994980558,
      "grad_norm": 8719.676943557028,
      "learning_rate": 1.64704865631234e-07,
      "loss": 1.6794,
      "step": 368992
    },
    {
      "epoch": 0.0013392613338488923,
      "grad_norm": 8883.96938310798,
      "learning_rate": 1.646977171958538e-07,
      "loss": 1.6885,
      "step": 369024
    },
    {
      "epoch": 0.001339377468199729,
      "grad_norm": 10778.150676252397,
      "learning_rate": 1.6469056969115096e-07,
      "loss": 1.6703,
      "step": 369056
    },
    {
      "epoch": 0.0013394936025505658,
      "grad_norm": 14559.235556855312,
      "learning_rate": 1.6468342311692341e-07,
      "loss": 1.6706,
      "step": 369088
    },
    {
      "epoch": 0.0013396097369014026,
      "grad_norm": 11991.673944866914,
      "learning_rate": 1.6467627747296942e-07,
      "loss": 1.68,
      "step": 369120
    },
    {
      "epoch": 0.0013397258712522394,
      "grad_norm": 7951.890718564988,
      "learning_rate": 1.646691327590871e-07,
      "loss": 1.6881,
      "step": 369152
    },
    {
      "epoch": 0.0013398420056030759,
      "grad_norm": 10465.947639846092,
      "learning_rate": 1.6466198897507471e-07,
      "loss": 1.6828,
      "step": 369184
    },
    {
      "epoch": 0.0013399581399539126,
      "grad_norm": 10962.3597824556,
      "learning_rate": 1.6465484612073064e-07,
      "loss": 1.7023,
      "step": 369216
    },
    {
      "epoch": 0.0013400742743047494,
      "grad_norm": 8671.107887692322,
      "learning_rate": 1.6464770419585317e-07,
      "loss": 1.7115,
      "step": 369248
    },
    {
      "epoch": 0.0013401904086555862,
      "grad_norm": 11848.930415864548,
      "learning_rate": 1.6464056320024081e-07,
      "loss": 1.7035,
      "step": 369280
    },
    {
      "epoch": 0.0013403065430064227,
      "grad_norm": 9648.103647867803,
      "learning_rate": 1.6463342313369202e-07,
      "loss": 1.6979,
      "step": 369312
    },
    {
      "epoch": 0.0013404226773572594,
      "grad_norm": 12093.760043923477,
      "learning_rate": 1.6462628399600537e-07,
      "loss": 1.6752,
      "step": 369344
    },
    {
      "epoch": 0.0013405388117080962,
      "grad_norm": 10255.532555650145,
      "learning_rate": 1.646191457869795e-07,
      "loss": 1.6951,
      "step": 369376
    },
    {
      "epoch": 0.001340654946058933,
      "grad_norm": 10404.793510685351,
      "learning_rate": 1.646120085064131e-07,
      "loss": 1.6945,
      "step": 369408
    },
    {
      "epoch": 0.0013407710804097697,
      "grad_norm": 12736.54678474507,
      "learning_rate": 1.6460487215410486e-07,
      "loss": 1.6805,
      "step": 369440
    },
    {
      "epoch": 0.0013408872147606062,
      "grad_norm": 10335.534625746266,
      "learning_rate": 1.6459773672985364e-07,
      "loss": 1.6596,
      "step": 369472
    },
    {
      "epoch": 0.001341003349111443,
      "grad_norm": 9200.328907164134,
      "learning_rate": 1.6459060223345827e-07,
      "loss": 1.6814,
      "step": 369504
    },
    {
      "epoch": 0.0013411194834622798,
      "grad_norm": 10681.172969295087,
      "learning_rate": 1.645834686647177e-07,
      "loss": 1.6904,
      "step": 369536
    },
    {
      "epoch": 0.0013412356178131165,
      "grad_norm": 9495.86710100768,
      "learning_rate": 1.6457633602343092e-07,
      "loss": 1.7046,
      "step": 369568
    },
    {
      "epoch": 0.001341351752163953,
      "grad_norm": 10475.760401994692,
      "learning_rate": 1.64569204309397e-07,
      "loss": 1.6837,
      "step": 369600
    },
    {
      "epoch": 0.0013414678865147898,
      "grad_norm": 8415.044741414034,
      "learning_rate": 1.64562073522415e-07,
      "loss": 1.659,
      "step": 369632
    },
    {
      "epoch": 0.0013415840208656265,
      "grad_norm": 13846.149356409529,
      "learning_rate": 1.645549436622841e-07,
      "loss": 1.6652,
      "step": 369664
    },
    {
      "epoch": 0.0013417001552164633,
      "grad_norm": 9531.96034402158,
      "learning_rate": 1.6454781472880357e-07,
      "loss": 1.6741,
      "step": 369696
    },
    {
      "epoch": 0.0013418162895673,
      "grad_norm": 10731.357602838514,
      "learning_rate": 1.6454068672177267e-07,
      "loss": 1.6788,
      "step": 369728
    },
    {
      "epoch": 0.0013419324239181366,
      "grad_norm": 8951.007541053688,
      "learning_rate": 1.6453355964099078e-07,
      "loss": 1.6715,
      "step": 369760
    },
    {
      "epoch": 0.0013420485582689733,
      "grad_norm": 8816.549665260214,
      "learning_rate": 1.645264334862573e-07,
      "loss": 1.6906,
      "step": 369792
    },
    {
      "epoch": 0.00134216469261981,
      "grad_norm": 16618.45335763831,
      "learning_rate": 1.6451930825737167e-07,
      "loss": 1.6938,
      "step": 369824
    },
    {
      "epoch": 0.0013422808269706469,
      "grad_norm": 15185.543915184599,
      "learning_rate": 1.6451218395413353e-07,
      "loss": 1.695,
      "step": 369856
    },
    {
      "epoch": 0.0013423969613214834,
      "grad_norm": 9288.495249500858,
      "learning_rate": 1.6450528316789207e-07,
      "loss": 1.6968,
      "step": 369888
    },
    {
      "epoch": 0.0013425130956723201,
      "grad_norm": 9322.99200900655,
      "learning_rate": 1.6449816068643666e-07,
      "loss": 1.6872,
      "step": 369920
    },
    {
      "epoch": 0.001342629230023157,
      "grad_norm": 9180.664899668214,
      "learning_rate": 1.6449103913003393e-07,
      "loss": 1.6986,
      "step": 369952
    },
    {
      "epoch": 0.0013427453643739937,
      "grad_norm": 9310.741968285878,
      "learning_rate": 1.644839184984836e-07,
      "loss": 1.6915,
      "step": 369984
    },
    {
      "epoch": 0.0013428614987248304,
      "grad_norm": 10021.503679588208,
      "learning_rate": 1.6447679879158554e-07,
      "loss": 1.6807,
      "step": 370016
    },
    {
      "epoch": 0.001342977633075667,
      "grad_norm": 12627.967849183018,
      "learning_rate": 1.6446968000913966e-07,
      "loss": 1.6838,
      "step": 370048
    },
    {
      "epoch": 0.0013430937674265037,
      "grad_norm": 7992.724066299299,
      "learning_rate": 1.644625621509459e-07,
      "loss": 1.7105,
      "step": 370080
    },
    {
      "epoch": 0.0013432099017773405,
      "grad_norm": 8925.394781184752,
      "learning_rate": 1.6445544521680433e-07,
      "loss": 1.7231,
      "step": 370112
    },
    {
      "epoch": 0.0013433260361281772,
      "grad_norm": 9751.517215285014,
      "learning_rate": 1.6444832920651494e-07,
      "loss": 1.7087,
      "step": 370144
    },
    {
      "epoch": 0.0013434421704790137,
      "grad_norm": 8276.935906481334,
      "learning_rate": 1.6444121411987794e-07,
      "loss": 1.701,
      "step": 370176
    },
    {
      "epoch": 0.0013435583048298505,
      "grad_norm": 8267.433338104396,
      "learning_rate": 1.644340999566935e-07,
      "loss": 1.656,
      "step": 370208
    },
    {
      "epoch": 0.0013436744391806873,
      "grad_norm": 8141.47627890667,
      "learning_rate": 1.6442698671676195e-07,
      "loss": 1.6628,
      "step": 370240
    },
    {
      "epoch": 0.001343790573531524,
      "grad_norm": 7848.155706916116,
      "learning_rate": 1.644198743998835e-07,
      "loss": 1.6857,
      "step": 370272
    },
    {
      "epoch": 0.0013439067078823608,
      "grad_norm": 12157.680370860224,
      "learning_rate": 1.644127630058586e-07,
      "loss": 1.6752,
      "step": 370304
    },
    {
      "epoch": 0.0013440228422331973,
      "grad_norm": 11062.31585157466,
      "learning_rate": 1.644056525344877e-07,
      "loss": 1.6648,
      "step": 370336
    },
    {
      "epoch": 0.001344138976584034,
      "grad_norm": 9095.319015845458,
      "learning_rate": 1.643985429855713e-07,
      "loss": 1.6903,
      "step": 370368
    },
    {
      "epoch": 0.0013442551109348708,
      "grad_norm": 9412.981992971196,
      "learning_rate": 1.643914343589099e-07,
      "loss": 1.7025,
      "step": 370400
    },
    {
      "epoch": 0.0013443712452857076,
      "grad_norm": 12174.042713905681,
      "learning_rate": 1.6438432665430422e-07,
      "loss": 1.7035,
      "step": 370432
    },
    {
      "epoch": 0.001344487379636544,
      "grad_norm": 8251.324742124745,
      "learning_rate": 1.643772198715549e-07,
      "loss": 1.705,
      "step": 370464
    },
    {
      "epoch": 0.0013446035139873809,
      "grad_norm": 12088.364157320873,
      "learning_rate": 1.643701140104627e-07,
      "loss": 1.6788,
      "step": 370496
    },
    {
      "epoch": 0.0013447196483382176,
      "grad_norm": 10574.157365955927,
      "learning_rate": 1.6436300907082837e-07,
      "loss": 1.6705,
      "step": 370528
    },
    {
      "epoch": 0.0013448357826890544,
      "grad_norm": 9562.478130693948,
      "learning_rate": 1.6435590505245285e-07,
      "loss": 1.6756,
      "step": 370560
    },
    {
      "epoch": 0.0013449519170398911,
      "grad_norm": 10482.551788567514,
      "learning_rate": 1.6434880195513704e-07,
      "loss": 1.67,
      "step": 370592
    },
    {
      "epoch": 0.0013450680513907277,
      "grad_norm": 10355.116995959052,
      "learning_rate": 1.643416997786819e-07,
      "loss": 1.6734,
      "step": 370624
    },
    {
      "epoch": 0.0013451841857415644,
      "grad_norm": 9215.870007763782,
      "learning_rate": 1.6433459852288852e-07,
      "loss": 1.6948,
      "step": 370656
    },
    {
      "epoch": 0.0013453003200924012,
      "grad_norm": 8665.104500235413,
      "learning_rate": 1.64327498187558e-07,
      "loss": 1.6976,
      "step": 370688
    },
    {
      "epoch": 0.001345416454443238,
      "grad_norm": 9380.261403607044,
      "learning_rate": 1.6432039877249147e-07,
      "loss": 1.688,
      "step": 370720
    },
    {
      "epoch": 0.0013455325887940745,
      "grad_norm": 8168.947790260383,
      "learning_rate": 1.643133002774902e-07,
      "loss": 1.6804,
      "step": 370752
    },
    {
      "epoch": 0.0013456487231449112,
      "grad_norm": 9323.482503871608,
      "learning_rate": 1.643062027023554e-07,
      "loss": 1.6764,
      "step": 370784
    },
    {
      "epoch": 0.001345764857495748,
      "grad_norm": 9877.020400910387,
      "learning_rate": 1.6429910604688854e-07,
      "loss": 1.6764,
      "step": 370816
    },
    {
      "epoch": 0.0013458809918465847,
      "grad_norm": 8643.08856833019,
      "learning_rate": 1.6429201031089093e-07,
      "loss": 1.7012,
      "step": 370848
    },
    {
      "epoch": 0.0013459971261974215,
      "grad_norm": 20385.017243063594,
      "learning_rate": 1.6428491549416405e-07,
      "loss": 1.6952,
      "step": 370880
    },
    {
      "epoch": 0.001346113260548258,
      "grad_norm": 10120.398015888506,
      "learning_rate": 1.6427804326690144e-07,
      "loss": 1.6828,
      "step": 370912
    },
    {
      "epoch": 0.0013462293948990948,
      "grad_norm": 9030.761429691298,
      "learning_rate": 1.6427095025940893e-07,
      "loss": 1.7186,
      "step": 370944
    },
    {
      "epoch": 0.0013463455292499315,
      "grad_norm": 10905.312650263631,
      "learning_rate": 1.642638581705981e-07,
      "loss": 1.7184,
      "step": 370976
    },
    {
      "epoch": 0.0013464616636007683,
      "grad_norm": 9075.839134757733,
      "learning_rate": 1.6425676700027072e-07,
      "loss": 1.7288,
      "step": 371008
    },
    {
      "epoch": 0.0013465777979516048,
      "grad_norm": 10105.477524590315,
      "learning_rate": 1.6424967674822848e-07,
      "loss": 1.7148,
      "step": 371040
    },
    {
      "epoch": 0.0013466939323024416,
      "grad_norm": 12586.028762083773,
      "learning_rate": 1.642425874142733e-07,
      "loss": 1.6698,
      "step": 371072
    },
    {
      "epoch": 0.0013468100666532783,
      "grad_norm": 7867.186536494479,
      "learning_rate": 1.6423549899820694e-07,
      "loss": 1.666,
      "step": 371104
    },
    {
      "epoch": 0.001346926201004115,
      "grad_norm": 13612.148250735443,
      "learning_rate": 1.6422841149983148e-07,
      "loss": 1.6804,
      "step": 371136
    },
    {
      "epoch": 0.0013470423353549518,
      "grad_norm": 10009.199968029414,
      "learning_rate": 1.642213249189488e-07,
      "loss": 1.6678,
      "step": 371168
    },
    {
      "epoch": 0.0013471584697057884,
      "grad_norm": 9238.979164388238,
      "learning_rate": 1.6421423925536107e-07,
      "loss": 1.6875,
      "step": 371200
    },
    {
      "epoch": 0.0013472746040566251,
      "grad_norm": 9174.955585723563,
      "learning_rate": 1.6420715450887035e-07,
      "loss": 1.6854,
      "step": 371232
    },
    {
      "epoch": 0.0013473907384074619,
      "grad_norm": 10493.813606120513,
      "learning_rate": 1.6420007067927883e-07,
      "loss": 1.6857,
      "step": 371264
    },
    {
      "epoch": 0.0013475068727582986,
      "grad_norm": 9094.831279358623,
      "learning_rate": 1.641929877663888e-07,
      "loss": 1.6926,
      "step": 371296
    },
    {
      "epoch": 0.0013476230071091352,
      "grad_norm": 8265.555516721184,
      "learning_rate": 1.641859057700025e-07,
      "loss": 1.6875,
      "step": 371328
    },
    {
      "epoch": 0.001347739141459972,
      "grad_norm": 13204.206299509258,
      "learning_rate": 1.6417882468992234e-07,
      "loss": 1.6609,
      "step": 371360
    },
    {
      "epoch": 0.0013478552758108087,
      "grad_norm": 8457.167374481838,
      "learning_rate": 1.641717445259507e-07,
      "loss": 1.6603,
      "step": 371392
    },
    {
      "epoch": 0.0013479714101616454,
      "grad_norm": 9288.38414364953,
      "learning_rate": 1.6416466527789009e-07,
      "loss": 1.683,
      "step": 371424
    },
    {
      "epoch": 0.0013480875445124822,
      "grad_norm": 10981.431964912408,
      "learning_rate": 1.6415758694554308e-07,
      "loss": 1.6764,
      "step": 371456
    },
    {
      "epoch": 0.0013482036788633187,
      "grad_norm": 9969.255940139165,
      "learning_rate": 1.641505095287122e-07,
      "loss": 1.6774,
      "step": 371488
    },
    {
      "epoch": 0.0013483198132141555,
      "grad_norm": 8140.284147374709,
      "learning_rate": 1.641434330272002e-07,
      "loss": 1.6959,
      "step": 371520
    },
    {
      "epoch": 0.0013484359475649922,
      "grad_norm": 11281.947349637827,
      "learning_rate": 1.6413635744080972e-07,
      "loss": 1.7089,
      "step": 371552
    },
    {
      "epoch": 0.001348552081915829,
      "grad_norm": 8057.709351918819,
      "learning_rate": 1.641292827693436e-07,
      "loss": 1.7119,
      "step": 371584
    },
    {
      "epoch": 0.0013486682162666655,
      "grad_norm": 9014.785410646222,
      "learning_rate": 1.6412220901260463e-07,
      "loss": 1.6839,
      "step": 371616
    },
    {
      "epoch": 0.0013487843506175023,
      "grad_norm": 13384.4971515556,
      "learning_rate": 1.6411513617039577e-07,
      "loss": 1.6732,
      "step": 371648
    },
    {
      "epoch": 0.001348900484968339,
      "grad_norm": 11333.430195664507,
      "learning_rate": 1.641080642425199e-07,
      "loss": 1.6833,
      "step": 371680
    },
    {
      "epoch": 0.0013490166193191758,
      "grad_norm": 10468.880742467172,
      "learning_rate": 1.6410099322878007e-07,
      "loss": 1.69,
      "step": 371712
    },
    {
      "epoch": 0.0013491327536700125,
      "grad_norm": 9261.996760958189,
      "learning_rate": 1.640939231289794e-07,
      "loss": 1.684,
      "step": 371744
    },
    {
      "epoch": 0.001349248888020849,
      "grad_norm": 9513.50366584257,
      "learning_rate": 1.6408685394292097e-07,
      "loss": 1.6917,
      "step": 371776
    },
    {
      "epoch": 0.0013493650223716858,
      "grad_norm": 10581.820448297165,
      "learning_rate": 1.6407978567040803e-07,
      "loss": 1.7034,
      "step": 371808
    },
    {
      "epoch": 0.0013494811567225226,
      "grad_norm": 10710.40223334306,
      "learning_rate": 1.640727183112438e-07,
      "loss": 1.705,
      "step": 371840
    },
    {
      "epoch": 0.0013495972910733593,
      "grad_norm": 10986.111049866555,
      "learning_rate": 1.6406565186523157e-07,
      "loss": 1.703,
      "step": 371872
    },
    {
      "epoch": 0.0013497134254241959,
      "grad_norm": 10091.744943269226,
      "learning_rate": 1.640588071162656e-07,
      "loss": 1.697,
      "step": 371904
    },
    {
      "epoch": 0.0013498295597750326,
      "grad_norm": 11208.22233897954,
      "learning_rate": 1.640517424674469e-07,
      "loss": 1.6846,
      "step": 371936
    },
    {
      "epoch": 0.0013499456941258694,
      "grad_norm": 9304.91225106395,
      "learning_rate": 1.6404467873119667e-07,
      "loss": 1.6723,
      "step": 371968
    },
    {
      "epoch": 0.0013500618284767061,
      "grad_norm": 9207.682010147832,
      "learning_rate": 1.6403761590731846e-07,
      "loss": 1.6825,
      "step": 372000
    },
    {
      "epoch": 0.0013501779628275429,
      "grad_norm": 13147.104319963388,
      "learning_rate": 1.6403055399561588e-07,
      "loss": 1.6754,
      "step": 372032
    },
    {
      "epoch": 0.0013502940971783794,
      "grad_norm": 9720.960034893673,
      "learning_rate": 1.640234929958926e-07,
      "loss": 1.6871,
      "step": 372064
    },
    {
      "epoch": 0.0013504102315292162,
      "grad_norm": 8895.335631666745,
      "learning_rate": 1.6401643290795236e-07,
      "loss": 1.6993,
      "step": 372096
    },
    {
      "epoch": 0.001350526365880053,
      "grad_norm": 10000.427890845473,
      "learning_rate": 1.6400937373159893e-07,
      "loss": 1.7063,
      "step": 372128
    },
    {
      "epoch": 0.0013506425002308897,
      "grad_norm": 10807.805512683875,
      "learning_rate": 1.6400231546663613e-07,
      "loss": 1.6957,
      "step": 372160
    },
    {
      "epoch": 0.0013507586345817262,
      "grad_norm": 10286.00038887808,
      "learning_rate": 1.6399525811286795e-07,
      "loss": 1.6904,
      "step": 372192
    },
    {
      "epoch": 0.001350874768932563,
      "grad_norm": 11191.687450961093,
      "learning_rate": 1.6398820167009826e-07,
      "loss": 1.6737,
      "step": 372224
    },
    {
      "epoch": 0.0013509909032833997,
      "grad_norm": 9940.902977094183,
      "learning_rate": 1.6398114613813113e-07,
      "loss": 1.6798,
      "step": 372256
    },
    {
      "epoch": 0.0013511070376342365,
      "grad_norm": 8520.871082230971,
      "learning_rate": 1.6397409151677064e-07,
      "loss": 1.7118,
      "step": 372288
    },
    {
      "epoch": 0.0013512231719850732,
      "grad_norm": 9497.92124625173,
      "learning_rate": 1.639670378058209e-07,
      "loss": 1.6742,
      "step": 372320
    },
    {
      "epoch": 0.0013513393063359098,
      "grad_norm": 12189.915094043929,
      "learning_rate": 1.639599850050862e-07,
      "loss": 1.6874,
      "step": 372352
    },
    {
      "epoch": 0.0013514554406867465,
      "grad_norm": 8153.744783840121,
      "learning_rate": 1.6395293311437066e-07,
      "loss": 1.6965,
      "step": 372384
    },
    {
      "epoch": 0.0013515715750375833,
      "grad_norm": 9991.90852640275,
      "learning_rate": 1.639458821334787e-07,
      "loss": 1.6945,
      "step": 372416
    },
    {
      "epoch": 0.00135168770938842,
      "grad_norm": 11326.484008729274,
      "learning_rate": 1.6393883206221467e-07,
      "loss": 1.6993,
      "step": 372448
    },
    {
      "epoch": 0.0013518038437392566,
      "grad_norm": 9617.149681688437,
      "learning_rate": 1.6393178290038302e-07,
      "loss": 1.6813,
      "step": 372480
    },
    {
      "epoch": 0.0013519199780900933,
      "grad_norm": 9312.305299978088,
      "learning_rate": 1.6392473464778824e-07,
      "loss": 1.6807,
      "step": 372512
    },
    {
      "epoch": 0.00135203611244093,
      "grad_norm": 12093.753263565452,
      "learning_rate": 1.6391768730423483e-07,
      "loss": 1.6922,
      "step": 372544
    },
    {
      "epoch": 0.0013521522467917668,
      "grad_norm": 9489.279635462326,
      "learning_rate": 1.6391064086952748e-07,
      "loss": 1.7062,
      "step": 372576
    },
    {
      "epoch": 0.0013522683811426036,
      "grad_norm": 9530.80919964302,
      "learning_rate": 1.6390359534347082e-07,
      "loss": 1.6973,
      "step": 372608
    },
    {
      "epoch": 0.0013523845154934401,
      "grad_norm": 9487.735451623848,
      "learning_rate": 1.6389655072586958e-07,
      "loss": 1.7223,
      "step": 372640
    },
    {
      "epoch": 0.0013525006498442769,
      "grad_norm": 8985.519795760287,
      "learning_rate": 1.6388950701652858e-07,
      "loss": 1.7345,
      "step": 372672
    },
    {
      "epoch": 0.0013526167841951136,
      "grad_norm": 8655.241648850713,
      "learning_rate": 1.638824642152526e-07,
      "loss": 1.7162,
      "step": 372704
    },
    {
      "epoch": 0.0013527329185459504,
      "grad_norm": 10073.567987560316,
      "learning_rate": 1.638754223218466e-07,
      "loss": 1.7204,
      "step": 372736
    },
    {
      "epoch": 0.001352849052896787,
      "grad_norm": 10622.687607192447,
      "learning_rate": 1.6386838133611554e-07,
      "loss": 1.7092,
      "step": 372768
    },
    {
      "epoch": 0.0013529651872476237,
      "grad_norm": 9766.786882081538,
      "learning_rate": 1.6386134125786444e-07,
      "loss": 1.7029,
      "step": 372800
    },
    {
      "epoch": 0.0013530813215984604,
      "grad_norm": 7944.32061286552,
      "learning_rate": 1.638543020868984e-07,
      "loss": 1.6738,
      "step": 372832
    },
    {
      "epoch": 0.0013531974559492972,
      "grad_norm": 10423.46717747986,
      "learning_rate": 1.638472638230225e-07,
      "loss": 1.6711,
      "step": 372864
    },
    {
      "epoch": 0.001353313590300134,
      "grad_norm": 10062.149273390849,
      "learning_rate": 1.6384022646604202e-07,
      "loss": 1.6647,
      "step": 372896
    },
    {
      "epoch": 0.0013534297246509705,
      "grad_norm": 7857.270900255381,
      "learning_rate": 1.6383340989111086e-07,
      "loss": 1.6706,
      "step": 372928
    },
    {
      "epoch": 0.0013535458590018072,
      "grad_norm": 9290.213991076847,
      "learning_rate": 1.6382637431901157e-07,
      "loss": 1.6764,
      "step": 372960
    },
    {
      "epoch": 0.001353661993352644,
      "grad_norm": 11707.066754742624,
      "learning_rate": 1.638193396532297e-07,
      "loss": 1.6744,
      "step": 372992
    },
    {
      "epoch": 0.0013537781277034807,
      "grad_norm": 11984.251499363654,
      "learning_rate": 1.6381230589357064e-07,
      "loss": 1.6851,
      "step": 373024
    },
    {
      "epoch": 0.0013538942620543173,
      "grad_norm": 9276.927508609733,
      "learning_rate": 1.6380527303983997e-07,
      "loss": 1.6792,
      "step": 373056
    },
    {
      "epoch": 0.001354010396405154,
      "grad_norm": 10132.704673481805,
      "learning_rate": 1.6379824109184314e-07,
      "loss": 1.6798,
      "step": 373088
    },
    {
      "epoch": 0.0013541265307559908,
      "grad_norm": 8126.142135109378,
      "learning_rate": 1.637912100493858e-07,
      "loss": 1.6727,
      "step": 373120
    },
    {
      "epoch": 0.0013542426651068275,
      "grad_norm": 9463.093468839881,
      "learning_rate": 1.637841799122736e-07,
      "loss": 1.69,
      "step": 373152
    },
    {
      "epoch": 0.0013543587994576643,
      "grad_norm": 8686.449447271307,
      "learning_rate": 1.6377715068031229e-07,
      "loss": 1.6756,
      "step": 373184
    },
    {
      "epoch": 0.0013544749338085008,
      "grad_norm": 10054.573884556223,
      "learning_rate": 1.6377012235330762e-07,
      "loss": 1.6996,
      "step": 373216
    },
    {
      "epoch": 0.0013545910681593376,
      "grad_norm": 9641.743825678008,
      "learning_rate": 1.6376309493106542e-07,
      "loss": 1.6861,
      "step": 373248
    },
    {
      "epoch": 0.0013547072025101743,
      "grad_norm": 9839.309325354092,
      "learning_rate": 1.6375606841339169e-07,
      "loss": 1.689,
      "step": 373280
    },
    {
      "epoch": 0.001354823336861011,
      "grad_norm": 11667.46142054903,
      "learning_rate": 1.6374904280009225e-07,
      "loss": 1.6855,
      "step": 373312
    },
    {
      "epoch": 0.0013549394712118476,
      "grad_norm": 9851.889158937996,
      "learning_rate": 1.637420180909732e-07,
      "loss": 1.6682,
      "step": 373344
    },
    {
      "epoch": 0.0013550556055626844,
      "grad_norm": 9000.815296405099,
      "learning_rate": 1.6373499428584057e-07,
      "loss": 1.675,
      "step": 373376
    },
    {
      "epoch": 0.0013551717399135211,
      "grad_norm": 8425.157209215742,
      "learning_rate": 1.6372797138450054e-07,
      "loss": 1.6687,
      "step": 373408
    },
    {
      "epoch": 0.001355287874264358,
      "grad_norm": 12967.758480169192,
      "learning_rate": 1.6372094938675925e-07,
      "loss": 1.6819,
      "step": 373440
    },
    {
      "epoch": 0.0013554040086151947,
      "grad_norm": 11115.813060680715,
      "learning_rate": 1.6371392829242297e-07,
      "loss": 1.6597,
      "step": 373472
    },
    {
      "epoch": 0.0013555201429660312,
      "grad_norm": 8262.507730707428,
      "learning_rate": 1.6370690810129803e-07,
      "loss": 1.6822,
      "step": 373504
    },
    {
      "epoch": 0.001355636277316868,
      "grad_norm": 13954.27647712342,
      "learning_rate": 1.6369988881319073e-07,
      "loss": 1.6974,
      "step": 373536
    },
    {
      "epoch": 0.0013557524116677047,
      "grad_norm": 9553.479156830772,
      "learning_rate": 1.6369287042790756e-07,
      "loss": 1.7048,
      "step": 373568
    },
    {
      "epoch": 0.0013558685460185415,
      "grad_norm": 9892.55912289636,
      "learning_rate": 1.6368585294525496e-07,
      "loss": 1.7112,
      "step": 373600
    },
    {
      "epoch": 0.001355984680369378,
      "grad_norm": 9394.738527495057,
      "learning_rate": 1.6367883636503947e-07,
      "loss": 1.6895,
      "step": 373632
    },
    {
      "epoch": 0.0013561008147202147,
      "grad_norm": 9268.542927558787,
      "learning_rate": 1.6367182068706772e-07,
      "loss": 1.6951,
      "step": 373664
    },
    {
      "epoch": 0.0013562169490710515,
      "grad_norm": 9307.904167963914,
      "learning_rate": 1.6366480591114634e-07,
      "loss": 1.6973,
      "step": 373696
    },
    {
      "epoch": 0.0013563330834218883,
      "grad_norm": 8395.998332539139,
      "learning_rate": 1.6365779203708203e-07,
      "loss": 1.6926,
      "step": 373728
    },
    {
      "epoch": 0.001356449217772725,
      "grad_norm": 9180.609892594282,
      "learning_rate": 1.636507790646816e-07,
      "loss": 1.68,
      "step": 373760
    },
    {
      "epoch": 0.0013565653521235615,
      "grad_norm": 10358.611296887242,
      "learning_rate": 1.6364376699375182e-07,
      "loss": 1.6784,
      "step": 373792
    },
    {
      "epoch": 0.0013566814864743983,
      "grad_norm": 10268.210554911699,
      "learning_rate": 1.6363675582409965e-07,
      "loss": 1.6846,
      "step": 373824
    },
    {
      "epoch": 0.001356797620825235,
      "grad_norm": 9438.689103895731,
      "learning_rate": 1.6362974555553198e-07,
      "loss": 1.673,
      "step": 373856
    },
    {
      "epoch": 0.0013569137551760718,
      "grad_norm": 11191.438870851236,
      "learning_rate": 1.6362273618785582e-07,
      "loss": 1.6891,
      "step": 373888
    },
    {
      "epoch": 0.0013570298895269083,
      "grad_norm": 19123.607191113293,
      "learning_rate": 1.6361572772087823e-07,
      "loss": 1.6773,
      "step": 373920
    },
    {
      "epoch": 0.001357146023877745,
      "grad_norm": 22189.82721879555,
      "learning_rate": 1.6360872015440633e-07,
      "loss": 1.6774,
      "step": 373952
    },
    {
      "epoch": 0.0013572621582285819,
      "grad_norm": 25778.475362208683,
      "learning_rate": 1.6360171348824732e-07,
      "loss": 1.6655,
      "step": 373984
    },
    {
      "epoch": 0.0013573782925794186,
      "grad_norm": 18677.700072546406,
      "learning_rate": 1.6359470772220843e-07,
      "loss": 1.6676,
      "step": 374016
    },
    {
      "epoch": 0.0013574944269302554,
      "grad_norm": 32422.176854739413,
      "learning_rate": 1.6358770285609691e-07,
      "loss": 1.6614,
      "step": 374048
    },
    {
      "epoch": 0.001357610561281092,
      "grad_norm": 9162.049006636016,
      "learning_rate": 1.6358091775005228e-07,
      "loss": 1.6788,
      "step": 374080
    },
    {
      "epoch": 0.0013577266956319287,
      "grad_norm": 12275.86282099959,
      "learning_rate": 1.635739146551099e-07,
      "loss": 1.6755,
      "step": 374112
    },
    {
      "epoch": 0.0013578428299827654,
      "grad_norm": 8386.34211083712,
      "learning_rate": 1.6356691245952315e-07,
      "loss": 1.6844,
      "step": 374144
    },
    {
      "epoch": 0.0013579589643336022,
      "grad_norm": 7877.7740510882895,
      "learning_rate": 1.6355991116309954e-07,
      "loss": 1.686,
      "step": 374176
    },
    {
      "epoch": 0.0013580750986844387,
      "grad_norm": 8108.049827177926,
      "learning_rate": 1.6355291076564666e-07,
      "loss": 1.671,
      "step": 374208
    },
    {
      "epoch": 0.0013581912330352755,
      "grad_norm": 9073.886598365663,
      "learning_rate": 1.6354591126697215e-07,
      "loss": 1.6839,
      "step": 374240
    },
    {
      "epoch": 0.0013583073673861122,
      "grad_norm": 9459.97674415746,
      "learning_rate": 1.6353891266688372e-07,
      "loss": 1.7014,
      "step": 374272
    },
    {
      "epoch": 0.001358423501736949,
      "grad_norm": 11078.670678380144,
      "learning_rate": 1.6353191496518908e-07,
      "loss": 1.6959,
      "step": 374304
    },
    {
      "epoch": 0.0013585396360877857,
      "grad_norm": 11455.778978314831,
      "learning_rate": 1.6352491816169607e-07,
      "loss": 1.666,
      "step": 374336
    },
    {
      "epoch": 0.0013586557704386223,
      "grad_norm": 9830.872392621115,
      "learning_rate": 1.6351792225621254e-07,
      "loss": 1.6872,
      "step": 374368
    },
    {
      "epoch": 0.001358771904789459,
      "grad_norm": 10609.357567732364,
      "learning_rate": 1.6351092724854643e-07,
      "loss": 1.6969,
      "step": 374400
    },
    {
      "epoch": 0.0013588880391402958,
      "grad_norm": 8426.05388067273,
      "learning_rate": 1.635039331385057e-07,
      "loss": 1.7003,
      "step": 374432
    },
    {
      "epoch": 0.0013590041734911325,
      "grad_norm": 15881.684797275131,
      "learning_rate": 1.634969399258984e-07,
      "loss": 1.709,
      "step": 374464
    },
    {
      "epoch": 0.001359120307841969,
      "grad_norm": 11857.859840628915,
      "learning_rate": 1.6348994761053265e-07,
      "loss": 1.6804,
      "step": 374496
    },
    {
      "epoch": 0.0013592364421928058,
      "grad_norm": 9661.07902876278,
      "learning_rate": 1.6348295619221653e-07,
      "loss": 1.6895,
      "step": 374528
    },
    {
      "epoch": 0.0013593525765436426,
      "grad_norm": 10509.98078019175,
      "learning_rate": 1.6347596567075836e-07,
      "loss": 1.6975,
      "step": 374560
    },
    {
      "epoch": 0.0013594687108944793,
      "grad_norm": 9715.983532303871,
      "learning_rate": 1.6346897604596632e-07,
      "loss": 1.668,
      "step": 374592
    },
    {
      "epoch": 0.001359584845245316,
      "grad_norm": 10152.076634856536,
      "learning_rate": 1.6346198731764875e-07,
      "loss": 1.6649,
      "step": 374624
    },
    {
      "epoch": 0.0013597009795961526,
      "grad_norm": 10706.040911560165,
      "learning_rate": 1.6345499948561407e-07,
      "loss": 1.6857,
      "step": 374656
    },
    {
      "epoch": 0.0013598171139469894,
      "grad_norm": 7614.094693395926,
      "learning_rate": 1.6344801254967071e-07,
      "loss": 1.6903,
      "step": 374688
    },
    {
      "epoch": 0.0013599332482978261,
      "grad_norm": 9039.90364992902,
      "learning_rate": 1.6344102650962712e-07,
      "loss": 1.6895,
      "step": 374720
    },
    {
      "epoch": 0.0013600493826486629,
      "grad_norm": 10009.268105111383,
      "learning_rate": 1.6343404136529193e-07,
      "loss": 1.6904,
      "step": 374752
    },
    {
      "epoch": 0.0013601655169994994,
      "grad_norm": 10459.002055645655,
      "learning_rate": 1.634270571164737e-07,
      "loss": 1.6916,
      "step": 374784
    },
    {
      "epoch": 0.0013602816513503362,
      "grad_norm": 14493.668410723352,
      "learning_rate": 1.634200737629811e-07,
      "loss": 1.7062,
      "step": 374816
    },
    {
      "epoch": 0.001360397785701173,
      "grad_norm": 10379.067395484048,
      "learning_rate": 1.6341309130462287e-07,
      "loss": 1.6978,
      "step": 374848
    },
    {
      "epoch": 0.0013605139200520097,
      "grad_norm": 8074.086078312517,
      "learning_rate": 1.6340610974120783e-07,
      "loss": 1.6705,
      "step": 374880
    },
    {
      "epoch": 0.0013606300544028464,
      "grad_norm": 10146.831229502144,
      "learning_rate": 1.6339912907254478e-07,
      "loss": 1.6698,
      "step": 374912
    },
    {
      "epoch": 0.001360746188753683,
      "grad_norm": 10737.890295584137,
      "learning_rate": 1.633921492984426e-07,
      "loss": 1.6802,
      "step": 374944
    },
    {
      "epoch": 0.0013608623231045197,
      "grad_norm": 10791.527602707598,
      "learning_rate": 1.6338517041871026e-07,
      "loss": 1.689,
      "step": 374976
    },
    {
      "epoch": 0.0013609784574553565,
      "grad_norm": 8952.02787082346,
      "learning_rate": 1.6337819243315682e-07,
      "loss": 1.6937,
      "step": 375008
    },
    {
      "epoch": 0.0013610945918061932,
      "grad_norm": 8986.330285494741,
      "learning_rate": 1.633712153415913e-07,
      "loss": 1.678,
      "step": 375040
    },
    {
      "epoch": 0.0013612107261570298,
      "grad_norm": 21450.47654482296,
      "learning_rate": 1.6336423914382283e-07,
      "loss": 1.6652,
      "step": 375072
    },
    {
      "epoch": 0.0013613268605078665,
      "grad_norm": 9799.83346797281,
      "learning_rate": 1.6335748180439132e-07,
      "loss": 1.6691,
      "step": 375104
    },
    {
      "epoch": 0.0013614429948587033,
      "grad_norm": 9852.836139914232,
      "learning_rate": 1.633505073657282e-07,
      "loss": 1.6788,
      "step": 375136
    },
    {
      "epoch": 0.00136155912920954,
      "grad_norm": 9280.26454364314,
      "learning_rate": 1.6334353382029589e-07,
      "loss": 1.6678,
      "step": 375168
    },
    {
      "epoch": 0.0013616752635603768,
      "grad_norm": 9701.217964771227,
      "learning_rate": 1.6333656116790363e-07,
      "loss": 1.6749,
      "step": 375200
    },
    {
      "epoch": 0.0013617913979112133,
      "grad_norm": 8517.062169551189,
      "learning_rate": 1.633295894083609e-07,
      "loss": 1.6982,
      "step": 375232
    },
    {
      "epoch": 0.00136190753226205,
      "grad_norm": 11650.133561466151,
      "learning_rate": 1.6332261854147712e-07,
      "loss": 1.6958,
      "step": 375264
    },
    {
      "epoch": 0.0013620236666128868,
      "grad_norm": 10127.875986602521,
      "learning_rate": 1.6331564856706182e-07,
      "loss": 1.7057,
      "step": 375296
    },
    {
      "epoch": 0.0013621398009637236,
      "grad_norm": 9471.141641850785,
      "learning_rate": 1.633086794849246e-07,
      "loss": 1.7146,
      "step": 375328
    },
    {
      "epoch": 0.0013622559353145601,
      "grad_norm": 9937.947675450903,
      "learning_rate": 1.6330171129487514e-07,
      "loss": 1.7057,
      "step": 375360
    },
    {
      "epoch": 0.0013623720696653969,
      "grad_norm": 9577.847774944014,
      "learning_rate": 1.6329474399672305e-07,
      "loss": 1.6994,
      "step": 375392
    },
    {
      "epoch": 0.0013624882040162336,
      "grad_norm": 11556.891104445001,
      "learning_rate": 1.632877775902781e-07,
      "loss": 1.6984,
      "step": 375424
    },
    {
      "epoch": 0.0013626043383670704,
      "grad_norm": 9412.123458603803,
      "learning_rate": 1.6328081207535017e-07,
      "loss": 1.6742,
      "step": 375456
    },
    {
      "epoch": 0.0013627204727179071,
      "grad_norm": 9682.355498534434,
      "learning_rate": 1.6327384745174905e-07,
      "loss": 1.6656,
      "step": 375488
    },
    {
      "epoch": 0.0013628366070687437,
      "grad_norm": 8725.415978622452,
      "learning_rate": 1.6326688371928468e-07,
      "loss": 1.6873,
      "step": 375520
    },
    {
      "epoch": 0.0013629527414195804,
      "grad_norm": 9660.55733381879,
      "learning_rate": 1.6325992087776703e-07,
      "loss": 1.6974,
      "step": 375552
    },
    {
      "epoch": 0.0013630688757704172,
      "grad_norm": 9605.01327432711,
      "learning_rate": 1.632529589270062e-07,
      "loss": 1.6855,
      "step": 375584
    },
    {
      "epoch": 0.001363185010121254,
      "grad_norm": 9926.797469476245,
      "learning_rate": 1.6324599786681217e-07,
      "loss": 1.6735,
      "step": 375616
    },
    {
      "epoch": 0.0013633011444720905,
      "grad_norm": 8099.1168654366265,
      "learning_rate": 1.6323903769699517e-07,
      "loss": 1.6703,
      "step": 375648
    },
    {
      "epoch": 0.0013634172788229272,
      "grad_norm": 9125.444646700784,
      "learning_rate": 1.6323207841736538e-07,
      "loss": 1.6881,
      "step": 375680
    },
    {
      "epoch": 0.001363533413173764,
      "grad_norm": 9658.531151267256,
      "learning_rate": 1.6322512002773307e-07,
      "loss": 1.7048,
      "step": 375712
    },
    {
      "epoch": 0.0013636495475246007,
      "grad_norm": 10745.508270900917,
      "learning_rate": 1.6321816252790855e-07,
      "loss": 1.6704,
      "step": 375744
    },
    {
      "epoch": 0.0013637656818754375,
      "grad_norm": 10191.483307154067,
      "learning_rate": 1.632112059177022e-07,
      "loss": 1.6842,
      "step": 375776
    },
    {
      "epoch": 0.001363881816226274,
      "grad_norm": 11033.514943117629,
      "learning_rate": 1.6320425019692443e-07,
      "loss": 1.7082,
      "step": 375808
    },
    {
      "epoch": 0.0013639979505771108,
      "grad_norm": 10153.441780992296,
      "learning_rate": 1.6319729536538578e-07,
      "loss": 1.6828,
      "step": 375840
    },
    {
      "epoch": 0.0013641140849279475,
      "grad_norm": 11660.26157511057,
      "learning_rate": 1.6319034142289675e-07,
      "loss": 1.6993,
      "step": 375872
    },
    {
      "epoch": 0.0013642302192787843,
      "grad_norm": 12463.851571645098,
      "learning_rate": 1.6318338836926797e-07,
      "loss": 1.6927,
      "step": 375904
    },
    {
      "epoch": 0.0013643463536296208,
      "grad_norm": 10191.313359915885,
      "learning_rate": 1.6317643620431006e-07,
      "loss": 1.6787,
      "step": 375936
    },
    {
      "epoch": 0.0013644624879804576,
      "grad_norm": 9096.041006943626,
      "learning_rate": 1.631694849278338e-07,
      "loss": 1.6675,
      "step": 375968
    },
    {
      "epoch": 0.0013645786223312943,
      "grad_norm": 10502.286417728285,
      "learning_rate": 1.6316253453964987e-07,
      "loss": 1.6807,
      "step": 376000
    },
    {
      "epoch": 0.001364694756682131,
      "grad_norm": 9605.554226592028,
      "learning_rate": 1.6315558503956916e-07,
      "loss": 1.6624,
      "step": 376032
    },
    {
      "epoch": 0.0013648108910329678,
      "grad_norm": 11004.763332302971,
      "learning_rate": 1.6314863642740253e-07,
      "loss": 1.6749,
      "step": 376064
    },
    {
      "epoch": 0.0013649270253838044,
      "grad_norm": 16390.17046891215,
      "learning_rate": 1.6314168870296095e-07,
      "loss": 1.6929,
      "step": 376096
    },
    {
      "epoch": 0.0013650431597346411,
      "grad_norm": 10559.007907942867,
      "learning_rate": 1.6313495894127618e-07,
      "loss": 1.6958,
      "step": 376128
    },
    {
      "epoch": 0.0013651592940854779,
      "grad_norm": 10528.846850439035,
      "learning_rate": 1.6312801296399096e-07,
      "loss": 1.6914,
      "step": 376160
    },
    {
      "epoch": 0.0013652754284363146,
      "grad_norm": 10133.201665811255,
      "learning_rate": 1.631210678738698e-07,
      "loss": 1.6911,
      "step": 376192
    },
    {
      "epoch": 0.0013653915627871512,
      "grad_norm": 10893.519357856763,
      "learning_rate": 1.6311412367072392e-07,
      "loss": 1.6851,
      "step": 376224
    },
    {
      "epoch": 0.001365507697137988,
      "grad_norm": 9409.367247588969,
      "learning_rate": 1.631071803543645e-07,
      "loss": 1.6937,
      "step": 376256
    },
    {
      "epoch": 0.0013656238314888247,
      "grad_norm": 9391.881600616567,
      "learning_rate": 1.6310023792460283e-07,
      "loss": 1.703,
      "step": 376288
    },
    {
      "epoch": 0.0013657399658396614,
      "grad_norm": 9029.65425694694,
      "learning_rate": 1.6309329638125022e-07,
      "loss": 1.6797,
      "step": 376320
    },
    {
      "epoch": 0.0013658561001904982,
      "grad_norm": 9692.876765955503,
      "learning_rate": 1.630863557241181e-07,
      "loss": 1.6788,
      "step": 376352
    },
    {
      "epoch": 0.0013659722345413347,
      "grad_norm": 11092.522887062258,
      "learning_rate": 1.630794159530179e-07,
      "loss": 1.6948,
      "step": 376384
    },
    {
      "epoch": 0.0013660883688921715,
      "grad_norm": 8660.550328934069,
      "learning_rate": 1.6307247706776105e-07,
      "loss": 1.7049,
      "step": 376416
    },
    {
      "epoch": 0.0013662045032430082,
      "grad_norm": 9137.075899870812,
      "learning_rate": 1.6306553906815921e-07,
      "loss": 1.7008,
      "step": 376448
    },
    {
      "epoch": 0.001366320637593845,
      "grad_norm": 9027.210643382594,
      "learning_rate": 1.6305860195402397e-07,
      "loss": 1.679,
      "step": 376480
    },
    {
      "epoch": 0.0013664367719446815,
      "grad_norm": 9735.831346115236,
      "learning_rate": 1.6305166572516695e-07,
      "loss": 1.6695,
      "step": 376512
    },
    {
      "epoch": 0.0013665529062955183,
      "grad_norm": 8626.404581284141,
      "learning_rate": 1.6304473038139992e-07,
      "loss": 1.6861,
      "step": 376544
    },
    {
      "epoch": 0.001366669040646355,
      "grad_norm": 10952.685150226862,
      "learning_rate": 1.6303779592253464e-07,
      "loss": 1.7059,
      "step": 376576
    },
    {
      "epoch": 0.0013667851749971918,
      "grad_norm": 8713.165785178198,
      "learning_rate": 1.6303086234838296e-07,
      "loss": 1.6882,
      "step": 376608
    },
    {
      "epoch": 0.0013669013093480285,
      "grad_norm": 14688.136028781868,
      "learning_rate": 1.6302392965875677e-07,
      "loss": 1.6725,
      "step": 376640
    },
    {
      "epoch": 0.001367017443698865,
      "grad_norm": 12653.508841424184,
      "learning_rate": 1.63016997853468e-07,
      "loss": 1.6856,
      "step": 376672
    },
    {
      "epoch": 0.0013671335780497018,
      "grad_norm": 10682.35086486116,
      "learning_rate": 1.630100669323287e-07,
      "loss": 1.6808,
      "step": 376704
    },
    {
      "epoch": 0.0013672497124005386,
      "grad_norm": 9770.659343155916,
      "learning_rate": 1.6300313689515084e-07,
      "loss": 1.6821,
      "step": 376736
    },
    {
      "epoch": 0.0013673658467513753,
      "grad_norm": 9078.89134200867,
      "learning_rate": 1.6299620774174667e-07,
      "loss": 1.6692,
      "step": 376768
    },
    {
      "epoch": 0.0013674819811022119,
      "grad_norm": 8293.754276562575,
      "learning_rate": 1.6298927947192826e-07,
      "loss": 1.6649,
      "step": 376800
    },
    {
      "epoch": 0.0013675981154530486,
      "grad_norm": 8232.624854807851,
      "learning_rate": 1.6298235208550785e-07,
      "loss": 1.6788,
      "step": 376832
    },
    {
      "epoch": 0.0013677142498038854,
      "grad_norm": 13360.937991024433,
      "learning_rate": 1.6297542558229777e-07,
      "loss": 1.6867,
      "step": 376864
    },
    {
      "epoch": 0.0013678303841547221,
      "grad_norm": 10335.924922327948,
      "learning_rate": 1.6296849996211033e-07,
      "loss": 1.6727,
      "step": 376896
    },
    {
      "epoch": 0.001367946518505559,
      "grad_norm": 10619.666096445782,
      "learning_rate": 1.6296157522475794e-07,
      "loss": 1.6858,
      "step": 376928
    },
    {
      "epoch": 0.0013680626528563954,
      "grad_norm": 11964.980902617437,
      "learning_rate": 1.6295465137005302e-07,
      "loss": 1.7088,
      "step": 376960
    },
    {
      "epoch": 0.0013681787872072322,
      "grad_norm": 8568.329592166725,
      "learning_rate": 1.6294772839780813e-07,
      "loss": 1.703,
      "step": 376992
    },
    {
      "epoch": 0.001368294921558069,
      "grad_norm": 9287.589353540563,
      "learning_rate": 1.6294080630783577e-07,
      "loss": 1.7,
      "step": 377024
    },
    {
      "epoch": 0.0013684110559089057,
      "grad_norm": 9422.240285622098,
      "learning_rate": 1.6293388509994862e-07,
      "loss": 1.6877,
      "step": 377056
    },
    {
      "epoch": 0.0013685271902597422,
      "grad_norm": 10474.682047680493,
      "learning_rate": 1.6292696477395935e-07,
      "loss": 1.6916,
      "step": 377088
    },
    {
      "epoch": 0.001368643324610579,
      "grad_norm": 23401.29466503937,
      "learning_rate": 1.6292004532968064e-07,
      "loss": 1.6994,
      "step": 377120
    },
    {
      "epoch": 0.0013687594589614157,
      "grad_norm": 8588.561928518651,
      "learning_rate": 1.629133429586699e-07,
      "loss": 1.7166,
      "step": 377152
    },
    {
      "epoch": 0.0013688755933122525,
      "grad_norm": 9265.502037126753,
      "learning_rate": 1.6290642524971187e-07,
      "loss": 1.699,
      "step": 377184
    },
    {
      "epoch": 0.001368991727663089,
      "grad_norm": 8891.311264374901,
      "learning_rate": 1.6289950842190878e-07,
      "loss": 1.6982,
      "step": 377216
    },
    {
      "epoch": 0.0013691078620139258,
      "grad_norm": 11052.403720458278,
      "learning_rate": 1.6289259247507363e-07,
      "loss": 1.6912,
      "step": 377248
    },
    {
      "epoch": 0.0013692239963647625,
      "grad_norm": 9381.370901952443,
      "learning_rate": 1.6288567740901939e-07,
      "loss": 1.6984,
      "step": 377280
    },
    {
      "epoch": 0.0013693401307155993,
      "grad_norm": 10007.523270020409,
      "learning_rate": 1.6287876322355915e-07,
      "loss": 1.6957,
      "step": 377312
    },
    {
      "epoch": 0.001369456265066436,
      "grad_norm": 7749.043682932753,
      "learning_rate": 1.62871849918506e-07,
      "loss": 1.6844,
      "step": 377344
    },
    {
      "epoch": 0.0013695723994172726,
      "grad_norm": 9762.601292688338,
      "learning_rate": 1.6286493749367312e-07,
      "loss": 1.678,
      "step": 377376
    },
    {
      "epoch": 0.0013696885337681093,
      "grad_norm": 9563.044912578838,
      "learning_rate": 1.6285802594887376e-07,
      "loss": 1.6906,
      "step": 377408
    },
    {
      "epoch": 0.001369804668118946,
      "grad_norm": 8795.809684162112,
      "learning_rate": 1.6285111528392118e-07,
      "loss": 1.71,
      "step": 377440
    },
    {
      "epoch": 0.0013699208024697828,
      "grad_norm": 12482.56079496511,
      "learning_rate": 1.628442054986287e-07,
      "loss": 1.6957,
      "step": 377472
    },
    {
      "epoch": 0.0013700369368206194,
      "grad_norm": 9479.363058771407,
      "learning_rate": 1.628372965928098e-07,
      "loss": 1.7078,
      "step": 377504
    },
    {
      "epoch": 0.0013701530711714561,
      "grad_norm": 7556.709601407216,
      "learning_rate": 1.6283038856627786e-07,
      "loss": 1.713,
      "step": 377536
    },
    {
      "epoch": 0.001370269205522293,
      "grad_norm": 8956.728420578576,
      "learning_rate": 1.6282348141884638e-07,
      "loss": 1.7034,
      "step": 377568
    },
    {
      "epoch": 0.0013703853398731296,
      "grad_norm": 9372.645304288433,
      "learning_rate": 1.6281657515032896e-07,
      "loss": 1.6933,
      "step": 377600
    },
    {
      "epoch": 0.0013705014742239664,
      "grad_norm": 8780.60681274364,
      "learning_rate": 1.6280966976053921e-07,
      "loss": 1.6797,
      "step": 377632
    },
    {
      "epoch": 0.001370617608574803,
      "grad_norm": 10467.317134777182,
      "learning_rate": 1.6280276524929077e-07,
      "loss": 1.6754,
      "step": 377664
    },
    {
      "epoch": 0.0013707337429256397,
      "grad_norm": 14466.31535671748,
      "learning_rate": 1.627958616163974e-07,
      "loss": 1.6957,
      "step": 377696
    },
    {
      "epoch": 0.0013708498772764764,
      "grad_norm": 9578.49006889917,
      "learning_rate": 1.6278895886167288e-07,
      "loss": 1.7025,
      "step": 377728
    },
    {
      "epoch": 0.0013709660116273132,
      "grad_norm": 9052.073574601569,
      "learning_rate": 1.6278205698493108e-07,
      "loss": 1.6546,
      "step": 377760
    },
    {
      "epoch": 0.0013710821459781497,
      "grad_norm": 9469.812775340386,
      "learning_rate": 1.627751559859858e-07,
      "loss": 1.6722,
      "step": 377792
    },
    {
      "epoch": 0.0013711982803289865,
      "grad_norm": 11098.496474748279,
      "learning_rate": 1.6276825586465106e-07,
      "loss": 1.6836,
      "step": 377824
    },
    {
      "epoch": 0.0013713144146798232,
      "grad_norm": 9521.50471301674,
      "learning_rate": 1.6276135662074085e-07,
      "loss": 1.6861,
      "step": 377856
    },
    {
      "epoch": 0.00137143054903066,
      "grad_norm": 9711.27530245127,
      "learning_rate": 1.6275445825406925e-07,
      "loss": 1.6854,
      "step": 377888
    },
    {
      "epoch": 0.0013715466833814968,
      "grad_norm": 8403.921227617499,
      "learning_rate": 1.6274756076445035e-07,
      "loss": 1.6709,
      "step": 377920
    },
    {
      "epoch": 0.0013716628177323333,
      "grad_norm": 8171.644632508195,
      "learning_rate": 1.627406641516983e-07,
      "loss": 1.6881,
      "step": 377952
    },
    {
      "epoch": 0.00137177895208317,
      "grad_norm": 10015.52634662802,
      "learning_rate": 1.6273376841562733e-07,
      "loss": 1.7023,
      "step": 377984
    },
    {
      "epoch": 0.0013718950864340068,
      "grad_norm": 10087.86845671572,
      "learning_rate": 1.6272687355605179e-07,
      "loss": 1.711,
      "step": 378016
    },
    {
      "epoch": 0.0013720112207848436,
      "grad_norm": 10490.492076161156,
      "learning_rate": 1.6271997957278596e-07,
      "loss": 1.7022,
      "step": 378048
    },
    {
      "epoch": 0.00137212735513568,
      "grad_norm": 8905.538501404617,
      "learning_rate": 1.627130864656442e-07,
      "loss": 1.7086,
      "step": 378080
    },
    {
      "epoch": 0.0013722434894865168,
      "grad_norm": 12382.700997762968,
      "learning_rate": 1.6270619423444098e-07,
      "loss": 1.6791,
      "step": 378112
    },
    {
      "epoch": 0.0013723596238373536,
      "grad_norm": 21413.97450264663,
      "learning_rate": 1.6269930287899082e-07,
      "loss": 1.6825,
      "step": 378144
    },
    {
      "epoch": 0.0013724757581881904,
      "grad_norm": 28340.642476838806,
      "learning_rate": 1.6269241239910824e-07,
      "loss": 1.6883,
      "step": 378176
    },
    {
      "epoch": 0.001372591892539027,
      "grad_norm": 19059.201242444553,
      "learning_rate": 1.626855227946079e-07,
      "loss": 1.6629,
      "step": 378208
    },
    {
      "epoch": 0.0013727080268898636,
      "grad_norm": 11991.466132212525,
      "learning_rate": 1.6267884932484942e-07,
      "loss": 1.6701,
      "step": 378240
    },
    {
      "epoch": 0.0013728241612407004,
      "grad_norm": 11489.127207930112,
      "learning_rate": 1.6267196144321622e-07,
      "loss": 1.6834,
      "step": 378272
    },
    {
      "epoch": 0.0013729402955915372,
      "grad_norm": 9996.107642477646,
      "learning_rate": 1.6266507443641518e-07,
      "loss": 1.6822,
      "step": 378304
    },
    {
      "epoch": 0.001373056429942374,
      "grad_norm": 10313.642421569599,
      "learning_rate": 1.6265818830426114e-07,
      "loss": 1.6753,
      "step": 378336
    },
    {
      "epoch": 0.0013731725642932104,
      "grad_norm": 13500.75116428712,
      "learning_rate": 1.6265130304656895e-07,
      "loss": 1.6844,
      "step": 378368
    },
    {
      "epoch": 0.0013732886986440472,
      "grad_norm": 11012.906791578689,
      "learning_rate": 1.626444186631536e-07,
      "loss": 1.6782,
      "step": 378400
    },
    {
      "epoch": 0.001373404832994884,
      "grad_norm": 11987.569061323484,
      "learning_rate": 1.6263753515383003e-07,
      "loss": 1.6757,
      "step": 378432
    },
    {
      "epoch": 0.0013735209673457207,
      "grad_norm": 11715.460383612759,
      "learning_rate": 1.626306525184133e-07,
      "loss": 1.6782,
      "step": 378464
    },
    {
      "epoch": 0.0013736371016965575,
      "grad_norm": 8656.760826082698,
      "learning_rate": 1.6262377075671854e-07,
      "loss": 1.661,
      "step": 378496
    },
    {
      "epoch": 0.001373753236047394,
      "grad_norm": 10475.028782776684,
      "learning_rate": 1.6261688986856087e-07,
      "loss": 1.6706,
      "step": 378528
    },
    {
      "epoch": 0.0013738693703982308,
      "grad_norm": 12564.23224872893,
      "learning_rate": 1.626100098537555e-07,
      "loss": 1.6883,
      "step": 378560
    },
    {
      "epoch": 0.0013739855047490675,
      "grad_norm": 9145.793568630335,
      "learning_rate": 1.626031307121177e-07,
      "loss": 1.693,
      "step": 378592
    },
    {
      "epoch": 0.0013741016390999043,
      "grad_norm": 10673.842607046443,
      "learning_rate": 1.6259625244346285e-07,
      "loss": 1.676,
      "step": 378624
    },
    {
      "epoch": 0.0013742177734507408,
      "grad_norm": 8593.171242329574,
      "learning_rate": 1.6258937504760625e-07,
      "loss": 1.6881,
      "step": 378656
    },
    {
      "epoch": 0.0013743339078015776,
      "grad_norm": 9129.448504701695,
      "learning_rate": 1.6258249852436337e-07,
      "loss": 1.6842,
      "step": 378688
    },
    {
      "epoch": 0.0013744500421524143,
      "grad_norm": 10334.514212095313,
      "learning_rate": 1.6257562287354968e-07,
      "loss": 1.6807,
      "step": 378720
    },
    {
      "epoch": 0.001374566176503251,
      "grad_norm": 10646.835774069214,
      "learning_rate": 1.6256874809498068e-07,
      "loss": 1.686,
      "step": 378752
    },
    {
      "epoch": 0.0013746823108540878,
      "grad_norm": 9840.653433588646,
      "learning_rate": 1.6256187418847207e-07,
      "loss": 1.6806,
      "step": 378784
    },
    {
      "epoch": 0.0013747984452049244,
      "grad_norm": 10944.110745053707,
      "learning_rate": 1.625550011538394e-07,
      "loss": 1.6822,
      "step": 378816
    },
    {
      "epoch": 0.001374914579555761,
      "grad_norm": 8714.833905474045,
      "learning_rate": 1.625481289908984e-07,
      "loss": 1.7005,
      "step": 378848
    },
    {
      "epoch": 0.0013750307139065979,
      "grad_norm": 14388.923934749255,
      "learning_rate": 1.6254125769946483e-07,
      "loss": 1.69,
      "step": 378880
    },
    {
      "epoch": 0.0013751468482574346,
      "grad_norm": 8894.827822954192,
      "learning_rate": 1.6253438727935453e-07,
      "loss": 1.6768,
      "step": 378912
    },
    {
      "epoch": 0.0013752629826082712,
      "grad_norm": 10674.014427571288,
      "learning_rate": 1.6252751773038336e-07,
      "loss": 1.7009,
      "step": 378944
    },
    {
      "epoch": 0.001375379116959108,
      "grad_norm": 9248.579134115684,
      "learning_rate": 1.625206490523672e-07,
      "loss": 1.6782,
      "step": 378976
    },
    {
      "epoch": 0.0013754952513099447,
      "grad_norm": 12973.52411644577,
      "learning_rate": 1.6251378124512204e-07,
      "loss": 1.681,
      "step": 379008
    },
    {
      "epoch": 0.0013756113856607814,
      "grad_norm": 11539.655280813202,
      "learning_rate": 1.6250691430846395e-07,
      "loss": 1.6826,
      "step": 379040
    },
    {
      "epoch": 0.0013757275200116182,
      "grad_norm": 13272.069921455357,
      "learning_rate": 1.6250004824220899e-07,
      "loss": 1.6666,
      "step": 379072
    },
    {
      "epoch": 0.0013758436543624547,
      "grad_norm": 10829.41087963699,
      "learning_rate": 1.6249318304617327e-07,
      "loss": 1.6723,
      "step": 379104
    },
    {
      "epoch": 0.0013759597887132915,
      "grad_norm": 11134.281835843747,
      "learning_rate": 1.6248631872017301e-07,
      "loss": 1.6934,
      "step": 379136
    },
    {
      "epoch": 0.0013760759230641282,
      "grad_norm": 8943.659094576447,
      "learning_rate": 1.624794552640245e-07,
      "loss": 1.6952,
      "step": 379168
    },
    {
      "epoch": 0.001376192057414965,
      "grad_norm": 9209.090943193036,
      "learning_rate": 1.6247259267754393e-07,
      "loss": 1.6854,
      "step": 379200
    },
    {
      "epoch": 0.0013763081917658015,
      "grad_norm": 18653.59075352518,
      "learning_rate": 1.6246573096054777e-07,
      "loss": 1.701,
      "step": 379232
    },
    {
      "epoch": 0.0013764243261166383,
      "grad_norm": 16558.72845359812,
      "learning_rate": 1.624588701128524e-07,
      "loss": 1.6823,
      "step": 379264
    },
    {
      "epoch": 0.001376540460467475,
      "grad_norm": 18555.45957393672,
      "learning_rate": 1.6245201013427422e-07,
      "loss": 1.6772,
      "step": 379296
    },
    {
      "epoch": 0.0013766565948183118,
      "grad_norm": 9079.138945957375,
      "learning_rate": 1.6244536535865524e-07,
      "loss": 1.6848,
      "step": 379328
    },
    {
      "epoch": 0.0013767727291691485,
      "grad_norm": 10186.541905867762,
      "learning_rate": 1.6243850709061548e-07,
      "loss": 1.6547,
      "step": 379360
    },
    {
      "epoch": 0.001376888863519985,
      "grad_norm": 8605.949105124897,
      "learning_rate": 1.6243164969114842e-07,
      "loss": 1.6616,
      "step": 379392
    },
    {
      "epoch": 0.0013770049978708218,
      "grad_norm": 11102.00054044315,
      "learning_rate": 1.6242479316007074e-07,
      "loss": 1.6708,
      "step": 379424
    },
    {
      "epoch": 0.0013771211322216586,
      "grad_norm": 9272.704783395187,
      "learning_rate": 1.6241793749719916e-07,
      "loss": 1.669,
      "step": 379456
    },
    {
      "epoch": 0.0013772372665724953,
      "grad_norm": 9097.157578057006,
      "learning_rate": 1.624110827023505e-07,
      "loss": 1.6686,
      "step": 379488
    },
    {
      "epoch": 0.0013773534009233319,
      "grad_norm": 11235.533098166727,
      "learning_rate": 1.6240422877534152e-07,
      "loss": 1.685,
      "step": 379520
    },
    {
      "epoch": 0.0013774695352741686,
      "grad_norm": 11720.338561662798,
      "learning_rate": 1.623973757159892e-07,
      "loss": 1.6872,
      "step": 379552
    },
    {
      "epoch": 0.0013775856696250054,
      "grad_norm": 9644.193278859564,
      "learning_rate": 1.6239052352411046e-07,
      "loss": 1.684,
      "step": 379584
    },
    {
      "epoch": 0.0013777018039758421,
      "grad_norm": 9604.035610096415,
      "learning_rate": 1.623836721995223e-07,
      "loss": 1.6917,
      "step": 379616
    },
    {
      "epoch": 0.0013778179383266789,
      "grad_norm": 9579.539028575435,
      "learning_rate": 1.6237682174204177e-07,
      "loss": 1.687,
      "step": 379648
    },
    {
      "epoch": 0.0013779340726775154,
      "grad_norm": 8048.352005224424,
      "learning_rate": 1.6236997215148601e-07,
      "loss": 1.702,
      "step": 379680
    },
    {
      "epoch": 0.0013780502070283522,
      "grad_norm": 12537.7013842251,
      "learning_rate": 1.6236312342767216e-07,
      "loss": 1.7259,
      "step": 379712
    },
    {
      "epoch": 0.001378166341379189,
      "grad_norm": 10406.242165162215,
      "learning_rate": 1.623562755704175e-07,
      "loss": 1.6985,
      "step": 379744
    },
    {
      "epoch": 0.0013782824757300257,
      "grad_norm": 9284.93252533372,
      "learning_rate": 1.623494285795392e-07,
      "loss": 1.6918,
      "step": 379776
    },
    {
      "epoch": 0.0013783986100808622,
      "grad_norm": 9328.215477785663,
      "learning_rate": 1.6234258245485466e-07,
      "loss": 1.7127,
      "step": 379808
    },
    {
      "epoch": 0.001378514744431699,
      "grad_norm": 10318.469653974857,
      "learning_rate": 1.6233573719618128e-07,
      "loss": 1.6915,
      "step": 379840
    },
    {
      "epoch": 0.0013786308787825357,
      "grad_norm": 10625.180657287667,
      "learning_rate": 1.6232889280333645e-07,
      "loss": 1.6929,
      "step": 379872
    },
    {
      "epoch": 0.0013787470131333725,
      "grad_norm": 10565.508411808682,
      "learning_rate": 1.6232204927613765e-07,
      "loss": 1.6724,
      "step": 379904
    },
    {
      "epoch": 0.0013788631474842092,
      "grad_norm": 8733.480176882525,
      "learning_rate": 1.6231520661440248e-07,
      "loss": 1.6657,
      "step": 379936
    },
    {
      "epoch": 0.0013789792818350458,
      "grad_norm": 11634.248922900008,
      "learning_rate": 1.6230836481794853e-07,
      "loss": 1.6739,
      "step": 379968
    },
    {
      "epoch": 0.0013790954161858825,
      "grad_norm": 9721.675575743104,
      "learning_rate": 1.6230152388659337e-07,
      "loss": 1.6776,
      "step": 380000
    },
    {
      "epoch": 0.0013792115505367193,
      "grad_norm": 9205.175718040367,
      "learning_rate": 1.622946838201548e-07,
      "loss": 1.668,
      "step": 380032
    },
    {
      "epoch": 0.001379327684887556,
      "grad_norm": 8987.400736586747,
      "learning_rate": 1.6228784461845054e-07,
      "loss": 1.6808,
      "step": 380064
    },
    {
      "epoch": 0.0013794438192383926,
      "grad_norm": 9228.40419574262,
      "learning_rate": 1.622810062812984e-07,
      "loss": 1.6996,
      "step": 380096
    },
    {
      "epoch": 0.0013795599535892293,
      "grad_norm": 7942.4669970985715,
      "learning_rate": 1.6227416880851626e-07,
      "loss": 1.6855,
      "step": 380128
    },
    {
      "epoch": 0.001379676087940066,
      "grad_norm": 8844.847313549284,
      "learning_rate": 1.6226733219992206e-07,
      "loss": 1.6894,
      "step": 380160
    },
    {
      "epoch": 0.0013797922222909028,
      "grad_norm": 9862.944185181217,
      "learning_rate": 1.622604964553337e-07,
      "loss": 1.6823,
      "step": 380192
    },
    {
      "epoch": 0.0013799083566417396,
      "grad_norm": 9403.797424445085,
      "learning_rate": 1.622536615745693e-07,
      "loss": 1.6772,
      "step": 380224
    },
    {
      "epoch": 0.0013800244909925761,
      "grad_norm": 8246.473306814254,
      "learning_rate": 1.6224682755744688e-07,
      "loss": 1.6864,
      "step": 380256
    },
    {
      "epoch": 0.0013801406253434129,
      "grad_norm": 9674.097994128444,
      "learning_rate": 1.6223999440378458e-07,
      "loss": 1.6804,
      "step": 380288
    },
    {
      "epoch": 0.0013802567596942496,
      "grad_norm": 11277.742327256818,
      "learning_rate": 1.6223316211340064e-07,
      "loss": 1.661,
      "step": 380320
    },
    {
      "epoch": 0.0013803728940450864,
      "grad_norm": 18752.514391408957,
      "learning_rate": 1.6222654415515333e-07,
      "loss": 1.6733,
      "step": 380352
    },
    {
      "epoch": 0.001380489028395923,
      "grad_norm": 9392.37797365502,
      "learning_rate": 1.6221971356381745e-07,
      "loss": 1.6816,
      "step": 380384
    },
    {
      "epoch": 0.0013806051627467597,
      "grad_norm": 14061.240059112853,
      "learning_rate": 1.6221288383522046e-07,
      "loss": 1.6956,
      "step": 380416
    },
    {
      "epoch": 0.0013807212970975964,
      "grad_norm": 9467.353378848811,
      "learning_rate": 1.6220605496918074e-07,
      "loss": 1.6852,
      "step": 380448
    },
    {
      "epoch": 0.0013808374314484332,
      "grad_norm": 8788.485876418075,
      "learning_rate": 1.621992269655168e-07,
      "loss": 1.6782,
      "step": 380480
    },
    {
      "epoch": 0.00138095356579927,
      "grad_norm": 9095.51251991882,
      "learning_rate": 1.6219239982404709e-07,
      "loss": 1.6702,
      "step": 380512
    },
    {
      "epoch": 0.0013810697001501065,
      "grad_norm": 10147.147973691917,
      "learning_rate": 1.6218557354459018e-07,
      "loss": 1.6799,
      "step": 380544
    },
    {
      "epoch": 0.0013811858345009432,
      "grad_norm": 10364.497286409987,
      "learning_rate": 1.6217874812696473e-07,
      "loss": 1.7103,
      "step": 380576
    },
    {
      "epoch": 0.00138130196885178,
      "grad_norm": 11543.31633457214,
      "learning_rate": 1.6217192357098935e-07,
      "loss": 1.6925,
      "step": 380608
    },
    {
      "epoch": 0.0013814181032026167,
      "grad_norm": 8979.346746840774,
      "learning_rate": 1.6216509987648279e-07,
      "loss": 1.7055,
      "step": 380640
    },
    {
      "epoch": 0.0013815342375534533,
      "grad_norm": 8109.892601015133,
      "learning_rate": 1.6215827704326384e-07,
      "loss": 1.7171,
      "step": 380672
    },
    {
      "epoch": 0.00138165037190429,
      "grad_norm": 10705.47691604629,
      "learning_rate": 1.621514550711513e-07,
      "loss": 1.7199,
      "step": 380704
    },
    {
      "epoch": 0.0013817665062551268,
      "grad_norm": 10892.13587869707,
      "learning_rate": 1.6214463395996406e-07,
      "loss": 1.7063,
      "step": 380736
    },
    {
      "epoch": 0.0013818826406059635,
      "grad_norm": 9483.908055227022,
      "learning_rate": 1.6213781370952107e-07,
      "loss": 1.6782,
      "step": 380768
    },
    {
      "epoch": 0.0013819987749568003,
      "grad_norm": 8195.146734500853,
      "learning_rate": 1.6213099431964132e-07,
      "loss": 1.6842,
      "step": 380800
    },
    {
      "epoch": 0.0013821149093076368,
      "grad_norm": 10369.659203657562,
      "learning_rate": 1.621241757901438e-07,
      "loss": 1.6691,
      "step": 380832
    },
    {
      "epoch": 0.0013822310436584736,
      "grad_norm": 11247.520793490448,
      "learning_rate": 1.6211735812084767e-07,
      "loss": 1.6811,
      "step": 380864
    },
    {
      "epoch": 0.0013823471780093103,
      "grad_norm": 8915.981830398714,
      "learning_rate": 1.621105413115721e-07,
      "loss": 1.6589,
      "step": 380896
    },
    {
      "epoch": 0.001382463312360147,
      "grad_norm": 11301.61174346385,
      "learning_rate": 1.621037253621362e-07,
      "loss": 1.6842,
      "step": 380928
    },
    {
      "epoch": 0.0013825794467109836,
      "grad_norm": 8559.496013200776,
      "learning_rate": 1.6209691027235928e-07,
      "loss": 1.7062,
      "step": 380960
    },
    {
      "epoch": 0.0013826955810618204,
      "grad_norm": 13983.596247031734,
      "learning_rate": 1.6209009604206063e-07,
      "loss": 1.6807,
      "step": 380992
    },
    {
      "epoch": 0.0013828117154126571,
      "grad_norm": 9176.969216467929,
      "learning_rate": 1.6208328267105966e-07,
      "loss": 1.6806,
      "step": 381024
    },
    {
      "epoch": 0.0013829278497634939,
      "grad_norm": 9350.967864344311,
      "learning_rate": 1.620764701591757e-07,
      "loss": 1.6734,
      "step": 381056
    },
    {
      "epoch": 0.0013830439841143306,
      "grad_norm": 9092.846419026333,
      "learning_rate": 1.6206965850622829e-07,
      "loss": 1.6586,
      "step": 381088
    },
    {
      "epoch": 0.0013831601184651672,
      "grad_norm": 7821.908590619045,
      "learning_rate": 1.6206284771203693e-07,
      "loss": 1.6687,
      "step": 381120
    },
    {
      "epoch": 0.001383276252816004,
      "grad_norm": 10254.101618376912,
      "learning_rate": 1.6205603777642116e-07,
      "loss": 1.6776,
      "step": 381152
    },
    {
      "epoch": 0.0013833923871668407,
      "grad_norm": 12019.281842106873,
      "learning_rate": 1.6204922869920066e-07,
      "loss": 1.6606,
      "step": 381184
    },
    {
      "epoch": 0.0013835085215176774,
      "grad_norm": 8560.130606480254,
      "learning_rate": 1.6204242048019508e-07,
      "loss": 1.6836,
      "step": 381216
    },
    {
      "epoch": 0.001383624655868514,
      "grad_norm": 7889.6998675488285,
      "learning_rate": 1.6203561311922418e-07,
      "loss": 1.6881,
      "step": 381248
    },
    {
      "epoch": 0.0013837407902193507,
      "grad_norm": 8265.585520699668,
      "learning_rate": 1.6202880661610772e-07,
      "loss": 1.6953,
      "step": 381280
    },
    {
      "epoch": 0.0013838569245701875,
      "grad_norm": 10407.477504179387,
      "learning_rate": 1.6202200097066553e-07,
      "loss": 1.7122,
      "step": 381312
    },
    {
      "epoch": 0.0013839730589210242,
      "grad_norm": 12776.239822420368,
      "learning_rate": 1.62015408819363e-07,
      "loss": 1.6821,
      "step": 381344
    },
    {
      "epoch": 0.001384089193271861,
      "grad_norm": 12285.048799251877,
      "learning_rate": 1.620086048619408e-07,
      "loss": 1.6691,
      "step": 381376
    },
    {
      "epoch": 0.0013842053276226975,
      "grad_norm": 9433.783334378631,
      "learning_rate": 1.6200180176165832e-07,
      "loss": 1.6847,
      "step": 381408
    },
    {
      "epoch": 0.0013843214619735343,
      "grad_norm": 9368.25672150374,
      "learning_rate": 1.6199499951833563e-07,
      "loss": 1.6972,
      "step": 381440
    },
    {
      "epoch": 0.001384437596324371,
      "grad_norm": 9469.586685806304,
      "learning_rate": 1.619881981317928e-07,
      "loss": 1.7007,
      "step": 381472
    },
    {
      "epoch": 0.0013845537306752078,
      "grad_norm": 10543.485192288173,
      "learning_rate": 1.6198139760185006e-07,
      "loss": 1.7193,
      "step": 381504
    },
    {
      "epoch": 0.0013846698650260443,
      "grad_norm": 10923.489918519632,
      "learning_rate": 1.6197459792832758e-07,
      "loss": 1.7155,
      "step": 381536
    },
    {
      "epoch": 0.001384785999376881,
      "grad_norm": 10639.546794859263,
      "learning_rate": 1.6196779911104562e-07,
      "loss": 1.6947,
      "step": 381568
    },
    {
      "epoch": 0.0013849021337277178,
      "grad_norm": 9969.44170954422,
      "learning_rate": 1.6196100114982447e-07,
      "loss": 1.6879,
      "step": 381600
    },
    {
      "epoch": 0.0013850182680785546,
      "grad_norm": 10318.85197102856,
      "learning_rate": 1.619542040444845e-07,
      "loss": 1.6628,
      "step": 381632
    },
    {
      "epoch": 0.0013851344024293913,
      "grad_norm": 9749.96943584953,
      "learning_rate": 1.6194740779484617e-07,
      "loss": 1.6674,
      "step": 381664
    },
    {
      "epoch": 0.0013852505367802279,
      "grad_norm": 9405.942164397993,
      "learning_rate": 1.6194061240072994e-07,
      "loss": 1.6721,
      "step": 381696
    },
    {
      "epoch": 0.0013853666711310646,
      "grad_norm": 9068.607390332872,
      "learning_rate": 1.6193381786195632e-07,
      "loss": 1.6775,
      "step": 381728
    },
    {
      "epoch": 0.0013854828054819014,
      "grad_norm": 9300.693092452842,
      "learning_rate": 1.6192702417834586e-07,
      "loss": 1.6634,
      "step": 381760
    },
    {
      "epoch": 0.0013855989398327381,
      "grad_norm": 9218.988556235441,
      "learning_rate": 1.6192023134971926e-07,
      "loss": 1.6982,
      "step": 381792
    },
    {
      "epoch": 0.0013857150741835747,
      "grad_norm": 9724.201766726152,
      "learning_rate": 1.6191343937589713e-07,
      "loss": 1.7141,
      "step": 381824
    },
    {
      "epoch": 0.0013858312085344114,
      "grad_norm": 8663.83194666194,
      "learning_rate": 1.6190664825670026e-07,
      "loss": 1.7118,
      "step": 381856
    },
    {
      "epoch": 0.0013859473428852482,
      "grad_norm": 10551.257365830861,
      "learning_rate": 1.6189985799194938e-07,
      "loss": 1.6952,
      "step": 381888
    },
    {
      "epoch": 0.001386063477236085,
      "grad_norm": 11675.710599359682,
      "learning_rate": 1.6189306858146542e-07,
      "loss": 1.6552,
      "step": 381920
    },
    {
      "epoch": 0.0013861796115869217,
      "grad_norm": 9325.338385281255,
      "learning_rate": 1.6188628002506917e-07,
      "loss": 1.6671,
      "step": 381952
    },
    {
      "epoch": 0.0013862957459377582,
      "grad_norm": 9812.333055904697,
      "learning_rate": 1.6187949232258165e-07,
      "loss": 1.6752,
      "step": 381984
    },
    {
      "epoch": 0.001386411880288595,
      "grad_norm": 12482.238260824859,
      "learning_rate": 1.618727054738238e-07,
      "loss": 1.6844,
      "step": 382016
    },
    {
      "epoch": 0.0013865280146394317,
      "grad_norm": 9463.212562338436,
      "learning_rate": 1.6186591947861673e-07,
      "loss": 1.6748,
      "step": 382048
    },
    {
      "epoch": 0.0013866441489902685,
      "grad_norm": 10616.856879510056,
      "learning_rate": 1.6185913433678152e-07,
      "loss": 1.6923,
      "step": 382080
    },
    {
      "epoch": 0.001386760283341105,
      "grad_norm": 7923.085636290952,
      "learning_rate": 1.6185235004813928e-07,
      "loss": 1.694,
      "step": 382112
    },
    {
      "epoch": 0.0013868764176919418,
      "grad_norm": 12871.78153947619,
      "learning_rate": 1.618455666125113e-07,
      "loss": 1.6959,
      "step": 382144
    },
    {
      "epoch": 0.0013869925520427785,
      "grad_norm": 9319.932295891425,
      "learning_rate": 1.6183878402971877e-07,
      "loss": 1.6979,
      "step": 382176
    },
    {
      "epoch": 0.0013871086863936153,
      "grad_norm": 7955.19968322606,
      "learning_rate": 1.6183200229958306e-07,
      "loss": 1.6746,
      "step": 382208
    },
    {
      "epoch": 0.001387224820744452,
      "grad_norm": 9569.221911942475,
      "learning_rate": 1.618252214219255e-07,
      "loss": 1.6859,
      "step": 382240
    },
    {
      "epoch": 0.0013873409550952886,
      "grad_norm": 10933.044223819825,
      "learning_rate": 1.618184413965675e-07,
      "loss": 1.6951,
      "step": 382272
    },
    {
      "epoch": 0.0013874570894461253,
      "grad_norm": 10723.656652467012,
      "learning_rate": 1.6181166222333057e-07,
      "loss": 1.7024,
      "step": 382304
    },
    {
      "epoch": 0.001387573223796962,
      "grad_norm": 18857.90529194587,
      "learning_rate": 1.6180488390203618e-07,
      "loss": 1.7099,
      "step": 382336
    },
    {
      "epoch": 0.0013876893581477989,
      "grad_norm": 15994.159433993398,
      "learning_rate": 1.6179810643250592e-07,
      "loss": 1.7398,
      "step": 382368
    },
    {
      "epoch": 0.0013878054924986354,
      "grad_norm": 9371.484941032557,
      "learning_rate": 1.617915415709838e-07,
      "loss": 1.74,
      "step": 382400
    },
    {
      "epoch": 0.0013879216268494721,
      "grad_norm": 10333.43118233242,
      "learning_rate": 1.61784765777843e-07,
      "loss": 1.7286,
      "step": 382432
    },
    {
      "epoch": 0.001388037761200309,
      "grad_norm": 9759.442811964216,
      "learning_rate": 1.6177799083593697e-07,
      "loss": 1.7079,
      "step": 382464
    },
    {
      "epoch": 0.0013881538955511457,
      "grad_norm": 9753.791878033897,
      "learning_rate": 1.6177121674508744e-07,
      "loss": 1.6711,
      "step": 382496
    },
    {
      "epoch": 0.0013882700299019824,
      "grad_norm": 13292.29265401571,
      "learning_rate": 1.6176444350511628e-07,
      "loss": 1.6723,
      "step": 382528
    },
    {
      "epoch": 0.001388386164252819,
      "grad_norm": 10883.68356761625,
      "learning_rate": 1.617576711158454e-07,
      "loss": 1.6979,
      "step": 382560
    },
    {
      "epoch": 0.0013885022986036557,
      "grad_norm": 8404.200973322806,
      "learning_rate": 1.6175089957709668e-07,
      "loss": 1.6942,
      "step": 382592
    },
    {
      "epoch": 0.0013886184329544925,
      "grad_norm": 9026.616863476593,
      "learning_rate": 1.6174412888869216e-07,
      "loss": 1.6729,
      "step": 382624
    },
    {
      "epoch": 0.0013887345673053292,
      "grad_norm": 9010.16004297371,
      "learning_rate": 1.6173735905045385e-07,
      "loss": 1.7021,
      "step": 382656
    },
    {
      "epoch": 0.0013888507016561657,
      "grad_norm": 9642.094896857217,
      "learning_rate": 1.6173059006220388e-07,
      "loss": 1.6842,
      "step": 382688
    },
    {
      "epoch": 0.0013889668360070025,
      "grad_norm": 9488.068507341208,
      "learning_rate": 1.6172382192376438e-07,
      "loss": 1.6916,
      "step": 382720
    },
    {
      "epoch": 0.0013890829703578393,
      "grad_norm": 14193.834999745488,
      "learning_rate": 1.6171705463495755e-07,
      "loss": 1.6717,
      "step": 382752
    },
    {
      "epoch": 0.001389199104708676,
      "grad_norm": 12356.544500789854,
      "learning_rate": 1.6171028819560566e-07,
      "loss": 1.6564,
      "step": 382784
    },
    {
      "epoch": 0.0013893152390595128,
      "grad_norm": 11651.566418297585,
      "learning_rate": 1.6170352260553095e-07,
      "loss": 1.6663,
      "step": 382816
    },
    {
      "epoch": 0.0013894313734103493,
      "grad_norm": 8246.7306249204,
      "learning_rate": 1.6169675786455585e-07,
      "loss": 1.6687,
      "step": 382848
    },
    {
      "epoch": 0.001389547507761186,
      "grad_norm": 9740.80427890839,
      "learning_rate": 1.6168999397250274e-07,
      "loss": 1.6695,
      "step": 382880
    },
    {
      "epoch": 0.0013896636421120228,
      "grad_norm": 8267.406485712432,
      "learning_rate": 1.6168323092919405e-07,
      "loss": 1.7009,
      "step": 382912
    },
    {
      "epoch": 0.0013897797764628596,
      "grad_norm": 7272.616173015045,
      "learning_rate": 1.6167646873445237e-07,
      "loss": 1.7005,
      "step": 382944
    },
    {
      "epoch": 0.001389895910813696,
      "grad_norm": 9100.977749670636,
      "learning_rate": 1.616697073881002e-07,
      "loss": 1.6788,
      "step": 382976
    },
    {
      "epoch": 0.0013900120451645329,
      "grad_norm": 10106.389661991072,
      "learning_rate": 1.6166294688996013e-07,
      "loss": 1.6562,
      "step": 383008
    },
    {
      "epoch": 0.0013901281795153696,
      "grad_norm": 8101.65452237998,
      "learning_rate": 1.6165618723985487e-07,
      "loss": 1.6695,
      "step": 383040
    },
    {
      "epoch": 0.0013902443138662064,
      "grad_norm": 11628.619006571675,
      "learning_rate": 1.616494284376072e-07,
      "loss": 1.6702,
      "step": 383072
    },
    {
      "epoch": 0.0013903604482170431,
      "grad_norm": 12703.603898107027,
      "learning_rate": 1.6164267048303976e-07,
      "loss": 1.6813,
      "step": 383104
    },
    {
      "epoch": 0.0013904765825678797,
      "grad_norm": 9165.772635190118,
      "learning_rate": 1.6163591337597549e-07,
      "loss": 1.6969,
      "step": 383136
    },
    {
      "epoch": 0.0013905927169187164,
      "grad_norm": 10980.031693943329,
      "learning_rate": 1.6162915711623718e-07,
      "loss": 1.6921,
      "step": 383168
    },
    {
      "epoch": 0.0013907088512695532,
      "grad_norm": 8321.292087170117,
      "learning_rate": 1.616224017036478e-07,
      "loss": 1.6835,
      "step": 383200
    },
    {
      "epoch": 0.00139082498562039,
      "grad_norm": 15554.20971955824,
      "learning_rate": 1.6161564713803034e-07,
      "loss": 1.6866,
      "step": 383232
    },
    {
      "epoch": 0.0013909411199712265,
      "grad_norm": 10726.278198890797,
      "learning_rate": 1.6160889341920779e-07,
      "loss": 1.6948,
      "step": 383264
    },
    {
      "epoch": 0.0013910572543220632,
      "grad_norm": 9383.412385694237,
      "learning_rate": 1.6160214054700323e-07,
      "loss": 1.6856,
      "step": 383296
    },
    {
      "epoch": 0.0013911733886729,
      "grad_norm": 8096.780224261987,
      "learning_rate": 1.6159538852123987e-07,
      "loss": 1.6877,
      "step": 383328
    },
    {
      "epoch": 0.0013912895230237367,
      "grad_norm": 8067.884233180344,
      "learning_rate": 1.6158863734174083e-07,
      "loss": 1.6713,
      "step": 383360
    },
    {
      "epoch": 0.0013914056573745735,
      "grad_norm": 13683.85428159771,
      "learning_rate": 1.6158209794344322e-07,
      "loss": 1.6862,
      "step": 383392
    },
    {
      "epoch": 0.00139152179172541,
      "grad_norm": 10729.034066494522,
      "learning_rate": 1.6157534842951056e-07,
      "loss": 1.6701,
      "step": 383424
    },
    {
      "epoch": 0.0013916379260762468,
      "grad_norm": 8292.23540428032,
      "learning_rate": 1.6156859976131762e-07,
      "loss": 1.6862,
      "step": 383456
    },
    {
      "epoch": 0.0013917540604270835,
      "grad_norm": 11457.433394962418,
      "learning_rate": 1.6156185193868783e-07,
      "loss": 1.6999,
      "step": 383488
    },
    {
      "epoch": 0.0013918701947779203,
      "grad_norm": 11310.076038648016,
      "learning_rate": 1.6155510496144455e-07,
      "loss": 1.6655,
      "step": 383520
    },
    {
      "epoch": 0.0013919863291287568,
      "grad_norm": 9442.685634923997,
      "learning_rate": 1.6154835882941131e-07,
      "loss": 1.6824,
      "step": 383552
    },
    {
      "epoch": 0.0013921024634795936,
      "grad_norm": 9480.878651264344,
      "learning_rate": 1.6154161354241169e-07,
      "loss": 1.6953,
      "step": 383584
    },
    {
      "epoch": 0.0013922185978304303,
      "grad_norm": 10130.194864858227,
      "learning_rate": 1.6153486910026924e-07,
      "loss": 1.6809,
      "step": 383616
    },
    {
      "epoch": 0.001392334732181267,
      "grad_norm": 10957.074609584439,
      "learning_rate": 1.6152812550280764e-07,
      "loss": 1.6613,
      "step": 383648
    },
    {
      "epoch": 0.0013924508665321038,
      "grad_norm": 8835.521716344769,
      "learning_rate": 1.615213827498506e-07,
      "loss": 1.6703,
      "step": 383680
    },
    {
      "epoch": 0.0013925670008829404,
      "grad_norm": 8530.066236554088,
      "learning_rate": 1.6151464084122182e-07,
      "loss": 1.6654,
      "step": 383712
    },
    {
      "epoch": 0.0013926831352337771,
      "grad_norm": 12762.862688284318,
      "learning_rate": 1.6150789977674514e-07,
      "loss": 1.6528,
      "step": 383744
    },
    {
      "epoch": 0.0013927992695846139,
      "grad_norm": 8718.256477071549,
      "learning_rate": 1.6150115955624442e-07,
      "loss": 1.6757,
      "step": 383776
    },
    {
      "epoch": 0.0013929154039354506,
      "grad_norm": 9697.555774523806,
      "learning_rate": 1.6149442017954353e-07,
      "loss": 1.6789,
      "step": 383808
    },
    {
      "epoch": 0.0013930315382862872,
      "grad_norm": 9452.069932030761,
      "learning_rate": 1.6148768164646648e-07,
      "loss": 1.6743,
      "step": 383840
    },
    {
      "epoch": 0.001393147672637124,
      "grad_norm": 9208.026933062261,
      "learning_rate": 1.6148094395683726e-07,
      "loss": 1.6554,
      "step": 383872
    },
    {
      "epoch": 0.0013932638069879607,
      "grad_norm": 9380.458837391698,
      "learning_rate": 1.6147420711047987e-07,
      "loss": 1.6682,
      "step": 383904
    },
    {
      "epoch": 0.0013933799413387974,
      "grad_norm": 12030.36258805195,
      "learning_rate": 1.6146747110721853e-07,
      "loss": 1.6791,
      "step": 383936
    },
    {
      "epoch": 0.0013934960756896342,
      "grad_norm": 11886.38380669243,
      "learning_rate": 1.6146073594687734e-07,
      "loss": 1.6697,
      "step": 383968
    },
    {
      "epoch": 0.0013936122100404707,
      "grad_norm": 9644.678532745402,
      "learning_rate": 1.614540016292805e-07,
      "loss": 1.7032,
      "step": 384000
    },
    {
      "epoch": 0.0013937283443913075,
      "grad_norm": 12281.157925863505,
      "learning_rate": 1.614472681542523e-07,
      "loss": 1.7108,
      "step": 384032
    },
    {
      "epoch": 0.0013938444787421442,
      "grad_norm": 10094.151375920614,
      "learning_rate": 1.6144053552161705e-07,
      "loss": 1.6838,
      "step": 384064
    },
    {
      "epoch": 0.001393960613092981,
      "grad_norm": 9266.870561306012,
      "learning_rate": 1.6143380373119914e-07,
      "loss": 1.6833,
      "step": 384096
    },
    {
      "epoch": 0.0013940767474438175,
      "grad_norm": 8655.48392639025,
      "learning_rate": 1.6142707278282298e-07,
      "loss": 1.7011,
      "step": 384128
    },
    {
      "epoch": 0.0013941928817946543,
      "grad_norm": 9865.103547353165,
      "learning_rate": 1.61420342676313e-07,
      "loss": 1.6969,
      "step": 384160
    },
    {
      "epoch": 0.001394309016145491,
      "grad_norm": 9458.686801031103,
      "learning_rate": 1.6141361341149376e-07,
      "loss": 1.6993,
      "step": 384192
    },
    {
      "epoch": 0.0013944251504963278,
      "grad_norm": 13887.729836081922,
      "learning_rate": 1.6140688498818986e-07,
      "loss": 1.6803,
      "step": 384224
    },
    {
      "epoch": 0.0013945412848471645,
      "grad_norm": 8106.038489916022,
      "learning_rate": 1.6140015740622587e-07,
      "loss": 1.6771,
      "step": 384256
    },
    {
      "epoch": 0.001394657419198001,
      "grad_norm": 10649.605626500917,
      "learning_rate": 1.6139343066542648e-07,
      "loss": 1.6554,
      "step": 384288
    },
    {
      "epoch": 0.0013947735535488378,
      "grad_norm": 11632.54787224192,
      "learning_rate": 1.6138670476561646e-07,
      "loss": 1.658,
      "step": 384320
    },
    {
      "epoch": 0.0013948896878996746,
      "grad_norm": 10196.186542036194,
      "learning_rate": 1.6137997970662054e-07,
      "loss": 1.6774,
      "step": 384352
    },
    {
      "epoch": 0.0013950058222505113,
      "grad_norm": 9158.493544246237,
      "learning_rate": 1.6137325548826356e-07,
      "loss": 1.6595,
      "step": 384384
    },
    {
      "epoch": 0.0013951219566013479,
      "grad_norm": 21838.46990977161,
      "learning_rate": 1.613665321103704e-07,
      "loss": 1.6754,
      "step": 384416
    },
    {
      "epoch": 0.0013952380909521846,
      "grad_norm": 22185.565036753065,
      "learning_rate": 1.6135980957276596e-07,
      "loss": 1.6993,
      "step": 384448
    },
    {
      "epoch": 0.0013953542253030214,
      "grad_norm": 8676.159864824991,
      "learning_rate": 1.6135329791560706e-07,
      "loss": 1.6997,
      "step": 384480
    },
    {
      "epoch": 0.0013954703596538581,
      "grad_norm": 10441.331141190763,
      "learning_rate": 1.6134657703180974e-07,
      "loss": 1.6656,
      "step": 384512
    },
    {
      "epoch": 0.0013955864940046949,
      "grad_norm": 11144.843830220323,
      "learning_rate": 1.613398569877817e-07,
      "loss": 1.6849,
      "step": 384544
    },
    {
      "epoch": 0.0013957026283555314,
      "grad_norm": 13745.883165515412,
      "learning_rate": 1.6133313778334814e-07,
      "loss": 1.7007,
      "step": 384576
    },
    {
      "epoch": 0.0013958187627063682,
      "grad_norm": 9949.522199583254,
      "learning_rate": 1.613264194183342e-07,
      "loss": 1.6643,
      "step": 384608
    },
    {
      "epoch": 0.001395934897057205,
      "grad_norm": 8595.824800448181,
      "learning_rate": 1.6131970189256514e-07,
      "loss": 1.6733,
      "step": 384640
    },
    {
      "epoch": 0.0013960510314080417,
      "grad_norm": 9343.878423866612,
      "learning_rate": 1.6131298520586626e-07,
      "loss": 1.6835,
      "step": 384672
    },
    {
      "epoch": 0.0013961671657588782,
      "grad_norm": 8864.1773447963,
      "learning_rate": 1.6130626935806285e-07,
      "loss": 1.6834,
      "step": 384704
    },
    {
      "epoch": 0.001396283300109715,
      "grad_norm": 13592.072542478576,
      "learning_rate": 1.6129955434898032e-07,
      "loss": 1.678,
      "step": 384736
    },
    {
      "epoch": 0.0013963994344605517,
      "grad_norm": 9790.65135728977,
      "learning_rate": 1.6129284017844416e-07,
      "loss": 1.6694,
      "step": 384768
    },
    {
      "epoch": 0.0013965155688113885,
      "grad_norm": 11135.434073263601,
      "learning_rate": 1.6128612684627974e-07,
      "loss": 1.6748,
      "step": 384800
    },
    {
      "epoch": 0.0013966317031622252,
      "grad_norm": 9861.3704929893,
      "learning_rate": 1.6127941435231273e-07,
      "loss": 1.6583,
      "step": 384832
    },
    {
      "epoch": 0.0013967478375130618,
      "grad_norm": 8397.231567606077,
      "learning_rate": 1.6127270269636865e-07,
      "loss": 1.6848,
      "step": 384864
    },
    {
      "epoch": 0.0013968639718638985,
      "grad_norm": 9415.383900829534,
      "learning_rate": 1.6126599187827317e-07,
      "loss": 1.692,
      "step": 384896
    },
    {
      "epoch": 0.0013969801062147353,
      "grad_norm": 10575.016595731659,
      "learning_rate": 1.6125928189785195e-07,
      "loss": 1.6844,
      "step": 384928
    },
    {
      "epoch": 0.001397096240565572,
      "grad_norm": 10805.241690957218,
      "learning_rate": 1.612525727549308e-07,
      "loss": 1.6865,
      "step": 384960
    },
    {
      "epoch": 0.0013972123749164086,
      "grad_norm": 10910.1231890387,
      "learning_rate": 1.6124586444933544e-07,
      "loss": 1.6945,
      "step": 384992
    },
    {
      "epoch": 0.0013973285092672453,
      "grad_norm": 9333.599948572897,
      "learning_rate": 1.6123915698089175e-07,
      "loss": 1.7013,
      "step": 385024
    },
    {
      "epoch": 0.001397444643618082,
      "grad_norm": 8742.574906742293,
      "learning_rate": 1.6123245034942562e-07,
      "loss": 1.7007,
      "step": 385056
    },
    {
      "epoch": 0.0013975607779689188,
      "grad_norm": 9689.10191916671,
      "learning_rate": 1.6122574455476303e-07,
      "loss": 1.7156,
      "step": 385088
    },
    {
      "epoch": 0.0013976769123197556,
      "grad_norm": 8708.276178440829,
      "learning_rate": 1.6121903959672994e-07,
      "loss": 1.6989,
      "step": 385120
    },
    {
      "epoch": 0.0013977930466705921,
      "grad_norm": 11970.369083699967,
      "learning_rate": 1.6121233547515237e-07,
      "loss": 1.6692,
      "step": 385152
    },
    {
      "epoch": 0.0013979091810214289,
      "grad_norm": 10251.242071085826,
      "learning_rate": 1.612056321898565e-07,
      "loss": 1.6596,
      "step": 385184
    },
    {
      "epoch": 0.0013980253153722656,
      "grad_norm": 9925.985492634976,
      "learning_rate": 1.611989297406684e-07,
      "loss": 1.6725,
      "step": 385216
    },
    {
      "epoch": 0.0013981414497231024,
      "grad_norm": 12576.979128550704,
      "learning_rate": 1.611922281274143e-07,
      "loss": 1.6612,
      "step": 385248
    },
    {
      "epoch": 0.001398257584073939,
      "grad_norm": 8263.640602059118,
      "learning_rate": 1.611855273499205e-07,
      "loss": 1.678,
      "step": 385280
    },
    {
      "epoch": 0.0013983737184247757,
      "grad_norm": 11920.679175281919,
      "learning_rate": 1.6117882740801323e-07,
      "loss": 1.6868,
      "step": 385312
    },
    {
      "epoch": 0.0013984898527756124,
      "grad_norm": 9743.440460124955,
      "learning_rate": 1.6117212830151887e-07,
      "loss": 1.6987,
      "step": 385344
    },
    {
      "epoch": 0.0013986059871264492,
      "grad_norm": 9801.10820264729,
      "learning_rate": 1.6116543003026382e-07,
      "loss": 1.6643,
      "step": 385376
    },
    {
      "epoch": 0.001398722121477286,
      "grad_norm": 9740.115605063422,
      "learning_rate": 1.6115873259407455e-07,
      "loss": 1.665,
      "step": 385408
    },
    {
      "epoch": 0.0013988382558281225,
      "grad_norm": 9574.62813899318,
      "learning_rate": 1.6115203599277755e-07,
      "loss": 1.6809,
      "step": 385440
    },
    {
      "epoch": 0.0013989543901789592,
      "grad_norm": 22485.125750148698,
      "learning_rate": 1.6114534022619935e-07,
      "loss": 1.6656,
      "step": 385472
    },
    {
      "epoch": 0.001399070524529796,
      "grad_norm": 17667.631420198915,
      "learning_rate": 1.6113864529416656e-07,
      "loss": 1.6765,
      "step": 385504
    },
    {
      "epoch": 0.0013991866588806327,
      "grad_norm": 8161.061695637401,
      "learning_rate": 1.6113216037442982e-07,
      "loss": 1.6908,
      "step": 385536
    },
    {
      "epoch": 0.0013993027932314693,
      "grad_norm": 10219.908805855364,
      "learning_rate": 1.6112546708490183e-07,
      "loss": 1.6967,
      "step": 385568
    },
    {
      "epoch": 0.001399418927582306,
      "grad_norm": 9848.08407762647,
      "learning_rate": 1.611187746294048e-07,
      "loss": 1.6811,
      "step": 385600
    },
    {
      "epoch": 0.0013995350619331428,
      "grad_norm": 11243.538588896292,
      "learning_rate": 1.6111208300776555e-07,
      "loss": 1.6983,
      "step": 385632
    },
    {
      "epoch": 0.0013996511962839795,
      "grad_norm": 10258.723702293575,
      "learning_rate": 1.6110539221981085e-07,
      "loss": 1.7104,
      "step": 385664
    },
    {
      "epoch": 0.0013997673306348163,
      "grad_norm": 11847.701886863966,
      "learning_rate": 1.610987022653677e-07,
      "loss": 1.6578,
      "step": 385696
    },
    {
      "epoch": 0.0013998834649856528,
      "grad_norm": 9833.668898229185,
      "learning_rate": 1.6109201314426302e-07,
      "loss": 1.6788,
      "step": 385728
    },
    {
      "epoch": 0.0013999995993364896,
      "grad_norm": 10195.072731471806,
      "learning_rate": 1.6108532485632381e-07,
      "loss": 1.6859,
      "step": 385760
    },
    {
      "epoch": 0.0014001157336873263,
      "grad_norm": 10462.92569026465,
      "learning_rate": 1.6107863740137714e-07,
      "loss": 1.6948,
      "step": 385792
    },
    {
      "epoch": 0.001400231868038163,
      "grad_norm": 11066.913210105156,
      "learning_rate": 1.6107195077925012e-07,
      "loss": 1.6855,
      "step": 385824
    },
    {
      "epoch": 0.0014003480023889996,
      "grad_norm": 11474.08384142281,
      "learning_rate": 1.6106526498976988e-07,
      "loss": 1.6887,
      "step": 385856
    },
    {
      "epoch": 0.0014004641367398364,
      "grad_norm": 8519.73262491259,
      "learning_rate": 1.6105858003276363e-07,
      "loss": 1.6976,
      "step": 385888
    },
    {
      "epoch": 0.0014005802710906731,
      "grad_norm": 9990.183381700257,
      "learning_rate": 1.6105189590805867e-07,
      "loss": 1.6737,
      "step": 385920
    },
    {
      "epoch": 0.00140069640544151,
      "grad_norm": 10114.350794786584,
      "learning_rate": 1.6104521261548228e-07,
      "loss": 1.6994,
      "step": 385952
    },
    {
      "epoch": 0.0014008125397923466,
      "grad_norm": 9901.337283417832,
      "learning_rate": 1.6103853015486177e-07,
      "loss": 1.6832,
      "step": 385984
    },
    {
      "epoch": 0.0014009286741431832,
      "grad_norm": 9372.572752451699,
      "learning_rate": 1.6103184852602463e-07,
      "loss": 1.671,
      "step": 386016
    },
    {
      "epoch": 0.00140104480849402,
      "grad_norm": 12191.422558503991,
      "learning_rate": 1.6102516772879824e-07,
      "loss": 1.6617,
      "step": 386048
    },
    {
      "epoch": 0.0014011609428448567,
      "grad_norm": 9025.216894900643,
      "learning_rate": 1.6101848776301018e-07,
      "loss": 1.6715,
      "step": 386080
    },
    {
      "epoch": 0.0014012770771956934,
      "grad_norm": 10534.006455285662,
      "learning_rate": 1.6101180862848794e-07,
      "loss": 1.6807,
      "step": 386112
    },
    {
      "epoch": 0.00140139321154653,
      "grad_norm": 10215.001027900094,
      "learning_rate": 1.610051303250592e-07,
      "loss": 1.6658,
      "step": 386144
    },
    {
      "epoch": 0.0014015093458973667,
      "grad_norm": 9619.361517273379,
      "learning_rate": 1.6099845285255152e-07,
      "loss": 1.7048,
      "step": 386176
    },
    {
      "epoch": 0.0014016254802482035,
      "grad_norm": 8575.935400876106,
      "learning_rate": 1.609917762107927e-07,
      "loss": 1.7089,
      "step": 386208
    },
    {
      "epoch": 0.0014017416145990402,
      "grad_norm": 9093.231218879238,
      "learning_rate": 1.6098510039961045e-07,
      "loss": 1.6811,
      "step": 386240
    },
    {
      "epoch": 0.001401857748949877,
      "grad_norm": 10285.226103494273,
      "learning_rate": 1.609784254188326e-07,
      "loss": 1.6604,
      "step": 386272
    },
    {
      "epoch": 0.0014019738833007135,
      "grad_norm": 9545.74638255176,
      "learning_rate": 1.6097175126828698e-07,
      "loss": 1.6722,
      "step": 386304
    },
    {
      "epoch": 0.0014020900176515503,
      "grad_norm": 11799.258281773478,
      "learning_rate": 1.6096507794780155e-07,
      "loss": 1.6694,
      "step": 386336
    },
    {
      "epoch": 0.001402206152002387,
      "grad_norm": 9549.188447192777,
      "learning_rate": 1.6095840545720418e-07,
      "loss": 1.6809,
      "step": 386368
    },
    {
      "epoch": 0.0014023222863532238,
      "grad_norm": 8198.433508908882,
      "learning_rate": 1.6095173379632297e-07,
      "loss": 1.6895,
      "step": 386400
    },
    {
      "epoch": 0.0014024384207040603,
      "grad_norm": 11392.786489704791,
      "learning_rate": 1.6094506296498594e-07,
      "loss": 1.6864,
      "step": 386432
    },
    {
      "epoch": 0.001402554555054897,
      "grad_norm": 9061.09728454562,
      "learning_rate": 1.6093839296302117e-07,
      "loss": 1.6773,
      "step": 386464
    },
    {
      "epoch": 0.0014026706894057338,
      "grad_norm": 12205.972472523441,
      "learning_rate": 1.6093172379025683e-07,
      "loss": 1.6572,
      "step": 386496
    },
    {
      "epoch": 0.0014027868237565706,
      "grad_norm": 10257.122403481397,
      "learning_rate": 1.6092505544652118e-07,
      "loss": 1.6771,
      "step": 386528
    },
    {
      "epoch": 0.0014029029581074074,
      "grad_norm": 30494.461136409675,
      "learning_rate": 1.6091838793164242e-07,
      "loss": 1.671,
      "step": 386560
    },
    {
      "epoch": 0.001403019092458244,
      "grad_norm": 19799.274734191655,
      "learning_rate": 1.6091172124544884e-07,
      "loss": 1.6702,
      "step": 386592
    },
    {
      "epoch": 0.0014031352268090806,
      "grad_norm": 19182.084141198004,
      "learning_rate": 1.6090505538776886e-07,
      "loss": 1.689,
      "step": 386624
    },
    {
      "epoch": 0.0014032513611599174,
      "grad_norm": 24968.89136505664,
      "learning_rate": 1.6089839035843083e-07,
      "loss": 1.6916,
      "step": 386656
    },
    {
      "epoch": 0.0014033674955107542,
      "grad_norm": 19356.312872032213,
      "learning_rate": 1.6089172615726323e-07,
      "loss": 1.6862,
      "step": 386688
    },
    {
      "epoch": 0.0014034836298615907,
      "grad_norm": 8558.454533383934,
      "learning_rate": 1.6088527100197462e-07,
      "loss": 1.7126,
      "step": 386720
    },
    {
      "epoch": 0.0014035997642124274,
      "grad_norm": 9055.111263811174,
      "learning_rate": 1.6087860843076644e-07,
      "loss": 1.7235,
      "step": 386752
    },
    {
      "epoch": 0.0014037158985632642,
      "grad_norm": 9834.918810036004,
      "learning_rate": 1.608719466872197e-07,
      "loss": 1.6794,
      "step": 386784
    },
    {
      "epoch": 0.001403832032914101,
      "grad_norm": 10618.540577687689,
      "learning_rate": 1.6086528577116307e-07,
      "loss": 1.7,
      "step": 386816
    },
    {
      "epoch": 0.0014039481672649377,
      "grad_norm": 9107.69323154881,
      "learning_rate": 1.6085862568242526e-07,
      "loss": 1.6833,
      "step": 386848
    },
    {
      "epoch": 0.0014040643016157742,
      "grad_norm": 9772.843393813288,
      "learning_rate": 1.60851966420835e-07,
      "loss": 1.6751,
      "step": 386880
    },
    {
      "epoch": 0.001404180435966611,
      "grad_norm": 10618.223956952499,
      "learning_rate": 1.6084530798622106e-07,
      "loss": 1.674,
      "step": 386912
    },
    {
      "epoch": 0.0014042965703174478,
      "grad_norm": 7749.725543527333,
      "learning_rate": 1.608386503784124e-07,
      "loss": 1.67,
      "step": 386944
    },
    {
      "epoch": 0.0014044127046682845,
      "grad_norm": 11945.170404812146,
      "learning_rate": 1.608319935972378e-07,
      "loss": 1.685,
      "step": 386976
    },
    {
      "epoch": 0.001404528839019121,
      "grad_norm": 9253.530461396882,
      "learning_rate": 1.6082533764252628e-07,
      "loss": 1.6596,
      "step": 387008
    },
    {
      "epoch": 0.0014046449733699578,
      "grad_norm": 11577.052992882083,
      "learning_rate": 1.6081868251410685e-07,
      "loss": 1.6901,
      "step": 387040
    },
    {
      "epoch": 0.0014047611077207946,
      "grad_norm": 8774.7795413902,
      "learning_rate": 1.608120282118085e-07,
      "loss": 1.7071,
      "step": 387072
    },
    {
      "epoch": 0.0014048772420716313,
      "grad_norm": 13697.201465992972,
      "learning_rate": 1.6080537473546036e-07,
      "loss": 1.7019,
      "step": 387104
    },
    {
      "epoch": 0.001404993376422468,
      "grad_norm": 9957.873266917992,
      "learning_rate": 1.6079872208489158e-07,
      "loss": 1.671,
      "step": 387136
    },
    {
      "epoch": 0.0014051095107733046,
      "grad_norm": 9273.33726335886,
      "learning_rate": 1.607920702599314e-07,
      "loss": 1.6884,
      "step": 387168
    },
    {
      "epoch": 0.0014052256451241414,
      "grad_norm": 7762.498180354054,
      "learning_rate": 1.6078541926040898e-07,
      "loss": 1.6978,
      "step": 387200
    },
    {
      "epoch": 0.001405341779474978,
      "grad_norm": 7289.041226389106,
      "learning_rate": 1.607787690861537e-07,
      "loss": 1.6817,
      "step": 387232
    },
    {
      "epoch": 0.0014054579138258149,
      "grad_norm": 9557.519343427979,
      "learning_rate": 1.6077211973699486e-07,
      "loss": 1.7164,
      "step": 387264
    },
    {
      "epoch": 0.0014055740481766514,
      "grad_norm": 10660.962433101433,
      "learning_rate": 1.6076547121276188e-07,
      "loss": 1.7104,
      "step": 387296
    },
    {
      "epoch": 0.0014056901825274882,
      "grad_norm": 11401.128891473862,
      "learning_rate": 1.6075882351328416e-07,
      "loss": 1.6807,
      "step": 387328
    },
    {
      "epoch": 0.001405806316878325,
      "grad_norm": 8998.434308256075,
      "learning_rate": 1.6075217663839125e-07,
      "loss": 1.6796,
      "step": 387360
    },
    {
      "epoch": 0.0014059224512291617,
      "grad_norm": 10292.253203259235,
      "learning_rate": 1.6074553058791264e-07,
      "loss": 1.6851,
      "step": 387392
    },
    {
      "epoch": 0.0014060385855799984,
      "grad_norm": 12506.71803472038,
      "learning_rate": 1.60738885361678e-07,
      "loss": 1.686,
      "step": 387424
    },
    {
      "epoch": 0.001406154719930835,
      "grad_norm": 10358.738629775346,
      "learning_rate": 1.6073224095951692e-07,
      "loss": 1.6923,
      "step": 387456
    },
    {
      "epoch": 0.0014062708542816717,
      "grad_norm": 10063.59200285862,
      "learning_rate": 1.6072559738125906e-07,
      "loss": 1.6924,
      "step": 387488
    },
    {
      "epoch": 0.0014063869886325085,
      "grad_norm": 10289.337393632304,
      "learning_rate": 1.6071895462673423e-07,
      "loss": 1.6987,
      "step": 387520
    },
    {
      "epoch": 0.0014065031229833452,
      "grad_norm": 15005.724374384597,
      "learning_rate": 1.607123126957722e-07,
      "loss": 1.6898,
      "step": 387552
    },
    {
      "epoch": 0.0014066192573341818,
      "grad_norm": 10175.8903295977,
      "learning_rate": 1.6070567158820277e-07,
      "loss": 1.6948,
      "step": 387584
    },
    {
      "epoch": 0.0014067353916850185,
      "grad_norm": 9052.406972733826,
      "learning_rate": 1.606990313038559e-07,
      "loss": 1.6913,
      "step": 387616
    },
    {
      "epoch": 0.0014068515260358553,
      "grad_norm": 9193.885794374431,
      "learning_rate": 1.6069239184256147e-07,
      "loss": 1.6813,
      "step": 387648
    },
    {
      "epoch": 0.001406967660386692,
      "grad_norm": 8099.294537180383,
      "learning_rate": 1.6068575320414946e-07,
      "loss": 1.6948,
      "step": 387680
    },
    {
      "epoch": 0.0014070837947375288,
      "grad_norm": 18239.91973666551,
      "learning_rate": 1.6067911538844997e-07,
      "loss": 1.6961,
      "step": 387712
    },
    {
      "epoch": 0.0014071999290883653,
      "grad_norm": 22034.945699955788,
      "learning_rate": 1.6067247839529306e-07,
      "loss": 1.6821,
      "step": 387744
    },
    {
      "epoch": 0.001407316063439202,
      "grad_norm": 25476.334116194976,
      "learning_rate": 1.606658422245088e-07,
      "loss": 1.6679,
      "step": 387776
    },
    {
      "epoch": 0.0014074321977900388,
      "grad_norm": 9841.95001003358,
      "learning_rate": 1.6065941421812687e-07,
      "loss": 1.6805,
      "step": 387808
    },
    {
      "epoch": 0.0014075483321408756,
      "grad_norm": 10071.96644156443,
      "learning_rate": 1.6065277966589267e-07,
      "loss": 1.6927,
      "step": 387840
    },
    {
      "epoch": 0.001407664466491712,
      "grad_norm": 8527.547126812024,
      "learning_rate": 1.6064614593552718e-07,
      "loss": 1.6592,
      "step": 387872
    },
    {
      "epoch": 0.0014077806008425489,
      "grad_norm": 7963.2512204501,
      "learning_rate": 1.606395130268607e-07,
      "loss": 1.6694,
      "step": 387904
    },
    {
      "epoch": 0.0014078967351933856,
      "grad_norm": 9401.198008764628,
      "learning_rate": 1.6063288093972368e-07,
      "loss": 1.6854,
      "step": 387936
    },
    {
      "epoch": 0.0014080128695442224,
      "grad_norm": 8553.976735998292,
      "learning_rate": 1.6062624967394648e-07,
      "loss": 1.6889,
      "step": 387968
    },
    {
      "epoch": 0.0014081290038950591,
      "grad_norm": 11429.180723043975,
      "learning_rate": 1.606196192293596e-07,
      "loss": 1.6704,
      "step": 388000
    },
    {
      "epoch": 0.0014082451382458957,
      "grad_norm": 9063.70796087341,
      "learning_rate": 1.6061298960579357e-07,
      "loss": 1.6643,
      "step": 388032
    },
    {
      "epoch": 0.0014083612725967324,
      "grad_norm": 10808.146187020233,
      "learning_rate": 1.6060636080307894e-07,
      "loss": 1.6789,
      "step": 388064
    },
    {
      "epoch": 0.0014084774069475692,
      "grad_norm": 10764.123559305699,
      "learning_rate": 1.6059973282104632e-07,
      "loss": 1.6583,
      "step": 388096
    },
    {
      "epoch": 0.001408593541298406,
      "grad_norm": 10007.3311127393,
      "learning_rate": 1.6059310565952645e-07,
      "loss": 1.6745,
      "step": 388128
    },
    {
      "epoch": 0.0014087096756492425,
      "grad_norm": 9268.144582385408,
      "learning_rate": 1.6058647931835002e-07,
      "loss": 1.6803,
      "step": 388160
    },
    {
      "epoch": 0.0014088258100000792,
      "grad_norm": 10999.049958973728,
      "learning_rate": 1.6057985379734782e-07,
      "loss": 1.6685,
      "step": 388192
    },
    {
      "epoch": 0.001408941944350916,
      "grad_norm": 9986.973315274252,
      "learning_rate": 1.605732290963506e-07,
      "loss": 1.6651,
      "step": 388224
    },
    {
      "epoch": 0.0014090580787017527,
      "grad_norm": 8365.830263637914,
      "learning_rate": 1.6056660521518928e-07,
      "loss": 1.669,
      "step": 388256
    },
    {
      "epoch": 0.0014091742130525895,
      "grad_norm": 8355.474373128076,
      "learning_rate": 1.6055998215369478e-07,
      "loss": 1.6775,
      "step": 388288
    },
    {
      "epoch": 0.001409290347403426,
      "grad_norm": 10636.394313864073,
      "learning_rate": 1.60553359911698e-07,
      "loss": 1.6775,
      "step": 388320
    },
    {
      "epoch": 0.0014094064817542628,
      "grad_norm": 10197.033294051756,
      "learning_rate": 1.605467384890301e-07,
      "loss": 1.6992,
      "step": 388352
    },
    {
      "epoch": 0.0014095226161050995,
      "grad_norm": 9992.869657911086,
      "learning_rate": 1.6054011788552197e-07,
      "loss": 1.6986,
      "step": 388384
    },
    {
      "epoch": 0.0014096387504559363,
      "grad_norm": 8793.865361716655,
      "learning_rate": 1.6053349810100485e-07,
      "loss": 1.6774,
      "step": 388416
    },
    {
      "epoch": 0.0014097548848067728,
      "grad_norm": 10330.612179343487,
      "learning_rate": 1.6052687913530986e-07,
      "loss": 1.6758,
      "step": 388448
    },
    {
      "epoch": 0.0014098710191576096,
      "grad_norm": 9320.348920507215,
      "learning_rate": 1.6052026098826815e-07,
      "loss": 1.6909,
      "step": 388480
    },
    {
      "epoch": 0.0014099871535084463,
      "grad_norm": 9167.229897848096,
      "learning_rate": 1.605136436597111e-07,
      "loss": 1.6957,
      "step": 388512
    },
    {
      "epoch": 0.001410103287859283,
      "grad_norm": 9128.363708792502,
      "learning_rate": 1.605070271494699e-07,
      "loss": 1.6948,
      "step": 388544
    },
    {
      "epoch": 0.0014102194222101198,
      "grad_norm": 9370.412157424027,
      "learning_rate": 1.6050041145737595e-07,
      "loss": 1.6945,
      "step": 388576
    },
    {
      "epoch": 0.0014103355565609564,
      "grad_norm": 11227.865870235537,
      "learning_rate": 1.604937965832607e-07,
      "loss": 1.6832,
      "step": 388608
    },
    {
      "epoch": 0.0014104516909117931,
      "grad_norm": 9168.038830633299,
      "learning_rate": 1.6048718252695555e-07,
      "loss": 1.6558,
      "step": 388640
    },
    {
      "epoch": 0.0014105678252626299,
      "grad_norm": 9164.664205523299,
      "learning_rate": 1.60480569288292e-07,
      "loss": 1.6598,
      "step": 388672
    },
    {
      "epoch": 0.0014106839596134666,
      "grad_norm": 10831.814252469436,
      "learning_rate": 1.604739568671016e-07,
      "loss": 1.6721,
      "step": 388704
    },
    {
      "epoch": 0.0014108000939643032,
      "grad_norm": 8644.336990191903,
      "learning_rate": 1.6046734526321598e-07,
      "loss": 1.6602,
      "step": 388736
    },
    {
      "epoch": 0.00141091622831514,
      "grad_norm": 10195.796977186237,
      "learning_rate": 1.6046073447646676e-07,
      "loss": 1.661,
      "step": 388768
    },
    {
      "epoch": 0.0014110323626659767,
      "grad_norm": 16368.197946017148,
      "learning_rate": 1.6045412450668565e-07,
      "loss": 1.6806,
      "step": 388800
    },
    {
      "epoch": 0.0014111484970168134,
      "grad_norm": 18862.170394734538,
      "learning_rate": 1.6044751535370438e-07,
      "loss": 1.699,
      "step": 388832
    },
    {
      "epoch": 0.0014112646313676502,
      "grad_norm": 19326.795078336192,
      "learning_rate": 1.6044090701735476e-07,
      "loss": 1.6876,
      "step": 388864
    },
    {
      "epoch": 0.0014113807657184867,
      "grad_norm": 8267.194929357842,
      "learning_rate": 1.6043450597010816e-07,
      "loss": 1.6723,
      "step": 388896
    },
    {
      "epoch": 0.0014114969000693235,
      "grad_norm": 15269.283283769411,
      "learning_rate": 1.604278992410107e-07,
      "loss": 1.6842,
      "step": 388928
    },
    {
      "epoch": 0.0014116130344201602,
      "grad_norm": 10340.97036065765,
      "learning_rate": 1.6042129332804578e-07,
      "loss": 1.6576,
      "step": 388960
    },
    {
      "epoch": 0.001411729168770997,
      "grad_norm": 8284.750810977963,
      "learning_rate": 1.6041468823104542e-07,
      "loss": 1.6671,
      "step": 388992
    },
    {
      "epoch": 0.0014118453031218335,
      "grad_norm": 10264.287505716116,
      "learning_rate": 1.6040808394984165e-07,
      "loss": 1.6811,
      "step": 389024
    },
    {
      "epoch": 0.0014119614374726703,
      "grad_norm": 13249.446328054619,
      "learning_rate": 1.6040148048426655e-07,
      "loss": 1.6862,
      "step": 389056
    },
    {
      "epoch": 0.001412077571823507,
      "grad_norm": 12383.410192673098,
      "learning_rate": 1.6039487783415223e-07,
      "loss": 1.6694,
      "step": 389088
    },
    {
      "epoch": 0.0014121937061743438,
      "grad_norm": 11342.530405513577,
      "learning_rate": 1.6038827599933089e-07,
      "loss": 1.6698,
      "step": 389120
    },
    {
      "epoch": 0.0014123098405251805,
      "grad_norm": 12434.259286342713,
      "learning_rate": 1.6038167497963473e-07,
      "loss": 1.6709,
      "step": 389152
    },
    {
      "epoch": 0.001412425974876017,
      "grad_norm": 16386.286461550706,
      "learning_rate": 1.6037507477489606e-07,
      "loss": 1.6546,
      "step": 389184
    },
    {
      "epoch": 0.0014125421092268538,
      "grad_norm": 9585.582089784637,
      "learning_rate": 1.6036847538494717e-07,
      "loss": 1.6762,
      "step": 389216
    },
    {
      "epoch": 0.0014126582435776906,
      "grad_norm": 11337.289976003965,
      "learning_rate": 1.6036187680962045e-07,
      "loss": 1.6776,
      "step": 389248
    },
    {
      "epoch": 0.0014127743779285273,
      "grad_norm": 11256.676774252692,
      "learning_rate": 1.6035527904874835e-07,
      "loss": 1.6758,
      "step": 389280
    },
    {
      "epoch": 0.0014128905122793639,
      "grad_norm": 10505.92099722818,
      "learning_rate": 1.6034868210216325e-07,
      "loss": 1.6737,
      "step": 389312
    },
    {
      "epoch": 0.0014130066466302006,
      "grad_norm": 8933.51621703347,
      "learning_rate": 1.6034208596969776e-07,
      "loss": 1.6943,
      "step": 389344
    },
    {
      "epoch": 0.0014131227809810374,
      "grad_norm": 10065.869460707307,
      "learning_rate": 1.6033549065118441e-07,
      "loss": 1.7078,
      "step": 389376
    },
    {
      "epoch": 0.0014132389153318741,
      "grad_norm": 9688.83832045927,
      "learning_rate": 1.603288961464558e-07,
      "loss": 1.6999,
      "step": 389408
    },
    {
      "epoch": 0.0014133550496827109,
      "grad_norm": 10179.680938025514,
      "learning_rate": 1.603223024553446e-07,
      "loss": 1.7203,
      "step": 389440
    },
    {
      "epoch": 0.0014134711840335474,
      "grad_norm": 11047.03797404535,
      "learning_rate": 1.6031570957768353e-07,
      "loss": 1.7091,
      "step": 389472
    },
    {
      "epoch": 0.0014135873183843842,
      "grad_norm": 11153.283821368486,
      "learning_rate": 1.6030911751330534e-07,
      "loss": 1.6685,
      "step": 389504
    },
    {
      "epoch": 0.001413703452735221,
      "grad_norm": 10107.900078651352,
      "learning_rate": 1.6030252626204283e-07,
      "loss": 1.6651,
      "step": 389536
    },
    {
      "epoch": 0.0014138195870860577,
      "grad_norm": 9647.110033580006,
      "learning_rate": 1.6029593582372887e-07,
      "loss": 1.6729,
      "step": 389568
    },
    {
      "epoch": 0.0014139357214368942,
      "grad_norm": 9788.867554523353,
      "learning_rate": 1.6028934619819634e-07,
      "loss": 1.6863,
      "step": 389600
    },
    {
      "epoch": 0.001414051855787731,
      "grad_norm": 8230.419794882884,
      "learning_rate": 1.602827573852782e-07,
      "loss": 1.6683,
      "step": 389632
    },
    {
      "epoch": 0.0014141679901385677,
      "grad_norm": 8518.31626555389,
      "learning_rate": 1.6027616938480747e-07,
      "loss": 1.6837,
      "step": 389664
    },
    {
      "epoch": 0.0014142841244894045,
      "grad_norm": 9149.94513644754,
      "learning_rate": 1.6026958219661716e-07,
      "loss": 1.6901,
      "step": 389696
    },
    {
      "epoch": 0.0014144002588402412,
      "grad_norm": 11208.482323668979,
      "learning_rate": 1.6026299582054037e-07,
      "loss": 1.6819,
      "step": 389728
    },
    {
      "epoch": 0.0014145163931910778,
      "grad_norm": 10541.045868413627,
      "learning_rate": 1.6025641025641027e-07,
      "loss": 1.6626,
      "step": 389760
    },
    {
      "epoch": 0.0014146325275419145,
      "grad_norm": 9508.690130612102,
      "learning_rate": 1.6024982550406e-07,
      "loss": 1.6698,
      "step": 389792
    },
    {
      "epoch": 0.0014147486618927513,
      "grad_norm": 10810.229229761966,
      "learning_rate": 1.6024324156332288e-07,
      "loss": 1.672,
      "step": 389824
    },
    {
      "epoch": 0.001414864796243588,
      "grad_norm": 8217.767215004329,
      "learning_rate": 1.6023665843403213e-07,
      "loss": 1.6663,
      "step": 389856
    },
    {
      "epoch": 0.0014149809305944246,
      "grad_norm": 15897.541193530527,
      "learning_rate": 1.6023007611602108e-07,
      "loss": 1.6849,
      "step": 389888
    },
    {
      "epoch": 0.0014150970649452613,
      "grad_norm": 20932.828571409074,
      "learning_rate": 1.6022349460912313e-07,
      "loss": 1.6883,
      "step": 389920
    },
    {
      "epoch": 0.001415213199296098,
      "grad_norm": 8973.444377718068,
      "learning_rate": 1.6021711954764682e-07,
      "loss": 1.6756,
      "step": 389952
    },
    {
      "epoch": 0.0014153293336469348,
      "grad_norm": 12326.815160454058,
      "learning_rate": 1.6021053963714107e-07,
      "loss": 1.6886,
      "step": 389984
    },
    {
      "epoch": 0.0014154454679977716,
      "grad_norm": 10355.020424895356,
      "learning_rate": 1.6020396053725406e-07,
      "loss": 1.6916,
      "step": 390016
    },
    {
      "epoch": 0.0014155616023486081,
      "grad_norm": 10004.300675209637,
      "learning_rate": 1.6019738224781937e-07,
      "loss": 1.6582,
      "step": 390048
    },
    {
      "epoch": 0.0014156777366994449,
      "grad_norm": 10874.11366503036,
      "learning_rate": 1.601908047686706e-07,
      "loss": 1.6756,
      "step": 390080
    },
    {
      "epoch": 0.0014157938710502816,
      "grad_norm": 11374.520121745794,
      "learning_rate": 1.6018422809964144e-07,
      "loss": 1.6811,
      "step": 390112
    },
    {
      "epoch": 0.0014159100054011184,
      "grad_norm": 11250.909829875982,
      "learning_rate": 1.6017765224056556e-07,
      "loss": 1.6851,
      "step": 390144
    },
    {
      "epoch": 0.001416026139751955,
      "grad_norm": 12313.838069424171,
      "learning_rate": 1.601710771912768e-07,
      "loss": 1.6727,
      "step": 390176
    },
    {
      "epoch": 0.0014161422741027917,
      "grad_norm": 11533.458111078393,
      "learning_rate": 1.601645029516089e-07,
      "loss": 1.6792,
      "step": 390208
    },
    {
      "epoch": 0.0014162584084536284,
      "grad_norm": 8388.360268848734,
      "learning_rate": 1.6015792952139577e-07,
      "loss": 1.6939,
      "step": 390240
    },
    {
      "epoch": 0.0014163745428044652,
      "grad_norm": 12456.56373162358,
      "learning_rate": 1.6015135690047126e-07,
      "loss": 1.6805,
      "step": 390272
    },
    {
      "epoch": 0.001416490677155302,
      "grad_norm": 9253.128984294988,
      "learning_rate": 1.601447850886694e-07,
      "loss": 1.6982,
      "step": 390304
    },
    {
      "epoch": 0.0014166068115061385,
      "grad_norm": 12734.088110265297,
      "learning_rate": 1.6013821408582411e-07,
      "loss": 1.7053,
      "step": 390336
    },
    {
      "epoch": 0.0014167229458569752,
      "grad_norm": 10566.717749613643,
      "learning_rate": 1.6013164389176947e-07,
      "loss": 1.6766,
      "step": 390368
    },
    {
      "epoch": 0.001416839080207812,
      "grad_norm": 9107.707505184826,
      "learning_rate": 1.601250745063396e-07,
      "loss": 1.6665,
      "step": 390400
    },
    {
      "epoch": 0.0014169552145586487,
      "grad_norm": 9121.461615333368,
      "learning_rate": 1.6011850592936864e-07,
      "loss": 1.6807,
      "step": 390432
    },
    {
      "epoch": 0.0014170713489094853,
      "grad_norm": 8938.044640747774,
      "learning_rate": 1.6011193816069076e-07,
      "loss": 1.6885,
      "step": 390464
    },
    {
      "epoch": 0.001417187483260322,
      "grad_norm": 9646.949362363213,
      "learning_rate": 1.601053712001402e-07,
      "loss": 1.6877,
      "step": 390496
    },
    {
      "epoch": 0.0014173036176111588,
      "grad_norm": 8194.584675259632,
      "learning_rate": 1.6009880504755128e-07,
      "loss": 1.7041,
      "step": 390528
    },
    {
      "epoch": 0.0014174197519619955,
      "grad_norm": 9203.837786488852,
      "learning_rate": 1.6009223970275832e-07,
      "loss": 1.704,
      "step": 390560
    },
    {
      "epoch": 0.0014175358863128323,
      "grad_norm": 8478.577239136293,
      "learning_rate": 1.600856751655957e-07,
      "loss": 1.6993,
      "step": 390592
    },
    {
      "epoch": 0.0014176520206636688,
      "grad_norm": 9994.319986872544,
      "learning_rate": 1.6007911143589786e-07,
      "loss": 1.6684,
      "step": 390624
    },
    {
      "epoch": 0.0014177681550145056,
      "grad_norm": 8576.286842217907,
      "learning_rate": 1.6007254851349923e-07,
      "loss": 1.6683,
      "step": 390656
    },
    {
      "epoch": 0.0014178842893653423,
      "grad_norm": 9883.520223078414,
      "learning_rate": 1.6006598639823437e-07,
      "loss": 1.6754,
      "step": 390688
    },
    {
      "epoch": 0.001418000423716179,
      "grad_norm": 8525.794977595931,
      "learning_rate": 1.6005942508993788e-07,
      "loss": 1.6689,
      "step": 390720
    },
    {
      "epoch": 0.0014181165580670156,
      "grad_norm": 10859.201812288047,
      "learning_rate": 1.6005286458844434e-07,
      "loss": 1.6766,
      "step": 390752
    },
    {
      "epoch": 0.0014182326924178524,
      "grad_norm": 8058.421061225332,
      "learning_rate": 1.6004630489358847e-07,
      "loss": 1.6744,
      "step": 390784
    },
    {
      "epoch": 0.0014183488267686891,
      "grad_norm": 8731.2104544559,
      "learning_rate": 1.6003974600520489e-07,
      "loss": 1.6594,
      "step": 390816
    },
    {
      "epoch": 0.001418464961119526,
      "grad_norm": 10753.810301469894,
      "learning_rate": 1.6003318792312846e-07,
      "loss": 1.6655,
      "step": 390848
    },
    {
      "epoch": 0.0014185810954703627,
      "grad_norm": 9758.26009081537,
      "learning_rate": 1.6002663064719394e-07,
      "loss": 1.6754,
      "step": 390880
    },
    {
      "epoch": 0.0014186972298211992,
      "grad_norm": 9623.308994311676,
      "learning_rate": 1.6002007417723617e-07,
      "loss": 1.6654,
      "step": 390912
    },
    {
      "epoch": 0.001418813364172036,
      "grad_norm": 18363.646261023434,
      "learning_rate": 1.6001351851309012e-07,
      "loss": 1.6722,
      "step": 390944
    },
    {
      "epoch": 0.0014189294985228727,
      "grad_norm": 17288.130957393863,
      "learning_rate": 1.6000696365459067e-07,
      "loss": 1.6895,
      "step": 390976
    },
    {
      "epoch": 0.0014190456328737095,
      "grad_norm": 20891.434417004497,
      "learning_rate": 1.600004096015729e-07,
      "loss": 1.6921,
      "step": 391008
    },
    {
      "epoch": 0.001419161767224546,
      "grad_norm": 18115.641197594967,
      "learning_rate": 1.5999385635387176e-07,
      "loss": 1.6863,
      "step": 391040
    },
    {
      "epoch": 0.0014192779015753827,
      "grad_norm": 8377.58652596319,
      "learning_rate": 1.5998750866296637e-07,
      "loss": 1.695,
      "step": 391072
    },
    {
      "epoch": 0.0014193940359262195,
      "grad_norm": 12860.767939746056,
      "learning_rate": 1.599809570002506e-07,
      "loss": 1.7024,
      "step": 391104
    },
    {
      "epoch": 0.0014195101702770563,
      "grad_norm": 9725.141232907623,
      "learning_rate": 1.599744061423621e-07,
      "loss": 1.6782,
      "step": 391136
    },
    {
      "epoch": 0.001419626304627893,
      "grad_norm": 9170.926016493646,
      "learning_rate": 1.5996785608913603e-07,
      "loss": 1.7065,
      "step": 391168
    },
    {
      "epoch": 0.0014197424389787295,
      "grad_norm": 7676.717267165699,
      "learning_rate": 1.5996130684040778e-07,
      "loss": 1.6995,
      "step": 391200
    },
    {
      "epoch": 0.0014198585733295663,
      "grad_norm": 8883.862898536876,
      "learning_rate": 1.599547583960126e-07,
      "loss": 1.6892,
      "step": 391232
    },
    {
      "epoch": 0.001419974707680403,
      "grad_norm": 9500.63345256515,
      "learning_rate": 1.5994821075578592e-07,
      "loss": 1.6626,
      "step": 391264
    },
    {
      "epoch": 0.0014200908420312398,
      "grad_norm": 12047.72277237487,
      "learning_rate": 1.5994166391956312e-07,
      "loss": 1.6744,
      "step": 391296
    },
    {
      "epoch": 0.0014202069763820763,
      "grad_norm": 12955.47637101778,
      "learning_rate": 1.5993511788717965e-07,
      "loss": 1.6826,
      "step": 391328
    },
    {
      "epoch": 0.001420323110732913,
      "grad_norm": 13381.736060765808,
      "learning_rate": 1.5992857265847112e-07,
      "loss": 1.6682,
      "step": 391360
    },
    {
      "epoch": 0.0014204392450837499,
      "grad_norm": 8464.991199050357,
      "learning_rate": 1.5992202823327302e-07,
      "loss": 1.6891,
      "step": 391392
    },
    {
      "epoch": 0.0014205553794345866,
      "grad_norm": 8886.965398829907,
      "learning_rate": 1.59915484611421e-07,
      "loss": 1.6766,
      "step": 391424
    },
    {
      "epoch": 0.0014206715137854234,
      "grad_norm": 9165.143424955226,
      "learning_rate": 1.5990894179275066e-07,
      "loss": 1.6906,
      "step": 391456
    },
    {
      "epoch": 0.00142078764813626,
      "grad_norm": 9852.36986719439,
      "learning_rate": 1.5990239977709777e-07,
      "loss": 1.6836,
      "step": 391488
    },
    {
      "epoch": 0.0014209037824870967,
      "grad_norm": 10576.33225650556,
      "learning_rate": 1.5989585856429804e-07,
      "loss": 1.6686,
      "step": 391520
    },
    {
      "epoch": 0.0014210199168379334,
      "grad_norm": 9554.675295372417,
      "learning_rate": 1.5988931815418729e-07,
      "loss": 1.6771,
      "step": 391552
    },
    {
      "epoch": 0.0014211360511887702,
      "grad_norm": 10997.719945515979,
      "learning_rate": 1.5988277854660136e-07,
      "loss": 1.6788,
      "step": 391584
    },
    {
      "epoch": 0.0014212521855396067,
      "grad_norm": 13726.756718176368,
      "learning_rate": 1.5987623974137616e-07,
      "loss": 1.692,
      "step": 391616
    },
    {
      "epoch": 0.0014213683198904435,
      "grad_norm": 8515.60344309198,
      "learning_rate": 1.5986970173834762e-07,
      "loss": 1.6851,
      "step": 391648
    },
    {
      "epoch": 0.0014214844542412802,
      "grad_norm": 12475.523716461767,
      "learning_rate": 1.598631645373517e-07,
      "loss": 1.6716,
      "step": 391680
    },
    {
      "epoch": 0.001421600588592117,
      "grad_norm": 10240.791961562349,
      "learning_rate": 1.5985662813822447e-07,
      "loss": 1.662,
      "step": 391712
    },
    {
      "epoch": 0.0014217167229429537,
      "grad_norm": 10262.576089851904,
      "learning_rate": 1.5985009254080204e-07,
      "loss": 1.6786,
      "step": 391744
    },
    {
      "epoch": 0.0014218328572937903,
      "grad_norm": 11202.685213822622,
      "learning_rate": 1.5984355774492047e-07,
      "loss": 1.6735,
      "step": 391776
    },
    {
      "epoch": 0.001421948991644627,
      "grad_norm": 15924.750547496811,
      "learning_rate": 1.5983702375041596e-07,
      "loss": 1.6619,
      "step": 391808
    },
    {
      "epoch": 0.0014220651259954638,
      "grad_norm": 9346.799452218926,
      "learning_rate": 1.5983049055712475e-07,
      "loss": 1.6863,
      "step": 391840
    },
    {
      "epoch": 0.0014221812603463005,
      "grad_norm": 10333.558922268745,
      "learning_rate": 1.5982395816488308e-07,
      "loss": 1.6845,
      "step": 391872
    },
    {
      "epoch": 0.001422297394697137,
      "grad_norm": 11458.295073875519,
      "learning_rate": 1.5981742657352727e-07,
      "loss": 1.6771,
      "step": 391904
    },
    {
      "epoch": 0.0014224135290479738,
      "grad_norm": 9002.617397179556,
      "learning_rate": 1.5981089578289372e-07,
      "loss": 1.6824,
      "step": 391936
    },
    {
      "epoch": 0.0014225296633988106,
      "grad_norm": 11535.32869059222,
      "learning_rate": 1.598043657928188e-07,
      "loss": 1.7037,
      "step": 391968
    },
    {
      "epoch": 0.0014226457977496473,
      "grad_norm": 11736.919527712544,
      "learning_rate": 1.59797836603139e-07,
      "loss": 1.7057,
      "step": 392000
    },
    {
      "epoch": 0.001422761932100484,
      "grad_norm": 12424.218285268495,
      "learning_rate": 1.5979130821369078e-07,
      "loss": 1.7066,
      "step": 392032
    },
    {
      "epoch": 0.0014228780664513206,
      "grad_norm": 22190.470026567713,
      "learning_rate": 1.5978478062431074e-07,
      "loss": 1.7218,
      "step": 392064
    },
    {
      "epoch": 0.0014229942008021574,
      "grad_norm": 8675.291349574376,
      "learning_rate": 1.5977845778490023e-07,
      "loss": 1.7211,
      "step": 392096
    },
    {
      "epoch": 0.0014231103351529941,
      "grad_norm": 9605.68456696346,
      "learning_rate": 1.5977193177017688e-07,
      "loss": 1.692,
      "step": 392128
    },
    {
      "epoch": 0.0014232264695038309,
      "grad_norm": 9731.439153588744,
      "learning_rate": 1.597654065550367e-07,
      "loss": 1.6894,
      "step": 392160
    },
    {
      "epoch": 0.0014233426038546674,
      "grad_norm": 7753.854525331256,
      "learning_rate": 1.5975888213931648e-07,
      "loss": 1.692,
      "step": 392192
    },
    {
      "epoch": 0.0014234587382055042,
      "grad_norm": 9502.27993694145,
      "learning_rate": 1.597523585228529e-07,
      "loss": 1.6752,
      "step": 392224
    },
    {
      "epoch": 0.001423574872556341,
      "grad_norm": 7939.00119662417,
      "learning_rate": 1.597458357054829e-07,
      "loss": 1.6829,
      "step": 392256
    },
    {
      "epoch": 0.0014236910069071777,
      "grad_norm": 9276.94604921253,
      "learning_rate": 1.597393136870433e-07,
      "loss": 1.6933,
      "step": 392288
    },
    {
      "epoch": 0.0014238071412580144,
      "grad_norm": 10885.439173501452,
      "learning_rate": 1.5973279246737103e-07,
      "loss": 1.7142,
      "step": 392320
    },
    {
      "epoch": 0.001423923275608851,
      "grad_norm": 10252.572165071553,
      "learning_rate": 1.5972627204630302e-07,
      "loss": 1.6806,
      "step": 392352
    },
    {
      "epoch": 0.0014240394099596877,
      "grad_norm": 10223.981611877049,
      "learning_rate": 1.5971975242367635e-07,
      "loss": 1.6812,
      "step": 392384
    },
    {
      "epoch": 0.0014241555443105245,
      "grad_norm": 10165.395614534635,
      "learning_rate": 1.5971323359932806e-07,
      "loss": 1.6873,
      "step": 392416
    },
    {
      "epoch": 0.0014242716786613612,
      "grad_norm": 8750.244339445613,
      "learning_rate": 1.5970671557309525e-07,
      "loss": 1.6705,
      "step": 392448
    },
    {
      "epoch": 0.0014243878130121978,
      "grad_norm": 9213.179472907277,
      "learning_rate": 1.5970019834481508e-07,
      "loss": 1.6911,
      "step": 392480
    },
    {
      "epoch": 0.0014245039473630345,
      "grad_norm": 8951.958221528963,
      "learning_rate": 1.5969368191432475e-07,
      "loss": 1.6945,
      "step": 392512
    },
    {
      "epoch": 0.0014246200817138713,
      "grad_norm": 9106.875754066265,
      "learning_rate": 1.5968716628146146e-07,
      "loss": 1.6719,
      "step": 392544
    },
    {
      "epoch": 0.001424736216064708,
      "grad_norm": 12057.835958413101,
      "learning_rate": 1.596806514460626e-07,
      "loss": 1.6539,
      "step": 392576
    },
    {
      "epoch": 0.0014248523504155448,
      "grad_norm": 8830.47303376212,
      "learning_rate": 1.5967413740796545e-07,
      "loss": 1.6821,
      "step": 392608
    },
    {
      "epoch": 0.0014249684847663813,
      "grad_norm": 9181.152868785053,
      "learning_rate": 1.5966762416700738e-07,
      "loss": 1.6803,
      "step": 392640
    },
    {
      "epoch": 0.001425084619117218,
      "grad_norm": 8988.770994969223,
      "learning_rate": 1.5966111172302586e-07,
      "loss": 1.6698,
      "step": 392672
    },
    {
      "epoch": 0.0014252007534680548,
      "grad_norm": 7893.673669464681,
      "learning_rate": 1.596546000758584e-07,
      "loss": 1.6943,
      "step": 392704
    },
    {
      "epoch": 0.0014253168878188916,
      "grad_norm": 10864.489311513911,
      "learning_rate": 1.5964808922534245e-07,
      "loss": 1.682,
      "step": 392736
    },
    {
      "epoch": 0.0014254330221697281,
      "grad_norm": 10044.156908372152,
      "learning_rate": 1.5964157917131562e-07,
      "loss": 1.672,
      "step": 392768
    },
    {
      "epoch": 0.0014255491565205649,
      "grad_norm": 14017.30230821894,
      "learning_rate": 1.5963506991361553e-07,
      "loss": 1.6704,
      "step": 392800
    },
    {
      "epoch": 0.0014256652908714016,
      "grad_norm": 8686.401326210987,
      "learning_rate": 1.5962856145207986e-07,
      "loss": 1.6852,
      "step": 392832
    },
    {
      "epoch": 0.0014257814252222384,
      "grad_norm": 7995.905452167378,
      "learning_rate": 1.596220537865463e-07,
      "loss": 1.6998,
      "step": 392864
    },
    {
      "epoch": 0.0014258975595730751,
      "grad_norm": 8398.55273246528,
      "learning_rate": 1.596155469168526e-07,
      "loss": 1.6807,
      "step": 392896
    },
    {
      "epoch": 0.0014260136939239117,
      "grad_norm": 7409.855329222023,
      "learning_rate": 1.5960904084283658e-07,
      "loss": 1.7016,
      "step": 392928
    },
    {
      "epoch": 0.0014261298282747484,
      "grad_norm": 10081.019789683978,
      "learning_rate": 1.5960253556433608e-07,
      "loss": 1.7005,
      "step": 392960
    },
    {
      "epoch": 0.0014262459626255852,
      "grad_norm": 11461.465351341425,
      "learning_rate": 1.59596031081189e-07,
      "loss": 1.673,
      "step": 392992
    },
    {
      "epoch": 0.001426362096976422,
      "grad_norm": 9429.99427359317,
      "learning_rate": 1.5958952739323332e-07,
      "loss": 1.6614,
      "step": 393024
    },
    {
      "epoch": 0.0014264782313272585,
      "grad_norm": 11305.810099236587,
      "learning_rate": 1.5958302450030696e-07,
      "loss": 1.6773,
      "step": 393056
    },
    {
      "epoch": 0.0014265943656780952,
      "grad_norm": 9230.916314212798,
      "learning_rate": 1.59576522402248e-07,
      "loss": 1.6657,
      "step": 393088
    },
    {
      "epoch": 0.001426710500028932,
      "grad_norm": 21868.138283813736,
      "learning_rate": 1.5957002109889453e-07,
      "loss": 1.6675,
      "step": 393120
    },
    {
      "epoch": 0.0014268266343797687,
      "grad_norm": 20027.91172339243,
      "learning_rate": 1.5956352059008465e-07,
      "loss": 1.6783,
      "step": 393152
    },
    {
      "epoch": 0.0014269427687306055,
      "grad_norm": 12382.67095581563,
      "learning_rate": 1.595572239797097e-07,
      "loss": 1.6897,
      "step": 393184
    },
    {
      "epoch": 0.001427058903081442,
      "grad_norm": 9478.85689310689,
      "learning_rate": 1.5955072503468465e-07,
      "loss": 1.691,
      "step": 393216
    },
    {
      "epoch": 0.0014271750374322788,
      "grad_norm": 9135.0763543607,
      "learning_rate": 1.5954422688372296e-07,
      "loss": 1.6889,
      "step": 393248
    },
    {
      "epoch": 0.0014272911717831155,
      "grad_norm": 8828.83197257712,
      "learning_rate": 1.5953772952666283e-07,
      "loss": 1.6694,
      "step": 393280
    },
    {
      "epoch": 0.0014274073061339523,
      "grad_norm": 9272.37013929017,
      "learning_rate": 1.5953123296334272e-07,
      "loss": 1.6579,
      "step": 393312
    },
    {
      "epoch": 0.0014275234404847888,
      "grad_norm": 11217.49009359937,
      "learning_rate": 1.59524737193601e-07,
      "loss": 1.6667,
      "step": 393344
    },
    {
      "epoch": 0.0014276395748356256,
      "grad_norm": 10000.642979328879,
      "learning_rate": 1.5951824221727612e-07,
      "loss": 1.6808,
      "step": 393376
    },
    {
      "epoch": 0.0014277557091864623,
      "grad_norm": 9390.569844263979,
      "learning_rate": 1.5951174803420657e-07,
      "loss": 1.6902,
      "step": 393408
    },
    {
      "epoch": 0.001427871843537299,
      "grad_norm": 9863.085724052084,
      "learning_rate": 1.5950525464423089e-07,
      "loss": 1.6553,
      "step": 393440
    },
    {
      "epoch": 0.0014279879778881358,
      "grad_norm": 11037.869450215472,
      "learning_rate": 1.5949876204718762e-07,
      "loss": 1.666,
      "step": 393472
    },
    {
      "epoch": 0.0014281041122389724,
      "grad_norm": 8749.588333173167,
      "learning_rate": 1.594922702429155e-07,
      "loss": 1.6729,
      "step": 393504
    },
    {
      "epoch": 0.0014282202465898091,
      "grad_norm": 9814.169959808114,
      "learning_rate": 1.5948577923125312e-07,
      "loss": 1.6531,
      "step": 393536
    },
    {
      "epoch": 0.0014283363809406459,
      "grad_norm": 8966.190049290724,
      "learning_rate": 1.5947928901203926e-07,
      "loss": 1.6709,
      "step": 393568
    },
    {
      "epoch": 0.0014284525152914826,
      "grad_norm": 9273.93185224045,
      "learning_rate": 1.5947279958511265e-07,
      "loss": 1.6777,
      "step": 393600
    },
    {
      "epoch": 0.0014285686496423192,
      "grad_norm": 10443.329737205466,
      "learning_rate": 1.5946631095031212e-07,
      "loss": 1.6784,
      "step": 393632
    },
    {
      "epoch": 0.001428684783993156,
      "grad_norm": 13506.49399363136,
      "learning_rate": 1.5945982310747656e-07,
      "loss": 1.6666,
      "step": 393664
    },
    {
      "epoch": 0.0014288009183439927,
      "grad_norm": 9699.847833858013,
      "learning_rate": 1.5945333605644484e-07,
      "loss": 1.6806,
      "step": 393696
    },
    {
      "epoch": 0.0014289170526948294,
      "grad_norm": 11582.21723160121,
      "learning_rate": 1.5944684979705593e-07,
      "loss": 1.7072,
      "step": 393728
    },
    {
      "epoch": 0.0014290331870456662,
      "grad_norm": 11343.895450858141,
      "learning_rate": 1.5944036432914885e-07,
      "loss": 1.7007,
      "step": 393760
    },
    {
      "epoch": 0.0014291493213965027,
      "grad_norm": 9610.400095729627,
      "learning_rate": 1.594338796525626e-07,
      "loss": 1.7145,
      "step": 393792
    },
    {
      "epoch": 0.0014292654557473395,
      "grad_norm": 10133.575676926679,
      "learning_rate": 1.5942739576713632e-07,
      "loss": 1.6933,
      "step": 393824
    },
    {
      "epoch": 0.0014293815900981762,
      "grad_norm": 9190.715967757898,
      "learning_rate": 1.5942091267270907e-07,
      "loss": 1.678,
      "step": 393856
    },
    {
      "epoch": 0.001429497724449013,
      "grad_norm": 10000.562884158071,
      "learning_rate": 1.5941443036912015e-07,
      "loss": 1.6557,
      "step": 393888
    },
    {
      "epoch": 0.0014296138587998495,
      "grad_norm": 8721.053032747823,
      "learning_rate": 1.5940794885620871e-07,
      "loss": 1.6679,
      "step": 393920
    },
    {
      "epoch": 0.0014297299931506863,
      "grad_norm": 10569.115194754952,
      "learning_rate": 1.5940146813381404e-07,
      "loss": 1.677,
      "step": 393952
    },
    {
      "epoch": 0.001429846127501523,
      "grad_norm": 9501.382004740153,
      "learning_rate": 1.593949882017755e-07,
      "loss": 1.6675,
      "step": 393984
    },
    {
      "epoch": 0.0014299622618523598,
      "grad_norm": 10915.21479403864,
      "learning_rate": 1.593885090599324e-07,
      "loss": 1.6824,
      "step": 394016
    },
    {
      "epoch": 0.0014300783962031965,
      "grad_norm": 8520.417830130164,
      "learning_rate": 1.5938203070812414e-07,
      "loss": 1.6784,
      "step": 394048
    },
    {
      "epoch": 0.001430194530554033,
      "grad_norm": 12639.129716875288,
      "learning_rate": 1.5937555314619027e-07,
      "loss": 1.6696,
      "step": 394080
    },
    {
      "epoch": 0.0014303106649048698,
      "grad_norm": 9661.712477609753,
      "learning_rate": 1.593690763739702e-07,
      "loss": 1.6788,
      "step": 394112
    },
    {
      "epoch": 0.0014304267992557066,
      "grad_norm": 10266.062925971182,
      "learning_rate": 1.5936260039130355e-07,
      "loss": 1.6651,
      "step": 394144
    },
    {
      "epoch": 0.0014305429336065433,
      "grad_norm": 19756.796501457415,
      "learning_rate": 1.5935612519802986e-07,
      "loss": 1.6576,
      "step": 394176
    },
    {
      "epoch": 0.0014306590679573799,
      "grad_norm": 8998.085796434707,
      "learning_rate": 1.5934985310717026e-07,
      "loss": 1.6677,
      "step": 394208
    },
    {
      "epoch": 0.0014307752023082166,
      "grad_norm": 10758.733940385366,
      "learning_rate": 1.5934337946754543e-07,
      "loss": 1.6793,
      "step": 394240
    },
    {
      "epoch": 0.0014308913366590534,
      "grad_norm": 10412.831315257152,
      "learning_rate": 1.5933690661683763e-07,
      "loss": 1.6918,
      "step": 394272
    },
    {
      "epoch": 0.0014310074710098901,
      "grad_norm": 15811.659621937222,
      "learning_rate": 1.5933043455488665e-07,
      "loss": 1.6817,
      "step": 394304
    },
    {
      "epoch": 0.001431123605360727,
      "grad_norm": 9631.1624428207,
      "learning_rate": 1.5932396328153234e-07,
      "loss": 1.6754,
      "step": 394336
    },
    {
      "epoch": 0.0014312397397115634,
      "grad_norm": 10777.40989291954,
      "learning_rate": 1.593174927966145e-07,
      "loss": 1.6709,
      "step": 394368
    },
    {
      "epoch": 0.0014313558740624002,
      "grad_norm": 9650.493873372492,
      "learning_rate": 1.5931102309997312e-07,
      "loss": 1.6636,
      "step": 394400
    },
    {
      "epoch": 0.001431472008413237,
      "grad_norm": 9236.237653936802,
      "learning_rate": 1.5930455419144814e-07,
      "loss": 1.6735,
      "step": 394432
    },
    {
      "epoch": 0.0014315881427640737,
      "grad_norm": 10515.189013993044,
      "learning_rate": 1.592980860708795e-07,
      "loss": 1.6891,
      "step": 394464
    },
    {
      "epoch": 0.0014317042771149102,
      "grad_norm": 11938.03467912537,
      "learning_rate": 1.5929161873810733e-07,
      "loss": 1.6951,
      "step": 394496
    },
    {
      "epoch": 0.001431820411465747,
      "grad_norm": 13119.150124912818,
      "learning_rate": 1.592851521929717e-07,
      "loss": 1.6691,
      "step": 394528
    },
    {
      "epoch": 0.0014319365458165837,
      "grad_norm": 10913.551575907817,
      "learning_rate": 1.5927868643531268e-07,
      "loss": 1.6771,
      "step": 394560
    },
    {
      "epoch": 0.0014320526801674205,
      "grad_norm": 9100.095054448608,
      "learning_rate": 1.5927222146497056e-07,
      "loss": 1.7025,
      "step": 394592
    },
    {
      "epoch": 0.0014321688145182573,
      "grad_norm": 8787.155853858518,
      "learning_rate": 1.5926575728178553e-07,
      "loss": 1.6806,
      "step": 394624
    },
    {
      "epoch": 0.0014322849488690938,
      "grad_norm": 8378.299230750834,
      "learning_rate": 1.5925929388559784e-07,
      "loss": 1.6977,
      "step": 394656
    },
    {
      "epoch": 0.0014324010832199305,
      "grad_norm": 9386.160769984712,
      "learning_rate": 1.5925283127624786e-07,
      "loss": 1.6997,
      "step": 394688
    },
    {
      "epoch": 0.0014325172175707673,
      "grad_norm": 10646.060304168861,
      "learning_rate": 1.592463694535759e-07,
      "loss": 1.6904,
      "step": 394720
    },
    {
      "epoch": 0.001432633351921604,
      "grad_norm": 10539.39447976021,
      "learning_rate": 1.5923990841742244e-07,
      "loss": 1.6692,
      "step": 394752
    },
    {
      "epoch": 0.0014327494862724406,
      "grad_norm": 10796.125045589273,
      "learning_rate": 1.592334481676279e-07,
      "loss": 1.6713,
      "step": 394784
    },
    {
      "epoch": 0.0014328656206232773,
      "grad_norm": 10520.81175575345,
      "learning_rate": 1.5922698870403275e-07,
      "loss": 1.6894,
      "step": 394816
    },
    {
      "epoch": 0.001432981754974114,
      "grad_norm": 8070.537404658999,
      "learning_rate": 1.5922053002647757e-07,
      "loss": 1.6765,
      "step": 394848
    },
    {
      "epoch": 0.0014330978893249509,
      "grad_norm": 9105.722156973603,
      "learning_rate": 1.5921407213480295e-07,
      "loss": 1.6895,
      "step": 394880
    },
    {
      "epoch": 0.0014332140236757876,
      "grad_norm": 9778.980519461116,
      "learning_rate": 1.5920761502884956e-07,
      "loss": 1.6755,
      "step": 394912
    },
    {
      "epoch": 0.0014333301580266241,
      "grad_norm": 12833.198665960097,
      "learning_rate": 1.5920115870845808e-07,
      "loss": 1.6818,
      "step": 394944
    },
    {
      "epoch": 0.001433446292377461,
      "grad_norm": 11415.120498706969,
      "learning_rate": 1.5919470317346917e-07,
      "loss": 1.674,
      "step": 394976
    },
    {
      "epoch": 0.0014335624267282977,
      "grad_norm": 9821.81917976502,
      "learning_rate": 1.5918824842372366e-07,
      "loss": 1.6792,
      "step": 395008
    },
    {
      "epoch": 0.0014336785610791344,
      "grad_norm": 10521.495901249024,
      "learning_rate": 1.591817944590624e-07,
      "loss": 1.6747,
      "step": 395040
    },
    {
      "epoch": 0.001433794695429971,
      "grad_norm": 11355.816042891853,
      "learning_rate": 1.5917534127932617e-07,
      "loss": 1.6607,
      "step": 395072
    },
    {
      "epoch": 0.0014339108297808077,
      "grad_norm": 11895.642899818404,
      "learning_rate": 1.5916888888435595e-07,
      "loss": 1.6727,
      "step": 395104
    },
    {
      "epoch": 0.0014340269641316445,
      "grad_norm": 9731.58424923712,
      "learning_rate": 1.5916243727399264e-07,
      "loss": 1.6736,
      "step": 395136
    },
    {
      "epoch": 0.0014341430984824812,
      "grad_norm": 11434.229488688778,
      "learning_rate": 1.5915598644807725e-07,
      "loss": 1.6621,
      "step": 395168
    },
    {
      "epoch": 0.001434259232833318,
      "grad_norm": 22675.50255231403,
      "learning_rate": 1.5914953640645093e-07,
      "loss": 1.6562,
      "step": 395200
    },
    {
      "epoch": 0.0014343753671841545,
      "grad_norm": 10522.83954073234,
      "learning_rate": 1.591432886763838e-07,
      "loss": 1.6789,
      "step": 395232
    },
    {
      "epoch": 0.0014344915015349912,
      "grad_norm": 9672.820064489983,
      "learning_rate": 1.5913684017836204e-07,
      "loss": 1.6709,
      "step": 395264
    },
    {
      "epoch": 0.001434607635885828,
      "grad_norm": 12309.736959009319,
      "learning_rate": 1.5913039246415758e-07,
      "loss": 1.6736,
      "step": 395296
    },
    {
      "epoch": 0.0014347237702366648,
      "grad_norm": 9891.571968094859,
      "learning_rate": 1.5912394553361175e-07,
      "loss": 1.6928,
      "step": 395328
    },
    {
      "epoch": 0.0014348399045875013,
      "grad_norm": 9623.04754222902,
      "learning_rate": 1.5911749938656576e-07,
      "loss": 1.6981,
      "step": 395360
    },
    {
      "epoch": 0.001434956038938338,
      "grad_norm": 10944.368597593924,
      "learning_rate": 1.5911105402286093e-07,
      "loss": 1.691,
      "step": 395392
    },
    {
      "epoch": 0.0014350721732891748,
      "grad_norm": 9609.422459232397,
      "learning_rate": 1.591046094423386e-07,
      "loss": 1.6906,
      "step": 395424
    },
    {
      "epoch": 0.0014351883076400116,
      "grad_norm": 9248.467548734763,
      "learning_rate": 1.590981656448402e-07,
      "loss": 1.6981,
      "step": 395456
    },
    {
      "epoch": 0.0014353044419908483,
      "grad_norm": 8871.47699089616,
      "learning_rate": 1.5909172263020715e-07,
      "loss": 1.6938,
      "step": 395488
    },
    {
      "epoch": 0.0014354205763416848,
      "grad_norm": 9850.926250865958,
      "learning_rate": 1.5908528039828101e-07,
      "loss": 1.706,
      "step": 395520
    },
    {
      "epoch": 0.0014355367106925216,
      "grad_norm": 10486.847572078083,
      "learning_rate": 1.5907883894890326e-07,
      "loss": 1.7033,
      "step": 395552
    },
    {
      "epoch": 0.0014356528450433584,
      "grad_norm": 12759.20655840323,
      "learning_rate": 1.590723982819155e-07,
      "loss": 1.7033,
      "step": 395584
    },
    {
      "epoch": 0.0014357689793941951,
      "grad_norm": 12612.854316133204,
      "learning_rate": 1.5906595839715936e-07,
      "loss": 1.6604,
      "step": 395616
    },
    {
      "epoch": 0.0014358851137450316,
      "grad_norm": 11607.990351477727,
      "learning_rate": 1.590595192944765e-07,
      "loss": 1.6626,
      "step": 395648
    },
    {
      "epoch": 0.0014360012480958684,
      "grad_norm": 9200.08358657681,
      "learning_rate": 1.5905308097370867e-07,
      "loss": 1.6737,
      "step": 395680
    },
    {
      "epoch": 0.0014361173824467052,
      "grad_norm": 12111.780215971556,
      "learning_rate": 1.590466434346976e-07,
      "loss": 1.6477,
      "step": 395712
    },
    {
      "epoch": 0.001436233516797542,
      "grad_norm": 9339.712950621128,
      "learning_rate": 1.590402066772851e-07,
      "loss": 1.6764,
      "step": 395744
    },
    {
      "epoch": 0.0014363496511483787,
      "grad_norm": 16820.60462646929,
      "learning_rate": 1.5903377070131306e-07,
      "loss": 1.6731,
      "step": 395776
    },
    {
      "epoch": 0.0014364657854992152,
      "grad_norm": 13011.731014742043,
      "learning_rate": 1.5902733550662333e-07,
      "loss": 1.6766,
      "step": 395808
    },
    {
      "epoch": 0.001436581919850052,
      "grad_norm": 12198.686158763165,
      "learning_rate": 1.5902090109305791e-07,
      "loss": 1.6787,
      "step": 395840
    },
    {
      "epoch": 0.0014366980542008887,
      "grad_norm": 10492.778278416064,
      "learning_rate": 1.590144674604587e-07,
      "loss": 1.6895,
      "step": 395872
    },
    {
      "epoch": 0.0014368141885517255,
      "grad_norm": 11476.094109059928,
      "learning_rate": 1.590080346086678e-07,
      "loss": 1.6884,
      "step": 395904
    },
    {
      "epoch": 0.001436930322902562,
      "grad_norm": 8390.375438560543,
      "learning_rate": 1.590016025375273e-07,
      "loss": 1.6728,
      "step": 395936
    },
    {
      "epoch": 0.0014370464572533988,
      "grad_norm": 11349.118556081788,
      "learning_rate": 1.5899517124687925e-07,
      "loss": 1.6916,
      "step": 395968
    },
    {
      "epoch": 0.0014371625916042355,
      "grad_norm": 10063.400121231392,
      "learning_rate": 1.589887407365659e-07,
      "loss": 1.6669,
      "step": 396000
    },
    {
      "epoch": 0.0014372787259550723,
      "grad_norm": 9383.675186194372,
      "learning_rate": 1.5898231100642938e-07,
      "loss": 1.6697,
      "step": 396032
    },
    {
      "epoch": 0.001437394860305909,
      "grad_norm": 9269.584996104195,
      "learning_rate": 1.5897588205631202e-07,
      "loss": 1.654,
      "step": 396064
    },
    {
      "epoch": 0.0014375109946567456,
      "grad_norm": 9368.741430950051,
      "learning_rate": 1.5896945388605605e-07,
      "loss": 1.6853,
      "step": 396096
    },
    {
      "epoch": 0.0014376271290075823,
      "grad_norm": 10046.172405448753,
      "learning_rate": 1.5896302649550388e-07,
      "loss": 1.6782,
      "step": 396128
    },
    {
      "epoch": 0.001437743263358419,
      "grad_norm": 10052.608417719253,
      "learning_rate": 1.5895659988449784e-07,
      "loss": 1.6621,
      "step": 396160
    },
    {
      "epoch": 0.0014378593977092558,
      "grad_norm": 12470.900047711071,
      "learning_rate": 1.589501740528804e-07,
      "loss": 1.678,
      "step": 396192
    },
    {
      "epoch": 0.0014379755320600924,
      "grad_norm": 16202.638056810378,
      "learning_rate": 1.5894374900049404e-07,
      "loss": 1.6851,
      "step": 396224
    },
    {
      "epoch": 0.0014380916664109291,
      "grad_norm": 17157.0076645084,
      "learning_rate": 1.5893732472718128e-07,
      "loss": 1.6784,
      "step": 396256
    },
    {
      "epoch": 0.0014382078007617659,
      "grad_norm": 9465.86372181641,
      "learning_rate": 1.5893110195519588e-07,
      "loss": 1.6768,
      "step": 396288
    },
    {
      "epoch": 0.0014383239351126026,
      "grad_norm": 9054.435156319802,
      "learning_rate": 1.5892467921522422e-07,
      "loss": 1.7007,
      "step": 396320
    },
    {
      "epoch": 0.0014384400694634394,
      "grad_norm": 11039.559411498269,
      "learning_rate": 1.589182572538589e-07,
      "loss": 1.6825,
      "step": 396352
    },
    {
      "epoch": 0.001438556203814276,
      "grad_norm": 8276.89265364726,
      "learning_rate": 1.5891183607094264e-07,
      "loss": 1.6962,
      "step": 396384
    },
    {
      "epoch": 0.0014386723381651127,
      "grad_norm": 11572.623730165948,
      "learning_rate": 1.5890541566631815e-07,
      "loss": 1.7102,
      "step": 396416
    },
    {
      "epoch": 0.0014387884725159494,
      "grad_norm": 11398.887314119744,
      "learning_rate": 1.5889899603982824e-07,
      "loss": 1.721,
      "step": 396448
    },
    {
      "epoch": 0.0014389046068667862,
      "grad_norm": 8026.7755668138625,
      "learning_rate": 1.5889257719131575e-07,
      "loss": 1.6919,
      "step": 396480
    },
    {
      "epoch": 0.0014390207412176227,
      "grad_norm": 11050.84937912014,
      "learning_rate": 1.5888615912062356e-07,
      "loss": 1.6663,
      "step": 396512
    },
    {
      "epoch": 0.0014391368755684595,
      "grad_norm": 8243.785902120457,
      "learning_rate": 1.588797418275946e-07,
      "loss": 1.6713,
      "step": 396544
    },
    {
      "epoch": 0.0014392530099192962,
      "grad_norm": 15384.778321444868,
      "learning_rate": 1.5887332531207178e-07,
      "loss": 1.6631,
      "step": 396576
    },
    {
      "epoch": 0.001439369144270133,
      "grad_norm": 8004.202771044722,
      "learning_rate": 1.5886690957389818e-07,
      "loss": 1.674,
      "step": 396608
    },
    {
      "epoch": 0.0014394852786209697,
      "grad_norm": 8248.515502804126,
      "learning_rate": 1.5886049461291677e-07,
      "loss": 1.6815,
      "step": 396640
    },
    {
      "epoch": 0.0014396014129718063,
      "grad_norm": 14377.07578056122,
      "learning_rate": 1.5885408042897076e-07,
      "loss": 1.6875,
      "step": 396672
    },
    {
      "epoch": 0.001439717547322643,
      "grad_norm": 9573.45851821587,
      "learning_rate": 1.5884766702190324e-07,
      "loss": 1.6595,
      "step": 396704
    },
    {
      "epoch": 0.0014398336816734798,
      "grad_norm": 9239.53050755286,
      "learning_rate": 1.588412543915574e-07,
      "loss": 1.6791,
      "step": 396736
    },
    {
      "epoch": 0.0014399498160243165,
      "grad_norm": 8818.865686696901,
      "learning_rate": 1.5883484253777644e-07,
      "loss": 1.6848,
      "step": 396768
    },
    {
      "epoch": 0.001440065950375153,
      "grad_norm": 11077.194771240596,
      "learning_rate": 1.5882843146040367e-07,
      "loss": 1.6529,
      "step": 396800
    },
    {
      "epoch": 0.0014401820847259898,
      "grad_norm": 11689.660388565613,
      "learning_rate": 1.5882202115928244e-07,
      "loss": 1.6804,
      "step": 396832
    },
    {
      "epoch": 0.0014402982190768266,
      "grad_norm": 9518.443990485,
      "learning_rate": 1.5881561163425605e-07,
      "loss": 1.6816,
      "step": 396864
    },
    {
      "epoch": 0.0014404143534276633,
      "grad_norm": 9304.406912855864,
      "learning_rate": 1.5880920288516795e-07,
      "loss": 1.6816,
      "step": 396896
    },
    {
      "epoch": 0.0014405304877785,
      "grad_norm": 10795.424586369913,
      "learning_rate": 1.588027949118616e-07,
      "loss": 1.6768,
      "step": 396928
    },
    {
      "epoch": 0.0014406466221293366,
      "grad_norm": 10246.962671933572,
      "learning_rate": 1.5879638771418045e-07,
      "loss": 1.6915,
      "step": 396960
    },
    {
      "epoch": 0.0014407627564801734,
      "grad_norm": 13079.957186474274,
      "learning_rate": 1.5878998129196812e-07,
      "loss": 1.7121,
      "step": 396992
    },
    {
      "epoch": 0.0014408788908310101,
      "grad_norm": 12224.9834355716,
      "learning_rate": 1.5878357564506813e-07,
      "loss": 1.6907,
      "step": 397024
    },
    {
      "epoch": 0.0014409950251818469,
      "grad_norm": 8066.36113746465,
      "learning_rate": 1.5877717077332414e-07,
      "loss": 1.6968,
      "step": 397056
    },
    {
      "epoch": 0.0014411111595326834,
      "grad_norm": 9066.132472008116,
      "learning_rate": 1.5877076667657983e-07,
      "loss": 1.6927,
      "step": 397088
    },
    {
      "epoch": 0.0014412272938835202,
      "grad_norm": 9782.070230784484,
      "learning_rate": 1.587643633546789e-07,
      "loss": 1.6843,
      "step": 397120
    },
    {
      "epoch": 0.001441343428234357,
      "grad_norm": 13403.029135236557,
      "learning_rate": 1.5875796080746514e-07,
      "loss": 1.6784,
      "step": 397152
    },
    {
      "epoch": 0.0014414595625851937,
      "grad_norm": 10074.496910516178,
      "learning_rate": 1.587515590347823e-07,
      "loss": 1.7001,
      "step": 397184
    },
    {
      "epoch": 0.0014415756969360304,
      "grad_norm": 9596.002084201524,
      "learning_rate": 1.587451580364743e-07,
      "loss": 1.7024,
      "step": 397216
    },
    {
      "epoch": 0.001441691831286867,
      "grad_norm": 8771.095940645046,
      "learning_rate": 1.5873875781238498e-07,
      "loss": 1.696,
      "step": 397248
    },
    {
      "epoch": 0.0014418079656377037,
      "grad_norm": 7926.863566379832,
      "learning_rate": 1.5873235836235834e-07,
      "loss": 1.7111,
      "step": 397280
    },
    {
      "epoch": 0.0014419240999885405,
      "grad_norm": 10986.356265841736,
      "learning_rate": 1.5872615963315423e-07,
      "loss": 1.7151,
      "step": 397312
    },
    {
      "epoch": 0.0014420402343393772,
      "grad_norm": 12060.905604472659,
      "learning_rate": 1.5871976170660747e-07,
      "loss": 1.7,
      "step": 397344
    },
    {
      "epoch": 0.0014421563686902138,
      "grad_norm": 9027.403170347496,
      "learning_rate": 1.5871336455366036e-07,
      "loss": 1.675,
      "step": 397376
    },
    {
      "epoch": 0.0014422725030410505,
      "grad_norm": 12703.590358634838,
      "learning_rate": 1.5870696817415694e-07,
      "loss": 1.6895,
      "step": 397408
    },
    {
      "epoch": 0.0014423886373918873,
      "grad_norm": 8433.350342538843,
      "learning_rate": 1.5870057256794144e-07,
      "loss": 1.6819,
      "step": 397440
    },
    {
      "epoch": 0.001442504771742724,
      "grad_norm": 10754.54750326577,
      "learning_rate": 1.5869417773485792e-07,
      "loss": 1.67,
      "step": 397472
    },
    {
      "epoch": 0.0014426209060935608,
      "grad_norm": 9410.539623209712,
      "learning_rate": 1.5868778367475082e-07,
      "loss": 1.6795,
      "step": 397504
    },
    {
      "epoch": 0.0014427370404443973,
      "grad_norm": 10267.82294354553,
      "learning_rate": 1.5868139038746427e-07,
      "loss": 1.6945,
      "step": 397536
    },
    {
      "epoch": 0.001442853174795234,
      "grad_norm": 11988.47780162269,
      "learning_rate": 1.5867499787284268e-07,
      "loss": 1.682,
      "step": 397568
    },
    {
      "epoch": 0.0014429693091460708,
      "grad_norm": 7928.434019401309,
      "learning_rate": 1.586686061307304e-07,
      "loss": 1.675,
      "step": 397600
    },
    {
      "epoch": 0.0014430854434969076,
      "grad_norm": 9672.171007586663,
      "learning_rate": 1.5866221516097187e-07,
      "loss": 1.6802,
      "step": 397632
    },
    {
      "epoch": 0.0014432015778477441,
      "grad_norm": 8912.459144366385,
      "learning_rate": 1.5865582496341155e-07,
      "loss": 1.6564,
      "step": 397664
    },
    {
      "epoch": 0.0014433177121985809,
      "grad_norm": 12302.135180528623,
      "learning_rate": 1.5864943553789392e-07,
      "loss": 1.6621,
      "step": 397696
    },
    {
      "epoch": 0.0014434338465494176,
      "grad_norm": 11629.037277436168,
      "learning_rate": 1.586430468842636e-07,
      "loss": 1.6842,
      "step": 397728
    },
    {
      "epoch": 0.0014435499809002544,
      "grad_norm": 9748.752535581156,
      "learning_rate": 1.586366590023651e-07,
      "loss": 1.6805,
      "step": 397760
    },
    {
      "epoch": 0.0014436661152510911,
      "grad_norm": 8545.97706526293,
      "learning_rate": 1.5863027189204313e-07,
      "loss": 1.6513,
      "step": 397792
    },
    {
      "epoch": 0.0014437822496019277,
      "grad_norm": 13566.621244805208,
      "learning_rate": 1.5862388555314236e-07,
      "loss": 1.6595,
      "step": 397824
    },
    {
      "epoch": 0.0014438983839527644,
      "grad_norm": 10040.117130790855,
      "learning_rate": 1.5861749998550752e-07,
      "loss": 1.6762,
      "step": 397856
    },
    {
      "epoch": 0.0014440145183036012,
      "grad_norm": 8909.450488105313,
      "learning_rate": 1.5861111518898336e-07,
      "loss": 1.6594,
      "step": 397888
    },
    {
      "epoch": 0.001444130652654438,
      "grad_norm": 10028.229754049316,
      "learning_rate": 1.5860473116341472e-07,
      "loss": 1.6781,
      "step": 397920
    },
    {
      "epoch": 0.0014442467870052745,
      "grad_norm": 8323.448564146955,
      "learning_rate": 1.585983479086464e-07,
      "loss": 1.6781,
      "step": 397952
    },
    {
      "epoch": 0.0014443629213561112,
      "grad_norm": 8468.378357159061,
      "learning_rate": 1.585919654245234e-07,
      "loss": 1.6837,
      "step": 397984
    },
    {
      "epoch": 0.001444479055706948,
      "grad_norm": 10513.332012259481,
      "learning_rate": 1.585855837108906e-07,
      "loss": 1.6674,
      "step": 398016
    },
    {
      "epoch": 0.0014445951900577847,
      "grad_norm": 9114.181257798202,
      "learning_rate": 1.58579202767593e-07,
      "loss": 1.6751,
      "step": 398048
    },
    {
      "epoch": 0.0014447113244086215,
      "grad_norm": 10936.471277336213,
      "learning_rate": 1.5857282259447566e-07,
      "loss": 1.7051,
      "step": 398080
    },
    {
      "epoch": 0.001444827458759458,
      "grad_norm": 11821.540001201198,
      "learning_rate": 1.5856644319138364e-07,
      "loss": 1.6911,
      "step": 398112
    },
    {
      "epoch": 0.0014449435931102948,
      "grad_norm": 8093.855200088521,
      "learning_rate": 1.5856006455816203e-07,
      "loss": 1.7009,
      "step": 398144
    },
    {
      "epoch": 0.0014450597274611315,
      "grad_norm": 8386.781504248218,
      "learning_rate": 1.5855368669465604e-07,
      "loss": 1.693,
      "step": 398176
    },
    {
      "epoch": 0.0014451758618119683,
      "grad_norm": 10001.74344801945,
      "learning_rate": 1.5854730960071088e-07,
      "loss": 1.6936,
      "step": 398208
    },
    {
      "epoch": 0.0014452919961628048,
      "grad_norm": 11655.304886617081,
      "learning_rate": 1.5854093327617177e-07,
      "loss": 1.6708,
      "step": 398240
    },
    {
      "epoch": 0.0014454081305136416,
      "grad_norm": 12450.761743764917,
      "learning_rate": 1.5853455772088403e-07,
      "loss": 1.6767,
      "step": 398272
    },
    {
      "epoch": 0.0014455242648644783,
      "grad_norm": 23854.005952879277,
      "learning_rate": 1.58528182934693e-07,
      "loss": 1.6696,
      "step": 398304
    },
    {
      "epoch": 0.001445640399215315,
      "grad_norm": 9109.942919689453,
      "learning_rate": 1.5852200809384535e-07,
      "loss": 1.6576,
      "step": 398336
    },
    {
      "epoch": 0.0014457565335661518,
      "grad_norm": 9610.283866775217,
      "learning_rate": 1.585156348213616e-07,
      "loss": 1.6808,
      "step": 398368
    },
    {
      "epoch": 0.0014458726679169884,
      "grad_norm": 12206.809247301278,
      "learning_rate": 1.5850926231751573e-07,
      "loss": 1.6752,
      "step": 398400
    },
    {
      "epoch": 0.0014459888022678251,
      "grad_norm": 9343.57404851056,
      "learning_rate": 1.5850289058215314e-07,
      "loss": 1.6636,
      "step": 398432
    },
    {
      "epoch": 0.0014461049366186619,
      "grad_norm": 10345.15790116323,
      "learning_rate": 1.5849651961511942e-07,
      "loss": 1.6655,
      "step": 398464
    },
    {
      "epoch": 0.0014462210709694986,
      "grad_norm": 9537.191620178342,
      "learning_rate": 1.584901494162602e-07,
      "loss": 1.6836,
      "step": 398496
    },
    {
      "epoch": 0.0014463372053203352,
      "grad_norm": 16280.436480635277,
      "learning_rate": 1.5848377998542108e-07,
      "loss": 1.6647,
      "step": 398528
    },
    {
      "epoch": 0.001446453339671172,
      "grad_norm": 11274.512672395202,
      "learning_rate": 1.584774113224478e-07,
      "loss": 1.6705,
      "step": 398560
    },
    {
      "epoch": 0.0014465694740220087,
      "grad_norm": 8906.570159157789,
      "learning_rate": 1.58471043427186e-07,
      "loss": 1.6822,
      "step": 398592
    },
    {
      "epoch": 0.0014466856083728454,
      "grad_norm": 9143.789477016628,
      "learning_rate": 1.5846467629948152e-07,
      "loss": 1.69,
      "step": 398624
    },
    {
      "epoch": 0.001446801742723682,
      "grad_norm": 12202.70707671048,
      "learning_rate": 1.5845830993918016e-07,
      "loss": 1.6722,
      "step": 398656
    },
    {
      "epoch": 0.0014469178770745187,
      "grad_norm": 13999.262409141418,
      "learning_rate": 1.584519443461278e-07,
      "loss": 1.6591,
      "step": 398688
    },
    {
      "epoch": 0.0014470340114253555,
      "grad_norm": 9265.90589203236,
      "learning_rate": 1.584455795201703e-07,
      "loss": 1.6718,
      "step": 398720
    },
    {
      "epoch": 0.0014471501457761922,
      "grad_norm": 12191.511637200696,
      "learning_rate": 1.5843921546115366e-07,
      "loss": 1.6612,
      "step": 398752
    },
    {
      "epoch": 0.001447266280127029,
      "grad_norm": 11224.554690498862,
      "learning_rate": 1.5843285216892383e-07,
      "loss": 1.666,
      "step": 398784
    },
    {
      "epoch": 0.0014473824144778655,
      "grad_norm": 8510.666718888715,
      "learning_rate": 1.584264896433268e-07,
      "loss": 1.686,
      "step": 398816
    },
    {
      "epoch": 0.0014474985488287023,
      "grad_norm": 11058.207449672844,
      "learning_rate": 1.5842012788420874e-07,
      "loss": 1.6796,
      "step": 398848
    },
    {
      "epoch": 0.001447614683179539,
      "grad_norm": 9201.083849199505,
      "learning_rate": 1.5841376689141573e-07,
      "loss": 1.6535,
      "step": 398880
    },
    {
      "epoch": 0.0014477308175303758,
      "grad_norm": 9844.547120106643,
      "learning_rate": 1.5840740666479393e-07,
      "loss": 1.671,
      "step": 398912
    },
    {
      "epoch": 0.0014478469518812123,
      "grad_norm": 8867.163244239953,
      "learning_rate": 1.5840104720418952e-07,
      "loss": 1.694,
      "step": 398944
    },
    {
      "epoch": 0.001447963086232049,
      "grad_norm": 8119.379409782498,
      "learning_rate": 1.583946885094488e-07,
      "loss": 1.6725,
      "step": 398976
    },
    {
      "epoch": 0.0014480792205828858,
      "grad_norm": 11058.11864649679,
      "learning_rate": 1.58388330580418e-07,
      "loss": 1.6937,
      "step": 399008
    },
    {
      "epoch": 0.0014481953549337226,
      "grad_norm": 10077.643970690768,
      "learning_rate": 1.5838197341694353e-07,
      "loss": 1.6964,
      "step": 399040
    },
    {
      "epoch": 0.0014483114892845594,
      "grad_norm": 10596.738083014037,
      "learning_rate": 1.583756170188717e-07,
      "loss": 1.7018,
      "step": 399072
    },
    {
      "epoch": 0.0014484276236353959,
      "grad_norm": 10477.011215036471,
      "learning_rate": 1.5836926138604898e-07,
      "loss": 1.6795,
      "step": 399104
    },
    {
      "epoch": 0.0014485437579862326,
      "grad_norm": 9900.41170861091,
      "learning_rate": 1.583629065183218e-07,
      "loss": 1.6784,
      "step": 399136
    },
    {
      "epoch": 0.0014486598923370694,
      "grad_norm": 13270.73471967547,
      "learning_rate": 1.5835655241553666e-07,
      "loss": 1.6878,
      "step": 399168
    },
    {
      "epoch": 0.0014487760266879062,
      "grad_norm": 11201.740043403972,
      "learning_rate": 1.583501990775402e-07,
      "loss": 1.6744,
      "step": 399200
    },
    {
      "epoch": 0.0014488921610387427,
      "grad_norm": 8579.22210925909,
      "learning_rate": 1.5834384650417887e-07,
      "loss": 1.6796,
      "step": 399232
    },
    {
      "epoch": 0.0014490082953895794,
      "grad_norm": 9430.77600200535,
      "learning_rate": 1.5833749469529942e-07,
      "loss": 1.6801,
      "step": 399264
    },
    {
      "epoch": 0.0014491244297404162,
      "grad_norm": 9133.588670396757,
      "learning_rate": 1.583311436507485e-07,
      "loss": 1.6665,
      "step": 399296
    },
    {
      "epoch": 0.001449240564091253,
      "grad_norm": 18289.32891059702,
      "learning_rate": 1.5832479337037278e-07,
      "loss": 1.6675,
      "step": 399328
    },
    {
      "epoch": 0.0014493566984420897,
      "grad_norm": 10047.759352213807,
      "learning_rate": 1.5831864226484193e-07,
      "loss": 1.6992,
      "step": 399360
    },
    {
      "epoch": 0.0014494728327929262,
      "grad_norm": 9981.452900254553,
      "learning_rate": 1.5831229348848848e-07,
      "loss": 1.6793,
      "step": 399392
    },
    {
      "epoch": 0.001449588967143763,
      "grad_norm": 11316.904877217974,
      "learning_rate": 1.5830594547585551e-07,
      "loss": 1.6567,
      "step": 399424
    },
    {
      "epoch": 0.0014497051014945998,
      "grad_norm": 7758.666251360474,
      "learning_rate": 1.582995982267899e-07,
      "loss": 1.6827,
      "step": 399456
    },
    {
      "epoch": 0.0014498212358454365,
      "grad_norm": 9183.324016934173,
      "learning_rate": 1.5829325174113857e-07,
      "loss": 1.6752,
      "step": 399488
    },
    {
      "epoch": 0.001449937370196273,
      "grad_norm": 7475.095049562916,
      "learning_rate": 1.582869060187485e-07,
      "loss": 1.6685,
      "step": 399520
    },
    {
      "epoch": 0.0014500535045471098,
      "grad_norm": 10062.377055149544,
      "learning_rate": 1.5828056105946675e-07,
      "loss": 1.6679,
      "step": 399552
    },
    {
      "epoch": 0.0014501696388979466,
      "grad_norm": 9459.225338261056,
      "learning_rate": 1.5827421686314033e-07,
      "loss": 1.6688,
      "step": 399584
    },
    {
      "epoch": 0.0014502857732487833,
      "grad_norm": 8445.709916874957,
      "learning_rate": 1.5826787342961637e-07,
      "loss": 1.6704,
      "step": 399616
    },
    {
      "epoch": 0.00145040190759962,
      "grad_norm": 11458.110751777538,
      "learning_rate": 1.5826153075874205e-07,
      "loss": 1.672,
      "step": 399648
    },
    {
      "epoch": 0.0014505180419504566,
      "grad_norm": 9467.0039611273,
      "learning_rate": 1.5825518885036457e-07,
      "loss": 1.6926,
      "step": 399680
    },
    {
      "epoch": 0.0014506341763012934,
      "grad_norm": 13929.429421193103,
      "learning_rate": 1.582488477043311e-07,
      "loss": 1.6965,
      "step": 399712
    },
    {
      "epoch": 0.00145075031065213,
      "grad_norm": 8662.523997080758,
      "learning_rate": 1.5824250732048895e-07,
      "loss": 1.6826,
      "step": 399744
    },
    {
      "epoch": 0.0014508664450029669,
      "grad_norm": 9016.701170605578,
      "learning_rate": 1.582361676986855e-07,
      "loss": 1.6636,
      "step": 399776
    },
    {
      "epoch": 0.0014509825793538034,
      "grad_norm": 11054.924151707239,
      "learning_rate": 1.58229828838768e-07,
      "loss": 1.6741,
      "step": 399808
    },
    {
      "epoch": 0.0014510987137046402,
      "grad_norm": 9757.748715764308,
      "learning_rate": 1.5822349074058397e-07,
      "loss": 1.6764,
      "step": 399840
    },
    {
      "epoch": 0.001451214848055477,
      "grad_norm": 9858.377959887723,
      "learning_rate": 1.5821715340398082e-07,
      "loss": 1.6951,
      "step": 399872
    },
    {
      "epoch": 0.0014513309824063137,
      "grad_norm": 9252.968820870414,
      "learning_rate": 1.5821081682880602e-07,
      "loss": 1.7152,
      "step": 399904
    },
    {
      "epoch": 0.0014514471167571504,
      "grad_norm": 13277.753876315075,
      "learning_rate": 1.5820448101490712e-07,
      "loss": 1.6968,
      "step": 399936
    },
    {
      "epoch": 0.001451563251107987,
      "grad_norm": 11578.002418379434,
      "learning_rate": 1.5819814596213175e-07,
      "loss": 1.6828,
      "step": 399968
    },
    {
      "epoch": 0.0014516793854588237,
      "grad_norm": 15293.99359225706,
      "learning_rate": 1.5819181167032747e-07,
      "loss": 1.6676,
      "step": 400000
    },
    {
      "epoch": 0.0014517955198096605,
      "grad_norm": 9715.47219645036,
      "learning_rate": 1.5818547813934195e-07,
      "loss": 1.6679,
      "step": 400032
    },
    {
      "epoch": 0.0014519116541604972,
      "grad_norm": 9579.00558513252,
      "learning_rate": 1.5817914536902293e-07,
      "loss": 1.6534,
      "step": 400064
    },
    {
      "epoch": 0.0014520277885113337,
      "grad_norm": 12367.188039324057,
      "learning_rate": 1.581728133592181e-07,
      "loss": 1.6679,
      "step": 400096
    },
    {
      "epoch": 0.0014521439228621705,
      "grad_norm": 9613.457026481161,
      "learning_rate": 1.5816648210977533e-07,
      "loss": 1.6806,
      "step": 400128
    },
    {
      "epoch": 0.0014522600572130073,
      "grad_norm": 9751.759225903807,
      "learning_rate": 1.5816015162054243e-07,
      "loss": 1.6762,
      "step": 400160
    },
    {
      "epoch": 0.001452376191563844,
      "grad_norm": 9246.822589408754,
      "learning_rate": 1.5815382189136725e-07,
      "loss": 1.6691,
      "step": 400192
    },
    {
      "epoch": 0.0014524923259146808,
      "grad_norm": 8962.135459810905,
      "learning_rate": 1.5814749292209775e-07,
      "loss": 1.6895,
      "step": 400224
    },
    {
      "epoch": 0.0014526084602655173,
      "grad_norm": 7816.478363048157,
      "learning_rate": 1.5814116471258187e-07,
      "loss": 1.7146,
      "step": 400256
    },
    {
      "epoch": 0.001452724594616354,
      "grad_norm": 8019.614828656049,
      "learning_rate": 1.5813483726266761e-07,
      "loss": 1.6746,
      "step": 400288
    },
    {
      "epoch": 0.0014528407289671908,
      "grad_norm": 9461.085772785278,
      "learning_rate": 1.5812851057220302e-07,
      "loss": 1.6784,
      "step": 400320
    },
    {
      "epoch": 0.0014529568633180276,
      "grad_norm": 8238.345343574765,
      "learning_rate": 1.581223823148934e-07,
      "loss": 1.6855,
      "step": 400352
    },
    {
      "epoch": 0.001453072997668864,
      "grad_norm": 12533.983245560847,
      "learning_rate": 1.581160571191515e-07,
      "loss": 1.6762,
      "step": 400384
    },
    {
      "epoch": 0.0014531891320197009,
      "grad_norm": 11135.992277296173,
      "learning_rate": 1.5810973268240837e-07,
      "loss": 1.6714,
      "step": 400416
    },
    {
      "epoch": 0.0014533052663705376,
      "grad_norm": 9540.675657415464,
      "learning_rate": 1.581034090045123e-07,
      "loss": 1.692,
      "step": 400448
    },
    {
      "epoch": 0.0014534214007213744,
      "grad_norm": 7553.604437617845,
      "learning_rate": 1.5809708608531155e-07,
      "loss": 1.6711,
      "step": 400480
    },
    {
      "epoch": 0.0014535375350722111,
      "grad_norm": 9684.416554444568,
      "learning_rate": 1.580907639246544e-07,
      "loss": 1.6541,
      "step": 400512
    },
    {
      "epoch": 0.0014536536694230477,
      "grad_norm": 11292.940980984537,
      "learning_rate": 1.580844425223892e-07,
      "loss": 1.6793,
      "step": 400544
    },
    {
      "epoch": 0.0014537698037738844,
      "grad_norm": 8710.54051135749,
      "learning_rate": 1.5807812187836428e-07,
      "loss": 1.6772,
      "step": 400576
    },
    {
      "epoch": 0.0014538859381247212,
      "grad_norm": 7637.2322211649425,
      "learning_rate": 1.5807180199242817e-07,
      "loss": 1.6756,
      "step": 400608
    },
    {
      "epoch": 0.001454002072475558,
      "grad_norm": 9645.457791105615,
      "learning_rate": 1.580654828644293e-07,
      "loss": 1.6562,
      "step": 400640
    },
    {
      "epoch": 0.0014541182068263945,
      "grad_norm": 9137.139158401824,
      "learning_rate": 1.5805916449421616e-07,
      "loss": 1.672,
      "step": 400672
    },
    {
      "epoch": 0.0014542343411772312,
      "grad_norm": 9552.144052515121,
      "learning_rate": 1.5805284688163733e-07,
      "loss": 1.6845,
      "step": 400704
    },
    {
      "epoch": 0.001454350475528068,
      "grad_norm": 10241.331554050967,
      "learning_rate": 1.580465300265414e-07,
      "loss": 1.6792,
      "step": 400736
    },
    {
      "epoch": 0.0014544666098789047,
      "grad_norm": 9446.838201218437,
      "learning_rate": 1.5804021392877704e-07,
      "loss": 1.7205,
      "step": 400768
    },
    {
      "epoch": 0.0014545827442297415,
      "grad_norm": 8927.968749945308,
      "learning_rate": 1.5803389858819289e-07,
      "loss": 1.7204,
      "step": 400800
    },
    {
      "epoch": 0.001454698878580578,
      "grad_norm": 12874.892465570343,
      "learning_rate": 1.580275840046377e-07,
      "loss": 1.7026,
      "step": 400832
    },
    {
      "epoch": 0.0014548150129314148,
      "grad_norm": 10399.389982109527,
      "learning_rate": 1.5802127017796023e-07,
      "loss": 1.6762,
      "step": 400864
    },
    {
      "epoch": 0.0014549311472822515,
      "grad_norm": 9301.137564835819,
      "learning_rate": 1.580149571080093e-07,
      "loss": 1.6691,
      "step": 400896
    },
    {
      "epoch": 0.0014550472816330883,
      "grad_norm": 10801.240113986913,
      "learning_rate": 1.5800864479463377e-07,
      "loss": 1.6606,
      "step": 400928
    },
    {
      "epoch": 0.0014551634159839248,
      "grad_norm": 9222.488167517484,
      "learning_rate": 1.580023332376825e-07,
      "loss": 1.6709,
      "step": 400960
    },
    {
      "epoch": 0.0014552795503347616,
      "grad_norm": 9465.729554556268,
      "learning_rate": 1.579960224370045e-07,
      "loss": 1.6791,
      "step": 400992
    },
    {
      "epoch": 0.0014553956846855983,
      "grad_norm": 10297.17844848772,
      "learning_rate": 1.5798971239244864e-07,
      "loss": 1.677,
      "step": 401024
    },
    {
      "epoch": 0.001455511819036435,
      "grad_norm": 11689.69255369875,
      "learning_rate": 1.5798340310386405e-07,
      "loss": 1.6586,
      "step": 401056
    },
    {
      "epoch": 0.0014556279533872718,
      "grad_norm": 10906.950994663908,
      "learning_rate": 1.5797709457109974e-07,
      "loss": 1.6688,
      "step": 401088
    },
    {
      "epoch": 0.0014557440877381084,
      "grad_norm": 10615.487365166047,
      "learning_rate": 1.5797078679400482e-07,
      "loss": 1.6938,
      "step": 401120
    },
    {
      "epoch": 0.0014558602220889451,
      "grad_norm": 7940.979788414022,
      "learning_rate": 1.5796447977242844e-07,
      "loss": 1.6704,
      "step": 401152
    },
    {
      "epoch": 0.0014559763564397819,
      "grad_norm": 8608.500450136482,
      "learning_rate": 1.579581735062198e-07,
      "loss": 1.6735,
      "step": 401184
    },
    {
      "epoch": 0.0014560924907906186,
      "grad_norm": 9303.682711700782,
      "learning_rate": 1.5795186799522812e-07,
      "loss": 1.6878,
      "step": 401216
    },
    {
      "epoch": 0.0014562086251414552,
      "grad_norm": 9154.195540843553,
      "learning_rate": 1.579455632393027e-07,
      "loss": 1.6943,
      "step": 401248
    },
    {
      "epoch": 0.001456324759492292,
      "grad_norm": 9415.693283024888,
      "learning_rate": 1.5793925923829282e-07,
      "loss": 1.6609,
      "step": 401280
    },
    {
      "epoch": 0.0014564408938431287,
      "grad_norm": 9783.932849319848,
      "learning_rate": 1.5793295599204788e-07,
      "loss": 1.6861,
      "step": 401312
    },
    {
      "epoch": 0.0014565570281939654,
      "grad_norm": 9997.778953347588,
      "learning_rate": 1.5792665350041724e-07,
      "loss": 1.6923,
      "step": 401344
    },
    {
      "epoch": 0.0014566731625448022,
      "grad_norm": 8271.208859652863,
      "learning_rate": 1.5792054868111823e-07,
      "loss": 1.6762,
      "step": 401376
    },
    {
      "epoch": 0.0014567892968956387,
      "grad_norm": 8348.75667390061,
      "learning_rate": 1.5791424767469455e-07,
      "loss": 1.6707,
      "step": 401408
    },
    {
      "epoch": 0.0014569054312464755,
      "grad_norm": 9628.599482790838,
      "learning_rate": 1.5790794742243842e-07,
      "loss": 1.6752,
      "step": 401440
    },
    {
      "epoch": 0.0014570215655973122,
      "grad_norm": 8738.356596065418,
      "learning_rate": 1.5790164792419929e-07,
      "loss": 1.6711,
      "step": 401472
    },
    {
      "epoch": 0.001457137699948149,
      "grad_norm": 11584.353758410523,
      "learning_rate": 1.5789534917982686e-07,
      "loss": 1.6695,
      "step": 401504
    },
    {
      "epoch": 0.0014572538342989855,
      "grad_norm": 10040.61890522691,
      "learning_rate": 1.578890511891707e-07,
      "loss": 1.6829,
      "step": 401536
    },
    {
      "epoch": 0.0014573699686498223,
      "grad_norm": 10255.040224201952,
      "learning_rate": 1.5788275395208056e-07,
      "loss": 1.6833,
      "step": 401568
    },
    {
      "epoch": 0.001457486103000659,
      "grad_norm": 11650.427974971562,
      "learning_rate": 1.5787645746840614e-07,
      "loss": 1.6716,
      "step": 401600
    },
    {
      "epoch": 0.0014576022373514958,
      "grad_norm": 10056.402338808844,
      "learning_rate": 1.5787016173799725e-07,
      "loss": 1.6955,
      "step": 401632
    },
    {
      "epoch": 0.0014577183717023325,
      "grad_norm": 8888.197230034895,
      "learning_rate": 1.578638667607037e-07,
      "loss": 1.6945,
      "step": 401664
    },
    {
      "epoch": 0.001457834506053169,
      "grad_norm": 10493.206754848587,
      "learning_rate": 1.5785757253637535e-07,
      "loss": 1.6989,
      "step": 401696
    },
    {
      "epoch": 0.0014579506404040058,
      "grad_norm": 10930.593579490547,
      "learning_rate": 1.5785127906486209e-07,
      "loss": 1.6826,
      "step": 401728
    },
    {
      "epoch": 0.0014580667747548426,
      "grad_norm": 11115.49890918082,
      "learning_rate": 1.5784498634601388e-07,
      "loss": 1.6758,
      "step": 401760
    },
    {
      "epoch": 0.0014581829091056793,
      "grad_norm": 12047.130529715365,
      "learning_rate": 1.5783869437968068e-07,
      "loss": 1.6753,
      "step": 401792
    },
    {
      "epoch": 0.0014582990434565159,
      "grad_norm": 10062.244481227834,
      "learning_rate": 1.5783240316571259e-07,
      "loss": 1.6775,
      "step": 401824
    },
    {
      "epoch": 0.0014584151778073526,
      "grad_norm": 9723.479624085197,
      "learning_rate": 1.5782611270395962e-07,
      "loss": 1.702,
      "step": 401856
    },
    {
      "epoch": 0.0014585313121581894,
      "grad_norm": 9564.310325371087,
      "learning_rate": 1.5781982299427193e-07,
      "loss": 1.7139,
      "step": 401888
    },
    {
      "epoch": 0.0014586474465090261,
      "grad_norm": 9940.544854282385,
      "learning_rate": 1.5781353403649957e-07,
      "loss": 1.6912,
      "step": 401920
    },
    {
      "epoch": 0.0014587635808598629,
      "grad_norm": 10304.8157673973,
      "learning_rate": 1.5780724583049285e-07,
      "loss": 1.6734,
      "step": 401952
    },
    {
      "epoch": 0.0014588797152106994,
      "grad_norm": 14137.759016194894,
      "learning_rate": 1.5780095837610198e-07,
      "loss": 1.6971,
      "step": 401984
    },
    {
      "epoch": 0.0014589958495615362,
      "grad_norm": 8178.489958421419,
      "learning_rate": 1.5779467167317723e-07,
      "loss": 1.6898,
      "step": 402016
    },
    {
      "epoch": 0.001459111983912373,
      "grad_norm": 8819.504407845148,
      "learning_rate": 1.577883857215689e-07,
      "loss": 1.6801,
      "step": 402048
    },
    {
      "epoch": 0.0014592281182632097,
      "grad_norm": 9344.188354265982,
      "learning_rate": 1.5778210052112738e-07,
      "loss": 1.6967,
      "step": 402080
    },
    {
      "epoch": 0.0014593442526140462,
      "grad_norm": 8954.783637810577,
      "learning_rate": 1.5777581607170308e-07,
      "loss": 1.6874,
      "step": 402112
    },
    {
      "epoch": 0.001459460386964883,
      "grad_norm": 8229.269833952463,
      "learning_rate": 1.5776953237314643e-07,
      "loss": 1.6678,
      "step": 402144
    },
    {
      "epoch": 0.0014595765213157197,
      "grad_norm": 7768.446949036853,
      "learning_rate": 1.5776324942530793e-07,
      "loss": 1.6728,
      "step": 402176
    },
    {
      "epoch": 0.0014596926556665565,
      "grad_norm": 8267.751084787205,
      "learning_rate": 1.577569672280381e-07,
      "loss": 1.6867,
      "step": 402208
    },
    {
      "epoch": 0.0014598087900173932,
      "grad_norm": 10551.254712118365,
      "learning_rate": 1.577506857811875e-07,
      "loss": 1.6745,
      "step": 402240
    },
    {
      "epoch": 0.0014599249243682298,
      "grad_norm": 7815.412593075301,
      "learning_rate": 1.577444050846068e-07,
      "loss": 1.6829,
      "step": 402272
    },
    {
      "epoch": 0.0014600410587190665,
      "grad_norm": 8063.927331021777,
      "learning_rate": 1.577381251381466e-07,
      "loss": 1.6911,
      "step": 402304
    },
    {
      "epoch": 0.0014601571930699033,
      "grad_norm": 10071.669970764531,
      "learning_rate": 1.577318459416576e-07,
      "loss": 1.6956,
      "step": 402336
    },
    {
      "epoch": 0.00146027332742074,
      "grad_norm": 9717.94463865688,
      "learning_rate": 1.577255674949906e-07,
      "loss": 1.6847,
      "step": 402368
    },
    {
      "epoch": 0.0014603894617715766,
      "grad_norm": 11243.008316282614,
      "learning_rate": 1.5771948596468124e-07,
      "loss": 1.6896,
      "step": 402400
    },
    {
      "epoch": 0.0014605055961224133,
      "grad_norm": 9250.313724409567,
      "learning_rate": 1.5771320899379016e-07,
      "loss": 1.6939,
      "step": 402432
    },
    {
      "epoch": 0.00146062173047325,
      "grad_norm": 13019.465580429944,
      "learning_rate": 1.5770693277227815e-07,
      "loss": 1.6765,
      "step": 402464
    },
    {
      "epoch": 0.0014607378648240868,
      "grad_norm": 8566.551348121366,
      "learning_rate": 1.577006572999961e-07,
      "loss": 1.6924,
      "step": 402496
    },
    {
      "epoch": 0.0014608539991749236,
      "grad_norm": 10321.66440066717,
      "learning_rate": 1.5769438257679502e-07,
      "loss": 1.6958,
      "step": 402528
    },
    {
      "epoch": 0.0014609701335257601,
      "grad_norm": 9959.01119589691,
      "learning_rate": 1.5768810860252582e-07,
      "loss": 1.692,
      "step": 402560
    },
    {
      "epoch": 0.0014610862678765969,
      "grad_norm": 10693.640166005212,
      "learning_rate": 1.5768183537703957e-07,
      "loss": 1.6821,
      "step": 402592
    },
    {
      "epoch": 0.0014612024022274336,
      "grad_norm": 8393.664872986055,
      "learning_rate": 1.5767556290018732e-07,
      "loss": 1.6758,
      "step": 402624
    },
    {
      "epoch": 0.0014613185365782704,
      "grad_norm": 9631.106686149833,
      "learning_rate": 1.576692911718202e-07,
      "loss": 1.6698,
      "step": 402656
    },
    {
      "epoch": 0.001461434670929107,
      "grad_norm": 8990.619111051252,
      "learning_rate": 1.5766302019178938e-07,
      "loss": 1.651,
      "step": 402688
    },
    {
      "epoch": 0.0014615508052799437,
      "grad_norm": 9146.658187556808,
      "learning_rate": 1.57656749959946e-07,
      "loss": 1.6757,
      "step": 402720
    },
    {
      "epoch": 0.0014616669396307804,
      "grad_norm": 8326.299778412978,
      "learning_rate": 1.5765048047614134e-07,
      "loss": 1.6738,
      "step": 402752
    },
    {
      "epoch": 0.0014617830739816172,
      "grad_norm": 11338.923405685391,
      "learning_rate": 1.5764421174022665e-07,
      "loss": 1.6635,
      "step": 402784
    },
    {
      "epoch": 0.001461899208332454,
      "grad_norm": 10424.983261377449,
      "learning_rate": 1.5763794375205324e-07,
      "loss": 1.6614,
      "step": 402816
    },
    {
      "epoch": 0.0014620153426832905,
      "grad_norm": 9018.49011753076,
      "learning_rate": 1.5763167651147254e-07,
      "loss": 1.6828,
      "step": 402848
    },
    {
      "epoch": 0.0014621314770341272,
      "grad_norm": 9590.538671002792,
      "learning_rate": 1.5762541001833584e-07,
      "loss": 1.6921,
      "step": 402880
    },
    {
      "epoch": 0.001462247611384964,
      "grad_norm": 9923.721882439067,
      "learning_rate": 1.576191442724947e-07,
      "loss": 1.669,
      "step": 402912
    },
    {
      "epoch": 0.0014623637457358007,
      "grad_norm": 7354.231706983402,
      "learning_rate": 1.576128792738005e-07,
      "loss": 1.6936,
      "step": 402944
    },
    {
      "epoch": 0.0014624798800866373,
      "grad_norm": 12320.90029178063,
      "learning_rate": 1.5760661502210486e-07,
      "loss": 1.6894,
      "step": 402976
    },
    {
      "epoch": 0.001462596014437474,
      "grad_norm": 12102.847268308396,
      "learning_rate": 1.5760035151725926e-07,
      "loss": 1.6728,
      "step": 403008
    },
    {
      "epoch": 0.0014627121487883108,
      "grad_norm": 11802.152176615924,
      "learning_rate": 1.5759408875911535e-07,
      "loss": 1.6523,
      "step": 403040
    },
    {
      "epoch": 0.0014628282831391475,
      "grad_norm": 9539.17187181361,
      "learning_rate": 1.5758782674752479e-07,
      "loss": 1.666,
      "step": 403072
    },
    {
      "epoch": 0.0014629444174899843,
      "grad_norm": 9279.204491765444,
      "learning_rate": 1.5758156548233923e-07,
      "loss": 1.6564,
      "step": 403104
    },
    {
      "epoch": 0.0014630605518408208,
      "grad_norm": 9247.931228117995,
      "learning_rate": 1.5757530496341043e-07,
      "loss": 1.6731,
      "step": 403136
    },
    {
      "epoch": 0.0014631766861916576,
      "grad_norm": 9455.642759749335,
      "learning_rate": 1.5756904519059017e-07,
      "loss": 1.6806,
      "step": 403168
    },
    {
      "epoch": 0.0014632928205424943,
      "grad_norm": 9244.213541453919,
      "learning_rate": 1.5756278616373026e-07,
      "loss": 1.6732,
      "step": 403200
    },
    {
      "epoch": 0.001463408954893331,
      "grad_norm": 9671.9791149485,
      "learning_rate": 1.575565278826825e-07,
      "loss": 1.6669,
      "step": 403232
    },
    {
      "epoch": 0.0014635250892441676,
      "grad_norm": 11581.419774794453,
      "learning_rate": 1.5755027034729887e-07,
      "loss": 1.6573,
      "step": 403264
    },
    {
      "epoch": 0.0014636412235950044,
      "grad_norm": 13180.729721832553,
      "learning_rate": 1.5754401355743126e-07,
      "loss": 1.676,
      "step": 403296
    },
    {
      "epoch": 0.0014637573579458411,
      "grad_norm": 9359.546142842612,
      "learning_rate": 1.5753775751293166e-07,
      "loss": 1.6727,
      "step": 403328
    },
    {
      "epoch": 0.001463873492296678,
      "grad_norm": 10826.33973233798,
      "learning_rate": 1.5753150221365207e-07,
      "loss": 1.6805,
      "step": 403360
    },
    {
      "epoch": 0.0014639896266475147,
      "grad_norm": 18003.181052247404,
      "learning_rate": 1.575252476594446e-07,
      "loss": 1.6978,
      "step": 403392
    },
    {
      "epoch": 0.0014641057609983512,
      "grad_norm": 17281.301802815666,
      "learning_rate": 1.575189938501613e-07,
      "loss": 1.7008,
      "step": 403424
    },
    {
      "epoch": 0.001464221895349188,
      "grad_norm": 12226.64810976418,
      "learning_rate": 1.575129361826482e-07,
      "loss": 1.6857,
      "step": 403456
    },
    {
      "epoch": 0.0014643380297000247,
      "grad_norm": 11345.99894235849,
      "learning_rate": 1.5750668383950234e-07,
      "loss": 1.6992,
      "step": 403488
    },
    {
      "epoch": 0.0014644541640508615,
      "grad_norm": 9873.024055475607,
      "learning_rate": 1.5750043224084183e-07,
      "loss": 1.6921,
      "step": 403520
    },
    {
      "epoch": 0.001464570298401698,
      "grad_norm": 10921.086575977684,
      "learning_rate": 1.57494181386519e-07,
      "loss": 1.6585,
      "step": 403552
    },
    {
      "epoch": 0.0014646864327525347,
      "grad_norm": 8702.282574129618,
      "learning_rate": 1.5748793127638606e-07,
      "loss": 1.6655,
      "step": 403584
    },
    {
      "epoch": 0.0014648025671033715,
      "grad_norm": 10071.681686788954,
      "learning_rate": 1.574816819102954e-07,
      "loss": 1.6708,
      "step": 403616
    },
    {
      "epoch": 0.0014649187014542083,
      "grad_norm": 9665.934202134835,
      "learning_rate": 1.574754332880994e-07,
      "loss": 1.6779,
      "step": 403648
    },
    {
      "epoch": 0.001465034835805045,
      "grad_norm": 9561.715118115577,
      "learning_rate": 1.5746918540965052e-07,
      "loss": 1.6563,
      "step": 403680
    },
    {
      "epoch": 0.0014651509701558815,
      "grad_norm": 12626.259937131026,
      "learning_rate": 1.5746293827480115e-07,
      "loss": 1.6702,
      "step": 403712
    },
    {
      "epoch": 0.0014652671045067183,
      "grad_norm": 9095.56760185971,
      "learning_rate": 1.574566918834039e-07,
      "loss": 1.6871,
      "step": 403744
    },
    {
      "epoch": 0.001465383238857555,
      "grad_norm": 9909.314809814046,
      "learning_rate": 1.5745044623531126e-07,
      "loss": 1.6594,
      "step": 403776
    },
    {
      "epoch": 0.0014654993732083918,
      "grad_norm": 9118.284049096079,
      "learning_rate": 1.5744420133037586e-07,
      "loss": 1.6694,
      "step": 403808
    },
    {
      "epoch": 0.0014656155075592283,
      "grad_norm": 7991.149354129229,
      "learning_rate": 1.574379571684503e-07,
      "loss": 1.678,
      "step": 403840
    },
    {
      "epoch": 0.001465731641910065,
      "grad_norm": 12027.83721206768,
      "learning_rate": 1.5743171374938727e-07,
      "loss": 1.6768,
      "step": 403872
    },
    {
      "epoch": 0.0014658477762609019,
      "grad_norm": 9852.598438990599,
      "learning_rate": 1.5742547107303948e-07,
      "loss": 1.6602,
      "step": 403904
    },
    {
      "epoch": 0.0014659639106117386,
      "grad_norm": 10200.407834983855,
      "learning_rate": 1.5741922913925968e-07,
      "loss": 1.6669,
      "step": 403936
    },
    {
      "epoch": 0.0014660800449625754,
      "grad_norm": 9873.960704803316,
      "learning_rate": 1.574129879479007e-07,
      "loss": 1.6758,
      "step": 403968
    },
    {
      "epoch": 0.001466196179313412,
      "grad_norm": 8391.663363124142,
      "learning_rate": 1.5740674749881536e-07,
      "loss": 1.6679,
      "step": 404000
    },
    {
      "epoch": 0.0014663123136642487,
      "grad_norm": 9066.75642112437,
      "learning_rate": 1.574005077918565e-07,
      "loss": 1.691,
      "step": 404032
    },
    {
      "epoch": 0.0014664284480150854,
      "grad_norm": 10394.30343986551,
      "learning_rate": 1.573942688268771e-07,
      "loss": 1.6923,
      "step": 404064
    },
    {
      "epoch": 0.0014665445823659222,
      "grad_norm": 10208.291140048857,
      "learning_rate": 1.5738803060373007e-07,
      "loss": 1.6667,
      "step": 404096
    },
    {
      "epoch": 0.0014666607167167587,
      "grad_norm": 11037.177175346964,
      "learning_rate": 1.5738179312226845e-07,
      "loss": 1.6585,
      "step": 404128
    },
    {
      "epoch": 0.0014667768510675955,
      "grad_norm": 9660.94115498071,
      "learning_rate": 1.5737555638234527e-07,
      "loss": 1.6769,
      "step": 404160
    },
    {
      "epoch": 0.0014668929854184322,
      "grad_norm": 10103.078936641048,
      "learning_rate": 1.573693203838136e-07,
      "loss": 1.6733,
      "step": 404192
    },
    {
      "epoch": 0.001467009119769269,
      "grad_norm": 9122.319222653854,
      "learning_rate": 1.5736308512652657e-07,
      "loss": 1.6847,
      "step": 404224
    },
    {
      "epoch": 0.0014671252541201057,
      "grad_norm": 9383.340130252127,
      "learning_rate": 1.5735685061033738e-07,
      "loss": 1.6979,
      "step": 404256
    },
    {
      "epoch": 0.0014672413884709423,
      "grad_norm": 9773.500908067692,
      "learning_rate": 1.5735061683509918e-07,
      "loss": 1.7039,
      "step": 404288
    },
    {
      "epoch": 0.001467357522821779,
      "grad_norm": 10501.103561054904,
      "learning_rate": 1.5734438380066525e-07,
      "loss": 1.6866,
      "step": 404320
    },
    {
      "epoch": 0.0014674736571726158,
      "grad_norm": 10625.966497218029,
      "learning_rate": 1.5733815150688884e-07,
      "loss": 1.6793,
      "step": 404352
    },
    {
      "epoch": 0.0014675897915234525,
      "grad_norm": 9794.144373042496,
      "learning_rate": 1.5733191995362333e-07,
      "loss": 1.687,
      "step": 404384
    },
    {
      "epoch": 0.001467705925874289,
      "grad_norm": 12828.989360039239,
      "learning_rate": 1.5732568914072206e-07,
      "loss": 1.6582,
      "step": 404416
    },
    {
      "epoch": 0.0014678220602251258,
      "grad_norm": 26076.30495296448,
      "learning_rate": 1.573194590680384e-07,
      "loss": 1.6676,
      "step": 404448
    },
    {
      "epoch": 0.0014679381945759626,
      "grad_norm": 21290.604500577247,
      "learning_rate": 1.573132297354259e-07,
      "loss": 1.6836,
      "step": 404480
    },
    {
      "epoch": 0.0014680543289267993,
      "grad_norm": 10168.896498637401,
      "learning_rate": 1.5730719577506086e-07,
      "loss": 1.6819,
      "step": 404512
    },
    {
      "epoch": 0.001468170463277636,
      "grad_norm": 9820.108960698959,
      "learning_rate": 1.573009678990352e-07,
      "loss": 1.6671,
      "step": 404544
    },
    {
      "epoch": 0.0014682865976284726,
      "grad_norm": 8139.321593351623,
      "learning_rate": 1.572947407626458e-07,
      "loss": 1.6853,
      "step": 404576
    },
    {
      "epoch": 0.0014684027319793094,
      "grad_norm": 9491.437193597185,
      "learning_rate": 1.5728851436574624e-07,
      "loss": 1.7105,
      "step": 404608
    },
    {
      "epoch": 0.0014685188663301461,
      "grad_norm": 12058.614845827027,
      "learning_rate": 1.5728228870819025e-07,
      "loss": 1.6791,
      "step": 404640
    },
    {
      "epoch": 0.0014686350006809829,
      "grad_norm": 9190.088791736454,
      "learning_rate": 1.5727606378983146e-07,
      "loss": 1.6642,
      "step": 404672
    },
    {
      "epoch": 0.0014687511350318194,
      "grad_norm": 13913.238156518417,
      "learning_rate": 1.5726983961052362e-07,
      "loss": 1.6737,
      "step": 404704
    },
    {
      "epoch": 0.0014688672693826562,
      "grad_norm": 8665.81490686248,
      "learning_rate": 1.572636161701205e-07,
      "loss": 1.676,
      "step": 404736
    },
    {
      "epoch": 0.001468983403733493,
      "grad_norm": 8994.285074423648,
      "learning_rate": 1.572573934684759e-07,
      "loss": 1.6614,
      "step": 404768
    },
    {
      "epoch": 0.0014690995380843297,
      "grad_norm": 8323.060614942078,
      "learning_rate": 1.572511715054437e-07,
      "loss": 1.6662,
      "step": 404800
    },
    {
      "epoch": 0.0014692156724351664,
      "grad_norm": 9175.820617252715,
      "learning_rate": 1.572449502808778e-07,
      "loss": 1.6771,
      "step": 404832
    },
    {
      "epoch": 0.001469331806786003,
      "grad_norm": 9577.599281657172,
      "learning_rate": 1.572387297946321e-07,
      "loss": 1.6477,
      "step": 404864
    },
    {
      "epoch": 0.0014694479411368397,
      "grad_norm": 9236.824995635676,
      "learning_rate": 1.572325100465606e-07,
      "loss": 1.6779,
      "step": 404896
    },
    {
      "epoch": 0.0014695640754876765,
      "grad_norm": 10613.16371304994,
      "learning_rate": 1.572262910365173e-07,
      "loss": 1.6767,
      "step": 404928
    },
    {
      "epoch": 0.0014696802098385132,
      "grad_norm": 11350.46642213438,
      "learning_rate": 1.572200727643562e-07,
      "loss": 1.6657,
      "step": 404960
    },
    {
      "epoch": 0.0014697963441893498,
      "grad_norm": 10578.770817065657,
      "learning_rate": 1.5721385522993154e-07,
      "loss": 1.6574,
      "step": 404992
    },
    {
      "epoch": 0.0014699124785401865,
      "grad_norm": 9699.728655998579,
      "learning_rate": 1.5720763843309734e-07,
      "loss": 1.6744,
      "step": 405024
    },
    {
      "epoch": 0.0014700286128910233,
      "grad_norm": 10099.324333835408,
      "learning_rate": 1.5720142237370781e-07,
      "loss": 1.6789,
      "step": 405056
    },
    {
      "epoch": 0.00147014474724186,
      "grad_norm": 9816.094539072044,
      "learning_rate": 1.5719520705161716e-07,
      "loss": 1.682,
      "step": 405088
    },
    {
      "epoch": 0.0014702608815926968,
      "grad_norm": 9716.802766342435,
      "learning_rate": 1.5718899246667967e-07,
      "loss": 1.7237,
      "step": 405120
    },
    {
      "epoch": 0.0014703770159435333,
      "grad_norm": 11212.838891199677,
      "learning_rate": 1.5718277861874963e-07,
      "loss": 1.7203,
      "step": 405152
    },
    {
      "epoch": 0.00147049315029437,
      "grad_norm": 10693.757244299124,
      "learning_rate": 1.5717656550768134e-07,
      "loss": 1.6898,
      "step": 405184
    },
    {
      "epoch": 0.0014706092846452068,
      "grad_norm": 9770.712972961595,
      "learning_rate": 1.5717035313332924e-07,
      "loss": 1.6856,
      "step": 405216
    },
    {
      "epoch": 0.0014707254189960436,
      "grad_norm": 11053.537352359199,
      "learning_rate": 1.571641414955477e-07,
      "loss": 1.6912,
      "step": 405248
    },
    {
      "epoch": 0.0014708415533468801,
      "grad_norm": 9112.749530191204,
      "learning_rate": 1.571579305941912e-07,
      "loss": 1.6742,
      "step": 405280
    },
    {
      "epoch": 0.0014709576876977169,
      "grad_norm": 9110.93441969593,
      "learning_rate": 1.5715172042911422e-07,
      "loss": 1.6796,
      "step": 405312
    },
    {
      "epoch": 0.0014710738220485536,
      "grad_norm": 9227.792368708779,
      "learning_rate": 1.5714551100017136e-07,
      "loss": 1.6813,
      "step": 405344
    },
    {
      "epoch": 0.0014711899563993904,
      "grad_norm": 8398.386630776175,
      "learning_rate": 1.5713930230721713e-07,
      "loss": 1.6944,
      "step": 405376
    },
    {
      "epoch": 0.0014713060907502271,
      "grad_norm": 10255.62928347159,
      "learning_rate": 1.5713309435010614e-07,
      "loss": 1.669,
      "step": 405408
    },
    {
      "epoch": 0.0014714222251010637,
      "grad_norm": 11839.06313861025,
      "learning_rate": 1.5712688712869312e-07,
      "loss": 1.6519,
      "step": 405440
    },
    {
      "epoch": 0.0014715383594519004,
      "grad_norm": 10077.657465899503,
      "learning_rate": 1.571206806428327e-07,
      "loss": 1.681,
      "step": 405472
    },
    {
      "epoch": 0.0014716544938027372,
      "grad_norm": 19232.31447330248,
      "learning_rate": 1.5711447489237972e-07,
      "loss": 1.6762,
      "step": 405504
    },
    {
      "epoch": 0.001471770628153574,
      "grad_norm": 19782.50297611503,
      "learning_rate": 1.5710826987718886e-07,
      "loss": 1.6729,
      "step": 405536
    },
    {
      "epoch": 0.0014718867625044105,
      "grad_norm": 19373.32557925975,
      "learning_rate": 1.5710206559711495e-07,
      "loss": 1.6733,
      "step": 405568
    },
    {
      "epoch": 0.0014720028968552472,
      "grad_norm": 17123.80985645426,
      "learning_rate": 1.5709586205201293e-07,
      "loss": 1.681,
      "step": 405600
    },
    {
      "epoch": 0.001472119031206084,
      "grad_norm": 19547.192944256727,
      "learning_rate": 1.5708965924173758e-07,
      "loss": 1.6653,
      "step": 405632
    },
    {
      "epoch": 0.0014722351655569207,
      "grad_norm": 17958.15135251956,
      "learning_rate": 1.57083457166144e-07,
      "loss": 1.6732,
      "step": 405664
    },
    {
      "epoch": 0.0014723512999077575,
      "grad_norm": 20708.253813395277,
      "learning_rate": 1.57077255825087e-07,
      "loss": 1.6983,
      "step": 405696
    },
    {
      "epoch": 0.001472467434258594,
      "grad_norm": 19301.38523526226,
      "learning_rate": 1.5707105521842174e-07,
      "loss": 1.6556,
      "step": 405728
    },
    {
      "epoch": 0.0014725835686094308,
      "grad_norm": 9923.865779019787,
      "learning_rate": 1.5706504908090361e-07,
      "loss": 1.6751,
      "step": 405760
    },
    {
      "epoch": 0.0014726997029602675,
      "grad_norm": 8434.943983216486,
      "learning_rate": 1.5705884991964843e-07,
      "loss": 1.6754,
      "step": 405792
    },
    {
      "epoch": 0.0014728158373111043,
      "grad_norm": 8632.781591121138,
      "learning_rate": 1.5705265149235476e-07,
      "loss": 1.6789,
      "step": 405824
    },
    {
      "epoch": 0.0014729319716619408,
      "grad_norm": 8838.015614378603,
      "learning_rate": 1.5704645379887777e-07,
      "loss": 1.6711,
      "step": 405856
    },
    {
      "epoch": 0.0014730481060127776,
      "grad_norm": 17494.8883963288,
      "learning_rate": 1.5704025683907267e-07,
      "loss": 1.6587,
      "step": 405888
    },
    {
      "epoch": 0.0014731642403636143,
      "grad_norm": 9788.30935350942,
      "learning_rate": 1.570340606127948e-07,
      "loss": 1.6762,
      "step": 405920
    },
    {
      "epoch": 0.001473280374714451,
      "grad_norm": 10766.548007602065,
      "learning_rate": 1.5702786511989937e-07,
      "loss": 1.6656,
      "step": 405952
    },
    {
      "epoch": 0.0014733965090652878,
      "grad_norm": 9695.032233056269,
      "learning_rate": 1.5702167036024174e-07,
      "loss": 1.6915,
      "step": 405984
    },
    {
      "epoch": 0.0014735126434161244,
      "grad_norm": 10673.769718332882,
      "learning_rate": 1.5701547633367734e-07,
      "loss": 1.7022,
      "step": 406016
    },
    {
      "epoch": 0.0014736287777669611,
      "grad_norm": 11030.97148940201,
      "learning_rate": 1.5700928304006153e-07,
      "loss": 1.696,
      "step": 406048
    },
    {
      "epoch": 0.0014737449121177979,
      "grad_norm": 9594.00854700474,
      "learning_rate": 1.570030904792498e-07,
      "loss": 1.6865,
      "step": 406080
    },
    {
      "epoch": 0.0014738610464686346,
      "grad_norm": 9870.593295238134,
      "learning_rate": 1.5699689865109764e-07,
      "loss": 1.6964,
      "step": 406112
    },
    {
      "epoch": 0.0014739771808194712,
      "grad_norm": 9137.112673049403,
      "learning_rate": 1.569907075554606e-07,
      "loss": 1.6932,
      "step": 406144
    },
    {
      "epoch": 0.001474093315170308,
      "grad_norm": 9482.190464233463,
      "learning_rate": 1.5698451719219423e-07,
      "loss": 1.6811,
      "step": 406176
    },
    {
      "epoch": 0.0014742094495211447,
      "grad_norm": 7319.153366339579,
      "learning_rate": 1.5697832756115418e-07,
      "loss": 1.6968,
      "step": 406208
    },
    {
      "epoch": 0.0014743255838719814,
      "grad_norm": 8750.49621450121,
      "learning_rate": 1.569721386621961e-07,
      "loss": 1.6947,
      "step": 406240
    },
    {
      "epoch": 0.0014744417182228182,
      "grad_norm": 10136.154892265606,
      "learning_rate": 1.5696595049517568e-07,
      "loss": 1.6647,
      "step": 406272
    },
    {
      "epoch": 0.0014745578525736547,
      "grad_norm": 10036.842531394024,
      "learning_rate": 1.5695976305994871e-07,
      "loss": 1.6571,
      "step": 406304
    },
    {
      "epoch": 0.0014746739869244915,
      "grad_norm": 11211.269865630744,
      "learning_rate": 1.569535763563709e-07,
      "loss": 1.6665,
      "step": 406336
    },
    {
      "epoch": 0.0014747901212753282,
      "grad_norm": 13505.7981622709,
      "learning_rate": 1.5694739038429807e-07,
      "loss": 1.6841,
      "step": 406368
    },
    {
      "epoch": 0.001474906255626165,
      "grad_norm": 11442.251351897492,
      "learning_rate": 1.5694120514358614e-07,
      "loss": 1.6764,
      "step": 406400
    },
    {
      "epoch": 0.0014750223899770015,
      "grad_norm": 9225.50638176572,
      "learning_rate": 1.5693502063409094e-07,
      "loss": 1.6768,
      "step": 406432
    },
    {
      "epoch": 0.0014751385243278383,
      "grad_norm": 10286.084969510996,
      "learning_rate": 1.5692883685566845e-07,
      "loss": 1.677,
      "step": 406464
    },
    {
      "epoch": 0.001475254658678675,
      "grad_norm": 8890.148030263614,
      "learning_rate": 1.5692265380817467e-07,
      "loss": 1.6643,
      "step": 406496
    },
    {
      "epoch": 0.0014753707930295118,
      "grad_norm": 9488.682521825673,
      "learning_rate": 1.5691647149146555e-07,
      "loss": 1.6551,
      "step": 406528
    },
    {
      "epoch": 0.0014754869273803485,
      "grad_norm": 9356.969594906248,
      "learning_rate": 1.5691028990539718e-07,
      "loss": 1.6695,
      "step": 406560
    },
    {
      "epoch": 0.001475603061731185,
      "grad_norm": 10409.940057464308,
      "learning_rate": 1.5690410904982564e-07,
      "loss": 1.6599,
      "step": 406592
    },
    {
      "epoch": 0.0014757191960820218,
      "grad_norm": 8133.818660383326,
      "learning_rate": 1.5689792892460715e-07,
      "loss": 1.676,
      "step": 406624
    },
    {
      "epoch": 0.0014758353304328586,
      "grad_norm": 9249.578152542957,
      "learning_rate": 1.5689174952959776e-07,
      "loss": 1.6846,
      "step": 406656
    },
    {
      "epoch": 0.0014759514647836953,
      "grad_norm": 10346.578951518226,
      "learning_rate": 1.5688557086465377e-07,
      "loss": 1.6857,
      "step": 406688
    },
    {
      "epoch": 0.0014760675991345319,
      "grad_norm": 9844.920416133387,
      "learning_rate": 1.568793929296314e-07,
      "loss": 1.679,
      "step": 406720
    },
    {
      "epoch": 0.0014761837334853686,
      "grad_norm": 20035.317616648856,
      "learning_rate": 1.5687321572438696e-07,
      "loss": 1.6919,
      "step": 406752
    },
    {
      "epoch": 0.0014762998678362054,
      "grad_norm": 19446.554861980054,
      "learning_rate": 1.5686703924877679e-07,
      "loss": 1.7039,
      "step": 406784
    },
    {
      "epoch": 0.0014764160021870421,
      "grad_norm": 23203.4414688856,
      "learning_rate": 1.5686086350265722e-07,
      "loss": 1.6774,
      "step": 406816
    },
    {
      "epoch": 0.001476532136537879,
      "grad_norm": 15191.80410616198,
      "learning_rate": 1.5685468848588474e-07,
      "loss": 1.699,
      "step": 406848
    },
    {
      "epoch": 0.0014766482708887154,
      "grad_norm": 16835.68448266954,
      "learning_rate": 1.5684851419831573e-07,
      "loss": 1.7079,
      "step": 406880
    },
    {
      "epoch": 0.0014767644052395522,
      "grad_norm": 8017.568958231665,
      "learning_rate": 1.5684253355247597e-07,
      "loss": 1.7178,
      "step": 406912
    },
    {
      "epoch": 0.001476880539590389,
      "grad_norm": 10165.069404583523,
      "learning_rate": 1.5683636070010703e-07,
      "loss": 1.6959,
      "step": 406944
    },
    {
      "epoch": 0.0014769966739412257,
      "grad_norm": 9257.955281810342,
      "learning_rate": 1.5683018857651564e-07,
      "loss": 1.6997,
      "step": 406976
    },
    {
      "epoch": 0.0014771128082920622,
      "grad_norm": 9575.53277891105,
      "learning_rate": 1.5682401718155848e-07,
      "loss": 1.7012,
      "step": 407008
    },
    {
      "epoch": 0.001477228942642899,
      "grad_norm": 11096.78403863029,
      "learning_rate": 1.5681784651509214e-07,
      "loss": 1.6646,
      "step": 407040
    },
    {
      "epoch": 0.0014773450769937357,
      "grad_norm": 8089.038632618835,
      "learning_rate": 1.568116765769733e-07,
      "loss": 1.6837,
      "step": 407072
    },
    {
      "epoch": 0.0014774612113445725,
      "grad_norm": 13476.258234391327,
      "learning_rate": 1.5680550736705876e-07,
      "loss": 1.6904,
      "step": 407104
    },
    {
      "epoch": 0.0014775773456954092,
      "grad_norm": 8526.212054599628,
      "learning_rate": 1.567993388852052e-07,
      "loss": 1.6866,
      "step": 407136
    },
    {
      "epoch": 0.0014776934800462458,
      "grad_norm": 11146.990984117641,
      "learning_rate": 1.5679317113126952e-07,
      "loss": 1.6688,
      "step": 407168
    },
    {
      "epoch": 0.0014778096143970825,
      "grad_norm": 11470.307406517055,
      "learning_rate": 1.5678700410510848e-07,
      "loss": 1.6832,
      "step": 407200
    },
    {
      "epoch": 0.0014779257487479193,
      "grad_norm": 8533.244283389524,
      "learning_rate": 1.5678083780657902e-07,
      "loss": 1.7017,
      "step": 407232
    },
    {
      "epoch": 0.001478041883098756,
      "grad_norm": 13824.529937759185,
      "learning_rate": 1.5677467223553805e-07,
      "loss": 1.702,
      "step": 407264
    },
    {
      "epoch": 0.0014781580174495926,
      "grad_norm": 9251.81928055234,
      "learning_rate": 1.5676850739184253e-07,
      "loss": 1.7134,
      "step": 407296
    },
    {
      "epoch": 0.0014782741518004293,
      "grad_norm": 8348.108288708287,
      "learning_rate": 1.5676234327534946e-07,
      "loss": 1.6889,
      "step": 407328
    },
    {
      "epoch": 0.001478390286151266,
      "grad_norm": 8052.936110512736,
      "learning_rate": 1.567561798859159e-07,
      "loss": 1.6566,
      "step": 407360
    },
    {
      "epoch": 0.0014785064205021028,
      "grad_norm": 8291.07061844247,
      "learning_rate": 1.5675001722339894e-07,
      "loss": 1.6526,
      "step": 407392
    },
    {
      "epoch": 0.0014786225548529396,
      "grad_norm": 8467.86194974859,
      "learning_rate": 1.567438552876557e-07,
      "loss": 1.6642,
      "step": 407424
    },
    {
      "epoch": 0.0014787386892037761,
      "grad_norm": 9259.50041848911,
      "learning_rate": 1.5673769407854329e-07,
      "loss": 1.6653,
      "step": 407456
    },
    {
      "epoch": 0.001478854823554613,
      "grad_norm": 10493.73527396227,
      "learning_rate": 1.5673153359591897e-07,
      "loss": 1.6615,
      "step": 407488
    },
    {
      "epoch": 0.0014789709579054496,
      "grad_norm": 7863.19667819647,
      "learning_rate": 1.5672537383963996e-07,
      "loss": 1.6754,
      "step": 407520
    },
    {
      "epoch": 0.0014790870922562864,
      "grad_norm": 11330.510315074074,
      "learning_rate": 1.5671921480956354e-07,
      "loss": 1.6684,
      "step": 407552
    },
    {
      "epoch": 0.001479203226607123,
      "grad_norm": 9269.0446109618,
      "learning_rate": 1.5671305650554704e-07,
      "loss": 1.6631,
      "step": 407584
    },
    {
      "epoch": 0.0014793193609579597,
      "grad_norm": 8537.783904503556,
      "learning_rate": 1.567068989274478e-07,
      "loss": 1.6622,
      "step": 407616
    },
    {
      "epoch": 0.0014794354953087964,
      "grad_norm": 9806.920005791828,
      "learning_rate": 1.5670074207512323e-07,
      "loss": 1.6694,
      "step": 407648
    },
    {
      "epoch": 0.0014795516296596332,
      "grad_norm": 10707.14817306644,
      "learning_rate": 1.5669458594843075e-07,
      "loss": 1.6652,
      "step": 407680
    },
    {
      "epoch": 0.00147966776401047,
      "grad_norm": 11510.646723794454,
      "learning_rate": 1.5668843054722786e-07,
      "loss": 1.6746,
      "step": 407712
    },
    {
      "epoch": 0.0014797838983613065,
      "grad_norm": 9623.298395041069,
      "learning_rate": 1.5668227587137206e-07,
      "loss": 1.7002,
      "step": 407744
    },
    {
      "epoch": 0.0014799000327121432,
      "grad_norm": 10502.43362273716,
      "learning_rate": 1.566761219207209e-07,
      "loss": 1.7113,
      "step": 407776
    },
    {
      "epoch": 0.00148001616706298,
      "grad_norm": 9474.000633312202,
      "learning_rate": 1.56669968695132e-07,
      "loss": 1.7018,
      "step": 407808
    },
    {
      "epoch": 0.0014801323014138168,
      "grad_norm": 8083.448026677725,
      "learning_rate": 1.5666381619446296e-07,
      "loss": 1.6999,
      "step": 407840
    },
    {
      "epoch": 0.0014802484357646533,
      "grad_norm": 9173.391303111408,
      "learning_rate": 1.5665766441857147e-07,
      "loss": 1.7056,
      "step": 407872
    },
    {
      "epoch": 0.00148036457011549,
      "grad_norm": 20761.976784497183,
      "learning_rate": 1.566515133673152e-07,
      "loss": 1.6539,
      "step": 407904
    },
    {
      "epoch": 0.0014804807044663268,
      "grad_norm": 17649.378006037492,
      "learning_rate": 1.5664536304055197e-07,
      "loss": 1.6653,
      "step": 407936
    },
    {
      "epoch": 0.0014805968388171636,
      "grad_norm": 25263.142480697054,
      "learning_rate": 1.5663921343813953e-07,
      "loss": 1.6736,
      "step": 407968
    },
    {
      "epoch": 0.0014807129731680003,
      "grad_norm": 9746.493420712908,
      "learning_rate": 1.5663325670141882e-07,
      "loss": 1.6777,
      "step": 408000
    },
    {
      "epoch": 0.0014808291075188368,
      "grad_norm": 11575.187082721384,
      "learning_rate": 1.5662710852465656e-07,
      "loss": 1.6567,
      "step": 408032
    },
    {
      "epoch": 0.0014809452418696736,
      "grad_norm": 11645.327131515027,
      "learning_rate": 1.5662096107182313e-07,
      "loss": 1.6582,
      "step": 408064
    },
    {
      "epoch": 0.0014810613762205104,
      "grad_norm": 8792.50567244628,
      "learning_rate": 1.566148143427765e-07,
      "loss": 1.6797,
      "step": 408096
    },
    {
      "epoch": 0.001481177510571347,
      "grad_norm": 9872.395656576979,
      "learning_rate": 1.5660866833737458e-07,
      "loss": 1.6658,
      "step": 408128
    },
    {
      "epoch": 0.0014812936449221836,
      "grad_norm": 10458.998804856992,
      "learning_rate": 1.5660252305547548e-07,
      "loss": 1.6813,
      "step": 408160
    },
    {
      "epoch": 0.0014814097792730204,
      "grad_norm": 11671.315778437322,
      "learning_rate": 1.565963784969372e-07,
      "loss": 1.6715,
      "step": 408192
    },
    {
      "epoch": 0.0014815259136238572,
      "grad_norm": 9147.100196237057,
      "learning_rate": 1.565902346616179e-07,
      "loss": 1.672,
      "step": 408224
    },
    {
      "epoch": 0.001481642047974694,
      "grad_norm": 10526.219644297757,
      "learning_rate": 1.5658409154937565e-07,
      "loss": 1.6557,
      "step": 408256
    },
    {
      "epoch": 0.0014817581823255307,
      "grad_norm": 13094.895417680891,
      "learning_rate": 1.5657794916006865e-07,
      "loss": 1.6621,
      "step": 408288
    },
    {
      "epoch": 0.0014818743166763672,
      "grad_norm": 10229.285312278664,
      "learning_rate": 1.5657180749355514e-07,
      "loss": 1.6812,
      "step": 408320
    },
    {
      "epoch": 0.001481990451027204,
      "grad_norm": 10364.366840285034,
      "learning_rate": 1.5656566654969337e-07,
      "loss": 1.6631,
      "step": 408352
    },
    {
      "epoch": 0.0014821065853780407,
      "grad_norm": 8983.121506469786,
      "learning_rate": 1.5655952632834165e-07,
      "loss": 1.6967,
      "step": 408384
    },
    {
      "epoch": 0.0014822227197288775,
      "grad_norm": 9100.265380745772,
      "learning_rate": 1.5655338682935824e-07,
      "loss": 1.6828,
      "step": 408416
    },
    {
      "epoch": 0.001482338854079714,
      "grad_norm": 10496.261334399025,
      "learning_rate": 1.5654724805260157e-07,
      "loss": 1.6595,
      "step": 408448
    },
    {
      "epoch": 0.0014824549884305508,
      "grad_norm": 8818.0661145174,
      "learning_rate": 1.5654110999793006e-07,
      "loss": 1.6555,
      "step": 408480
    },
    {
      "epoch": 0.0014825711227813875,
      "grad_norm": 9228.28445595388,
      "learning_rate": 1.5653497266520215e-07,
      "loss": 1.6616,
      "step": 408512
    },
    {
      "epoch": 0.0014826872571322243,
      "grad_norm": 10743.402999050162,
      "learning_rate": 1.565288360542763e-07,
      "loss": 1.6806,
      "step": 408544
    },
    {
      "epoch": 0.001482803391483061,
      "grad_norm": 11880.03939387408,
      "learning_rate": 1.5652270016501107e-07,
      "loss": 1.6739,
      "step": 408576
    },
    {
      "epoch": 0.0014829195258338976,
      "grad_norm": 9864.527865032365,
      "learning_rate": 1.5651656499726502e-07,
      "loss": 1.6826,
      "step": 408608
    },
    {
      "epoch": 0.0014830356601847343,
      "grad_norm": 10313.087219644756,
      "learning_rate": 1.5651043055089677e-07,
      "loss": 1.6939,
      "step": 408640
    },
    {
      "epoch": 0.001483151794535571,
      "grad_norm": 10530.953992872632,
      "learning_rate": 1.5650429682576493e-07,
      "loss": 1.6862,
      "step": 408672
    },
    {
      "epoch": 0.0014832679288864078,
      "grad_norm": 11198.98852575535,
      "learning_rate": 1.564981638217282e-07,
      "loss": 1.6807,
      "step": 408704
    },
    {
      "epoch": 0.0014833840632372444,
      "grad_norm": 10692.286004405232,
      "learning_rate": 1.564920315386453e-07,
      "loss": 1.6894,
      "step": 408736
    },
    {
      "epoch": 0.001483500197588081,
      "grad_norm": 10384.464357876144,
      "learning_rate": 1.5648589997637498e-07,
      "loss": 1.6697,
      "step": 408768
    },
    {
      "epoch": 0.0014836163319389179,
      "grad_norm": 11139.839137079136,
      "learning_rate": 1.5647976913477606e-07,
      "loss": 1.6714,
      "step": 408800
    },
    {
      "epoch": 0.0014837324662897546,
      "grad_norm": 10217.63534287655,
      "learning_rate": 1.5647363901370736e-07,
      "loss": 1.6806,
      "step": 408832
    },
    {
      "epoch": 0.0014838486006405914,
      "grad_norm": 9535.142474027329,
      "learning_rate": 1.564675096130278e-07,
      "loss": 1.6804,
      "step": 408864
    },
    {
      "epoch": 0.001483964734991428,
      "grad_norm": 8012.634023840101,
      "learning_rate": 1.564613809325962e-07,
      "loss": 1.6749,
      "step": 408896
    },
    {
      "epoch": 0.0014840808693422647,
      "grad_norm": 9863.13580967027,
      "learning_rate": 1.5645525297227163e-07,
      "loss": 1.6763,
      "step": 408928
    },
    {
      "epoch": 0.0014841970036931014,
      "grad_norm": 11985.04251139728,
      "learning_rate": 1.56449125731913e-07,
      "loss": 1.6745,
      "step": 408960
    },
    {
      "epoch": 0.0014843131380439382,
      "grad_norm": 23274.59731123183,
      "learning_rate": 1.5644299921137936e-07,
      "loss": 1.6693,
      "step": 408992
    },
    {
      "epoch": 0.0014844292723947747,
      "grad_norm": 19571.78990281676,
      "learning_rate": 1.5643687341052982e-07,
      "loss": 1.6805,
      "step": 409024
    },
    {
      "epoch": 0.0014845454067456115,
      "grad_norm": 9331.482090214822,
      "learning_rate": 1.5643093972712415e-07,
      "loss": 1.6669,
      "step": 409056
    },
    {
      "epoch": 0.0014846615410964482,
      "grad_norm": 12123.867534743193,
      "learning_rate": 1.564248153427409e-07,
      "loss": 1.681,
      "step": 409088
    },
    {
      "epoch": 0.001484777675447285,
      "grad_norm": 10376.835355733463,
      "learning_rate": 1.5641869167762356e-07,
      "loss": 1.6582,
      "step": 409120
    },
    {
      "epoch": 0.0014848938097981217,
      "grad_norm": 10311.981768796917,
      "learning_rate": 1.5641256873163136e-07,
      "loss": 1.6559,
      "step": 409152
    },
    {
      "epoch": 0.0014850099441489583,
      "grad_norm": 9877.262576240444,
      "learning_rate": 1.564064465046235e-07,
      "loss": 1.6685,
      "step": 409184
    },
    {
      "epoch": 0.001485126078499795,
      "grad_norm": 8774.348522824928,
      "learning_rate": 1.5640032499645941e-07,
      "loss": 1.658,
      "step": 409216
    },
    {
      "epoch": 0.0014852422128506318,
      "grad_norm": 12613.985730133041,
      "learning_rate": 1.5639420420699832e-07,
      "loss": 1.6757,
      "step": 409248
    },
    {
      "epoch": 0.0014853583472014685,
      "grad_norm": 7808.109630377893,
      "learning_rate": 1.5638808413609964e-07,
      "loss": 1.6798,
      "step": 409280
    },
    {
      "epoch": 0.001485474481552305,
      "grad_norm": 9964.089321157253,
      "learning_rate": 1.5638196478362282e-07,
      "loss": 1.6828,
      "step": 409312
    },
    {
      "epoch": 0.0014855906159031418,
      "grad_norm": 8674.736191954196,
      "learning_rate": 1.5637584614942725e-07,
      "loss": 1.6542,
      "step": 409344
    },
    {
      "epoch": 0.0014857067502539786,
      "grad_norm": 9216.780131911577,
      "learning_rate": 1.5636972823337246e-07,
      "loss": 1.6728,
      "step": 409376
    },
    {
      "epoch": 0.0014858228846048153,
      "grad_norm": 9304.0423472811,
      "learning_rate": 1.5636361103531797e-07,
      "loss": 1.6929,
      "step": 409408
    },
    {
      "epoch": 0.001485939018955652,
      "grad_norm": 10050.806136823056,
      "learning_rate": 1.5635749455512334e-07,
      "loss": 1.6743,
      "step": 409440
    },
    {
      "epoch": 0.0014860551533064886,
      "grad_norm": 9427.686672773974,
      "learning_rate": 1.563513787926482e-07,
      "loss": 1.704,
      "step": 409472
    },
    {
      "epoch": 0.0014861712876573254,
      "grad_norm": 11615.9103818857,
      "learning_rate": 1.563452637477522e-07,
      "loss": 1.699,
      "step": 409504
    },
    {
      "epoch": 0.0014862874220081621,
      "grad_norm": 9684.809135961326,
      "learning_rate": 1.5633914942029504e-07,
      "loss": 1.6924,
      "step": 409536
    },
    {
      "epoch": 0.0014864035563589989,
      "grad_norm": 10426.80392066524,
      "learning_rate": 1.5633303581013637e-07,
      "loss": 1.6841,
      "step": 409568
    },
    {
      "epoch": 0.0014865196907098354,
      "grad_norm": 10424.138909281668,
      "learning_rate": 1.5632692291713604e-07,
      "loss": 1.6891,
      "step": 409600
    },
    {
      "epoch": 0.0014866358250606722,
      "grad_norm": 14232.352721879823,
      "learning_rate": 1.563208107411538e-07,
      "loss": 1.6953,
      "step": 409632
    },
    {
      "epoch": 0.001486751959411509,
      "grad_norm": 10460.843560631236,
      "learning_rate": 1.563146992820495e-07,
      "loss": 1.6534,
      "step": 409664
    },
    {
      "epoch": 0.0014868680937623457,
      "grad_norm": 9556.507521056006,
      "learning_rate": 1.56308588539683e-07,
      "loss": 1.6702,
      "step": 409696
    },
    {
      "epoch": 0.0014869842281131824,
      "grad_norm": 11048.132692903358,
      "learning_rate": 1.5630247851391424e-07,
      "loss": 1.6779,
      "step": 409728
    },
    {
      "epoch": 0.001487100362464019,
      "grad_norm": 9316.981270776496,
      "learning_rate": 1.5629636920460316e-07,
      "loss": 1.6639,
      "step": 409760
    },
    {
      "epoch": 0.0014872164968148557,
      "grad_norm": 8970.175472085259,
      "learning_rate": 1.5629026061160977e-07,
      "loss": 1.6568,
      "step": 409792
    },
    {
      "epoch": 0.0014873326311656925,
      "grad_norm": 10147.596759824466,
      "learning_rate": 1.562841527347941e-07,
      "loss": 1.6677,
      "step": 409824
    },
    {
      "epoch": 0.0014874487655165292,
      "grad_norm": 12551.505726405896,
      "learning_rate": 1.562780455740162e-07,
      "loss": 1.6745,
      "step": 409856
    },
    {
      "epoch": 0.0014875648998673658,
      "grad_norm": 12521.106820085835,
      "learning_rate": 1.5627193912913616e-07,
      "loss": 1.6739,
      "step": 409888
    },
    {
      "epoch": 0.0014876810342182025,
      "grad_norm": 8671.24051102263,
      "learning_rate": 1.5626583340001415e-07,
      "loss": 1.6836,
      "step": 409920
    },
    {
      "epoch": 0.0014877971685690393,
      "grad_norm": 10129.778477340953,
      "learning_rate": 1.562597283865104e-07,
      "loss": 1.6815,
      "step": 409952
    },
    {
      "epoch": 0.001487913302919876,
      "grad_norm": 9284.097802156115,
      "learning_rate": 1.56253624088485e-07,
      "loss": 1.6812,
      "step": 409984
    },
    {
      "epoch": 0.0014880294372707128,
      "grad_norm": 10262.171115314732,
      "learning_rate": 1.5624752050579836e-07,
      "loss": 1.675,
      "step": 410016
    },
    {
      "epoch": 0.0014881455716215493,
      "grad_norm": 10107.463974707009,
      "learning_rate": 1.562414176383107e-07,
      "loss": 1.6793,
      "step": 410048
    },
    {
      "epoch": 0.001488261705972386,
      "grad_norm": 21527.73466949089,
      "learning_rate": 1.5623531548588235e-07,
      "loss": 1.6662,
      "step": 410080
    },
    {
      "epoch": 0.0014883778403232228,
      "grad_norm": 12078.911705944372,
      "learning_rate": 1.562294047074757e-07,
      "loss": 1.6659,
      "step": 410112
    },
    {
      "epoch": 0.0014884939746740596,
      "grad_norm": 13000.799052365974,
      "learning_rate": 1.562233039624124e-07,
      "loss": 1.6802,
      "step": 410144
    },
    {
      "epoch": 0.0014886101090248961,
      "grad_norm": 8834.131989052461,
      "learning_rate": 1.5621720393199403e-07,
      "loss": 1.6912,
      "step": 410176
    },
    {
      "epoch": 0.0014887262433757329,
      "grad_norm": 10025.3404929708,
      "learning_rate": 1.5621110461608108e-07,
      "loss": 1.6643,
      "step": 410208
    },
    {
      "epoch": 0.0014888423777265696,
      "grad_norm": 9378.67176096914,
      "learning_rate": 1.562050060145341e-07,
      "loss": 1.6721,
      "step": 410240
    },
    {
      "epoch": 0.0014889585120774064,
      "grad_norm": 9640.751008090605,
      "learning_rate": 1.5619890812721357e-07,
      "loss": 1.6824,
      "step": 410272
    },
    {
      "epoch": 0.0014890746464282431,
      "grad_norm": 9017.646256091442,
      "learning_rate": 1.5619281095398015e-07,
      "loss": 1.6785,
      "step": 410304
    },
    {
      "epoch": 0.0014891907807790797,
      "grad_norm": 12175.234371460781,
      "learning_rate": 1.5618671449469452e-07,
      "loss": 1.6872,
      "step": 410336
    },
    {
      "epoch": 0.0014893069151299164,
      "grad_norm": 12111.88127418693,
      "learning_rate": 1.5618061874921724e-07,
      "loss": 1.683,
      "step": 410368
    },
    {
      "epoch": 0.0014894230494807532,
      "grad_norm": 12361.92137169623,
      "learning_rate": 1.5617452371740916e-07,
      "loss": 1.7001,
      "step": 410400
    },
    {
      "epoch": 0.00148953918383159,
      "grad_norm": 9074.2518148881,
      "learning_rate": 1.5616842939913094e-07,
      "loss": 1.6744,
      "step": 410432
    },
    {
      "epoch": 0.0014896553181824265,
      "grad_norm": 12108.111826374912,
      "learning_rate": 1.5616233579424344e-07,
      "loss": 1.6976,
      "step": 410464
    },
    {
      "epoch": 0.0014897714525332632,
      "grad_norm": 7842.175973542037,
      "learning_rate": 1.5615624290260743e-07,
      "loss": 1.7046,
      "step": 410496
    },
    {
      "epoch": 0.0014898875868841,
      "grad_norm": 10690.310566115468,
      "learning_rate": 1.5615015072408383e-07,
      "loss": 1.6741,
      "step": 410528
    },
    {
      "epoch": 0.0014900037212349367,
      "grad_norm": 12415.130124167044,
      "learning_rate": 1.561440592585335e-07,
      "loss": 1.6906,
      "step": 410560
    },
    {
      "epoch": 0.0014901198555857735,
      "grad_norm": 15962.833457754297,
      "learning_rate": 1.5613796850581738e-07,
      "loss": 1.6705,
      "step": 410592
    },
    {
      "epoch": 0.00149023598993661,
      "grad_norm": 9867.301556149989,
      "learning_rate": 1.561318784657965e-07,
      "loss": 1.6812,
      "step": 410624
    },
    {
      "epoch": 0.0014903521242874468,
      "grad_norm": 10957.507198263664,
      "learning_rate": 1.5612578913833187e-07,
      "loss": 1.6605,
      "step": 410656
    },
    {
      "epoch": 0.0014904682586382835,
      "grad_norm": 10371.489189118407,
      "learning_rate": 1.5611970052328454e-07,
      "loss": 1.6563,
      "step": 410688
    },
    {
      "epoch": 0.0014905843929891203,
      "grad_norm": 11266.8746331891,
      "learning_rate": 1.561136126205156e-07,
      "loss": 1.6818,
      "step": 410720
    },
    {
      "epoch": 0.0014907005273399568,
      "grad_norm": 11810.982516285425,
      "learning_rate": 1.5610752542988617e-07,
      "loss": 1.6859,
      "step": 410752
    },
    {
      "epoch": 0.0014908166616907936,
      "grad_norm": 9536.018036895694,
      "learning_rate": 1.5610143895125744e-07,
      "loss": 1.6726,
      "step": 410784
    },
    {
      "epoch": 0.0014909327960416303,
      "grad_norm": 11122.115805906717,
      "learning_rate": 1.560953531844906e-07,
      "loss": 1.6585,
      "step": 410816
    },
    {
      "epoch": 0.001491048930392467,
      "grad_norm": 12655.413703233886,
      "learning_rate": 1.5608926812944697e-07,
      "loss": 1.6731,
      "step": 410848
    },
    {
      "epoch": 0.0014911650647433038,
      "grad_norm": 10661.967173087713,
      "learning_rate": 1.5608318378598774e-07,
      "loss": 1.6812,
      "step": 410880
    },
    {
      "epoch": 0.0014912811990941404,
      "grad_norm": 9226.768123237953,
      "learning_rate": 1.5607710015397428e-07,
      "loss": 1.6783,
      "step": 410912
    },
    {
      "epoch": 0.0014913973334449771,
      "grad_norm": 9688.529919445984,
      "learning_rate": 1.5607101723326795e-07,
      "loss": 1.6621,
      "step": 410944
    },
    {
      "epoch": 0.0014915134677958139,
      "grad_norm": 8993.175190109441,
      "learning_rate": 1.5606493502373012e-07,
      "loss": 1.6675,
      "step": 410976
    },
    {
      "epoch": 0.0014916296021466506,
      "grad_norm": 12918.107446526368,
      "learning_rate": 1.560588535252223e-07,
      "loss": 1.6793,
      "step": 411008
    },
    {
      "epoch": 0.0014917457364974872,
      "grad_norm": 10551.024784351519,
      "learning_rate": 1.560527727376059e-07,
      "loss": 1.7002,
      "step": 411040
    },
    {
      "epoch": 0.001491861870848324,
      "grad_norm": 11896.229822931296,
      "learning_rate": 1.560466926607424e-07,
      "loss": 1.6873,
      "step": 411072
    },
    {
      "epoch": 0.0014919780051991607,
      "grad_norm": 19930.732048773323,
      "learning_rate": 1.5604061329449346e-07,
      "loss": 1.6921,
      "step": 411104
    },
    {
      "epoch": 0.0014920941395499974,
      "grad_norm": 20999.597329472774,
      "learning_rate": 1.5603453463872058e-07,
      "loss": 1.6824,
      "step": 411136
    },
    {
      "epoch": 0.0014922102739008342,
      "grad_norm": 17196.847152894043,
      "learning_rate": 1.5602845669328542e-07,
      "loss": 1.6825,
      "step": 411168
    },
    {
      "epoch": 0.0014923264082516707,
      "grad_norm": 20932.590475141868,
      "learning_rate": 1.5602237945804963e-07,
      "loss": 1.682,
      "step": 411200
    },
    {
      "epoch": 0.0014924425426025075,
      "grad_norm": 19976.112734964227,
      "learning_rate": 1.5601630293287491e-07,
      "loss": 1.6845,
      "step": 411232
    },
    {
      "epoch": 0.0014925586769533442,
      "grad_norm": 23941.56653187088,
      "learning_rate": 1.560104169761051e-07,
      "loss": 1.6951,
      "step": 411264
    },
    {
      "epoch": 0.001492674811304181,
      "grad_norm": 9278.505052000564,
      "learning_rate": 1.5600434184845913e-07,
      "loss": 1.6834,
      "step": 411296
    },
    {
      "epoch": 0.0014927909456550175,
      "grad_norm": 10224.454802090917,
      "learning_rate": 1.5599826743046385e-07,
      "loss": 1.6947,
      "step": 411328
    },
    {
      "epoch": 0.0014929070800058543,
      "grad_norm": 10630.118719939115,
      "learning_rate": 1.559921937219812e-07,
      "loss": 1.6944,
      "step": 411360
    },
    {
      "epoch": 0.001493023214356691,
      "grad_norm": 12567.767343486272,
      "learning_rate": 1.5598612072287297e-07,
      "loss": 1.6802,
      "step": 411392
    },
    {
      "epoch": 0.0014931393487075278,
      "grad_norm": 9356.489512632395,
      "learning_rate": 1.5598004843300114e-07,
      "loss": 1.6733,
      "step": 411424
    },
    {
      "epoch": 0.0014932554830583645,
      "grad_norm": 12416.141107445583,
      "learning_rate": 1.5597397685222768e-07,
      "loss": 1.6763,
      "step": 411456
    },
    {
      "epoch": 0.001493371617409201,
      "grad_norm": 9317.808969924206,
      "learning_rate": 1.5596790598041452e-07,
      "loss": 1.6724,
      "step": 411488
    },
    {
      "epoch": 0.0014934877517600378,
      "grad_norm": 10048.359070017354,
      "learning_rate": 1.559618358174238e-07,
      "loss": 1.679,
      "step": 411520
    },
    {
      "epoch": 0.0014936038861108746,
      "grad_norm": 8340.645418671147,
      "learning_rate": 1.5595576636311751e-07,
      "loss": 1.6661,
      "step": 411552
    },
    {
      "epoch": 0.0014937200204617113,
      "grad_norm": 9569.91410619761,
      "learning_rate": 1.5594969761735785e-07,
      "loss": 1.6767,
      "step": 411584
    },
    {
      "epoch": 0.0014938361548125479,
      "grad_norm": 11647.478697125829,
      "learning_rate": 1.5594362958000688e-07,
      "loss": 1.707,
      "step": 411616
    },
    {
      "epoch": 0.0014939522891633846,
      "grad_norm": 10138.669143432979,
      "learning_rate": 1.5593756225092687e-07,
      "loss": 1.7106,
      "step": 411648
    },
    {
      "epoch": 0.0014940684235142214,
      "grad_norm": 9180.489965138026,
      "learning_rate": 1.5593149562998e-07,
      "loss": 1.6676,
      "step": 411680
    },
    {
      "epoch": 0.0014941845578650581,
      "grad_norm": 9173.428148734802,
      "learning_rate": 1.5592542971702854e-07,
      "loss": 1.6791,
      "step": 411712
    },
    {
      "epoch": 0.001494300692215895,
      "grad_norm": 13937.313514447467,
      "learning_rate": 1.5591936451193478e-07,
      "loss": 1.6921,
      "step": 411744
    },
    {
      "epoch": 0.0014944168265667314,
      "grad_norm": 9910.404028090883,
      "learning_rate": 1.559133000145611e-07,
      "loss": 1.6915,
      "step": 411776
    },
    {
      "epoch": 0.0014945329609175682,
      "grad_norm": 9360.47819291301,
      "learning_rate": 1.5590723622476983e-07,
      "loss": 1.7024,
      "step": 411808
    },
    {
      "epoch": 0.001494649095268405,
      "grad_norm": 8642.9589840517,
      "learning_rate": 1.559011731424234e-07,
      "loss": 1.6686,
      "step": 411840
    },
    {
      "epoch": 0.0014947652296192417,
      "grad_norm": 7363.423524421232,
      "learning_rate": 1.558951107673843e-07,
      "loss": 1.6734,
      "step": 411872
    },
    {
      "epoch": 0.0014948813639700782,
      "grad_norm": 9967.003762415263,
      "learning_rate": 1.5588904909951496e-07,
      "loss": 1.6849,
      "step": 411904
    },
    {
      "epoch": 0.001494997498320915,
      "grad_norm": 10846.129447872176,
      "learning_rate": 1.5588298813867793e-07,
      "loss": 1.6893,
      "step": 411936
    },
    {
      "epoch": 0.0014951136326717517,
      "grad_norm": 13579.892488528765,
      "learning_rate": 1.5587692788473577e-07,
      "loss": 1.669,
      "step": 411968
    },
    {
      "epoch": 0.0014952297670225885,
      "grad_norm": 10594.923123836246,
      "learning_rate": 1.5587086833755113e-07,
      "loss": 1.6864,
      "step": 412000
    },
    {
      "epoch": 0.0014953459013734253,
      "grad_norm": 10002.713431864377,
      "learning_rate": 1.5586480949698654e-07,
      "loss": 1.6976,
      "step": 412032
    },
    {
      "epoch": 0.0014954620357242618,
      "grad_norm": 9708.90457260756,
      "learning_rate": 1.5585875136290477e-07,
      "loss": 1.6994,
      "step": 412064
    },
    {
      "epoch": 0.0014955781700750985,
      "grad_norm": 9452.618261624659,
      "learning_rate": 1.558526939351685e-07,
      "loss": 1.7126,
      "step": 412096
    },
    {
      "epoch": 0.0014956943044259353,
      "grad_norm": 8186.789236324579,
      "learning_rate": 1.5584663721364052e-07,
      "loss": 1.7103,
      "step": 412128
    },
    {
      "epoch": 0.001495810438776772,
      "grad_norm": 8220.211432803902,
      "learning_rate": 1.5584058119818354e-07,
      "loss": 1.7162,
      "step": 412160
    },
    {
      "epoch": 0.0014959265731276086,
      "grad_norm": 11894.454842488578,
      "learning_rate": 1.5583452588866045e-07,
      "loss": 1.7122,
      "step": 412192
    },
    {
      "epoch": 0.0014960427074784453,
      "grad_norm": 8391.998093422091,
      "learning_rate": 1.5582847128493407e-07,
      "loss": 1.7146,
      "step": 412224
    },
    {
      "epoch": 0.001496158841829282,
      "grad_norm": 11478.44013792815,
      "learning_rate": 1.5582241738686735e-07,
      "loss": 1.6876,
      "step": 412256
    },
    {
      "epoch": 0.0014962749761801189,
      "grad_norm": 25822.38501765474,
      "learning_rate": 1.5581636419432317e-07,
      "loss": 1.6561,
      "step": 412288
    },
    {
      "epoch": 0.0014963911105309556,
      "grad_norm": 18800.977846909984,
      "learning_rate": 1.5581031170716452e-07,
      "loss": 1.6724,
      "step": 412320
    },
    {
      "epoch": 0.0014965072448817921,
      "grad_norm": 16518.566523763497,
      "learning_rate": 1.5580425992525448e-07,
      "loss": 1.6834,
      "step": 412352
    },
    {
      "epoch": 0.001496623379232629,
      "grad_norm": 8253.959534671829,
      "learning_rate": 1.557983979339342e-07,
      "loss": 1.6662,
      "step": 412384
    },
    {
      "epoch": 0.0014967395135834657,
      "grad_norm": 9817.01859018307,
      "learning_rate": 1.5579234754008204e-07,
      "loss": 1.6638,
      "step": 412416
    },
    {
      "epoch": 0.0014968556479343024,
      "grad_norm": 9766.80152352857,
      "learning_rate": 1.5578629785107195e-07,
      "loss": 1.6501,
      "step": 412448
    },
    {
      "epoch": 0.001496971782285139,
      "grad_norm": 11958.505759500224,
      "learning_rate": 1.5578024886676713e-07,
      "loss": 1.6585,
      "step": 412480
    },
    {
      "epoch": 0.0014970879166359757,
      "grad_norm": 9980.191180533568,
      "learning_rate": 1.5577420058703072e-07,
      "loss": 1.6901,
      "step": 412512
    },
    {
      "epoch": 0.0014972040509868125,
      "grad_norm": 9999.976599972622,
      "learning_rate": 1.5576815301172603e-07,
      "loss": 1.6778,
      "step": 412544
    },
    {
      "epoch": 0.0014973201853376492,
      "grad_norm": 11904.5544225729,
      "learning_rate": 1.5576210614071626e-07,
      "loss": 1.6576,
      "step": 412576
    },
    {
      "epoch": 0.001497436319688486,
      "grad_norm": 8864.317006966752,
      "learning_rate": 1.5575605997386476e-07,
      "loss": 1.67,
      "step": 412608
    },
    {
      "epoch": 0.0014975524540393225,
      "grad_norm": 11204.347192050058,
      "learning_rate": 1.5575001451103483e-07,
      "loss": 1.6816,
      "step": 412640
    },
    {
      "epoch": 0.0014976685883901593,
      "grad_norm": 11932.57541354757,
      "learning_rate": 1.5574396975208994e-07,
      "loss": 1.6913,
      "step": 412672
    },
    {
      "epoch": 0.001497784722740996,
      "grad_norm": 10513.233375132504,
      "learning_rate": 1.557379256968934e-07,
      "loss": 1.7025,
      "step": 412704
    },
    {
      "epoch": 0.0014979008570918328,
      "grad_norm": 9119.89024056759,
      "learning_rate": 1.5573188234530876e-07,
      "loss": 1.6727,
      "step": 412736
    },
    {
      "epoch": 0.0014980169914426693,
      "grad_norm": 14183.673431096755,
      "learning_rate": 1.5572583969719946e-07,
      "loss": 1.6591,
      "step": 412768
    },
    {
      "epoch": 0.001498133125793506,
      "grad_norm": 9989.378158824502,
      "learning_rate": 1.5571979775242905e-07,
      "loss": 1.6733,
      "step": 412800
    },
    {
      "epoch": 0.0014982492601443428,
      "grad_norm": 11856.003964236854,
      "learning_rate": 1.5571375651086106e-07,
      "loss": 1.6701,
      "step": 412832
    },
    {
      "epoch": 0.0014983653944951796,
      "grad_norm": 9874.720249201999,
      "learning_rate": 1.5570771597235918e-07,
      "loss": 1.6621,
      "step": 412864
    },
    {
      "epoch": 0.0014984815288460163,
      "grad_norm": 7625.182358475107,
      "learning_rate": 1.5570167613678697e-07,
      "loss": 1.6793,
      "step": 412896
    },
    {
      "epoch": 0.0014985976631968529,
      "grad_norm": 9616.290760995114,
      "learning_rate": 1.5569563700400813e-07,
      "loss": 1.6763,
      "step": 412928
    },
    {
      "epoch": 0.0014987137975476896,
      "grad_norm": 8295.879700188521,
      "learning_rate": 1.556895985738864e-07,
      "loss": 1.6796,
      "step": 412960
    },
    {
      "epoch": 0.0014988299318985264,
      "grad_norm": 13409.626169286003,
      "learning_rate": 1.5568356084628552e-07,
      "loss": 1.6892,
      "step": 412992
    },
    {
      "epoch": 0.0014989460662493631,
      "grad_norm": 10296.034187977428,
      "learning_rate": 1.5567752382106929e-07,
      "loss": 1.6813,
      "step": 413024
    },
    {
      "epoch": 0.0014990622006001997,
      "grad_norm": 9889.281065881381,
      "learning_rate": 1.5567148749810147e-07,
      "loss": 1.6804,
      "step": 413056
    },
    {
      "epoch": 0.0014991783349510364,
      "grad_norm": 11594.238396721019,
      "learning_rate": 1.55665451877246e-07,
      "loss": 1.6912,
      "step": 413088
    },
    {
      "epoch": 0.0014992944693018732,
      "grad_norm": 8050.041739022227,
      "learning_rate": 1.556594169583668e-07,
      "loss": 1.6992,
      "step": 413120
    },
    {
      "epoch": 0.00149941060365271,
      "grad_norm": 13835.502448411478,
      "learning_rate": 1.5565338274132772e-07,
      "loss": 1.6662,
      "step": 413152
    },
    {
      "epoch": 0.0014995267380035467,
      "grad_norm": 14773.668332543546,
      "learning_rate": 1.556473492259928e-07,
      "loss": 1.6707,
      "step": 413184
    },
    {
      "epoch": 0.0014996428723543832,
      "grad_norm": 11811.628168885101,
      "learning_rate": 1.5564131641222602e-07,
      "loss": 1.6875,
      "step": 413216
    },
    {
      "epoch": 0.00149975900670522,
      "grad_norm": 9899.417760656432,
      "learning_rate": 1.5563528429989146e-07,
      "loss": 1.6936,
      "step": 413248
    },
    {
      "epoch": 0.0014998751410560567,
      "grad_norm": 10870.53706124955,
      "learning_rate": 1.5562925288885317e-07,
      "loss": 1.6812,
      "step": 413280
    },
    {
      "epoch": 0.0014999912754068935,
      "grad_norm": 15199.39393528571,
      "learning_rate": 1.5562322217897528e-07,
      "loss": 1.6612,
      "step": 413312
    },
    {
      "epoch": 0.00150010740975773,
      "grad_norm": 9605.132586279065,
      "learning_rate": 1.5561719217012198e-07,
      "loss": 1.6625,
      "step": 413344
    },
    {
      "epoch": 0.0015002235441085668,
      "grad_norm": 10175.591383305446,
      "learning_rate": 1.5561116286215742e-07,
      "loss": 1.6662,
      "step": 413376
    },
    {
      "epoch": 0.0015003396784594035,
      "grad_norm": 21271.44038376339,
      "learning_rate": 1.5560513425494586e-07,
      "loss": 1.696,
      "step": 413408
    },
    {
      "epoch": 0.0015004558128102403,
      "grad_norm": 20828.883791504526,
      "learning_rate": 1.555991063483516e-07,
      "loss": 1.6597,
      "step": 413440
    },
    {
      "epoch": 0.001500571947161077,
      "grad_norm": 19545.092887985975,
      "learning_rate": 1.5559307914223886e-07,
      "loss": 1.659,
      "step": 413472
    },
    {
      "epoch": 0.0015006880815119136,
      "grad_norm": 17508.478403333625,
      "learning_rate": 1.5558705263647205e-07,
      "loss": 1.6745,
      "step": 413504
    },
    {
      "epoch": 0.0015008042158627503,
      "grad_norm": 12777.598678938073,
      "learning_rate": 1.5558121512674165e-07,
      "loss": 1.6677,
      "step": 413536
    },
    {
      "epoch": 0.001500920350213587,
      "grad_norm": 9747.460797561589,
      "learning_rate": 1.5557518999938457e-07,
      "loss": 1.6735,
      "step": 413568
    },
    {
      "epoch": 0.0015010364845644238,
      "grad_norm": 7708.223790212633,
      "learning_rate": 1.5556916557197088e-07,
      "loss": 1.6727,
      "step": 413600
    },
    {
      "epoch": 0.0015011526189152604,
      "grad_norm": 9976.268841606065,
      "learning_rate": 1.5556314184436507e-07,
      "loss": 1.6533,
      "step": 413632
    },
    {
      "epoch": 0.0015012687532660971,
      "grad_norm": 12200.707192617976,
      "learning_rate": 1.555571188164317e-07,
      "loss": 1.6645,
      "step": 413664
    },
    {
      "epoch": 0.0015013848876169339,
      "grad_norm": 8688.61795684446,
      "learning_rate": 1.555510964880353e-07,
      "loss": 1.6808,
      "step": 413696
    },
    {
      "epoch": 0.0015015010219677706,
      "grad_norm": 9942.432901458275,
      "learning_rate": 1.5554507485904047e-07,
      "loss": 1.6859,
      "step": 413728
    },
    {
      "epoch": 0.0015016171563186074,
      "grad_norm": 9785.399736341893,
      "learning_rate": 1.5553905392931186e-07,
      "loss": 1.6857,
      "step": 413760
    },
    {
      "epoch": 0.001501733290669444,
      "grad_norm": 9395.083182175664,
      "learning_rate": 1.5553303369871412e-07,
      "loss": 1.6915,
      "step": 413792
    },
    {
      "epoch": 0.0015018494250202807,
      "grad_norm": 9969.707317669861,
      "learning_rate": 1.5552701416711196e-07,
      "loss": 1.6894,
      "step": 413824
    },
    {
      "epoch": 0.0015019655593711174,
      "grad_norm": 10719.093991564772,
      "learning_rate": 1.5552099533437014e-07,
      "loss": 1.6873,
      "step": 413856
    },
    {
      "epoch": 0.0015020816937219542,
      "grad_norm": 8758.595549515916,
      "learning_rate": 1.5551497720035344e-07,
      "loss": 1.6961,
      "step": 413888
    },
    {
      "epoch": 0.0015021978280727907,
      "grad_norm": 10214.414912269816,
      "learning_rate": 1.555089597649267e-07,
      "loss": 1.6723,
      "step": 413920
    },
    {
      "epoch": 0.0015023139624236275,
      "grad_norm": 11711.826330679602,
      "learning_rate": 1.5550294302795475e-07,
      "loss": 1.6831,
      "step": 413952
    },
    {
      "epoch": 0.0015024300967744642,
      "grad_norm": 8688.206259061762,
      "learning_rate": 1.5549692698930248e-07,
      "loss": 1.6974,
      "step": 413984
    },
    {
      "epoch": 0.001502546231125301,
      "grad_norm": 9040.317029839163,
      "learning_rate": 1.5549091164883485e-07,
      "loss": 1.6882,
      "step": 414016
    },
    {
      "epoch": 0.0015026623654761377,
      "grad_norm": 10912.537560072817,
      "learning_rate": 1.554848970064168e-07,
      "loss": 1.6531,
      "step": 414048
    },
    {
      "epoch": 0.0015027784998269743,
      "grad_norm": 7387.827962263334,
      "learning_rate": 1.554788830619133e-07,
      "loss": 1.6641,
      "step": 414080
    },
    {
      "epoch": 0.001502894634177811,
      "grad_norm": 10872.923249982041,
      "learning_rate": 1.5547286981518944e-07,
      "loss": 1.6793,
      "step": 414112
    },
    {
      "epoch": 0.0015030107685286478,
      "grad_norm": 9661.002225442244,
      "learning_rate": 1.5546685726611026e-07,
      "loss": 1.6845,
      "step": 414144
    },
    {
      "epoch": 0.0015031269028794845,
      "grad_norm": 15569.29388251118,
      "learning_rate": 1.5546084541454093e-07,
      "loss": 1.679,
      "step": 414176
    },
    {
      "epoch": 0.001503243037230321,
      "grad_norm": 12420.451521583263,
      "learning_rate": 1.5545483426034653e-07,
      "loss": 1.6685,
      "step": 414208
    },
    {
      "epoch": 0.0015033591715811578,
      "grad_norm": 8890.052193322601,
      "learning_rate": 1.5544882380339227e-07,
      "loss": 1.6849,
      "step": 414240
    },
    {
      "epoch": 0.0015034753059319946,
      "grad_norm": 9702.05813216969,
      "learning_rate": 1.5544281404354338e-07,
      "loss": 1.6839,
      "step": 414272
    },
    {
      "epoch": 0.0015035914402828313,
      "grad_norm": 8956.135550559738,
      "learning_rate": 1.554368049806651e-07,
      "loss": 1.6999,
      "step": 414304
    },
    {
      "epoch": 0.001503707574633668,
      "grad_norm": 11569.900604586022,
      "learning_rate": 1.5543079661462276e-07,
      "loss": 1.6814,
      "step": 414336
    },
    {
      "epoch": 0.0015038237089845046,
      "grad_norm": 12210.474519853846,
      "learning_rate": 1.5542478894528162e-07,
      "loss": 1.6733,
      "step": 414368
    },
    {
      "epoch": 0.0015039398433353414,
      "grad_norm": 12817.584483825336,
      "learning_rate": 1.5541878197250714e-07,
      "loss": 1.6715,
      "step": 414400
    },
    {
      "epoch": 0.0015040559776861781,
      "grad_norm": 8527.01213790622,
      "learning_rate": 1.5541277569616462e-07,
      "loss": 1.6777,
      "step": 414432
    },
    {
      "epoch": 0.0015041721120370149,
      "grad_norm": 8976.391479876533,
      "learning_rate": 1.554067701161196e-07,
      "loss": 1.68,
      "step": 414464
    },
    {
      "epoch": 0.0015042882463878514,
      "grad_norm": 11808.85836988487,
      "learning_rate": 1.5540076523223748e-07,
      "loss": 1.6733,
      "step": 414496
    },
    {
      "epoch": 0.0015044043807386882,
      "grad_norm": 12600.160316440422,
      "learning_rate": 1.5539476104438384e-07,
      "loss": 1.6646,
      "step": 414528
    },
    {
      "epoch": 0.001504520515089525,
      "grad_norm": 19682.44944106297,
      "learning_rate": 1.5538875755242418e-07,
      "loss": 1.6578,
      "step": 414560
    },
    {
      "epoch": 0.0015046366494403617,
      "grad_norm": 19671.42943458863,
      "learning_rate": 1.5538275475622407e-07,
      "loss": 1.6688,
      "step": 414592
    },
    {
      "epoch": 0.0015047527837911984,
      "grad_norm": 14098.307841723416,
      "learning_rate": 1.5537694021076398e-07,
      "loss": 1.6642,
      "step": 414624
    },
    {
      "epoch": 0.001504868918142035,
      "grad_norm": 8885.613991165721,
      "learning_rate": 1.553709387839479e-07,
      "loss": 1.6727,
      "step": 414656
    },
    {
      "epoch": 0.0015049850524928717,
      "grad_norm": 9403.290062525988,
      "learning_rate": 1.5536493805249254e-07,
      "loss": 1.6797,
      "step": 414688
    },
    {
      "epoch": 0.0015051011868437085,
      "grad_norm": 10304.137130298684,
      "learning_rate": 1.553589380162637e-07,
      "loss": 1.6846,
      "step": 414720
    },
    {
      "epoch": 0.0015052173211945452,
      "grad_norm": 9679.530980372963,
      "learning_rate": 1.5535293867512704e-07,
      "loss": 1.6899,
      "step": 414752
    },
    {
      "epoch": 0.0015053334555453818,
      "grad_norm": 9103.306651980916,
      "learning_rate": 1.5534694002894845e-07,
      "loss": 1.7006,
      "step": 414784
    },
    {
      "epoch": 0.0015054495898962185,
      "grad_norm": 9755.084110349844,
      "learning_rate": 1.5534094207759374e-07,
      "loss": 1.6796,
      "step": 414816
    },
    {
      "epoch": 0.0015055657242470553,
      "grad_norm": 11481.955060006114,
      "learning_rate": 1.553349448209288e-07,
      "loss": 1.7009,
      "step": 414848
    },
    {
      "epoch": 0.001505681858597892,
      "grad_norm": 10538.093945301494,
      "learning_rate": 1.5532894825881952e-07,
      "loss": 1.7164,
      "step": 414880
    },
    {
      "epoch": 0.0015057979929487288,
      "grad_norm": 8266.732728230663,
      "learning_rate": 1.5532295239113184e-07,
      "loss": 1.6761,
      "step": 414912
    },
    {
      "epoch": 0.0015059141272995653,
      "grad_norm": 9666.07241851622,
      "learning_rate": 1.5531695721773177e-07,
      "loss": 1.6557,
      "step": 414944
    },
    {
      "epoch": 0.001506030261650402,
      "grad_norm": 10180.949268118371,
      "learning_rate": 1.553109627384853e-07,
      "loss": 1.6739,
      "step": 414976
    },
    {
      "epoch": 0.0015061463960012388,
      "grad_norm": 9904.150645057858,
      "learning_rate": 1.5530496895325857e-07,
      "loss": 1.678,
      "step": 415008
    },
    {
      "epoch": 0.0015062625303520756,
      "grad_norm": 10648.838058680392,
      "learning_rate": 1.5529897586191752e-07,
      "loss": 1.6882,
      "step": 415040
    },
    {
      "epoch": 0.0015063786647029121,
      "grad_norm": 11546.110167497969,
      "learning_rate": 1.552929834643284e-07,
      "loss": 1.6797,
      "step": 415072
    },
    {
      "epoch": 0.0015064947990537489,
      "grad_norm": 9388.484222706027,
      "learning_rate": 1.5528699176035734e-07,
      "loss": 1.6646,
      "step": 415104
    },
    {
      "epoch": 0.0015066109334045856,
      "grad_norm": 8447.790953852966,
      "learning_rate": 1.552810007498705e-07,
      "loss": 1.6754,
      "step": 415136
    },
    {
      "epoch": 0.0015067270677554224,
      "grad_norm": 9289.4606947874,
      "learning_rate": 1.552750104327342e-07,
      "loss": 1.6873,
      "step": 415168
    },
    {
      "epoch": 0.0015068432021062591,
      "grad_norm": 10744.30509618933,
      "learning_rate": 1.5526902080881467e-07,
      "loss": 1.6793,
      "step": 415200
    },
    {
      "epoch": 0.0015069593364570957,
      "grad_norm": 9144.910497101653,
      "learning_rate": 1.552630318779782e-07,
      "loss": 1.6696,
      "step": 415232
    },
    {
      "epoch": 0.0015070754708079324,
      "grad_norm": 9566.846084264134,
      "learning_rate": 1.5525704364009112e-07,
      "loss": 1.6803,
      "step": 415264
    },
    {
      "epoch": 0.0015071916051587692,
      "grad_norm": 9148.50982400959,
      "learning_rate": 1.5525105609501985e-07,
      "loss": 1.6696,
      "step": 415296
    },
    {
      "epoch": 0.001507307739509606,
      "grad_norm": 14763.033563600673,
      "learning_rate": 1.552450692426308e-07,
      "loss": 1.6821,
      "step": 415328
    },
    {
      "epoch": 0.0015074238738604425,
      "grad_norm": 11774.528610521951,
      "learning_rate": 1.5523908308279044e-07,
      "loss": 1.6852,
      "step": 415360
    },
    {
      "epoch": 0.0015075400082112792,
      "grad_norm": 8587.13747415284,
      "learning_rate": 1.552330976153652e-07,
      "loss": 1.6873,
      "step": 415392
    },
    {
      "epoch": 0.001507656142562116,
      "grad_norm": 10026.369033703078,
      "learning_rate": 1.5522711284022166e-07,
      "loss": 1.678,
      "step": 415424
    },
    {
      "epoch": 0.0015077722769129527,
      "grad_norm": 8905.22992403902,
      "learning_rate": 1.552211287572264e-07,
      "loss": 1.6632,
      "step": 415456
    },
    {
      "epoch": 0.0015078884112637895,
      "grad_norm": 11393.262921569043,
      "learning_rate": 1.5521514536624593e-07,
      "loss": 1.6741,
      "step": 415488
    },
    {
      "epoch": 0.001508004545614626,
      "grad_norm": 11107.746126014943,
      "learning_rate": 1.5520916266714695e-07,
      "loss": 1.6562,
      "step": 415520
    },
    {
      "epoch": 0.0015081206799654628,
      "grad_norm": 10878.727866805015,
      "learning_rate": 1.552031806597961e-07,
      "loss": 1.675,
      "step": 415552
    },
    {
      "epoch": 0.0015082368143162995,
      "grad_norm": 10886.78189365434,
      "learning_rate": 1.5519719934406014e-07,
      "loss": 1.6933,
      "step": 415584
    },
    {
      "epoch": 0.0015083529486671363,
      "grad_norm": 20469.38240397106,
      "learning_rate": 1.5519121871980574e-07,
      "loss": 1.6884,
      "step": 415616
    },
    {
      "epoch": 0.0015084690830179728,
      "grad_norm": 9291.417545240338,
      "learning_rate": 1.551854256493396e-07,
      "loss": 1.6973,
      "step": 415648
    },
    {
      "epoch": 0.0015085852173688096,
      "grad_norm": 8717.84503188718,
      "learning_rate": 1.551794463860503e-07,
      "loss": 1.6872,
      "step": 415680
    },
    {
      "epoch": 0.0015087013517196463,
      "grad_norm": 8957.292782978571,
      "learning_rate": 1.5517346781384716e-07,
      "loss": 1.6757,
      "step": 415712
    },
    {
      "epoch": 0.001508817486070483,
      "grad_norm": 8614.153469726436,
      "learning_rate": 1.551674899325971e-07,
      "loss": 1.6774,
      "step": 415744
    },
    {
      "epoch": 0.0015089336204213199,
      "grad_norm": 10947.670802504064,
      "learning_rate": 1.55161512742167e-07,
      "loss": 1.6835,
      "step": 415776
    },
    {
      "epoch": 0.0015090497547721564,
      "grad_norm": 10868.546912996235,
      "learning_rate": 1.551555362424239e-07,
      "loss": 1.6687,
      "step": 415808
    },
    {
      "epoch": 0.0015091658891229931,
      "grad_norm": 9205.205483855318,
      "learning_rate": 1.5514956043323472e-07,
      "loss": 1.6676,
      "step": 415840
    },
    {
      "epoch": 0.00150928202347383,
      "grad_norm": 11320.395576127186,
      "learning_rate": 1.551435853144665e-07,
      "loss": 1.6798,
      "step": 415872
    },
    {
      "epoch": 0.0015093981578246666,
      "grad_norm": 8836.043911162958,
      "learning_rate": 1.551376108859863e-07,
      "loss": 1.6942,
      "step": 415904
    },
    {
      "epoch": 0.0015095142921755032,
      "grad_norm": 8441.99028665634,
      "learning_rate": 1.551316371476612e-07,
      "loss": 1.6969,
      "step": 415936
    },
    {
      "epoch": 0.00150963042652634,
      "grad_norm": 9859.895536971982,
      "learning_rate": 1.551256640993584e-07,
      "loss": 1.6998,
      "step": 415968
    },
    {
      "epoch": 0.0015097465608771767,
      "grad_norm": 10698.730391967076,
      "learning_rate": 1.55119691740945e-07,
      "loss": 1.673,
      "step": 416000
    },
    {
      "epoch": 0.0015098626952280134,
      "grad_norm": 10319.111298944303,
      "learning_rate": 1.5511372007228827e-07,
      "loss": 1.6814,
      "step": 416032
    },
    {
      "epoch": 0.0015099788295788502,
      "grad_norm": 13152.399172774523,
      "learning_rate": 1.551077490932554e-07,
      "loss": 1.6762,
      "step": 416064
    },
    {
      "epoch": 0.0015100949639296867,
      "grad_norm": 9166.95434700097,
      "learning_rate": 1.551017788037137e-07,
      "loss": 1.6724,
      "step": 416096
    },
    {
      "epoch": 0.0015102110982805235,
      "grad_norm": 8404.223105082348,
      "learning_rate": 1.5509580920353042e-07,
      "loss": 1.665,
      "step": 416128
    },
    {
      "epoch": 0.0015103272326313602,
      "grad_norm": 9172.60464644585,
      "learning_rate": 1.55089840292573e-07,
      "loss": 1.6686,
      "step": 416160
    },
    {
      "epoch": 0.001510443366982197,
      "grad_norm": 9820.75984840277,
      "learning_rate": 1.5508387207070877e-07,
      "loss": 1.6743,
      "step": 416192
    },
    {
      "epoch": 0.0015105595013330335,
      "grad_norm": 10299.999223300942,
      "learning_rate": 1.5507790453780515e-07,
      "loss": 1.6738,
      "step": 416224
    },
    {
      "epoch": 0.0015106756356838703,
      "grad_norm": 13718.165620810969,
      "learning_rate": 1.5507193769372966e-07,
      "loss": 1.666,
      "step": 416256
    },
    {
      "epoch": 0.001510791770034707,
      "grad_norm": 10803.568854781275,
      "learning_rate": 1.5506597153834968e-07,
      "loss": 1.6669,
      "step": 416288
    },
    {
      "epoch": 0.0015109079043855438,
      "grad_norm": 10872.316772427117,
      "learning_rate": 1.5506000607153281e-07,
      "loss": 1.6619,
      "step": 416320
    },
    {
      "epoch": 0.0015110240387363806,
      "grad_norm": 9099.67867564564,
      "learning_rate": 1.5505404129314664e-07,
      "loss": 1.6656,
      "step": 416352
    },
    {
      "epoch": 0.001511140173087217,
      "grad_norm": 11981.322130716626,
      "learning_rate": 1.550480772030587e-07,
      "loss": 1.6868,
      "step": 416384
    },
    {
      "epoch": 0.0015112563074380538,
      "grad_norm": 9128.83957576208,
      "learning_rate": 1.5504211380113666e-07,
      "loss": 1.6667,
      "step": 416416
    },
    {
      "epoch": 0.0015113724417888906,
      "grad_norm": 9819.497746830028,
      "learning_rate": 1.5503615108724818e-07,
      "loss": 1.6798,
      "step": 416448
    },
    {
      "epoch": 0.0015114885761397274,
      "grad_norm": 8329.820406227256,
      "learning_rate": 1.55030189061261e-07,
      "loss": 1.7155,
      "step": 416480
    },
    {
      "epoch": 0.001511604710490564,
      "grad_norm": 8680.931516836197,
      "learning_rate": 1.550242277230428e-07,
      "loss": 1.7174,
      "step": 416512
    },
    {
      "epoch": 0.0015117208448414006,
      "grad_norm": 9085.731671142397,
      "learning_rate": 1.550182670724614e-07,
      "loss": 1.7011,
      "step": 416544
    },
    {
      "epoch": 0.0015118369791922374,
      "grad_norm": 9285.610588431975,
      "learning_rate": 1.550123071093846e-07,
      "loss": 1.6926,
      "step": 416576
    },
    {
      "epoch": 0.0015119531135430742,
      "grad_norm": 9747.729171453217,
      "learning_rate": 1.5500634783368023e-07,
      "loss": 1.6898,
      "step": 416608
    },
    {
      "epoch": 0.001512069247893911,
      "grad_norm": 16378.659530010385,
      "learning_rate": 1.5500038924521623e-07,
      "loss": 1.6898,
      "step": 416640
    },
    {
      "epoch": 0.0015121853822447474,
      "grad_norm": 17700.59727805816,
      "learning_rate": 1.549944313438605e-07,
      "loss": 1.7057,
      "step": 416672
    },
    {
      "epoch": 0.0015123015165955842,
      "grad_norm": 20381.026078193412,
      "learning_rate": 1.5498847412948093e-07,
      "loss": 1.6739,
      "step": 416704
    },
    {
      "epoch": 0.001512417650946421,
      "grad_norm": 12501.170025241638,
      "learning_rate": 1.5498270373303583e-07,
      "loss": 1.6827,
      "step": 416736
    },
    {
      "epoch": 0.0015125337852972577,
      "grad_norm": 8830.820007224696,
      "learning_rate": 1.5497674787075496e-07,
      "loss": 1.6858,
      "step": 416768
    },
    {
      "epoch": 0.0015126499196480942,
      "grad_norm": 10752.177639901603,
      "learning_rate": 1.5497079269505846e-07,
      "loss": 1.6912,
      "step": 416800
    },
    {
      "epoch": 0.001512766053998931,
      "grad_norm": 10480.618684028152,
      "learning_rate": 1.549648382058145e-07,
      "loss": 1.6876,
      "step": 416832
    },
    {
      "epoch": 0.0015128821883497678,
      "grad_norm": 9343.845782117767,
      "learning_rate": 1.5495888440289118e-07,
      "loss": 1.6891,
      "step": 416864
    },
    {
      "epoch": 0.0015129983227006045,
      "grad_norm": 9407.101891656112,
      "learning_rate": 1.5495293128615667e-07,
      "loss": 1.6816,
      "step": 416896
    },
    {
      "epoch": 0.0015131144570514413,
      "grad_norm": 10549.942938234311,
      "learning_rate": 1.5494697885547915e-07,
      "loss": 1.6886,
      "step": 416928
    },
    {
      "epoch": 0.0015132305914022778,
      "grad_norm": 11654.268745828715,
      "learning_rate": 1.5494102711072688e-07,
      "loss": 1.6927,
      "step": 416960
    },
    {
      "epoch": 0.0015133467257531146,
      "grad_norm": 8431.908680719924,
      "learning_rate": 1.549350760517681e-07,
      "loss": 1.6942,
      "step": 416992
    },
    {
      "epoch": 0.0015134628601039513,
      "grad_norm": 8745.004516865614,
      "learning_rate": 1.549291256784712e-07,
      "loss": 1.6949,
      "step": 417024
    },
    {
      "epoch": 0.001513578994454788,
      "grad_norm": 8089.2124462150205,
      "learning_rate": 1.5492317599070442e-07,
      "loss": 1.6992,
      "step": 417056
    },
    {
      "epoch": 0.0015136951288056246,
      "grad_norm": 8408.08801095707,
      "learning_rate": 1.5491722698833618e-07,
      "loss": 1.6917,
      "step": 417088
    },
    {
      "epoch": 0.0015138112631564614,
      "grad_norm": 9575.189815350921,
      "learning_rate": 1.5491127867123494e-07,
      "loss": 1.6867,
      "step": 417120
    },
    {
      "epoch": 0.0015139273975072981,
      "grad_norm": 9389.739719502346,
      "learning_rate": 1.549053310392691e-07,
      "loss": 1.6947,
      "step": 417152
    },
    {
      "epoch": 0.0015140435318581349,
      "grad_norm": 11210.530674325815,
      "learning_rate": 1.5489938409230714e-07,
      "loss": 1.6655,
      "step": 417184
    },
    {
      "epoch": 0.0015141596662089716,
      "grad_norm": 9215.825953217649,
      "learning_rate": 1.5489343783021763e-07,
      "loss": 1.6615,
      "step": 417216
    },
    {
      "epoch": 0.0015142758005598082,
      "grad_norm": 10869.983256656838,
      "learning_rate": 1.5488749225286907e-07,
      "loss": 1.66,
      "step": 417248
    },
    {
      "epoch": 0.001514391934910645,
      "grad_norm": 14225.224919135724,
      "learning_rate": 1.548815473601301e-07,
      "loss": 1.6765,
      "step": 417280
    },
    {
      "epoch": 0.0015145080692614817,
      "grad_norm": 11497.660283727293,
      "learning_rate": 1.548756031518693e-07,
      "loss": 1.6535,
      "step": 417312
    },
    {
      "epoch": 0.0015146242036123184,
      "grad_norm": 10642.229277740636,
      "learning_rate": 1.5486965962795536e-07,
      "loss": 1.6683,
      "step": 417344
    },
    {
      "epoch": 0.001514740337963155,
      "grad_norm": 12609.07086188352,
      "learning_rate": 1.5486371678825697e-07,
      "loss": 1.6877,
      "step": 417376
    },
    {
      "epoch": 0.0015148564723139917,
      "grad_norm": 8801.274907648323,
      "learning_rate": 1.5485777463264287e-07,
      "loss": 1.6904,
      "step": 417408
    },
    {
      "epoch": 0.0015149726066648285,
      "grad_norm": 10902.660225834794,
      "learning_rate": 1.5485183316098183e-07,
      "loss": 1.7009,
      "step": 417440
    },
    {
      "epoch": 0.0015150887410156652,
      "grad_norm": 12157.95657172701,
      "learning_rate": 1.5484589237314264e-07,
      "loss": 1.6914,
      "step": 417472
    },
    {
      "epoch": 0.001515204875366502,
      "grad_norm": 8565.790097825186,
      "learning_rate": 1.5483995226899412e-07,
      "loss": 1.6871,
      "step": 417504
    },
    {
      "epoch": 0.0015153210097173385,
      "grad_norm": 10407.47231560094,
      "learning_rate": 1.5483401284840518e-07,
      "loss": 1.6847,
      "step": 417536
    },
    {
      "epoch": 0.0015154371440681753,
      "grad_norm": 10172.532428063329,
      "learning_rate": 1.5482807411124472e-07,
      "loss": 1.6895,
      "step": 417568
    },
    {
      "epoch": 0.001515553278419012,
      "grad_norm": 9240.757111838835,
      "learning_rate": 1.548221360573817e-07,
      "loss": 1.673,
      "step": 417600
    },
    {
      "epoch": 0.0015156694127698488,
      "grad_norm": 10374.692477370112,
      "learning_rate": 1.5481619868668506e-07,
      "loss": 1.6692,
      "step": 417632
    },
    {
      "epoch": 0.0015157855471206853,
      "grad_norm": 11326.937273596954,
      "learning_rate": 1.548102619990238e-07,
      "loss": 1.6801,
      "step": 417664
    },
    {
      "epoch": 0.001515901681471522,
      "grad_norm": 8740.687272749208,
      "learning_rate": 1.5480432599426707e-07,
      "loss": 1.6722,
      "step": 417696
    },
    {
      "epoch": 0.0015160178158223588,
      "grad_norm": 10074.616717275154,
      "learning_rate": 1.5479839067228385e-07,
      "loss": 1.6832,
      "step": 417728
    },
    {
      "epoch": 0.0015161339501731956,
      "grad_norm": 20099.796217872456,
      "learning_rate": 1.5479245603294327e-07,
      "loss": 1.6808,
      "step": 417760
    },
    {
      "epoch": 0.0015162500845240323,
      "grad_norm": 23646.654562538017,
      "learning_rate": 1.5478652207611455e-07,
      "loss": 1.6651,
      "step": 417792
    },
    {
      "epoch": 0.0015163662188748689,
      "grad_norm": 9347.206213623405,
      "learning_rate": 1.5478077420616558e-07,
      "loss": 1.6597,
      "step": 417824
    },
    {
      "epoch": 0.0015164823532257056,
      "grad_norm": 9479.607375835774,
      "learning_rate": 1.5477484159264978e-07,
      "loss": 1.6655,
      "step": 417856
    },
    {
      "epoch": 0.0015165984875765424,
      "grad_norm": 9494.713055169177,
      "learning_rate": 1.5476890966125751e-07,
      "loss": 1.6575,
      "step": 417888
    },
    {
      "epoch": 0.0015167146219273791,
      "grad_norm": 7989.156901701205,
      "learning_rate": 1.5476297841185812e-07,
      "loss": 1.6589,
      "step": 417920
    },
    {
      "epoch": 0.0015168307562782157,
      "grad_norm": 9040.819874325558,
      "learning_rate": 1.5475704784432096e-07,
      "loss": 1.6661,
      "step": 417952
    },
    {
      "epoch": 0.0015169468906290524,
      "grad_norm": 8898.921507688447,
      "learning_rate": 1.5475111795851536e-07,
      "loss": 1.6784,
      "step": 417984
    },
    {
      "epoch": 0.0015170630249798892,
      "grad_norm": 8922.404384469468,
      "learning_rate": 1.5474518875431073e-07,
      "loss": 1.681,
      "step": 418016
    },
    {
      "epoch": 0.001517179159330726,
      "grad_norm": 10853.738987095645,
      "learning_rate": 1.5473926023157646e-07,
      "loss": 1.6881,
      "step": 418048
    },
    {
      "epoch": 0.0015172952936815627,
      "grad_norm": 11305.646907629833,
      "learning_rate": 1.547333323901821e-07,
      "loss": 1.6686,
      "step": 418080
    },
    {
      "epoch": 0.0015174114280323992,
      "grad_norm": 9355.397158859692,
      "learning_rate": 1.5472740522999713e-07,
      "loss": 1.679,
      "step": 418112
    },
    {
      "epoch": 0.001517527562383236,
      "grad_norm": 9516.321768414517,
      "learning_rate": 1.5472147875089101e-07,
      "loss": 1.6828,
      "step": 418144
    },
    {
      "epoch": 0.0015176436967340727,
      "grad_norm": 12115.472669277084,
      "learning_rate": 1.5471555295273343e-07,
      "loss": 1.6679,
      "step": 418176
    },
    {
      "epoch": 0.0015177598310849095,
      "grad_norm": 12463.604133636465,
      "learning_rate": 1.5470962783539393e-07,
      "loss": 1.6571,
      "step": 418208
    },
    {
      "epoch": 0.001517875965435746,
      "grad_norm": 9892.36068893568,
      "learning_rate": 1.5470370339874218e-07,
      "loss": 1.6785,
      "step": 418240
    },
    {
      "epoch": 0.0015179920997865828,
      "grad_norm": 8663.749765546094,
      "learning_rate": 1.5469777964264783e-07,
      "loss": 1.6973,
      "step": 418272
    },
    {
      "epoch": 0.0015181082341374195,
      "grad_norm": 10818.488803894932,
      "learning_rate": 1.5469185656698064e-07,
      "loss": 1.7016,
      "step": 418304
    },
    {
      "epoch": 0.0015182243684882563,
      "grad_norm": 9190.659823973467,
      "learning_rate": 1.5468593417161032e-07,
      "loss": 1.6965,
      "step": 418336
    },
    {
      "epoch": 0.001518340502839093,
      "grad_norm": 13409.247555325392,
      "learning_rate": 1.5468001245640665e-07,
      "loss": 1.6726,
      "step": 418368
    },
    {
      "epoch": 0.0015184566371899296,
      "grad_norm": 9477.912744903279,
      "learning_rate": 1.546740914212395e-07,
      "loss": 1.6782,
      "step": 418400
    },
    {
      "epoch": 0.0015185727715407663,
      "grad_norm": 9242.371016140825,
      "learning_rate": 1.5466817106597866e-07,
      "loss": 1.6646,
      "step": 418432
    },
    {
      "epoch": 0.001518688905891603,
      "grad_norm": 10588.8977707786,
      "learning_rate": 1.5466225139049404e-07,
      "loss": 1.6711,
      "step": 418464
    },
    {
      "epoch": 0.0015188050402424398,
      "grad_norm": 10373.126433240848,
      "learning_rate": 1.546563323946556e-07,
      "loss": 1.6547,
      "step": 418496
    },
    {
      "epoch": 0.0015189211745932764,
      "grad_norm": 10708.30845652104,
      "learning_rate": 1.5465041407833325e-07,
      "loss": 1.6626,
      "step": 418528
    },
    {
      "epoch": 0.0015190373089441131,
      "grad_norm": 8604.971818663906,
      "learning_rate": 1.54644496441397e-07,
      "loss": 1.6784,
      "step": 418560
    },
    {
      "epoch": 0.0015191534432949499,
      "grad_norm": 10491.420685493456,
      "learning_rate": 1.546385794837169e-07,
      "loss": 1.6768,
      "step": 418592
    },
    {
      "epoch": 0.0015192695776457866,
      "grad_norm": 8736.934244916805,
      "learning_rate": 1.5463266320516297e-07,
      "loss": 1.7011,
      "step": 418624
    },
    {
      "epoch": 0.0015193857119966234,
      "grad_norm": 9812.931977752622,
      "learning_rate": 1.546267476056053e-07,
      "loss": 1.7024,
      "step": 418656
    },
    {
      "epoch": 0.00151950184634746,
      "grad_norm": 11362.243264426264,
      "learning_rate": 1.5462083268491411e-07,
      "loss": 1.6781,
      "step": 418688
    },
    {
      "epoch": 0.0015196179806982967,
      "grad_norm": 11418.576005789864,
      "learning_rate": 1.5461491844295946e-07,
      "loss": 1.6553,
      "step": 418720
    },
    {
      "epoch": 0.0015197341150491334,
      "grad_norm": 8898.862062084117,
      "learning_rate": 1.5460900487961163e-07,
      "loss": 1.6681,
      "step": 418752
    },
    {
      "epoch": 0.0015198502493999702,
      "grad_norm": 11051.488044602862,
      "learning_rate": 1.5460309199474084e-07,
      "loss": 1.6614,
      "step": 418784
    },
    {
      "epoch": 0.0015199663837508067,
      "grad_norm": 17664.350540000047,
      "learning_rate": 1.545971797882173e-07,
      "loss": 1.6613,
      "step": 418816
    },
    {
      "epoch": 0.0015200825181016435,
      "grad_norm": 18318.807821471353,
      "learning_rate": 1.5459126825991138e-07,
      "loss": 1.6779,
      "step": 418848
    },
    {
      "epoch": 0.0015201986524524802,
      "grad_norm": 19912.796287814526,
      "learning_rate": 1.545853574096934e-07,
      "loss": 1.6666,
      "step": 418880
    },
    {
      "epoch": 0.001520314786803317,
      "grad_norm": 9210.02540713108,
      "learning_rate": 1.545796319200561e-07,
      "loss": 1.6778,
      "step": 418912
    },
    {
      "epoch": 0.0015204309211541537,
      "grad_norm": 11705.136308475865,
      "learning_rate": 1.5457372240444499e-07,
      "loss": 1.6738,
      "step": 418944
    },
    {
      "epoch": 0.0015205470555049903,
      "grad_norm": 10506.649799055835,
      "learning_rate": 1.545678135665371e-07,
      "loss": 1.6509,
      "step": 418976
    },
    {
      "epoch": 0.001520663189855827,
      "grad_norm": 9988.115537978123,
      "learning_rate": 1.545619054062029e-07,
      "loss": 1.6602,
      "step": 419008
    },
    {
      "epoch": 0.0015207793242066638,
      "grad_norm": 7890.387062749203,
      "learning_rate": 1.545559979233129e-07,
      "loss": 1.673,
      "step": 419040
    },
    {
      "epoch": 0.0015208954585575005,
      "grad_norm": 10228.491579895835,
      "learning_rate": 1.5455009111773772e-07,
      "loss": 1.6816,
      "step": 419072
    },
    {
      "epoch": 0.001521011592908337,
      "grad_norm": 9779.613796055548,
      "learning_rate": 1.5454418498934784e-07,
      "loss": 1.6731,
      "step": 419104
    },
    {
      "epoch": 0.0015211277272591738,
      "grad_norm": 9536.055788427415,
      "learning_rate": 1.5453827953801392e-07,
      "loss": 1.6913,
      "step": 419136
    },
    {
      "epoch": 0.0015212438616100106,
      "grad_norm": 9158.675013341177,
      "learning_rate": 1.545323747636066e-07,
      "loss": 1.7231,
      "step": 419168
    },
    {
      "epoch": 0.0015213599959608473,
      "grad_norm": 10135.295160970893,
      "learning_rate": 1.545264706659966e-07,
      "loss": 1.7209,
      "step": 419200
    },
    {
      "epoch": 0.001521476130311684,
      "grad_norm": 14435.902327184123,
      "learning_rate": 1.545205672450546e-07,
      "loss": 1.7104,
      "step": 419232
    },
    {
      "epoch": 0.0015215922646625206,
      "grad_norm": 9649.493043678513,
      "learning_rate": 1.5451466450065137e-07,
      "loss": 1.6808,
      "step": 419264
    },
    {
      "epoch": 0.0015217083990133574,
      "grad_norm": 8193.499862696039,
      "learning_rate": 1.5450876243265774e-07,
      "loss": 1.6672,
      "step": 419296
    },
    {
      "epoch": 0.0015218245333641941,
      "grad_norm": 9581.154210219143,
      "learning_rate": 1.5450286104094445e-07,
      "loss": 1.6637,
      "step": 419328
    },
    {
      "epoch": 0.0015219406677150309,
      "grad_norm": 9919.835280890506,
      "learning_rate": 1.544969603253824e-07,
      "loss": 1.6768,
      "step": 419360
    },
    {
      "epoch": 0.0015220568020658674,
      "grad_norm": 9561.03739141313,
      "learning_rate": 1.5449106028584253e-07,
      "loss": 1.6635,
      "step": 419392
    },
    {
      "epoch": 0.0015221729364167042,
      "grad_norm": 9216.292421576043,
      "learning_rate": 1.544851609221957e-07,
      "loss": 1.6709,
      "step": 419424
    },
    {
      "epoch": 0.001522289070767541,
      "grad_norm": 14826.613166869904,
      "learning_rate": 1.544792622343129e-07,
      "loss": 1.672,
      "step": 419456
    },
    {
      "epoch": 0.0015224052051183777,
      "grad_norm": 9677.794376819546,
      "learning_rate": 1.5447336422206513e-07,
      "loss": 1.6763,
      "step": 419488
    },
    {
      "epoch": 0.0015225213394692144,
      "grad_norm": 10682.300875747696,
      "learning_rate": 1.5446746688532341e-07,
      "loss": 1.6872,
      "step": 419520
    },
    {
      "epoch": 0.001522637473820051,
      "grad_norm": 9609.019929212343,
      "learning_rate": 1.5446157022395882e-07,
      "loss": 1.6808,
      "step": 419552
    },
    {
      "epoch": 0.0015227536081708877,
      "grad_norm": 10167.142764808606,
      "learning_rate": 1.5445567423784243e-07,
      "loss": 1.6595,
      "step": 419584
    },
    {
      "epoch": 0.0015228697425217245,
      "grad_norm": 9115.123915778655,
      "learning_rate": 1.5444977892684542e-07,
      "loss": 1.6601,
      "step": 419616
    },
    {
      "epoch": 0.0015229858768725612,
      "grad_norm": 10806.062187494574,
      "learning_rate": 1.5444388429083896e-07,
      "loss": 1.6775,
      "step": 419648
    },
    {
      "epoch": 0.0015231020112233978,
      "grad_norm": 12937.30466519205,
      "learning_rate": 1.544379903296942e-07,
      "loss": 1.6649,
      "step": 419680
    },
    {
      "epoch": 0.0015232181455742345,
      "grad_norm": 10043.228763699452,
      "learning_rate": 1.5443209704328238e-07,
      "loss": 1.6817,
      "step": 419712
    },
    {
      "epoch": 0.0015233342799250713,
      "grad_norm": 8536.518376949703,
      "learning_rate": 1.5442620443147483e-07,
      "loss": 1.6893,
      "step": 419744
    },
    {
      "epoch": 0.001523450414275908,
      "grad_norm": 11045.166182543384,
      "learning_rate": 1.5442031249414282e-07,
      "loss": 1.6855,
      "step": 419776
    },
    {
      "epoch": 0.0015235665486267448,
      "grad_norm": 10559.559839311485,
      "learning_rate": 1.544144212311577e-07,
      "loss": 1.6722,
      "step": 419808
    },
    {
      "epoch": 0.0015236826829775813,
      "grad_norm": 16254.474706984536,
      "learning_rate": 1.5440853064239085e-07,
      "loss": 1.6676,
      "step": 419840
    },
    {
      "epoch": 0.001523798817328418,
      "grad_norm": 9654.926410905471,
      "learning_rate": 1.5440264072771364e-07,
      "loss": 1.6551,
      "step": 419872
    },
    {
      "epoch": 0.0015239149516792548,
      "grad_norm": 19712.219560465535,
      "learning_rate": 1.5439675148699754e-07,
      "loss": 1.6731,
      "step": 419904
    },
    {
      "epoch": 0.0015240310860300916,
      "grad_norm": 17043.60102795181,
      "learning_rate": 1.5439086292011407e-07,
      "loss": 1.6849,
      "step": 419936
    },
    {
      "epoch": 0.0015241472203809281,
      "grad_norm": 17686.03471669102,
      "learning_rate": 1.5438497502693473e-07,
      "loss": 1.6778,
      "step": 419968
    },
    {
      "epoch": 0.0015242633547317649,
      "grad_norm": 9394.357455409072,
      "learning_rate": 1.5437927177274915e-07,
      "loss": 1.673,
      "step": 420000
    },
    {
      "epoch": 0.0015243794890826016,
      "grad_norm": 10192.482131453555,
      "learning_rate": 1.543733852055494e-07,
      "loss": 1.6954,
      "step": 420032
    },
    {
      "epoch": 0.0015244956234334384,
      "grad_norm": 12430.544959896168,
      "learning_rate": 1.5436749931167248e-07,
      "loss": 1.7038,
      "step": 420064
    },
    {
      "epoch": 0.001524611757784275,
      "grad_norm": 11611.92283818662,
      "learning_rate": 1.543616140909901e-07,
      "loss": 1.7051,
      "step": 420096
    },
    {
      "epoch": 0.0015247278921351117,
      "grad_norm": 11388.201438330812,
      "learning_rate": 1.543557295433739e-07,
      "loss": 1.7088,
      "step": 420128
    },
    {
      "epoch": 0.0015248440264859484,
      "grad_norm": 9914.827482109813,
      "learning_rate": 1.5434984566869558e-07,
      "loss": 1.684,
      "step": 420160
    },
    {
      "epoch": 0.0015249601608367852,
      "grad_norm": 11309.59963924453,
      "learning_rate": 1.5434396246682695e-07,
      "loss": 1.672,
      "step": 420192
    },
    {
      "epoch": 0.001525076295187622,
      "grad_norm": 9549.99120418443,
      "learning_rate": 1.5433807993763978e-07,
      "loss": 1.6698,
      "step": 420224
    },
    {
      "epoch": 0.0015251924295384585,
      "grad_norm": 10398.313901782345,
      "learning_rate": 1.5433219808100585e-07,
      "loss": 1.6955,
      "step": 420256
    },
    {
      "epoch": 0.0015253085638892952,
      "grad_norm": 9424.969389870717,
      "learning_rate": 1.5432631689679707e-07,
      "loss": 1.677,
      "step": 420288
    },
    {
      "epoch": 0.001525424698240132,
      "grad_norm": 8973.175803471142,
      "learning_rate": 1.543204363848853e-07,
      "loss": 1.6789,
      "step": 420320
    },
    {
      "epoch": 0.0015255408325909688,
      "grad_norm": 11415.948318032977,
      "learning_rate": 1.5431455654514246e-07,
      "loss": 1.6741,
      "step": 420352
    },
    {
      "epoch": 0.0015256569669418053,
      "grad_norm": 12841.311225883439,
      "learning_rate": 1.543086773774405e-07,
      "loss": 1.6832,
      "step": 420384
    },
    {
      "epoch": 0.001525773101292642,
      "grad_norm": 14721.031893179228,
      "learning_rate": 1.5430279888165143e-07,
      "loss": 1.6879,
      "step": 420416
    },
    {
      "epoch": 0.0015258892356434788,
      "grad_norm": 8868.729559525424,
      "learning_rate": 1.5429692105764724e-07,
      "loss": 1.6646,
      "step": 420448
    },
    {
      "epoch": 0.0015260053699943156,
      "grad_norm": 9717.568008509124,
      "learning_rate": 1.542910439053e-07,
      "loss": 1.6656,
      "step": 420480
    },
    {
      "epoch": 0.0015261215043451523,
      "grad_norm": 11104.32996627892,
      "learning_rate": 1.5428516742448186e-07,
      "loss": 1.6647,
      "step": 420512
    },
    {
      "epoch": 0.0015262376386959888,
      "grad_norm": 13709.55987623235,
      "learning_rate": 1.5427929161506487e-07,
      "loss": 1.6697,
      "step": 420544
    },
    {
      "epoch": 0.0015263537730468256,
      "grad_norm": 9184.233664274881,
      "learning_rate": 1.5427341647692126e-07,
      "loss": 1.654,
      "step": 420576
    },
    {
      "epoch": 0.0015264699073976624,
      "grad_norm": 10719.62219483504,
      "learning_rate": 1.5426754200992317e-07,
      "loss": 1.6659,
      "step": 420608
    },
    {
      "epoch": 0.001526586041748499,
      "grad_norm": 10368.313845558496,
      "learning_rate": 1.5426166821394286e-07,
      "loss": 1.6726,
      "step": 420640
    },
    {
      "epoch": 0.0015267021760993356,
      "grad_norm": 9727.622114371014,
      "learning_rate": 1.5425579508885256e-07,
      "loss": 1.6766,
      "step": 420672
    },
    {
      "epoch": 0.0015268183104501724,
      "grad_norm": 8943.108855426059,
      "learning_rate": 1.542499226345246e-07,
      "loss": 1.6731,
      "step": 420704
    },
    {
      "epoch": 0.0015269344448010091,
      "grad_norm": 9040.331409854398,
      "learning_rate": 1.5424405085083134e-07,
      "loss": 1.6767,
      "step": 420736
    },
    {
      "epoch": 0.001527050579151846,
      "grad_norm": 12100.330573996729,
      "learning_rate": 1.5423817973764507e-07,
      "loss": 1.6754,
      "step": 420768
    },
    {
      "epoch": 0.0015271667135026827,
      "grad_norm": 9904.561979209378,
      "learning_rate": 1.5423230929483823e-07,
      "loss": 1.6803,
      "step": 420800
    },
    {
      "epoch": 0.0015272828478535192,
      "grad_norm": 12881.789627221833,
      "learning_rate": 1.5422643952228324e-07,
      "loss": 1.7016,
      "step": 420832
    },
    {
      "epoch": 0.001527398982204356,
      "grad_norm": 11190.905950815599,
      "learning_rate": 1.542205704198526e-07,
      "loss": 1.6848,
      "step": 420864
    },
    {
      "epoch": 0.0015275151165551927,
      "grad_norm": 11521.157580729465,
      "learning_rate": 1.5421470198741877e-07,
      "loss": 1.6867,
      "step": 420896
    },
    {
      "epoch": 0.0015276312509060295,
      "grad_norm": 9233.015108836333,
      "learning_rate": 1.542088342248543e-07,
      "loss": 1.7066,
      "step": 420928
    },
    {
      "epoch": 0.001527747385256866,
      "grad_norm": 10486.892390026704,
      "learning_rate": 1.5420296713203177e-07,
      "loss": 1.7096,
      "step": 420960
    },
    {
      "epoch": 0.0015278635196077027,
      "grad_norm": 11282.73938367806,
      "learning_rate": 1.541971007088238e-07,
      "loss": 1.7079,
      "step": 420992
    },
    {
      "epoch": 0.0015279796539585395,
      "grad_norm": 11255.9237737291,
      "learning_rate": 1.541914182497742e-07,
      "loss": 1.6996,
      "step": 421024
    },
    {
      "epoch": 0.0015280957883093763,
      "grad_norm": 10545.709459301446,
      "learning_rate": 1.5418555314449767e-07,
      "loss": 1.6546,
      "step": 421056
    },
    {
      "epoch": 0.001528211922660213,
      "grad_norm": 10217.092149922111,
      "learning_rate": 1.5417968870845767e-07,
      "loss": 1.656,
      "step": 421088
    },
    {
      "epoch": 0.0015283280570110495,
      "grad_norm": 9197.851488255286,
      "learning_rate": 1.5417382494152692e-07,
      "loss": 1.6726,
      "step": 421120
    },
    {
      "epoch": 0.0015284441913618863,
      "grad_norm": 11036.194996465041,
      "learning_rate": 1.5416796184357822e-07,
      "loss": 1.666,
      "step": 421152
    },
    {
      "epoch": 0.001528560325712723,
      "grad_norm": 8026.894044398493,
      "learning_rate": 1.5416209941448437e-07,
      "loss": 1.6541,
      "step": 421184
    },
    {
      "epoch": 0.0015286764600635598,
      "grad_norm": 11887.711974976513,
      "learning_rate": 1.541562376541182e-07,
      "loss": 1.6755,
      "step": 421216
    },
    {
      "epoch": 0.0015287925944143963,
      "grad_norm": 11273.384318828132,
      "learning_rate": 1.5415037656235256e-07,
      "loss": 1.6852,
      "step": 421248
    },
    {
      "epoch": 0.001528908728765233,
      "grad_norm": 9392.049084198825,
      "learning_rate": 1.5414451613906044e-07,
      "loss": 1.6917,
      "step": 421280
    },
    {
      "epoch": 0.0015290248631160699,
      "grad_norm": 12214.834751235892,
      "learning_rate": 1.541386563841147e-07,
      "loss": 1.6997,
      "step": 421312
    },
    {
      "epoch": 0.0015291409974669066,
      "grad_norm": 13625.29236383572,
      "learning_rate": 1.5413279729738832e-07,
      "loss": 1.6677,
      "step": 421344
    },
    {
      "epoch": 0.0015292571318177434,
      "grad_norm": 10880.580866847136,
      "learning_rate": 1.5412693887875433e-07,
      "loss": 1.673,
      "step": 421376
    },
    {
      "epoch": 0.00152937326616858,
      "grad_norm": 10065.106258753556,
      "learning_rate": 1.541210811280858e-07,
      "loss": 1.6774,
      "step": 421408
    },
    {
      "epoch": 0.0015294894005194167,
      "grad_norm": 8820.53887242724,
      "learning_rate": 1.5411522404525572e-07,
      "loss": 1.6721,
      "step": 421440
    },
    {
      "epoch": 0.0015296055348702534,
      "grad_norm": 10367.715080961669,
      "learning_rate": 1.541093676301373e-07,
      "loss": 1.657,
      "step": 421472
    },
    {
      "epoch": 0.0015297216692210902,
      "grad_norm": 10995.92579094639,
      "learning_rate": 1.5410351188260363e-07,
      "loss": 1.6733,
      "step": 421504
    },
    {
      "epoch": 0.0015298378035719267,
      "grad_norm": 8605.114409466036,
      "learning_rate": 1.540976568025279e-07,
      "loss": 1.6943,
      "step": 421536
    },
    {
      "epoch": 0.0015299539379227635,
      "grad_norm": 9510.050367900267,
      "learning_rate": 1.5409180238978333e-07,
      "loss": 1.6918,
      "step": 421568
    },
    {
      "epoch": 0.0015300700722736002,
      "grad_norm": 10668.53860657588,
      "learning_rate": 1.5408594864424314e-07,
      "loss": 1.6907,
      "step": 421600
    },
    {
      "epoch": 0.001530186206624437,
      "grad_norm": 14698.645855996396,
      "learning_rate": 1.5408009556578062e-07,
      "loss": 1.6722,
      "step": 421632
    },
    {
      "epoch": 0.0015303023409752737,
      "grad_norm": 8932.767768166817,
      "learning_rate": 1.540742431542691e-07,
      "loss": 1.6782,
      "step": 421664
    },
    {
      "epoch": 0.0015304184753261103,
      "grad_norm": 9567.855559110412,
      "learning_rate": 1.5406839140958187e-07,
      "loss": 1.68,
      "step": 421696
    },
    {
      "epoch": 0.001530534609676947,
      "grad_norm": 8556.808517198453,
      "learning_rate": 1.5406254033159238e-07,
      "loss": 1.6933,
      "step": 421728
    },
    {
      "epoch": 0.0015306507440277838,
      "grad_norm": 14136.414679825997,
      "learning_rate": 1.5405668992017403e-07,
      "loss": 1.6847,
      "step": 421760
    },
    {
      "epoch": 0.0015307668783786205,
      "grad_norm": 13746.355589755418,
      "learning_rate": 1.5405084017520022e-07,
      "loss": 1.7066,
      "step": 421792
    },
    {
      "epoch": 0.001530883012729457,
      "grad_norm": 9654.413291339873,
      "learning_rate": 1.5404499109654444e-07,
      "loss": 1.7142,
      "step": 421824
    },
    {
      "epoch": 0.0015309991470802938,
      "grad_norm": 9167.19324548141,
      "learning_rate": 1.5403914268408025e-07,
      "loss": 1.7216,
      "step": 421856
    },
    {
      "epoch": 0.0015311152814311306,
      "grad_norm": 10360.247680436989,
      "learning_rate": 1.540332949376812e-07,
      "loss": 1.7294,
      "step": 421888
    },
    {
      "epoch": 0.0015312314157819673,
      "grad_norm": 10954.09850238713,
      "learning_rate": 1.540274478572208e-07,
      "loss": 1.7089,
      "step": 421920
    },
    {
      "epoch": 0.001531347550132804,
      "grad_norm": 14373.970502265545,
      "learning_rate": 1.5402160144257274e-07,
      "loss": 1.6727,
      "step": 421952
    },
    {
      "epoch": 0.0015314636844836406,
      "grad_norm": 9579.455934446381,
      "learning_rate": 1.5401575569361062e-07,
      "loss": 1.668,
      "step": 421984
    },
    {
      "epoch": 0.0015315798188344774,
      "grad_norm": 18938.009187874,
      "learning_rate": 1.5400991061020813e-07,
      "loss": 1.6869,
      "step": 422016
    },
    {
      "epoch": 0.0015316959531853141,
      "grad_norm": 8936.755339607324,
      "learning_rate": 1.540042488202293e-07,
      "loss": 1.6782,
      "step": 422048
    },
    {
      "epoch": 0.0015318120875361509,
      "grad_norm": 10314.137869933676,
      "learning_rate": 1.5399840504677836e-07,
      "loss": 1.6834,
      "step": 422080
    },
    {
      "epoch": 0.0015319282218869874,
      "grad_norm": 9090.054565292774,
      "learning_rate": 1.5399256193851223e-07,
      "loss": 1.6777,
      "step": 422112
    },
    {
      "epoch": 0.0015320443562378242,
      "grad_norm": 7385.959517896101,
      "learning_rate": 1.5398671949530474e-07,
      "loss": 1.6779,
      "step": 422144
    },
    {
      "epoch": 0.001532160490588661,
      "grad_norm": 9003.934029078622,
      "learning_rate": 1.5398087771702976e-07,
      "loss": 1.6849,
      "step": 422176
    },
    {
      "epoch": 0.0015322766249394977,
      "grad_norm": 9930.007854981788,
      "learning_rate": 1.5397503660356116e-07,
      "loss": 1.6717,
      "step": 422208
    },
    {
      "epoch": 0.0015323927592903344,
      "grad_norm": 8789.589068892812,
      "learning_rate": 1.5396919615477281e-07,
      "loss": 1.6438,
      "step": 422240
    },
    {
      "epoch": 0.001532508893641171,
      "grad_norm": 8118.688317702558,
      "learning_rate": 1.5396335637053875e-07,
      "loss": 1.6529,
      "step": 422272
    },
    {
      "epoch": 0.0015326250279920077,
      "grad_norm": 8886.02723380927,
      "learning_rate": 1.5395751725073287e-07,
      "loss": 1.6639,
      "step": 422304
    },
    {
      "epoch": 0.0015327411623428445,
      "grad_norm": 9379.1340751692,
      "learning_rate": 1.5395167879522926e-07,
      "loss": 1.6677,
      "step": 422336
    },
    {
      "epoch": 0.0015328572966936812,
      "grad_norm": 11011.947330059293,
      "learning_rate": 1.5394584100390194e-07,
      "loss": 1.6639,
      "step": 422368
    },
    {
      "epoch": 0.0015329734310445178,
      "grad_norm": 8757.922927269914,
      "learning_rate": 1.53940003876625e-07,
      "loss": 1.6794,
      "step": 422400
    },
    {
      "epoch": 0.0015330895653953545,
      "grad_norm": 10638.285952163535,
      "learning_rate": 1.5393416741327254e-07,
      "loss": 1.6946,
      "step": 422432
    },
    {
      "epoch": 0.0015332056997461913,
      "grad_norm": 9851.885301808989,
      "learning_rate": 1.5392833161371868e-07,
      "loss": 1.6927,
      "step": 422464
    },
    {
      "epoch": 0.001533321834097028,
      "grad_norm": 8983.190079253583,
      "learning_rate": 1.539224964778377e-07,
      "loss": 1.6738,
      "step": 422496
    },
    {
      "epoch": 0.0015334379684478648,
      "grad_norm": 12406.660146872728,
      "learning_rate": 1.5391666200550373e-07,
      "loss": 1.6636,
      "step": 422528
    },
    {
      "epoch": 0.0015335541027987013,
      "grad_norm": 9836.050020206281,
      "learning_rate": 1.5391082819659102e-07,
      "loss": 1.6681,
      "step": 422560
    },
    {
      "epoch": 0.001533670237149538,
      "grad_norm": 11390.635627567059,
      "learning_rate": 1.5390499505097392e-07,
      "loss": 1.6706,
      "step": 422592
    },
    {
      "epoch": 0.0015337863715003748,
      "grad_norm": 8923.297036409804,
      "learning_rate": 1.5389916256852671e-07,
      "loss": 1.6984,
      "step": 422624
    },
    {
      "epoch": 0.0015339025058512116,
      "grad_norm": 12340.60687324574,
      "learning_rate": 1.5389333074912371e-07,
      "loss": 1.6767,
      "step": 422656
    },
    {
      "epoch": 0.0015340186402020481,
      "grad_norm": 8935.51520618705,
      "learning_rate": 1.5388749959263938e-07,
      "loss": 1.6871,
      "step": 422688
    },
    {
      "epoch": 0.0015341347745528849,
      "grad_norm": 13070.326698288762,
      "learning_rate": 1.5388166909894802e-07,
      "loss": 1.6923,
      "step": 422720
    },
    {
      "epoch": 0.0015342509089037216,
      "grad_norm": 11509.969939144063,
      "learning_rate": 1.538758392679242e-07,
      "loss": 1.6928,
      "step": 422752
    },
    {
      "epoch": 0.0015343670432545584,
      "grad_norm": 9889.54508559418,
      "learning_rate": 1.5387001009944233e-07,
      "loss": 1.6883,
      "step": 422784
    },
    {
      "epoch": 0.0015344831776053951,
      "grad_norm": 9349.212052360348,
      "learning_rate": 1.5386418159337694e-07,
      "loss": 1.6578,
      "step": 422816
    },
    {
      "epoch": 0.0015345993119562317,
      "grad_norm": 8751.086903922278,
      "learning_rate": 1.538583537496026e-07,
      "loss": 1.6533,
      "step": 422848
    },
    {
      "epoch": 0.0015347154463070684,
      "grad_norm": 9552.072026529113,
      "learning_rate": 1.5385252656799387e-07,
      "loss": 1.6625,
      "step": 422880
    },
    {
      "epoch": 0.0015348315806579052,
      "grad_norm": 20146.29057667937,
      "learning_rate": 1.538467000484254e-07,
      "loss": 1.6801,
      "step": 422912
    },
    {
      "epoch": 0.001534947715008742,
      "grad_norm": 11101.60312747668,
      "learning_rate": 1.5384087419077178e-07,
      "loss": 1.6771,
      "step": 422944
    },
    {
      "epoch": 0.0015350638493595785,
      "grad_norm": 11506.158264164455,
      "learning_rate": 1.5383504899490776e-07,
      "loss": 1.6798,
      "step": 422976
    },
    {
      "epoch": 0.0015351799837104152,
      "grad_norm": 10315.884353752712,
      "learning_rate": 1.5382922446070804e-07,
      "loss": 1.6985,
      "step": 423008
    },
    {
      "epoch": 0.001535296118061252,
      "grad_norm": 28213.238594673956,
      "learning_rate": 1.538234005880473e-07,
      "loss": 1.6914,
      "step": 423040
    },
    {
      "epoch": 0.0015354122524120887,
      "grad_norm": 19165.105374090694,
      "learning_rate": 1.538175773768004e-07,
      "loss": 1.6739,
      "step": 423072
    },
    {
      "epoch": 0.0015355283867629255,
      "grad_norm": 13392.593326163531,
      "learning_rate": 1.5381193677151983e-07,
      "loss": 1.6677,
      "step": 423104
    },
    {
      "epoch": 0.001535644521113762,
      "grad_norm": 9285.388306366083,
      "learning_rate": 1.5380611486206557e-07,
      "loss": 1.6494,
      "step": 423136
    },
    {
      "epoch": 0.0015357606554645988,
      "grad_norm": 9357.109382710027,
      "learning_rate": 1.5380029361365358e-07,
      "loss": 1.6575,
      "step": 423168
    },
    {
      "epoch": 0.0015358767898154355,
      "grad_norm": 12232.583373923924,
      "learning_rate": 1.537944730261588e-07,
      "loss": 1.6692,
      "step": 423200
    },
    {
      "epoch": 0.0015359929241662723,
      "grad_norm": 10427.801877672973,
      "learning_rate": 1.5378865309945613e-07,
      "loss": 1.6652,
      "step": 423232
    },
    {
      "epoch": 0.0015361090585171088,
      "grad_norm": 10374.584329022537,
      "learning_rate": 1.537828338334206e-07,
      "loss": 1.6477,
      "step": 423264
    },
    {
      "epoch": 0.0015362251928679456,
      "grad_norm": 11911.580667568851,
      "learning_rate": 1.5377701522792716e-07,
      "loss": 1.6673,
      "step": 423296
    },
    {
      "epoch": 0.0015363413272187823,
      "grad_norm": 9578.472320782683,
      "learning_rate": 1.537711972828509e-07,
      "loss": 1.6704,
      "step": 423328
    },
    {
      "epoch": 0.001536457461569619,
      "grad_norm": 10973.672858254888,
      "learning_rate": 1.5376537999806691e-07,
      "loss": 1.673,
      "step": 423360
    },
    {
      "epoch": 0.0015365735959204558,
      "grad_norm": 9759.37129122568,
      "learning_rate": 1.5375956337345029e-07,
      "loss": 1.6745,
      "step": 423392
    },
    {
      "epoch": 0.0015366897302712924,
      "grad_norm": 12332.047032021894,
      "learning_rate": 1.5375374740887614e-07,
      "loss": 1.6594,
      "step": 423424
    },
    {
      "epoch": 0.0015368058646221291,
      "grad_norm": 9617.272170423379,
      "learning_rate": 1.537479321042197e-07,
      "loss": 1.6712,
      "step": 423456
    },
    {
      "epoch": 0.0015369219989729659,
      "grad_norm": 11323.970858316441,
      "learning_rate": 1.5374211745935616e-07,
      "loss": 1.6849,
      "step": 423488
    },
    {
      "epoch": 0.0015370381333238026,
      "grad_norm": 9681.43832289397,
      "learning_rate": 1.537363034741608e-07,
      "loss": 1.7055,
      "step": 423520
    },
    {
      "epoch": 0.0015371542676746392,
      "grad_norm": 8800.799509135519,
      "learning_rate": 1.537304901485088e-07,
      "loss": 1.695,
      "step": 423552
    },
    {
      "epoch": 0.001537270402025476,
      "grad_norm": 11467.237505170982,
      "learning_rate": 1.5372467748227557e-07,
      "loss": 1.6941,
      "step": 423584
    },
    {
      "epoch": 0.0015373865363763127,
      "grad_norm": 11111.880128943078,
      "learning_rate": 1.5371886547533643e-07,
      "loss": 1.7026,
      "step": 423616
    },
    {
      "epoch": 0.0015375026707271494,
      "grad_norm": 10125.80623950508,
      "learning_rate": 1.5371305412756674e-07,
      "loss": 1.6971,
      "step": 423648
    },
    {
      "epoch": 0.0015376188050779862,
      "grad_norm": 10861.005478315532,
      "learning_rate": 1.537072434388419e-07,
      "loss": 1.6763,
      "step": 423680
    },
    {
      "epoch": 0.0015377349394288227,
      "grad_norm": 11656.604651441174,
      "learning_rate": 1.5370143340903734e-07,
      "loss": 1.671,
      "step": 423712
    },
    {
      "epoch": 0.0015378510737796595,
      "grad_norm": 11309.32199559284,
      "learning_rate": 1.5369562403802862e-07,
      "loss": 1.6517,
      "step": 423744
    },
    {
      "epoch": 0.0015379672081304962,
      "grad_norm": 9787.6452735068,
      "learning_rate": 1.5368981532569114e-07,
      "loss": 1.6579,
      "step": 423776
    },
    {
      "epoch": 0.001538083342481333,
      "grad_norm": 10098.147057752723,
      "learning_rate": 1.5368400727190055e-07,
      "loss": 1.6726,
      "step": 423808
    },
    {
      "epoch": 0.0015381994768321695,
      "grad_norm": 11906.532660686738,
      "learning_rate": 1.5367819987653231e-07,
      "loss": 1.6516,
      "step": 423840
    },
    {
      "epoch": 0.0015383156111830063,
      "grad_norm": 9658.958743053001,
      "learning_rate": 1.5367239313946212e-07,
      "loss": 1.6643,
      "step": 423872
    },
    {
      "epoch": 0.001538431745533843,
      "grad_norm": 9599.156837972801,
      "learning_rate": 1.5366658706056556e-07,
      "loss": 1.6879,
      "step": 423904
    },
    {
      "epoch": 0.0015385478798846798,
      "grad_norm": 8675.864683131014,
      "learning_rate": 1.5366078163971834e-07,
      "loss": 1.6895,
      "step": 423936
    },
    {
      "epoch": 0.0015386640142355165,
      "grad_norm": 9730.217058216122,
      "learning_rate": 1.5365497687679615e-07,
      "loss": 1.6725,
      "step": 423968
    },
    {
      "epoch": 0.001538780148586353,
      "grad_norm": 8109.505410319423,
      "learning_rate": 1.5364917277167477e-07,
      "loss": 1.6753,
      "step": 424000
    },
    {
      "epoch": 0.0015388962829371898,
      "grad_norm": 10632.419574113881,
      "learning_rate": 1.536433693242299e-07,
      "loss": 1.672,
      "step": 424032
    },
    {
      "epoch": 0.0015390124172880266,
      "grad_norm": 10575.12250520059,
      "learning_rate": 1.536375665343374e-07,
      "loss": 1.6749,
      "step": 424064
    },
    {
      "epoch": 0.0015391285516388633,
      "grad_norm": 18566.425611840314,
      "learning_rate": 1.5363176440187306e-07,
      "loss": 1.6988,
      "step": 424096
    },
    {
      "epoch": 0.0015392446859896999,
      "grad_norm": 9146.168159398776,
      "learning_rate": 1.5362614421286335e-07,
      "loss": 1.6644,
      "step": 424128
    },
    {
      "epoch": 0.0015393608203405366,
      "grad_norm": 9149.58031824411,
      "learning_rate": 1.5362034337434806e-07,
      "loss": 1.6589,
      "step": 424160
    },
    {
      "epoch": 0.0015394769546913734,
      "grad_norm": 10136.404096127975,
      "learning_rate": 1.536145431928925e-07,
      "loss": 1.6797,
      "step": 424192
    },
    {
      "epoch": 0.0015395930890422101,
      "grad_norm": 8614.960707977722,
      "learning_rate": 1.5360874366837274e-07,
      "loss": 1.6751,
      "step": 424224
    },
    {
      "epoch": 0.001539709223393047,
      "grad_norm": 8904.990286350681,
      "learning_rate": 1.5360294480066468e-07,
      "loss": 1.6828,
      "step": 424256
    },
    {
      "epoch": 0.0015398253577438834,
      "grad_norm": 9435.420287406385,
      "learning_rate": 1.535971465896444e-07,
      "loss": 1.6765,
      "step": 424288
    },
    {
      "epoch": 0.0015399414920947202,
      "grad_norm": 8778.927497137676,
      "learning_rate": 1.5359134903518798e-07,
      "loss": 1.6596,
      "step": 424320
    },
    {
      "epoch": 0.001540057626445557,
      "grad_norm": 7649.07798365267,
      "learning_rate": 1.535855521371715e-07,
      "loss": 1.6675,
      "step": 424352
    },
    {
      "epoch": 0.0015401737607963937,
      "grad_norm": 9385.462162301865,
      "learning_rate": 1.5357975589547107e-07,
      "loss": 1.6756,
      "step": 424384
    },
    {
      "epoch": 0.0015402898951472302,
      "grad_norm": 7853.675190635274,
      "learning_rate": 1.5357396030996288e-07,
      "loss": 1.6937,
      "step": 424416
    },
    {
      "epoch": 0.001540406029498067,
      "grad_norm": 8005.805893225241,
      "learning_rate": 1.5356816538052312e-07,
      "loss": 1.6765,
      "step": 424448
    },
    {
      "epoch": 0.0015405221638489037,
      "grad_norm": 8946.486237624244,
      "learning_rate": 1.5356237110702802e-07,
      "loss": 1.6938,
      "step": 424480
    },
    {
      "epoch": 0.0015406382981997405,
      "grad_norm": 8822.214914634533,
      "learning_rate": 1.5355657748935383e-07,
      "loss": 1.7032,
      "step": 424512
    },
    {
      "epoch": 0.0015407544325505773,
      "grad_norm": 15565.8047013317,
      "learning_rate": 1.535507845273769e-07,
      "loss": 1.6943,
      "step": 424544
    },
    {
      "epoch": 0.0015408705669014138,
      "grad_norm": 8743.075660201048,
      "learning_rate": 1.535449922209735e-07,
      "loss": 1.6843,
      "step": 424576
    },
    {
      "epoch": 0.0015409867012522505,
      "grad_norm": 11018.51968278861,
      "learning_rate": 1.5353920057001997e-07,
      "loss": 1.6758,
      "step": 424608
    },
    {
      "epoch": 0.0015411028356030873,
      "grad_norm": 10321.308444184779,
      "learning_rate": 1.535334095743928e-07,
      "loss": 1.6748,
      "step": 424640
    },
    {
      "epoch": 0.001541218969953924,
      "grad_norm": 9457.964262990214,
      "learning_rate": 1.535276192339683e-07,
      "loss": 1.658,
      "step": 424672
    },
    {
      "epoch": 0.0015413351043047606,
      "grad_norm": 15231.703319064485,
      "learning_rate": 1.53521829548623e-07,
      "loss": 1.6728,
      "step": 424704
    },
    {
      "epoch": 0.0015414512386555973,
      "grad_norm": 10072.627264026005,
      "learning_rate": 1.535160405182334e-07,
      "loss": 1.6546,
      "step": 424736
    },
    {
      "epoch": 0.001541567373006434,
      "grad_norm": 8232.121354790635,
      "learning_rate": 1.5351025214267597e-07,
      "loss": 1.6772,
      "step": 424768
    },
    {
      "epoch": 0.0015416835073572709,
      "grad_norm": 9689.675329958172,
      "learning_rate": 1.535044644218273e-07,
      "loss": 1.7027,
      "step": 424800
    },
    {
      "epoch": 0.0015417996417081076,
      "grad_norm": 13290.753327031542,
      "learning_rate": 1.5349867735556395e-07,
      "loss": 1.6748,
      "step": 424832
    },
    {
      "epoch": 0.0015419157760589441,
      "grad_norm": 9818.942916628042,
      "learning_rate": 1.5349289094376258e-07,
      "loss": 1.6743,
      "step": 424864
    },
    {
      "epoch": 0.001542031910409781,
      "grad_norm": 8070.367773528044,
      "learning_rate": 1.5348710518629983e-07,
      "loss": 1.667,
      "step": 424896
    },
    {
      "epoch": 0.0015421480447606177,
      "grad_norm": 8434.363757865794,
      "learning_rate": 1.5348132008305237e-07,
      "loss": 1.656,
      "step": 424928
    },
    {
      "epoch": 0.0015422641791114544,
      "grad_norm": 10062.127508633548,
      "learning_rate": 1.5347553563389696e-07,
      "loss": 1.665,
      "step": 424960
    },
    {
      "epoch": 0.001542380313462291,
      "grad_norm": 8756.88163674718,
      "learning_rate": 1.5346975183871027e-07,
      "loss": 1.6815,
      "step": 424992
    },
    {
      "epoch": 0.0015424964478131277,
      "grad_norm": 10887.6061648096,
      "learning_rate": 1.5346396869736918e-07,
      "loss": 1.671,
      "step": 425024
    },
    {
      "epoch": 0.0015426125821639645,
      "grad_norm": 8889.160815285097,
      "learning_rate": 1.5345818620975043e-07,
      "loss": 1.6777,
      "step": 425056
    },
    {
      "epoch": 0.0015427287165148012,
      "grad_norm": 10419.426663689323,
      "learning_rate": 1.5345240437573095e-07,
      "loss": 1.6942,
      "step": 425088
    },
    {
      "epoch": 0.001542844850865638,
      "grad_norm": 23619.623028321177,
      "learning_rate": 1.5344662319518754e-07,
      "loss": 1.6961,
      "step": 425120
    },
    {
      "epoch": 0.0015429609852164745,
      "grad_norm": 19013.7078971988,
      "learning_rate": 1.5344084266799714e-07,
      "loss": 1.6914,
      "step": 425152
    },
    {
      "epoch": 0.0015430771195673113,
      "grad_norm": 9219.465928132713,
      "learning_rate": 1.5343524340521144e-07,
      "loss": 1.6909,
      "step": 425184
    },
    {
      "epoch": 0.001543193253918148,
      "grad_norm": 13006.640611626048,
      "learning_rate": 1.5342946416395022e-07,
      "loss": 1.6551,
      "step": 425216
    },
    {
      "epoch": 0.0015433093882689848,
      "grad_norm": 9530.866487366193,
      "learning_rate": 1.534236855756768e-07,
      "loss": 1.6732,
      "step": 425248
    },
    {
      "epoch": 0.0015434255226198213,
      "grad_norm": 10425.15400365865,
      "learning_rate": 1.534179076402682e-07,
      "loss": 1.6834,
      "step": 425280
    },
    {
      "epoch": 0.001543541656970658,
      "grad_norm": 9027.55083065169,
      "learning_rate": 1.534121303576015e-07,
      "loss": 1.6949,
      "step": 425312
    },
    {
      "epoch": 0.0015436577913214948,
      "grad_norm": 10788.682032574694,
      "learning_rate": 1.5340635372755384e-07,
      "loss": 1.6889,
      "step": 425344
    },
    {
      "epoch": 0.0015437739256723316,
      "grad_norm": 10085.674890655559,
      "learning_rate": 1.5340057775000234e-07,
      "loss": 1.6842,
      "step": 425376
    },
    {
      "epoch": 0.0015438900600231683,
      "grad_norm": 8964.334219561428,
      "learning_rate": 1.5339480242482416e-07,
      "loss": 1.6977,
      "step": 425408
    },
    {
      "epoch": 0.0015440061943740048,
      "grad_norm": 7708.187205822132,
      "learning_rate": 1.5338902775189653e-07,
      "loss": 1.6754,
      "step": 425440
    },
    {
      "epoch": 0.0015441223287248416,
      "grad_norm": 10583.764925582956,
      "learning_rate": 1.5338325373109667e-07,
      "loss": 1.6704,
      "step": 425472
    },
    {
      "epoch": 0.0015442384630756784,
      "grad_norm": 8618.82683432032,
      "learning_rate": 1.5337748036230186e-07,
      "loss": 1.6537,
      "step": 425504
    },
    {
      "epoch": 0.0015443545974265151,
      "grad_norm": 8611.690542512546,
      "learning_rate": 1.5337170764538937e-07,
      "loss": 1.6574,
      "step": 425536
    },
    {
      "epoch": 0.0015444707317773516,
      "grad_norm": 10044.3791246647,
      "learning_rate": 1.5336593558023657e-07,
      "loss": 1.664,
      "step": 425568
    },
    {
      "epoch": 0.0015445868661281884,
      "grad_norm": 10108.940597312856,
      "learning_rate": 1.533601641667208e-07,
      "loss": 1.6696,
      "step": 425600
    },
    {
      "epoch": 0.0015447030004790252,
      "grad_norm": 12880.883044263697,
      "learning_rate": 1.5335439340471947e-07,
      "loss": 1.6664,
      "step": 425632
    },
    {
      "epoch": 0.001544819134829862,
      "grad_norm": 8764.468723202794,
      "learning_rate": 1.5334862329411002e-07,
      "loss": 1.6983,
      "step": 425664
    },
    {
      "epoch": 0.0015449352691806987,
      "grad_norm": 13659.987847725195,
      "learning_rate": 1.5334285383476992e-07,
      "loss": 1.709,
      "step": 425696
    },
    {
      "epoch": 0.0015450514035315352,
      "grad_norm": 10003.165898854222,
      "learning_rate": 1.533370850265766e-07,
      "loss": 1.6819,
      "step": 425728
    },
    {
      "epoch": 0.001545167537882372,
      "grad_norm": 8729.763914333536,
      "learning_rate": 1.5333131686940766e-07,
      "loss": 1.6732,
      "step": 425760
    },
    {
      "epoch": 0.0015452836722332087,
      "grad_norm": 9482.232648485271,
      "learning_rate": 1.5332554936314064e-07,
      "loss": 1.6613,
      "step": 425792
    },
    {
      "epoch": 0.0015453998065840455,
      "grad_norm": 7880.60936730149,
      "learning_rate": 1.5331978250765313e-07,
      "loss": 1.6579,
      "step": 425824
    },
    {
      "epoch": 0.001545515940934882,
      "grad_norm": 8082.260574863941,
      "learning_rate": 1.5331401630282275e-07,
      "loss": 1.6775,
      "step": 425856
    },
    {
      "epoch": 0.0015456320752857188,
      "grad_norm": 8996.298572190677,
      "learning_rate": 1.5330825074852714e-07,
      "loss": 1.6892,
      "step": 425888
    },
    {
      "epoch": 0.0015457482096365555,
      "grad_norm": 11646.997724735762,
      "learning_rate": 1.5330248584464401e-07,
      "loss": 1.6652,
      "step": 425920
    },
    {
      "epoch": 0.0015458643439873923,
      "grad_norm": 10759.446825929295,
      "learning_rate": 1.5329672159105111e-07,
      "loss": 1.6605,
      "step": 425952
    },
    {
      "epoch": 0.001545980478338229,
      "grad_norm": 9312.235070057028,
      "learning_rate": 1.5329095798762612e-07,
      "loss": 1.6688,
      "step": 425984
    },
    {
      "epoch": 0.0015460966126890656,
      "grad_norm": 8921.526775165785,
      "learning_rate": 1.532851950342469e-07,
      "loss": 1.6764,
      "step": 426016
    },
    {
      "epoch": 0.0015462127470399023,
      "grad_norm": 10278.42127955456,
      "learning_rate": 1.532794327307912e-07,
      "loss": 1.6757,
      "step": 426048
    },
    {
      "epoch": 0.001546328881390739,
      "grad_norm": 9636.960827978912,
      "learning_rate": 1.5327367107713693e-07,
      "loss": 1.6736,
      "step": 426080
    },
    {
      "epoch": 0.0015464450157415758,
      "grad_norm": 8717.417278070381,
      "learning_rate": 1.5326791007316193e-07,
      "loss": 1.6588,
      "step": 426112
    },
    {
      "epoch": 0.0015465611500924124,
      "grad_norm": 10825.785329480721,
      "learning_rate": 1.5326214971874414e-07,
      "loss": 1.6794,
      "step": 426144
    },
    {
      "epoch": 0.0015466772844432491,
      "grad_norm": 9439.230265228198,
      "learning_rate": 1.5325656999471307e-07,
      "loss": 1.7039,
      "step": 426176
    },
    {
      "epoch": 0.0015467934187940859,
      "grad_norm": 16490.82763235369,
      "learning_rate": 1.5325081091875438e-07,
      "loss": 1.708,
      "step": 426208
    },
    {
      "epoch": 0.0015469095531449226,
      "grad_norm": 9624.118245325128,
      "learning_rate": 1.532450524919906e-07,
      "loss": 1.6999,
      "step": 426240
    },
    {
      "epoch": 0.0015470256874957594,
      "grad_norm": 11142.744006751658,
      "learning_rate": 1.5323929471429978e-07,
      "loss": 1.7042,
      "step": 426272
    },
    {
      "epoch": 0.001547141821846596,
      "grad_norm": 9377.84666114775,
      "learning_rate": 1.5323353758556004e-07,
      "loss": 1.6775,
      "step": 426304
    },
    {
      "epoch": 0.0015472579561974327,
      "grad_norm": 9278.578986030134,
      "learning_rate": 1.5322778110564947e-07,
      "loss": 1.6758,
      "step": 426336
    },
    {
      "epoch": 0.0015473740905482694,
      "grad_norm": 10893.174927448837,
      "learning_rate": 1.532220252744462e-07,
      "loss": 1.6787,
      "step": 426368
    },
    {
      "epoch": 0.0015474902248991062,
      "grad_norm": 7831.897854287938,
      "learning_rate": 1.532162700918284e-07,
      "loss": 1.6611,
      "step": 426400
    },
    {
      "epoch": 0.0015476063592499427,
      "grad_norm": 10158.972192106836,
      "learning_rate": 1.5321051555767426e-07,
      "loss": 1.6741,
      "step": 426432
    },
    {
      "epoch": 0.0015477224936007795,
      "grad_norm": 10865.681939022512,
      "learning_rate": 1.5320476167186202e-07,
      "loss": 1.675,
      "step": 426464
    },
    {
      "epoch": 0.0015478386279516162,
      "grad_norm": 9812.849535175805,
      "learning_rate": 1.5319900843426995e-07,
      "loss": 1.6829,
      "step": 426496
    },
    {
      "epoch": 0.001547954762302453,
      "grad_norm": 8557.23483375325,
      "learning_rate": 1.5319325584477633e-07,
      "loss": 1.6816,
      "step": 426528
    },
    {
      "epoch": 0.0015480708966532897,
      "grad_norm": 10813.069869375671,
      "learning_rate": 1.5318750390325956e-07,
      "loss": 1.6974,
      "step": 426560
    },
    {
      "epoch": 0.0015481870310041263,
      "grad_norm": 9585.522729616783,
      "learning_rate": 1.531817526095979e-07,
      "loss": 1.6836,
      "step": 426592
    },
    {
      "epoch": 0.001548303165354963,
      "grad_norm": 9905.096264045089,
      "learning_rate": 1.531760019636698e-07,
      "loss": 1.6837,
      "step": 426624
    },
    {
      "epoch": 0.0015484192997057998,
      "grad_norm": 9328.102915384243,
      "learning_rate": 1.5317025196535368e-07,
      "loss": 1.6892,
      "step": 426656
    },
    {
      "epoch": 0.0015485354340566365,
      "grad_norm": 8132.419689120821,
      "learning_rate": 1.53164502614528e-07,
      "loss": 1.6709,
      "step": 426688
    },
    {
      "epoch": 0.001548651568407473,
      "grad_norm": 11166.786646121613,
      "learning_rate": 1.5315875391107124e-07,
      "loss": 1.6823,
      "step": 426720
    },
    {
      "epoch": 0.0015487677027583098,
      "grad_norm": 8261.044728120045,
      "learning_rate": 1.531530058548619e-07,
      "loss": 1.698,
      "step": 426752
    },
    {
      "epoch": 0.0015488838371091466,
      "grad_norm": 10104.406365541718,
      "learning_rate": 1.5314725844577856e-07,
      "loss": 1.707,
      "step": 426784
    },
    {
      "epoch": 0.0015489999714599833,
      "grad_norm": 10342.179654212163,
      "learning_rate": 1.5314151168369983e-07,
      "loss": 1.6783,
      "step": 426816
    },
    {
      "epoch": 0.00154911610581082,
      "grad_norm": 11325.15236100601,
      "learning_rate": 1.5313576556850426e-07,
      "loss": 1.6784,
      "step": 426848
    },
    {
      "epoch": 0.0015492322401616566,
      "grad_norm": 8494.578506318016,
      "learning_rate": 1.5313002010007058e-07,
      "loss": 1.6916,
      "step": 426880
    },
    {
      "epoch": 0.0015493483745124934,
      "grad_norm": 8354.440974715184,
      "learning_rate": 1.5312427527827738e-07,
      "loss": 1.6894,
      "step": 426912
    },
    {
      "epoch": 0.0015494645088633301,
      "grad_norm": 8087.429505102347,
      "learning_rate": 1.5311853110300347e-07,
      "loss": 1.6988,
      "step": 426944
    },
    {
      "epoch": 0.0015495806432141669,
      "grad_norm": 9860.897525073466,
      "learning_rate": 1.531127875741275e-07,
      "loss": 1.6875,
      "step": 426976
    },
    {
      "epoch": 0.0015496967775650034,
      "grad_norm": 10141.10289860033,
      "learning_rate": 1.5310704469152828e-07,
      "loss": 1.6833,
      "step": 427008
    },
    {
      "epoch": 0.0015498129119158402,
      "grad_norm": 10641.790074982686,
      "learning_rate": 1.5310130245508468e-07,
      "loss": 1.6922,
      "step": 427040
    },
    {
      "epoch": 0.001549929046266677,
      "grad_norm": 8872.539208141037,
      "learning_rate": 1.5309556086467545e-07,
      "loss": 1.6892,
      "step": 427072
    },
    {
      "epoch": 0.0015500451806175137,
      "grad_norm": 9225.159944412888,
      "learning_rate": 1.5308981992017952e-07,
      "loss": 1.6765,
      "step": 427104
    },
    {
      "epoch": 0.0015501613149683504,
      "grad_norm": 9762.156114301799,
      "learning_rate": 1.5308407962147574e-07,
      "loss": 1.6853,
      "step": 427136
    },
    {
      "epoch": 0.001550277449319187,
      "grad_norm": 11782.391777563671,
      "learning_rate": 1.530783399684431e-07,
      "loss": 1.6844,
      "step": 427168
    },
    {
      "epoch": 0.0015503935836700237,
      "grad_norm": 12276.70151139955,
      "learning_rate": 1.5307278029517405e-07,
      "loss": 1.6679,
      "step": 427200
    },
    {
      "epoch": 0.0015505097180208605,
      "grad_norm": 9323.473172589707,
      "learning_rate": 1.5306704191295275e-07,
      "loss": 1.6713,
      "step": 427232
    },
    {
      "epoch": 0.0015506258523716972,
      "grad_norm": 8631.344738799395,
      "learning_rate": 1.5306130417604332e-07,
      "loss": 1.6736,
      "step": 427264
    },
    {
      "epoch": 0.0015507419867225338,
      "grad_norm": 8292.308604966413,
      "learning_rate": 1.5305556708432486e-07,
      "loss": 1.6734,
      "step": 427296
    },
    {
      "epoch": 0.0015508581210733705,
      "grad_norm": 12997.73611056941,
      "learning_rate": 1.5304983063767645e-07,
      "loss": 1.6768,
      "step": 427328
    },
    {
      "epoch": 0.0015509742554242073,
      "grad_norm": 10869.776630639657,
      "learning_rate": 1.530440948359772e-07,
      "loss": 1.6781,
      "step": 427360
    },
    {
      "epoch": 0.001551090389775044,
      "grad_norm": 9537.898510678335,
      "learning_rate": 1.5303835967910629e-07,
      "loss": 1.6746,
      "step": 427392
    },
    {
      "epoch": 0.0015512065241258808,
      "grad_norm": 8846.18245346545,
      "learning_rate": 1.530326251669429e-07,
      "loss": 1.67,
      "step": 427424
    },
    {
      "epoch": 0.0015513226584767173,
      "grad_norm": 11463.464223348892,
      "learning_rate": 1.5302689129936623e-07,
      "loss": 1.671,
      "step": 427456
    },
    {
      "epoch": 0.001551438792827554,
      "grad_norm": 9267.439667998924,
      "learning_rate": 1.530211580762555e-07,
      "loss": 1.6846,
      "step": 427488
    },
    {
      "epoch": 0.0015515549271783908,
      "grad_norm": 10333.10640611041,
      "learning_rate": 1.5301542549749011e-07,
      "loss": 1.6839,
      "step": 427520
    },
    {
      "epoch": 0.0015516710615292276,
      "grad_norm": 9582.39792536294,
      "learning_rate": 1.5300969356294926e-07,
      "loss": 1.6725,
      "step": 427552
    },
    {
      "epoch": 0.0015517871958800641,
      "grad_norm": 9120.741417231386,
      "learning_rate": 1.5300396227251237e-07,
      "loss": 1.6483,
      "step": 427584
    },
    {
      "epoch": 0.0015519033302309009,
      "grad_norm": 8865.746443475586,
      "learning_rate": 1.5299823162605878e-07,
      "loss": 1.6587,
      "step": 427616
    },
    {
      "epoch": 0.0015520194645817376,
      "grad_norm": 9651.384874721347,
      "learning_rate": 1.529925016234679e-07,
      "loss": 1.6585,
      "step": 427648
    },
    {
      "epoch": 0.0015521355989325744,
      "grad_norm": 17414.273455990064,
      "learning_rate": 1.529867722646192e-07,
      "loss": 1.6767,
      "step": 427680
    },
    {
      "epoch": 0.0015522517332834111,
      "grad_norm": 10356.505395161053,
      "learning_rate": 1.529810435493921e-07,
      "loss": 1.6522,
      "step": 427712
    },
    {
      "epoch": 0.0015523678676342477,
      "grad_norm": 12899.039499125507,
      "learning_rate": 1.5297531547766613e-07,
      "loss": 1.6655,
      "step": 427744
    },
    {
      "epoch": 0.0015524840019850844,
      "grad_norm": 11435.974991228339,
      "learning_rate": 1.5296958804932084e-07,
      "loss": 1.6832,
      "step": 427776
    },
    {
      "epoch": 0.0015526001363359212,
      "grad_norm": 9431.357484476983,
      "learning_rate": 1.5296386126423578e-07,
      "loss": 1.6815,
      "step": 427808
    },
    {
      "epoch": 0.001552716270686758,
      "grad_norm": 9459.79534662352,
      "learning_rate": 1.529581351222906e-07,
      "loss": 1.6985,
      "step": 427840
    },
    {
      "epoch": 0.0015528324050375945,
      "grad_norm": 9558.975468113724,
      "learning_rate": 1.5295240962336485e-07,
      "loss": 1.6914,
      "step": 427872
    },
    {
      "epoch": 0.0015529485393884312,
      "grad_norm": 11279.70513799009,
      "learning_rate": 1.5294668476733826e-07,
      "loss": 1.6801,
      "step": 427904
    },
    {
      "epoch": 0.001553064673739268,
      "grad_norm": 11039.980253605529,
      "learning_rate": 1.5294096055409045e-07,
      "loss": 1.6805,
      "step": 427936
    },
    {
      "epoch": 0.0015531808080901047,
      "grad_norm": 10459.697701176645,
      "learning_rate": 1.5293523698350123e-07,
      "loss": 1.698,
      "step": 427968
    },
    {
      "epoch": 0.0015532969424409415,
      "grad_norm": 9707.795424296908,
      "learning_rate": 1.529295140554503e-07,
      "loss": 1.6787,
      "step": 428000
    },
    {
      "epoch": 0.001553413076791778,
      "grad_norm": 9476.876067565725,
      "learning_rate": 1.5292379176981746e-07,
      "loss": 1.6859,
      "step": 428032
    },
    {
      "epoch": 0.0015535292111426148,
      "grad_norm": 11259.804083553141,
      "learning_rate": 1.5291807012648252e-07,
      "loss": 1.6777,
      "step": 428064
    },
    {
      "epoch": 0.0015536453454934515,
      "grad_norm": 11985.55864363443,
      "learning_rate": 1.5291234912532536e-07,
      "loss": 1.6662,
      "step": 428096
    },
    {
      "epoch": 0.0015537614798442883,
      "grad_norm": 7848.0998974274025,
      "learning_rate": 1.5290662876622585e-07,
      "loss": 1.6713,
      "step": 428128
    },
    {
      "epoch": 0.0015538776141951248,
      "grad_norm": 9528.897942574471,
      "learning_rate": 1.5290090904906392e-07,
      "loss": 1.6622,
      "step": 428160
    },
    {
      "epoch": 0.0015539937485459616,
      "grad_norm": 24430.970508762028,
      "learning_rate": 1.5289518997371947e-07,
      "loss": 1.6507,
      "step": 428192
    },
    {
      "epoch": 0.0015541098828967983,
      "grad_norm": 23161.25626990039,
      "learning_rate": 1.5288947154007252e-07,
      "loss": 1.6465,
      "step": 428224
    },
    {
      "epoch": 0.001554226017247635,
      "grad_norm": 17455.873510082503,
      "learning_rate": 1.5288375374800303e-07,
      "loss": 1.6726,
      "step": 428256
    },
    {
      "epoch": 0.0015543421515984718,
      "grad_norm": 16960.76059615252,
      "learning_rate": 1.5287803659739108e-07,
      "loss": 1.6749,
      "step": 428288
    },
    {
      "epoch": 0.0015544582859493084,
      "grad_norm": 18165.302750023187,
      "learning_rate": 1.5287232008811677e-07,
      "loss": 1.6725,
      "step": 428320
    },
    {
      "epoch": 0.0015545744203001451,
      "grad_norm": 16234.949707344338,
      "learning_rate": 1.5286660422006012e-07,
      "loss": 1.6687,
      "step": 428352
    },
    {
      "epoch": 0.001554690554650982,
      "grad_norm": 20829.129218476704,
      "learning_rate": 1.5286088899310137e-07,
      "loss": 1.6865,
      "step": 428384
    },
    {
      "epoch": 0.0015548066890018186,
      "grad_norm": 18816.539320502055,
      "learning_rate": 1.5285517440712059e-07,
      "loss": 1.6913,
      "step": 428416
    },
    {
      "epoch": 0.0015549228233526552,
      "grad_norm": 17921.505293919927,
      "learning_rate": 1.52849460461998e-07,
      "loss": 1.6676,
      "step": 428448
    },
    {
      "epoch": 0.001555038957703492,
      "grad_norm": 20382.493664907637,
      "learning_rate": 1.5284374715761382e-07,
      "loss": 1.652,
      "step": 428480
    },
    {
      "epoch": 0.0015551550920543287,
      "grad_norm": 11971.45337876734,
      "learning_rate": 1.5283821300489536e-07,
      "loss": 1.66,
      "step": 428512
    },
    {
      "epoch": 0.0015552712264051654,
      "grad_norm": 12826.442063175587,
      "learning_rate": 1.528325009616151e-07,
      "loss": 1.681,
      "step": 428544
    },
    {
      "epoch": 0.0015553873607560022,
      "grad_norm": 12271.420618656994,
      "learning_rate": 1.5282678955871786e-07,
      "loss": 1.6813,
      "step": 428576
    },
    {
      "epoch": 0.0015555034951068387,
      "grad_norm": 9864.737401471972,
      "learning_rate": 1.5282107879608403e-07,
      "loss": 1.6526,
      "step": 428608
    },
    {
      "epoch": 0.0015556196294576755,
      "grad_norm": 12302.52299327256,
      "learning_rate": 1.5281536867359397e-07,
      "loss": 1.6723,
      "step": 428640
    },
    {
      "epoch": 0.0015557357638085122,
      "grad_norm": 10497.30289169556,
      "learning_rate": 1.528096591911281e-07,
      "loss": 1.6698,
      "step": 428672
    },
    {
      "epoch": 0.001555851898159349,
      "grad_norm": 8652.784985194074,
      "learning_rate": 1.5280395034856687e-07,
      "loss": 1.6736,
      "step": 428704
    },
    {
      "epoch": 0.0015559680325101855,
      "grad_norm": 11688.426412481707,
      "learning_rate": 1.5279824214579075e-07,
      "loss": 1.6798,
      "step": 428736
    },
    {
      "epoch": 0.0015560841668610223,
      "grad_norm": 11469.897471206968,
      "learning_rate": 1.5279253458268025e-07,
      "loss": 1.6635,
      "step": 428768
    },
    {
      "epoch": 0.001556200301211859,
      "grad_norm": 8744.731671126336,
      "learning_rate": 1.5278682765911595e-07,
      "loss": 1.6813,
      "step": 428800
    },
    {
      "epoch": 0.0015563164355626958,
      "grad_norm": 11433.085497799797,
      "learning_rate": 1.5278112137497832e-07,
      "loss": 1.6912,
      "step": 428832
    },
    {
      "epoch": 0.0015564325699135326,
      "grad_norm": 10510.482196359975,
      "learning_rate": 1.5277541573014805e-07,
      "loss": 1.7017,
      "step": 428864
    },
    {
      "epoch": 0.001556548704264369,
      "grad_norm": 9880.891457758253,
      "learning_rate": 1.5276971072450578e-07,
      "loss": 1.6825,
      "step": 428896
    },
    {
      "epoch": 0.0015566648386152058,
      "grad_norm": 12928.222615657576,
      "learning_rate": 1.5276400635793214e-07,
      "loss": 1.6997,
      "step": 428928
    },
    {
      "epoch": 0.0015567809729660426,
      "grad_norm": 11391.851122622698,
      "learning_rate": 1.527583026303078e-07,
      "loss": 1.6952,
      "step": 428960
    },
    {
      "epoch": 0.0015568971073168794,
      "grad_norm": 10626.993836452528,
      "learning_rate": 1.5275259954151356e-07,
      "loss": 1.6768,
      "step": 428992
    },
    {
      "epoch": 0.0015570132416677159,
      "grad_norm": 10626.746068293907,
      "learning_rate": 1.527468970914301e-07,
      "loss": 1.6719,
      "step": 429024
    },
    {
      "epoch": 0.0015571293760185526,
      "grad_norm": 11440.169753985296,
      "learning_rate": 1.5274119527993825e-07,
      "loss": 1.6666,
      "step": 429056
    },
    {
      "epoch": 0.0015572455103693894,
      "grad_norm": 12075.501977143642,
      "learning_rate": 1.5273549410691884e-07,
      "loss": 1.654,
      "step": 429088
    },
    {
      "epoch": 0.0015573616447202262,
      "grad_norm": 9925.858149298729,
      "learning_rate": 1.527297935722527e-07,
      "loss": 1.6786,
      "step": 429120
    },
    {
      "epoch": 0.001557477779071063,
      "grad_norm": 8980.868332182585,
      "learning_rate": 1.527240936758207e-07,
      "loss": 1.6856,
      "step": 429152
    },
    {
      "epoch": 0.0015575939134218994,
      "grad_norm": 10584.994378836487,
      "learning_rate": 1.5271839441750381e-07,
      "loss": 1.6747,
      "step": 429184
    },
    {
      "epoch": 0.0015577100477727362,
      "grad_norm": 12552.631118614137,
      "learning_rate": 1.5271269579718295e-07,
      "loss": 1.659,
      "step": 429216
    },
    {
      "epoch": 0.001557826182123573,
      "grad_norm": 9036.716217741929,
      "learning_rate": 1.5270699781473903e-07,
      "loss": 1.6653,
      "step": 429248
    },
    {
      "epoch": 0.0015579423164744097,
      "grad_norm": 10736.872449647522,
      "learning_rate": 1.5270130047005317e-07,
      "loss": 1.678,
      "step": 429280
    },
    {
      "epoch": 0.0015580584508252462,
      "grad_norm": 9211.945071481918,
      "learning_rate": 1.526956037630063e-07,
      "loss": 1.6689,
      "step": 429312
    },
    {
      "epoch": 0.001558174585176083,
      "grad_norm": 7588.04256709199,
      "learning_rate": 1.526899076934796e-07,
      "loss": 1.6675,
      "step": 429344
    },
    {
      "epoch": 0.0015582907195269198,
      "grad_norm": 12393.551710466214,
      "learning_rate": 1.5268421226135407e-07,
      "loss": 1.6533,
      "step": 429376
    },
    {
      "epoch": 0.0015584068538777565,
      "grad_norm": 10042.1527572528,
      "learning_rate": 1.5267851746651087e-07,
      "loss": 1.6625,
      "step": 429408
    },
    {
      "epoch": 0.0015585229882285933,
      "grad_norm": 10425.298940558012,
      "learning_rate": 1.5267282330883123e-07,
      "loss": 1.6909,
      "step": 429440
    },
    {
      "epoch": 0.0015586391225794298,
      "grad_norm": 13125.936614200145,
      "learning_rate": 1.5266712978819623e-07,
      "loss": 1.6811,
      "step": 429472
    },
    {
      "epoch": 0.0015587552569302666,
      "grad_norm": 20988.585088090145,
      "learning_rate": 1.526614369044872e-07,
      "loss": 1.6774,
      "step": 429504
    },
    {
      "epoch": 0.0015588713912811033,
      "grad_norm": 19144.132678186284,
      "learning_rate": 1.5265574465758533e-07,
      "loss": 1.671,
      "step": 429536
    },
    {
      "epoch": 0.00155898752563194,
      "grad_norm": 18987.36358739675,
      "learning_rate": 1.5265005304737192e-07,
      "loss": 1.675,
      "step": 429568
    },
    {
      "epoch": 0.0015591036599827766,
      "grad_norm": 24852.09818103896,
      "learning_rate": 1.526443620737283e-07,
      "loss": 1.6758,
      "step": 429600
    },
    {
      "epoch": 0.0015592197943336134,
      "grad_norm": 21672.041712769013,
      "learning_rate": 1.526386717365358e-07,
      "loss": 1.684,
      "step": 429632
    },
    {
      "epoch": 0.00155933592868445,
      "grad_norm": 29148.47673550026,
      "learning_rate": 1.5263298203567584e-07,
      "loss": 1.6782,
      "step": 429664
    },
    {
      "epoch": 0.0015594520630352869,
      "grad_norm": 12215.176052763218,
      "learning_rate": 1.5262747074467097e-07,
      "loss": 1.6764,
      "step": 429696
    },
    {
      "epoch": 0.0015595681973861236,
      "grad_norm": 10287.778769005485,
      "learning_rate": 1.526217822962441e-07,
      "loss": 1.6923,
      "step": 429728
    },
    {
      "epoch": 0.0015596843317369602,
      "grad_norm": 11692.738601371366,
      "learning_rate": 1.526160944837977e-07,
      "loss": 1.6908,
      "step": 429760
    },
    {
      "epoch": 0.001559800466087797,
      "grad_norm": 10639.592849352835,
      "learning_rate": 1.526104073072134e-07,
      "loss": 1.6793,
      "step": 429792
    },
    {
      "epoch": 0.0015599166004386337,
      "grad_norm": 11409.797719504057,
      "learning_rate": 1.526047207663727e-07,
      "loss": 1.6709,
      "step": 429824
    },
    {
      "epoch": 0.0015600327347894704,
      "grad_norm": 8154.281206826265,
      "learning_rate": 1.525990348611571e-07,
      "loss": 1.6734,
      "step": 429856
    },
    {
      "epoch": 0.001560148869140307,
      "grad_norm": 9892.401730621335,
      "learning_rate": 1.5259334959144826e-07,
      "loss": 1.681,
      "step": 429888
    },
    {
      "epoch": 0.0015602650034911437,
      "grad_norm": 10709.738558900493,
      "learning_rate": 1.5258766495712776e-07,
      "loss": 1.6851,
      "step": 429920
    },
    {
      "epoch": 0.0015603811378419805,
      "grad_norm": 8287.902750394698,
      "learning_rate": 1.5258198095807726e-07,
      "loss": 1.667,
      "step": 429952
    },
    {
      "epoch": 0.0015604972721928172,
      "grad_norm": 10634.753217635094,
      "learning_rate": 1.525762975941785e-07,
      "loss": 1.6752,
      "step": 429984
    },
    {
      "epoch": 0.001560613406543654,
      "grad_norm": 8391.09432672521,
      "learning_rate": 1.5257061486531314e-07,
      "loss": 1.7053,
      "step": 430016
    },
    {
      "epoch": 0.0015607295408944905,
      "grad_norm": 8871.465493366923,
      "learning_rate": 1.5256493277136295e-07,
      "loss": 1.7219,
      "step": 430048
    },
    {
      "epoch": 0.0015608456752453273,
      "grad_norm": 12529.5626420079,
      "learning_rate": 1.5255925131220972e-07,
      "loss": 1.6494,
      "step": 430080
    },
    {
      "epoch": 0.001560961809596164,
      "grad_norm": 9670.479409005533,
      "learning_rate": 1.5255357048773526e-07,
      "loss": 1.6619,
      "step": 430112
    },
    {
      "epoch": 0.0015610779439470008,
      "grad_norm": 10088.341588189805,
      "learning_rate": 1.5254789029782136e-07,
      "loss": 1.6725,
      "step": 430144
    },
    {
      "epoch": 0.0015611940782978373,
      "grad_norm": 8668.90835111319,
      "learning_rate": 1.5254221074234996e-07,
      "loss": 1.678,
      "step": 430176
    },
    {
      "epoch": 0.001561310212648674,
      "grad_norm": 11972.59587558187,
      "learning_rate": 1.5253653182120295e-07,
      "loss": 1.6819,
      "step": 430208
    },
    {
      "epoch": 0.0015614263469995108,
      "grad_norm": 7672.846147291108,
      "learning_rate": 1.5253085353426225e-07,
      "loss": 1.6599,
      "step": 430240
    },
    {
      "epoch": 0.0015615424813503476,
      "grad_norm": 10869.19647444097,
      "learning_rate": 1.525251758814098e-07,
      "loss": 1.6511,
      "step": 430272
    },
    {
      "epoch": 0.0015616586157011843,
      "grad_norm": 10189.01447638583,
      "learning_rate": 1.5251949886252764e-07,
      "loss": 1.6574,
      "step": 430304
    },
    {
      "epoch": 0.0015617747500520209,
      "grad_norm": 10195.819535476292,
      "learning_rate": 1.5251382247749778e-07,
      "loss": 1.6734,
      "step": 430336
    },
    {
      "epoch": 0.0015618908844028576,
      "grad_norm": 10912.860303330195,
      "learning_rate": 1.525081467262023e-07,
      "loss": 1.6603,
      "step": 430368
    },
    {
      "epoch": 0.0015620070187536944,
      "grad_norm": 8691.560849467718,
      "learning_rate": 1.5250247160852325e-07,
      "loss": 1.6683,
      "step": 430400
    },
    {
      "epoch": 0.0015621231531045311,
      "grad_norm": 9161.79916828567,
      "learning_rate": 1.5249679712434277e-07,
      "loss": 1.6731,
      "step": 430432
    },
    {
      "epoch": 0.0015622392874553677,
      "grad_norm": 9187.371114742236,
      "learning_rate": 1.5249112327354295e-07,
      "loss": 1.6823,
      "step": 430464
    },
    {
      "epoch": 0.0015623554218062044,
      "grad_norm": 10092.870354859415,
      "learning_rate": 1.5248545005600608e-07,
      "loss": 1.6918,
      "step": 430496
    },
    {
      "epoch": 0.0015624715561570412,
      "grad_norm": 8374.09648857714,
      "learning_rate": 1.5247977747161427e-07,
      "loss": 1.6858,
      "step": 430528
    },
    {
      "epoch": 0.001562587690507878,
      "grad_norm": 7879.5843799022805,
      "learning_rate": 1.5247410552024984e-07,
      "loss": 1.6944,
      "step": 430560
    },
    {
      "epoch": 0.0015627038248587147,
      "grad_norm": 9443.73231302116,
      "learning_rate": 1.52468434201795e-07,
      "loss": 1.6932,
      "step": 430592
    },
    {
      "epoch": 0.0015628199592095512,
      "grad_norm": 10963.289469862591,
      "learning_rate": 1.5246276351613208e-07,
      "loss": 1.7006,
      "step": 430624
    },
    {
      "epoch": 0.001562936093560388,
      "grad_norm": 10766.765159508217,
      "learning_rate": 1.5245709346314342e-07,
      "loss": 1.687,
      "step": 430656
    },
    {
      "epoch": 0.0015630522279112247,
      "grad_norm": 17705.39285076725,
      "learning_rate": 1.524514240427114e-07,
      "loss": 1.6634,
      "step": 430688
    },
    {
      "epoch": 0.0015631683622620615,
      "grad_norm": 18578.536863811423,
      "learning_rate": 1.5244575525471832e-07,
      "loss": 1.6708,
      "step": 430720
    },
    {
      "epoch": 0.001563284496612898,
      "grad_norm": 17800.49437515711,
      "learning_rate": 1.5244008709904672e-07,
      "loss": 1.6865,
      "step": 430752
    },
    {
      "epoch": 0.0015634006309637348,
      "grad_norm": 17226.613596409483,
      "learning_rate": 1.5243441957557902e-07,
      "loss": 1.682,
      "step": 430784
    },
    {
      "epoch": 0.0015635167653145715,
      "grad_norm": 19026.62050917083,
      "learning_rate": 1.5242875268419763e-07,
      "loss": 1.6821,
      "step": 430816
    },
    {
      "epoch": 0.0015636328996654083,
      "grad_norm": 21284.03194885781,
      "learning_rate": 1.524230864247852e-07,
      "loss": 1.6608,
      "step": 430848
    },
    {
      "epoch": 0.001563749034016245,
      "grad_norm": 21734.61239589977,
      "learning_rate": 1.5241742079722418e-07,
      "loss": 1.6542,
      "step": 430880
    },
    {
      "epoch": 0.0015638651683670816,
      "grad_norm": 20871.696049914102,
      "learning_rate": 1.5241175580139718e-07,
      "loss": 1.6855,
      "step": 430912
    },
    {
      "epoch": 0.0015639813027179183,
      "grad_norm": 25936.211596916,
      "learning_rate": 1.5240609143718683e-07,
      "loss": 1.6856,
      "step": 430944
    },
    {
      "epoch": 0.001564097437068755,
      "grad_norm": 15849.057006648693,
      "learning_rate": 1.5240042770447572e-07,
      "loss": 1.6509,
      "step": 430976
    },
    {
      "epoch": 0.0015642135714195918,
      "grad_norm": 10108.273542005083,
      "learning_rate": 1.5239494156550718e-07,
      "loss": 1.6669,
      "step": 431008
    },
    {
      "epoch": 0.0015643297057704284,
      "grad_norm": 8426.501409244527,
      "learning_rate": 1.523892790757174e-07,
      "loss": 1.6773,
      "step": 431040
    },
    {
      "epoch": 0.0015644458401212651,
      "grad_norm": 9610.461695465,
      "learning_rate": 1.5238361721707862e-07,
      "loss": 1.6815,
      "step": 431072
    },
    {
      "epoch": 0.0015645619744721019,
      "grad_norm": 8930.048712073189,
      "learning_rate": 1.5237795598947367e-07,
      "loss": 1.6994,
      "step": 431104
    },
    {
      "epoch": 0.0015646781088229386,
      "grad_norm": 9285.231822631033,
      "learning_rate": 1.523722953927853e-07,
      "loss": 1.6721,
      "step": 431136
    },
    {
      "epoch": 0.0015647942431737754,
      "grad_norm": 9149.734750253692,
      "learning_rate": 1.523666354268963e-07,
      "loss": 1.6555,
      "step": 431168
    },
    {
      "epoch": 0.001564910377524612,
      "grad_norm": 11038.155824230786,
      "learning_rate": 1.5236097609168953e-07,
      "loss": 1.6685,
      "step": 431200
    },
    {
      "epoch": 0.0015650265118754487,
      "grad_norm": 9152.96039541306,
      "learning_rate": 1.5235531738704795e-07,
      "loss": 1.6664,
      "step": 431232
    },
    {
      "epoch": 0.0015651426462262854,
      "grad_norm": 8411.340083482537,
      "learning_rate": 1.523496593128544e-07,
      "loss": 1.6521,
      "step": 431264
    },
    {
      "epoch": 0.0015652587805771222,
      "grad_norm": 9910.879274817144,
      "learning_rate": 1.523440018689918e-07,
      "loss": 1.6845,
      "step": 431296
    },
    {
      "epoch": 0.0015653749149279587,
      "grad_norm": 10535.850606382002,
      "learning_rate": 1.5233834505534323e-07,
      "loss": 1.6783,
      "step": 431328
    },
    {
      "epoch": 0.0015654910492787955,
      "grad_norm": 8357.676830315946,
      "learning_rate": 1.5233268887179159e-07,
      "loss": 1.6848,
      "step": 431360
    },
    {
      "epoch": 0.0015656071836296322,
      "grad_norm": 14317.329220214222,
      "learning_rate": 1.5232703331821993e-07,
      "loss": 1.6916,
      "step": 431392
    },
    {
      "epoch": 0.001565723317980469,
      "grad_norm": 12101.4002495579,
      "learning_rate": 1.5232137839451136e-07,
      "loss": 1.691,
      "step": 431424
    },
    {
      "epoch": 0.0015658394523313057,
      "grad_norm": 9412.795546488833,
      "learning_rate": 1.5231572410054893e-07,
      "loss": 1.6913,
      "step": 431456
    },
    {
      "epoch": 0.0015659555866821423,
      "grad_norm": 9752.58160693875,
      "learning_rate": 1.523100704362158e-07,
      "loss": 1.6971,
      "step": 431488
    },
    {
      "epoch": 0.001566071721032979,
      "grad_norm": 9320.652552262636,
      "learning_rate": 1.523044174013951e-07,
      "loss": 1.7118,
      "step": 431520
    },
    {
      "epoch": 0.0015661878553838158,
      "grad_norm": 9639.250178307439,
      "learning_rate": 1.5229876499597003e-07,
      "loss": 1.6903,
      "step": 431552
    },
    {
      "epoch": 0.0015663039897346525,
      "grad_norm": 10230.59138075605,
      "learning_rate": 1.522931132198238e-07,
      "loss": 1.6815,
      "step": 431584
    },
    {
      "epoch": 0.001566420124085489,
      "grad_norm": 8323.645595530843,
      "learning_rate": 1.5228746207283967e-07,
      "loss": 1.6963,
      "step": 431616
    },
    {
      "epoch": 0.0015665362584363258,
      "grad_norm": 7936.61048559144,
      "learning_rate": 1.5228181155490088e-07,
      "loss": 1.7063,
      "step": 431648
    },
    {
      "epoch": 0.0015666523927871626,
      "grad_norm": 9236.729616049179,
      "learning_rate": 1.5227616166589078e-07,
      "loss": 1.7023,
      "step": 431680
    },
    {
      "epoch": 0.0015667685271379993,
      "grad_norm": 7947.819449383585,
      "learning_rate": 1.522705124056927e-07,
      "loss": 1.6756,
      "step": 431712
    },
    {
      "epoch": 0.001566884661488836,
      "grad_norm": 10885.502101419117,
      "learning_rate": 1.5226486377419e-07,
      "loss": 1.6676,
      "step": 431744
    },
    {
      "epoch": 0.0015670007958396726,
      "grad_norm": 7612.068312883168,
      "learning_rate": 1.5225921577126606e-07,
      "loss": 1.6792,
      "step": 431776
    },
    {
      "epoch": 0.0015671169301905094,
      "grad_norm": 10591.127796415263,
      "learning_rate": 1.522535683968043e-07,
      "loss": 1.7021,
      "step": 431808
    },
    {
      "epoch": 0.0015672330645413461,
      "grad_norm": 10874.25841149639,
      "learning_rate": 1.5224792165068825e-07,
      "loss": 1.6915,
      "step": 431840
    },
    {
      "epoch": 0.0015673491988921829,
      "grad_norm": 10097.34252167371,
      "learning_rate": 1.5224227553280134e-07,
      "loss": 1.668,
      "step": 431872
    },
    {
      "epoch": 0.0015674653332430194,
      "grad_norm": 12032.167385803774,
      "learning_rate": 1.522366300430271e-07,
      "loss": 1.6831,
      "step": 431904
    },
    {
      "epoch": 0.0015675814675938562,
      "grad_norm": 11648.019402456368,
      "learning_rate": 1.522309851812491e-07,
      "loss": 1.6832,
      "step": 431936
    },
    {
      "epoch": 0.001567697601944693,
      "grad_norm": 12962.34446386918,
      "learning_rate": 1.5222534094735084e-07,
      "loss": 1.6876,
      "step": 431968
    },
    {
      "epoch": 0.0015678137362955297,
      "grad_norm": 17128.194767692246,
      "learning_rate": 1.5221969734121603e-07,
      "loss": 1.6658,
      "step": 432000
    },
    {
      "epoch": 0.0015679298706463664,
      "grad_norm": 10763.253597309691,
      "learning_rate": 1.5221423069630664e-07,
      "loss": 1.639,
      "step": 432032
    },
    {
      "epoch": 0.001568046004997203,
      "grad_norm": 8858.970594826467,
      "learning_rate": 1.5220858832574105e-07,
      "loss": 1.6609,
      "step": 432064
    },
    {
      "epoch": 0.0015681621393480397,
      "grad_norm": 9885.358364773632,
      "learning_rate": 1.522029465825935e-07,
      "loss": 1.6703,
      "step": 432096
    },
    {
      "epoch": 0.0015682782736988765,
      "grad_norm": 9195.250078165356,
      "learning_rate": 1.521973054667477e-07,
      "loss": 1.6686,
      "step": 432128
    },
    {
      "epoch": 0.0015683944080497132,
      "grad_norm": 10063.010285197963,
      "learning_rate": 1.521916649780875e-07,
      "loss": 1.6663,
      "step": 432160
    },
    {
      "epoch": 0.0015685105424005498,
      "grad_norm": 9659.336830238399,
      "learning_rate": 1.521860251164966e-07,
      "loss": 1.6883,
      "step": 432192
    },
    {
      "epoch": 0.0015686266767513865,
      "grad_norm": 9200.94451673305,
      "learning_rate": 1.5218038588185884e-07,
      "loss": 1.6887,
      "step": 432224
    },
    {
      "epoch": 0.0015687428111022233,
      "grad_norm": 8081.085570639628,
      "learning_rate": 1.521747472740581e-07,
      "loss": 1.6834,
      "step": 432256
    },
    {
      "epoch": 0.00156885894545306,
      "grad_norm": 11423.595230924457,
      "learning_rate": 1.5216910929297826e-07,
      "loss": 1.6869,
      "step": 432288
    },
    {
      "epoch": 0.0015689750798038968,
      "grad_norm": 11466.447313793406,
      "learning_rate": 1.521634719385032e-07,
      "loss": 1.6761,
      "step": 432320
    },
    {
      "epoch": 0.0015690912141547333,
      "grad_norm": 13044.687807686314,
      "learning_rate": 1.5215783521051687e-07,
      "loss": 1.6791,
      "step": 432352
    },
    {
      "epoch": 0.00156920734850557,
      "grad_norm": 9199.138872742384,
      "learning_rate": 1.5215219910890327e-07,
      "loss": 1.6882,
      "step": 432384
    },
    {
      "epoch": 0.0015693234828564068,
      "grad_norm": 9203.83213667003,
      "learning_rate": 1.5214656363354634e-07,
      "loss": 1.6961,
      "step": 432416
    },
    {
      "epoch": 0.0015694396172072436,
      "grad_norm": 9404.198849450175,
      "learning_rate": 1.521409287843302e-07,
      "loss": 1.6644,
      "step": 432448
    },
    {
      "epoch": 0.0015695557515580801,
      "grad_norm": 11675.095031733146,
      "learning_rate": 1.5213529456113883e-07,
      "loss": 1.6617,
      "step": 432480
    },
    {
      "epoch": 0.0015696718859089169,
      "grad_norm": 10179.366384996662,
      "learning_rate": 1.5212966096385632e-07,
      "loss": 1.6767,
      "step": 432512
    },
    {
      "epoch": 0.0015697880202597536,
      "grad_norm": 9323.382647944896,
      "learning_rate": 1.5212402799236687e-07,
      "loss": 1.6713,
      "step": 432544
    },
    {
      "epoch": 0.0015699041546105904,
      "grad_norm": 9231.03634485316,
      "learning_rate": 1.5211839564655458e-07,
      "loss": 1.669,
      "step": 432576
    },
    {
      "epoch": 0.0015700202889614271,
      "grad_norm": 8964.193326786299,
      "learning_rate": 1.5211276392630363e-07,
      "loss": 1.6602,
      "step": 432608
    },
    {
      "epoch": 0.0015701364233122637,
      "grad_norm": 12100.86939025457,
      "learning_rate": 1.521071328314982e-07,
      "loss": 1.6592,
      "step": 432640
    },
    {
      "epoch": 0.0015702525576631004,
      "grad_norm": 12300.196583794912,
      "learning_rate": 1.521015023620226e-07,
      "loss": 1.6743,
      "step": 432672
    },
    {
      "epoch": 0.0015703686920139372,
      "grad_norm": 9261.873676530036,
      "learning_rate": 1.5209587251776106e-07,
      "loss": 1.6998,
      "step": 432704
    },
    {
      "epoch": 0.001570484826364774,
      "grad_norm": 8846.032557028038,
      "learning_rate": 1.5209024329859787e-07,
      "loss": 1.6721,
      "step": 432736
    },
    {
      "epoch": 0.0015706009607156105,
      "grad_norm": 15568.068987514154,
      "learning_rate": 1.520846147044174e-07,
      "loss": 1.6694,
      "step": 432768
    },
    {
      "epoch": 0.0015707170950664472,
      "grad_norm": 11426.882952056523,
      "learning_rate": 1.5207898673510397e-07,
      "loss": 1.6665,
      "step": 432800
    },
    {
      "epoch": 0.001570833229417284,
      "grad_norm": 11096.554960887635,
      "learning_rate": 1.52073359390542e-07,
      "loss": 1.6731,
      "step": 432832
    },
    {
      "epoch": 0.0015709493637681207,
      "grad_norm": 9304.04858112854,
      "learning_rate": 1.5206773267061593e-07,
      "loss": 1.6646,
      "step": 432864
    },
    {
      "epoch": 0.0015710654981189575,
      "grad_norm": 9197.28720873715,
      "learning_rate": 1.5206210657521014e-07,
      "loss": 1.6719,
      "step": 432896
    },
    {
      "epoch": 0.001571181632469794,
      "grad_norm": 9161.724291856855,
      "learning_rate": 1.5205648110420919e-07,
      "loss": 1.6599,
      "step": 432928
    },
    {
      "epoch": 0.0015712977668206308,
      "grad_norm": 9228.876204609096,
      "learning_rate": 1.520508562574975e-07,
      "loss": 1.657,
      "step": 432960
    },
    {
      "epoch": 0.0015714139011714675,
      "grad_norm": 9682.88531378948,
      "learning_rate": 1.5204523203495973e-07,
      "loss": 1.6649,
      "step": 432992
    },
    {
      "epoch": 0.0015715300355223043,
      "grad_norm": 11902.377073509308,
      "learning_rate": 1.5203978416448778e-07,
      "loss": 1.6586,
      "step": 433024
    },
    {
      "epoch": 0.0015716461698731408,
      "grad_norm": 8960.521190198704,
      "learning_rate": 1.5203416117045492e-07,
      "loss": 1.6591,
      "step": 433056
    },
    {
      "epoch": 0.0015717623042239776,
      "grad_norm": 9005.236698721472,
      "learning_rate": 1.520285388002534e-07,
      "loss": 1.6734,
      "step": 433088
    },
    {
      "epoch": 0.0015718784385748143,
      "grad_norm": 9534.961667463589,
      "learning_rate": 1.5202291705376772e-07,
      "loss": 1.6759,
      "step": 433120
    },
    {
      "epoch": 0.001571994572925651,
      "grad_norm": 10254.25550686153,
      "learning_rate": 1.5201729593088269e-07,
      "loss": 1.6802,
      "step": 433152
    },
    {
      "epoch": 0.0015721107072764879,
      "grad_norm": 9819.567607588431,
      "learning_rate": 1.5201167543148296e-07,
      "loss": 1.6968,
      "step": 433184
    },
    {
      "epoch": 0.0015722268416273244,
      "grad_norm": 9682.029952442825,
      "learning_rate": 1.5200605555545332e-07,
      "loss": 1.6796,
      "step": 433216
    },
    {
      "epoch": 0.0015723429759781611,
      "grad_norm": 9023.048708723676,
      "learning_rate": 1.5200043630267855e-07,
      "loss": 1.6958,
      "step": 433248
    },
    {
      "epoch": 0.001572459110328998,
      "grad_norm": 10289.998056365219,
      "learning_rate": 1.5199481767304344e-07,
      "loss": 1.7028,
      "step": 433280
    },
    {
      "epoch": 0.0015725752446798347,
      "grad_norm": 9076.629771010825,
      "learning_rate": 1.519891996664328e-07,
      "loss": 1.6925,
      "step": 433312
    },
    {
      "epoch": 0.0015726913790306712,
      "grad_norm": 12138.813780596522,
      "learning_rate": 1.5198358228273156e-07,
      "loss": 1.6502,
      "step": 433344
    },
    {
      "epoch": 0.001572807513381508,
      "grad_norm": 13568.36703512991,
      "learning_rate": 1.5197796552182456e-07,
      "loss": 1.6598,
      "step": 433376
    },
    {
      "epoch": 0.0015729236477323447,
      "grad_norm": 9583.617271156023,
      "learning_rate": 1.5197234938359677e-07,
      "loss": 1.6687,
      "step": 433408
    },
    {
      "epoch": 0.0015730397820831815,
      "grad_norm": 8052.867439614289,
      "learning_rate": 1.5196673386793313e-07,
      "loss": 1.6849,
      "step": 433440
    },
    {
      "epoch": 0.0015731559164340182,
      "grad_norm": 14452.340848457734,
      "learning_rate": 1.519611189747186e-07,
      "loss": 1.6848,
      "step": 433472
    },
    {
      "epoch": 0.0015732720507848547,
      "grad_norm": 10767.89385163134,
      "learning_rate": 1.5195550470383826e-07,
      "loss": 1.656,
      "step": 433504
    },
    {
      "epoch": 0.0015733881851356915,
      "grad_norm": 10259.588490772912,
      "learning_rate": 1.5194989105517714e-07,
      "loss": 1.6608,
      "step": 433536
    },
    {
      "epoch": 0.0015735043194865283,
      "grad_norm": 9320.881503377243,
      "learning_rate": 1.5194427802862024e-07,
      "loss": 1.6746,
      "step": 433568
    },
    {
      "epoch": 0.001573620453837365,
      "grad_norm": 9681.476953440524,
      "learning_rate": 1.5193866562405277e-07,
      "loss": 1.6709,
      "step": 433600
    },
    {
      "epoch": 0.0015737365881882015,
      "grad_norm": 8751.62899122215,
      "learning_rate": 1.5193305384135978e-07,
      "loss": 1.6473,
      "step": 433632
    },
    {
      "epoch": 0.0015738527225390383,
      "grad_norm": 13876.415243138264,
      "learning_rate": 1.5192744268042649e-07,
      "loss": 1.661,
      "step": 433664
    },
    {
      "epoch": 0.001573968856889875,
      "grad_norm": 8406.29407051645,
      "learning_rate": 1.5192183214113806e-07,
      "loss": 1.6673,
      "step": 433696
    },
    {
      "epoch": 0.0015740849912407118,
      "grad_norm": 8217.70697944384,
      "learning_rate": 1.5191622222337977e-07,
      "loss": 1.6769,
      "step": 433728
    },
    {
      "epoch": 0.0015742011255915486,
      "grad_norm": 9112.20346568271,
      "learning_rate": 1.519106129270368e-07,
      "loss": 1.6829,
      "step": 433760
    },
    {
      "epoch": 0.001574317259942385,
      "grad_norm": 8508.426529035787,
      "learning_rate": 1.519050042519945e-07,
      "loss": 1.6814,
      "step": 433792
    },
    {
      "epoch": 0.0015744333942932219,
      "grad_norm": 7609.835083626977,
      "learning_rate": 1.5189939619813814e-07,
      "loss": 1.6729,
      "step": 433824
    },
    {
      "epoch": 0.0015745495286440586,
      "grad_norm": 12197.59992785466,
      "learning_rate": 1.5189378876535308e-07,
      "loss": 1.6678,
      "step": 433856
    },
    {
      "epoch": 0.0015746656629948954,
      "grad_norm": 11517.76037257244,
      "learning_rate": 1.518881819535247e-07,
      "loss": 1.6732,
      "step": 433888
    },
    {
      "epoch": 0.001574781797345732,
      "grad_norm": 12306.695575986269,
      "learning_rate": 1.5188257576253837e-07,
      "loss": 1.6691,
      "step": 433920
    },
    {
      "epoch": 0.0015748979316965687,
      "grad_norm": 12731.87975123862,
      "learning_rate": 1.5187697019227958e-07,
      "loss": 1.6651,
      "step": 433952
    },
    {
      "epoch": 0.0015750140660474054,
      "grad_norm": 10039.536941512792,
      "learning_rate": 1.5187136524263372e-07,
      "loss": 1.6887,
      "step": 433984
    },
    {
      "epoch": 0.0015751302003982422,
      "grad_norm": 17857.41168254795,
      "learning_rate": 1.5186576091348636e-07,
      "loss": 1.6908,
      "step": 434016
    },
    {
      "epoch": 0.001575246334749079,
      "grad_norm": 18893.5773214074,
      "learning_rate": 1.5186015720472296e-07,
      "loss": 1.6926,
      "step": 434048
    },
    {
      "epoch": 0.0015753624690999155,
      "grad_norm": 12679.868453576322,
      "learning_rate": 1.518547292033568e-07,
      "loss": 1.6995,
      "step": 434080
    },
    {
      "epoch": 0.0015754786034507522,
      "grad_norm": 9255.830378739662,
      "learning_rate": 1.5184912671563993e-07,
      "loss": 1.6735,
      "step": 434112
    },
    {
      "epoch": 0.001575594737801589,
      "grad_norm": 11791.194341541488,
      "learning_rate": 1.5184352484796734e-07,
      "loss": 1.6794,
      "step": 434144
    },
    {
      "epoch": 0.0015757108721524257,
      "grad_norm": 9993.021565072298,
      "learning_rate": 1.518379236002247e-07,
      "loss": 1.6851,
      "step": 434176
    },
    {
      "epoch": 0.0015758270065032623,
      "grad_norm": 9599.327476443337,
      "learning_rate": 1.5183232297229762e-07,
      "loss": 1.6761,
      "step": 434208
    },
    {
      "epoch": 0.001575943140854099,
      "grad_norm": 11154.069302277085,
      "learning_rate": 1.5182672296407185e-07,
      "loss": 1.6573,
      "step": 434240
    },
    {
      "epoch": 0.0015760592752049358,
      "grad_norm": 10293.514171554823,
      "learning_rate": 1.5182112357543315e-07,
      "loss": 1.6735,
      "step": 434272
    },
    {
      "epoch": 0.0015761754095557725,
      "grad_norm": 13827.89166865289,
      "learning_rate": 1.518155248062672e-07,
      "loss": 1.6824,
      "step": 434304
    },
    {
      "epoch": 0.0015762915439066093,
      "grad_norm": 10076.920759835319,
      "learning_rate": 1.518099266564598e-07,
      "loss": 1.6909,
      "step": 434336
    },
    {
      "epoch": 0.0015764076782574458,
      "grad_norm": 9618.719873247168,
      "learning_rate": 1.5180432912589679e-07,
      "loss": 1.6927,
      "step": 434368
    },
    {
      "epoch": 0.0015765238126082826,
      "grad_norm": 10416.267949702524,
      "learning_rate": 1.5179873221446398e-07,
      "loss": 1.668,
      "step": 434400
    },
    {
      "epoch": 0.0015766399469591193,
      "grad_norm": 8924.864816903391,
      "learning_rate": 1.5179313592204732e-07,
      "loss": 1.6687,
      "step": 434432
    },
    {
      "epoch": 0.001576756081309956,
      "grad_norm": 10366.239819722483,
      "learning_rate": 1.5178754024853263e-07,
      "loss": 1.6725,
      "step": 434464
    },
    {
      "epoch": 0.0015768722156607926,
      "grad_norm": 9724.022830084265,
      "learning_rate": 1.517819451938059e-07,
      "loss": 1.6667,
      "step": 434496
    },
    {
      "epoch": 0.0015769883500116294,
      "grad_norm": 9662.745779539064,
      "learning_rate": 1.5177635075775302e-07,
      "loss": 1.6555,
      "step": 434528
    },
    {
      "epoch": 0.0015771044843624661,
      "grad_norm": 10215.420108835466,
      "learning_rate": 1.5177075694026007e-07,
      "loss": 1.6657,
      "step": 434560
    },
    {
      "epoch": 0.0015772206187133029,
      "grad_norm": 11792.115501469616,
      "learning_rate": 1.5176516374121303e-07,
      "loss": 1.6655,
      "step": 434592
    },
    {
      "epoch": 0.0015773367530641396,
      "grad_norm": 9896.27505680799,
      "learning_rate": 1.5175957116049796e-07,
      "loss": 1.6675,
      "step": 434624
    },
    {
      "epoch": 0.0015774528874149762,
      "grad_norm": 14911.870573472665,
      "learning_rate": 1.5175397919800089e-07,
      "loss": 1.6732,
      "step": 434656
    },
    {
      "epoch": 0.001577569021765813,
      "grad_norm": 10596.864347532246,
      "learning_rate": 1.51748387853608e-07,
      "loss": 1.6606,
      "step": 434688
    },
    {
      "epoch": 0.0015776851561166497,
      "grad_norm": 8855.739381892401,
      "learning_rate": 1.5174279712720539e-07,
      "loss": 1.6618,
      "step": 434720
    },
    {
      "epoch": 0.0015778012904674864,
      "grad_norm": 9303.476339519546,
      "learning_rate": 1.5173720701867923e-07,
      "loss": 1.6586,
      "step": 434752
    },
    {
      "epoch": 0.001577917424818323,
      "grad_norm": 10703.157478052914,
      "learning_rate": 1.5173161752791576e-07,
      "loss": 1.6777,
      "step": 434784
    },
    {
      "epoch": 0.0015780335591691597,
      "grad_norm": 10301.192164016746,
      "learning_rate": 1.5172602865480116e-07,
      "loss": 1.6719,
      "step": 434816
    },
    {
      "epoch": 0.0015781496935199965,
      "grad_norm": 8074.3056667431165,
      "learning_rate": 1.5172044039922166e-07,
      "loss": 1.6857,
      "step": 434848
    },
    {
      "epoch": 0.0015782658278708332,
      "grad_norm": 11313.356000762991,
      "learning_rate": 1.5171485276106363e-07,
      "loss": 1.7043,
      "step": 434880
    },
    {
      "epoch": 0.00157838196222167,
      "grad_norm": 9163.432108113204,
      "learning_rate": 1.517092657402133e-07,
      "loss": 1.7126,
      "step": 434912
    },
    {
      "epoch": 0.0015784980965725065,
      "grad_norm": 9187.713099569446,
      "learning_rate": 1.5170367933655707e-07,
      "loss": 1.7093,
      "step": 434944
    },
    {
      "epoch": 0.0015786142309233433,
      "grad_norm": 10806.503227223873,
      "learning_rate": 1.5169809354998133e-07,
      "loss": 1.7052,
      "step": 434976
    },
    {
      "epoch": 0.00157873036527418,
      "grad_norm": 9564.42899497926,
      "learning_rate": 1.5169250838037242e-07,
      "loss": 1.6752,
      "step": 435008
    },
    {
      "epoch": 0.0015788464996250168,
      "grad_norm": 11586.242531554395,
      "learning_rate": 1.516869238276168e-07,
      "loss": 1.6781,
      "step": 435040
    },
    {
      "epoch": 0.0015789626339758533,
      "grad_norm": 20988.295023655446,
      "learning_rate": 1.5168133989160091e-07,
      "loss": 1.6912,
      "step": 435072
    },
    {
      "epoch": 0.00157907876832669,
      "grad_norm": 22335.650247978007,
      "learning_rate": 1.516757565722113e-07,
      "loss": 1.6624,
      "step": 435104
    },
    {
      "epoch": 0.0015791949026775268,
      "grad_norm": 22524.11436660718,
      "learning_rate": 1.5167017386933445e-07,
      "loss": 1.658,
      "step": 435136
    },
    {
      "epoch": 0.0015793110370283636,
      "grad_norm": 16482.50781889698,
      "learning_rate": 1.516645917828569e-07,
      "loss": 1.6633,
      "step": 435168
    },
    {
      "epoch": 0.0015794271713792003,
      "grad_norm": 20644.2100357461,
      "learning_rate": 1.5165901031266525e-07,
      "loss": 1.6707,
      "step": 435200
    },
    {
      "epoch": 0.0015795433057300369,
      "grad_norm": 23852.938099949028,
      "learning_rate": 1.5165342945864606e-07,
      "loss": 1.6666,
      "step": 435232
    },
    {
      "epoch": 0.0015796594400808736,
      "grad_norm": 20418.644813013423,
      "learning_rate": 1.5164784922068603e-07,
      "loss": 1.6695,
      "step": 435264
    },
    {
      "epoch": 0.0015797755744317104,
      "grad_norm": 16289.334915827594,
      "learning_rate": 1.5164226959867182e-07,
      "loss": 1.6624,
      "step": 435296
    },
    {
      "epoch": 0.0015798917087825471,
      "grad_norm": 17628.986584599807,
      "learning_rate": 1.5163669059249008e-07,
      "loss": 1.6775,
      "step": 435328
    },
    {
      "epoch": 0.0015800078431333837,
      "grad_norm": 17757.935690839742,
      "learning_rate": 1.5163111220202755e-07,
      "loss": 1.6732,
      "step": 435360
    },
    {
      "epoch": 0.0015801239774842204,
      "grad_norm": 18118.35754145502,
      "learning_rate": 1.51625534427171e-07,
      "loss": 1.676,
      "step": 435392
    },
    {
      "epoch": 0.0015802401118350572,
      "grad_norm": 10564.956033983293,
      "learning_rate": 1.5162013154472191e-07,
      "loss": 1.6745,
      "step": 435424
    },
    {
      "epoch": 0.001580356246185894,
      "grad_norm": 11416.475988675315,
      "learning_rate": 1.5161455498150882e-07,
      "loss": 1.6776,
      "step": 435456
    },
    {
      "epoch": 0.0015804723805367307,
      "grad_norm": 10529.381558287267,
      "learning_rate": 1.5160897903356563e-07,
      "loss": 1.6689,
      "step": 435488
    },
    {
      "epoch": 0.0015805885148875672,
      "grad_norm": 8885.98255681385,
      "learning_rate": 1.5160340370077925e-07,
      "loss": 1.6682,
      "step": 435520
    },
    {
      "epoch": 0.001580704649238404,
      "grad_norm": 8819.19463443233,
      "learning_rate": 1.515978289830366e-07,
      "loss": 1.6781,
      "step": 435552
    },
    {
      "epoch": 0.0015808207835892407,
      "grad_norm": 9793.85113221556,
      "learning_rate": 1.5159225488022456e-07,
      "loss": 1.6652,
      "step": 435584
    },
    {
      "epoch": 0.0015809369179400775,
      "grad_norm": 9552.306318371497,
      "learning_rate": 1.5158668139223015e-07,
      "loss": 1.6648,
      "step": 435616
    },
    {
      "epoch": 0.001581053052290914,
      "grad_norm": 10181.396564322598,
      "learning_rate": 1.5158110851894027e-07,
      "loss": 1.6605,
      "step": 435648
    },
    {
      "epoch": 0.0015811691866417508,
      "grad_norm": 8937.847951268806,
      "learning_rate": 1.5157553626024202e-07,
      "loss": 1.6801,
      "step": 435680
    },
    {
      "epoch": 0.0015812853209925875,
      "grad_norm": 7424.625647128614,
      "learning_rate": 1.5156996461602242e-07,
      "loss": 1.663,
      "step": 435712
    },
    {
      "epoch": 0.0015814014553434243,
      "grad_norm": 10182.250635296698,
      "learning_rate": 1.5156439358616852e-07,
      "loss": 1.676,
      "step": 435744
    },
    {
      "epoch": 0.001581517589694261,
      "grad_norm": 12941.412133148377,
      "learning_rate": 1.515588231705674e-07,
      "loss": 1.7048,
      "step": 435776
    },
    {
      "epoch": 0.0015816337240450976,
      "grad_norm": 12603.429057205027,
      "learning_rate": 1.5155325336910625e-07,
      "loss": 1.6881,
      "step": 435808
    },
    {
      "epoch": 0.0015817498583959343,
      "grad_norm": 12088.489731972311,
      "learning_rate": 1.515476841816722e-07,
      "loss": 1.7014,
      "step": 435840
    },
    {
      "epoch": 0.001581865992746771,
      "grad_norm": 10117.54199398253,
      "learning_rate": 1.5154211560815244e-07,
      "loss": 1.6907,
      "step": 435872
    },
    {
      "epoch": 0.0015819821270976078,
      "grad_norm": 10385.470620053768,
      "learning_rate": 1.5153654764843417e-07,
      "loss": 1.6849,
      "step": 435904
    },
    {
      "epoch": 0.0015820982614484444,
      "grad_norm": 11518.855325074624,
      "learning_rate": 1.515309803024047e-07,
      "loss": 1.6873,
      "step": 435936
    },
    {
      "epoch": 0.0015822143957992811,
      "grad_norm": 11892.51832035587,
      "learning_rate": 1.5152541356995123e-07,
      "loss": 1.6882,
      "step": 435968
    },
    {
      "epoch": 0.0015823305301501179,
      "grad_norm": 10529.740357672643,
      "learning_rate": 1.5151984745096108e-07,
      "loss": 1.6739,
      "step": 436000
    },
    {
      "epoch": 0.0015824466645009546,
      "grad_norm": 9780.748642103017,
      "learning_rate": 1.5151428194532162e-07,
      "loss": 1.6654,
      "step": 436032
    },
    {
      "epoch": 0.0015825627988517914,
      "grad_norm": 10455.865530887435,
      "learning_rate": 1.515087170529202e-07,
      "loss": 1.6679,
      "step": 436064
    },
    {
      "epoch": 0.001582678933202628,
      "grad_norm": 11394.00342285362,
      "learning_rate": 1.515031527736442e-07,
      "loss": 1.674,
      "step": 436096
    },
    {
      "epoch": 0.0015827950675534647,
      "grad_norm": 8285.920467878024,
      "learning_rate": 1.5149758910738103e-07,
      "loss": 1.6734,
      "step": 436128
    },
    {
      "epoch": 0.0015829112019043014,
      "grad_norm": 10608.2765801048,
      "learning_rate": 1.5149202605401816e-07,
      "loss": 1.6878,
      "step": 436160
    },
    {
      "epoch": 0.0015830273362551382,
      "grad_norm": 8508.659001276288,
      "learning_rate": 1.5148646361344307e-07,
      "loss": 1.6638,
      "step": 436192
    },
    {
      "epoch": 0.0015831434706059747,
      "grad_norm": 9787.744581873803,
      "learning_rate": 1.5148090178554327e-07,
      "loss": 1.6646,
      "step": 436224
    },
    {
      "epoch": 0.0015832596049568115,
      "grad_norm": 9864.121248241021,
      "learning_rate": 1.5147534057020626e-07,
      "loss": 1.668,
      "step": 436256
    },
    {
      "epoch": 0.0015833757393076482,
      "grad_norm": 9024.702321960542,
      "learning_rate": 1.5146977996731963e-07,
      "loss": 1.6676,
      "step": 436288
    },
    {
      "epoch": 0.001583491873658485,
      "grad_norm": 12673.987217920017,
      "learning_rate": 1.51464219976771e-07,
      "loss": 1.6624,
      "step": 436320
    },
    {
      "epoch": 0.0015836080080093217,
      "grad_norm": 9108.873036770246,
      "learning_rate": 1.5145866059844796e-07,
      "loss": 1.6779,
      "step": 436352
    },
    {
      "epoch": 0.0015837241423601583,
      "grad_norm": 10295.073093475345,
      "learning_rate": 1.5145310183223816e-07,
      "loss": 1.6869,
      "step": 436384
    },
    {
      "epoch": 0.001583840276710995,
      "grad_norm": 19578.272446771192,
      "learning_rate": 1.514475436780293e-07,
      "loss": 1.6887,
      "step": 436416
    },
    {
      "epoch": 0.0015839564110618318,
      "grad_norm": 19879.87162936421,
      "learning_rate": 1.514419861357091e-07,
      "loss": 1.698,
      "step": 436448
    },
    {
      "epoch": 0.0015840725454126685,
      "grad_norm": 20007.886445099593,
      "learning_rate": 1.5143642920516528e-07,
      "loss": 1.678,
      "step": 436480
    },
    {
      "epoch": 0.001584188679763505,
      "grad_norm": 11458.94532668692,
      "learning_rate": 1.5143104651199307e-07,
      "loss": 1.688,
      "step": 436512
    },
    {
      "epoch": 0.0015843048141143418,
      "grad_norm": 10613.70265270325,
      "learning_rate": 1.5142549078555604e-07,
      "loss": 1.6969,
      "step": 436544
    },
    {
      "epoch": 0.0015844209484651786,
      "grad_norm": 9987.965959092973,
      "learning_rate": 1.5141993567056228e-07,
      "loss": 1.686,
      "step": 436576
    },
    {
      "epoch": 0.0015845370828160153,
      "grad_norm": 12240.442802447957,
      "learning_rate": 1.5141438116689962e-07,
      "loss": 1.6707,
      "step": 436608
    },
    {
      "epoch": 0.001584653217166852,
      "grad_norm": 10459.907074157016,
      "learning_rate": 1.5140882727445597e-07,
      "loss": 1.6881,
      "step": 436640
    },
    {
      "epoch": 0.0015847693515176886,
      "grad_norm": 8208.600246083372,
      "learning_rate": 1.5140327399311927e-07,
      "loss": 1.7008,
      "step": 436672
    },
    {
      "epoch": 0.0015848854858685254,
      "grad_norm": 7482.860549281939,
      "learning_rate": 1.513977213227774e-07,
      "loss": 1.7224,
      "step": 436704
    },
    {
      "epoch": 0.0015850016202193621,
      "grad_norm": 10778.242342794116,
      "learning_rate": 1.5139216926331836e-07,
      "loss": 1.7054,
      "step": 436736
    },
    {
      "epoch": 0.001585117754570199,
      "grad_norm": 8774.92039849935,
      "learning_rate": 1.5138661781463014e-07,
      "loss": 1.6941,
      "step": 436768
    },
    {
      "epoch": 0.0015852338889210354,
      "grad_norm": 12068.035465642284,
      "learning_rate": 1.5138106697660077e-07,
      "loss": 1.6868,
      "step": 436800
    },
    {
      "epoch": 0.0015853500232718722,
      "grad_norm": 12885.212144159676,
      "learning_rate": 1.513755167491183e-07,
      "loss": 1.6734,
      "step": 436832
    },
    {
      "epoch": 0.001585466157622709,
      "grad_norm": 10175.771223843429,
      "learning_rate": 1.5136996713207083e-07,
      "loss": 1.6877,
      "step": 436864
    },
    {
      "epoch": 0.0015855822919735457,
      "grad_norm": 11381.839218685176,
      "learning_rate": 1.5136441812534644e-07,
      "loss": 1.6756,
      "step": 436896
    },
    {
      "epoch": 0.0015856984263243824,
      "grad_norm": 13423.96171031488,
      "learning_rate": 1.5135886972883328e-07,
      "loss": 1.6566,
      "step": 436928
    },
    {
      "epoch": 0.001585814560675219,
      "grad_norm": 9049.70430456156,
      "learning_rate": 1.5135332194241954e-07,
      "loss": 1.6643,
      "step": 436960
    },
    {
      "epoch": 0.0015859306950260557,
      "grad_norm": 8562.493445252965,
      "learning_rate": 1.5134777476599337e-07,
      "loss": 1.6794,
      "step": 436992
    },
    {
      "epoch": 0.0015860468293768925,
      "grad_norm": 12732.803933148425,
      "learning_rate": 1.5134222819944308e-07,
      "loss": 1.687,
      "step": 437024
    },
    {
      "epoch": 0.0015861629637277292,
      "grad_norm": 11453.539191009913,
      "learning_rate": 1.5133668224265685e-07,
      "loss": 1.6898,
      "step": 437056
    },
    {
      "epoch": 0.0015862790980785658,
      "grad_norm": 10494.476833077482,
      "learning_rate": 1.5133113689552297e-07,
      "loss": 1.6762,
      "step": 437088
    },
    {
      "epoch": 0.0015863952324294025,
      "grad_norm": 10174.872382492078,
      "learning_rate": 1.5132559215792979e-07,
      "loss": 1.6517,
      "step": 437120
    },
    {
      "epoch": 0.0015865113667802393,
      "grad_norm": 8963.58901333612,
      "learning_rate": 1.5132004802976563e-07,
      "loss": 1.6582,
      "step": 437152
    },
    {
      "epoch": 0.001586627501131076,
      "grad_norm": 9284.444625285887,
      "learning_rate": 1.5131450451091887e-07,
      "loss": 1.6603,
      "step": 437184
    },
    {
      "epoch": 0.0015867436354819128,
      "grad_norm": 10070.95725340943,
      "learning_rate": 1.513089616012779e-07,
      "loss": 1.6623,
      "step": 437216
    },
    {
      "epoch": 0.0015868597698327493,
      "grad_norm": 13362.812353692616,
      "learning_rate": 1.5130341930073113e-07,
      "loss": 1.6748,
      "step": 437248
    },
    {
      "epoch": 0.001586975904183586,
      "grad_norm": 9355.187010423684,
      "learning_rate": 1.5129787760916702e-07,
      "loss": 1.6639,
      "step": 437280
    },
    {
      "epoch": 0.0015870920385344228,
      "grad_norm": 10925.05560626581,
      "learning_rate": 1.512923365264741e-07,
      "loss": 1.6713,
      "step": 437312
    },
    {
      "epoch": 0.0015872081728852596,
      "grad_norm": 11237.041603553847,
      "learning_rate": 1.5128679605254084e-07,
      "loss": 1.6761,
      "step": 437344
    },
    {
      "epoch": 0.0015873243072360961,
      "grad_norm": 10924.214937468047,
      "learning_rate": 1.5128125618725576e-07,
      "loss": 1.6536,
      "step": 437376
    },
    {
      "epoch": 0.001587440441586933,
      "grad_norm": 10937.362204846286,
      "learning_rate": 1.512757169305075e-07,
      "loss": 1.6576,
      "step": 437408
    },
    {
      "epoch": 0.0015875565759377696,
      "grad_norm": 13562.598571070368,
      "learning_rate": 1.5127017828218462e-07,
      "loss": 1.6697,
      "step": 437440
    },
    {
      "epoch": 0.0015876727102886064,
      "grad_norm": 8841.805697932974,
      "learning_rate": 1.512646402421757e-07,
      "loss": 1.6713,
      "step": 437472
    },
    {
      "epoch": 0.0015877888446394432,
      "grad_norm": 18957.029514140657,
      "learning_rate": 1.5125910281036946e-07,
      "loss": 1.6585,
      "step": 437504
    },
    {
      "epoch": 0.0015879049789902797,
      "grad_norm": 8739.935926538592,
      "learning_rate": 1.5125373900319227e-07,
      "loss": 1.6841,
      "step": 437536
    },
    {
      "epoch": 0.0015880211133411164,
      "grad_norm": 8802.417168028336,
      "learning_rate": 1.5124820276845976e-07,
      "loss": 1.7077,
      "step": 437568
    },
    {
      "epoch": 0.0015881372476919532,
      "grad_norm": 9020.640442895394,
      "learning_rate": 1.512426671415995e-07,
      "loss": 1.7088,
      "step": 437600
    },
    {
      "epoch": 0.00158825338204279,
      "grad_norm": 9962.190722928366,
      "learning_rate": 1.512371321225003e-07,
      "loss": 1.7081,
      "step": 437632
    },
    {
      "epoch": 0.0015883695163936265,
      "grad_norm": 10159.473017829221,
      "learning_rate": 1.5123159771105095e-07,
      "loss": 1.6719,
      "step": 437664
    },
    {
      "epoch": 0.0015884856507444632,
      "grad_norm": 13177.355425122294,
      "learning_rate": 1.5122606390714026e-07,
      "loss": 1.6702,
      "step": 437696
    },
    {
      "epoch": 0.0015886017850953,
      "grad_norm": 8798.827194575422,
      "learning_rate": 1.5122053071065709e-07,
      "loss": 1.6608,
      "step": 437728
    },
    {
      "epoch": 0.0015887179194461368,
      "grad_norm": 12604.309897808766,
      "learning_rate": 1.5121499812149032e-07,
      "loss": 1.6753,
      "step": 437760
    },
    {
      "epoch": 0.0015888340537969735,
      "grad_norm": 8956.809923181356,
      "learning_rate": 1.5120946613952884e-07,
      "loss": 1.6547,
      "step": 437792
    },
    {
      "epoch": 0.00158895018814781,
      "grad_norm": 11389.85952503366,
      "learning_rate": 1.5120393476466165e-07,
      "loss": 1.6603,
      "step": 437824
    },
    {
      "epoch": 0.0015890663224986468,
      "grad_norm": 9286.898405818813,
      "learning_rate": 1.5119840399677766e-07,
      "loss": 1.672,
      "step": 437856
    },
    {
      "epoch": 0.0015891824568494836,
      "grad_norm": 9230.34560566396,
      "learning_rate": 1.511928738357659e-07,
      "loss": 1.6684,
      "step": 437888
    },
    {
      "epoch": 0.0015892985912003203,
      "grad_norm": 9829.096397940148,
      "learning_rate": 1.5118734428151536e-07,
      "loss": 1.6753,
      "step": 437920
    },
    {
      "epoch": 0.0015894147255511568,
      "grad_norm": 11931.081593887455,
      "learning_rate": 1.5118181533391513e-07,
      "loss": 1.6704,
      "step": 437952
    },
    {
      "epoch": 0.0015895308599019936,
      "grad_norm": 10440.583125477235,
      "learning_rate": 1.511762869928543e-07,
      "loss": 1.6549,
      "step": 437984
    },
    {
      "epoch": 0.0015896469942528304,
      "grad_norm": 9953.431066722671,
      "learning_rate": 1.5117075925822196e-07,
      "loss": 1.6555,
      "step": 438016
    },
    {
      "epoch": 0.0015897631286036671,
      "grad_norm": 8827.215982403512,
      "learning_rate": 1.511652321299072e-07,
      "loss": 1.6731,
      "step": 438048
    },
    {
      "epoch": 0.0015898792629545039,
      "grad_norm": 9474.743057202131,
      "learning_rate": 1.5115970560779926e-07,
      "loss": 1.6577,
      "step": 438080
    },
    {
      "epoch": 0.0015899953973053404,
      "grad_norm": 10211.134119185783,
      "learning_rate": 1.511541796917873e-07,
      "loss": 1.6721,
      "step": 438112
    },
    {
      "epoch": 0.0015901115316561772,
      "grad_norm": 7753.223329686822,
      "learning_rate": 1.5114865438176057e-07,
      "loss": 1.6848,
      "step": 438144
    },
    {
      "epoch": 0.001590227666007014,
      "grad_norm": 10871.624533619619,
      "learning_rate": 1.5114312967760826e-07,
      "loss": 1.684,
      "step": 438176
    },
    {
      "epoch": 0.0015903438003578507,
      "grad_norm": 7767.092119963558,
      "learning_rate": 1.5113760557921975e-07,
      "loss": 1.6661,
      "step": 438208
    },
    {
      "epoch": 0.0015904599347086872,
      "grad_norm": 11992.544017013239,
      "learning_rate": 1.5113208208648425e-07,
      "loss": 1.6621,
      "step": 438240
    },
    {
      "epoch": 0.001590576069059524,
      "grad_norm": 14888.027673268209,
      "learning_rate": 1.5112655919929114e-07,
      "loss": 1.6483,
      "step": 438272
    },
    {
      "epoch": 0.0015906922034103607,
      "grad_norm": 10877.607641388799,
      "learning_rate": 1.511210369175298e-07,
      "loss": 1.6654,
      "step": 438304
    },
    {
      "epoch": 0.0015908083377611975,
      "grad_norm": 9787.357661800246,
      "learning_rate": 1.511155152410896e-07,
      "loss": 1.6761,
      "step": 438336
    },
    {
      "epoch": 0.0015909244721120342,
      "grad_norm": 8841.531315332204,
      "learning_rate": 1.5110999416985998e-07,
      "loss": 1.6689,
      "step": 438368
    },
    {
      "epoch": 0.0015910406064628708,
      "grad_norm": 8267.112434217886,
      "learning_rate": 1.5110447370373035e-07,
      "loss": 1.6759,
      "step": 438400
    },
    {
      "epoch": 0.0015911567408137075,
      "grad_norm": 10927.15525651576,
      "learning_rate": 1.5109895384259024e-07,
      "loss": 1.688,
      "step": 438432
    },
    {
      "epoch": 0.0015912728751645443,
      "grad_norm": 8737.910619822109,
      "learning_rate": 1.5109343458632912e-07,
      "loss": 1.6932,
      "step": 438464
    },
    {
      "epoch": 0.001591389009515381,
      "grad_norm": 8819.292261854122,
      "learning_rate": 1.5108791593483653e-07,
      "loss": 1.6927,
      "step": 438496
    },
    {
      "epoch": 0.0015915051438662176,
      "grad_norm": 18171.006356280875,
      "learning_rate": 1.5108239788800206e-07,
      "loss": 1.6955,
      "step": 438528
    },
    {
      "epoch": 0.0015916212782170543,
      "grad_norm": 15508.60922197732,
      "learning_rate": 1.5107688044571526e-07,
      "loss": 1.6667,
      "step": 438560
    },
    {
      "epoch": 0.001591737412567891,
      "grad_norm": 18965.771906252587,
      "learning_rate": 1.510713636078658e-07,
      "loss": 1.6561,
      "step": 438592
    },
    {
      "epoch": 0.0015918535469187278,
      "grad_norm": 12533.301241093664,
      "learning_rate": 1.5106601974749443e-07,
      "loss": 1.6674,
      "step": 438624
    },
    {
      "epoch": 0.0015919696812695646,
      "grad_norm": 8756.03220642775,
      "learning_rate": 1.5106050409930845e-07,
      "loss": 1.6856,
      "step": 438656
    },
    {
      "epoch": 0.001592085815620401,
      "grad_norm": 10205.504985055859,
      "learning_rate": 1.5105498905523225e-07,
      "loss": 1.6685,
      "step": 438688
    },
    {
      "epoch": 0.0015922019499712379,
      "grad_norm": 11674.581448600202,
      "learning_rate": 1.5104947461515555e-07,
      "loss": 1.6745,
      "step": 438720
    },
    {
      "epoch": 0.0015923180843220746,
      "grad_norm": 7732.197229765935,
      "learning_rate": 1.5104396077896816e-07,
      "loss": 1.6686,
      "step": 438752
    },
    {
      "epoch": 0.0015924342186729114,
      "grad_norm": 9832.43876156877,
      "learning_rate": 1.510384475465598e-07,
      "loss": 1.6788,
      "step": 438784
    },
    {
      "epoch": 0.001592550353023748,
      "grad_norm": 8534.709368221042,
      "learning_rate": 1.5103293491782034e-07,
      "loss": 1.6888,
      "step": 438816
    },
    {
      "epoch": 0.0015926664873745847,
      "grad_norm": 8365.527837500751,
      "learning_rate": 1.510274228926396e-07,
      "loss": 1.6707,
      "step": 438848
    },
    {
      "epoch": 0.0015927826217254214,
      "grad_norm": 12516.192232464313,
      "learning_rate": 1.5102191147090745e-07,
      "loss": 1.6583,
      "step": 438880
    },
    {
      "epoch": 0.0015928987560762582,
      "grad_norm": 7795.265486178133,
      "learning_rate": 1.510164006525138e-07,
      "loss": 1.6671,
      "step": 438912
    },
    {
      "epoch": 0.001593014890427095,
      "grad_norm": 8356.789694613595,
      "learning_rate": 1.5101089043734857e-07,
      "loss": 1.6711,
      "step": 438944
    },
    {
      "epoch": 0.0015931310247779315,
      "grad_norm": 9656.486835283316,
      "learning_rate": 1.510053808253017e-07,
      "loss": 1.6523,
      "step": 438976
    },
    {
      "epoch": 0.0015932471591287682,
      "grad_norm": 9616.88910199135,
      "learning_rate": 1.5099987181626323e-07,
      "loss": 1.6673,
      "step": 439008
    },
    {
      "epoch": 0.001593363293479605,
      "grad_norm": 10918.26378138942,
      "learning_rate": 1.509943634101231e-07,
      "loss": 1.6716,
      "step": 439040
    },
    {
      "epoch": 0.0015934794278304417,
      "grad_norm": 9482.006327776839,
      "learning_rate": 1.5098885560677142e-07,
      "loss": 1.6779,
      "step": 439072
    },
    {
      "epoch": 0.0015935955621812783,
      "grad_norm": 9811.205838223965,
      "learning_rate": 1.5098334840609823e-07,
      "loss": 1.6723,
      "step": 439104
    },
    {
      "epoch": 0.001593711696532115,
      "grad_norm": 10277.624628288388,
      "learning_rate": 1.5097784180799362e-07,
      "loss": 1.6704,
      "step": 439136
    },
    {
      "epoch": 0.0015938278308829518,
      "grad_norm": 9181.32855310167,
      "learning_rate": 1.509723358123477e-07,
      "loss": 1.6637,
      "step": 439168
    },
    {
      "epoch": 0.0015939439652337885,
      "grad_norm": 8913.167113882697,
      "learning_rate": 1.5096683041905065e-07,
      "loss": 1.6763,
      "step": 439200
    },
    {
      "epoch": 0.0015940600995846253,
      "grad_norm": 8808.093323756282,
      "learning_rate": 1.5096132562799262e-07,
      "loss": 1.6967,
      "step": 439232
    },
    {
      "epoch": 0.0015941762339354618,
      "grad_norm": 8706.355839270527,
      "learning_rate": 1.5095582143906387e-07,
      "loss": 1.6742,
      "step": 439264
    },
    {
      "epoch": 0.0015942923682862986,
      "grad_norm": 8656.57322501231,
      "learning_rate": 1.509503178521546e-07,
      "loss": 1.6696,
      "step": 439296
    },
    {
      "epoch": 0.0015944085026371353,
      "grad_norm": 7961.84463551004,
      "learning_rate": 1.5094481486715508e-07,
      "loss": 1.6962,
      "step": 439328
    },
    {
      "epoch": 0.001594524636987972,
      "grad_norm": 7540.072546070097,
      "learning_rate": 1.5093931248395563e-07,
      "loss": 1.6913,
      "step": 439360
    },
    {
      "epoch": 0.0015946407713388086,
      "grad_norm": 8238.043457035172,
      "learning_rate": 1.5093381070244653e-07,
      "loss": 1.7002,
      "step": 439392
    },
    {
      "epoch": 0.0015947569056896454,
      "grad_norm": 10743.27007944974,
      "learning_rate": 1.5092830952251816e-07,
      "loss": 1.6949,
      "step": 439424
    },
    {
      "epoch": 0.0015948730400404821,
      "grad_norm": 8073.534665807784,
      "learning_rate": 1.5092280894406084e-07,
      "loss": 1.6558,
      "step": 439456
    },
    {
      "epoch": 0.0015949891743913189,
      "grad_norm": 10955.93565150873,
      "learning_rate": 1.5091730896696506e-07,
      "loss": 1.6451,
      "step": 439488
    },
    {
      "epoch": 0.0015951053087421556,
      "grad_norm": 10594.683949981707,
      "learning_rate": 1.509118095911212e-07,
      "loss": 1.668,
      "step": 439520
    },
    {
      "epoch": 0.0015952214430929922,
      "grad_norm": 12925.960544578496,
      "learning_rate": 1.5090631081641972e-07,
      "loss": 1.6661,
      "step": 439552
    },
    {
      "epoch": 0.001595337577443829,
      "grad_norm": 11142.05043966325,
      "learning_rate": 1.5090081264275117e-07,
      "loss": 1.6516,
      "step": 439584
    },
    {
      "epoch": 0.0015954537117946657,
      "grad_norm": 16550.438302353203,
      "learning_rate": 1.5089531507000598e-07,
      "loss": 1.6638,
      "step": 439616
    },
    {
      "epoch": 0.0015955698461455024,
      "grad_norm": 15221.526598866489,
      "learning_rate": 1.5088981809807471e-07,
      "loss": 1.6804,
      "step": 439648
    },
    {
      "epoch": 0.001595685980496339,
      "grad_norm": 8578.473523885237,
      "learning_rate": 1.5088449347935723e-07,
      "loss": 1.6901,
      "step": 439680
    },
    {
      "epoch": 0.0015958021148471757,
      "grad_norm": 10250.024390214883,
      "learning_rate": 1.5087899768995868e-07,
      "loss": 1.6999,
      "step": 439712
    },
    {
      "epoch": 0.0015959182491980125,
      "grad_norm": 9474.601205327852,
      "learning_rate": 1.5087350250104926e-07,
      "loss": 1.6713,
      "step": 439744
    },
    {
      "epoch": 0.0015960343835488492,
      "grad_norm": 10661.393811317543,
      "learning_rate": 1.508680079125197e-07,
      "loss": 1.6754,
      "step": 439776
    },
    {
      "epoch": 0.001596150517899686,
      "grad_norm": 13151.739960932926,
      "learning_rate": 1.5086251392426062e-07,
      "loss": 1.674,
      "step": 439808
    },
    {
      "epoch": 0.0015962666522505225,
      "grad_norm": 9122.412838717617,
      "learning_rate": 1.5085702053616278e-07,
      "loss": 1.6725,
      "step": 439840
    },
    {
      "epoch": 0.0015963827866013593,
      "grad_norm": 10588.450500427341,
      "learning_rate": 1.5085152774811687e-07,
      "loss": 1.6613,
      "step": 439872
    },
    {
      "epoch": 0.001596498920952196,
      "grad_norm": 11350.022555043668,
      "learning_rate": 1.5084603556001368e-07,
      "loss": 1.6787,
      "step": 439904
    },
    {
      "epoch": 0.0015966150553030328,
      "grad_norm": 10149.402346936493,
      "learning_rate": 1.50840543971744e-07,
      "loss": 1.6838,
      "step": 439936
    },
    {
      "epoch": 0.0015967311896538693,
      "grad_norm": 12392.194962959547,
      "learning_rate": 1.5083505298319865e-07,
      "loss": 1.6709,
      "step": 439968
    },
    {
      "epoch": 0.001596847324004706,
      "grad_norm": 11798.136463018216,
      "learning_rate": 1.5082956259426848e-07,
      "loss": 1.6742,
      "step": 440000
    },
    {
      "epoch": 0.0015969634583555428,
      "grad_norm": 9754.706863868334,
      "learning_rate": 1.508240728048444e-07,
      "loss": 1.6571,
      "step": 440032
    },
    {
      "epoch": 0.0015970795927063796,
      "grad_norm": 10527.344774443363,
      "learning_rate": 1.5081858361481724e-07,
      "loss": 1.6625,
      "step": 440064
    },
    {
      "epoch": 0.0015971957270572163,
      "grad_norm": 9306.701456477478,
      "learning_rate": 1.50813095024078e-07,
      "loss": 1.6577,
      "step": 440096
    },
    {
      "epoch": 0.0015973118614080529,
      "grad_norm": 9847.266625820588,
      "learning_rate": 1.508076070325177e-07,
      "loss": 1.6748,
      "step": 440128
    },
    {
      "epoch": 0.0015974279957588896,
      "grad_norm": 13457.27899688492,
      "learning_rate": 1.5080211964002716e-07,
      "loss": 1.6672,
      "step": 440160
    },
    {
      "epoch": 0.0015975441301097264,
      "grad_norm": 11512.592931221012,
      "learning_rate": 1.5079663284649753e-07,
      "loss": 1.6779,
      "step": 440192
    },
    {
      "epoch": 0.0015976602644605631,
      "grad_norm": 8263.535441927988,
      "learning_rate": 1.507911466518198e-07,
      "loss": 1.703,
      "step": 440224
    },
    {
      "epoch": 0.0015977763988113997,
      "grad_norm": 10068.640821878591,
      "learning_rate": 1.5078566105588506e-07,
      "loss": 1.7033,
      "step": 440256
    },
    {
      "epoch": 0.0015978925331622364,
      "grad_norm": 9488.798027147592,
      "learning_rate": 1.5078017605858443e-07,
      "loss": 1.7135,
      "step": 440288
    },
    {
      "epoch": 0.0015980086675130732,
      "grad_norm": 9645.359091293594,
      "learning_rate": 1.5077469165980898e-07,
      "loss": 1.6997,
      "step": 440320
    },
    {
      "epoch": 0.00159812480186391,
      "grad_norm": 9563.052023282107,
      "learning_rate": 1.5076920785944992e-07,
      "loss": 1.6547,
      "step": 440352
    },
    {
      "epoch": 0.0015982409362147467,
      "grad_norm": 11867.073438721107,
      "learning_rate": 1.5076372465739843e-07,
      "loss": 1.6487,
      "step": 440384
    },
    {
      "epoch": 0.0015983570705655832,
      "grad_norm": 11097.584602065443,
      "learning_rate": 1.507582420535457e-07,
      "loss": 1.6669,
      "step": 440416
    },
    {
      "epoch": 0.00159847320491642,
      "grad_norm": 10599.185817788082,
      "learning_rate": 1.5075276004778296e-07,
      "loss": 1.6607,
      "step": 440448
    },
    {
      "epoch": 0.0015985893392672567,
      "grad_norm": 12401.914207089161,
      "learning_rate": 1.507472786400015e-07,
      "loss": 1.6686,
      "step": 440480
    },
    {
      "epoch": 0.0015987054736180935,
      "grad_norm": 9153.279849321772,
      "learning_rate": 1.507417978300926e-07,
      "loss": 1.6732,
      "step": 440512
    },
    {
      "epoch": 0.00159882160796893,
      "grad_norm": 8598.46172288974,
      "learning_rate": 1.5073631761794759e-07,
      "loss": 1.6853,
      "step": 440544
    },
    {
      "epoch": 0.0015989377423197668,
      "grad_norm": 11083.64569985887,
      "learning_rate": 1.507308380034578e-07,
      "loss": 1.6852,
      "step": 440576
    },
    {
      "epoch": 0.0015990538766706035,
      "grad_norm": 9722.602120831645,
      "learning_rate": 1.5072535898651468e-07,
      "loss": 1.6833,
      "step": 440608
    },
    {
      "epoch": 0.0015991700110214403,
      "grad_norm": 10392.723945145468,
      "learning_rate": 1.5071988056700952e-07,
      "loss": 1.6538,
      "step": 440640
    },
    {
      "epoch": 0.001599286145372277,
      "grad_norm": 24551.67741723567,
      "learning_rate": 1.5071440274483383e-07,
      "loss": 1.6635,
      "step": 440672
    },
    {
      "epoch": 0.0015994022797231136,
      "grad_norm": 19641.576515137476,
      "learning_rate": 1.5070892551987907e-07,
      "loss": 1.6768,
      "step": 440704
    },
    {
      "epoch": 0.0015995184140739503,
      "grad_norm": 17696.32187772363,
      "learning_rate": 1.507034488920367e-07,
      "loss": 1.6702,
      "step": 440736
    },
    {
      "epoch": 0.001599634548424787,
      "grad_norm": 10282.442025122242,
      "learning_rate": 1.5069814397812634e-07,
      "loss": 1.6622,
      "step": 440768
    },
    {
      "epoch": 0.0015997506827756238,
      "grad_norm": 10155.937179797835,
      "learning_rate": 1.5069266852553198e-07,
      "loss": 1.6745,
      "step": 440800
    },
    {
      "epoch": 0.0015998668171264604,
      "grad_norm": 9738.007188331707,
      "learning_rate": 1.5068719366972806e-07,
      "loss": 1.7026,
      "step": 440832
    },
    {
      "epoch": 0.0015999829514772971,
      "grad_norm": 11404.85615867206,
      "learning_rate": 1.5068171941060614e-07,
      "loss": 1.6881,
      "step": 440864
    },
    {
      "epoch": 0.0016000990858281339,
      "grad_norm": 12281.182190652495,
      "learning_rate": 1.5067624574805787e-07,
      "loss": 1.6727,
      "step": 440896
    },
    {
      "epoch": 0.0016002152201789706,
      "grad_norm": 9760.681943388996,
      "learning_rate": 1.5067077268197485e-07,
      "loss": 1.6521,
      "step": 440928
    },
    {
      "epoch": 0.0016003313545298074,
      "grad_norm": 8512.15577864973,
      "learning_rate": 1.5066530021224884e-07,
      "loss": 1.6635,
      "step": 440960
    },
    {
      "epoch": 0.001600447488880644,
      "grad_norm": 10194.39610766621,
      "learning_rate": 1.506598283387715e-07,
      "loss": 1.6709,
      "step": 440992
    },
    {
      "epoch": 0.0016005636232314807,
      "grad_norm": 9022.395912394888,
      "learning_rate": 1.5065435706143458e-07,
      "loss": 1.6929,
      "step": 441024
    },
    {
      "epoch": 0.0016006797575823174,
      "grad_norm": 11176.232996855424,
      "learning_rate": 1.506488863801298e-07,
      "loss": 1.6622,
      "step": 441056
    },
    {
      "epoch": 0.0016007958919331542,
      "grad_norm": 9404.759220734999,
      "learning_rate": 1.5064341629474902e-07,
      "loss": 1.6809,
      "step": 441088
    },
    {
      "epoch": 0.0016009120262839907,
      "grad_norm": 9120.98832364125,
      "learning_rate": 1.50637946805184e-07,
      "loss": 1.6925,
      "step": 441120
    },
    {
      "epoch": 0.0016010281606348275,
      "grad_norm": 9125.976112175618,
      "learning_rate": 1.5063247791132662e-07,
      "loss": 1.7002,
      "step": 441152
    },
    {
      "epoch": 0.0016011442949856642,
      "grad_norm": 14053.519274544722,
      "learning_rate": 1.5062700961306873e-07,
      "loss": 1.6939,
      "step": 441184
    },
    {
      "epoch": 0.001601260429336501,
      "grad_norm": 13042.096917290562,
      "learning_rate": 1.5062154191030224e-07,
      "loss": 1.6663,
      "step": 441216
    },
    {
      "epoch": 0.0016013765636873378,
      "grad_norm": 9221.901213957997,
      "learning_rate": 1.5061607480291908e-07,
      "loss": 1.6609,
      "step": 441248
    },
    {
      "epoch": 0.0016014926980381743,
      "grad_norm": 9960.992219653623,
      "learning_rate": 1.5061060829081117e-07,
      "loss": 1.6699,
      "step": 441280
    },
    {
      "epoch": 0.001601608832389011,
      "grad_norm": 11447.214508342193,
      "learning_rate": 1.5060514237387055e-07,
      "loss": 1.6867,
      "step": 441312
    },
    {
      "epoch": 0.0016017249667398478,
      "grad_norm": 14625.31394534832,
      "learning_rate": 1.505996770519892e-07,
      "loss": 1.6906,
      "step": 441344
    },
    {
      "epoch": 0.0016018411010906845,
      "grad_norm": 12766.242360224876,
      "learning_rate": 1.5059421232505914e-07,
      "loss": 1.6927,
      "step": 441376
    },
    {
      "epoch": 0.001601957235441521,
      "grad_norm": 10528.545958488285,
      "learning_rate": 1.5058874819297251e-07,
      "loss": 1.7051,
      "step": 441408
    },
    {
      "epoch": 0.0016020733697923578,
      "grad_norm": 10862.561023994296,
      "learning_rate": 1.5058328465562128e-07,
      "loss": 1.6988,
      "step": 441440
    },
    {
      "epoch": 0.0016021895041431946,
      "grad_norm": 9305.975714561047,
      "learning_rate": 1.5057782171289768e-07,
      "loss": 1.6934,
      "step": 441472
    },
    {
      "epoch": 0.0016023056384940313,
      "grad_norm": 11644.19821198523,
      "learning_rate": 1.505723593646938e-07,
      "loss": 1.6766,
      "step": 441504
    },
    {
      "epoch": 0.001602421772844868,
      "grad_norm": 15963.706587130697,
      "learning_rate": 1.505668976109018e-07,
      "loss": 1.6641,
      "step": 441536
    },
    {
      "epoch": 0.0016025379071957046,
      "grad_norm": 8843.70193979874,
      "learning_rate": 1.5056143645141394e-07,
      "loss": 1.6805,
      "step": 441568
    },
    {
      "epoch": 0.0016026540415465414,
      "grad_norm": 12911.096467767562,
      "learning_rate": 1.5055597588612238e-07,
      "loss": 1.6769,
      "step": 441600
    },
    {
      "epoch": 0.0016027701758973781,
      "grad_norm": 12193.267322584214,
      "learning_rate": 1.5055051591491946e-07,
      "loss": 1.675,
      "step": 441632
    },
    {
      "epoch": 0.001602886310248215,
      "grad_norm": 7856.268452643405,
      "learning_rate": 1.5054505653769738e-07,
      "loss": 1.6692,
      "step": 441664
    },
    {
      "epoch": 0.0016030024445990514,
      "grad_norm": 10156.114020628165,
      "learning_rate": 1.505395977543485e-07,
      "loss": 1.6778,
      "step": 441696
    },
    {
      "epoch": 0.0016031185789498882,
      "grad_norm": 11553.955340055629,
      "learning_rate": 1.5053413956476514e-07,
      "loss": 1.6877,
      "step": 441728
    },
    {
      "epoch": 0.001603234713300725,
      "grad_norm": 17056.12241982333,
      "learning_rate": 1.5052868196883966e-07,
      "loss": 1.683,
      "step": 441760
    },
    {
      "epoch": 0.0016033508476515617,
      "grad_norm": 22965.884960088082,
      "learning_rate": 1.5052322496646448e-07,
      "loss": 1.6907,
      "step": 441792
    },
    {
      "epoch": 0.0016034669820023982,
      "grad_norm": 17720.788695766336,
      "learning_rate": 1.5051776855753202e-07,
      "loss": 1.6729,
      "step": 441824
    },
    {
      "epoch": 0.001603583116353235,
      "grad_norm": 15337.924240261458,
      "learning_rate": 1.5051231274193467e-07,
      "loss": 1.6692,
      "step": 441856
    },
    {
      "epoch": 0.0016036992507040717,
      "grad_norm": 13368.324502345085,
      "learning_rate": 1.5050702798628557e-07,
      "loss": 1.6745,
      "step": 441888
    },
    {
      "epoch": 0.0016038153850549085,
      "grad_norm": 10860.114548198835,
      "learning_rate": 1.5050157333850263e-07,
      "loss": 1.6982,
      "step": 441920
    },
    {
      "epoch": 0.0016039315194057453,
      "grad_norm": 8606.12165844755,
      "learning_rate": 1.5049611928373566e-07,
      "loss": 1.6877,
      "step": 441952
    },
    {
      "epoch": 0.0016040476537565818,
      "grad_norm": 9933.723370418566,
      "learning_rate": 1.5049066582187728e-07,
      "loss": 1.6777,
      "step": 441984
    },
    {
      "epoch": 0.0016041637881074185,
      "grad_norm": 8432.39586357282,
      "learning_rate": 1.504852129528201e-07,
      "loss": 1.6896,
      "step": 442016
    },
    {
      "epoch": 0.0016042799224582553,
      "grad_norm": 11624.172744759087,
      "learning_rate": 1.5047976067645662e-07,
      "loss": 1.6922,
      "step": 442048
    },
    {
      "epoch": 0.001604396056809092,
      "grad_norm": 9970.291570460715,
      "learning_rate": 1.5047430899267958e-07,
      "loss": 1.6805,
      "step": 442080
    },
    {
      "epoch": 0.0016045121911599286,
      "grad_norm": 10343.354581565885,
      "learning_rate": 1.504688579013816e-07,
      "loss": 1.6674,
      "step": 442112
    },
    {
      "epoch": 0.0016046283255107653,
      "grad_norm": 11281.752966627128,
      "learning_rate": 1.5046340740245538e-07,
      "loss": 1.652,
      "step": 442144
    },
    {
      "epoch": 0.001604744459861602,
      "grad_norm": 10818.010907740849,
      "learning_rate": 1.5045795749579362e-07,
      "loss": 1.6547,
      "step": 442176
    },
    {
      "epoch": 0.0016048605942124389,
      "grad_norm": 8450.852264712714,
      "learning_rate": 1.5045250818128908e-07,
      "loss": 1.6718,
      "step": 442208
    },
    {
      "epoch": 0.0016049767285632756,
      "grad_norm": 9289.919267679348,
      "learning_rate": 1.5044705945883454e-07,
      "loss": 1.6426,
      "step": 442240
    },
    {
      "epoch": 0.0016050928629141121,
      "grad_norm": 13035.96854859661,
      "learning_rate": 1.5044161132832282e-07,
      "loss": 1.665,
      "step": 442272
    },
    {
      "epoch": 0.001605208997264949,
      "grad_norm": 10366.604072694201,
      "learning_rate": 1.504361637896467e-07,
      "loss": 1.686,
      "step": 442304
    },
    {
      "epoch": 0.0016053251316157857,
      "grad_norm": 11166.932971948921,
      "learning_rate": 1.5043071684269908e-07,
      "loss": 1.6912,
      "step": 442336
    },
    {
      "epoch": 0.0016054412659666224,
      "grad_norm": 8888.512136460186,
      "learning_rate": 1.504252704873728e-07,
      "loss": 1.6768,
      "step": 442368
    },
    {
      "epoch": 0.001605557400317459,
      "grad_norm": 10223.284990647575,
      "learning_rate": 1.504198247235608e-07,
      "loss": 1.6703,
      "step": 442400
    },
    {
      "epoch": 0.0016056735346682957,
      "grad_norm": 10287.209923006334,
      "learning_rate": 1.5041437955115604e-07,
      "loss": 1.6644,
      "step": 442432
    },
    {
      "epoch": 0.0016057896690191325,
      "grad_norm": 10264.199335554626,
      "learning_rate": 1.504089349700514e-07,
      "loss": 1.6713,
      "step": 442464
    },
    {
      "epoch": 0.0016059058033699692,
      "grad_norm": 9596.486231949692,
      "learning_rate": 1.5040349098013997e-07,
      "loss": 1.679,
      "step": 442496
    },
    {
      "epoch": 0.001606021937720806,
      "grad_norm": 9329.748549666278,
      "learning_rate": 1.5039804758131467e-07,
      "loss": 1.6523,
      "step": 442528
    },
    {
      "epoch": 0.0016061380720716425,
      "grad_norm": 9748.010463679242,
      "learning_rate": 1.5039260477346862e-07,
      "loss": 1.6551,
      "step": 442560
    },
    {
      "epoch": 0.0016062542064224793,
      "grad_norm": 12916.643449441499,
      "learning_rate": 1.5038716255649486e-07,
      "loss": 1.667,
      "step": 442592
    },
    {
      "epoch": 0.001606370340773316,
      "grad_norm": 8773.397631476644,
      "learning_rate": 1.503817209302865e-07,
      "loss": 1.6744,
      "step": 442624
    },
    {
      "epoch": 0.0016064864751241528,
      "grad_norm": 11119.837408883279,
      "learning_rate": 1.5037627989473668e-07,
      "loss": 1.6786,
      "step": 442656
    },
    {
      "epoch": 0.0016066026094749893,
      "grad_norm": 8987.25085885556,
      "learning_rate": 1.503708394497385e-07,
      "loss": 1.6693,
      "step": 442688
    },
    {
      "epoch": 0.001606718743825826,
      "grad_norm": 9956.478795236797,
      "learning_rate": 1.503653995951852e-07,
      "loss": 1.6494,
      "step": 442720
    },
    {
      "epoch": 0.0016068348781766628,
      "grad_norm": 12180.998973811631,
      "learning_rate": 1.5035996033096994e-07,
      "loss": 1.6592,
      "step": 442752
    },
    {
      "epoch": 0.0016069510125274996,
      "grad_norm": 7926.554358610051,
      "learning_rate": 1.50354521656986e-07,
      "loss": 1.6805,
      "step": 442784
    },
    {
      "epoch": 0.0016070671468783363,
      "grad_norm": 9748.774897390953,
      "learning_rate": 1.503490835731266e-07,
      "loss": 1.677,
      "step": 442816
    },
    {
      "epoch": 0.0016071832812291729,
      "grad_norm": 8907.38165792844,
      "learning_rate": 1.5034364607928506e-07,
      "loss": 1.6761,
      "step": 442848
    },
    {
      "epoch": 0.0016072994155800096,
      "grad_norm": 9176.670202202975,
      "learning_rate": 1.5033837906967423e-07,
      "loss": 1.6881,
      "step": 442880
    },
    {
      "epoch": 0.0016074155499308464,
      "grad_norm": 7922.539618077021,
      "learning_rate": 1.5033294273711857e-07,
      "loss": 1.6946,
      "step": 442912
    },
    {
      "epoch": 0.0016075316842816831,
      "grad_norm": 9004.85024861602,
      "learning_rate": 1.5032750699426414e-07,
      "loss": 1.6976,
      "step": 442944
    },
    {
      "epoch": 0.0016076478186325197,
      "grad_norm": 10559.056207824637,
      "learning_rate": 1.5032207184100428e-07,
      "loss": 1.6825,
      "step": 442976
    },
    {
      "epoch": 0.0016077639529833564,
      "grad_norm": 8786.268946486898,
      "learning_rate": 1.5031663727723247e-07,
      "loss": 1.6642,
      "step": 443008
    },
    {
      "epoch": 0.0016078800873341932,
      "grad_norm": 8893.429709622716,
      "learning_rate": 1.503112033028421e-07,
      "loss": 1.661,
      "step": 443040
    },
    {
      "epoch": 0.00160799622168503,
      "grad_norm": 7886.692462623352,
      "learning_rate": 1.503057699177267e-07,
      "loss": 1.6591,
      "step": 443072
    },
    {
      "epoch": 0.0016081123560358667,
      "grad_norm": 10407.860490994295,
      "learning_rate": 1.5030033712177975e-07,
      "loss": 1.6707,
      "step": 443104
    },
    {
      "epoch": 0.0016082284903867032,
      "grad_norm": 10466.245171980254,
      "learning_rate": 1.5029490491489477e-07,
      "loss": 1.6404,
      "step": 443136
    },
    {
      "epoch": 0.00160834462473754,
      "grad_norm": 11841.238617644693,
      "learning_rate": 1.5028947329696536e-07,
      "loss": 1.6723,
      "step": 443168
    },
    {
      "epoch": 0.0016084607590883767,
      "grad_norm": 7896.735654686689,
      "learning_rate": 1.5028404226788507e-07,
      "loss": 1.6965,
      "step": 443200
    },
    {
      "epoch": 0.0016085768934392135,
      "grad_norm": 8453.805888474137,
      "learning_rate": 1.502786118275475e-07,
      "loss": 1.6761,
      "step": 443232
    },
    {
      "epoch": 0.00160869302779005,
      "grad_norm": 10967.730850089274,
      "learning_rate": 1.5027318197584628e-07,
      "loss": 1.6651,
      "step": 443264
    },
    {
      "epoch": 0.0016088091621408868,
      "grad_norm": 8793.028147344918,
      "learning_rate": 1.5026775271267513e-07,
      "loss": 1.6578,
      "step": 443296
    },
    {
      "epoch": 0.0016089252964917235,
      "grad_norm": 8732.839629811142,
      "learning_rate": 1.502623240379277e-07,
      "loss": 1.6595,
      "step": 443328
    },
    {
      "epoch": 0.0016090414308425603,
      "grad_norm": 10684.39104488412,
      "learning_rate": 1.5025689595149774e-07,
      "loss": 1.6575,
      "step": 443360
    },
    {
      "epoch": 0.001609157565193397,
      "grad_norm": 8685.014910752889,
      "learning_rate": 1.5025146845327898e-07,
      "loss": 1.6714,
      "step": 443392
    },
    {
      "epoch": 0.0016092736995442336,
      "grad_norm": 11374.71441399739,
      "learning_rate": 1.5024604154316514e-07,
      "loss": 1.6483,
      "step": 443424
    },
    {
      "epoch": 0.0016093898338950703,
      "grad_norm": 9240.472174082879,
      "learning_rate": 1.502406152210501e-07,
      "loss": 1.6692,
      "step": 443456
    },
    {
      "epoch": 0.001609505968245907,
      "grad_norm": 8980.654987248981,
      "learning_rate": 1.5023518948682765e-07,
      "loss": 1.6753,
      "step": 443488
    },
    {
      "epoch": 0.0016096221025967438,
      "grad_norm": 8424.765516024763,
      "learning_rate": 1.5022976434039163e-07,
      "loss": 1.6864,
      "step": 443520
    },
    {
      "epoch": 0.0016097382369475804,
      "grad_norm": 8176.256478364656,
      "learning_rate": 1.5022433978163596e-07,
      "loss": 1.6867,
      "step": 443552
    },
    {
      "epoch": 0.0016098543712984171,
      "grad_norm": 8260.911571975577,
      "learning_rate": 1.5021891581045453e-07,
      "loss": 1.6767,
      "step": 443584
    },
    {
      "epoch": 0.0016099705056492539,
      "grad_norm": 8959.911160273856,
      "learning_rate": 1.5021349242674122e-07,
      "loss": 1.6557,
      "step": 443616
    },
    {
      "epoch": 0.0016100866400000906,
      "grad_norm": 11747.005575890395,
      "learning_rate": 1.5020806963039009e-07,
      "loss": 1.6665,
      "step": 443648
    },
    {
      "epoch": 0.0016102027743509274,
      "grad_norm": 8242.391764530486,
      "learning_rate": 1.5020264742129503e-07,
      "loss": 1.6785,
      "step": 443680
    },
    {
      "epoch": 0.001610318908701764,
      "grad_norm": 9546.71817956307,
      "learning_rate": 1.501972257993501e-07,
      "loss": 1.6863,
      "step": 443712
    },
    {
      "epoch": 0.0016104350430526007,
      "grad_norm": 8373.573669586958,
      "learning_rate": 1.5019180476444936e-07,
      "loss": 1.6948,
      "step": 443744
    },
    {
      "epoch": 0.0016105511774034374,
      "grad_norm": 7997.610143036481,
      "learning_rate": 1.5018638431648686e-07,
      "loss": 1.6947,
      "step": 443776
    },
    {
      "epoch": 0.0016106673117542742,
      "grad_norm": 8158.954344767471,
      "learning_rate": 1.5018096445535664e-07,
      "loss": 1.6947,
      "step": 443808
    },
    {
      "epoch": 0.0016107834461051107,
      "grad_norm": 9683.545425101283,
      "learning_rate": 1.5017554518095292e-07,
      "loss": 1.6827,
      "step": 443840
    },
    {
      "epoch": 0.0016108995804559475,
      "grad_norm": 9230.525012153967,
      "learning_rate": 1.5017012649316977e-07,
      "loss": 1.6693,
      "step": 443872
    },
    {
      "epoch": 0.0016110157148067842,
      "grad_norm": 19369.135447923327,
      "learning_rate": 1.5016470839190141e-07,
      "loss": 1.6508,
      "step": 443904
    },
    {
      "epoch": 0.001611131849157621,
      "grad_norm": 24293.002449265095,
      "learning_rate": 1.50159290877042e-07,
      "loss": 1.6631,
      "step": 443936
    },
    {
      "epoch": 0.0016112479835084577,
      "grad_norm": 21750.6998508094,
      "learning_rate": 1.5015387394848584e-07,
      "loss": 1.6688,
      "step": 443968
    },
    {
      "epoch": 0.0016113641178592943,
      "grad_norm": 24398.160750351653,
      "learning_rate": 1.501484576061271e-07,
      "loss": 1.6673,
      "step": 444000
    },
    {
      "epoch": 0.001611480252210131,
      "grad_norm": 25373.05279228339,
      "learning_rate": 1.501430418498601e-07,
      "loss": 1.6663,
      "step": 444032
    },
    {
      "epoch": 0.0016115963865609678,
      "grad_norm": 16376.371759336682,
      "learning_rate": 1.5013762667957912e-07,
      "loss": 1.7001,
      "step": 444064
    },
    {
      "epoch": 0.0016117125209118045,
      "grad_norm": 21111.617654741665,
      "learning_rate": 1.5013221209517856e-07,
      "loss": 1.7087,
      "step": 444096
    },
    {
      "epoch": 0.001611828655262641,
      "grad_norm": 18018.30535871784,
      "learning_rate": 1.5012679809655272e-07,
      "loss": 1.6763,
      "step": 444128
    },
    {
      "epoch": 0.0016119447896134778,
      "grad_norm": 19491.356853744175,
      "learning_rate": 1.50121384683596e-07,
      "loss": 1.6689,
      "step": 444160
    },
    {
      "epoch": 0.0016120609239643146,
      "grad_norm": 19647.05535188416,
      "learning_rate": 1.5011597185620284e-07,
      "loss": 1.6482,
      "step": 444192
    },
    {
      "epoch": 0.0016121770583151513,
      "grad_norm": 22362.925032293962,
      "learning_rate": 1.5011055961426763e-07,
      "loss": 1.6478,
      "step": 444224
    },
    {
      "epoch": 0.001612293192665988,
      "grad_norm": 17645.211475071643,
      "learning_rate": 1.5010514795768492e-07,
      "loss": 1.6721,
      "step": 444256
    },
    {
      "epoch": 0.0016124093270168246,
      "grad_norm": 17813.777140180013,
      "learning_rate": 1.5009973688634912e-07,
      "loss": 1.6785,
      "step": 444288
    },
    {
      "epoch": 0.0016125254613676614,
      "grad_norm": 8370.369048016939,
      "learning_rate": 1.500944954689923e-07,
      "loss": 1.6436,
      "step": 444320
    },
    {
      "epoch": 0.0016126415957184981,
      "grad_norm": 11303.757516861373,
      "learning_rate": 1.5008908554955323e-07,
      "loss": 1.663,
      "step": 444352
    },
    {
      "epoch": 0.0016127577300693349,
      "grad_norm": 7537.089623986171,
      "learning_rate": 1.5008367621504803e-07,
      "loss": 1.6715,
      "step": 444384
    },
    {
      "epoch": 0.0016128738644201714,
      "grad_norm": 13369.78174840562,
      "learning_rate": 1.5007826746537133e-07,
      "loss": 1.6675,
      "step": 444416
    },
    {
      "epoch": 0.0016129899987710082,
      "grad_norm": 9985.784896541683,
      "learning_rate": 1.500728593004177e-07,
      "loss": 1.6729,
      "step": 444448
    },
    {
      "epoch": 0.001613106133121845,
      "grad_norm": 11219.806415442292,
      "learning_rate": 1.5006745172008188e-07,
      "loss": 1.6614,
      "step": 444480
    },
    {
      "epoch": 0.0016132222674726817,
      "grad_norm": 10783.030186362274,
      "learning_rate": 1.5006204472425847e-07,
      "loss": 1.6601,
      "step": 444512
    },
    {
      "epoch": 0.0016133384018235184,
      "grad_norm": 11587.287862135816,
      "learning_rate": 1.5005663831284224e-07,
      "loss": 1.6687,
      "step": 444544
    },
    {
      "epoch": 0.001613454536174355,
      "grad_norm": 9284.234809611398,
      "learning_rate": 1.5005123248572786e-07,
      "loss": 1.7027,
      "step": 444576
    },
    {
      "epoch": 0.0016135706705251917,
      "grad_norm": 11375.490670735922,
      "learning_rate": 1.5004582724281012e-07,
      "loss": 1.699,
      "step": 444608
    },
    {
      "epoch": 0.0016136868048760285,
      "grad_norm": 10187.38023242482,
      "learning_rate": 1.5004042258398386e-07,
      "loss": 1.7034,
      "step": 444640
    },
    {
      "epoch": 0.0016138029392268652,
      "grad_norm": 10015.268144188653,
      "learning_rate": 1.5003501850914377e-07,
      "loss": 1.7049,
      "step": 444672
    },
    {
      "epoch": 0.0016139190735777018,
      "grad_norm": 10125.325476250135,
      "learning_rate": 1.500296150181848e-07,
      "loss": 1.6942,
      "step": 444704
    },
    {
      "epoch": 0.0016140352079285385,
      "grad_norm": 10055.526639614654,
      "learning_rate": 1.5002421211100178e-07,
      "loss": 1.6782,
      "step": 444736
    },
    {
      "epoch": 0.0016141513422793753,
      "grad_norm": 7877.2111816302095,
      "learning_rate": 1.5001880978748959e-07,
      "loss": 1.6684,
      "step": 444768
    },
    {
      "epoch": 0.001614267476630212,
      "grad_norm": 14582.077972634765,
      "learning_rate": 1.5001340804754314e-07,
      "loss": 1.6639,
      "step": 444800
    },
    {
      "epoch": 0.0016143836109810488,
      "grad_norm": 8876.76067042477,
      "learning_rate": 1.500080068910574e-07,
      "loss": 1.6801,
      "step": 444832
    },
    {
      "epoch": 0.0016144997453318853,
      "grad_norm": 11372.059707898126,
      "learning_rate": 1.5000260631792735e-07,
      "loss": 1.6733,
      "step": 444864
    },
    {
      "epoch": 0.001614615879682722,
      "grad_norm": 11995.130178534953,
      "learning_rate": 1.4999720632804797e-07,
      "loss": 1.6568,
      "step": 444896
    },
    {
      "epoch": 0.0016147320140335588,
      "grad_norm": 10303.68089567995,
      "learning_rate": 1.499918069213143e-07,
      "loss": 1.6635,
      "step": 444928
    },
    {
      "epoch": 0.0016148481483843956,
      "grad_norm": 9668.213692301179,
      "learning_rate": 1.4998640809762134e-07,
      "loss": 1.6803,
      "step": 444960
    },
    {
      "epoch": 0.0016149642827352321,
      "grad_norm": 8869.219018605865,
      "learning_rate": 1.499810098568642e-07,
      "loss": 1.68,
      "step": 444992
    },
    {
      "epoch": 0.0016150804170860689,
      "grad_norm": 11970.120717854103,
      "learning_rate": 1.49975612198938e-07,
      "loss": 1.6641,
      "step": 445024
    },
    {
      "epoch": 0.0016151965514369056,
      "grad_norm": 11778.644573973696,
      "learning_rate": 1.4997021512373785e-07,
      "loss": 1.6733,
      "step": 445056
    },
    {
      "epoch": 0.0016153126857877424,
      "grad_norm": 11054.555260163115,
      "learning_rate": 1.4996481863115896e-07,
      "loss": 1.6467,
      "step": 445088
    },
    {
      "epoch": 0.0016154288201385791,
      "grad_norm": 9401.821738365388,
      "learning_rate": 1.4995942272109643e-07,
      "loss": 1.6529,
      "step": 445120
    },
    {
      "epoch": 0.0016155449544894157,
      "grad_norm": 8737.88555658633,
      "learning_rate": 1.499540273934455e-07,
      "loss": 1.6886,
      "step": 445152
    },
    {
      "epoch": 0.0016156610888402524,
      "grad_norm": 10432.43308150117,
      "learning_rate": 1.499486326481014e-07,
      "loss": 1.6877,
      "step": 445184
    },
    {
      "epoch": 0.0016157772231910892,
      "grad_norm": 15713.716174094528,
      "learning_rate": 1.4994323848495942e-07,
      "loss": 1.6625,
      "step": 445216
    },
    {
      "epoch": 0.001615893357541926,
      "grad_norm": 9438.238394954857,
      "learning_rate": 1.4993784490391485e-07,
      "loss": 1.6648,
      "step": 445248
    },
    {
      "epoch": 0.0016160094918927625,
      "grad_norm": 8814.020875854561,
      "learning_rate": 1.49932451904863e-07,
      "loss": 1.6719,
      "step": 445280
    },
    {
      "epoch": 0.0016161256262435992,
      "grad_norm": 8968.547708520036,
      "learning_rate": 1.4992705948769917e-07,
      "loss": 1.6717,
      "step": 445312
    },
    {
      "epoch": 0.001616241760594436,
      "grad_norm": 20038.454231801414,
      "learning_rate": 1.4992166765231875e-07,
      "loss": 1.6767,
      "step": 445344
    },
    {
      "epoch": 0.0016163578949452727,
      "grad_norm": 22160.390971280267,
      "learning_rate": 1.4991627639861715e-07,
      "loss": 1.6586,
      "step": 445376
    },
    {
      "epoch": 0.0016164740292961095,
      "grad_norm": 20807.46481434007,
      "learning_rate": 1.4991088572648976e-07,
      "loss": 1.6682,
      "step": 445408
    },
    {
      "epoch": 0.001616590163646946,
      "grad_norm": 12339.683464335703,
      "learning_rate": 1.4990566406736465e-07,
      "loss": 1.6756,
      "step": 445440
    },
    {
      "epoch": 0.0016167062979977828,
      "grad_norm": 10709.308847913575,
      "learning_rate": 1.4990027453990603e-07,
      "loss": 1.6962,
      "step": 445472
    },
    {
      "epoch": 0.0016168224323486195,
      "grad_norm": 9711.347383344908,
      "learning_rate": 1.4989488559371135e-07,
      "loss": 1.677,
      "step": 445504
    },
    {
      "epoch": 0.0016169385666994563,
      "grad_norm": 8674.240024347955,
      "learning_rate": 1.4988949722867608e-07,
      "loss": 1.6884,
      "step": 445536
    },
    {
      "epoch": 0.0016170547010502928,
      "grad_norm": 9112.24033923601,
      "learning_rate": 1.498841094446958e-07,
      "loss": 1.6958,
      "step": 445568
    },
    {
      "epoch": 0.0016171708354011296,
      "grad_norm": 8111.855028290385,
      "learning_rate": 1.498787222416661e-07,
      "loss": 1.6833,
      "step": 445600
    },
    {
      "epoch": 0.0016172869697519663,
      "grad_norm": 11000.912325802801,
      "learning_rate": 1.4987333561948255e-07,
      "loss": 1.6877,
      "step": 445632
    },
    {
      "epoch": 0.001617403104102803,
      "grad_norm": 11331.95746550436,
      "learning_rate": 1.4986794957804083e-07,
      "loss": 1.6735,
      "step": 445664
    },
    {
      "epoch": 0.0016175192384536399,
      "grad_norm": 11351.055105143309,
      "learning_rate": 1.4986256411723656e-07,
      "loss": 1.6723,
      "step": 445696
    },
    {
      "epoch": 0.0016176353728044764,
      "grad_norm": 13266.819211853306,
      "learning_rate": 1.4985717923696544e-07,
      "loss": 1.6746,
      "step": 445728
    },
    {
      "epoch": 0.0016177515071553131,
      "grad_norm": 13122.512488087026,
      "learning_rate": 1.4985179493712316e-07,
      "loss": 1.6767,
      "step": 445760
    },
    {
      "epoch": 0.00161786764150615,
      "grad_norm": 8244.716611260814,
      "learning_rate": 1.4984641121760548e-07,
      "loss": 1.6619,
      "step": 445792
    },
    {
      "epoch": 0.0016179837758569867,
      "grad_norm": 10217.322545559575,
      "learning_rate": 1.498410280783081e-07,
      "loss": 1.6728,
      "step": 445824
    },
    {
      "epoch": 0.0016180999102078232,
      "grad_norm": 9364.468271076581,
      "learning_rate": 1.4983564551912688e-07,
      "loss": 1.6742,
      "step": 445856
    },
    {
      "epoch": 0.00161821604455866,
      "grad_norm": 8111.688356932853,
      "learning_rate": 1.4983026353995763e-07,
      "loss": 1.6804,
      "step": 445888
    },
    {
      "epoch": 0.0016183321789094967,
      "grad_norm": 9240.845415869697,
      "learning_rate": 1.4982488214069612e-07,
      "loss": 1.6788,
      "step": 445920
    },
    {
      "epoch": 0.0016184483132603335,
      "grad_norm": 10624.058170021473,
      "learning_rate": 1.4981950132123827e-07,
      "loss": 1.6667,
      "step": 445952
    },
    {
      "epoch": 0.0016185644476111702,
      "grad_norm": 10758.752901707521,
      "learning_rate": 1.4981412108148e-07,
      "loss": 1.6475,
      "step": 445984
    },
    {
      "epoch": 0.0016186805819620067,
      "grad_norm": 8766.438273324007,
      "learning_rate": 1.4980874142131713e-07,
      "loss": 1.65,
      "step": 446016
    },
    {
      "epoch": 0.0016187967163128435,
      "grad_norm": 9846.488917375575,
      "learning_rate": 1.4980336234064568e-07,
      "loss": 1.664,
      "step": 446048
    },
    {
      "epoch": 0.0016189128506636803,
      "grad_norm": 11934.302325649373,
      "learning_rate": 1.4979798383936163e-07,
      "loss": 1.6676,
      "step": 446080
    },
    {
      "epoch": 0.001619028985014517,
      "grad_norm": 8653.1270648246,
      "learning_rate": 1.4979260591736097e-07,
      "loss": 1.6596,
      "step": 446112
    },
    {
      "epoch": 0.0016191451193653535,
      "grad_norm": 7613.44744514599,
      "learning_rate": 1.4978722857453967e-07,
      "loss": 1.6656,
      "step": 446144
    },
    {
      "epoch": 0.0016192612537161903,
      "grad_norm": 13493.178276447696,
      "learning_rate": 1.497818518107938e-07,
      "loss": 1.6824,
      "step": 446176
    },
    {
      "epoch": 0.001619377388067027,
      "grad_norm": 11043.670585453008,
      "learning_rate": 1.4977647562601948e-07,
      "loss": 1.6907,
      "step": 446208
    },
    {
      "epoch": 0.0016194935224178638,
      "grad_norm": 9340.436178252063,
      "learning_rate": 1.497711000201128e-07,
      "loss": 1.7061,
      "step": 446240
    },
    {
      "epoch": 0.0016196096567687006,
      "grad_norm": 13338.307238926534,
      "learning_rate": 1.497657249929698e-07,
      "loss": 1.6905,
      "step": 446272
    },
    {
      "epoch": 0.001619725791119537,
      "grad_norm": 10452.614983821033,
      "learning_rate": 1.4976035054448677e-07,
      "loss": 1.686,
      "step": 446304
    },
    {
      "epoch": 0.0016198419254703738,
      "grad_norm": 9272.51659475463,
      "learning_rate": 1.4975497667455978e-07,
      "loss": 1.6981,
      "step": 446336
    },
    {
      "epoch": 0.0016199580598212106,
      "grad_norm": 8791.279201572432,
      "learning_rate": 1.4974960338308504e-07,
      "loss": 1.7064,
      "step": 446368
    },
    {
      "epoch": 0.0016200741941720474,
      "grad_norm": 9592.31129603288,
      "learning_rate": 1.497442306699589e-07,
      "loss": 1.6831,
      "step": 446400
    },
    {
      "epoch": 0.001620190328522884,
      "grad_norm": 19519.5376994436,
      "learning_rate": 1.4973885853507747e-07,
      "loss": 1.7096,
      "step": 446432
    },
    {
      "epoch": 0.0016203064628737206,
      "grad_norm": 20687.44044100188,
      "learning_rate": 1.497334869783371e-07,
      "loss": 1.7013,
      "step": 446464
    },
    {
      "epoch": 0.0016204225972245574,
      "grad_norm": 22694.248786862277,
      "learning_rate": 1.4972811599963413e-07,
      "loss": 1.6814,
      "step": 446496
    },
    {
      "epoch": 0.0016205387315753942,
      "grad_norm": 19574.476442551408,
      "learning_rate": 1.4972274559886487e-07,
      "loss": 1.6796,
      "step": 446528
    },
    {
      "epoch": 0.001620654865926231,
      "grad_norm": 22323.945171048956,
      "learning_rate": 1.4971737577592564e-07,
      "loss": 1.6692,
      "step": 446560
    },
    {
      "epoch": 0.0016207710002770674,
      "grad_norm": 8960.310262485335,
      "learning_rate": 1.497121743108819e-07,
      "loss": 1.6717,
      "step": 446592
    },
    {
      "epoch": 0.0016208871346279042,
      "grad_norm": 10483.790726640818,
      "learning_rate": 1.4970680562524286e-07,
      "loss": 1.6672,
      "step": 446624
    },
    {
      "epoch": 0.001621003268978741,
      "grad_norm": 8330.46481296212,
      "learning_rate": 1.4970143751712637e-07,
      "loss": 1.6891,
      "step": 446656
    },
    {
      "epoch": 0.0016211194033295777,
      "grad_norm": 10960.864746907517,
      "learning_rate": 1.4969606998642885e-07,
      "loss": 1.6784,
      "step": 446688
    },
    {
      "epoch": 0.0016212355376804142,
      "grad_norm": 8534.150924374375,
      "learning_rate": 1.4969070303304685e-07,
      "loss": 1.6969,
      "step": 446720
    },
    {
      "epoch": 0.001621351672031251,
      "grad_norm": 8225.363700165482,
      "learning_rate": 1.4968533665687686e-07,
      "loss": 1.6938,
      "step": 446752
    },
    {
      "epoch": 0.0016214678063820878,
      "grad_norm": 9117.596942177253,
      "learning_rate": 1.4967997085781544e-07,
      "loss": 1.6962,
      "step": 446784
    },
    {
      "epoch": 0.0016215839407329245,
      "grad_norm": 10669.372240202327,
      "learning_rate": 1.4967460563575915e-07,
      "loss": 1.6829,
      "step": 446816
    },
    {
      "epoch": 0.0016217000750837613,
      "grad_norm": 11152.536034463192,
      "learning_rate": 1.4966924099060457e-07,
      "loss": 1.6557,
      "step": 446848
    },
    {
      "epoch": 0.0016218162094345978,
      "grad_norm": 8121.285612512344,
      "learning_rate": 1.4966387692224832e-07,
      "loss": 1.65,
      "step": 446880
    },
    {
      "epoch": 0.0016219323437854346,
      "grad_norm": 9769.656493449502,
      "learning_rate": 1.4965851343058707e-07,
      "loss": 1.6485,
      "step": 446912
    },
    {
      "epoch": 0.0016220484781362713,
      "grad_norm": 9244.469049112557,
      "learning_rate": 1.4965315051551747e-07,
      "loss": 1.6624,
      "step": 446944
    },
    {
      "epoch": 0.001622164612487108,
      "grad_norm": 10916.360199260558,
      "learning_rate": 1.4964778817693623e-07,
      "loss": 1.6679,
      "step": 446976
    },
    {
      "epoch": 0.0016222807468379446,
      "grad_norm": 11062.218041604496,
      "learning_rate": 1.496424264147401e-07,
      "loss": 1.653,
      "step": 447008
    },
    {
      "epoch": 0.0016223968811887814,
      "grad_norm": 8197.926689108655,
      "learning_rate": 1.4963706522882577e-07,
      "loss": 1.6676,
      "step": 447040
    },
    {
      "epoch": 0.0016225130155396181,
      "grad_norm": 11804.898983049368,
      "learning_rate": 1.4963170461909007e-07,
      "loss": 1.6743,
      "step": 447072
    },
    {
      "epoch": 0.0016226291498904549,
      "grad_norm": 8231.604703822943,
      "learning_rate": 1.4962634458542976e-07,
      "loss": 1.669,
      "step": 447104
    },
    {
      "epoch": 0.0016227452842412916,
      "grad_norm": 9984.027644192498,
      "learning_rate": 1.496209851277417e-07,
      "loss": 1.6756,
      "step": 447136
    },
    {
      "epoch": 0.0016228614185921282,
      "grad_norm": 10128.482709665846,
      "learning_rate": 1.4961562624592276e-07,
      "loss": 1.6568,
      "step": 447168
    },
    {
      "epoch": 0.001622977552942965,
      "grad_norm": 9423.09556356084,
      "learning_rate": 1.4961026793986976e-07,
      "loss": 1.6717,
      "step": 447200
    },
    {
      "epoch": 0.0016230936872938017,
      "grad_norm": 10369.55621036889,
      "learning_rate": 1.4960491020947965e-07,
      "loss": 1.6933,
      "step": 447232
    },
    {
      "epoch": 0.0016232098216446384,
      "grad_norm": 11847.45229996728,
      "learning_rate": 1.4959955305464936e-07,
      "loss": 1.7006,
      "step": 447264
    },
    {
      "epoch": 0.001623325955995475,
      "grad_norm": 9131.71856771769,
      "learning_rate": 1.4959419647527583e-07,
      "loss": 1.6863,
      "step": 447296
    },
    {
      "epoch": 0.0016234420903463117,
      "grad_norm": 8783.138163549518,
      "learning_rate": 1.4958884047125605e-07,
      "loss": 1.7042,
      "step": 447328
    },
    {
      "epoch": 0.0016235582246971485,
      "grad_norm": 9753.93397558134,
      "learning_rate": 1.4958348504248702e-07,
      "loss": 1.6877,
      "step": 447360
    },
    {
      "epoch": 0.0016236743590479852,
      "grad_norm": 10068.282872466387,
      "learning_rate": 1.4957813018886578e-07,
      "loss": 1.6693,
      "step": 447392
    },
    {
      "epoch": 0.001623790493398822,
      "grad_norm": 9741.29560171541,
      "learning_rate": 1.4957277591028944e-07,
      "loss": 1.6742,
      "step": 447424
    },
    {
      "epoch": 0.0016239066277496585,
      "grad_norm": 13573.37732474862,
      "learning_rate": 1.49567422206655e-07,
      "loss": 1.6465,
      "step": 447456
    },
    {
      "epoch": 0.0016240227621004953,
      "grad_norm": 13160.945254805978,
      "learning_rate": 1.495620690778596e-07,
      "loss": 1.6493,
      "step": 447488
    },
    {
      "epoch": 0.001624138896451332,
      "grad_norm": 9556.804905406409,
      "learning_rate": 1.4955671652380043e-07,
      "loss": 1.6681,
      "step": 447520
    },
    {
      "epoch": 0.0016242550308021688,
      "grad_norm": 7739.860722261093,
      "learning_rate": 1.4955136454437459e-07,
      "loss": 1.6755,
      "step": 447552
    },
    {
      "epoch": 0.0016243711651530053,
      "grad_norm": 13411.145663216099,
      "learning_rate": 1.495460131394793e-07,
      "loss": 1.6654,
      "step": 447584
    },
    {
      "epoch": 0.001624487299503842,
      "grad_norm": 19058.614849983194,
      "learning_rate": 1.4954066230901175e-07,
      "loss": 1.6696,
      "step": 447616
    },
    {
      "epoch": 0.0016246034338546788,
      "grad_norm": 19223.824801532082,
      "learning_rate": 1.4953531205286922e-07,
      "loss": 1.6672,
      "step": 447648
    },
    {
      "epoch": 0.0016247195682055156,
      "grad_norm": 17752.830534875276,
      "learning_rate": 1.4952996237094896e-07,
      "loss": 1.6702,
      "step": 447680
    },
    {
      "epoch": 0.0016248357025563523,
      "grad_norm": 11906.851137055506,
      "learning_rate": 1.4952478041407775e-07,
      "loss": 1.6655,
      "step": 447712
    },
    {
      "epoch": 0.0016249518369071889,
      "grad_norm": 8576.989798291706,
      "learning_rate": 1.4951943186235745e-07,
      "loss": 1.6562,
      "step": 447744
    },
    {
      "epoch": 0.0016250679712580256,
      "grad_norm": 10988.451028238693,
      "learning_rate": 1.4951408388455458e-07,
      "loss": 1.6539,
      "step": 447776
    },
    {
      "epoch": 0.0016251841056088624,
      "grad_norm": 11901.183134461884,
      "learning_rate": 1.4950873648056646e-07,
      "loss": 1.6527,
      "step": 447808
    },
    {
      "epoch": 0.0016253002399596991,
      "grad_norm": 9857.475944682797,
      "learning_rate": 1.4950338965029059e-07,
      "loss": 1.6819,
      "step": 447840
    },
    {
      "epoch": 0.0016254163743105357,
      "grad_norm": 11891.720817442696,
      "learning_rate": 1.494980433936243e-07,
      "loss": 1.675,
      "step": 447872
    },
    {
      "epoch": 0.0016255325086613724,
      "grad_norm": 7762.204454921295,
      "learning_rate": 1.4949269771046507e-07,
      "loss": 1.6681,
      "step": 447904
    },
    {
      "epoch": 0.0016256486430122092,
      "grad_norm": 9418.604142865332,
      "learning_rate": 1.4948735260071037e-07,
      "loss": 1.6689,
      "step": 447936
    },
    {
      "epoch": 0.001625764777363046,
      "grad_norm": 9913.145817549543,
      "learning_rate": 1.4948200806425768e-07,
      "loss": 1.6655,
      "step": 447968
    },
    {
      "epoch": 0.0016258809117138827,
      "grad_norm": 8957.361665133321,
      "learning_rate": 1.4947666410100456e-07,
      "loss": 1.6749,
      "step": 448000
    },
    {
      "epoch": 0.0016259970460647192,
      "grad_norm": 7549.502235247037,
      "learning_rate": 1.4947132071084852e-07,
      "loss": 1.6726,
      "step": 448032
    },
    {
      "epoch": 0.001626113180415556,
      "grad_norm": 9545.143791478471,
      "learning_rate": 1.4946597789368717e-07,
      "loss": 1.6709,
      "step": 448064
    },
    {
      "epoch": 0.0016262293147663927,
      "grad_norm": 9825.102543994133,
      "learning_rate": 1.4946063564941806e-07,
      "loss": 1.6704,
      "step": 448096
    },
    {
      "epoch": 0.0016263454491172295,
      "grad_norm": 10101.867352128516,
      "learning_rate": 1.4945529397793883e-07,
      "loss": 1.6895,
      "step": 448128
    },
    {
      "epoch": 0.001626461583468066,
      "grad_norm": 9637.032738348458,
      "learning_rate": 1.4944995287914719e-07,
      "loss": 1.6786,
      "step": 448160
    },
    {
      "epoch": 0.0016265777178189028,
      "grad_norm": 9576.214701018353,
      "learning_rate": 1.4944461235294073e-07,
      "loss": 1.673,
      "step": 448192
    },
    {
      "epoch": 0.0016266938521697395,
      "grad_norm": 9163.239820063643,
      "learning_rate": 1.4943927239921724e-07,
      "loss": 1.6767,
      "step": 448224
    },
    {
      "epoch": 0.0016268099865205763,
      "grad_norm": 8705.849987221236,
      "learning_rate": 1.4943393301787434e-07,
      "loss": 1.6747,
      "step": 448256
    },
    {
      "epoch": 0.001626926120871413,
      "grad_norm": 8563.137392334658,
      "learning_rate": 1.4942859420880986e-07,
      "loss": 1.6757,
      "step": 448288
    },
    {
      "epoch": 0.0016270422552222496,
      "grad_norm": 8157.194493206595,
      "learning_rate": 1.4942325597192158e-07,
      "loss": 1.6702,
      "step": 448320
    },
    {
      "epoch": 0.0016271583895730863,
      "grad_norm": 14969.860119586956,
      "learning_rate": 1.4941791830710732e-07,
      "loss": 1.6557,
      "step": 448352
    },
    {
      "epoch": 0.001627274523923923,
      "grad_norm": 9065.145558676926,
      "learning_rate": 1.4941258121426482e-07,
      "loss": 1.6611,
      "step": 448384
    },
    {
      "epoch": 0.0016273906582747598,
      "grad_norm": 10638.363971964862,
      "learning_rate": 1.4940724469329201e-07,
      "loss": 1.6822,
      "step": 448416
    },
    {
      "epoch": 0.0016275067926255964,
      "grad_norm": 9318.694007209378,
      "learning_rate": 1.4940190874408675e-07,
      "loss": 1.693,
      "step": 448448
    },
    {
      "epoch": 0.0016276229269764331,
      "grad_norm": 10039.9854581568,
      "learning_rate": 1.4939657336654695e-07,
      "loss": 1.6608,
      "step": 448480
    },
    {
      "epoch": 0.0016277390613272699,
      "grad_norm": 9518.215589069203,
      "learning_rate": 1.4939123856057054e-07,
      "loss": 1.6582,
      "step": 448512
    },
    {
      "epoch": 0.0016278551956781066,
      "grad_norm": 10348.364315194938,
      "learning_rate": 1.4938590432605544e-07,
      "loss": 1.6685,
      "step": 448544
    },
    {
      "epoch": 0.0016279713300289434,
      "grad_norm": 13513.517380756202,
      "learning_rate": 1.4938057066289974e-07,
      "loss": 1.6715,
      "step": 448576
    },
    {
      "epoch": 0.00162808746437978,
      "grad_norm": 11282.362075381201,
      "learning_rate": 1.4937523757100133e-07,
      "loss": 1.6821,
      "step": 448608
    },
    {
      "epoch": 0.0016282035987306167,
      "grad_norm": 13152.581495660843,
      "learning_rate": 1.493699050502583e-07,
      "loss": 1.6731,
      "step": 448640
    },
    {
      "epoch": 0.0016283197330814534,
      "grad_norm": 9788.059664713941,
      "learning_rate": 1.4936457310056873e-07,
      "loss": 1.6864,
      "step": 448672
    },
    {
      "epoch": 0.0016284358674322902,
      "grad_norm": 17590.132688527396,
      "learning_rate": 1.4935924172183066e-07,
      "loss": 1.6771,
      "step": 448704
    },
    {
      "epoch": 0.0016285520017831267,
      "grad_norm": 19422.158067526892,
      "learning_rate": 1.493539109139422e-07,
      "loss": 1.6775,
      "step": 448736
    },
    {
      "epoch": 0.0016286681361339635,
      "grad_norm": 9331.527742015238,
      "learning_rate": 1.4934874723807394e-07,
      "loss": 1.6774,
      "step": 448768
    },
    {
      "epoch": 0.0016287842704848002,
      "grad_norm": 10099.457411168187,
      "learning_rate": 1.4934341755374805e-07,
      "loss": 1.6703,
      "step": 448800
    },
    {
      "epoch": 0.001628900404835637,
      "grad_norm": 10401.247425188962,
      "learning_rate": 1.4933808843996944e-07,
      "loss": 1.6831,
      "step": 448832
    },
    {
      "epoch": 0.0016290165391864737,
      "grad_norm": 8577.854510307341,
      "learning_rate": 1.4933275989663634e-07,
      "loss": 1.6936,
      "step": 448864
    },
    {
      "epoch": 0.0016291326735373103,
      "grad_norm": 9513.59395812119,
      "learning_rate": 1.4932743192364697e-07,
      "loss": 1.6947,
      "step": 448896
    },
    {
      "epoch": 0.001629248807888147,
      "grad_norm": 10707.511008633146,
      "learning_rate": 1.493221045208996e-07,
      "loss": 1.6979,
      "step": 448928
    },
    {
      "epoch": 0.0016293649422389838,
      "grad_norm": 9526.2076399793,
      "learning_rate": 1.4931677768829254e-07,
      "loss": 1.7086,
      "step": 448960
    },
    {
      "epoch": 0.0016294810765898205,
      "grad_norm": 10240.363860722919,
      "learning_rate": 1.4931145142572408e-07,
      "loss": 1.7083,
      "step": 448992
    },
    {
      "epoch": 0.001629597210940657,
      "grad_norm": 9595.237777147579,
      "learning_rate": 1.4930612573309254e-07,
      "loss": 1.6983,
      "step": 449024
    },
    {
      "epoch": 0.0016297133452914938,
      "grad_norm": 9549.625542396938,
      "learning_rate": 1.4930080061029633e-07,
      "loss": 1.7069,
      "step": 449056
    },
    {
      "epoch": 0.0016298294796423306,
      "grad_norm": 9871.639782731134,
      "learning_rate": 1.492954760572338e-07,
      "loss": 1.7176,
      "step": 449088
    },
    {
      "epoch": 0.0016299456139931673,
      "grad_norm": 10011.064478865372,
      "learning_rate": 1.492901520738034e-07,
      "loss": 1.6668,
      "step": 449120
    },
    {
      "epoch": 0.001630061748344004,
      "grad_norm": 10564.480772853913,
      "learning_rate": 1.492848286599035e-07,
      "loss": 1.6785,
      "step": 449152
    },
    {
      "epoch": 0.0016301778826948406,
      "grad_norm": 9854.215747587425,
      "learning_rate": 1.4927950581543266e-07,
      "loss": 1.6764,
      "step": 449184
    },
    {
      "epoch": 0.0016302940170456774,
      "grad_norm": 10882.455605239104,
      "learning_rate": 1.492741835402893e-07,
      "loss": 1.6724,
      "step": 449216
    },
    {
      "epoch": 0.0016304101513965141,
      "grad_norm": 12498.94619557985,
      "learning_rate": 1.4926886183437199e-07,
      "loss": 1.6666,
      "step": 449248
    },
    {
      "epoch": 0.001630526285747351,
      "grad_norm": 19285.684639130654,
      "learning_rate": 1.4926354069757923e-07,
      "loss": 1.6618,
      "step": 449280
    },
    {
      "epoch": 0.0016306424200981874,
      "grad_norm": 9630.491368564743,
      "learning_rate": 1.4925822012980958e-07,
      "loss": 1.6815,
      "step": 449312
    },
    {
      "epoch": 0.0016307585544490242,
      "grad_norm": 10884.592596877477,
      "learning_rate": 1.4925290013096167e-07,
      "loss": 1.6877,
      "step": 449344
    },
    {
      "epoch": 0.001630874688799861,
      "grad_norm": 8045.832461591529,
      "learning_rate": 1.4924758070093408e-07,
      "loss": 1.6796,
      "step": 449376
    },
    {
      "epoch": 0.0016309908231506977,
      "grad_norm": 9730.297837168193,
      "learning_rate": 1.4924226183962546e-07,
      "loss": 1.6784,
      "step": 449408
    },
    {
      "epoch": 0.0016311069575015344,
      "grad_norm": 9201.226222629242,
      "learning_rate": 1.4923694354693453e-07,
      "loss": 1.6788,
      "step": 449440
    },
    {
      "epoch": 0.001631223091852371,
      "grad_norm": 10262.557868289952,
      "learning_rate": 1.492316258227599e-07,
      "loss": 1.6726,
      "step": 449472
    },
    {
      "epoch": 0.0016313392262032077,
      "grad_norm": 11297.740305034455,
      "learning_rate": 1.4922630866700032e-07,
      "loss": 1.6918,
      "step": 449504
    },
    {
      "epoch": 0.0016314553605540445,
      "grad_norm": 13347.25394978308,
      "learning_rate": 1.4922099207955459e-07,
      "loss": 1.6977,
      "step": 449536
    },
    {
      "epoch": 0.0016315714949048812,
      "grad_norm": 10744.416782682994,
      "learning_rate": 1.4921567606032137e-07,
      "loss": 1.6962,
      "step": 449568
    },
    {
      "epoch": 0.0016316876292557178,
      "grad_norm": 13207.959569895722,
      "learning_rate": 1.4921036060919955e-07,
      "loss": 1.682,
      "step": 449600
    },
    {
      "epoch": 0.0016318037636065545,
      "grad_norm": 14385.445283341076,
      "learning_rate": 1.492050457260879e-07,
      "loss": 1.677,
      "step": 449632
    },
    {
      "epoch": 0.0016319198979573913,
      "grad_norm": 8738.97431052409,
      "learning_rate": 1.491997314108853e-07,
      "loss": 1.6818,
      "step": 449664
    },
    {
      "epoch": 0.001632036032308228,
      "grad_norm": 10372.292899836564,
      "learning_rate": 1.4919441766349055e-07,
      "loss": 1.6877,
      "step": 449696
    },
    {
      "epoch": 0.0016321521666590648,
      "grad_norm": 9031.730952591535,
      "learning_rate": 1.4918910448380262e-07,
      "loss": 1.6767,
      "step": 449728
    },
    {
      "epoch": 0.0016322683010099013,
      "grad_norm": 19153.031718242415,
      "learning_rate": 1.4918379187172041e-07,
      "loss": 1.6892,
      "step": 449760
    },
    {
      "epoch": 0.001632384435360738,
      "grad_norm": 17367.494234920592,
      "learning_rate": 1.4917847982714284e-07,
      "loss": 1.6881,
      "step": 449792
    },
    {
      "epoch": 0.0016325005697115748,
      "grad_norm": 10404.8146547644,
      "learning_rate": 1.4917333432504297e-07,
      "loss": 1.6864,
      "step": 449824
    },
    {
      "epoch": 0.0016326167040624116,
      "grad_norm": 11647.326731915784,
      "learning_rate": 1.49168023397445e-07,
      "loss": 1.6966,
      "step": 449856
    },
    {
      "epoch": 0.0016327328384132481,
      "grad_norm": 14163.151767879915,
      "learning_rate": 1.491627130370518e-07,
      "loss": 1.6987,
      "step": 449888
    },
    {
      "epoch": 0.0016328489727640849,
      "grad_norm": 11428.240459493316,
      "learning_rate": 1.4915740324376245e-07,
      "loss": 1.7026,
      "step": 449920
    },
    {
      "epoch": 0.0016329651071149216,
      "grad_norm": 10318.297533992709,
      "learning_rate": 1.4915209401747602e-07,
      "loss": 1.6854,
      "step": 449952
    },
    {
      "epoch": 0.0016330812414657584,
      "grad_norm": 9722.286973752627,
      "learning_rate": 1.491467853580916e-07,
      "loss": 1.6776,
      "step": 449984
    },
    {
      "epoch": 0.0016331973758165952,
      "grad_norm": 9985.3358481325,
      "learning_rate": 1.491414772655083e-07,
      "loss": 1.6861,
      "step": 450016
    },
    {
      "epoch": 0.0016333135101674317,
      "grad_norm": 9376.126918936197,
      "learning_rate": 1.491361697396253e-07,
      "loss": 1.703,
      "step": 450048
    },
    {
      "epoch": 0.0016334296445182684,
      "grad_norm": 10809.265284930332,
      "learning_rate": 1.4913086278034172e-07,
      "loss": 1.6958,
      "step": 450080
    },
    {
      "epoch": 0.0016335457788691052,
      "grad_norm": 11217.758599649042,
      "learning_rate": 1.491255563875568e-07,
      "loss": 1.6714,
      "step": 450112
    },
    {
      "epoch": 0.001633661913219942,
      "grad_norm": 9367.587949947414,
      "learning_rate": 1.491202505611697e-07,
      "loss": 1.6696,
      "step": 450144
    },
    {
      "epoch": 0.0016337780475707785,
      "grad_norm": 10889.862625396154,
      "learning_rate": 1.4911494530107977e-07,
      "loss": 1.6685,
      "step": 450176
    },
    {
      "epoch": 0.0016338941819216152,
      "grad_norm": 9489.915911113227,
      "learning_rate": 1.4910964060718623e-07,
      "loss": 1.6916,
      "step": 450208
    },
    {
      "epoch": 0.001634010316272452,
      "grad_norm": 11517.435652088532,
      "learning_rate": 1.491043364793883e-07,
      "loss": 1.6862,
      "step": 450240
    },
    {
      "epoch": 0.0016341264506232888,
      "grad_norm": 12439.527804543064,
      "learning_rate": 1.4909903291758543e-07,
      "loss": 1.6584,
      "step": 450272
    },
    {
      "epoch": 0.0016342425849741255,
      "grad_norm": 9666.122076613765,
      "learning_rate": 1.4909372992167687e-07,
      "loss": 1.6669,
      "step": 450304
    },
    {
      "epoch": 0.001634358719324962,
      "grad_norm": 11364.601708814964,
      "learning_rate": 1.4908842749156205e-07,
      "loss": 1.6719,
      "step": 450336
    },
    {
      "epoch": 0.0016344748536757988,
      "grad_norm": 13788.218739199056,
      "learning_rate": 1.4908312562714032e-07,
      "loss": 1.6768,
      "step": 450368
    },
    {
      "epoch": 0.0016345909880266356,
      "grad_norm": 9963.41347129587,
      "learning_rate": 1.4907782432831116e-07,
      "loss": 1.6745,
      "step": 450400
    },
    {
      "epoch": 0.0016347071223774723,
      "grad_norm": 10504.243713852034,
      "learning_rate": 1.4907252359497393e-07,
      "loss": 1.6638,
      "step": 450432
    },
    {
      "epoch": 0.0016348232567283088,
      "grad_norm": 10096.737096706045,
      "learning_rate": 1.490672234270282e-07,
      "loss": 1.6739,
      "step": 450464
    },
    {
      "epoch": 0.0016349393910791456,
      "grad_norm": 10704.56930474085,
      "learning_rate": 1.490619238243734e-07,
      "loss": 1.6867,
      "step": 450496
    },
    {
      "epoch": 0.0016350555254299824,
      "grad_norm": 12363.793754345792,
      "learning_rate": 1.4905662478690906e-07,
      "loss": 1.704,
      "step": 450528
    },
    {
      "epoch": 0.001635171659780819,
      "grad_norm": 9956.939288757363,
      "learning_rate": 1.4905132631453477e-07,
      "loss": 1.714,
      "step": 450560
    },
    {
      "epoch": 0.0016352877941316559,
      "grad_norm": 8041.212719484543,
      "learning_rate": 1.4904602840715004e-07,
      "loss": 1.6941,
      "step": 450592
    },
    {
      "epoch": 0.0016354039284824924,
      "grad_norm": 15849.365286976006,
      "learning_rate": 1.4904073106465452e-07,
      "loss": 1.6905,
      "step": 450624
    },
    {
      "epoch": 0.0016355200628333292,
      "grad_norm": 10696.852901671595,
      "learning_rate": 1.490354342869478e-07,
      "loss": 1.6737,
      "step": 450656
    },
    {
      "epoch": 0.001635636197184166,
      "grad_norm": 13816.703224720433,
      "learning_rate": 1.4903013807392954e-07,
      "loss": 1.6851,
      "step": 450688
    },
    {
      "epoch": 0.0016357523315350027,
      "grad_norm": 10567.808192808952,
      "learning_rate": 1.490248424254994e-07,
      "loss": 1.6867,
      "step": 450720
    },
    {
      "epoch": 0.0016358684658858392,
      "grad_norm": 11459.340120617766,
      "learning_rate": 1.4901954734155708e-07,
      "loss": 1.6923,
      "step": 450752
    },
    {
      "epoch": 0.001635984600236676,
      "grad_norm": 12257.570232309501,
      "learning_rate": 1.490142528220023e-07,
      "loss": 1.6979,
      "step": 450784
    },
    {
      "epoch": 0.0016361007345875127,
      "grad_norm": 10072.734881848128,
      "learning_rate": 1.4900895886673484e-07,
      "loss": 1.7001,
      "step": 450816
    },
    {
      "epoch": 0.0016362168689383495,
      "grad_norm": 11326.888010393675,
      "learning_rate": 1.4900383088558673e-07,
      "loss": 1.6899,
      "step": 450848
    },
    {
      "epoch": 0.0016363330032891862,
      "grad_norm": 12016.844094852859,
      "learning_rate": 1.4899853804096697e-07,
      "loss": 1.6705,
      "step": 450880
    },
    {
      "epoch": 0.0016364491376400227,
      "grad_norm": 10678.140287521981,
      "learning_rate": 1.4899324576033704e-07,
      "loss": 1.6615,
      "step": 450912
    },
    {
      "epoch": 0.0016365652719908595,
      "grad_norm": 10301.602399626963,
      "learning_rate": 1.4898795404359674e-07,
      "loss": 1.6645,
      "step": 450944
    },
    {
      "epoch": 0.0016366814063416963,
      "grad_norm": 11250.639626261256,
      "learning_rate": 1.4898266289064596e-07,
      "loss": 1.6692,
      "step": 450976
    },
    {
      "epoch": 0.001636797540692533,
      "grad_norm": 10696.746421225474,
      "learning_rate": 1.4897737230138464e-07,
      "loss": 1.6794,
      "step": 451008
    },
    {
      "epoch": 0.0016369136750433695,
      "grad_norm": 8237.099489504786,
      "learning_rate": 1.4897208227571264e-07,
      "loss": 1.7084,
      "step": 451040
    },
    {
      "epoch": 0.0016370298093942063,
      "grad_norm": 10984.349958008439,
      "learning_rate": 1.489667928135299e-07,
      "loss": 1.6834,
      "step": 451072
    },
    {
      "epoch": 0.001637145943745043,
      "grad_norm": 10517.730363533761,
      "learning_rate": 1.4896150391473645e-07,
      "loss": 1.7073,
      "step": 451104
    },
    {
      "epoch": 0.0016372620780958798,
      "grad_norm": 9785.228357069649,
      "learning_rate": 1.4895621557923222e-07,
      "loss": 1.7005,
      "step": 451136
    },
    {
      "epoch": 0.0016373782124467166,
      "grad_norm": 9019.150957823025,
      "learning_rate": 1.489509278069173e-07,
      "loss": 1.6873,
      "step": 451168
    },
    {
      "epoch": 0.001637494346797553,
      "grad_norm": 12082.567773449484,
      "learning_rate": 1.4894564059769165e-07,
      "loss": 1.6808,
      "step": 451200
    },
    {
      "epoch": 0.0016376104811483899,
      "grad_norm": 12956.171811148539,
      "learning_rate": 1.4894035395145543e-07,
      "loss": 1.6731,
      "step": 451232
    },
    {
      "epoch": 0.0016377266154992266,
      "grad_norm": 9051.772644073646,
      "learning_rate": 1.4893506786810867e-07,
      "loss": 1.6828,
      "step": 451264
    },
    {
      "epoch": 0.0016378427498500634,
      "grad_norm": 9917.667669366623,
      "learning_rate": 1.4892978234755146e-07,
      "loss": 1.6938,
      "step": 451296
    },
    {
      "epoch": 0.0016379588842009,
      "grad_norm": 9052.372064823672,
      "learning_rate": 1.4892449738968404e-07,
      "loss": 1.6908,
      "step": 451328
    },
    {
      "epoch": 0.0016380750185517367,
      "grad_norm": 10136.075571936113,
      "learning_rate": 1.489192129944065e-07,
      "loss": 1.6895,
      "step": 451360
    },
    {
      "epoch": 0.0016381911529025734,
      "grad_norm": 11294.635894972445,
      "learning_rate": 1.489139291616191e-07,
      "loss": 1.6834,
      "step": 451392
    },
    {
      "epoch": 0.0016383072872534102,
      "grad_norm": 9518.112838162826,
      "learning_rate": 1.4890864589122198e-07,
      "loss": 1.6745,
      "step": 451424
    },
    {
      "epoch": 0.001638423421604247,
      "grad_norm": 10349.337273468287,
      "learning_rate": 1.4890336318311543e-07,
      "loss": 1.6814,
      "step": 451456
    },
    {
      "epoch": 0.0016385395559550835,
      "grad_norm": 11784.784766808429,
      "learning_rate": 1.4889808103719972e-07,
      "loss": 1.6912,
      "step": 451488
    },
    {
      "epoch": 0.0016386556903059202,
      "grad_norm": 8534.725537473363,
      "learning_rate": 1.4889279945337512e-07,
      "loss": 1.7165,
      "step": 451520
    },
    {
      "epoch": 0.001638771824656757,
      "grad_norm": 14158.043791428248,
      "learning_rate": 1.4888751843154195e-07,
      "loss": 1.6949,
      "step": 451552
    },
    {
      "epoch": 0.0016388879590075937,
      "grad_norm": 13416.404361825116,
      "learning_rate": 1.4888223797160059e-07,
      "loss": 1.7017,
      "step": 451584
    },
    {
      "epoch": 0.0016390040933584303,
      "grad_norm": 12522.745385896817,
      "learning_rate": 1.4887695807345136e-07,
      "loss": 1.715,
      "step": 451616
    },
    {
      "epoch": 0.001639120227709267,
      "grad_norm": 8737.12092167666,
      "learning_rate": 1.4887167873699467e-07,
      "loss": 1.7199,
      "step": 451648
    },
    {
      "epoch": 0.0016392363620601038,
      "grad_norm": 9323.44024488815,
      "learning_rate": 1.488663999621309e-07,
      "loss": 1.7294,
      "step": 451680
    },
    {
      "epoch": 0.0016393524964109405,
      "grad_norm": 8668.025034573908,
      "learning_rate": 1.4886112174876057e-07,
      "loss": 1.7028,
      "step": 451712
    },
    {
      "epoch": 0.0016394686307617773,
      "grad_norm": 9819.159841860199,
      "learning_rate": 1.4885584409678409e-07,
      "loss": 1.6616,
      "step": 451744
    },
    {
      "epoch": 0.0016395847651126138,
      "grad_norm": 12532.39753598648,
      "learning_rate": 1.4885056700610195e-07,
      "loss": 1.665,
      "step": 451776
    },
    {
      "epoch": 0.0016397008994634506,
      "grad_norm": 8211.574514062453,
      "learning_rate": 1.4884529047661467e-07,
      "loss": 1.6692,
      "step": 451808
    },
    {
      "epoch": 0.0016398170338142873,
      "grad_norm": 10459.054068126812,
      "learning_rate": 1.488400145082228e-07,
      "loss": 1.6745,
      "step": 451840
    },
    {
      "epoch": 0.001639933168165124,
      "grad_norm": 21395.687042018537,
      "learning_rate": 1.488347391008269e-07,
      "loss": 1.6756,
      "step": 451872
    },
    {
      "epoch": 0.0016400493025159606,
      "grad_norm": 20146.65232737191,
      "learning_rate": 1.4882946425432755e-07,
      "loss": 1.6506,
      "step": 451904
    },
    {
      "epoch": 0.0016401654368667974,
      "grad_norm": 11750.561178088474,
      "learning_rate": 1.4882435478156593e-07,
      "loss": 1.6665,
      "step": 451936
    },
    {
      "epoch": 0.0016402815712176341,
      "grad_norm": 8503.942850231297,
      "learning_rate": 1.488190810390413e-07,
      "loss": 1.6875,
      "step": 451968
    },
    {
      "epoch": 0.0016403977055684709,
      "grad_norm": 10761.328728368075,
      "learning_rate": 1.4881380785711816e-07,
      "loss": 1.701,
      "step": 452000
    },
    {
      "epoch": 0.0016405138399193076,
      "grad_norm": 11419.910857795696,
      "learning_rate": 1.4880853523569735e-07,
      "loss": 1.6705,
      "step": 452032
    },
    {
      "epoch": 0.0016406299742701442,
      "grad_norm": 12608.500148709203,
      "learning_rate": 1.488032631746795e-07,
      "loss": 1.6633,
      "step": 452064
    },
    {
      "epoch": 0.001640746108620981,
      "grad_norm": 9934.213607528278,
      "learning_rate": 1.4879799167396535e-07,
      "loss": 1.6698,
      "step": 452096
    },
    {
      "epoch": 0.0016408622429718177,
      "grad_norm": 9727.187671675714,
      "learning_rate": 1.4879272073345567e-07,
      "loss": 1.6674,
      "step": 452128
    },
    {
      "epoch": 0.0016409783773226544,
      "grad_norm": 12158.349230055863,
      "learning_rate": 1.4878745035305126e-07,
      "loss": 1.683,
      "step": 452160
    },
    {
      "epoch": 0.001641094511673491,
      "grad_norm": 9108.362970369593,
      "learning_rate": 1.487821805326529e-07,
      "loss": 1.6864,
      "step": 452192
    },
    {
      "epoch": 0.0016412106460243277,
      "grad_norm": 9910.112007439673,
      "learning_rate": 1.4877691127216143e-07,
      "loss": 1.6858,
      "step": 452224
    },
    {
      "epoch": 0.0016413267803751645,
      "grad_norm": 10303.102251263937,
      "learning_rate": 1.4877164257147773e-07,
      "loss": 1.6837,
      "step": 452256
    },
    {
      "epoch": 0.0016414429147260012,
      "grad_norm": 12345.664016163732,
      "learning_rate": 1.4876637443050267e-07,
      "loss": 1.6638,
      "step": 452288
    },
    {
      "epoch": 0.001641559049076838,
      "grad_norm": 9719.840430788974,
      "learning_rate": 1.4876110684913714e-07,
      "loss": 1.6736,
      "step": 452320
    },
    {
      "epoch": 0.0016416751834276745,
      "grad_norm": 8514.119332027241,
      "learning_rate": 1.487558398272821e-07,
      "loss": 1.6725,
      "step": 452352
    },
    {
      "epoch": 0.0016417913177785113,
      "grad_norm": 11505.50546477642,
      "learning_rate": 1.4875057336483852e-07,
      "loss": 1.6718,
      "step": 452384
    },
    {
      "epoch": 0.001641907452129348,
      "grad_norm": 9586.275397671403,
      "learning_rate": 1.4874530746170736e-07,
      "loss": 1.6711,
      "step": 452416
    },
    {
      "epoch": 0.0016420235864801848,
      "grad_norm": 12684.616194430166,
      "learning_rate": 1.487400421177896e-07,
      "loss": 1.6811,
      "step": 452448
    },
    {
      "epoch": 0.0016421397208310213,
      "grad_norm": 10213.63089209709,
      "learning_rate": 1.487347773329863e-07,
      "loss": 1.7007,
      "step": 452480
    },
    {
      "epoch": 0.001642255855181858,
      "grad_norm": 10160.818372552478,
      "learning_rate": 1.4872951310719855e-07,
      "loss": 1.7067,
      "step": 452512
    },
    {
      "epoch": 0.0016423719895326948,
      "grad_norm": 8702.23281692693,
      "learning_rate": 1.4872424944032738e-07,
      "loss": 1.691,
      "step": 452544
    },
    {
      "epoch": 0.0016424881238835316,
      "grad_norm": 10608.369054666226,
      "learning_rate": 1.487189863322739e-07,
      "loss": 1.6906,
      "step": 452576
    },
    {
      "epoch": 0.0016426042582343683,
      "grad_norm": 12385.345211175989,
      "learning_rate": 1.487137237829392e-07,
      "loss": 1.6766,
      "step": 452608
    },
    {
      "epoch": 0.0016427203925852049,
      "grad_norm": 11767.742519277008,
      "learning_rate": 1.4870846179222453e-07,
      "loss": 1.6688,
      "step": 452640
    },
    {
      "epoch": 0.0016428365269360416,
      "grad_norm": 9079.744049256014,
      "learning_rate": 1.4870320036003103e-07,
      "loss": 1.669,
      "step": 452672
    },
    {
      "epoch": 0.0016429526612868784,
      "grad_norm": 12235.895226749859,
      "learning_rate": 1.4869793948625985e-07,
      "loss": 1.6676,
      "step": 452704
    },
    {
      "epoch": 0.0016430687956377151,
      "grad_norm": 9024.859444888878,
      "learning_rate": 1.4869267917081226e-07,
      "loss": 1.6887,
      "step": 452736
    },
    {
      "epoch": 0.0016431849299885517,
      "grad_norm": 12751.395845161423,
      "learning_rate": 1.486874194135895e-07,
      "loss": 1.684,
      "step": 452768
    },
    {
      "epoch": 0.0016433010643393884,
      "grad_norm": 9024.001994680631,
      "learning_rate": 1.486821602144929e-07,
      "loss": 1.684,
      "step": 452800
    },
    {
      "epoch": 0.0016434171986902252,
      "grad_norm": 8761.629528803418,
      "learning_rate": 1.4867690157342366e-07,
      "loss": 1.6779,
      "step": 452832
    },
    {
      "epoch": 0.001643533333041062,
      "grad_norm": 10013.646288939908,
      "learning_rate": 1.4867164349028318e-07,
      "loss": 1.677,
      "step": 452864
    },
    {
      "epoch": 0.0016436494673918987,
      "grad_norm": 10241.347762867934,
      "learning_rate": 1.4866638596497277e-07,
      "loss": 1.653,
      "step": 452896
    },
    {
      "epoch": 0.0016437656017427352,
      "grad_norm": 9127.740574753427,
      "learning_rate": 1.4866112899739384e-07,
      "loss": 1.6721,
      "step": 452928
    },
    {
      "epoch": 0.001643881736093572,
      "grad_norm": 10939.134335037668,
      "learning_rate": 1.4865603684181883e-07,
      "loss": 1.6861,
      "step": 452960
    },
    {
      "epoch": 0.0016439978704444087,
      "grad_norm": 11583.58580060596,
      "learning_rate": 1.4865078097198557e-07,
      "loss": 1.6857,
      "step": 452992
    },
    {
      "epoch": 0.0016441140047952455,
      "grad_norm": 9422.246229005055,
      "learning_rate": 1.486455256595911e-07,
      "loss": 1.6566,
      "step": 453024
    },
    {
      "epoch": 0.001644230139146082,
      "grad_norm": 9065.542234196475,
      "learning_rate": 1.4864027090453692e-07,
      "loss": 1.6639,
      "step": 453056
    },
    {
      "epoch": 0.0016443462734969188,
      "grad_norm": 9738.359410085459,
      "learning_rate": 1.4863501670672452e-07,
      "loss": 1.6687,
      "step": 453088
    },
    {
      "epoch": 0.0016444624078477555,
      "grad_norm": 9805.21147145741,
      "learning_rate": 1.486297630660554e-07,
      "loss": 1.6666,
      "step": 453120
    },
    {
      "epoch": 0.0016445785421985923,
      "grad_norm": 10562.79148710226,
      "learning_rate": 1.4862450998243113e-07,
      "loss": 1.669,
      "step": 453152
    },
    {
      "epoch": 0.001644694676549429,
      "grad_norm": 14818.267780007216,
      "learning_rate": 1.4861925745575322e-07,
      "loss": 1.6681,
      "step": 453184
    },
    {
      "epoch": 0.0016448108109002656,
      "grad_norm": 9974.989323302556,
      "learning_rate": 1.4861400548592332e-07,
      "loss": 1.6878,
      "step": 453216
    },
    {
      "epoch": 0.0016449269452511023,
      "grad_norm": 9682.998915625261,
      "learning_rate": 1.4860875407284305e-07,
      "loss": 1.6895,
      "step": 453248
    },
    {
      "epoch": 0.001645043079601939,
      "grad_norm": 12559.324981861088,
      "learning_rate": 1.48603503216414e-07,
      "loss": 1.6931,
      "step": 453280
    },
    {
      "epoch": 0.0016451592139527758,
      "grad_norm": 12046.950817530551,
      "learning_rate": 1.4859825291653786e-07,
      "loss": 1.7049,
      "step": 453312
    },
    {
      "epoch": 0.0016452753483036124,
      "grad_norm": 9593.777149798718,
      "learning_rate": 1.4859300317311633e-07,
      "loss": 1.684,
      "step": 453344
    },
    {
      "epoch": 0.0016453914826544491,
      "grad_norm": 12194.30424419532,
      "learning_rate": 1.4858775398605113e-07,
      "loss": 1.6777,
      "step": 453376
    },
    {
      "epoch": 0.0016455076170052859,
      "grad_norm": 14066.238018745453,
      "learning_rate": 1.4858250535524396e-07,
      "loss": 1.6868,
      "step": 453408
    },
    {
      "epoch": 0.0016456237513561226,
      "grad_norm": 10083.436517378388,
      "learning_rate": 1.485772572805966e-07,
      "loss": 1.6939,
      "step": 453440
    },
    {
      "epoch": 0.0016457398857069594,
      "grad_norm": 11169.227905276175,
      "learning_rate": 1.4857200976201087e-07,
      "loss": 1.7243,
      "step": 453472
    },
    {
      "epoch": 0.001645856020057796,
      "grad_norm": 8966.123354047722,
      "learning_rate": 1.4856676279938855e-07,
      "loss": 1.6687,
      "step": 453504
    },
    {
      "epoch": 0.0016459721544086327,
      "grad_norm": 8670.762826879767,
      "learning_rate": 1.4856151639263145e-07,
      "loss": 1.6654,
      "step": 453536
    },
    {
      "epoch": 0.0016460882887594694,
      "grad_norm": 11768.79195159809,
      "learning_rate": 1.485562705416415e-07,
      "loss": 1.6662,
      "step": 453568
    },
    {
      "epoch": 0.0016462044231103062,
      "grad_norm": 10468.128963668722,
      "learning_rate": 1.4855102524632052e-07,
      "loss": 1.6679,
      "step": 453600
    },
    {
      "epoch": 0.0016463205574611427,
      "grad_norm": 11237.300921484659,
      "learning_rate": 1.4854578050657042e-07,
      "loss": 1.6692,
      "step": 453632
    },
    {
      "epoch": 0.0016464366918119795,
      "grad_norm": 10852.661240451578,
      "learning_rate": 1.4854053632229317e-07,
      "loss": 1.6607,
      "step": 453664
    },
    {
      "epoch": 0.0016465528261628162,
      "grad_norm": 8322.821276466291,
      "learning_rate": 1.4853529269339073e-07,
      "loss": 1.68,
      "step": 453696
    },
    {
      "epoch": 0.001646668960513653,
      "grad_norm": 10828.101587997779,
      "learning_rate": 1.4853004961976502e-07,
      "loss": 1.6927,
      "step": 453728
    },
    {
      "epoch": 0.0016467850948644897,
      "grad_norm": 9811.884018882409,
      "learning_rate": 1.4852480710131811e-07,
      "loss": 1.6846,
      "step": 453760
    },
    {
      "epoch": 0.0016469012292153263,
      "grad_norm": 9103.508444550374,
      "learning_rate": 1.48519565137952e-07,
      "loss": 1.6869,
      "step": 453792
    },
    {
      "epoch": 0.001647017363566163,
      "grad_norm": 8043.848084095074,
      "learning_rate": 1.485143237295687e-07,
      "loss": 1.6907,
      "step": 453824
    },
    {
      "epoch": 0.0016471334979169998,
      "grad_norm": 10657.966879288,
      "learning_rate": 1.4850908287607038e-07,
      "loss": 1.6733,
      "step": 453856
    },
    {
      "epoch": 0.0016472496322678365,
      "grad_norm": 9626.236440063167,
      "learning_rate": 1.485038425773591e-07,
      "loss": 1.6666,
      "step": 453888
    },
    {
      "epoch": 0.001647365766618673,
      "grad_norm": 12346.872883447048,
      "learning_rate": 1.4849860283333692e-07,
      "loss": 1.6734,
      "step": 453920
    },
    {
      "epoch": 0.0016474819009695098,
      "grad_norm": 27316.57987376897,
      "learning_rate": 1.484933636439061e-07,
      "loss": 1.6898,
      "step": 453952
    },
    {
      "epoch": 0.0016475980353203466,
      "grad_norm": 17695.36662519316,
      "learning_rate": 1.4848812500896873e-07,
      "loss": 1.6746,
      "step": 453984
    },
    {
      "epoch": 0.0016477141696711833,
      "grad_norm": 19045.028747681114,
      "learning_rate": 1.4848288692842707e-07,
      "loss": 1.6785,
      "step": 454016
    },
    {
      "epoch": 0.00164783030402202,
      "grad_norm": 10237.696420582122,
      "learning_rate": 1.4847781306648915e-07,
      "loss": 1.6709,
      "step": 454048
    },
    {
      "epoch": 0.0016479464383728566,
      "grad_norm": 10815.860206197194,
      "learning_rate": 1.4847257607712826e-07,
      "loss": 1.6648,
      "step": 454080
    },
    {
      "epoch": 0.0016480625727236934,
      "grad_norm": 9045.373955785355,
      "learning_rate": 1.4846733964187285e-07,
      "loss": 1.6749,
      "step": 454112
    },
    {
      "epoch": 0.0016481787070745301,
      "grad_norm": 8482.83926524604,
      "learning_rate": 1.4846210376062518e-07,
      "loss": 1.6731,
      "step": 454144
    },
    {
      "epoch": 0.001648294841425367,
      "grad_norm": 10296.389464273387,
      "learning_rate": 1.4845686843328762e-07,
      "loss": 1.6709,
      "step": 454176
    },
    {
      "epoch": 0.0016484109757762034,
      "grad_norm": 11355.212723678935,
      "learning_rate": 1.484516336597625e-07,
      "loss": 1.683,
      "step": 454208
    },
    {
      "epoch": 0.0016485271101270402,
      "grad_norm": 7709.734366370867,
      "learning_rate": 1.4844639943995214e-07,
      "loss": 1.6973,
      "step": 454240
    },
    {
      "epoch": 0.001648643244477877,
      "grad_norm": 8380.246416424758,
      "learning_rate": 1.48441165773759e-07,
      "loss": 1.6957,
      "step": 454272
    },
    {
      "epoch": 0.0016487593788287137,
      "grad_norm": 8339.022364761951,
      "learning_rate": 1.4843593266108543e-07,
      "loss": 1.6998,
      "step": 454304
    },
    {
      "epoch": 0.0016488755131795505,
      "grad_norm": 7866.1531894567115,
      "learning_rate": 1.484307001018339e-07,
      "loss": 1.6913,
      "step": 454336
    },
    {
      "epoch": 0.001648991647530387,
      "grad_norm": 8465.497622703582,
      "learning_rate": 1.484254680959069e-07,
      "loss": 1.6874,
      "step": 454368
    },
    {
      "epoch": 0.0016491077818812237,
      "grad_norm": 8394.761819134597,
      "learning_rate": 1.4842023664320687e-07,
      "loss": 1.6881,
      "step": 454400
    },
    {
      "epoch": 0.0016492239162320605,
      "grad_norm": 9504.68768555811,
      "learning_rate": 1.4841500574363636e-07,
      "loss": 1.6861,
      "step": 454432
    },
    {
      "epoch": 0.0016493400505828973,
      "grad_norm": 12944.124535865683,
      "learning_rate": 1.4840977539709785e-07,
      "loss": 1.6835,
      "step": 454464
    },
    {
      "epoch": 0.0016494561849337338,
      "grad_norm": 11839.956418838712,
      "learning_rate": 1.4840454560349396e-07,
      "loss": 1.6646,
      "step": 454496
    },
    {
      "epoch": 0.0016495723192845705,
      "grad_norm": 10257.373737950666,
      "learning_rate": 1.483993163627272e-07,
      "loss": 1.6711,
      "step": 454528
    },
    {
      "epoch": 0.0016496884536354073,
      "grad_norm": 9296.614222393011,
      "learning_rate": 1.4839408767470028e-07,
      "loss": 1.6805,
      "step": 454560
    },
    {
      "epoch": 0.001649804587986244,
      "grad_norm": 10728.060961795472,
      "learning_rate": 1.4838885953931576e-07,
      "loss": 1.6867,
      "step": 454592
    },
    {
      "epoch": 0.0016499207223370808,
      "grad_norm": 12165.408172354924,
      "learning_rate": 1.4838363195647627e-07,
      "loss": 1.691,
      "step": 454624
    },
    {
      "epoch": 0.0016500368566879173,
      "grad_norm": 12394.174599383374,
      "learning_rate": 1.4837840492608456e-07,
      "loss": 1.662,
      "step": 454656
    },
    {
      "epoch": 0.001650152991038754,
      "grad_norm": 10822.03381994346,
      "learning_rate": 1.483731784480433e-07,
      "loss": 1.6831,
      "step": 454688
    },
    {
      "epoch": 0.0016502691253895909,
      "grad_norm": 10537.05566085707,
      "learning_rate": 1.483679525222552e-07,
      "loss": 1.6811,
      "step": 454720
    },
    {
      "epoch": 0.0016503852597404276,
      "grad_norm": 10960.243610431293,
      "learning_rate": 1.4836272714862305e-07,
      "loss": 1.6673,
      "step": 454752
    },
    {
      "epoch": 0.0016505013940912641,
      "grad_norm": 9045.712796678878,
      "learning_rate": 1.4835750232704956e-07,
      "loss": 1.6781,
      "step": 454784
    },
    {
      "epoch": 0.001650617528442101,
      "grad_norm": 9683.496269426658,
      "learning_rate": 1.483522780574376e-07,
      "loss": 1.667,
      "step": 454816
    },
    {
      "epoch": 0.0016507336627929377,
      "grad_norm": 9990.134934023665,
      "learning_rate": 1.4834705433968997e-07,
      "loss": 1.6712,
      "step": 454848
    },
    {
      "epoch": 0.0016508497971437744,
      "grad_norm": 10146.669798510247,
      "learning_rate": 1.4834183117370953e-07,
      "loss": 1.6813,
      "step": 454880
    },
    {
      "epoch": 0.0016509659314946112,
      "grad_norm": 11231.24872843621,
      "learning_rate": 1.4833660855939908e-07,
      "loss": 1.6956,
      "step": 454912
    },
    {
      "epoch": 0.0016510820658454477,
      "grad_norm": 14353.833355588326,
      "learning_rate": 1.483313864966616e-07,
      "loss": 1.7097,
      "step": 454944
    },
    {
      "epoch": 0.0016511982001962845,
      "grad_norm": 11132.028027273378,
      "learning_rate": 1.4832616498539997e-07,
      "loss": 1.6596,
      "step": 454976
    },
    {
      "epoch": 0.0016513143345471212,
      "grad_norm": 12818.144327475798,
      "learning_rate": 1.4832094402551714e-07,
      "loss": 1.6699,
      "step": 455008
    },
    {
      "epoch": 0.001651430468897958,
      "grad_norm": 19553.396022174766,
      "learning_rate": 1.4831572361691608e-07,
      "loss": 1.6778,
      "step": 455040
    },
    {
      "epoch": 0.0016515466032487945,
      "grad_norm": 9854.481620054908,
      "learning_rate": 1.4831066687170185e-07,
      "loss": 1.6803,
      "step": 455072
    },
    {
      "epoch": 0.0016516627375996313,
      "grad_norm": 9143.749996582364,
      "learning_rate": 1.4830544754815328e-07,
      "loss": 1.6963,
      "step": 455104
    },
    {
      "epoch": 0.001651778871950468,
      "grad_norm": 12843.041072892354,
      "learning_rate": 1.4830022877559853e-07,
      "loss": 1.6794,
      "step": 455136
    },
    {
      "epoch": 0.0016518950063013048,
      "grad_norm": 9598.78471474384,
      "learning_rate": 1.482950105539407e-07,
      "loss": 1.6922,
      "step": 455168
    },
    {
      "epoch": 0.0016520111406521415,
      "grad_norm": 12011.459195285142,
      "learning_rate": 1.4828979288308285e-07,
      "loss": 1.6909,
      "step": 455200
    },
    {
      "epoch": 0.001652127275002978,
      "grad_norm": 9731.578289260175,
      "learning_rate": 1.4828457576292808e-07,
      "loss": 1.6875,
      "step": 455232
    },
    {
      "epoch": 0.0016522434093538148,
      "grad_norm": 9853.646634622128,
      "learning_rate": 1.4827935919337953e-07,
      "loss": 1.6723,
      "step": 455264
    },
    {
      "epoch": 0.0016523595437046516,
      "grad_norm": 8445.212371515592,
      "learning_rate": 1.4827414317434038e-07,
      "loss": 1.6691,
      "step": 455296
    },
    {
      "epoch": 0.0016524756780554883,
      "grad_norm": 9455.751900298568,
      "learning_rate": 1.4826892770571377e-07,
      "loss": 1.6643,
      "step": 455328
    },
    {
      "epoch": 0.0016525918124063249,
      "grad_norm": 10624.737549699757,
      "learning_rate": 1.4826371278740296e-07,
      "loss": 1.6746,
      "step": 455360
    },
    {
      "epoch": 0.0016527079467571616,
      "grad_norm": 8730.006758302081,
      "learning_rate": 1.482584984193111e-07,
      "loss": 1.683,
      "step": 455392
    },
    {
      "epoch": 0.0016528240811079984,
      "grad_norm": 9466.516782851018,
      "learning_rate": 1.482532846013415e-07,
      "loss": 1.7117,
      "step": 455424
    },
    {
      "epoch": 0.0016529402154588351,
      "grad_norm": 10027.770041240476,
      "learning_rate": 1.4824807133339747e-07,
      "loss": 1.7039,
      "step": 455456
    },
    {
      "epoch": 0.0016530563498096719,
      "grad_norm": 9799.895917814638,
      "learning_rate": 1.4824285861538222e-07,
      "loss": 1.7103,
      "step": 455488
    },
    {
      "epoch": 0.0016531724841605084,
      "grad_norm": 9942.416808804588,
      "learning_rate": 1.482376464471991e-07,
      "loss": 1.683,
      "step": 455520
    },
    {
      "epoch": 0.0016532886185113452,
      "grad_norm": 13795.694255817647,
      "learning_rate": 1.482324348287515e-07,
      "loss": 1.659,
      "step": 455552
    },
    {
      "epoch": 0.001653404752862182,
      "grad_norm": 9052.183824912086,
      "learning_rate": 1.4822722375994275e-07,
      "loss": 1.6679,
      "step": 455584
    },
    {
      "epoch": 0.0016535208872130187,
      "grad_norm": 8137.119760701571,
      "learning_rate": 1.4822201324067627e-07,
      "loss": 1.6674,
      "step": 455616
    },
    {
      "epoch": 0.0016536370215638552,
      "grad_norm": 11379.221590249485,
      "learning_rate": 1.4821680327085544e-07,
      "loss": 1.6765,
      "step": 455648
    },
    {
      "epoch": 0.001653753155914692,
      "grad_norm": 12925.068510456724,
      "learning_rate": 1.482115938503838e-07,
      "loss": 1.6715,
      "step": 455680
    },
    {
      "epoch": 0.0016538692902655287,
      "grad_norm": 9944.075824328775,
      "learning_rate": 1.4820638497916467e-07,
      "loss": 1.6618,
      "step": 455712
    },
    {
      "epoch": 0.0016539854246163655,
      "grad_norm": 8488.76975774464,
      "learning_rate": 1.4820117665710163e-07,
      "loss": 1.6813,
      "step": 455744
    },
    {
      "epoch": 0.0016541015589672022,
      "grad_norm": 11103.565013093768,
      "learning_rate": 1.481959688840982e-07,
      "loss": 1.6693,
      "step": 455776
    },
    {
      "epoch": 0.0016542176933180388,
      "grad_norm": 8317.07761175763,
      "learning_rate": 1.4819076166005788e-07,
      "loss": 1.6593,
      "step": 455808
    },
    {
      "epoch": 0.0016543338276688755,
      "grad_norm": 9747.658795834002,
      "learning_rate": 1.4818555498488425e-07,
      "loss": 1.6683,
      "step": 455840
    },
    {
      "epoch": 0.0016544499620197123,
      "grad_norm": 8885.269157431305,
      "learning_rate": 1.481803488584809e-07,
      "loss": 1.6786,
      "step": 455872
    },
    {
      "epoch": 0.001654566096370549,
      "grad_norm": 9909.408054974829,
      "learning_rate": 1.4817514328075143e-07,
      "loss": 1.696,
      "step": 455904
    },
    {
      "epoch": 0.0016546822307213856,
      "grad_norm": 13623.58308228786,
      "learning_rate": 1.4816993825159949e-07,
      "loss": 1.6973,
      "step": 455936
    },
    {
      "epoch": 0.0016547983650722223,
      "grad_norm": 8550.479518717064,
      "learning_rate": 1.4816473377092867e-07,
      "loss": 1.6978,
      "step": 455968
    },
    {
      "epoch": 0.001654914499423059,
      "grad_norm": 9052.36112845704,
      "learning_rate": 1.4815952983864273e-07,
      "loss": 1.7122,
      "step": 456000
    },
    {
      "epoch": 0.0016550306337738958,
      "grad_norm": 11879.617670615498,
      "learning_rate": 1.4815432645464531e-07,
      "loss": 1.7093,
      "step": 456032
    },
    {
      "epoch": 0.0016551467681247326,
      "grad_norm": 9058.754439767092,
      "learning_rate": 1.4814912361884021e-07,
      "loss": 1.6957,
      "step": 456064
    },
    {
      "epoch": 0.0016552629024755691,
      "grad_norm": 18875.510483163096,
      "learning_rate": 1.4814392133113113e-07,
      "loss": 1.6927,
      "step": 456096
    },
    {
      "epoch": 0.0016553790368264059,
      "grad_norm": 13545.191766822647,
      "learning_rate": 1.481388821374938e-07,
      "loss": 1.6882,
      "step": 456128
    },
    {
      "epoch": 0.0016554951711772426,
      "grad_norm": 10248.659229382154,
      "learning_rate": 1.4813368092856757e-07,
      "loss": 1.6855,
      "step": 456160
    },
    {
      "epoch": 0.0016556113055280794,
      "grad_norm": 11113.876731366063,
      "learning_rate": 1.4812848026745177e-07,
      "loss": 1.6964,
      "step": 456192
    },
    {
      "epoch": 0.001655727439878916,
      "grad_norm": 7737.012343275665,
      "learning_rate": 1.4812328015405027e-07,
      "loss": 1.685,
      "step": 456224
    },
    {
      "epoch": 0.0016558435742297527,
      "grad_norm": 10094.827982684994,
      "learning_rate": 1.481180805882669e-07,
      "loss": 1.6841,
      "step": 456256
    },
    {
      "epoch": 0.0016559597085805894,
      "grad_norm": 12044.805187299628,
      "learning_rate": 1.4811288157000554e-07,
      "loss": 1.671,
      "step": 456288
    },
    {
      "epoch": 0.0016560758429314262,
      "grad_norm": 10086.033115154838,
      "learning_rate": 1.4810768309917012e-07,
      "loss": 1.6787,
      "step": 456320
    },
    {
      "epoch": 0.001656191977282263,
      "grad_norm": 10313.502024045954,
      "learning_rate": 1.4810248517566463e-07,
      "loss": 1.7029,
      "step": 456352
    },
    {
      "epoch": 0.0016563081116330995,
      "grad_norm": 11957.214224057374,
      "learning_rate": 1.4809728779939297e-07,
      "loss": 1.7074,
      "step": 456384
    },
    {
      "epoch": 0.0016564242459839362,
      "grad_norm": 9541.389626254659,
      "learning_rate": 1.4809209097025912e-07,
      "loss": 1.6931,
      "step": 456416
    },
    {
      "epoch": 0.001656540380334773,
      "grad_norm": 9419.091038948503,
      "learning_rate": 1.4808689468816713e-07,
      "loss": 1.6777,
      "step": 456448
    },
    {
      "epoch": 0.0016566565146856097,
      "grad_norm": 8804.683299244783,
      "learning_rate": 1.4808169895302102e-07,
      "loss": 1.6869,
      "step": 456480
    },
    {
      "epoch": 0.0016567726490364463,
      "grad_norm": 10609.706970505831,
      "learning_rate": 1.4807650376472486e-07,
      "loss": 1.6897,
      "step": 456512
    },
    {
      "epoch": 0.001656888783387283,
      "grad_norm": 12182.40583792873,
      "learning_rate": 1.480713091231827e-07,
      "loss": 1.7024,
      "step": 456544
    },
    {
      "epoch": 0.0016570049177381198,
      "grad_norm": 10320.657730978195,
      "learning_rate": 1.4806611502829867e-07,
      "loss": 1.6919,
      "step": 456576
    },
    {
      "epoch": 0.0016571210520889565,
      "grad_norm": 8704.238507761607,
      "learning_rate": 1.4806092147997685e-07,
      "loss": 1.6751,
      "step": 456608
    },
    {
      "epoch": 0.0016572371864397933,
      "grad_norm": 9627.558361287664,
      "learning_rate": 1.4805572847812146e-07,
      "loss": 1.681,
      "step": 456640
    },
    {
      "epoch": 0.0016573533207906298,
      "grad_norm": 11011.837267232022,
      "learning_rate": 1.4805053602263663e-07,
      "loss": 1.6603,
      "step": 456672
    },
    {
      "epoch": 0.0016574694551414666,
      "grad_norm": 9539.529233667667,
      "learning_rate": 1.4804534411342656e-07,
      "loss": 1.6697,
      "step": 456704
    },
    {
      "epoch": 0.0016575855894923033,
      "grad_norm": 12971.153996464618,
      "learning_rate": 1.480401527503955e-07,
      "loss": 1.6846,
      "step": 456736
    },
    {
      "epoch": 0.00165770172384314,
      "grad_norm": 9867.405332710317,
      "learning_rate": 1.4803496193344765e-07,
      "loss": 1.6592,
      "step": 456768
    },
    {
      "epoch": 0.0016578178581939766,
      "grad_norm": 9607.951290467703,
      "learning_rate": 1.4802977166248734e-07,
      "loss": 1.6615,
      "step": 456800
    },
    {
      "epoch": 0.0016579339925448134,
      "grad_norm": 9446.830685473304,
      "learning_rate": 1.480245819374188e-07,
      "loss": 1.6758,
      "step": 456832
    },
    {
      "epoch": 0.0016580501268956501,
      "grad_norm": 10209.270101236425,
      "learning_rate": 1.4801939275814636e-07,
      "loss": 1.6942,
      "step": 456864
    },
    {
      "epoch": 0.0016581662612464869,
      "grad_norm": 9640.02427382836,
      "learning_rate": 1.480142041245744e-07,
      "loss": 1.7145,
      "step": 456896
    },
    {
      "epoch": 0.0016582823955973236,
      "grad_norm": 10266.825994434697,
      "learning_rate": 1.4800901603660725e-07,
      "loss": 1.6875,
      "step": 456928
    },
    {
      "epoch": 0.0016583985299481602,
      "grad_norm": 9467.940641977008,
      "learning_rate": 1.4800382849414927e-07,
      "loss": 1.6863,
      "step": 456960
    },
    {
      "epoch": 0.001658514664298997,
      "grad_norm": 9951.445925090484,
      "learning_rate": 1.4799864149710494e-07,
      "loss": 1.6825,
      "step": 456992
    },
    {
      "epoch": 0.0016586307986498337,
      "grad_norm": 9067.74084323102,
      "learning_rate": 1.4799345504537863e-07,
      "loss": 1.6685,
      "step": 457024
    },
    {
      "epoch": 0.0016587469330006704,
      "grad_norm": 8386.440007535975,
      "learning_rate": 1.4798826913887482e-07,
      "loss": 1.6796,
      "step": 457056
    },
    {
      "epoch": 0.001658863067351507,
      "grad_norm": 10080.194839386786,
      "learning_rate": 1.47983083777498e-07,
      "loss": 1.6813,
      "step": 457088
    },
    {
      "epoch": 0.0016589792017023437,
      "grad_norm": 20668.974236763665,
      "learning_rate": 1.4797789896115264e-07,
      "loss": 1.6924,
      "step": 457120
    },
    {
      "epoch": 0.0016590953360531805,
      "grad_norm": 18034.844274348477,
      "learning_rate": 1.479727146897433e-07,
      "loss": 1.6657,
      "step": 457152
    },
    {
      "epoch": 0.0016592114704040172,
      "grad_norm": 18571.39865492096,
      "learning_rate": 1.479675309631745e-07,
      "loss": 1.6742,
      "step": 457184
    },
    {
      "epoch": 0.001659327604754854,
      "grad_norm": 8930.484309375388,
      "learning_rate": 1.4796250974753813e-07,
      "loss": 1.6796,
      "step": 457216
    },
    {
      "epoch": 0.0016594437391056905,
      "grad_norm": 8222.803901346548,
      "learning_rate": 1.4795732709334533e-07,
      "loss": 1.6852,
      "step": 457248
    },
    {
      "epoch": 0.0016595598734565273,
      "grad_norm": 9075.605324164333,
      "learning_rate": 1.4795214498370989e-07,
      "loss": 1.6641,
      "step": 457280
    },
    {
      "epoch": 0.001659676007807364,
      "grad_norm": 8506.162471996406,
      "learning_rate": 1.479469634185364e-07,
      "loss": 1.6574,
      "step": 457312
    },
    {
      "epoch": 0.0016597921421582008,
      "grad_norm": 10644.457055200139,
      "learning_rate": 1.4794178239772953e-07,
      "loss": 1.6668,
      "step": 457344
    },
    {
      "epoch": 0.0016599082765090373,
      "grad_norm": 9252.60871322245,
      "learning_rate": 1.47936601921194e-07,
      "loss": 1.6885,
      "step": 457376
    },
    {
      "epoch": 0.001660024410859874,
      "grad_norm": 11799.772879170176,
      "learning_rate": 1.4793142198883452e-07,
      "loss": 1.6648,
      "step": 457408
    },
    {
      "epoch": 0.0016601405452107108,
      "grad_norm": 9728.871671473522,
      "learning_rate": 1.479262426005558e-07,
      "loss": 1.6678,
      "step": 457440
    },
    {
      "epoch": 0.0016602566795615476,
      "grad_norm": 11537.505276271815,
      "learning_rate": 1.4792106375626263e-07,
      "loss": 1.669,
      "step": 457472
    },
    {
      "epoch": 0.0016603728139123843,
      "grad_norm": 9881.90528187758,
      "learning_rate": 1.4791588545585978e-07,
      "loss": 1.6677,
      "step": 457504
    },
    {
      "epoch": 0.0016604889482632209,
      "grad_norm": 9605.580253165344,
      "learning_rate": 1.4791070769925204e-07,
      "loss": 1.6677,
      "step": 457536
    },
    {
      "epoch": 0.0016606050826140576,
      "grad_norm": 10906.727648566273,
      "learning_rate": 1.4790553048634426e-07,
      "loss": 1.6681,
      "step": 457568
    },
    {
      "epoch": 0.0016607212169648944,
      "grad_norm": 9327.317728050224,
      "learning_rate": 1.4790035381704131e-07,
      "loss": 1.684,
      "step": 457600
    },
    {
      "epoch": 0.0016608373513157311,
      "grad_norm": 9758.050829955744,
      "learning_rate": 1.47895177691248e-07,
      "loss": 1.6913,
      "step": 457632
    },
    {
      "epoch": 0.0016609534856665677,
      "grad_norm": 8175.608234253889,
      "learning_rate": 1.478900021088693e-07,
      "loss": 1.6848,
      "step": 457664
    },
    {
      "epoch": 0.0016610696200174044,
      "grad_norm": 9099.894944448535,
      "learning_rate": 1.478848270698101e-07,
      "loss": 1.6788,
      "step": 457696
    },
    {
      "epoch": 0.0016611857543682412,
      "grad_norm": 11650.863487312861,
      "learning_rate": 1.4787965257397537e-07,
      "loss": 1.6868,
      "step": 457728
    },
    {
      "epoch": 0.001661301888719078,
      "grad_norm": 11053.71213665346,
      "learning_rate": 1.4787447862127003e-07,
      "loss": 1.6728,
      "step": 457760
    },
    {
      "epoch": 0.0016614180230699147,
      "grad_norm": 9542.499462928987,
      "learning_rate": 1.4786930521159913e-07,
      "loss": 1.6885,
      "step": 457792
    },
    {
      "epoch": 0.0016615341574207512,
      "grad_norm": 11686.693287666962,
      "learning_rate": 1.4786413234486767e-07,
      "loss": 1.6932,
      "step": 457824
    },
    {
      "epoch": 0.001661650291771588,
      "grad_norm": 8907.183393194506,
      "learning_rate": 1.4785896002098065e-07,
      "loss": 1.6976,
      "step": 457856
    },
    {
      "epoch": 0.0016617664261224247,
      "grad_norm": 9488.46520781944,
      "learning_rate": 1.4785378823984317e-07,
      "loss": 1.671,
      "step": 457888
    },
    {
      "epoch": 0.0016618825604732615,
      "grad_norm": 10753.137774621879,
      "learning_rate": 1.478486170013603e-07,
      "loss": 1.6572,
      "step": 457920
    },
    {
      "epoch": 0.001661998694824098,
      "grad_norm": 10550.174974852313,
      "learning_rate": 1.4784344630543718e-07,
      "loss": 1.6635,
      "step": 457952
    },
    {
      "epoch": 0.0016621148291749348,
      "grad_norm": 10978.50117274667,
      "learning_rate": 1.478382761519789e-07,
      "loss": 1.6613,
      "step": 457984
    },
    {
      "epoch": 0.0016622309635257715,
      "grad_norm": 8654.324699247192,
      "learning_rate": 1.4783310654089066e-07,
      "loss": 1.6635,
      "step": 458016
    },
    {
      "epoch": 0.0016623470978766083,
      "grad_norm": 12211.530616593482,
      "learning_rate": 1.478279374720776e-07,
      "loss": 1.6606,
      "step": 458048
    },
    {
      "epoch": 0.001662463232227445,
      "grad_norm": 9643.97314388629,
      "learning_rate": 1.478227689454449e-07,
      "loss": 1.6775,
      "step": 458080
    },
    {
      "epoch": 0.0016625793665782816,
      "grad_norm": 9416.079332715926,
      "learning_rate": 1.4781760096089785e-07,
      "loss": 1.6983,
      "step": 458112
    },
    {
      "epoch": 0.0016626955009291183,
      "grad_norm": 11482.001741856688,
      "learning_rate": 1.478124335183417e-07,
      "loss": 1.6972,
      "step": 458144
    },
    {
      "epoch": 0.001662811635279955,
      "grad_norm": 14547.176220834062,
      "learning_rate": 1.4780726661768163e-07,
      "loss": 1.693,
      "step": 458176
    },
    {
      "epoch": 0.0016629277696307918,
      "grad_norm": 20215.229110747176,
      "learning_rate": 1.4780210025882302e-07,
      "loss": 1.6791,
      "step": 458208
    },
    {
      "epoch": 0.0016630439039816284,
      "grad_norm": 21028.388050442667,
      "learning_rate": 1.4779693444167115e-07,
      "loss": 1.6568,
      "step": 458240
    },
    {
      "epoch": 0.0016631600383324651,
      "grad_norm": 22751.69936510238,
      "learning_rate": 1.477917691661314e-07,
      "loss": 1.6573,
      "step": 458272
    },
    {
      "epoch": 0.001663276172683302,
      "grad_norm": 9431.100890140027,
      "learning_rate": 1.4778676582185144e-07,
      "loss": 1.669,
      "step": 458304
    },
    {
      "epoch": 0.0016633923070341386,
      "grad_norm": 10950.44200021168,
      "learning_rate": 1.4778160161233394e-07,
      "loss": 1.6754,
      "step": 458336
    },
    {
      "epoch": 0.0016635084413849754,
      "grad_norm": 9967.652080605543,
      "learning_rate": 1.4777643794414764e-07,
      "loss": 1.694,
      "step": 458368
    },
    {
      "epoch": 0.001663624575735812,
      "grad_norm": 11096.366612544847,
      "learning_rate": 1.4777127481719798e-07,
      "loss": 1.6583,
      "step": 458400
    },
    {
      "epoch": 0.0016637407100866487,
      "grad_norm": 11510.737422076832,
      "learning_rate": 1.4776611223139046e-07,
      "loss": 1.6665,
      "step": 458432
    },
    {
      "epoch": 0.0016638568444374854,
      "grad_norm": 9676.019222800252,
      "learning_rate": 1.477609501866305e-07,
      "loss": 1.6654,
      "step": 458464
    },
    {
      "epoch": 0.0016639729787883222,
      "grad_norm": 9415.790991732983,
      "learning_rate": 1.4775578868282358e-07,
      "loss": 1.6616,
      "step": 458496
    },
    {
      "epoch": 0.0016640891131391587,
      "grad_norm": 10327.928156217973,
      "learning_rate": 1.4775062771987534e-07,
      "loss": 1.6751,
      "step": 458528
    },
    {
      "epoch": 0.0016642052474899955,
      "grad_norm": 10352.125578836454,
      "learning_rate": 1.477454672976912e-07,
      "loss": 1.6742,
      "step": 458560
    },
    {
      "epoch": 0.0016643213818408322,
      "grad_norm": 9879.884817142354,
      "learning_rate": 1.4774030741617677e-07,
      "loss": 1.6902,
      "step": 458592
    },
    {
      "epoch": 0.001664437516191669,
      "grad_norm": 14268.365288287232,
      "learning_rate": 1.4773514807523767e-07,
      "loss": 1.7025,
      "step": 458624
    },
    {
      "epoch": 0.0016645536505425058,
      "grad_norm": 10042.401903927168,
      "learning_rate": 1.4772998927477955e-07,
      "loss": 1.706,
      "step": 458656
    },
    {
      "epoch": 0.0016646697848933423,
      "grad_norm": 12811.65360131158,
      "learning_rate": 1.4772483101470796e-07,
      "loss": 1.7076,
      "step": 458688
    },
    {
      "epoch": 0.001664785919244179,
      "grad_norm": 9612.01019558344,
      "learning_rate": 1.4771967329492865e-07,
      "loss": 1.7115,
      "step": 458720
    },
    {
      "epoch": 0.0016649020535950158,
      "grad_norm": 9091.32641587574,
      "learning_rate": 1.4771451611534724e-07,
      "loss": 1.6821,
      "step": 458752
    },
    {
      "epoch": 0.0016650181879458526,
      "grad_norm": 14255.28758040328,
      "learning_rate": 1.4770935947586948e-07,
      "loss": 1.6598,
      "step": 458784
    },
    {
      "epoch": 0.001665134322296689,
      "grad_norm": 9986.172039375248,
      "learning_rate": 1.4770420337640107e-07,
      "loss": 1.6786,
      "step": 458816
    },
    {
      "epoch": 0.0016652504566475258,
      "grad_norm": 15458.259798567236,
      "learning_rate": 1.476990478168478e-07,
      "loss": 1.6909,
      "step": 458848
    },
    {
      "epoch": 0.0016653665909983626,
      "grad_norm": 9897.975146463039,
      "learning_rate": 1.4769389279711545e-07,
      "loss": 1.6745,
      "step": 458880
    },
    {
      "epoch": 0.0016654827253491994,
      "grad_norm": 13147.199245466694,
      "learning_rate": 1.4768873831710978e-07,
      "loss": 1.6687,
      "step": 458912
    },
    {
      "epoch": 0.001665598859700036,
      "grad_norm": 8004.0573461214035,
      "learning_rate": 1.4768358437673666e-07,
      "loss": 1.6698,
      "step": 458944
    },
    {
      "epoch": 0.0016657149940508726,
      "grad_norm": 9723.665358289538,
      "learning_rate": 1.4767843097590192e-07,
      "loss": 1.6766,
      "step": 458976
    },
    {
      "epoch": 0.0016658311284017094,
      "grad_norm": 11546.647305603476,
      "learning_rate": 1.4767327811451142e-07,
      "loss": 1.6744,
      "step": 459008
    },
    {
      "epoch": 0.0016659472627525462,
      "grad_norm": 13426.300160505873,
      "learning_rate": 1.4766812579247105e-07,
      "loss": 1.6557,
      "step": 459040
    },
    {
      "epoch": 0.001666063397103383,
      "grad_norm": 8626.047298734224,
      "learning_rate": 1.4766297400968678e-07,
      "loss": 1.6689,
      "step": 459072
    },
    {
      "epoch": 0.0016661795314542194,
      "grad_norm": 9909.290186486618,
      "learning_rate": 1.4765782276606449e-07,
      "loss": 1.6715,
      "step": 459104
    },
    {
      "epoch": 0.0016662956658050562,
      "grad_norm": 9107.884716002942,
      "learning_rate": 1.476526720615102e-07,
      "loss": 1.6654,
      "step": 459136
    },
    {
      "epoch": 0.001666411800155893,
      "grad_norm": 11285.957114928267,
      "learning_rate": 1.476475218959298e-07,
      "loss": 1.6755,
      "step": 459168
    },
    {
      "epoch": 0.0016665279345067297,
      "grad_norm": 10081.465171293308,
      "learning_rate": 1.476423722692294e-07,
      "loss": 1.671,
      "step": 459200
    },
    {
      "epoch": 0.0016666440688575665,
      "grad_norm": 15418.349198276708,
      "learning_rate": 1.4763722318131497e-07,
      "loss": 1.6732,
      "step": 459232
    },
    {
      "epoch": 0.001666760203208403,
      "grad_norm": 12175.198971680093,
      "learning_rate": 1.476320746320926e-07,
      "loss": 1.6822,
      "step": 459264
    },
    {
      "epoch": 0.0016668763375592398,
      "grad_norm": 10345.302315543997,
      "learning_rate": 1.4762708748864868e-07,
      "loss": 1.6794,
      "step": 459296
    },
    {
      "epoch": 0.0016669924719100765,
      "grad_norm": 12762.951696218239,
      "learning_rate": 1.476219399997018e-07,
      "loss": 1.6926,
      "step": 459328
    },
    {
      "epoch": 0.0016671086062609133,
      "grad_norm": 10661.209124672492,
      "learning_rate": 1.4761679304916823e-07,
      "loss": 1.6713,
      "step": 459360
    },
    {
      "epoch": 0.0016672247406117498,
      "grad_norm": 12217.793254102804,
      "learning_rate": 1.4761164663695408e-07,
      "loss": 1.6776,
      "step": 459392
    },
    {
      "epoch": 0.0016673408749625866,
      "grad_norm": 10977.857805601236,
      "learning_rate": 1.4760650076296553e-07,
      "loss": 1.6811,
      "step": 459424
    },
    {
      "epoch": 0.0016674570093134233,
      "grad_norm": 9417.131197981686,
      "learning_rate": 1.4760135542710873e-07,
      "loss": 1.6874,
      "step": 459456
    },
    {
      "epoch": 0.00166757314366426,
      "grad_norm": 11773.136030811842,
      "learning_rate": 1.4759621062928996e-07,
      "loss": 1.6924,
      "step": 459488
    },
    {
      "epoch": 0.0016676892780150968,
      "grad_norm": 10252.569629122252,
      "learning_rate": 1.475910663694154e-07,
      "loss": 1.688,
      "step": 459520
    },
    {
      "epoch": 0.0016678054123659334,
      "grad_norm": 10167.46615435724,
      "learning_rate": 1.4758592264739137e-07,
      "loss": 1.6982,
      "step": 459552
    },
    {
      "epoch": 0.00166792154671677,
      "grad_norm": 11872.852395275535,
      "learning_rate": 1.475807794631241e-07,
      "loss": 1.6993,
      "step": 459584
    },
    {
      "epoch": 0.0016680376810676069,
      "grad_norm": 10072.043486800481,
      "learning_rate": 1.4757563681651994e-07,
      "loss": 1.693,
      "step": 459616
    },
    {
      "epoch": 0.0016681538154184436,
      "grad_norm": 12379.42648106123,
      "learning_rate": 1.4757049470748518e-07,
      "loss": 1.6853,
      "step": 459648
    },
    {
      "epoch": 0.0016682699497692802,
      "grad_norm": 13781.084717829724,
      "learning_rate": 1.4756535313592618e-07,
      "loss": 1.6621,
      "step": 459680
    },
    {
      "epoch": 0.001668386084120117,
      "grad_norm": 10187.006233432863,
      "learning_rate": 1.475602121017493e-07,
      "loss": 1.661,
      "step": 459712
    },
    {
      "epoch": 0.0016685022184709537,
      "grad_norm": 11503.515810394663,
      "learning_rate": 1.47555071604861e-07,
      "loss": 1.6731,
      "step": 459744
    },
    {
      "epoch": 0.0016686183528217904,
      "grad_norm": 9431.531158830998,
      "learning_rate": 1.4754993164516764e-07,
      "loss": 1.6905,
      "step": 459776
    },
    {
      "epoch": 0.0016687344871726272,
      "grad_norm": 11593.881662325177,
      "learning_rate": 1.475447922225757e-07,
      "loss": 1.7058,
      "step": 459808
    },
    {
      "epoch": 0.0016688506215234637,
      "grad_norm": 9990.883444420719,
      "learning_rate": 1.475396533369916e-07,
      "loss": 1.6903,
      "step": 459840
    },
    {
      "epoch": 0.0016689667558743005,
      "grad_norm": 10416.427986598861,
      "learning_rate": 1.4753451498832187e-07,
      "loss": 1.6744,
      "step": 459872
    },
    {
      "epoch": 0.0016690828902251372,
      "grad_norm": 15274.488796683181,
      "learning_rate": 1.47529377176473e-07,
      "loss": 1.6632,
      "step": 459904
    },
    {
      "epoch": 0.001669199024575974,
      "grad_norm": 10227.947203618134,
      "learning_rate": 1.4752423990135151e-07,
      "loss": 1.6607,
      "step": 459936
    },
    {
      "epoch": 0.0016693151589268105,
      "grad_norm": 10689.204179919101,
      "learning_rate": 1.47519103162864e-07,
      "loss": 1.6761,
      "step": 459968
    },
    {
      "epoch": 0.0016694312932776473,
      "grad_norm": 9220.764610378035,
      "learning_rate": 1.4751396696091702e-07,
      "loss": 1.6673,
      "step": 460000
    },
    {
      "epoch": 0.001669547427628484,
      "grad_norm": 9794.605658218201,
      "learning_rate": 1.4750883129541716e-07,
      "loss": 1.6563,
      "step": 460032
    },
    {
      "epoch": 0.0016696635619793208,
      "grad_norm": 10697.060156884227,
      "learning_rate": 1.475036961662711e-07,
      "loss": 1.6669,
      "step": 460064
    },
    {
      "epoch": 0.0016697796963301575,
      "grad_norm": 9942.955295082042,
      "learning_rate": 1.4749856157338544e-07,
      "loss": 1.6715,
      "step": 460096
    },
    {
      "epoch": 0.001669895830680994,
      "grad_norm": 9594.159473346272,
      "learning_rate": 1.4749342751666685e-07,
      "loss": 1.6735,
      "step": 460128
    },
    {
      "epoch": 0.0016700119650318308,
      "grad_norm": 11992.781995850672,
      "learning_rate": 1.4748829399602203e-07,
      "loss": 1.6641,
      "step": 460160
    },
    {
      "epoch": 0.0016701280993826676,
      "grad_norm": 7582.93294181084,
      "learning_rate": 1.474831610113577e-07,
      "loss": 1.6567,
      "step": 460192
    },
    {
      "epoch": 0.0016702442337335043,
      "grad_norm": 9290.569627315646,
      "learning_rate": 1.474780285625806e-07,
      "loss": 1.6761,
      "step": 460224
    },
    {
      "epoch": 0.0016703603680843409,
      "grad_norm": 9206.048446537743,
      "learning_rate": 1.4747289664959754e-07,
      "loss": 1.6732,
      "step": 460256
    },
    {
      "epoch": 0.0016704765024351776,
      "grad_norm": 15582.644961623171,
      "learning_rate": 1.4746776527231522e-07,
      "loss": 1.6897,
      "step": 460288
    },
    {
      "epoch": 0.0016705926367860144,
      "grad_norm": 9623.06292195993,
      "learning_rate": 1.474627947613364e-07,
      "loss": 1.7235,
      "step": 460320
    },
    {
      "epoch": 0.0016707087711368511,
      "grad_norm": 11035.520649248952,
      "learning_rate": 1.4745766443844264e-07,
      "loss": 1.7028,
      "step": 460352
    },
    {
      "epoch": 0.0016708249054876879,
      "grad_norm": 8907.355836610548,
      "learning_rate": 1.4745253465097308e-07,
      "loss": 1.7086,
      "step": 460384
    },
    {
      "epoch": 0.0016709410398385244,
      "grad_norm": 10899.253919420356,
      "learning_rate": 1.4744740539883457e-07,
      "loss": 1.6972,
      "step": 460416
    },
    {
      "epoch": 0.0016710571741893612,
      "grad_norm": 10739.599992550933,
      "learning_rate": 1.47442276681934e-07,
      "loss": 1.7012,
      "step": 460448
    },
    {
      "epoch": 0.001671173308540198,
      "grad_norm": 11530.97740870218,
      "learning_rate": 1.4743714850017828e-07,
      "loss": 1.6791,
      "step": 460480
    },
    {
      "epoch": 0.0016712894428910347,
      "grad_norm": 10012.196562193532,
      "learning_rate": 1.4743202085347438e-07,
      "loss": 1.6649,
      "step": 460512
    },
    {
      "epoch": 0.0016714055772418712,
      "grad_norm": 11315.230797469401,
      "learning_rate": 1.4742689374172923e-07,
      "loss": 1.6827,
      "step": 460544
    },
    {
      "epoch": 0.001671521711592708,
      "grad_norm": 9681.163359844724,
      "learning_rate": 1.4742176716484988e-07,
      "loss": 1.669,
      "step": 460576
    },
    {
      "epoch": 0.0016716378459435447,
      "grad_norm": 11978.56218416885,
      "learning_rate": 1.4741664112274328e-07,
      "loss": 1.67,
      "step": 460608
    },
    {
      "epoch": 0.0016717539802943815,
      "grad_norm": 11647.156734585484,
      "learning_rate": 1.474115156153165e-07,
      "loss": 1.6654,
      "step": 460640
    },
    {
      "epoch": 0.0016718701146452182,
      "grad_norm": 8928.634610062169,
      "learning_rate": 1.4740639064247656e-07,
      "loss": 1.6602,
      "step": 460672
    },
    {
      "epoch": 0.0016719862489960548,
      "grad_norm": 14262.388439528633,
      "learning_rate": 1.4740126620413054e-07,
      "loss": 1.6747,
      "step": 460704
    },
    {
      "epoch": 0.0016721023833468915,
      "grad_norm": 13739.922561644953,
      "learning_rate": 1.473961423001856e-07,
      "loss": 1.6851,
      "step": 460736
    },
    {
      "epoch": 0.0016722185176977283,
      "grad_norm": 11436.656679292248,
      "learning_rate": 1.473910189305488e-07,
      "loss": 1.6828,
      "step": 460768
    },
    {
      "epoch": 0.001672334652048565,
      "grad_norm": 12860.944444324452,
      "learning_rate": 1.473858960951273e-07,
      "loss": 1.7002,
      "step": 460800
    },
    {
      "epoch": 0.0016724507863994016,
      "grad_norm": 10460.577421920838,
      "learning_rate": 1.4738077379382828e-07,
      "loss": 1.6651,
      "step": 460832
    },
    {
      "epoch": 0.0016725669207502383,
      "grad_norm": 8531.479590317262,
      "learning_rate": 1.4737565202655892e-07,
      "loss": 1.6834,
      "step": 460864
    },
    {
      "epoch": 0.001672683055101075,
      "grad_norm": 9772.901820851368,
      "learning_rate": 1.4737053079322647e-07,
      "loss": 1.6889,
      "step": 460896
    },
    {
      "epoch": 0.0016727991894519118,
      "grad_norm": 12362.906777938593,
      "learning_rate": 1.473654100937381e-07,
      "loss": 1.6796,
      "step": 460928
    },
    {
      "epoch": 0.0016729153238027486,
      "grad_norm": 10910.653692607057,
      "learning_rate": 1.4736028992800112e-07,
      "loss": 1.6662,
      "step": 460960
    },
    {
      "epoch": 0.0016730314581535851,
      "grad_norm": 10326.070307721133,
      "learning_rate": 1.4735517029592279e-07,
      "loss": 1.6702,
      "step": 460992
    },
    {
      "epoch": 0.0016731475925044219,
      "grad_norm": 9486.769102281345,
      "learning_rate": 1.4735005119741043e-07,
      "loss": 1.678,
      "step": 461024
    },
    {
      "epoch": 0.0016732637268552586,
      "grad_norm": 9422.933725756538,
      "learning_rate": 1.4734493263237136e-07,
      "loss": 1.6942,
      "step": 461056
    },
    {
      "epoch": 0.0016733798612060954,
      "grad_norm": 11791.169746891102,
      "learning_rate": 1.473398146007129e-07,
      "loss": 1.6925,
      "step": 461088
    },
    {
      "epoch": 0.001673495995556932,
      "grad_norm": 11558.83748479924,
      "learning_rate": 1.4733469710234247e-07,
      "loss": 1.687,
      "step": 461120
    },
    {
      "epoch": 0.0016736121299077687,
      "grad_norm": 13053.535306574997,
      "learning_rate": 1.4732958013716745e-07,
      "loss": 1.685,
      "step": 461152
    },
    {
      "epoch": 0.0016737282642586054,
      "grad_norm": 10518.028902793527,
      "learning_rate": 1.4732446370509523e-07,
      "loss": 1.6807,
      "step": 461184
    },
    {
      "epoch": 0.0016738443986094422,
      "grad_norm": 11189.028733540727,
      "learning_rate": 1.4731934780603326e-07,
      "loss": 1.687,
      "step": 461216
    },
    {
      "epoch": 0.001673960532960279,
      "grad_norm": 13846.433475808852,
      "learning_rate": 1.47314232439889e-07,
      "loss": 1.7078,
      "step": 461248
    },
    {
      "epoch": 0.0016740766673111155,
      "grad_norm": 11578.302638988152,
      "learning_rate": 1.4730911760656997e-07,
      "loss": 1.7339,
      "step": 461280
    },
    {
      "epoch": 0.0016741928016619522,
      "grad_norm": 16609.148322535988,
      "learning_rate": 1.4730400330598363e-07,
      "loss": 1.702,
      "step": 461312
    },
    {
      "epoch": 0.001674308936012789,
      "grad_norm": 17375.958563486503,
      "learning_rate": 1.4729888953803752e-07,
      "loss": 1.7088,
      "step": 461344
    },
    {
      "epoch": 0.0016744250703636257,
      "grad_norm": 20142.35537368954,
      "learning_rate": 1.472937763026392e-07,
      "loss": 1.6968,
      "step": 461376
    },
    {
      "epoch": 0.0016745412047144623,
      "grad_norm": 18058.83938684876,
      "learning_rate": 1.4728866359969627e-07,
      "loss": 1.6938,
      "step": 461408
    },
    {
      "epoch": 0.001674657339065299,
      "grad_norm": 16039.433406451737,
      "learning_rate": 1.4728355142911626e-07,
      "loss": 1.6972,
      "step": 461440
    },
    {
      "epoch": 0.0016747734734161358,
      "grad_norm": 10720.65240552085,
      "learning_rate": 1.4727859952144813e-07,
      "loss": 1.6785,
      "step": 461472
    },
    {
      "epoch": 0.0016748896077669725,
      "grad_norm": 9786.471069798346,
      "learning_rate": 1.4727348839868776e-07,
      "loss": 1.6838,
      "step": 461504
    },
    {
      "epoch": 0.0016750057421178093,
      "grad_norm": 16870.767261745983,
      "learning_rate": 1.4726837780801616e-07,
      "loss": 1.6849,
      "step": 461536
    },
    {
      "epoch": 0.0016751218764686458,
      "grad_norm": 11190.536716351007,
      "learning_rate": 1.4726326774934101e-07,
      "loss": 1.6847,
      "step": 461568
    },
    {
      "epoch": 0.0016752380108194826,
      "grad_norm": 9214.815679111547,
      "learning_rate": 1.4725815822257003e-07,
      "loss": 1.6846,
      "step": 461600
    },
    {
      "epoch": 0.0016753541451703193,
      "grad_norm": 9787.80874353397,
      "learning_rate": 1.4725304922761093e-07,
      "loss": 1.6834,
      "step": 461632
    },
    {
      "epoch": 0.001675470279521156,
      "grad_norm": 12078.479374490813,
      "learning_rate": 1.4724794076437147e-07,
      "loss": 1.6518,
      "step": 461664
    },
    {
      "epoch": 0.0016755864138719926,
      "grad_norm": 10562.769523188508,
      "learning_rate": 1.4724283283275945e-07,
      "loss": 1.6533,
      "step": 461696
    },
    {
      "epoch": 0.0016757025482228294,
      "grad_norm": 12916.427679509532,
      "learning_rate": 1.4723772543268263e-07,
      "loss": 1.6632,
      "step": 461728
    },
    {
      "epoch": 0.0016758186825736661,
      "grad_norm": 8943.063457227618,
      "learning_rate": 1.4723261856404887e-07,
      "loss": 1.6792,
      "step": 461760
    },
    {
      "epoch": 0.0016759348169245029,
      "grad_norm": 10009.050504418488,
      "learning_rate": 1.4722751222676597e-07,
      "loss": 1.6692,
      "step": 461792
    },
    {
      "epoch": 0.0016760509512753396,
      "grad_norm": 12367.700837261547,
      "learning_rate": 1.472224064207418e-07,
      "loss": 1.6672,
      "step": 461824
    },
    {
      "epoch": 0.0016761670856261762,
      "grad_norm": 8222.983765033225,
      "learning_rate": 1.4721730114588427e-07,
      "loss": 1.6717,
      "step": 461856
    },
    {
      "epoch": 0.001676283219977013,
      "grad_norm": 11485.5324648011,
      "learning_rate": 1.4721219640210126e-07,
      "loss": 1.6676,
      "step": 461888
    },
    {
      "epoch": 0.0016763993543278497,
      "grad_norm": 10128.075631629139,
      "learning_rate": 1.4720709218930077e-07,
      "loss": 1.6714,
      "step": 461920
    },
    {
      "epoch": 0.0016765154886786864,
      "grad_norm": 8804.728502344635,
      "learning_rate": 1.4720198850739066e-07,
      "loss": 1.6784,
      "step": 461952
    },
    {
      "epoch": 0.001676631623029523,
      "grad_norm": 8675.53986792753,
      "learning_rate": 1.4719688535627894e-07,
      "loss": 1.6901,
      "step": 461984
    },
    {
      "epoch": 0.0016767477573803597,
      "grad_norm": 9519.201437095446,
      "learning_rate": 1.4719178273587365e-07,
      "loss": 1.6851,
      "step": 462016
    },
    {
      "epoch": 0.0016768638917311965,
      "grad_norm": 10416.720021196692,
      "learning_rate": 1.4718668064608278e-07,
      "loss": 1.6748,
      "step": 462048
    },
    {
      "epoch": 0.0016769800260820332,
      "grad_norm": 8568.123248413272,
      "learning_rate": 1.4718157908681437e-07,
      "loss": 1.6869,
      "step": 462080
    },
    {
      "epoch": 0.00167709616043287,
      "grad_norm": 9455.129613072473,
      "learning_rate": 1.4717647805797647e-07,
      "loss": 1.6885,
      "step": 462112
    },
    {
      "epoch": 0.0016772122947837065,
      "grad_norm": 8703.637630324461,
      "learning_rate": 1.4717137755947723e-07,
      "loss": 1.6876,
      "step": 462144
    },
    {
      "epoch": 0.0016773284291345433,
      "grad_norm": 13808.852957432779,
      "learning_rate": 1.4716627759122466e-07,
      "loss": 1.685,
      "step": 462176
    },
    {
      "epoch": 0.00167744456348538,
      "grad_norm": 8822.628859926048,
      "learning_rate": 1.4716117815312697e-07,
      "loss": 1.6917,
      "step": 462208
    },
    {
      "epoch": 0.0016775606978362168,
      "grad_norm": 10292.84100722439,
      "learning_rate": 1.471560792450923e-07,
      "loss": 1.6904,
      "step": 462240
    },
    {
      "epoch": 0.0016776768321870533,
      "grad_norm": 10185.232447028393,
      "learning_rate": 1.4715098086702885e-07,
      "loss": 1.679,
      "step": 462272
    },
    {
      "epoch": 0.00167779296653789,
      "grad_norm": 9679.656812098247,
      "learning_rate": 1.4714588301884475e-07,
      "loss": 1.6556,
      "step": 462304
    },
    {
      "epoch": 0.0016779091008887268,
      "grad_norm": 10098.67357626733,
      "learning_rate": 1.4714078570044828e-07,
      "loss": 1.6659,
      "step": 462336
    },
    {
      "epoch": 0.0016780252352395636,
      "grad_norm": 9404.287320153506,
      "learning_rate": 1.4713568891174768e-07,
      "loss": 1.6645,
      "step": 462368
    },
    {
      "epoch": 0.0016781413695904003,
      "grad_norm": 8195.642500743917,
      "learning_rate": 1.471305926526512e-07,
      "loss": 1.6663,
      "step": 462400
    },
    {
      "epoch": 0.0016782575039412369,
      "grad_norm": 8765.823749083711,
      "learning_rate": 1.4712549692306709e-07,
      "loss": 1.6698,
      "step": 462432
    },
    {
      "epoch": 0.0016783736382920736,
      "grad_norm": 9657.148026203182,
      "learning_rate": 1.4712040172290374e-07,
      "loss": 1.6759,
      "step": 462464
    },
    {
      "epoch": 0.0016784897726429104,
      "grad_norm": 20359.668759584474,
      "learning_rate": 1.471153070520694e-07,
      "loss": 1.7015,
      "step": 462496
    },
    {
      "epoch": 0.0016786059069937471,
      "grad_norm": 19173.141213687442,
      "learning_rate": 1.471102129104725e-07,
      "loss": 1.6898,
      "step": 462528
    },
    {
      "epoch": 0.0016787220413445837,
      "grad_norm": 19816.623930427708,
      "learning_rate": 1.471051192980214e-07,
      "loss": 1.6726,
      "step": 462560
    },
    {
      "epoch": 0.0016788381756954204,
      "grad_norm": 10183.216976967544,
      "learning_rate": 1.4710018536547338e-07,
      "loss": 1.6635,
      "step": 462592
    },
    {
      "epoch": 0.0016789543100462572,
      "grad_norm": 8363.434581557985,
      "learning_rate": 1.4709509279451036e-07,
      "loss": 1.6519,
      "step": 462624
    },
    {
      "epoch": 0.001679070444397094,
      "grad_norm": 8332.718283969523,
      "learning_rate": 1.4709000075242128e-07,
      "loss": 1.657,
      "step": 462656
    },
    {
      "epoch": 0.0016791865787479307,
      "grad_norm": 8277.904324163212,
      "learning_rate": 1.4708490923911453e-07,
      "loss": 1.6743,
      "step": 462688
    },
    {
      "epoch": 0.0016793027130987672,
      "grad_norm": 11125.232941381499,
      "learning_rate": 1.4707981825449866e-07,
      "loss": 1.6755,
      "step": 462720
    },
    {
      "epoch": 0.001679418847449604,
      "grad_norm": 11734.375824900104,
      "learning_rate": 1.4707472779848216e-07,
      "loss": 1.6875,
      "step": 462752
    },
    {
      "epoch": 0.0016795349818004407,
      "grad_norm": 15037.20891655097,
      "learning_rate": 1.4706963787097358e-07,
      "loss": 1.6546,
      "step": 462784
    },
    {
      "epoch": 0.0016796511161512775,
      "grad_norm": 9243.306983974946,
      "learning_rate": 1.4706454847188145e-07,
      "loss": 1.6629,
      "step": 462816
    },
    {
      "epoch": 0.001679767250502114,
      "grad_norm": 12361.628371699257,
      "learning_rate": 1.4705945960111434e-07,
      "loss": 1.6648,
      "step": 462848
    },
    {
      "epoch": 0.0016798833848529508,
      "grad_norm": 8903.0850832731,
      "learning_rate": 1.470543712585809e-07,
      "loss": 1.6596,
      "step": 462880
    },
    {
      "epoch": 0.0016799995192037875,
      "grad_norm": 7958.438414664022,
      "learning_rate": 1.470492834441897e-07,
      "loss": 1.67,
      "step": 462912
    },
    {
      "epoch": 0.0016801156535546243,
      "grad_norm": 11839.809289004615,
      "learning_rate": 1.4704419615784938e-07,
      "loss": 1.672,
      "step": 462944
    },
    {
      "epoch": 0.001680231787905461,
      "grad_norm": 10583.997732426062,
      "learning_rate": 1.4703910939946862e-07,
      "loss": 1.6867,
      "step": 462976
    },
    {
      "epoch": 0.0016803479222562976,
      "grad_norm": 8718.186738078051,
      "learning_rate": 1.4703402316895615e-07,
      "loss": 1.7041,
      "step": 463008
    },
    {
      "epoch": 0.0016804640566071343,
      "grad_norm": 8928.56864228528,
      "learning_rate": 1.470289374662206e-07,
      "loss": 1.7153,
      "step": 463040
    },
    {
      "epoch": 0.001680580190957971,
      "grad_norm": 10849.723498780972,
      "learning_rate": 1.4702385229117075e-07,
      "loss": 1.72,
      "step": 463072
    },
    {
      "epoch": 0.0016806963253088079,
      "grad_norm": 9154.939541034664,
      "learning_rate": 1.4701876764371534e-07,
      "loss": 1.6887,
      "step": 463104
    },
    {
      "epoch": 0.0016808124596596444,
      "grad_norm": 8204.710841949276,
      "learning_rate": 1.4701368352376312e-07,
      "loss": 1.6571,
      "step": 463136
    },
    {
      "epoch": 0.0016809285940104811,
      "grad_norm": 8010.555536290851,
      "learning_rate": 1.4700859993122293e-07,
      "loss": 1.6626,
      "step": 463168
    },
    {
      "epoch": 0.001681044728361318,
      "grad_norm": 12820.833826237667,
      "learning_rate": 1.470035168660036e-07,
      "loss": 1.668,
      "step": 463200
    },
    {
      "epoch": 0.0016811608627121547,
      "grad_norm": 9357.083519986343,
      "learning_rate": 1.4699843432801392e-07,
      "loss": 1.6998,
      "step": 463232
    },
    {
      "epoch": 0.0016812769970629912,
      "grad_norm": 10407.036081421069,
      "learning_rate": 1.4699335231716278e-07,
      "loss": 1.6603,
      "step": 463264
    },
    {
      "epoch": 0.001681393131413828,
      "grad_norm": 9861.214124031583,
      "learning_rate": 1.4698827083335905e-07,
      "loss": 1.6582,
      "step": 463296
    },
    {
      "epoch": 0.0016815092657646647,
      "grad_norm": 11441.993707392083,
      "learning_rate": 1.469831898765117e-07,
      "loss": 1.6691,
      "step": 463328
    },
    {
      "epoch": 0.0016816254001155015,
      "grad_norm": 9402.296315262565,
      "learning_rate": 1.4697810944652954e-07,
      "loss": 1.6782,
      "step": 463360
    },
    {
      "epoch": 0.0016817415344663382,
      "grad_norm": 12660.49967418348,
      "learning_rate": 1.4697302954332164e-07,
      "loss": 1.6801,
      "step": 463392
    },
    {
      "epoch": 0.0016818576688171747,
      "grad_norm": 10209.423294192478,
      "learning_rate": 1.469679501667969e-07,
      "loss": 1.6521,
      "step": 463424
    },
    {
      "epoch": 0.0016819738031680115,
      "grad_norm": 11173.472871045959,
      "learning_rate": 1.4696287131686436e-07,
      "loss": 1.6701,
      "step": 463456
    },
    {
      "epoch": 0.0016820899375188483,
      "grad_norm": 9945.20326589658,
      "learning_rate": 1.46957792993433e-07,
      "loss": 1.6772,
      "step": 463488
    },
    {
      "epoch": 0.001682206071869685,
      "grad_norm": 11555.762372080866,
      "learning_rate": 1.469527151964119e-07,
      "loss": 1.674,
      "step": 463520
    },
    {
      "epoch": 0.0016823222062205215,
      "grad_norm": 10533.738177873987,
      "learning_rate": 1.4694763792571009e-07,
      "loss": 1.6849,
      "step": 463552
    },
    {
      "epoch": 0.0016824383405713583,
      "grad_norm": 9012.222589350531,
      "learning_rate": 1.4694256118123664e-07,
      "loss": 1.6918,
      "step": 463584
    },
    {
      "epoch": 0.001682554474922195,
      "grad_norm": 17949.86930314536,
      "learning_rate": 1.469374849629007e-07,
      "loss": 1.6804,
      "step": 463616
    },
    {
      "epoch": 0.0016826706092730318,
      "grad_norm": 11333.579840456412,
      "learning_rate": 1.4693256787803372e-07,
      "loss": 1.6759,
      "step": 463648
    },
    {
      "epoch": 0.0016827867436238686,
      "grad_norm": 11449.647592830097,
      "learning_rate": 1.469274926952654e-07,
      "loss": 1.665,
      "step": 463680
    },
    {
      "epoch": 0.001682902877974705,
      "grad_norm": 7727.453396818385,
      "learning_rate": 1.4692241803836488e-07,
      "loss": 1.6866,
      "step": 463712
    },
    {
      "epoch": 0.0016830190123255419,
      "grad_norm": 12212.358494574255,
      "learning_rate": 1.469173439072413e-07,
      "loss": 1.6765,
      "step": 463744
    },
    {
      "epoch": 0.0016831351466763786,
      "grad_norm": 9932.33467015686,
      "learning_rate": 1.4691227030180394e-07,
      "loss": 1.6793,
      "step": 463776
    },
    {
      "epoch": 0.0016832512810272154,
      "grad_norm": 12183.368171404818,
      "learning_rate": 1.46907197221962e-07,
      "loss": 1.6771,
      "step": 463808
    },
    {
      "epoch": 0.001683367415378052,
      "grad_norm": 11138.868344674875,
      "learning_rate": 1.4690212466762473e-07,
      "loss": 1.6721,
      "step": 463840
    },
    {
      "epoch": 0.0016834835497288887,
      "grad_norm": 10353.92708106446,
      "learning_rate": 1.4689705263870138e-07,
      "loss": 1.685,
      "step": 463872
    },
    {
      "epoch": 0.0016835996840797254,
      "grad_norm": 13947.2922103181,
      "learning_rate": 1.4689198113510136e-07,
      "loss": 1.6801,
      "step": 463904
    },
    {
      "epoch": 0.0016837158184305622,
      "grad_norm": 10118.661571571607,
      "learning_rate": 1.4688691015673387e-07,
      "loss": 1.685,
      "step": 463936
    },
    {
      "epoch": 0.001683831952781399,
      "grad_norm": 10689.563695492909,
      "learning_rate": 1.4688183970350835e-07,
      "loss": 1.6934,
      "step": 463968
    },
    {
      "epoch": 0.0016839480871322355,
      "grad_norm": 10185.839778830217,
      "learning_rate": 1.468767697753341e-07,
      "loss": 1.684,
      "step": 464000
    },
    {
      "epoch": 0.0016840642214830722,
      "grad_norm": 13113.921305238948,
      "learning_rate": 1.4687170037212057e-07,
      "loss": 1.6732,
      "step": 464032
    },
    {
      "epoch": 0.001684180355833909,
      "grad_norm": 7770.377210920973,
      "learning_rate": 1.4686663149377714e-07,
      "loss": 1.6693,
      "step": 464064
    },
    {
      "epoch": 0.0016842964901847457,
      "grad_norm": 10026.275878909377,
      "learning_rate": 1.4686156314021324e-07,
      "loss": 1.6711,
      "step": 464096
    },
    {
      "epoch": 0.0016844126245355823,
      "grad_norm": 9714.097590615404,
      "learning_rate": 1.468564953113383e-07,
      "loss": 1.6821,
      "step": 464128
    },
    {
      "epoch": 0.001684528758886419,
      "grad_norm": 8652.436651024958,
      "learning_rate": 1.4685142800706189e-07,
      "loss": 1.6846,
      "step": 464160
    },
    {
      "epoch": 0.0016846448932372558,
      "grad_norm": 11374.455239702691,
      "learning_rate": 1.468463612272934e-07,
      "loss": 1.6801,
      "step": 464192
    },
    {
      "epoch": 0.0016847610275880925,
      "grad_norm": 10341.223912090869,
      "learning_rate": 1.4684129497194243e-07,
      "loss": 1.6943,
      "step": 464224
    },
    {
      "epoch": 0.0016848771619389293,
      "grad_norm": 9412.714167550186,
      "learning_rate": 1.468362292409185e-07,
      "loss": 1.6676,
      "step": 464256
    },
    {
      "epoch": 0.0016849932962897658,
      "grad_norm": 10954.0363336991,
      "learning_rate": 1.4683116403413113e-07,
      "loss": 1.6606,
      "step": 464288
    },
    {
      "epoch": 0.0016851094306406026,
      "grad_norm": 11048.632223040098,
      "learning_rate": 1.4682609935149e-07,
      "loss": 1.6636,
      "step": 464320
    },
    {
      "epoch": 0.0016852255649914393,
      "grad_norm": 8443.764207982125,
      "learning_rate": 1.4682103519290462e-07,
      "loss": 1.6654,
      "step": 464352
    },
    {
      "epoch": 0.001685341699342276,
      "grad_norm": 10624.363980963753,
      "learning_rate": 1.468159715582847e-07,
      "loss": 1.6592,
      "step": 464384
    },
    {
      "epoch": 0.0016854578336931126,
      "grad_norm": 9211.035989507369,
      "learning_rate": 1.4681090844753982e-07,
      "loss": 1.6515,
      "step": 464416
    },
    {
      "epoch": 0.0016855739680439494,
      "grad_norm": 9464.09678733264,
      "learning_rate": 1.468058458605797e-07,
      "loss": 1.6668,
      "step": 464448
    },
    {
      "epoch": 0.0016856901023947861,
      "grad_norm": 10832.724680337813,
      "learning_rate": 1.4680078379731404e-07,
      "loss": 1.6654,
      "step": 464480
    },
    {
      "epoch": 0.0016858062367456229,
      "grad_norm": 13215.893764706192,
      "learning_rate": 1.4679572225765256e-07,
      "loss": 1.6775,
      "step": 464512
    },
    {
      "epoch": 0.0016859223710964596,
      "grad_norm": 9978.078372111537,
      "learning_rate": 1.46790661241505e-07,
      "loss": 1.6653,
      "step": 464544
    },
    {
      "epoch": 0.0016860385054472962,
      "grad_norm": 9452.60651883913,
      "learning_rate": 1.4678560074878108e-07,
      "loss": 1.6791,
      "step": 464576
    },
    {
      "epoch": 0.001686154639798133,
      "grad_norm": 11544.222277832318,
      "learning_rate": 1.4678054077939059e-07,
      "loss": 1.6558,
      "step": 464608
    },
    {
      "epoch": 0.0016862707741489697,
      "grad_norm": 26357.582286696936,
      "learning_rate": 1.467754813332434e-07,
      "loss": 1.6818,
      "step": 464640
    },
    {
      "epoch": 0.0016863869084998064,
      "grad_norm": 21010.918875670337,
      "learning_rate": 1.4677042241024924e-07,
      "loss": 1.6882,
      "step": 464672
    },
    {
      "epoch": 0.001686503042850643,
      "grad_norm": 9903.069928057663,
      "learning_rate": 1.4676552207739932e-07,
      "loss": 1.6872,
      "step": 464704
    },
    {
      "epoch": 0.0016866191772014797,
      "grad_norm": 13674.737291809302,
      "learning_rate": 1.467604641840994e-07,
      "loss": 1.6744,
      "step": 464736
    },
    {
      "epoch": 0.0016867353115523165,
      "grad_norm": 9848.75718047714,
      "learning_rate": 1.4675540681368495e-07,
      "loss": 1.6735,
      "step": 464768
    },
    {
      "epoch": 0.0016868514459031532,
      "grad_norm": 9913.736732433437,
      "learning_rate": 1.4675034996606597e-07,
      "loss": 1.6885,
      "step": 464800
    },
    {
      "epoch": 0.00168696758025399,
      "grad_norm": 9519.240936125107,
      "learning_rate": 1.4674529364115228e-07,
      "loss": 1.6766,
      "step": 464832
    },
    {
      "epoch": 0.0016870837146048265,
      "grad_norm": 13237.137908173354,
      "learning_rate": 1.4674023783885396e-07,
      "loss": 1.6807,
      "step": 464864
    },
    {
      "epoch": 0.0016871998489556633,
      "grad_norm": 8857.495808635756,
      "learning_rate": 1.467351825590809e-07,
      "loss": 1.6557,
      "step": 464896
    },
    {
      "epoch": 0.0016873159833065,
      "grad_norm": 9955.36659294875,
      "learning_rate": 1.4673012780174313e-07,
      "loss": 1.6555,
      "step": 464928
    },
    {
      "epoch": 0.0016874321176573368,
      "grad_norm": 11515.506241585734,
      "learning_rate": 1.4672507356675068e-07,
      "loss": 1.665,
      "step": 464960
    },
    {
      "epoch": 0.0016875482520081733,
      "grad_norm": 10805.10768109231,
      "learning_rate": 1.4672001985401358e-07,
      "loss": 1.6441,
      "step": 464992
    },
    {
      "epoch": 0.00168766438635901,
      "grad_norm": 10118.52677023686,
      "learning_rate": 1.467149666634419e-07,
      "loss": 1.6575,
      "step": 465024
    },
    {
      "epoch": 0.0016877805207098468,
      "grad_norm": 9975.036641536712,
      "learning_rate": 1.4670991399494573e-07,
      "loss": 1.6432,
      "step": 465056
    },
    {
      "epoch": 0.0016878966550606836,
      "grad_norm": 10877.529039262548,
      "learning_rate": 1.4670486184843517e-07,
      "loss": 1.6735,
      "step": 465088
    },
    {
      "epoch": 0.0016880127894115203,
      "grad_norm": 12552.842865263628,
      "learning_rate": 1.4669981022382037e-07,
      "loss": 1.6758,
      "step": 465120
    },
    {
      "epoch": 0.0016881289237623569,
      "grad_norm": 9259.956695363106,
      "learning_rate": 1.4669475912101142e-07,
      "loss": 1.6684,
      "step": 465152
    },
    {
      "epoch": 0.0016882450581131936,
      "grad_norm": 8987.577871707148,
      "learning_rate": 1.4668970853991857e-07,
      "loss": 1.673,
      "step": 465184
    },
    {
      "epoch": 0.0016883611924640304,
      "grad_norm": 11061.581984508364,
      "learning_rate": 1.4668465848045199e-07,
      "loss": 1.6738,
      "step": 465216
    },
    {
      "epoch": 0.0016884773268148671,
      "grad_norm": 10806.503042150129,
      "learning_rate": 1.466796089425219e-07,
      "loss": 1.687,
      "step": 465248
    },
    {
      "epoch": 0.0016885934611657037,
      "grad_norm": 10271.636675817539,
      "learning_rate": 1.466745599260385e-07,
      "loss": 1.6523,
      "step": 465280
    },
    {
      "epoch": 0.0016887095955165404,
      "grad_norm": 9835.292471502817,
      "learning_rate": 1.466695114309121e-07,
      "loss": 1.6643,
      "step": 465312
    },
    {
      "epoch": 0.0016888257298673772,
      "grad_norm": 12611.266391603976,
      "learning_rate": 1.4666446345705295e-07,
      "loss": 1.6582,
      "step": 465344
    },
    {
      "epoch": 0.001688941864218214,
      "grad_norm": 11679.2570825374,
      "learning_rate": 1.4665941600437137e-07,
      "loss": 1.681,
      "step": 465376
    },
    {
      "epoch": 0.0016890579985690507,
      "grad_norm": 8292.220450518667,
      "learning_rate": 1.4665436907277765e-07,
      "loss": 1.6721,
      "step": 465408
    },
    {
      "epoch": 0.0016891741329198872,
      "grad_norm": 9224.187769120921,
      "learning_rate": 1.466493226621822e-07,
      "loss": 1.645,
      "step": 465440
    },
    {
      "epoch": 0.001689290267270724,
      "grad_norm": 9270.509155380842,
      "learning_rate": 1.466442767724953e-07,
      "loss": 1.6657,
      "step": 465472
    },
    {
      "epoch": 0.0016894064016215607,
      "grad_norm": 11408.391735910895,
      "learning_rate": 1.4663923140362744e-07,
      "loss": 1.6485,
      "step": 465504
    },
    {
      "epoch": 0.0016895225359723975,
      "grad_norm": 8484.458733472631,
      "learning_rate": 1.4663418655548897e-07,
      "loss": 1.6705,
      "step": 465536
    },
    {
      "epoch": 0.001689638670323234,
      "grad_norm": 10028.59551482659,
      "learning_rate": 1.4662914222799032e-07,
      "loss": 1.663,
      "step": 465568
    },
    {
      "epoch": 0.0016897548046740708,
      "grad_norm": 10651.603635134008,
      "learning_rate": 1.46624098421042e-07,
      "loss": 1.661,
      "step": 465600
    },
    {
      "epoch": 0.0016898709390249075,
      "grad_norm": 9277.382173867798,
      "learning_rate": 1.466190551345544e-07,
      "loss": 1.6855,
      "step": 465632
    },
    {
      "epoch": 0.0016899870733757443,
      "grad_norm": 9462.445455589163,
      "learning_rate": 1.4661401236843809e-07,
      "loss": 1.6886,
      "step": 465664
    },
    {
      "epoch": 0.001690103207726581,
      "grad_norm": 18407.4176352904,
      "learning_rate": 1.4660897012260356e-07,
      "loss": 1.6931,
      "step": 465696
    },
    {
      "epoch": 0.0016902193420774176,
      "grad_norm": 28278.493877857072,
      "learning_rate": 1.4660392839696129e-07,
      "loss": 1.691,
      "step": 465728
    },
    {
      "epoch": 0.0016903354764282543,
      "grad_norm": 14178.96202124824,
      "learning_rate": 1.465990447212233e-07,
      "loss": 1.6777,
      "step": 465760
    },
    {
      "epoch": 0.001690451610779091,
      "grad_norm": 8886.92455239719,
      "learning_rate": 1.4659400401944836e-07,
      "loss": 1.6676,
      "step": 465792
    },
    {
      "epoch": 0.0016905677451299278,
      "grad_norm": 8486.719389728873,
      "learning_rate": 1.4658896383760027e-07,
      "loss": 1.6679,
      "step": 465824
    },
    {
      "epoch": 0.0016906838794807644,
      "grad_norm": 13973.166427120232,
      "learning_rate": 1.465839241755897e-07,
      "loss": 1.6633,
      "step": 465856
    },
    {
      "epoch": 0.0016908000138316011,
      "grad_norm": 10661.928718576204,
      "learning_rate": 1.4657888503332724e-07,
      "loss": 1.6549,
      "step": 465888
    },
    {
      "epoch": 0.0016909161481824379,
      "grad_norm": 9878.26462492274,
      "learning_rate": 1.465738464107236e-07,
      "loss": 1.669,
      "step": 465920
    },
    {
      "epoch": 0.0016910322825332746,
      "grad_norm": 11097.147020743665,
      "learning_rate": 1.4656880830768944e-07,
      "loss": 1.6755,
      "step": 465952
    },
    {
      "epoch": 0.0016911484168841114,
      "grad_norm": 8693.070228636141,
      "learning_rate": 1.465637707241355e-07,
      "loss": 1.6907,
      "step": 465984
    },
    {
      "epoch": 0.001691264551234948,
      "grad_norm": 8139.846927307663,
      "learning_rate": 1.465587336599725e-07,
      "loss": 1.6824,
      "step": 466016
    },
    {
      "epoch": 0.0016913806855857847,
      "grad_norm": 8206.648767919825,
      "learning_rate": 1.4655369711511118e-07,
      "loss": 1.6597,
      "step": 466048
    },
    {
      "epoch": 0.0016914968199366214,
      "grad_norm": 10862.36493586917,
      "learning_rate": 1.4654866108946236e-07,
      "loss": 1.6688,
      "step": 466080
    },
    {
      "epoch": 0.0016916129542874582,
      "grad_norm": 8534.706204668091,
      "learning_rate": 1.4654362558293676e-07,
      "loss": 1.6752,
      "step": 466112
    },
    {
      "epoch": 0.0016917290886382947,
      "grad_norm": 9343.255535411627,
      "learning_rate": 1.4653859059544532e-07,
      "loss": 1.674,
      "step": 466144
    },
    {
      "epoch": 0.0016918452229891315,
      "grad_norm": 14581.431891278715,
      "learning_rate": 1.4653355612689876e-07,
      "loss": 1.6662,
      "step": 466176
    },
    {
      "epoch": 0.0016919613573399682,
      "grad_norm": 10475.043293466619,
      "learning_rate": 1.4652852217720802e-07,
      "loss": 1.6721,
      "step": 466208
    },
    {
      "epoch": 0.001692077491690805,
      "grad_norm": 10924.549418625924,
      "learning_rate": 1.465234887462839e-07,
      "loss": 1.6732,
      "step": 466240
    },
    {
      "epoch": 0.0016921936260416417,
      "grad_norm": 10020.962029665616,
      "learning_rate": 1.465184558340374e-07,
      "loss": 1.6977,
      "step": 466272
    },
    {
      "epoch": 0.0016923097603924783,
      "grad_norm": 9686.839525872203,
      "learning_rate": 1.465134234403794e-07,
      "loss": 1.6925,
      "step": 466304
    },
    {
      "epoch": 0.001692425894743315,
      "grad_norm": 14110.544709542577,
      "learning_rate": 1.4650839156522087e-07,
      "loss": 1.6744,
      "step": 466336
    },
    {
      "epoch": 0.0016925420290941518,
      "grad_norm": 9073.2470483284,
      "learning_rate": 1.4650336020847274e-07,
      "loss": 1.6722,
      "step": 466368
    },
    {
      "epoch": 0.0016926581634449885,
      "grad_norm": 10924.430145321083,
      "learning_rate": 1.4649832937004603e-07,
      "loss": 1.6722,
      "step": 466400
    },
    {
      "epoch": 0.001692774297795825,
      "grad_norm": 11242.619623557492,
      "learning_rate": 1.4649329904985174e-07,
      "loss": 1.6858,
      "step": 466432
    },
    {
      "epoch": 0.0016928904321466618,
      "grad_norm": 9827.343689929645,
      "learning_rate": 1.464882692478009e-07,
      "loss": 1.6823,
      "step": 466464
    },
    {
      "epoch": 0.0016930065664974986,
      "grad_norm": 11446.359770686922,
      "learning_rate": 1.4648323996380461e-07,
      "loss": 1.6986,
      "step": 466496
    },
    {
      "epoch": 0.0016931227008483353,
      "grad_norm": 10736.441309856818,
      "learning_rate": 1.4647821119777385e-07,
      "loss": 1.6689,
      "step": 466528
    },
    {
      "epoch": 0.001693238835199172,
      "grad_norm": 14099.826452832674,
      "learning_rate": 1.4647318294961982e-07,
      "loss": 1.6733,
      "step": 466560
    },
    {
      "epoch": 0.0016933549695500086,
      "grad_norm": 9369.303922917647,
      "learning_rate": 1.464681552192536e-07,
      "loss": 1.6838,
      "step": 466592
    },
    {
      "epoch": 0.0016934711039008454,
      "grad_norm": 8995.065536170374,
      "learning_rate": 1.464631280065863e-07,
      "loss": 1.6563,
      "step": 466624
    },
    {
      "epoch": 0.0016935872382516821,
      "grad_norm": 9781.666115749402,
      "learning_rate": 1.464581013115291e-07,
      "loss": 1.6538,
      "step": 466656
    },
    {
      "epoch": 0.001693703372602519,
      "grad_norm": 10610.65634162185,
      "learning_rate": 1.464530751339932e-07,
      "loss": 1.6389,
      "step": 466688
    },
    {
      "epoch": 0.0016938195069533554,
      "grad_norm": 9072.151453762222,
      "learning_rate": 1.464480494738898e-07,
      "loss": 1.6625,
      "step": 466720
    },
    {
      "epoch": 0.0016939356413041922,
      "grad_norm": 24290.814066226765,
      "learning_rate": 1.464430243311301e-07,
      "loss": 1.6624,
      "step": 466752
    },
    {
      "epoch": 0.001694051775655029,
      "grad_norm": 18664.821242112124,
      "learning_rate": 1.4643799970562538e-07,
      "loss": 1.6542,
      "step": 466784
    },
    {
      "epoch": 0.0016941679100058657,
      "grad_norm": 22704.094080143346,
      "learning_rate": 1.4643297559728688e-07,
      "loss": 1.6717,
      "step": 466816
    },
    {
      "epoch": 0.0016942840443567025,
      "grad_norm": 8823.488652454878,
      "learning_rate": 1.4642810898542687e-07,
      "loss": 1.685,
      "step": 466848
    },
    {
      "epoch": 0.001694400178707539,
      "grad_norm": 9666.67740229289,
      "learning_rate": 1.4642308589500016e-07,
      "loss": 1.6893,
      "step": 466880
    },
    {
      "epoch": 0.0016945163130583757,
      "grad_norm": 11700.043760601924,
      "learning_rate": 1.4641806332147642e-07,
      "loss": 1.6473,
      "step": 466912
    },
    {
      "epoch": 0.0016946324474092125,
      "grad_norm": 7608.437684570992,
      "learning_rate": 1.4641304126476695e-07,
      "loss": 1.6543,
      "step": 466944
    },
    {
      "epoch": 0.0016947485817600492,
      "grad_norm": 9674.340184219283,
      "learning_rate": 1.4640801972478316e-07,
      "loss": 1.6561,
      "step": 466976
    },
    {
      "epoch": 0.0016948647161108858,
      "grad_norm": 10089.875717767785,
      "learning_rate": 1.4640299870143638e-07,
      "loss": 1.6678,
      "step": 467008
    },
    {
      "epoch": 0.0016949808504617225,
      "grad_norm": 10306.03551323204,
      "learning_rate": 1.4639797819463812e-07,
      "loss": 1.667,
      "step": 467040
    },
    {
      "epoch": 0.0016950969848125593,
      "grad_norm": 9618.125597017332,
      "learning_rate": 1.4639295820429975e-07,
      "loss": 1.645,
      "step": 467072
    },
    {
      "epoch": 0.001695213119163396,
      "grad_norm": 10996.211711312219,
      "learning_rate": 1.4638793873033276e-07,
      "loss": 1.6523,
      "step": 467104
    },
    {
      "epoch": 0.0016953292535142328,
      "grad_norm": 12801.74738072893,
      "learning_rate": 1.4638291977264862e-07,
      "loss": 1.6549,
      "step": 467136
    },
    {
      "epoch": 0.0016954453878650693,
      "grad_norm": 9987.073445209062,
      "learning_rate": 1.4637790133115883e-07,
      "loss": 1.6594,
      "step": 467168
    },
    {
      "epoch": 0.001695561522215906,
      "grad_norm": 10139.799998027574,
      "learning_rate": 1.463728834057749e-07,
      "loss": 1.6475,
      "step": 467200
    },
    {
      "epoch": 0.0016956776565667428,
      "grad_norm": 8809.31768072874,
      "learning_rate": 1.4636786599640842e-07,
      "loss": 1.6519,
      "step": 467232
    },
    {
      "epoch": 0.0016957937909175796,
      "grad_norm": 10710.130904895608,
      "learning_rate": 1.4636284910297092e-07,
      "loss": 1.6576,
      "step": 467264
    },
    {
      "epoch": 0.0016959099252684161,
      "grad_norm": 12080.030629100242,
      "learning_rate": 1.46357832725374e-07,
      "loss": 1.6666,
      "step": 467296
    },
    {
      "epoch": 0.001696026059619253,
      "grad_norm": 17090.57412727846,
      "learning_rate": 1.463528168635292e-07,
      "loss": 1.6848,
      "step": 467328
    },
    {
      "epoch": 0.0016961421939700896,
      "grad_norm": 10192.743104778026,
      "learning_rate": 1.4634780151734827e-07,
      "loss": 1.6827,
      "step": 467360
    },
    {
      "epoch": 0.0016962583283209264,
      "grad_norm": 13755.233622152697,
      "learning_rate": 1.4634278668674276e-07,
      "loss": 1.6931,
      "step": 467392
    },
    {
      "epoch": 0.0016963744626717632,
      "grad_norm": 10568.815638471513,
      "learning_rate": 1.463377723716244e-07,
      "loss": 1.6867,
      "step": 467424
    },
    {
      "epoch": 0.0016964905970225997,
      "grad_norm": 11401.872302389638,
      "learning_rate": 1.4633275857190483e-07,
      "loss": 1.6868,
      "step": 467456
    },
    {
      "epoch": 0.0016966067313734364,
      "grad_norm": 12012.31742837326,
      "learning_rate": 1.463277452874958e-07,
      "loss": 1.6743,
      "step": 467488
    },
    {
      "epoch": 0.0016967228657242732,
      "grad_norm": 10582.837993657467,
      "learning_rate": 1.4632273251830906e-07,
      "loss": 1.6447,
      "step": 467520
    },
    {
      "epoch": 0.00169683900007511,
      "grad_norm": 9861.936523827357,
      "learning_rate": 1.4631772026425635e-07,
      "loss": 1.653,
      "step": 467552
    },
    {
      "epoch": 0.0016969551344259465,
      "grad_norm": 9441.872589693212,
      "learning_rate": 1.463127085252494e-07,
      "loss": 1.6613,
      "step": 467584
    },
    {
      "epoch": 0.0016970712687767832,
      "grad_norm": 9925.483162043043,
      "learning_rate": 1.4630769730120007e-07,
      "loss": 1.6561,
      "step": 467616
    },
    {
      "epoch": 0.00169718740312762,
      "grad_norm": 8732.225833085171,
      "learning_rate": 1.4630268659202016e-07,
      "loss": 1.6496,
      "step": 467648
    },
    {
      "epoch": 0.0016973035374784568,
      "grad_norm": 10338.664323789606,
      "learning_rate": 1.462976763976215e-07,
      "loss": 1.6424,
      "step": 467680
    },
    {
      "epoch": 0.0016974196718292935,
      "grad_norm": 10388.20484973222,
      "learning_rate": 1.46292666717916e-07,
      "loss": 1.653,
      "step": 467712
    },
    {
      "epoch": 0.00169753580618013,
      "grad_norm": 10931.03965778187,
      "learning_rate": 1.4628765755281546e-07,
      "loss": 1.6727,
      "step": 467744
    },
    {
      "epoch": 0.0016976519405309668,
      "grad_norm": 10170.5486577667,
      "learning_rate": 1.4628264890223184e-07,
      "loss": 1.6689,
      "step": 467776
    },
    {
      "epoch": 0.0016977680748818036,
      "grad_norm": 10446.868238855124,
      "learning_rate": 1.4627764076607708e-07,
      "loss": 1.6416,
      "step": 467808
    },
    {
      "epoch": 0.0016978842092326403,
      "grad_norm": 17751.38124203297,
      "learning_rate": 1.4627263314426307e-07,
      "loss": 1.6592,
      "step": 467840
    },
    {
      "epoch": 0.0016980003435834768,
      "grad_norm": 29481.358516866214,
      "learning_rate": 1.4626762603670182e-07,
      "loss": 1.6554,
      "step": 467872
    },
    {
      "epoch": 0.0016981164779343136,
      "grad_norm": 19991.62904817914,
      "learning_rate": 1.462626194433053e-07,
      "loss": 1.674,
      "step": 467904
    },
    {
      "epoch": 0.0016982326122851504,
      "grad_norm": 10569.145093147317,
      "learning_rate": 1.4625776979618367e-07,
      "loss": 1.6771,
      "step": 467936
    },
    {
      "epoch": 0.0016983487466359871,
      "grad_norm": 12305.594662591484,
      "learning_rate": 1.4625276421479186e-07,
      "loss": 1.6614,
      "step": 467968
    },
    {
      "epoch": 0.0016984648809868239,
      "grad_norm": 10313.413983739816,
      "learning_rate": 1.4624775914730358e-07,
      "loss": 1.6651,
      "step": 468000
    },
    {
      "epoch": 0.0016985810153376604,
      "grad_norm": 8421.23233262211,
      "learning_rate": 1.4624275459363097e-07,
      "loss": 1.6583,
      "step": 468032
    },
    {
      "epoch": 0.0016986971496884972,
      "grad_norm": 11453.19702091953,
      "learning_rate": 1.4623775055368612e-07,
      "loss": 1.6564,
      "step": 468064
    },
    {
      "epoch": 0.001698813284039334,
      "grad_norm": 15097.18834750365,
      "learning_rate": 1.4623274702738107e-07,
      "loss": 1.6571,
      "step": 468096
    },
    {
      "epoch": 0.0016989294183901707,
      "grad_norm": 8712.43548039238,
      "learning_rate": 1.4622774401462805e-07,
      "loss": 1.6595,
      "step": 468128
    },
    {
      "epoch": 0.0016990455527410072,
      "grad_norm": 13414.09408048117,
      "learning_rate": 1.4622274151533917e-07,
      "loss": 1.6496,
      "step": 468160
    },
    {
      "epoch": 0.001699161687091844,
      "grad_norm": 9097.264533913478,
      "learning_rate": 1.4621773952942656e-07,
      "loss": 1.6687,
      "step": 468192
    },
    {
      "epoch": 0.0016992778214426807,
      "grad_norm": 9816.716457145943,
      "learning_rate": 1.4621273805680252e-07,
      "loss": 1.6707,
      "step": 468224
    },
    {
      "epoch": 0.0016993939557935175,
      "grad_norm": 10609.56191367014,
      "learning_rate": 1.462077370973792e-07,
      "loss": 1.6683,
      "step": 468256
    },
    {
      "epoch": 0.0016995100901443542,
      "grad_norm": 14964.586329063693,
      "learning_rate": 1.4620273665106888e-07,
      "loss": 1.6811,
      "step": 468288
    },
    {
      "epoch": 0.0016996262244951908,
      "grad_norm": 9306.085965646353,
      "learning_rate": 1.4619773671778378e-07,
      "loss": 1.676,
      "step": 468320
    },
    {
      "epoch": 0.0016997423588460275,
      "grad_norm": 12156.251642673411,
      "learning_rate": 1.461927372974362e-07,
      "loss": 1.6862,
      "step": 468352
    },
    {
      "epoch": 0.0016998584931968643,
      "grad_norm": 8673.977403705869,
      "learning_rate": 1.4618773838993843e-07,
      "loss": 1.6723,
      "step": 468384
    },
    {
      "epoch": 0.001699974627547701,
      "grad_norm": 11137.548742878747,
      "learning_rate": 1.4618273999520284e-07,
      "loss": 1.6543,
      "step": 468416
    },
    {
      "epoch": 0.0017000907618985376,
      "grad_norm": 9328.282800172816,
      "learning_rate": 1.461777421131417e-07,
      "loss": 1.6738,
      "step": 468448
    },
    {
      "epoch": 0.0017002068962493743,
      "grad_norm": 11745.834836230246,
      "learning_rate": 1.4617274474366744e-07,
      "loss": 1.6793,
      "step": 468480
    },
    {
      "epoch": 0.001700323030600211,
      "grad_norm": 9122.529254543391,
      "learning_rate": 1.4616774788669243e-07,
      "loss": 1.6831,
      "step": 468512
    },
    {
      "epoch": 0.0017004391649510478,
      "grad_norm": 10102.047317252083,
      "learning_rate": 1.461627515421291e-07,
      "loss": 1.6424,
      "step": 468544
    },
    {
      "epoch": 0.0017005552993018846,
      "grad_norm": 9512.266186351178,
      "learning_rate": 1.461577557098898e-07,
      "loss": 1.6644,
      "step": 468576
    },
    {
      "epoch": 0.001700671433652721,
      "grad_norm": 10123.336307759413,
      "learning_rate": 1.461527603898871e-07,
      "loss": 1.6584,
      "step": 468608
    },
    {
      "epoch": 0.0017007875680035579,
      "grad_norm": 9972.896670476437,
      "learning_rate": 1.4614776558203336e-07,
      "loss": 1.6862,
      "step": 468640
    },
    {
      "epoch": 0.0017009037023543946,
      "grad_norm": 8719.716509153264,
      "learning_rate": 1.461427712862411e-07,
      "loss": 1.6619,
      "step": 468672
    },
    {
      "epoch": 0.0017010198367052314,
      "grad_norm": 11271.322548840486,
      "learning_rate": 1.4613777750242288e-07,
      "loss": 1.6436,
      "step": 468704
    },
    {
      "epoch": 0.001701135971056068,
      "grad_norm": 8948.40153323486,
      "learning_rate": 1.461327842304912e-07,
      "loss": 1.6578,
      "step": 468736
    },
    {
      "epoch": 0.0017012521054069047,
      "grad_norm": 8464.866921576498,
      "learning_rate": 1.4612779147035862e-07,
      "loss": 1.6512,
      "step": 468768
    },
    {
      "epoch": 0.0017013682397577414,
      "grad_norm": 9893.762479461491,
      "learning_rate": 1.4612279922193772e-07,
      "loss": 1.661,
      "step": 468800
    },
    {
      "epoch": 0.0017014843741085782,
      "grad_norm": 10087.3459343873,
      "learning_rate": 1.4611780748514109e-07,
      "loss": 1.6508,
      "step": 468832
    },
    {
      "epoch": 0.001701600508459415,
      "grad_norm": 11910.394955667927,
      "learning_rate": 1.4611281625988133e-07,
      "loss": 1.6509,
      "step": 468864
    },
    {
      "epoch": 0.0017017166428102515,
      "grad_norm": 8889.170489983866,
      "learning_rate": 1.4610782554607108e-07,
      "loss": 1.6563,
      "step": 468896
    },
    {
      "epoch": 0.0017018327771610882,
      "grad_norm": 19534.410254727427,
      "learning_rate": 1.4610283534362303e-07,
      "loss": 1.6703,
      "step": 468928
    },
    {
      "epoch": 0.001701948911511925,
      "grad_norm": 11559.698438973222,
      "learning_rate": 1.4609800157256086e-07,
      "loss": 1.6681,
      "step": 468960
    },
    {
      "epoch": 0.0017020650458627617,
      "grad_norm": 9807.919657093445,
      "learning_rate": 1.4609301237660194e-07,
      "loss": 1.6638,
      "step": 468992
    },
    {
      "epoch": 0.0017021811802135983,
      "grad_norm": 12027.58396353981,
      "learning_rate": 1.4608802369174606e-07,
      "loss": 1.6809,
      "step": 469024
    },
    {
      "epoch": 0.001702297314564435,
      "grad_norm": 10056.19361388791,
      "learning_rate": 1.4608303551790593e-07,
      "loss": 1.6602,
      "step": 469056
    },
    {
      "epoch": 0.0017024134489152718,
      "grad_norm": 13241.038327865379,
      "learning_rate": 1.4607804785499432e-07,
      "loss": 1.6726,
      "step": 469088
    },
    {
      "epoch": 0.0017025295832661085,
      "grad_norm": 10887.583386592269,
      "learning_rate": 1.46073060702924e-07,
      "loss": 1.6705,
      "step": 469120
    },
    {
      "epoch": 0.0017026457176169453,
      "grad_norm": 12058.214461519583,
      "learning_rate": 1.460680740616078e-07,
      "loss": 1.6674,
      "step": 469152
    },
    {
      "epoch": 0.0017027618519677818,
      "grad_norm": 13624.712694218546,
      "learning_rate": 1.460630879309585e-07,
      "loss": 1.6863,
      "step": 469184
    },
    {
      "epoch": 0.0017028779863186186,
      "grad_norm": 9832.074653906977,
      "learning_rate": 1.4605810231088901e-07,
      "loss": 1.685,
      "step": 469216
    },
    {
      "epoch": 0.0017029941206694553,
      "grad_norm": 10950.035616380434,
      "learning_rate": 1.4605311720131217e-07,
      "loss": 1.6903,
      "step": 469248
    },
    {
      "epoch": 0.001703110255020292,
      "grad_norm": 9487.683911260956,
      "learning_rate": 1.4604813260214086e-07,
      "loss": 1.6465,
      "step": 469280
    },
    {
      "epoch": 0.0017032263893711286,
      "grad_norm": 11201.30385267715,
      "learning_rate": 1.4604314851328798e-07,
      "loss": 1.6536,
      "step": 469312
    },
    {
      "epoch": 0.0017033425237219654,
      "grad_norm": 9962.511731486193,
      "learning_rate": 1.4603816493466646e-07,
      "loss": 1.6538,
      "step": 469344
    },
    {
      "epoch": 0.0017034586580728021,
      "grad_norm": 9214.79071927301,
      "learning_rate": 1.460331818661893e-07,
      "loss": 1.6702,
      "step": 469376
    },
    {
      "epoch": 0.0017035747924236389,
      "grad_norm": 13004.733445941905,
      "learning_rate": 1.460281993077694e-07,
      "loss": 1.6648,
      "step": 469408
    },
    {
      "epoch": 0.0017036909267744756,
      "grad_norm": 9538.270283442382,
      "learning_rate": 1.460232172593198e-07,
      "loss": 1.6544,
      "step": 469440
    },
    {
      "epoch": 0.0017038070611253122,
      "grad_norm": 10340.340710053997,
      "learning_rate": 1.460182357207535e-07,
      "loss": 1.6853,
      "step": 469472
    },
    {
      "epoch": 0.001703923195476149,
      "grad_norm": 10375.946800172022,
      "learning_rate": 1.460132546919835e-07,
      "loss": 1.6887,
      "step": 469504
    },
    {
      "epoch": 0.0017040393298269857,
      "grad_norm": 8028.413043684287,
      "learning_rate": 1.4600827417292292e-07,
      "loss": 1.6875,
      "step": 469536
    },
    {
      "epoch": 0.0017041554641778224,
      "grad_norm": 9234.898375185296,
      "learning_rate": 1.460032941634848e-07,
      "loss": 1.6659,
      "step": 469568
    },
    {
      "epoch": 0.001704271598528659,
      "grad_norm": 11636.632502575649,
      "learning_rate": 1.4599831466358222e-07,
      "loss": 1.6583,
      "step": 469600
    },
    {
      "epoch": 0.0017043877328794957,
      "grad_norm": 8328.499864921654,
      "learning_rate": 1.4599333567312832e-07,
      "loss": 1.6542,
      "step": 469632
    },
    {
      "epoch": 0.0017045038672303325,
      "grad_norm": 9853.570317402722,
      "learning_rate": 1.4598835719203623e-07,
      "loss": 1.6577,
      "step": 469664
    },
    {
      "epoch": 0.0017046200015811692,
      "grad_norm": 12499.539671523908,
      "learning_rate": 1.459833792202191e-07,
      "loss": 1.6629,
      "step": 469696
    },
    {
      "epoch": 0.001704736135932006,
      "grad_norm": 14707.612858652488,
      "learning_rate": 1.4597840175759012e-07,
      "loss": 1.6524,
      "step": 469728
    },
    {
      "epoch": 0.0017048522702828425,
      "grad_norm": 10706.990426819293,
      "learning_rate": 1.4597342480406247e-07,
      "loss": 1.6614,
      "step": 469760
    },
    {
      "epoch": 0.0017049684046336793,
      "grad_norm": 9049.261737843592,
      "learning_rate": 1.459684483595494e-07,
      "loss": 1.6459,
      "step": 469792
    },
    {
      "epoch": 0.001705084538984516,
      "grad_norm": 9534.367309895293,
      "learning_rate": 1.4596347242396412e-07,
      "loss": 1.662,
      "step": 469824
    },
    {
      "epoch": 0.0017052006733353528,
      "grad_norm": 11001.181754702537,
      "learning_rate": 1.4595849699721992e-07,
      "loss": 1.6591,
      "step": 469856
    },
    {
      "epoch": 0.0017053168076861893,
      "grad_norm": 10078.657648714932,
      "learning_rate": 1.4595352207923008e-07,
      "loss": 1.6507,
      "step": 469888
    },
    {
      "epoch": 0.001705432942037026,
      "grad_norm": 9762.773786173682,
      "learning_rate": 1.4594854766990785e-07,
      "loss": 1.6593,
      "step": 469920
    },
    {
      "epoch": 0.0017055490763878628,
      "grad_norm": 19711.83441488894,
      "learning_rate": 1.4594357376916665e-07,
      "loss": 1.6669,
      "step": 469952
    },
    {
      "epoch": 0.0017056652107386996,
      "grad_norm": 18375.79494879065,
      "learning_rate": 1.4593860037691973e-07,
      "loss": 1.6718,
      "step": 469984
    },
    {
      "epoch": 0.0017057813450895363,
      "grad_norm": 15667.690321167316,
      "learning_rate": 1.459336274930805e-07,
      "loss": 1.676,
      "step": 470016
    },
    {
      "epoch": 0.0017058974794403729,
      "grad_norm": 8365.623467500793,
      "learning_rate": 1.4592881049660383e-07,
      "loss": 1.6829,
      "step": 470048
    },
    {
      "epoch": 0.0017060136137912096,
      "grad_norm": 13603.997353719236,
      "learning_rate": 1.4592383861343913e-07,
      "loss": 1.7012,
      "step": 470080
    },
    {
      "epoch": 0.0017061297481420464,
      "grad_norm": 9720.26851480966,
      "learning_rate": 1.4591886723842505e-07,
      "loss": 1.7013,
      "step": 470112
    },
    {
      "epoch": 0.0017062458824928831,
      "grad_norm": 9252.241674318717,
      "learning_rate": 1.4591389637147502e-07,
      "loss": 1.6739,
      "step": 470144
    },
    {
      "epoch": 0.0017063620168437197,
      "grad_norm": 10920.362081909188,
      "learning_rate": 1.4590892601250248e-07,
      "loss": 1.6453,
      "step": 470176
    },
    {
      "epoch": 0.0017064781511945564,
      "grad_norm": 8809.115960185789,
      "learning_rate": 1.4590395616142098e-07,
      "loss": 1.6664,
      "step": 470208
    },
    {
      "epoch": 0.0017065942855453932,
      "grad_norm": 9304.898064997811,
      "learning_rate": 1.45898986818144e-07,
      "loss": 1.6549,
      "step": 470240
    },
    {
      "epoch": 0.00170671041989623,
      "grad_norm": 16317.143254871546,
      "learning_rate": 1.4589401798258506e-07,
      "loss": 1.6805,
      "step": 470272
    },
    {
      "epoch": 0.0017068265542470667,
      "grad_norm": 18709.885836102792,
      "learning_rate": 1.4588904965465772e-07,
      "loss": 1.6676,
      "step": 470304
    },
    {
      "epoch": 0.0017069426885979032,
      "grad_norm": 10309.222279105248,
      "learning_rate": 1.4588408183427556e-07,
      "loss": 1.6502,
      "step": 470336
    },
    {
      "epoch": 0.00170705882294874,
      "grad_norm": 8315.414601810304,
      "learning_rate": 1.4587911452135214e-07,
      "loss": 1.6673,
      "step": 470368
    },
    {
      "epoch": 0.0017071749572995767,
      "grad_norm": 8741.776249710352,
      "learning_rate": 1.458741477158011e-07,
      "loss": 1.6728,
      "step": 470400
    },
    {
      "epoch": 0.0017072910916504135,
      "grad_norm": 8316.780146186384,
      "learning_rate": 1.4586918141753604e-07,
      "loss": 1.6574,
      "step": 470432
    },
    {
      "epoch": 0.00170740722600125,
      "grad_norm": 8329.437075817308,
      "learning_rate": 1.4586421562647068e-07,
      "loss": 1.6432,
      "step": 470464
    },
    {
      "epoch": 0.0017075233603520868,
      "grad_norm": 8921.564885153277,
      "learning_rate": 1.4585925034251862e-07,
      "loss": 1.653,
      "step": 470496
    },
    {
      "epoch": 0.0017076394947029235,
      "grad_norm": 8256.263925045032,
      "learning_rate": 1.4585428556559358e-07,
      "loss": 1.6458,
      "step": 470528
    },
    {
      "epoch": 0.0017077556290537603,
      "grad_norm": 7487.770562724261,
      "learning_rate": 1.4584932129560928e-07,
      "loss": 1.6757,
      "step": 470560
    },
    {
      "epoch": 0.001707871763404597,
      "grad_norm": 12372.879697144072,
      "learning_rate": 1.458443575324795e-07,
      "loss": 1.6705,
      "step": 470592
    },
    {
      "epoch": 0.0017079878977554336,
      "grad_norm": 10101.179931077359,
      "learning_rate": 1.4583939427611788e-07,
      "loss": 1.6668,
      "step": 470624
    },
    {
      "epoch": 0.0017081040321062703,
      "grad_norm": 10261.305667408997,
      "learning_rate": 1.458344315264383e-07,
      "loss": 1.682,
      "step": 470656
    },
    {
      "epoch": 0.001708220166457107,
      "grad_norm": 11669.758009487601,
      "learning_rate": 1.4582946928335453e-07,
      "loss": 1.6546,
      "step": 470688
    },
    {
      "epoch": 0.0017083363008079438,
      "grad_norm": 11356.750767715208,
      "learning_rate": 1.4582450754678037e-07,
      "loss": 1.6599,
      "step": 470720
    },
    {
      "epoch": 0.0017084524351587804,
      "grad_norm": 14621.166027372783,
      "learning_rate": 1.4581954631662966e-07,
      "loss": 1.6609,
      "step": 470752
    },
    {
      "epoch": 0.0017085685695096171,
      "grad_norm": 9705.348010246722,
      "learning_rate": 1.4581458559281627e-07,
      "loss": 1.6584,
      "step": 470784
    },
    {
      "epoch": 0.0017086847038604539,
      "grad_norm": 15170.35121544653,
      "learning_rate": 1.4580962537525407e-07,
      "loss": 1.674,
      "step": 470816
    },
    {
      "epoch": 0.0017088008382112906,
      "grad_norm": 13138.809382893109,
      "learning_rate": 1.45804665663857e-07,
      "loss": 1.6805,
      "step": 470848
    },
    {
      "epoch": 0.0017089169725621274,
      "grad_norm": 9826.683367240445,
      "learning_rate": 1.4579970645853892e-07,
      "loss": 1.6938,
      "step": 470880
    },
    {
      "epoch": 0.001709033106912964,
      "grad_norm": 10661.42692138346,
      "learning_rate": 1.4579474775921376e-07,
      "loss": 1.6809,
      "step": 470912
    },
    {
      "epoch": 0.0017091492412638007,
      "grad_norm": 8693.868644050242,
      "learning_rate": 1.4578978956579555e-07,
      "loss": 1.6914,
      "step": 470944
    },
    {
      "epoch": 0.0017092653756146374,
      "grad_norm": 14790.461520858638,
      "learning_rate": 1.4578483187819823e-07,
      "loss": 1.686,
      "step": 470976
    },
    {
      "epoch": 0.0017093815099654742,
      "grad_norm": 8996.296793681275,
      "learning_rate": 1.4577987469633583e-07,
      "loss": 1.6902,
      "step": 471008
    },
    {
      "epoch": 0.0017094976443163107,
      "grad_norm": 18737.827835691096,
      "learning_rate": 1.4577491802012231e-07,
      "loss": 1.6719,
      "step": 471040
    },
    {
      "epoch": 0.0017096137786671475,
      "grad_norm": 10919.990201460807,
      "learning_rate": 1.4577011672215288e-07,
      "loss": 1.662,
      "step": 471072
    },
    {
      "epoch": 0.0017097299130179842,
      "grad_norm": 11529.97467473368,
      "learning_rate": 1.4576516104118453e-07,
      "loss": 1.6765,
      "step": 471104
    },
    {
      "epoch": 0.001709846047368821,
      "grad_norm": 9995.907162434034,
      "learning_rate": 1.4576020586560995e-07,
      "loss": 1.6798,
      "step": 471136
    },
    {
      "epoch": 0.0017099621817196578,
      "grad_norm": 9637.624188564316,
      "learning_rate": 1.4575525119534328e-07,
      "loss": 1.6948,
      "step": 471168
    },
    {
      "epoch": 0.0017100783160704943,
      "grad_norm": 13944.803046296494,
      "learning_rate": 1.4575029703029864e-07,
      "loss": 1.6829,
      "step": 471200
    },
    {
      "epoch": 0.001710194450421331,
      "grad_norm": 11294.897963239862,
      "learning_rate": 1.4574534337039013e-07,
      "loss": 1.6737,
      "step": 471232
    },
    {
      "epoch": 0.0017103105847721678,
      "grad_norm": 11046.215279452052,
      "learning_rate": 1.4574039021553197e-07,
      "loss": 1.6778,
      "step": 471264
    },
    {
      "epoch": 0.0017104267191230046,
      "grad_norm": 8214.597251235145,
      "learning_rate": 1.4573543756563833e-07,
      "loss": 1.6766,
      "step": 471296
    },
    {
      "epoch": 0.001710542853473841,
      "grad_norm": 9486.11490548159,
      "learning_rate": 1.4573048542062344e-07,
      "loss": 1.6722,
      "step": 471328
    },
    {
      "epoch": 0.0017106589878246778,
      "grad_norm": 10002.79670892096,
      "learning_rate": 1.4572553378040146e-07,
      "loss": 1.67,
      "step": 471360
    },
    {
      "epoch": 0.0017107751221755146,
      "grad_norm": 10517.525944821815,
      "learning_rate": 1.457205826448867e-07,
      "loss": 1.6647,
      "step": 471392
    },
    {
      "epoch": 0.0017108912565263514,
      "grad_norm": 9870.927818599424,
      "learning_rate": 1.4571563201399338e-07,
      "loss": 1.6651,
      "step": 471424
    },
    {
      "epoch": 0.001711007390877188,
      "grad_norm": 10542.321660810772,
      "learning_rate": 1.4571068188763582e-07,
      "loss": 1.6553,
      "step": 471456
    },
    {
      "epoch": 0.0017111235252280246,
      "grad_norm": 16381.19287475732,
      "learning_rate": 1.4570573226572832e-07,
      "loss": 1.6459,
      "step": 471488
    },
    {
      "epoch": 0.0017112396595788614,
      "grad_norm": 9979.466619013263,
      "learning_rate": 1.4570078314818518e-07,
      "loss": 1.6527,
      "step": 471520
    },
    {
      "epoch": 0.0017113557939296982,
      "grad_norm": 8566.48866222328,
      "learning_rate": 1.4569583453492078e-07,
      "loss": 1.6509,
      "step": 471552
    },
    {
      "epoch": 0.001711471928280535,
      "grad_norm": 9550.206280494678,
      "learning_rate": 1.456908864258495e-07,
      "loss": 1.6524,
      "step": 471584
    },
    {
      "epoch": 0.0017115880626313714,
      "grad_norm": 17937.135222771776,
      "learning_rate": 1.456859388208857e-07,
      "loss": 1.6637,
      "step": 471616
    },
    {
      "epoch": 0.0017117041969822082,
      "grad_norm": 12471.399600686364,
      "learning_rate": 1.456809917199438e-07,
      "loss": 1.6589,
      "step": 471648
    },
    {
      "epoch": 0.001711820331333045,
      "grad_norm": 9748.708837584596,
      "learning_rate": 1.4567604512293822e-07,
      "loss": 1.6679,
      "step": 471680
    },
    {
      "epoch": 0.0017119364656838817,
      "grad_norm": 8623.33253446717,
      "learning_rate": 1.4567109902978342e-07,
      "loss": 1.6801,
      "step": 471712
    },
    {
      "epoch": 0.0017120526000347185,
      "grad_norm": 9788.867554523353,
      "learning_rate": 1.4566615344039387e-07,
      "loss": 1.6987,
      "step": 471744
    },
    {
      "epoch": 0.001712168734385555,
      "grad_norm": 9020.393782978657,
      "learning_rate": 1.4566120835468407e-07,
      "loss": 1.6759,
      "step": 471776
    },
    {
      "epoch": 0.0017122848687363917,
      "grad_norm": 15194.037514762163,
      "learning_rate": 1.4565626377256853e-07,
      "loss": 1.6633,
      "step": 471808
    },
    {
      "epoch": 0.0017124010030872285,
      "grad_norm": 8584.469698239956,
      "learning_rate": 1.4565131969396174e-07,
      "loss": 1.6784,
      "step": 471840
    },
    {
      "epoch": 0.0017125171374380653,
      "grad_norm": 10776.200814758418,
      "learning_rate": 1.4564637611877829e-07,
      "loss": 1.6773,
      "step": 471872
    },
    {
      "epoch": 0.0017126332717889018,
      "grad_norm": 13370.727130563992,
      "learning_rate": 1.4564143304693275e-07,
      "loss": 1.6768,
      "step": 471904
    },
    {
      "epoch": 0.0017127494061397385,
      "grad_norm": 11015.209848205344,
      "learning_rate": 1.4563649047833973e-07,
      "loss": 1.647,
      "step": 471936
    },
    {
      "epoch": 0.0017128655404905753,
      "grad_norm": 14992.882978266722,
      "learning_rate": 1.4563154841291378e-07,
      "loss": 1.656,
      "step": 471968
    },
    {
      "epoch": 0.001712981674841412,
      "grad_norm": 10224.52013543912,
      "learning_rate": 1.456266068505696e-07,
      "loss": 1.6525,
      "step": 472000
    },
    {
      "epoch": 0.0017130978091922488,
      "grad_norm": 10642.91219544726,
      "learning_rate": 1.4562166579122182e-07,
      "loss": 1.6528,
      "step": 472032
    },
    {
      "epoch": 0.0017132139435430853,
      "grad_norm": 12932.918309492256,
      "learning_rate": 1.456167252347851e-07,
      "loss": 1.6609,
      "step": 472064
    },
    {
      "epoch": 0.001713330077893922,
      "grad_norm": 19681.2788202393,
      "learning_rate": 1.4561178518117416e-07,
      "loss": 1.6464,
      "step": 472096
    },
    {
      "epoch": 0.0017134462122447589,
      "grad_norm": 17876.39896623478,
      "learning_rate": 1.456068456303037e-07,
      "loss": 1.6638,
      "step": 472128
    },
    {
      "epoch": 0.0017135623465955956,
      "grad_norm": 15994.657858172523,
      "learning_rate": 1.456019065820884e-07,
      "loss": 1.6565,
      "step": 472160
    },
    {
      "epoch": 0.0017136784809464321,
      "grad_norm": 19263.746260787386,
      "learning_rate": 1.4559696803644314e-07,
      "loss": 1.6649,
      "step": 472192
    },
    {
      "epoch": 0.001713794615297269,
      "grad_norm": 23896.9571284714,
      "learning_rate": 1.4559202999328255e-07,
      "loss": 1.6684,
      "step": 472224
    },
    {
      "epoch": 0.0017139107496481057,
      "grad_norm": 16828.813624257655,
      "learning_rate": 1.4558709245252154e-07,
      "loss": 1.6624,
      "step": 472256
    },
    {
      "epoch": 0.0017140268839989424,
      "grad_norm": 20491.981065773023,
      "learning_rate": 1.4558215541407485e-07,
      "loss": 1.6726,
      "step": 472288
    },
    {
      "epoch": 0.0017141430183497792,
      "grad_norm": 18040.39001795693,
      "learning_rate": 1.4557721887785735e-07,
      "loss": 1.6523,
      "step": 472320
    },
    {
      "epoch": 0.0017142591527006157,
      "grad_norm": 20947.483667495722,
      "learning_rate": 1.455722828437839e-07,
      "loss": 1.6565,
      "step": 472352
    },
    {
      "epoch": 0.0017143752870514525,
      "grad_norm": 8078.068952416784,
      "learning_rate": 1.4556750153954616e-07,
      "loss": 1.6539,
      "step": 472384
    },
    {
      "epoch": 0.0017144914214022892,
      "grad_norm": 8527.094581391719,
      "learning_rate": 1.4556256649382005e-07,
      "loss": 1.6524,
      "step": 472416
    },
    {
      "epoch": 0.001714607555753126,
      "grad_norm": 9486.775005237554,
      "learning_rate": 1.4555763194998533e-07,
      "loss": 1.6568,
      "step": 472448
    },
    {
      "epoch": 0.0017147236901039625,
      "grad_norm": 9732.703221613203,
      "learning_rate": 1.455526979079569e-07,
      "loss": 1.6534,
      "step": 472480
    },
    {
      "epoch": 0.0017148398244547993,
      "grad_norm": 10228.49138436358,
      "learning_rate": 1.455477643676498e-07,
      "loss": 1.6618,
      "step": 472512
    },
    {
      "epoch": 0.001714955958805636,
      "grad_norm": 8293.203602951033,
      "learning_rate": 1.455428313289789e-07,
      "loss": 1.6471,
      "step": 472544
    },
    {
      "epoch": 0.0017150720931564728,
      "grad_norm": 9923.35104689943,
      "learning_rate": 1.455378987918593e-07,
      "loss": 1.6596,
      "step": 472576
    },
    {
      "epoch": 0.0017151882275073095,
      "grad_norm": 8147.6522998959645,
      "learning_rate": 1.4553296675620597e-07,
      "loss": 1.6571,
      "step": 472608
    },
    {
      "epoch": 0.001715304361858146,
      "grad_norm": 11008.962348922807,
      "learning_rate": 1.4552803522193388e-07,
      "loss": 1.6809,
      "step": 472640
    },
    {
      "epoch": 0.0017154204962089828,
      "grad_norm": 9751.520189180761,
      "learning_rate": 1.455231041889582e-07,
      "loss": 1.6749,
      "step": 472672
    },
    {
      "epoch": 0.0017155366305598196,
      "grad_norm": 10547.891163640246,
      "learning_rate": 1.4551817365719395e-07,
      "loss": 1.6718,
      "step": 472704
    },
    {
      "epoch": 0.0017156527649106563,
      "grad_norm": 9765.797765671783,
      "learning_rate": 1.4551324362655625e-07,
      "loss": 1.6793,
      "step": 472736
    },
    {
      "epoch": 0.0017157688992614929,
      "grad_norm": 8836.64913867242,
      "learning_rate": 1.4550831409696016e-07,
      "loss": 1.6752,
      "step": 472768
    },
    {
      "epoch": 0.0017158850336123296,
      "grad_norm": 10739.716569816916,
      "learning_rate": 1.4550338506832088e-07,
      "loss": 1.6798,
      "step": 472800
    },
    {
      "epoch": 0.0017160011679631664,
      "grad_norm": 9510.341108498686,
      "learning_rate": 1.4549845654055357e-07,
      "loss": 1.6621,
      "step": 472832
    },
    {
      "epoch": 0.0017161173023140031,
      "grad_norm": 12375.743371612067,
      "learning_rate": 1.4549352851357334e-07,
      "loss": 1.6493,
      "step": 472864
    },
    {
      "epoch": 0.0017162334366648399,
      "grad_norm": 8715.059035944621,
      "learning_rate": 1.4548860098729545e-07,
      "loss": 1.6502,
      "step": 472896
    },
    {
      "epoch": 0.0017163495710156764,
      "grad_norm": 9687.09698516537,
      "learning_rate": 1.4548367396163511e-07,
      "loss": 1.6645,
      "step": 472928
    },
    {
      "epoch": 0.0017164657053665132,
      "grad_norm": 8440.215874016492,
      "learning_rate": 1.4547874743650753e-07,
      "loss": 1.6657,
      "step": 472960
    },
    {
      "epoch": 0.00171658183971735,
      "grad_norm": 9193.877310471355,
      "learning_rate": 1.4547382141182798e-07,
      "loss": 1.6607,
      "step": 472992
    },
    {
      "epoch": 0.0017166979740681867,
      "grad_norm": 10373.454005296402,
      "learning_rate": 1.4546889588751176e-07,
      "loss": 1.669,
      "step": 473024
    },
    {
      "epoch": 0.0017168141084190232,
      "grad_norm": 8803.447392925114,
      "learning_rate": 1.4546397086347413e-07,
      "loss": 1.6447,
      "step": 473056
    },
    {
      "epoch": 0.00171693024276986,
      "grad_norm": 7638.140742353469,
      "learning_rate": 1.4545904633963045e-07,
      "loss": 1.6544,
      "step": 473088
    },
    {
      "epoch": 0.0017170463771206967,
      "grad_norm": 10386.87190640185,
      "learning_rate": 1.45454122315896e-07,
      "loss": 1.6484,
      "step": 473120
    },
    {
      "epoch": 0.0017171625114715335,
      "grad_norm": 9592.88861605304,
      "learning_rate": 1.454491987921862e-07,
      "loss": 1.6439,
      "step": 473152
    },
    {
      "epoch": 0.0017172786458223702,
      "grad_norm": 13069.40411801548,
      "learning_rate": 1.454442757684164e-07,
      "loss": 1.6547,
      "step": 473184
    },
    {
      "epoch": 0.0017173947801732068,
      "grad_norm": 9818.098492070652,
      "learning_rate": 1.45439353244502e-07,
      "loss": 1.6634,
      "step": 473216
    },
    {
      "epoch": 0.0017175109145240435,
      "grad_norm": 11207.268712759591,
      "learning_rate": 1.4543443122035843e-07,
      "loss": 1.671,
      "step": 473248
    },
    {
      "epoch": 0.0017176270488748803,
      "grad_norm": 9294.538934234446,
      "learning_rate": 1.454295096959011e-07,
      "loss": 1.6535,
      "step": 473280
    },
    {
      "epoch": 0.001717743183225717,
      "grad_norm": 11252.774768918109,
      "learning_rate": 1.454245886710455e-07,
      "loss": 1.673,
      "step": 473312
    },
    {
      "epoch": 0.0017178593175765536,
      "grad_norm": 9486.7958763747,
      "learning_rate": 1.4541966814570709e-07,
      "loss": 1.6683,
      "step": 473344
    },
    {
      "epoch": 0.0017179754519273903,
      "grad_norm": 19206.023638431772,
      "learning_rate": 1.4541474811980134e-07,
      "loss": 1.685,
      "step": 473376
    },
    {
      "epoch": 0.001718091586278227,
      "grad_norm": 15966.701349997125,
      "learning_rate": 1.4540982859324384e-07,
      "loss": 1.6626,
      "step": 473408
    },
    {
      "epoch": 0.0017182077206290638,
      "grad_norm": 20185.19298892136,
      "learning_rate": 1.4540490956595008e-07,
      "loss": 1.6604,
      "step": 473440
    },
    {
      "epoch": 0.0017183238549799006,
      "grad_norm": 14974.319617264753,
      "learning_rate": 1.4539999103783562e-07,
      "loss": 1.6646,
      "step": 473472
    },
    {
      "epoch": 0.0017184399893307371,
      "grad_norm": 20813.049944686147,
      "learning_rate": 1.4539507300881606e-07,
      "loss": 1.6817,
      "step": 473504
    },
    {
      "epoch": 0.0017185561236815739,
      "grad_norm": 8797.07269493665,
      "learning_rate": 1.4539030914406722e-07,
      "loss": 1.6995,
      "step": 473536
    },
    {
      "epoch": 0.0017186722580324106,
      "grad_norm": 9993.753649155056,
      "learning_rate": 1.453853920973941e-07,
      "loss": 1.666,
      "step": 473568
    },
    {
      "epoch": 0.0017187883923832474,
      "grad_norm": 10670.584894934298,
      "learning_rate": 1.4538047554956532e-07,
      "loss": 1.6717,
      "step": 473600
    },
    {
      "epoch": 0.001718904526734084,
      "grad_norm": 8623.643777429585,
      "learning_rate": 1.453755595004966e-07,
      "loss": 1.6648,
      "step": 473632
    },
    {
      "epoch": 0.0017190206610849207,
      "grad_norm": 8422.814019079371,
      "learning_rate": 1.4537064395010358e-07,
      "loss": 1.6622,
      "step": 473664
    },
    {
      "epoch": 0.0017191367954357574,
      "grad_norm": 8486.234147134994,
      "learning_rate": 1.4536572889830197e-07,
      "loss": 1.6635,
      "step": 473696
    },
    {
      "epoch": 0.0017192529297865942,
      "grad_norm": 12433.86633352635,
      "learning_rate": 1.453608143450075e-07,
      "loss": 1.6445,
      "step": 473728
    },
    {
      "epoch": 0.001719369064137431,
      "grad_norm": 9701.543897751533,
      "learning_rate": 1.4535590029013587e-07,
      "loss": 1.6587,
      "step": 473760
    },
    {
      "epoch": 0.0017194851984882675,
      "grad_norm": 8754.547732464538,
      "learning_rate": 1.453509867336029e-07,
      "loss": 1.6472,
      "step": 473792
    },
    {
      "epoch": 0.0017196013328391042,
      "grad_norm": 11841.181697786755,
      "learning_rate": 1.453460736753243e-07,
      "loss": 1.6709,
      "step": 473824
    },
    {
      "epoch": 0.001719717467189941,
      "grad_norm": 10896.775302813212,
      "learning_rate": 1.4534116111521592e-07,
      "loss": 1.6841,
      "step": 473856
    },
    {
      "epoch": 0.0017198336015407777,
      "grad_norm": 10092.918705706492,
      "learning_rate": 1.4533624905319357e-07,
      "loss": 1.6815,
      "step": 473888
    },
    {
      "epoch": 0.0017199497358916143,
      "grad_norm": 11985.567487607752,
      "learning_rate": 1.4533133748917304e-07,
      "loss": 1.6699,
      "step": 473920
    },
    {
      "epoch": 0.001720065870242451,
      "grad_norm": 9704.294101066806,
      "learning_rate": 1.4532642642307022e-07,
      "loss": 1.6479,
      "step": 473952
    },
    {
      "epoch": 0.0017201820045932878,
      "grad_norm": 8201.62691177793,
      "learning_rate": 1.4532151585480098e-07,
      "loss": 1.6594,
      "step": 473984
    },
    {
      "epoch": 0.0017202981389441245,
      "grad_norm": 9225.60935656827,
      "learning_rate": 1.4531660578428125e-07,
      "loss": 1.6397,
      "step": 474016
    },
    {
      "epoch": 0.0017204142732949613,
      "grad_norm": 13833.022952341256,
      "learning_rate": 1.4531169621142692e-07,
      "loss": 1.6514,
      "step": 474048
    },
    {
      "epoch": 0.0017205304076457978,
      "grad_norm": 9887.501403286878,
      "learning_rate": 1.4530678713615392e-07,
      "loss": 1.6582,
      "step": 474080
    },
    {
      "epoch": 0.0017206465419966346,
      "grad_norm": 8211.654400910938,
      "learning_rate": 1.4530187855837822e-07,
      "loss": 1.6559,
      "step": 474112
    },
    {
      "epoch": 0.0017207626763474713,
      "grad_norm": 9874.546369327554,
      "learning_rate": 1.4529697047801577e-07,
      "loss": 1.6669,
      "step": 474144
    },
    {
      "epoch": 0.001720878810698308,
      "grad_norm": 9476.360060698411,
      "learning_rate": 1.452920628949826e-07,
      "loss": 1.6479,
      "step": 474176
    },
    {
      "epoch": 0.0017209949450491446,
      "grad_norm": 8764.15997115525,
      "learning_rate": 1.4528715580919472e-07,
      "loss": 1.6573,
      "step": 474208
    },
    {
      "epoch": 0.0017211110793999814,
      "grad_norm": 9384.867393842067,
      "learning_rate": 1.4528224922056814e-07,
      "loss": 1.657,
      "step": 474240
    },
    {
      "epoch": 0.0017212272137508181,
      "grad_norm": 12166.28661506871,
      "learning_rate": 1.4527734312901896e-07,
      "loss": 1.6687,
      "step": 474272
    },
    {
      "epoch": 0.0017213433481016549,
      "grad_norm": 11557.002552565264,
      "learning_rate": 1.452724375344632e-07,
      "loss": 1.6665,
      "step": 474304
    },
    {
      "epoch": 0.0017214594824524916,
      "grad_norm": 9780.279443860487,
      "learning_rate": 1.45267532436817e-07,
      "loss": 1.6685,
      "step": 474336
    },
    {
      "epoch": 0.0017215756168033282,
      "grad_norm": 11316.662582228031,
      "learning_rate": 1.4526262783599644e-07,
      "loss": 1.6879,
      "step": 474368
    },
    {
      "epoch": 0.001721691751154165,
      "grad_norm": 10910.21576321935,
      "learning_rate": 1.4525772373191772e-07,
      "loss": 1.7053,
      "step": 474400
    },
    {
      "epoch": 0.0017218078855050017,
      "grad_norm": 8762.739525970175,
      "learning_rate": 1.4525282012449695e-07,
      "loss": 1.7201,
      "step": 474432
    },
    {
      "epoch": 0.0017219240198558384,
      "grad_norm": 9530.059810935081,
      "learning_rate": 1.4524791701365027e-07,
      "loss": 1.6856,
      "step": 474464
    },
    {
      "epoch": 0.001722040154206675,
      "grad_norm": 10201.914330163727,
      "learning_rate": 1.4524301439929392e-07,
      "loss": 1.6686,
      "step": 474496
    },
    {
      "epoch": 0.0017221562885575117,
      "grad_norm": 10760.245164493233,
      "learning_rate": 1.4523811228134411e-07,
      "loss": 1.6534,
      "step": 474528
    },
    {
      "epoch": 0.0017222724229083485,
      "grad_norm": 24475.819904550695,
      "learning_rate": 1.4523321065971707e-07,
      "loss": 1.6612,
      "step": 474560
    },
    {
      "epoch": 0.0017223885572591852,
      "grad_norm": 17492.905076058694,
      "learning_rate": 1.4522830953432905e-07,
      "loss": 1.6704,
      "step": 474592
    },
    {
      "epoch": 0.001722504691610022,
      "grad_norm": 24431.934184587186,
      "learning_rate": 1.4522340890509634e-07,
      "loss": 1.6461,
      "step": 474624
    },
    {
      "epoch": 0.0017226208259608585,
      "grad_norm": 28880.678662386035,
      "learning_rate": 1.4521850877193523e-07,
      "loss": 1.6525,
      "step": 474656
    },
    {
      "epoch": 0.0017227369603116953,
      "grad_norm": 20896.197548836488,
      "learning_rate": 1.45213609134762e-07,
      "loss": 1.6447,
      "step": 474688
    },
    {
      "epoch": 0.001722853094662532,
      "grad_norm": 22693.691810721324,
      "learning_rate": 1.45208709993493e-07,
      "loss": 1.6615,
      "step": 474720
    },
    {
      "epoch": 0.0017229692290133688,
      "grad_norm": 25042.973305899603,
      "learning_rate": 1.452038113480446e-07,
      "loss": 1.6672,
      "step": 474752
    },
    {
      "epoch": 0.0017230853633642053,
      "grad_norm": 20831.513435177963,
      "learning_rate": 1.4519891319833317e-07,
      "loss": 1.6593,
      "step": 474784
    },
    {
      "epoch": 0.001723201497715042,
      "grad_norm": 11679.255113233892,
      "learning_rate": 1.4519416858846266e-07,
      "loss": 1.6494,
      "step": 474816
    },
    {
      "epoch": 0.0017233176320658788,
      "grad_norm": 10882.149420036467,
      "learning_rate": 1.4518927141448908e-07,
      "loss": 1.6575,
      "step": 474848
    },
    {
      "epoch": 0.0017234337664167156,
      "grad_norm": 9153.498129130743,
      "learning_rate": 1.4518437473600428e-07,
      "loss": 1.6665,
      "step": 474880
    },
    {
      "epoch": 0.0017235499007675523,
      "grad_norm": 11294.02585440639,
      "learning_rate": 1.4517947855292473e-07,
      "loss": 1.6499,
      "step": 474912
    },
    {
      "epoch": 0.0017236660351183889,
      "grad_norm": 10274.895814556954,
      "learning_rate": 1.4517458286516694e-07,
      "loss": 1.6683,
      "step": 474944
    },
    {
      "epoch": 0.0017237821694692256,
      "grad_norm": 9926.067902246086,
      "learning_rate": 1.4516968767264736e-07,
      "loss": 1.6604,
      "step": 474976
    },
    {
      "epoch": 0.0017238983038200624,
      "grad_norm": 10948.328822244974,
      "learning_rate": 1.451647929752825e-07,
      "loss": 1.6813,
      "step": 475008
    },
    {
      "epoch": 0.0017240144381708991,
      "grad_norm": 11128.363761128587,
      "learning_rate": 1.451598987729889e-07,
      "loss": 1.659,
      "step": 475040
    },
    {
      "epoch": 0.0017241305725217357,
      "grad_norm": 9307.732484337956,
      "learning_rate": 1.4515500506568314e-07,
      "loss": 1.6453,
      "step": 475072
    },
    {
      "epoch": 0.0017242467068725724,
      "grad_norm": 11416.739464488099,
      "learning_rate": 1.4515011185328175e-07,
      "loss": 1.6568,
      "step": 475104
    },
    {
      "epoch": 0.0017243628412234092,
      "grad_norm": 9072.55487721072,
      "learning_rate": 1.4514521913570134e-07,
      "loss": 1.6657,
      "step": 475136
    },
    {
      "epoch": 0.001724478975574246,
      "grad_norm": 8582.362029185206,
      "learning_rate": 1.4514032691285848e-07,
      "loss": 1.685,
      "step": 475168
    },
    {
      "epoch": 0.0017245951099250827,
      "grad_norm": 15869.543534708238,
      "learning_rate": 1.4513543518466986e-07,
      "loss": 1.6666,
      "step": 475200
    },
    {
      "epoch": 0.0017247112442759192,
      "grad_norm": 8762.935238834074,
      "learning_rate": 1.4513054395105205e-07,
      "loss": 1.6746,
      "step": 475232
    },
    {
      "epoch": 0.001724827378626756,
      "grad_norm": 11040.174545721638,
      "learning_rate": 1.451256532119218e-07,
      "loss": 1.6686,
      "step": 475264
    },
    {
      "epoch": 0.0017249435129775927,
      "grad_norm": 10057.08466703945,
      "learning_rate": 1.4512076296719574e-07,
      "loss": 1.6852,
      "step": 475296
    },
    {
      "epoch": 0.0017250596473284295,
      "grad_norm": 8416.35514935058,
      "learning_rate": 1.451158732167906e-07,
      "loss": 1.6883,
      "step": 475328
    },
    {
      "epoch": 0.001725175781679266,
      "grad_norm": 9558.353310063403,
      "learning_rate": 1.4511098396062308e-07,
      "loss": 1.6677,
      "step": 475360
    },
    {
      "epoch": 0.0017252919160301028,
      "grad_norm": 14578.138564302371,
      "learning_rate": 1.4510609519860999e-07,
      "loss": 1.6707,
      "step": 475392
    },
    {
      "epoch": 0.0017254080503809395,
      "grad_norm": 10924.478843404842,
      "learning_rate": 1.4510120693066803e-07,
      "loss": 1.6518,
      "step": 475424
    },
    {
      "epoch": 0.0017255241847317763,
      "grad_norm": 12776.087272713818,
      "learning_rate": 1.45096319156714e-07,
      "loss": 1.6646,
      "step": 475456
    },
    {
      "epoch": 0.001725640319082613,
      "grad_norm": 9926.563755902644,
      "learning_rate": 1.4509143187666471e-07,
      "loss": 1.6791,
      "step": 475488
    },
    {
      "epoch": 0.0017257564534334496,
      "grad_norm": 10744.0696200276,
      "learning_rate": 1.4508654509043702e-07,
      "loss": 1.6697,
      "step": 475520
    },
    {
      "epoch": 0.0017258725877842863,
      "grad_norm": 10154.37521465501,
      "learning_rate": 1.4508165879794774e-07,
      "loss": 1.6594,
      "step": 475552
    },
    {
      "epoch": 0.001725988722135123,
      "grad_norm": 10962.372644642217,
      "learning_rate": 1.4507677299911373e-07,
      "loss": 1.6581,
      "step": 475584
    },
    {
      "epoch": 0.0017261048564859599,
      "grad_norm": 10597.62331846155,
      "learning_rate": 1.4507188769385186e-07,
      "loss": 1.6715,
      "step": 475616
    },
    {
      "epoch": 0.0017262209908367964,
      "grad_norm": 9071.941578295133,
      "learning_rate": 1.4506700288207906e-07,
      "loss": 1.6594,
      "step": 475648
    },
    {
      "epoch": 0.0017263371251876331,
      "grad_norm": 9576.208853194463,
      "learning_rate": 1.4506211856371227e-07,
      "loss": 1.6679,
      "step": 475680
    },
    {
      "epoch": 0.00172645325953847,
      "grad_norm": 8563.204540357541,
      "learning_rate": 1.450572347386684e-07,
      "loss": 1.652,
      "step": 475712
    },
    {
      "epoch": 0.0017265693938893067,
      "grad_norm": 11004.812038376667,
      "learning_rate": 1.450523514068644e-07,
      "loss": 1.6666,
      "step": 475744
    },
    {
      "epoch": 0.0017266855282401434,
      "grad_norm": 9634.945978053016,
      "learning_rate": 1.450474685682173e-07,
      "loss": 1.6595,
      "step": 475776
    },
    {
      "epoch": 0.00172680166259098,
      "grad_norm": 26411.170061169196,
      "learning_rate": 1.4504258622264406e-07,
      "loss": 1.6519,
      "step": 475808
    },
    {
      "epoch": 0.0017269177969418167,
      "grad_norm": 20824.48520372112,
      "learning_rate": 1.450377043700617e-07,
      "loss": 1.6693,
      "step": 475840
    },
    {
      "epoch": 0.0017270339312926535,
      "grad_norm": 23051.513095673352,
      "learning_rate": 1.4503282301038726e-07,
      "loss": 1.664,
      "step": 475872
    },
    {
      "epoch": 0.0017271500656434902,
      "grad_norm": 16808.163254799736,
      "learning_rate": 1.4502794214353785e-07,
      "loss": 1.6824,
      "step": 475904
    },
    {
      "epoch": 0.0017272661999943267,
      "grad_norm": 18882.052854496513,
      "learning_rate": 1.450230617694305e-07,
      "loss": 1.6656,
      "step": 475936
    },
    {
      "epoch": 0.0017273823343451635,
      "grad_norm": 22842.975638038053,
      "learning_rate": 1.4501818188798233e-07,
      "loss": 1.671,
      "step": 475968
    },
    {
      "epoch": 0.0017274984686960003,
      "grad_norm": 24648.675745362063,
      "learning_rate": 1.4501330249911042e-07,
      "loss": 1.6759,
      "step": 476000
    },
    {
      "epoch": 0.001727614603046837,
      "grad_norm": 18749.56895504534,
      "learning_rate": 1.4500842360273194e-07,
      "loss": 1.6936,
      "step": 476032
    },
    {
      "epoch": 0.0017277307373976738,
      "grad_norm": 25908.375479755578,
      "learning_rate": 1.4500354519876405e-07,
      "loss": 1.7014,
      "step": 476064
    },
    {
      "epoch": 0.0017278468717485103,
      "grad_norm": 19389.99102630014,
      "learning_rate": 1.4499866728712392e-07,
      "loss": 1.6772,
      "step": 476096
    },
    {
      "epoch": 0.001727963006099347,
      "grad_norm": 26626.584009219056,
      "learning_rate": 1.4499378986772875e-07,
      "loss": 1.6843,
      "step": 476128
    },
    {
      "epoch": 0.0017280791404501838,
      "grad_norm": 20351.9901729536,
      "learning_rate": 1.4498891294049575e-07,
      "loss": 1.6844,
      "step": 476160
    },
    {
      "epoch": 0.0017281952748010206,
      "grad_norm": 17063.750584206275,
      "learning_rate": 1.4498403650534215e-07,
      "loss": 1.7,
      "step": 476192
    },
    {
      "epoch": 0.001728311409151857,
      "grad_norm": 27042.812279790724,
      "learning_rate": 1.4497916056218523e-07,
      "loss": 1.6997,
      "step": 476224
    },
    {
      "epoch": 0.0017284275435026939,
      "grad_norm": 17587.034542525926,
      "learning_rate": 1.4497428511094222e-07,
      "loss": 1.683,
      "step": 476256
    },
    {
      "epoch": 0.0017285436778535306,
      "grad_norm": 20800.870366405343,
      "learning_rate": 1.4496941015153045e-07,
      "loss": 1.6736,
      "step": 476288
    },
    {
      "epoch": 0.0017286598122043674,
      "grad_norm": 23398.54935674432,
      "learning_rate": 1.4496453568386723e-07,
      "loss": 1.6601,
      "step": 476320
    },
    {
      "epoch": 0.0017287759465552041,
      "grad_norm": 17584.888285115714,
      "learning_rate": 1.4495966170786986e-07,
      "loss": 1.6813,
      "step": 476352
    },
    {
      "epoch": 0.0017288920809060406,
      "grad_norm": 8712.812978596523,
      "learning_rate": 1.4495494051240354e-07,
      "loss": 1.6472,
      "step": 476384
    },
    {
      "epoch": 0.0017290082152568774,
      "grad_norm": 11659.136160110662,
      "learning_rate": 1.4495006750413187e-07,
      "loss": 1.6391,
      "step": 476416
    },
    {
      "epoch": 0.0017291243496077142,
      "grad_norm": 8506.41428570229,
      "learning_rate": 1.4494519498728074e-07,
      "loss": 1.6484,
      "step": 476448
    },
    {
      "epoch": 0.001729240483958551,
      "grad_norm": 10370.468841860526,
      "learning_rate": 1.449403229617676e-07,
      "loss": 1.6642,
      "step": 476480
    },
    {
      "epoch": 0.0017293566183093874,
      "grad_norm": 13829.055282267114,
      "learning_rate": 1.449354514275099e-07,
      "loss": 1.6762,
      "step": 476512
    },
    {
      "epoch": 0.0017294727526602242,
      "grad_norm": 13155.59363920914,
      "learning_rate": 1.4493058038442506e-07,
      "loss": 1.6547,
      "step": 476544
    },
    {
      "epoch": 0.001729588887011061,
      "grad_norm": 8945.018278349127,
      "learning_rate": 1.4492570983243054e-07,
      "loss": 1.673,
      "step": 476576
    },
    {
      "epoch": 0.0017297050213618977,
      "grad_norm": 11644.051356808763,
      "learning_rate": 1.449208397714438e-07,
      "loss": 1.6586,
      "step": 476608
    },
    {
      "epoch": 0.0017298211557127345,
      "grad_norm": 10976.0030976672,
      "learning_rate": 1.449159702013824e-07,
      "loss": 1.6647,
      "step": 476640
    },
    {
      "epoch": 0.001729937290063571,
      "grad_norm": 8527.538918116998,
      "learning_rate": 1.4491110112216385e-07,
      "loss": 1.6516,
      "step": 476672
    },
    {
      "epoch": 0.0017300534244144078,
      "grad_norm": 12151.767937218025,
      "learning_rate": 1.4490623253370567e-07,
      "loss": 1.6437,
      "step": 476704
    },
    {
      "epoch": 0.0017301695587652445,
      "grad_norm": 8686.608083711386,
      "learning_rate": 1.4490136443592548e-07,
      "loss": 1.648,
      "step": 476736
    },
    {
      "epoch": 0.0017302856931160813,
      "grad_norm": 9171.724592463514,
      "learning_rate": 1.448964968287408e-07,
      "loss": 1.6631,
      "step": 476768
    },
    {
      "epoch": 0.0017304018274669178,
      "grad_norm": 9590.7328187162,
      "learning_rate": 1.4489162971206926e-07,
      "loss": 1.6659,
      "step": 476800
    },
    {
      "epoch": 0.0017305179618177546,
      "grad_norm": 8698.755083343824,
      "learning_rate": 1.4488676308582853e-07,
      "loss": 1.654,
      "step": 476832
    },
    {
      "epoch": 0.0017306340961685913,
      "grad_norm": 9716.064635437539,
      "learning_rate": 1.4488189694993618e-07,
      "loss": 1.6545,
      "step": 476864
    },
    {
      "epoch": 0.001730750230519428,
      "grad_norm": 9473.087353128334,
      "learning_rate": 1.4487703130430991e-07,
      "loss": 1.6497,
      "step": 476896
    },
    {
      "epoch": 0.0017308663648702648,
      "grad_norm": 10644.051860076595,
      "learning_rate": 1.4487216614886739e-07,
      "loss": 1.6663,
      "step": 476928
    },
    {
      "epoch": 0.0017309824992211014,
      "grad_norm": 7578.682999044094,
      "learning_rate": 1.448673014835263e-07,
      "loss": 1.6604,
      "step": 476960
    },
    {
      "epoch": 0.0017310986335719381,
      "grad_norm": 10340.672898801122,
      "learning_rate": 1.4486243730820441e-07,
      "loss": 1.6564,
      "step": 476992
    },
    {
      "epoch": 0.0017312147679227749,
      "grad_norm": 10021.090160257017,
      "learning_rate": 1.4485757362281943e-07,
      "loss": 1.68,
      "step": 477024
    },
    {
      "epoch": 0.0017313309022736116,
      "grad_norm": 10760.441069026863,
      "learning_rate": 1.448527104272891e-07,
      "loss": 1.676,
      "step": 477056
    },
    {
      "epoch": 0.0017314470366244482,
      "grad_norm": 7847.700172662052,
      "learning_rate": 1.4484784772153125e-07,
      "loss": 1.6942,
      "step": 477088
    },
    {
      "epoch": 0.001731563170975285,
      "grad_norm": 11205.980903071359,
      "learning_rate": 1.4484298550546365e-07,
      "loss": 1.6935,
      "step": 477120
    },
    {
      "epoch": 0.0017316793053261217,
      "grad_norm": 9767.973382437116,
      "learning_rate": 1.448381237790041e-07,
      "loss": 1.6712,
      "step": 477152
    },
    {
      "epoch": 0.0017317954396769584,
      "grad_norm": 9667.534949510138,
      "learning_rate": 1.4483326254207044e-07,
      "loss": 1.6521,
      "step": 477184
    },
    {
      "epoch": 0.0017319115740277952,
      "grad_norm": 13646.668018238004,
      "learning_rate": 1.4482840179458052e-07,
      "loss": 1.6581,
      "step": 477216
    },
    {
      "epoch": 0.0017320277083786317,
      "grad_norm": 10984.154223243591,
      "learning_rate": 1.4482354153645225e-07,
      "loss": 1.6596,
      "step": 477248
    },
    {
      "epoch": 0.0017321438427294685,
      "grad_norm": 13576.059811300185,
      "learning_rate": 1.4481868176760352e-07,
      "loss": 1.65,
      "step": 477280
    },
    {
      "epoch": 0.0017322599770803052,
      "grad_norm": 9366.296813575791,
      "learning_rate": 1.4481382248795218e-07,
      "loss": 1.6608,
      "step": 477312
    },
    {
      "epoch": 0.001732376111431142,
      "grad_norm": 10382.910189344795,
      "learning_rate": 1.4480896369741623e-07,
      "loss": 1.6446,
      "step": 477344
    },
    {
      "epoch": 0.0017324922457819785,
      "grad_norm": 19272.20796898996,
      "learning_rate": 1.448041053959136e-07,
      "loss": 1.6711,
      "step": 477376
    },
    {
      "epoch": 0.0017326083801328153,
      "grad_norm": 20483.586795285635,
      "learning_rate": 1.4479924758336228e-07,
      "loss": 1.6702,
      "step": 477408
    },
    {
      "epoch": 0.001732724514483652,
      "grad_norm": 20582.706527568233,
      "learning_rate": 1.4479439025968022e-07,
      "loss": 1.6367,
      "step": 477440
    },
    {
      "epoch": 0.0017328406488344888,
      "grad_norm": 18515.344987334156,
      "learning_rate": 1.4478953342478542e-07,
      "loss": 1.6555,
      "step": 477472
    },
    {
      "epoch": 0.0017329567831853255,
      "grad_norm": 15490.120980805797,
      "learning_rate": 1.4478467707859595e-07,
      "loss": 1.644,
      "step": 477504
    },
    {
      "epoch": 0.001733072917536162,
      "grad_norm": 16277.438619143984,
      "learning_rate": 1.4477982122102988e-07,
      "loss": 1.6569,
      "step": 477536
    },
    {
      "epoch": 0.0017331890518869988,
      "grad_norm": 21332.495775225176,
      "learning_rate": 1.4477496585200525e-07,
      "loss": 1.6484,
      "step": 477568
    },
    {
      "epoch": 0.0017333051862378356,
      "grad_norm": 22077.781410277617,
      "learning_rate": 1.4477011097144008e-07,
      "loss": 1.6539,
      "step": 477600
    },
    {
      "epoch": 0.0017334213205886723,
      "grad_norm": 9877.318057043622,
      "learning_rate": 1.4476540827161682e-07,
      "loss": 1.6628,
      "step": 477632
    },
    {
      "epoch": 0.0017335374549395089,
      "grad_norm": 11062.076387369598,
      "learning_rate": 1.4476055435246704e-07,
      "loss": 1.672,
      "step": 477664
    },
    {
      "epoch": 0.0017336535892903456,
      "grad_norm": 9402.608255159841,
      "learning_rate": 1.4475570092153373e-07,
      "loss": 1.6813,
      "step": 477696
    },
    {
      "epoch": 0.0017337697236411824,
      "grad_norm": 8738.39012633334,
      "learning_rate": 1.4475084797873503e-07,
      "loss": 1.643,
      "step": 477728
    },
    {
      "epoch": 0.0017338858579920191,
      "grad_norm": 13760.400139530826,
      "learning_rate": 1.447459955239891e-07,
      "loss": 1.6518,
      "step": 477760
    },
    {
      "epoch": 0.0017340019923428559,
      "grad_norm": 11505.723097658834,
      "learning_rate": 1.447411435572142e-07,
      "loss": 1.6491,
      "step": 477792
    },
    {
      "epoch": 0.0017341181266936924,
      "grad_norm": 8697.642899084785,
      "learning_rate": 1.447362920783285e-07,
      "loss": 1.6715,
      "step": 477824
    },
    {
      "epoch": 0.0017342342610445292,
      "grad_norm": 9089.660499710646,
      "learning_rate": 1.4473144108725026e-07,
      "loss": 1.6895,
      "step": 477856
    },
    {
      "epoch": 0.001734350395395366,
      "grad_norm": 9639.582148620342,
      "learning_rate": 1.4472659058389773e-07,
      "loss": 1.6646,
      "step": 477888
    },
    {
      "epoch": 0.0017344665297462027,
      "grad_norm": 12001.902015930642,
      "learning_rate": 1.447217405681892e-07,
      "loss": 1.6744,
      "step": 477920
    },
    {
      "epoch": 0.0017345826640970392,
      "grad_norm": 8100.702191785598,
      "learning_rate": 1.4471689104004297e-07,
      "loss": 1.6668,
      "step": 477952
    },
    {
      "epoch": 0.001734698798447876,
      "grad_norm": 9745.090764071929,
      "learning_rate": 1.4471204199937732e-07,
      "loss": 1.6859,
      "step": 477984
    },
    {
      "epoch": 0.0017348149327987127,
      "grad_norm": 9005.872750600021,
      "learning_rate": 1.4470719344611064e-07,
      "loss": 1.6654,
      "step": 478016
    },
    {
      "epoch": 0.0017349310671495495,
      "grad_norm": 9922.148759215415,
      "learning_rate": 1.447023453801612e-07,
      "loss": 1.6479,
      "step": 478048
    },
    {
      "epoch": 0.0017350472015003862,
      "grad_norm": 10850.9601418492,
      "learning_rate": 1.4469749780144745e-07,
      "loss": 1.6451,
      "step": 478080
    },
    {
      "epoch": 0.0017351633358512228,
      "grad_norm": 7808.864065918935,
      "learning_rate": 1.446926507098878e-07,
      "loss": 1.6634,
      "step": 478112
    },
    {
      "epoch": 0.0017352794702020595,
      "grad_norm": 10327.821745169695,
      "learning_rate": 1.4468780410540058e-07,
      "loss": 1.6704,
      "step": 478144
    },
    {
      "epoch": 0.0017353956045528963,
      "grad_norm": 10469.248301573518,
      "learning_rate": 1.446829579879043e-07,
      "loss": 1.6572,
      "step": 478176
    },
    {
      "epoch": 0.001735511738903733,
      "grad_norm": 8043.990427642241,
      "learning_rate": 1.4467811235731733e-07,
      "loss": 1.6702,
      "step": 478208
    },
    {
      "epoch": 0.0017356278732545696,
      "grad_norm": 10233.879811684325,
      "learning_rate": 1.446732672135582e-07,
      "loss": 1.6761,
      "step": 478240
    },
    {
      "epoch": 0.0017357440076054063,
      "grad_norm": 12265.803438829435,
      "learning_rate": 1.446684225565454e-07,
      "loss": 1.6764,
      "step": 478272
    },
    {
      "epoch": 0.001735860141956243,
      "grad_norm": 8811.829095029023,
      "learning_rate": 1.446635783861974e-07,
      "loss": 1.664,
      "step": 478304
    },
    {
      "epoch": 0.0017359762763070798,
      "grad_norm": 9534.014684276504,
      "learning_rate": 1.4465873470243276e-07,
      "loss": 1.6426,
      "step": 478336
    },
    {
      "epoch": 0.0017360924106579166,
      "grad_norm": 9067.848587178769,
      "learning_rate": 1.4465389150517e-07,
      "loss": 1.6654,
      "step": 478368
    },
    {
      "epoch": 0.0017362085450087531,
      "grad_norm": 10946.48363630988,
      "learning_rate": 1.446490487943277e-07,
      "loss": 1.6603,
      "step": 478400
    },
    {
      "epoch": 0.0017363246793595899,
      "grad_norm": 7729.35417742,
      "learning_rate": 1.446442065698244e-07,
      "loss": 1.6583,
      "step": 478432
    },
    {
      "epoch": 0.0017364408137104266,
      "grad_norm": 8407.939224328396,
      "learning_rate": 1.446393648315788e-07,
      "loss": 1.6538,
      "step": 478464
    },
    {
      "epoch": 0.0017365569480612634,
      "grad_norm": 8792.644425882352,
      "learning_rate": 1.446345235795094e-07,
      "loss": 1.6436,
      "step": 478496
    },
    {
      "epoch": 0.0017366730824121,
      "grad_norm": 9167.222261950454,
      "learning_rate": 1.4462968281353497e-07,
      "loss": 1.6517,
      "step": 478528
    },
    {
      "epoch": 0.0017367892167629367,
      "grad_norm": 11484.11668348942,
      "learning_rate": 1.4462484253357408e-07,
      "loss": 1.6575,
      "step": 478560
    },
    {
      "epoch": 0.0017369053511137734,
      "grad_norm": 14260.324540486446,
      "learning_rate": 1.4462000273954543e-07,
      "loss": 1.6646,
      "step": 478592
    },
    {
      "epoch": 0.0017370214854646102,
      "grad_norm": 21540.11624852568,
      "learning_rate": 1.446151634313677e-07,
      "loss": 1.6454,
      "step": 478624
    },
    {
      "epoch": 0.001737137619815447,
      "grad_norm": 17578.26316790143,
      "learning_rate": 1.4461032460895965e-07,
      "loss": 1.6586,
      "step": 478656
    },
    {
      "epoch": 0.0017372537541662835,
      "grad_norm": 24262.529134449276,
      "learning_rate": 1.4460548627224e-07,
      "loss": 1.6638,
      "step": 478688
    },
    {
      "epoch": 0.0017373698885171202,
      "grad_norm": 18307.882455379706,
      "learning_rate": 1.4460064842112748e-07,
      "loss": 1.6844,
      "step": 478720
    },
    {
      "epoch": 0.001737486022867957,
      "grad_norm": 19578.01154356591,
      "learning_rate": 1.445958110555409e-07,
      "loss": 1.6868,
      "step": 478752
    },
    {
      "epoch": 0.0017376021572187937,
      "grad_norm": 27676.60528316289,
      "learning_rate": 1.4459097417539907e-07,
      "loss": 1.6895,
      "step": 478784
    },
    {
      "epoch": 0.0017377182915696303,
      "grad_norm": 19628.798638734872,
      "learning_rate": 1.445861377806207e-07,
      "loss": 1.6752,
      "step": 478816
    },
    {
      "epoch": 0.001737834425920467,
      "grad_norm": 9720.810460038812,
      "learning_rate": 1.445814529859517e-07,
      "loss": 1.6718,
      "step": 478848
    },
    {
      "epoch": 0.0017379505602713038,
      "grad_norm": 9449.773753905434,
      "learning_rate": 1.4457661754649565e-07,
      "loss": 1.6854,
      "step": 478880
    },
    {
      "epoch": 0.0017380666946221405,
      "grad_norm": 9677.31202349082,
      "learning_rate": 1.4457178259216217e-07,
      "loss": 1.6521,
      "step": 478912
    },
    {
      "epoch": 0.0017381828289729773,
      "grad_norm": 13025.552579449364,
      "learning_rate": 1.4456694812287023e-07,
      "loss": 1.6566,
      "step": 478944
    },
    {
      "epoch": 0.0017382989633238138,
      "grad_norm": 9371.813271720686,
      "learning_rate": 1.4456211413853868e-07,
      "loss": 1.6451,
      "step": 478976
    },
    {
      "epoch": 0.0017384150976746506,
      "grad_norm": 10980.287245787335,
      "learning_rate": 1.4455728063908646e-07,
      "loss": 1.6594,
      "step": 479008
    },
    {
      "epoch": 0.0017385312320254873,
      "grad_norm": 13619.589127429652,
      "learning_rate": 1.4455244762443255e-07,
      "loss": 1.6539,
      "step": 479040
    },
    {
      "epoch": 0.001738647366376324,
      "grad_norm": 13357.457841969783,
      "learning_rate": 1.4454761509449585e-07,
      "loss": 1.6412,
      "step": 479072
    },
    {
      "epoch": 0.0017387635007271606,
      "grad_norm": 16329.766930363703,
      "learning_rate": 1.445427830491954e-07,
      "loss": 1.6659,
      "step": 479104
    },
    {
      "epoch": 0.0017388796350779974,
      "grad_norm": 12064.15732656036,
      "learning_rate": 1.4453795148845015e-07,
      "loss": 1.6679,
      "step": 479136
    },
    {
      "epoch": 0.0017389957694288341,
      "grad_norm": 9429.974761366013,
      "learning_rate": 1.445331204121792e-07,
      "loss": 1.6805,
      "step": 479168
    },
    {
      "epoch": 0.001739111903779671,
      "grad_norm": 12043.089304659332,
      "learning_rate": 1.445282898203015e-07,
      "loss": 1.6511,
      "step": 479200
    },
    {
      "epoch": 0.0017392280381305076,
      "grad_norm": 10890.599983471984,
      "learning_rate": 1.4452345971273617e-07,
      "loss": 1.6629,
      "step": 479232
    },
    {
      "epoch": 0.0017393441724813442,
      "grad_norm": 10016.777725396525,
      "learning_rate": 1.4451863008940224e-07,
      "loss": 1.6689,
      "step": 479264
    },
    {
      "epoch": 0.001739460306832181,
      "grad_norm": 10013.932494280156,
      "learning_rate": 1.4451380095021883e-07,
      "loss": 1.6826,
      "step": 479296
    },
    {
      "epoch": 0.0017395764411830177,
      "grad_norm": 10219.041050901009,
      "learning_rate": 1.4450897229510505e-07,
      "loss": 1.6872,
      "step": 479328
    },
    {
      "epoch": 0.0017396925755338544,
      "grad_norm": 8505.848576126898,
      "learning_rate": 1.4450414412398005e-07,
      "loss": 1.6566,
      "step": 479360
    },
    {
      "epoch": 0.001739808709884691,
      "grad_norm": 11193.388584338525,
      "learning_rate": 1.4449931643676297e-07,
      "loss": 1.6608,
      "step": 479392
    },
    {
      "epoch": 0.0017399248442355277,
      "grad_norm": 9414.129168436133,
      "learning_rate": 1.44494489233373e-07,
      "loss": 1.6443,
      "step": 479424
    },
    {
      "epoch": 0.0017400409785863645,
      "grad_norm": 7718.522527012537,
      "learning_rate": 1.444896625137293e-07,
      "loss": 1.6672,
      "step": 479456
    },
    {
      "epoch": 0.0017401571129372012,
      "grad_norm": 10531.856436545268,
      "learning_rate": 1.4448483627775108e-07,
      "loss": 1.6649,
      "step": 479488
    },
    {
      "epoch": 0.001740273247288038,
      "grad_norm": 11076.730203449031,
      "learning_rate": 1.4448001052535762e-07,
      "loss": 1.6495,
      "step": 479520
    },
    {
      "epoch": 0.0017403893816388745,
      "grad_norm": 11401.779510234357,
      "learning_rate": 1.4447518525646806e-07,
      "loss": 1.6546,
      "step": 479552
    },
    {
      "epoch": 0.0017405055159897113,
      "grad_norm": 9934.067444908958,
      "learning_rate": 1.444703604710018e-07,
      "loss": 1.6585,
      "step": 479584
    },
    {
      "epoch": 0.001740621650340548,
      "grad_norm": 8789.127829312758,
      "learning_rate": 1.4446553616887804e-07,
      "loss": 1.6687,
      "step": 479616
    },
    {
      "epoch": 0.0017407377846913848,
      "grad_norm": 8439.23503642362,
      "learning_rate": 1.4446071235001612e-07,
      "loss": 1.6682,
      "step": 479648
    },
    {
      "epoch": 0.0017408539190422213,
      "grad_norm": 9617.26717940185,
      "learning_rate": 1.4445588901433535e-07,
      "loss": 1.6771,
      "step": 479680
    },
    {
      "epoch": 0.001740970053393058,
      "grad_norm": 10613.384756994348,
      "learning_rate": 1.4445106616175505e-07,
      "loss": 1.6709,
      "step": 479712
    },
    {
      "epoch": 0.0017410861877438948,
      "grad_norm": 9249.306136137997,
      "learning_rate": 1.4444624379219463e-07,
      "loss": 1.6889,
      "step": 479744
    },
    {
      "epoch": 0.0017412023220947316,
      "grad_norm": 10525.516614399505,
      "learning_rate": 1.444414219055734e-07,
      "loss": 1.6736,
      "step": 479776
    },
    {
      "epoch": 0.0017413184564455684,
      "grad_norm": 9649.387338064525,
      "learning_rate": 1.4443660050181082e-07,
      "loss": 1.6567,
      "step": 479808
    },
    {
      "epoch": 0.0017414345907964049,
      "grad_norm": 24553.05748781605,
      "learning_rate": 1.444317795808263e-07,
      "loss": 1.6722,
      "step": 479840
    },
    {
      "epoch": 0.0017415507251472416,
      "grad_norm": 22513.797724950804,
      "learning_rate": 1.4442695914253922e-07,
      "loss": 1.6626,
      "step": 479872
    },
    {
      "epoch": 0.0017416668594980784,
      "grad_norm": 18495.04582313869,
      "learning_rate": 1.4442213918686912e-07,
      "loss": 1.6622,
      "step": 479904
    },
    {
      "epoch": 0.0017417829938489152,
      "grad_norm": 25155.739861908256,
      "learning_rate": 1.444173197137354e-07,
      "loss": 1.6524,
      "step": 479936
    },
    {
      "epoch": 0.0017418991281997517,
      "grad_norm": 21713.888458772188,
      "learning_rate": 1.444125007230576e-07,
      "loss": 1.6522,
      "step": 479968
    },
    {
      "epoch": 0.0017420152625505884,
      "grad_norm": 24401.01358550501,
      "learning_rate": 1.4440768221475521e-07,
      "loss": 1.6649,
      "step": 480000
    },
    {
      "epoch": 0.0017421313969014252,
      "grad_norm": 24872.19877694773,
      "learning_rate": 1.4440286418874778e-07,
      "loss": 1.6845,
      "step": 480032
    },
    {
      "epoch": 0.001742247531252262,
      "grad_norm": 16253.06764890862,
      "learning_rate": 1.4439804664495482e-07,
      "loss": 1.6667,
      "step": 480064
    },
    {
      "epoch": 0.0017423636656030987,
      "grad_norm": 25105.27976342825,
      "learning_rate": 1.4439322958329592e-07,
      "loss": 1.6529,
      "step": 480096
    },
    {
      "epoch": 0.0017424797999539352,
      "grad_norm": 17256.692151162693,
      "learning_rate": 1.4438841300369065e-07,
      "loss": 1.6581,
      "step": 480128
    },
    {
      "epoch": 0.001742595934304772,
      "grad_norm": 21673.545718225247,
      "learning_rate": 1.4438359690605867e-07,
      "loss": 1.6552,
      "step": 480160
    },
    {
      "epoch": 0.0017427120686556088,
      "grad_norm": 23682.697819294153,
      "learning_rate": 1.4437878129031956e-07,
      "loss": 1.6588,
      "step": 480192
    },
    {
      "epoch": 0.0017428282030064455,
      "grad_norm": 28427.55930430891,
      "learning_rate": 1.4437396615639296e-07,
      "loss": 1.6556,
      "step": 480224
    },
    {
      "epoch": 0.001742944337357282,
      "grad_norm": 25987.712019337137,
      "learning_rate": 1.4436915150419854e-07,
      "loss": 1.6458,
      "step": 480256
    },
    {
      "epoch": 0.0017430604717081188,
      "grad_norm": 17376.832162393697,
      "learning_rate": 1.4436433733365598e-07,
      "loss": 1.6625,
      "step": 480288
    },
    {
      "epoch": 0.0017431766060589556,
      "grad_norm": 24609.517183398784,
      "learning_rate": 1.4435952364468498e-07,
      "loss": 1.6595,
      "step": 480320
    },
    {
      "epoch": 0.0017432927404097923,
      "grad_norm": 14670.06557585889,
      "learning_rate": 1.4435471043720525e-07,
      "loss": 1.6776,
      "step": 480352
    },
    {
      "epoch": 0.001743408874760629,
      "grad_norm": 22521.93171111217,
      "learning_rate": 1.443498977111365e-07,
      "loss": 1.6709,
      "step": 480384
    },
    {
      "epoch": 0.0017435250091114656,
      "grad_norm": 18401.657534037524,
      "learning_rate": 1.4434508546639858e-07,
      "loss": 1.6677,
      "step": 480416
    },
    {
      "epoch": 0.0017436411434623024,
      "grad_norm": 25314.59310358355,
      "learning_rate": 1.4434027370291118e-07,
      "loss": 1.6569,
      "step": 480448
    },
    {
      "epoch": 0.001743757277813139,
      "grad_norm": 20110.372249165353,
      "learning_rate": 1.443354624205941e-07,
      "loss": 1.6717,
      "step": 480480
    },
    {
      "epoch": 0.0017438734121639759,
      "grad_norm": 14778.564747633649,
      "learning_rate": 1.4433065161936717e-07,
      "loss": 1.6794,
      "step": 480512
    },
    {
      "epoch": 0.0017439895465148124,
      "grad_norm": 19647.99144950954,
      "learning_rate": 1.4432584129915022e-07,
      "loss": 1.6697,
      "step": 480544
    },
    {
      "epoch": 0.0017441056808656492,
      "grad_norm": 12315.620731412606,
      "learning_rate": 1.4432118176006191e-07,
      "loss": 1.6869,
      "step": 480576
    },
    {
      "epoch": 0.001744221815216486,
      "grad_norm": 9801.72025717935,
      "learning_rate": 1.4431637238659913e-07,
      "loss": 1.6748,
      "step": 480608
    },
    {
      "epoch": 0.0017443379495673227,
      "grad_norm": 12817.475570485789,
      "learning_rate": 1.4431156349390844e-07,
      "loss": 1.6815,
      "step": 480640
    },
    {
      "epoch": 0.0017444540839181594,
      "grad_norm": 10053.507447652288,
      "learning_rate": 1.4430675508190973e-07,
      "loss": 1.6576,
      "step": 480672
    },
    {
      "epoch": 0.001744570218268996,
      "grad_norm": 10188.412732118777,
      "learning_rate": 1.4430194715052293e-07,
      "loss": 1.65,
      "step": 480704
    },
    {
      "epoch": 0.0017446863526198327,
      "grad_norm": 11331.009840256958,
      "learning_rate": 1.44297139699668e-07,
      "loss": 1.6663,
      "step": 480736
    },
    {
      "epoch": 0.0017448024869706695,
      "grad_norm": 8848.291360483108,
      "learning_rate": 1.4429233272926484e-07,
      "loss": 1.6619,
      "step": 480768
    },
    {
      "epoch": 0.0017449186213215062,
      "grad_norm": 9735.819225930605,
      "learning_rate": 1.442875262392335e-07,
      "loss": 1.6813,
      "step": 480800
    },
    {
      "epoch": 0.0017450347556723428,
      "grad_norm": 9370.080469238244,
      "learning_rate": 1.4428272022949393e-07,
      "loss": 1.6619,
      "step": 480832
    },
    {
      "epoch": 0.0017451508900231795,
      "grad_norm": 11816.579538935961,
      "learning_rate": 1.4427791469996619e-07,
      "loss": 1.6812,
      "step": 480864
    },
    {
      "epoch": 0.0017452670243740163,
      "grad_norm": 8209.101656088807,
      "learning_rate": 1.4427310965057026e-07,
      "loss": 1.6907,
      "step": 480896
    },
    {
      "epoch": 0.001745383158724853,
      "grad_norm": 7522.235704895187,
      "learning_rate": 1.4426830508122622e-07,
      "loss": 1.7052,
      "step": 480928
    },
    {
      "epoch": 0.0017454992930756898,
      "grad_norm": 9840.185160859524,
      "learning_rate": 1.4426350099185413e-07,
      "loss": 1.6773,
      "step": 480960
    },
    {
      "epoch": 0.0017456154274265263,
      "grad_norm": 12365.394777361536,
      "learning_rate": 1.4425869738237413e-07,
      "loss": 1.6552,
      "step": 480992
    },
    {
      "epoch": 0.001745731561777363,
      "grad_norm": 10752.063615883231,
      "learning_rate": 1.4425389425270627e-07,
      "loss": 1.67,
      "step": 481024
    },
    {
      "epoch": 0.0017458476961281998,
      "grad_norm": 12203.90199895099,
      "learning_rate": 1.4424909160277072e-07,
      "loss": 1.6587,
      "step": 481056
    },
    {
      "epoch": 0.0017459638304790366,
      "grad_norm": 9734.50974625841,
      "learning_rate": 1.4424428943248758e-07,
      "loss": 1.6835,
      "step": 481088
    },
    {
      "epoch": 0.001746079964829873,
      "grad_norm": 8146.925555079044,
      "learning_rate": 1.4423948774177705e-07,
      "loss": 1.6695,
      "step": 481120
    },
    {
      "epoch": 0.0017461960991807099,
      "grad_norm": 9026.08974030283,
      "learning_rate": 1.4423468653055933e-07,
      "loss": 1.6591,
      "step": 481152
    },
    {
      "epoch": 0.0017463122335315466,
      "grad_norm": 9530.263375164403,
      "learning_rate": 1.4422988579875458e-07,
      "loss": 1.6685,
      "step": 481184
    },
    {
      "epoch": 0.0017464283678823834,
      "grad_norm": 8605.118709233477,
      "learning_rate": 1.4422508554628305e-07,
      "loss": 1.6686,
      "step": 481216
    },
    {
      "epoch": 0.0017465445022332201,
      "grad_norm": 10878.815928215718,
      "learning_rate": 1.4422028577306495e-07,
      "loss": 1.6756,
      "step": 481248
    },
    {
      "epoch": 0.0017466606365840567,
      "grad_norm": 9389.543226376883,
      "learning_rate": 1.4421548647902057e-07,
      "loss": 1.6678,
      "step": 481280
    },
    {
      "epoch": 0.0017467767709348934,
      "grad_norm": 10460.36643717609,
      "learning_rate": 1.4421068766407017e-07,
      "loss": 1.6592,
      "step": 481312
    },
    {
      "epoch": 0.0017468929052857302,
      "grad_norm": 10942.198682166212,
      "learning_rate": 1.442058893281341e-07,
      "loss": 1.644,
      "step": 481344
    },
    {
      "epoch": 0.001747009039636567,
      "grad_norm": 12825.456405134282,
      "learning_rate": 1.4420109147113256e-07,
      "loss": 1.6788,
      "step": 481376
    },
    {
      "epoch": 0.0017471251739874035,
      "grad_norm": 9245.351264284121,
      "learning_rate": 1.4419629409298594e-07,
      "loss": 1.6821,
      "step": 481408
    },
    {
      "epoch": 0.0017472413083382402,
      "grad_norm": 10036.907292587692,
      "learning_rate": 1.4419149719361463e-07,
      "loss": 1.6793,
      "step": 481440
    },
    {
      "epoch": 0.001747357442689077,
      "grad_norm": 13605.074494467128,
      "learning_rate": 1.4418670077293897e-07,
      "loss": 1.6892,
      "step": 481472
    },
    {
      "epoch": 0.0017474735770399137,
      "grad_norm": 10860.564073748656,
      "learning_rate": 1.4418190483087932e-07,
      "loss": 1.6854,
      "step": 481504
    },
    {
      "epoch": 0.0017475897113907505,
      "grad_norm": 8239.693440899364,
      "learning_rate": 1.4417710936735613e-07,
      "loss": 1.6639,
      "step": 481536
    },
    {
      "epoch": 0.001747705845741587,
      "grad_norm": 19567.68254035209,
      "learning_rate": 1.441723143822898e-07,
      "loss": 1.6468,
      "step": 481568
    },
    {
      "epoch": 0.0017478219800924238,
      "grad_norm": 17049.4541848119,
      "learning_rate": 1.4416751987560077e-07,
      "loss": 1.6483,
      "step": 481600
    },
    {
      "epoch": 0.0017479381144432605,
      "grad_norm": 15249.362478477584,
      "learning_rate": 1.4416272584720953e-07,
      "loss": 1.6521,
      "step": 481632
    },
    {
      "epoch": 0.0017480542487940973,
      "grad_norm": 21778.072641994746,
      "learning_rate": 1.441579322970365e-07,
      "loss": 1.6658,
      "step": 481664
    },
    {
      "epoch": 0.0017481703831449338,
      "grad_norm": 27581.511198627242,
      "learning_rate": 1.441531392250023e-07,
      "loss": 1.6522,
      "step": 481696
    },
    {
      "epoch": 0.0017482865174957706,
      "grad_norm": 20562.107090471054,
      "learning_rate": 1.4414834663102733e-07,
      "loss": 1.6467,
      "step": 481728
    },
    {
      "epoch": 0.0017484026518466073,
      "grad_norm": 24698.54084758855,
      "learning_rate": 1.4414355451503213e-07,
      "loss": 1.6711,
      "step": 481760
    },
    {
      "epoch": 0.001748518786197444,
      "grad_norm": 23026.84485551592,
      "learning_rate": 1.4413876287693737e-07,
      "loss": 1.6677,
      "step": 481792
    },
    {
      "epoch": 0.0017486349205482808,
      "grad_norm": 27919.486814767926,
      "learning_rate": 1.4413397171666347e-07,
      "loss": 1.6597,
      "step": 481824
    },
    {
      "epoch": 0.0017487510548991174,
      "grad_norm": 20708.36797046064,
      "learning_rate": 1.4412918103413114e-07,
      "loss": 1.6505,
      "step": 481856
    },
    {
      "epoch": 0.0017488671892499541,
      "grad_norm": 19006.344624887763,
      "learning_rate": 1.441243908292609e-07,
      "loss": 1.644,
      "step": 481888
    },
    {
      "epoch": 0.0017489833236007909,
      "grad_norm": 19489.160884963723,
      "learning_rate": 1.4411960110197348e-07,
      "loss": 1.6541,
      "step": 481920
    },
    {
      "epoch": 0.0017490994579516276,
      "grad_norm": 16292.319908472213,
      "learning_rate": 1.4411481185218944e-07,
      "loss": 1.6603,
      "step": 481952
    },
    {
      "epoch": 0.0017492155923024642,
      "grad_norm": 22828.250217657944,
      "learning_rate": 1.4411002307982948e-07,
      "loss": 1.6765,
      "step": 481984
    },
    {
      "epoch": 0.001749331726653301,
      "grad_norm": 19389.926869382463,
      "learning_rate": 1.4410523478481426e-07,
      "loss": 1.6613,
      "step": 482016
    },
    {
      "epoch": 0.0017494478610041377,
      "grad_norm": 25474.15066297599,
      "learning_rate": 1.4410044696706453e-07,
      "loss": 1.66,
      "step": 482048
    },
    {
      "epoch": 0.0017495639953549744,
      "grad_norm": 24051.670544891473,
      "learning_rate": 1.44095659626501e-07,
      "loss": 1.6458,
      "step": 482080
    },
    {
      "epoch": 0.0017496801297058112,
      "grad_norm": 17713.927176095087,
      "learning_rate": 1.4409087276304436e-07,
      "loss": 1.6588,
      "step": 482112
    },
    {
      "epoch": 0.0017497962640566477,
      "grad_norm": 19138.617295928147,
      "learning_rate": 1.4408608637661542e-07,
      "loss": 1.6604,
      "step": 482144
    },
    {
      "epoch": 0.0017499123984074845,
      "grad_norm": 27252.39394988998,
      "learning_rate": 1.4408130046713492e-07,
      "loss": 1.6487,
      "step": 482176
    },
    {
      "epoch": 0.0017500285327583212,
      "grad_norm": 25616.5106132744,
      "learning_rate": 1.4407651503452367e-07,
      "loss": 1.6691,
      "step": 482208
    },
    {
      "epoch": 0.001750144667109158,
      "grad_norm": 17134.66311311664,
      "learning_rate": 1.440717300787025e-07,
      "loss": 1.6596,
      "step": 482240
    },
    {
      "epoch": 0.0017502608014599945,
      "grad_norm": 20699.155924819737,
      "learning_rate": 1.4406694559959223e-07,
      "loss": 1.6744,
      "step": 482272
    },
    {
      "epoch": 0.0017503769358108313,
      "grad_norm": 19472.300326361034,
      "learning_rate": 1.440621615971137e-07,
      "loss": 1.6712,
      "step": 482304
    },
    {
      "epoch": 0.001750493070161668,
      "grad_norm": 8983.449003584314,
      "learning_rate": 1.440575275491603e-07,
      "loss": 1.6606,
      "step": 482336
    },
    {
      "epoch": 0.0017506092045125048,
      "grad_norm": 15707.98319326832,
      "learning_rate": 1.440527444848193e-07,
      "loss": 1.6741,
      "step": 482368
    },
    {
      "epoch": 0.0017507253388633415,
      "grad_norm": 10081.326896792902,
      "learning_rate": 1.4404796189687516e-07,
      "loss": 1.6713,
      "step": 482400
    },
    {
      "epoch": 0.001750841473214178,
      "grad_norm": 10209.258739007451,
      "learning_rate": 1.440431797852488e-07,
      "loss": 1.6597,
      "step": 482432
    },
    {
      "epoch": 0.0017509576075650148,
      "grad_norm": 8689.63934809725,
      "learning_rate": 1.440383981498612e-07,
      "loss": 1.6434,
      "step": 482464
    },
    {
      "epoch": 0.0017510737419158516,
      "grad_norm": 9014.333253214018,
      "learning_rate": 1.4403361699063332e-07,
      "loss": 1.6577,
      "step": 482496
    },
    {
      "epoch": 0.0017511898762666883,
      "grad_norm": 9258.740519098697,
      "learning_rate": 1.440288363074861e-07,
      "loss": 1.6611,
      "step": 482528
    },
    {
      "epoch": 0.0017513060106175249,
      "grad_norm": 11097.575500982184,
      "learning_rate": 1.4402405610034054e-07,
      "loss": 1.6787,
      "step": 482560
    },
    {
      "epoch": 0.0017514221449683616,
      "grad_norm": 12310.24873834806,
      "learning_rate": 1.4401927636911767e-07,
      "loss": 1.6599,
      "step": 482592
    },
    {
      "epoch": 0.0017515382793191984,
      "grad_norm": 9366.200510345698,
      "learning_rate": 1.4401449711373854e-07,
      "loss": 1.6511,
      "step": 482624
    },
    {
      "epoch": 0.0017516544136700351,
      "grad_norm": 11120.964886195801,
      "learning_rate": 1.4400971833412418e-07,
      "loss": 1.6682,
      "step": 482656
    },
    {
      "epoch": 0.0017517705480208719,
      "grad_norm": 8292.747071990078,
      "learning_rate": 1.4400494003019563e-07,
      "loss": 1.6508,
      "step": 482688
    },
    {
      "epoch": 0.0017518866823717084,
      "grad_norm": 9531.957406535133,
      "learning_rate": 1.4400016220187407e-07,
      "loss": 1.6665,
      "step": 482720
    },
    {
      "epoch": 0.0017520028167225452,
      "grad_norm": 9119.309403677451,
      "learning_rate": 1.439953848490805e-07,
      "loss": 1.6554,
      "step": 482752
    },
    {
      "epoch": 0.001752118951073382,
      "grad_norm": 12093.177084620898,
      "learning_rate": 1.4399060797173614e-07,
      "loss": 1.6525,
      "step": 482784
    },
    {
      "epoch": 0.0017522350854242187,
      "grad_norm": 8894.275799636527,
      "learning_rate": 1.4398583156976203e-07,
      "loss": 1.6481,
      "step": 482816
    },
    {
      "epoch": 0.0017523512197750552,
      "grad_norm": 9388.894503614363,
      "learning_rate": 1.4398105564307942e-07,
      "loss": 1.6585,
      "step": 482848
    },
    {
      "epoch": 0.001752467354125892,
      "grad_norm": 11717.197958556473,
      "learning_rate": 1.4397628019160942e-07,
      "loss": 1.6552,
      "step": 482880
    },
    {
      "epoch": 0.0017525834884767287,
      "grad_norm": 11392.07443795905,
      "learning_rate": 1.439715052152733e-07,
      "loss": 1.6453,
      "step": 482912
    },
    {
      "epoch": 0.0017526996228275655,
      "grad_norm": 7897.422617537952,
      "learning_rate": 1.4396673071399222e-07,
      "loss": 1.6508,
      "step": 482944
    },
    {
      "epoch": 0.0017528157571784022,
      "grad_norm": 9820.489804485314,
      "learning_rate": 1.4396195668768744e-07,
      "loss": 1.6429,
      "step": 482976
    },
    {
      "epoch": 0.0017529318915292388,
      "grad_norm": 9301.210458859641,
      "learning_rate": 1.439571831362802e-07,
      "loss": 1.668,
      "step": 483008
    },
    {
      "epoch": 0.0017530480258800755,
      "grad_norm": 9832.97127017058,
      "learning_rate": 1.4395241005969178e-07,
      "loss": 1.6601,
      "step": 483040
    },
    {
      "epoch": 0.0017531641602309123,
      "grad_norm": 11930.500743891684,
      "learning_rate": 1.4394763745784346e-07,
      "loss": 1.6727,
      "step": 483072
    },
    {
      "epoch": 0.001753280294581749,
      "grad_norm": 8771.660618149792,
      "learning_rate": 1.4394286533065656e-07,
      "loss": 1.6832,
      "step": 483104
    },
    {
      "epoch": 0.0017533964289325856,
      "grad_norm": 8206.407496584605,
      "learning_rate": 1.439380936780524e-07,
      "loss": 1.6697,
      "step": 483136
    },
    {
      "epoch": 0.0017535125632834223,
      "grad_norm": 8272.089337041762,
      "learning_rate": 1.4393332249995233e-07,
      "loss": 1.6839,
      "step": 483168
    },
    {
      "epoch": 0.001753628697634259,
      "grad_norm": 10017.424419480289,
      "learning_rate": 1.4392855179627773e-07,
      "loss": 1.6974,
      "step": 483200
    },
    {
      "epoch": 0.0017537448319850958,
      "grad_norm": 8505.160315949372,
      "learning_rate": 1.439237815669499e-07,
      "loss": 1.6951,
      "step": 483232
    },
    {
      "epoch": 0.0017538609663359326,
      "grad_norm": 11079.5962922843,
      "learning_rate": 1.439190118118903e-07,
      "loss": 1.6942,
      "step": 483264
    },
    {
      "epoch": 0.0017539771006867691,
      "grad_norm": 11566.236034250727,
      "learning_rate": 1.439142425310204e-07,
      "loss": 1.6894,
      "step": 483296
    },
    {
      "epoch": 0.0017540932350376059,
      "grad_norm": 24691.714885766844,
      "learning_rate": 1.4390947372426152e-07,
      "loss": 1.6536,
      "step": 483328
    },
    {
      "epoch": 0.0017542093693884426,
      "grad_norm": 21559.76437719114,
      "learning_rate": 1.4390470539153522e-07,
      "loss": 1.6495,
      "step": 483360
    },
    {
      "epoch": 0.0017543255037392794,
      "grad_norm": 26491.70798570753,
      "learning_rate": 1.4389993753276289e-07,
      "loss": 1.6517,
      "step": 483392
    },
    {
      "epoch": 0.001754441638090116,
      "grad_norm": 20971.34244630038,
      "learning_rate": 1.4389517014786605e-07,
      "loss": 1.6623,
      "step": 483424
    },
    {
      "epoch": 0.0017545577724409527,
      "grad_norm": 24316.998498992427,
      "learning_rate": 1.4389040323676623e-07,
      "loss": 1.6697,
      "step": 483456
    },
    {
      "epoch": 0.0017546739067917894,
      "grad_norm": 19551.963993420202,
      "learning_rate": 1.438856367993849e-07,
      "loss": 1.645,
      "step": 483488
    },
    {
      "epoch": 0.0017547900411426262,
      "grad_norm": 10507.135289887534,
      "learning_rate": 1.43881019764842e-07,
      "loss": 1.666,
      "step": 483520
    },
    {
      "epoch": 0.001754906175493463,
      "grad_norm": 8811.235781659687,
      "learning_rate": 1.438762542598648e-07,
      "loss": 1.6735,
      "step": 483552
    },
    {
      "epoch": 0.0017550223098442995,
      "grad_norm": 9793.272180430808,
      "learning_rate": 1.4387148922837324e-07,
      "loss": 1.6554,
      "step": 483584
    },
    {
      "epoch": 0.0017551384441951362,
      "grad_norm": 9853.090987096384,
      "learning_rate": 1.4386672467028894e-07,
      "loss": 1.6785,
      "step": 483616
    },
    {
      "epoch": 0.001755254578545973,
      "grad_norm": 9347.629111170383,
      "learning_rate": 1.4386196058553353e-07,
      "loss": 1.6889,
      "step": 483648
    },
    {
      "epoch": 0.0017553707128968097,
      "grad_norm": 9174.465434018486,
      "learning_rate": 1.4385719697402866e-07,
      "loss": 1.6775,
      "step": 483680
    },
    {
      "epoch": 0.0017554868472476463,
      "grad_norm": 13564.142287664194,
      "learning_rate": 1.4385243383569593e-07,
      "loss": 1.6684,
      "step": 483712
    },
    {
      "epoch": 0.001755602981598483,
      "grad_norm": 13760.259445228496,
      "learning_rate": 1.4384767117045705e-07,
      "loss": 1.6654,
      "step": 483744
    },
    {
      "epoch": 0.0017557191159493198,
      "grad_norm": 9849.468005938188,
      "learning_rate": 1.4384290897823368e-07,
      "loss": 1.6586,
      "step": 483776
    },
    {
      "epoch": 0.0017558352503001565,
      "grad_norm": 10262.414140931945,
      "learning_rate": 1.4383814725894755e-07,
      "loss": 1.6803,
      "step": 483808
    },
    {
      "epoch": 0.0017559513846509933,
      "grad_norm": 13702.073127815367,
      "learning_rate": 1.4383338601252038e-07,
      "loss": 1.6531,
      "step": 483840
    },
    {
      "epoch": 0.0017560675190018298,
      "grad_norm": 8616.509734225338,
      "learning_rate": 1.4382862523887393e-07,
      "loss": 1.6442,
      "step": 483872
    },
    {
      "epoch": 0.0017561836533526666,
      "grad_norm": 16389.27197895013,
      "learning_rate": 1.4382386493792992e-07,
      "loss": 1.6441,
      "step": 483904
    },
    {
      "epoch": 0.0017562997877035033,
      "grad_norm": 10117.279080859636,
      "learning_rate": 1.4381910510961018e-07,
      "loss": 1.6704,
      "step": 483936
    },
    {
      "epoch": 0.00175641592205434,
      "grad_norm": 7937.298532876283,
      "learning_rate": 1.4381434575383648e-07,
      "loss": 1.6695,
      "step": 483968
    },
    {
      "epoch": 0.0017565320564051766,
      "grad_norm": 9557.919857374825,
      "learning_rate": 1.4380958687053064e-07,
      "loss": 1.6512,
      "step": 484000
    },
    {
      "epoch": 0.0017566481907560134,
      "grad_norm": 9364.176845831138,
      "learning_rate": 1.438048284596145e-07,
      "loss": 1.6759,
      "step": 484032
    },
    {
      "epoch": 0.0017567643251068501,
      "grad_norm": 9955.384472736348,
      "learning_rate": 1.4380007052100988e-07,
      "loss": 1.6836,
      "step": 484064
    },
    {
      "epoch": 0.001756880459457687,
      "grad_norm": 10123.99881469768,
      "learning_rate": 1.437953130546387e-07,
      "loss": 1.6835,
      "step": 484096
    },
    {
      "epoch": 0.0017569965938085237,
      "grad_norm": 10803.035962172857,
      "learning_rate": 1.4379055606042283e-07,
      "loss": 1.6963,
      "step": 484128
    },
    {
      "epoch": 0.0017571127281593602,
      "grad_norm": 8099.89382646464,
      "learning_rate": 1.4378579953828417e-07,
      "loss": 1.7199,
      "step": 484160
    },
    {
      "epoch": 0.001757228862510197,
      "grad_norm": 10579.715308078947,
      "learning_rate": 1.4378104348814464e-07,
      "loss": 1.6966,
      "step": 484192
    },
    {
      "epoch": 0.0017573449968610337,
      "grad_norm": 11595.56932625561,
      "learning_rate": 1.437762879099262e-07,
      "loss": 1.6747,
      "step": 484224
    },
    {
      "epoch": 0.0017574611312118705,
      "grad_norm": 9857.89287829808,
      "learning_rate": 1.4377153280355077e-07,
      "loss": 1.6734,
      "step": 484256
    },
    {
      "epoch": 0.001757577265562707,
      "grad_norm": 12331.186479816126,
      "learning_rate": 1.4376677816894039e-07,
      "loss": 1.681,
      "step": 484288
    },
    {
      "epoch": 0.0017576933999135437,
      "grad_norm": 10333.132922787745,
      "learning_rate": 1.4376202400601703e-07,
      "loss": 1.6672,
      "step": 484320
    },
    {
      "epoch": 0.0017578095342643805,
      "grad_norm": 7444.313400173316,
      "learning_rate": 1.4375727031470269e-07,
      "loss": 1.656,
      "step": 484352
    },
    {
      "epoch": 0.0017579256686152173,
      "grad_norm": 18364.215692481943,
      "learning_rate": 1.437525170949194e-07,
      "loss": 1.6591,
      "step": 484384
    },
    {
      "epoch": 0.001758041802966054,
      "grad_norm": 8881.946633480748,
      "learning_rate": 1.4374776434658925e-07,
      "loss": 1.6639,
      "step": 484416
    },
    {
      "epoch": 0.0017581579373168905,
      "grad_norm": 11296.661453721626,
      "learning_rate": 1.4374301206963427e-07,
      "loss": 1.6721,
      "step": 484448
    },
    {
      "epoch": 0.0017582740716677273,
      "grad_norm": 8801.941831209748,
      "learning_rate": 1.437382602639766e-07,
      "loss": 1.6456,
      "step": 484480
    },
    {
      "epoch": 0.001758390206018564,
      "grad_norm": 17506.646966224,
      "learning_rate": 1.4373350892953827e-07,
      "loss": 1.6424,
      "step": 484512
    },
    {
      "epoch": 0.0017585063403694008,
      "grad_norm": 16969.872598225364,
      "learning_rate": 1.4372875806624144e-07,
      "loss": 1.6424,
      "step": 484544
    },
    {
      "epoch": 0.0017586224747202373,
      "grad_norm": 17507.63719066625,
      "learning_rate": 1.4372400767400828e-07,
      "loss": 1.6539,
      "step": 484576
    },
    {
      "epoch": 0.001758738609071074,
      "grad_norm": 16175.751234486761,
      "learning_rate": 1.437192577527609e-07,
      "loss": 1.658,
      "step": 484608
    },
    {
      "epoch": 0.0017588547434219109,
      "grad_norm": 24070.561937769548,
      "learning_rate": 1.437145083024215e-07,
      "loss": 1.669,
      "step": 484640
    },
    {
      "epoch": 0.0017589708777727476,
      "grad_norm": 9660.13519574131,
      "learning_rate": 1.437099077213959e-07,
      "loss": 1.6943,
      "step": 484672
    },
    {
      "epoch": 0.0017590870121235844,
      "grad_norm": 8230.789148070797,
      "learning_rate": 1.4370515919792923e-07,
      "loss": 1.6859,
      "step": 484704
    },
    {
      "epoch": 0.001759203146474421,
      "grad_norm": 17243.644626354373,
      "learning_rate": 1.437004111451396e-07,
      "loss": 1.6833,
      "step": 484736
    },
    {
      "epoch": 0.0017593192808252577,
      "grad_norm": 13926.244863566057,
      "learning_rate": 1.4369566356294928e-07,
      "loss": 1.6636,
      "step": 484768
    },
    {
      "epoch": 0.0017594354151760944,
      "grad_norm": 10598.69718408824,
      "learning_rate": 1.4369091645128055e-07,
      "loss": 1.6607,
      "step": 484800
    },
    {
      "epoch": 0.0017595515495269312,
      "grad_norm": 8399.5557025357,
      "learning_rate": 1.4368616981005563e-07,
      "loss": 1.6668,
      "step": 484832
    },
    {
      "epoch": 0.0017596676838777677,
      "grad_norm": 11438.287634082297,
      "learning_rate": 1.4368142363919687e-07,
      "loss": 1.6629,
      "step": 484864
    },
    {
      "epoch": 0.0017597838182286045,
      "grad_norm": 8743.209250612728,
      "learning_rate": 1.436766779386266e-07,
      "loss": 1.6769,
      "step": 484896
    },
    {
      "epoch": 0.0017598999525794412,
      "grad_norm": 9986.246341844368,
      "learning_rate": 1.4367193270826712e-07,
      "loss": 1.6777,
      "step": 484928
    },
    {
      "epoch": 0.001760016086930278,
      "grad_norm": 9690.921008861851,
      "learning_rate": 1.4366718794804083e-07,
      "loss": 1.6948,
      "step": 484960
    },
    {
      "epoch": 0.0017601322212811145,
      "grad_norm": 11994.740430705451,
      "learning_rate": 1.4366244365787008e-07,
      "loss": 1.6727,
      "step": 484992
    },
    {
      "epoch": 0.0017602483556319513,
      "grad_norm": 9630.016199363323,
      "learning_rate": 1.4365769983767725e-07,
      "loss": 1.6727,
      "step": 485024
    },
    {
      "epoch": 0.001760364489982788,
      "grad_norm": 8957.62245241448,
      "learning_rate": 1.4365295648738476e-07,
      "loss": 1.6582,
      "step": 485056
    },
    {
      "epoch": 0.0017604806243336248,
      "grad_norm": 10606.289454847063,
      "learning_rate": 1.4364821360691504e-07,
      "loss": 1.6646,
      "step": 485088
    },
    {
      "epoch": 0.0017605967586844615,
      "grad_norm": 9428.753682221208,
      "learning_rate": 1.4364347119619056e-07,
      "loss": 1.6563,
      "step": 485120
    },
    {
      "epoch": 0.001760712893035298,
      "grad_norm": 10803.046792456284,
      "learning_rate": 1.4363872925513376e-07,
      "loss": 1.6748,
      "step": 485152
    },
    {
      "epoch": 0.0017608290273861348,
      "grad_norm": 9158.125572408362,
      "learning_rate": 1.4363398778366713e-07,
      "loss": 1.6858,
      "step": 485184
    },
    {
      "epoch": 0.0017609451617369716,
      "grad_norm": 8347.179523647494,
      "learning_rate": 1.4362924678171314e-07,
      "loss": 1.6756,
      "step": 485216
    },
    {
      "epoch": 0.0017610612960878083,
      "grad_norm": 9557.86796309721,
      "learning_rate": 1.4362450624919437e-07,
      "loss": 1.6842,
      "step": 485248
    },
    {
      "epoch": 0.0017611774304386449,
      "grad_norm": 7877.6921747425495,
      "learning_rate": 1.436197661860333e-07,
      "loss": 1.6938,
      "step": 485280
    },
    {
      "epoch": 0.0017612935647894816,
      "grad_norm": 9668.452616628992,
      "learning_rate": 1.436150265921525e-07,
      "loss": 1.6739,
      "step": 485312
    },
    {
      "epoch": 0.0017614096991403184,
      "grad_norm": 10739.380429056419,
      "learning_rate": 1.4361028746747455e-07,
      "loss": 1.6561,
      "step": 485344
    },
    {
      "epoch": 0.0017615258334911551,
      "grad_norm": 10537.097892683736,
      "learning_rate": 1.4360554881192203e-07,
      "loss": 1.6459,
      "step": 485376
    },
    {
      "epoch": 0.0017616419678419919,
      "grad_norm": 9035.189870722143,
      "learning_rate": 1.4360081062541754e-07,
      "loss": 1.6439,
      "step": 485408
    },
    {
      "epoch": 0.0017617581021928284,
      "grad_norm": 11565.717790089813,
      "learning_rate": 1.4359607290788375e-07,
      "loss": 1.6729,
      "step": 485440
    },
    {
      "epoch": 0.0017618742365436652,
      "grad_norm": 8372.211177460827,
      "learning_rate": 1.4359133565924326e-07,
      "loss": 1.6658,
      "step": 485472
    },
    {
      "epoch": 0.001761990370894502,
      "grad_norm": 12491.032783561175,
      "learning_rate": 1.435865988794187e-07,
      "loss": 1.6345,
      "step": 485504
    },
    {
      "epoch": 0.0017621065052453387,
      "grad_norm": 12952.50747924895,
      "learning_rate": 1.4358186256833282e-07,
      "loss": 1.6533,
      "step": 485536
    },
    {
      "epoch": 0.0017622226395961752,
      "grad_norm": 9377.435683597088,
      "learning_rate": 1.435771267259083e-07,
      "loss": 1.6545,
      "step": 485568
    },
    {
      "epoch": 0.001762338773947012,
      "grad_norm": 9060.903376595516,
      "learning_rate": 1.4357239135206781e-07,
      "loss": 1.649,
      "step": 485600
    },
    {
      "epoch": 0.0017624549082978487,
      "grad_norm": 9307.193884302615,
      "learning_rate": 1.4356765644673415e-07,
      "loss": 1.6678,
      "step": 485632
    },
    {
      "epoch": 0.0017625710426486855,
      "grad_norm": 16561.999879241637,
      "learning_rate": 1.4356292200983e-07,
      "loss": 1.6874,
      "step": 485664
    },
    {
      "epoch": 0.0017626871769995222,
      "grad_norm": 20439.367113489596,
      "learning_rate": 1.4355818804127818e-07,
      "loss": 1.7005,
      "step": 485696
    },
    {
      "epoch": 0.0017628033113503588,
      "grad_norm": 21539.11827350414,
      "learning_rate": 1.4355345454100145e-07,
      "loss": 1.6978,
      "step": 485728
    },
    {
      "epoch": 0.0017629194457011955,
      "grad_norm": 10453.140772035935,
      "learning_rate": 1.4354886940908887e-07,
      "loss": 1.7026,
      "step": 485760
    },
    {
      "epoch": 0.0017630355800520323,
      "grad_norm": 8007.5125975548735,
      "learning_rate": 1.4354413683050315e-07,
      "loss": 1.7152,
      "step": 485792
    },
    {
      "epoch": 0.001763151714402869,
      "grad_norm": 9713.205547088974,
      "learning_rate": 1.4353940471996338e-07,
      "loss": 1.7248,
      "step": 485824
    },
    {
      "epoch": 0.0017632678487537056,
      "grad_norm": 7983.1040328934705,
      "learning_rate": 1.4353467307739247e-07,
      "loss": 1.6874,
      "step": 485856
    },
    {
      "epoch": 0.0017633839831045423,
      "grad_norm": 7761.082269889941,
      "learning_rate": 1.4352994190271325e-07,
      "loss": 1.6788,
      "step": 485888
    },
    {
      "epoch": 0.001763500117455379,
      "grad_norm": 12410.101530608039,
      "learning_rate": 1.4352521119584864e-07,
      "loss": 1.6655,
      "step": 485920
    },
    {
      "epoch": 0.0017636162518062158,
      "grad_norm": 7394.566383500794,
      "learning_rate": 1.4352048095672152e-07,
      "loss": 1.6802,
      "step": 485952
    },
    {
      "epoch": 0.0017637323861570526,
      "grad_norm": 7470.8809386845405,
      "learning_rate": 1.4351575118525484e-07,
      "loss": 1.6808,
      "step": 485984
    },
    {
      "epoch": 0.0017638485205078891,
      "grad_norm": 8740.270705189858,
      "learning_rate": 1.4351102188137155e-07,
      "loss": 1.6523,
      "step": 486016
    },
    {
      "epoch": 0.0017639646548587259,
      "grad_norm": 10916.414063235234,
      "learning_rate": 1.435062930449946e-07,
      "loss": 1.6631,
      "step": 486048
    },
    {
      "epoch": 0.0017640807892095626,
      "grad_norm": 8428.653866424935,
      "learning_rate": 1.4350156467604695e-07,
      "loss": 1.6641,
      "step": 486080
    },
    {
      "epoch": 0.0017641969235603994,
      "grad_norm": 13562.249444690213,
      "learning_rate": 1.434968367744516e-07,
      "loss": 1.6618,
      "step": 486112
    },
    {
      "epoch": 0.001764313057911236,
      "grad_norm": 9658.010768269001,
      "learning_rate": 1.4349210934013166e-07,
      "loss": 1.6896,
      "step": 486144
    },
    {
      "epoch": 0.0017644291922620727,
      "grad_norm": 10239.534950377385,
      "learning_rate": 1.4348738237301002e-07,
      "loss": 1.7072,
      "step": 486176
    },
    {
      "epoch": 0.0017645453266129094,
      "grad_norm": 15162.45929920341,
      "learning_rate": 1.4348265587300984e-07,
      "loss": 1.6816,
      "step": 486208
    },
    {
      "epoch": 0.0017646614609637462,
      "grad_norm": 9001.894022926508,
      "learning_rate": 1.4347792984005415e-07,
      "loss": 1.6671,
      "step": 486240
    },
    {
      "epoch": 0.001764777595314583,
      "grad_norm": 9130.203064554478,
      "learning_rate": 1.4347320427406604e-07,
      "loss": 1.6622,
      "step": 486272
    },
    {
      "epoch": 0.0017648937296654195,
      "grad_norm": 10431.073961965758,
      "learning_rate": 1.434684791749686e-07,
      "loss": 1.67,
      "step": 486304
    },
    {
      "epoch": 0.0017650098640162562,
      "grad_norm": 8782.533689089954,
      "learning_rate": 1.4346375454268498e-07,
      "loss": 1.6895,
      "step": 486336
    },
    {
      "epoch": 0.001765125998367093,
      "grad_norm": 10024.830272877442,
      "learning_rate": 1.4345903037713831e-07,
      "loss": 1.6589,
      "step": 486368
    },
    {
      "epoch": 0.0017652421327179297,
      "grad_norm": 8704.114314506674,
      "learning_rate": 1.4345430667825177e-07,
      "loss": 1.6384,
      "step": 486400
    },
    {
      "epoch": 0.0017653582670687663,
      "grad_norm": 10875.248778763638,
      "learning_rate": 1.4344958344594847e-07,
      "loss": 1.6476,
      "step": 486432
    },
    {
      "epoch": 0.001765474401419603,
      "grad_norm": 8739.99565217283,
      "learning_rate": 1.434448606801517e-07,
      "loss": 1.6655,
      "step": 486464
    },
    {
      "epoch": 0.0017655905357704398,
      "grad_norm": 8859.78080992978,
      "learning_rate": 1.4344013838078458e-07,
      "loss": 1.6503,
      "step": 486496
    },
    {
      "epoch": 0.0017657066701212765,
      "grad_norm": 12055.129696523385,
      "learning_rate": 1.4343541654777037e-07,
      "loss": 1.6617,
      "step": 486528
    },
    {
      "epoch": 0.0017658228044721133,
      "grad_norm": 8161.415808546945,
      "learning_rate": 1.4343069518103237e-07,
      "loss": 1.6442,
      "step": 486560
    },
    {
      "epoch": 0.0017659389388229498,
      "grad_norm": 9194.452022823329,
      "learning_rate": 1.4342597428049378e-07,
      "loss": 1.655,
      "step": 486592
    },
    {
      "epoch": 0.0017660550731737866,
      "grad_norm": 9204.585596321,
      "learning_rate": 1.4342125384607788e-07,
      "loss": 1.6618,
      "step": 486624
    },
    {
      "epoch": 0.0017661712075246233,
      "grad_norm": 10806.553011946038,
      "learning_rate": 1.4341653387770803e-07,
      "loss": 1.6857,
      "step": 486656
    },
    {
      "epoch": 0.00176628734187546,
      "grad_norm": 12198.617462647151,
      "learning_rate": 1.4341181437530746e-07,
      "loss": 1.7074,
      "step": 486688
    },
    {
      "epoch": 0.0017664034762262966,
      "grad_norm": 9909.245783610375,
      "learning_rate": 1.434070953387996e-07,
      "loss": 1.6903,
      "step": 486720
    },
    {
      "epoch": 0.0017665196105771334,
      "grad_norm": 28059.88453290569,
      "learning_rate": 1.4340237676810773e-07,
      "loss": 1.6907,
      "step": 486752
    },
    {
      "epoch": 0.0017666357449279701,
      "grad_norm": 21724.013993735134,
      "learning_rate": 1.4339765866315528e-07,
      "loss": 1.6825,
      "step": 486784
    },
    {
      "epoch": 0.0017667518792788069,
      "grad_norm": 20561.186736178435,
      "learning_rate": 1.4339294102386557e-07,
      "loss": 1.6691,
      "step": 486816
    },
    {
      "epoch": 0.0017668680136296436,
      "grad_norm": 22388.5649383787,
      "learning_rate": 1.4338822385016207e-07,
      "loss": 1.6688,
      "step": 486848
    },
    {
      "epoch": 0.0017669841479804802,
      "grad_norm": 16822.874665169446,
      "learning_rate": 1.4338350714196817e-07,
      "loss": 1.6612,
      "step": 486880
    },
    {
      "epoch": 0.001767100282331317,
      "grad_norm": 24376.661625415403,
      "learning_rate": 1.433787908992073e-07,
      "loss": 1.662,
      "step": 486912
    },
    {
      "epoch": 0.0017672164166821537,
      "grad_norm": 12524.111146105339,
      "learning_rate": 1.4337422248280363e-07,
      "loss": 1.6476,
      "step": 486944
    },
    {
      "epoch": 0.0017673325510329904,
      "grad_norm": 8992.525006915466,
      "learning_rate": 1.4336950715614042e-07,
      "loss": 1.6689,
      "step": 486976
    },
    {
      "epoch": 0.001767448685383827,
      "grad_norm": 10885.950211166686,
      "learning_rate": 1.4336479229468308e-07,
      "loss": 1.6491,
      "step": 487008
    },
    {
      "epoch": 0.0017675648197346637,
      "grad_norm": 8350.209338693252,
      "learning_rate": 1.433600778983551e-07,
      "loss": 1.6614,
      "step": 487040
    },
    {
      "epoch": 0.0017676809540855005,
      "grad_norm": 9797.60807544372,
      "learning_rate": 1.4335536396708006e-07,
      "loss": 1.668,
      "step": 487072
    },
    {
      "epoch": 0.0017677970884363372,
      "grad_norm": 10291.463647120365,
      "learning_rate": 1.4335065050078144e-07,
      "loss": 1.6414,
      "step": 487104
    },
    {
      "epoch": 0.001767913222787174,
      "grad_norm": 12849.098022818567,
      "learning_rate": 1.4334593749938286e-07,
      "loss": 1.6578,
      "step": 487136
    },
    {
      "epoch": 0.0017680293571380105,
      "grad_norm": 10697.284702203639,
      "learning_rate": 1.4334122496280785e-07,
      "loss": 1.6638,
      "step": 487168
    },
    {
      "epoch": 0.0017681454914888473,
      "grad_norm": 10594.700939620712,
      "learning_rate": 1.4333651289098007e-07,
      "loss": 1.669,
      "step": 487200
    },
    {
      "epoch": 0.001768261625839684,
      "grad_norm": 9000.021888862271,
      "learning_rate": 1.4333180128382308e-07,
      "loss": 1.6625,
      "step": 487232
    },
    {
      "epoch": 0.0017683777601905208,
      "grad_norm": 14633.603247320872,
      "learning_rate": 1.4332709014126056e-07,
      "loss": 1.6547,
      "step": 487264
    },
    {
      "epoch": 0.0017684938945413573,
      "grad_norm": 9837.27075971786,
      "learning_rate": 1.4332237946321613e-07,
      "loss": 1.6621,
      "step": 487296
    },
    {
      "epoch": 0.001768610028892194,
      "grad_norm": 9526.826963895166,
      "learning_rate": 1.4331766924961347e-07,
      "loss": 1.6704,
      "step": 487328
    },
    {
      "epoch": 0.0017687261632430308,
      "grad_norm": 12029.539475807043,
      "learning_rate": 1.4331295950037623e-07,
      "loss": 1.6539,
      "step": 487360
    },
    {
      "epoch": 0.0017688422975938676,
      "grad_norm": 9481.409705312813,
      "learning_rate": 1.433082502154282e-07,
      "loss": 1.6562,
      "step": 487392
    },
    {
      "epoch": 0.0017689584319447043,
      "grad_norm": 9797.555409386568,
      "learning_rate": 1.4330354139469303e-07,
      "loss": 1.6591,
      "step": 487424
    },
    {
      "epoch": 0.0017690745662955409,
      "grad_norm": 10518.035558030786,
      "learning_rate": 1.4329883303809448e-07,
      "loss": 1.6823,
      "step": 487456
    },
    {
      "epoch": 0.0017691907006463776,
      "grad_norm": 7926.127427691281,
      "learning_rate": 1.432941251455563e-07,
      "loss": 1.6743,
      "step": 487488
    },
    {
      "epoch": 0.0017693068349972144,
      "grad_norm": 8500.150351611434,
      "learning_rate": 1.4328941771700228e-07,
      "loss": 1.6479,
      "step": 487520
    },
    {
      "epoch": 0.0017694229693480511,
      "grad_norm": 8384.768690906149,
      "learning_rate": 1.4328471075235622e-07,
      "loss": 1.6634,
      "step": 487552
    },
    {
      "epoch": 0.0017695391036988877,
      "grad_norm": 9057.70059120967,
      "learning_rate": 1.432800042515419e-07,
      "loss": 1.6789,
      "step": 487584
    },
    {
      "epoch": 0.0017696552380497244,
      "grad_norm": 11413.208838884882,
      "learning_rate": 1.4327529821448318e-07,
      "loss": 1.6764,
      "step": 487616
    },
    {
      "epoch": 0.0017697713724005612,
      "grad_norm": 8989.736481121123,
      "learning_rate": 1.4327059264110385e-07,
      "loss": 1.6797,
      "step": 487648
    },
    {
      "epoch": 0.001769887506751398,
      "grad_norm": 8965.801916170132,
      "learning_rate": 1.4326588753132783e-07,
      "loss": 1.6835,
      "step": 487680
    },
    {
      "epoch": 0.0017700036411022347,
      "grad_norm": 11926.42461092175,
      "learning_rate": 1.43261182885079e-07,
      "loss": 1.6749,
      "step": 487712
    },
    {
      "epoch": 0.0017701197754530712,
      "grad_norm": 13180.003490136107,
      "learning_rate": 1.4325647870228122e-07,
      "loss": 1.6624,
      "step": 487744
    },
    {
      "epoch": 0.001770235909803908,
      "grad_norm": 13314.60881888762,
      "learning_rate": 1.4325177498285843e-07,
      "loss": 1.6608,
      "step": 487776
    },
    {
      "epoch": 0.0017703520441547447,
      "grad_norm": 14285.470660779783,
      "learning_rate": 1.4324707172673453e-07,
      "loss": 1.657,
      "step": 487808
    },
    {
      "epoch": 0.0017704681785055815,
      "grad_norm": 10567.014431711541,
      "learning_rate": 1.432423689338335e-07,
      "loss": 1.6661,
      "step": 487840
    },
    {
      "epoch": 0.001770584312856418,
      "grad_norm": 9549.601981234611,
      "learning_rate": 1.432376666040793e-07,
      "loss": 1.6509,
      "step": 487872
    },
    {
      "epoch": 0.0017707004472072548,
      "grad_norm": 12556.801901758266,
      "learning_rate": 1.432329647373959e-07,
      "loss": 1.6646,
      "step": 487904
    },
    {
      "epoch": 0.0017708165815580915,
      "grad_norm": 24099.510700427094,
      "learning_rate": 1.4322826333370736e-07,
      "loss": 1.6671,
      "step": 487936
    },
    {
      "epoch": 0.0017709327159089283,
      "grad_norm": 20338.35627576624,
      "learning_rate": 1.432235623929376e-07,
      "loss": 1.6835,
      "step": 487968
    },
    {
      "epoch": 0.001771048850259765,
      "grad_norm": 18530.293899450164,
      "learning_rate": 1.4321886191501073e-07,
      "loss": 1.6686,
      "step": 488000
    },
    {
      "epoch": 0.0017711649846106016,
      "grad_norm": 17572.44160610585,
      "learning_rate": 1.432141618998508e-07,
      "loss": 1.64,
      "step": 488032
    },
    {
      "epoch": 0.0017712811189614383,
      "grad_norm": 22573.214569484782,
      "learning_rate": 1.4320946234738187e-07,
      "loss": 1.6414,
      "step": 488064
    },
    {
      "epoch": 0.001771397253312275,
      "grad_norm": 17441.11923014117,
      "learning_rate": 1.4320476325752804e-07,
      "loss": 1.6513,
      "step": 488096
    },
    {
      "epoch": 0.0017715133876631118,
      "grad_norm": 16743.72598915785,
      "learning_rate": 1.4320006463021335e-07,
      "loss": 1.6597,
      "step": 488128
    },
    {
      "epoch": 0.0017716295220139484,
      "grad_norm": 20398.672113645043,
      "learning_rate": 1.4319536646536203e-07,
      "loss": 1.6755,
      "step": 488160
    },
    {
      "epoch": 0.0017717456563647851,
      "grad_norm": 16545.887706617617,
      "learning_rate": 1.4319066876289816e-07,
      "loss": 1.6768,
      "step": 488192
    },
    {
      "epoch": 0.001771861790715622,
      "grad_norm": 28290.89295161961,
      "learning_rate": 1.4318597152274588e-07,
      "loss": 1.6721,
      "step": 488224
    },
    {
      "epoch": 0.0017719779250664586,
      "grad_norm": 25578.47126002647,
      "learning_rate": 1.4318127474482943e-07,
      "loss": 1.6606,
      "step": 488256
    },
    {
      "epoch": 0.0017720940594172954,
      "grad_norm": 10301.613077571881,
      "learning_rate": 1.4317672518194553e-07,
      "loss": 1.665,
      "step": 488288
    },
    {
      "epoch": 0.001772210193768132,
      "grad_norm": 8486.388159871076,
      "learning_rate": 1.4317202931383426e-07,
      "loss": 1.679,
      "step": 488320
    },
    {
      "epoch": 0.0017723263281189687,
      "grad_norm": 19195.71889771258,
      "learning_rate": 1.431673339077338e-07,
      "loss": 1.6719,
      "step": 488352
    },
    {
      "epoch": 0.0017724424624698054,
      "grad_norm": 17956.704708826728,
      "learning_rate": 1.4316263896356838e-07,
      "loss": 1.6561,
      "step": 488384
    },
    {
      "epoch": 0.0017725585968206422,
      "grad_norm": 12919.25562871174,
      "learning_rate": 1.4315794448126228e-07,
      "loss": 1.6691,
      "step": 488416
    },
    {
      "epoch": 0.0017726747311714787,
      "grad_norm": 8128.7540865743995,
      "learning_rate": 1.4315325046073973e-07,
      "loss": 1.6869,
      "step": 488448
    },
    {
      "epoch": 0.0017727908655223155,
      "grad_norm": 9038.771819224114,
      "learning_rate": 1.431485569019251e-07,
      "loss": 1.7122,
      "step": 488480
    },
    {
      "epoch": 0.0017729069998731522,
      "grad_norm": 9405.441191140371,
      "learning_rate": 1.4314386380474264e-07,
      "loss": 1.6855,
      "step": 488512
    },
    {
      "epoch": 0.001773023134223989,
      "grad_norm": 15887.066060163532,
      "learning_rate": 1.4313917116911675e-07,
      "loss": 1.6691,
      "step": 488544
    },
    {
      "epoch": 0.0017731392685748258,
      "grad_norm": 9701.262392080735,
      "learning_rate": 1.431344789949717e-07,
      "loss": 1.6428,
      "step": 488576
    },
    {
      "epoch": 0.0017732554029256623,
      "grad_norm": 9574.5550288251,
      "learning_rate": 1.4312978728223191e-07,
      "loss": 1.6507,
      "step": 488608
    },
    {
      "epoch": 0.001773371537276499,
      "grad_norm": 10120.355230919515,
      "learning_rate": 1.4312509603082177e-07,
      "loss": 1.6604,
      "step": 488640
    },
    {
      "epoch": 0.0017734876716273358,
      "grad_norm": 8525.867580487044,
      "learning_rate": 1.4312040524066563e-07,
      "loss": 1.6784,
      "step": 488672
    },
    {
      "epoch": 0.0017736038059781726,
      "grad_norm": 8024.483784020003,
      "learning_rate": 1.4311571491168797e-07,
      "loss": 1.6758,
      "step": 488704
    },
    {
      "epoch": 0.001773719940329009,
      "grad_norm": 11397.413127547848,
      "learning_rate": 1.4311102504381318e-07,
      "loss": 1.6642,
      "step": 488736
    },
    {
      "epoch": 0.0017738360746798458,
      "grad_norm": 12437.186980985693,
      "learning_rate": 1.4310633563696574e-07,
      "loss": 1.6696,
      "step": 488768
    },
    {
      "epoch": 0.0017739522090306826,
      "grad_norm": 12920.162382880488,
      "learning_rate": 1.4310164669107011e-07,
      "loss": 1.6732,
      "step": 488800
    },
    {
      "epoch": 0.0017740683433815194,
      "grad_norm": 9556.52970486672,
      "learning_rate": 1.430969582060508e-07,
      "loss": 1.6653,
      "step": 488832
    },
    {
      "epoch": 0.0017741844777323561,
      "grad_norm": 17312.672699499635,
      "learning_rate": 1.4309227018183227e-07,
      "loss": 1.6541,
      "step": 488864
    },
    {
      "epoch": 0.0017743006120831926,
      "grad_norm": 12610.497611117495,
      "learning_rate": 1.430875826183391e-07,
      "loss": 1.6421,
      "step": 488896
    },
    {
      "epoch": 0.0017744167464340294,
      "grad_norm": 8811.709595759497,
      "learning_rate": 1.4308289551549578e-07,
      "loss": 1.6436,
      "step": 488928
    },
    {
      "epoch": 0.0017745328807848662,
      "grad_norm": 11742.792683173795,
      "learning_rate": 1.4307820887322688e-07,
      "loss": 1.6649,
      "step": 488960
    },
    {
      "epoch": 0.001774649015135703,
      "grad_norm": 8481.80004480181,
      "learning_rate": 1.4307352269145702e-07,
      "loss": 1.6826,
      "step": 488992
    },
    {
      "epoch": 0.0017747651494865394,
      "grad_norm": 9057.949436820676,
      "learning_rate": 1.430688369701107e-07,
      "loss": 1.6571,
      "step": 489024
    },
    {
      "epoch": 0.0017748812838373762,
      "grad_norm": 9800.101734165824,
      "learning_rate": 1.4306415170911265e-07,
      "loss": 1.6704,
      "step": 489056
    },
    {
      "epoch": 0.001774997418188213,
      "grad_norm": 9466.44030245794,
      "learning_rate": 1.4305946690838742e-07,
      "loss": 1.6664,
      "step": 489088
    },
    {
      "epoch": 0.0017751135525390497,
      "grad_norm": 14552.82680443906,
      "learning_rate": 1.4305478256785963e-07,
      "loss": 1.6523,
      "step": 489120
    },
    {
      "epoch": 0.0017752296868898865,
      "grad_norm": 14142.813440047918,
      "learning_rate": 1.4305009868745402e-07,
      "loss": 1.6714,
      "step": 489152
    },
    {
      "epoch": 0.001775345821240723,
      "grad_norm": 10411.07179881111,
      "learning_rate": 1.4304541526709518e-07,
      "loss": 1.6803,
      "step": 489184
    },
    {
      "epoch": 0.0017754619555915598,
      "grad_norm": 9711.46013738408,
      "learning_rate": 1.430407323067079e-07,
      "loss": 1.7037,
      "step": 489216
    },
    {
      "epoch": 0.0017755780899423965,
      "grad_norm": 8980.899732209462,
      "learning_rate": 1.4303604980621682e-07,
      "loss": 1.6902,
      "step": 489248
    },
    {
      "epoch": 0.0017756942242932333,
      "grad_norm": 13811.099159733812,
      "learning_rate": 1.4303136776554672e-07,
      "loss": 1.6765,
      "step": 489280
    },
    {
      "epoch": 0.0017758103586440698,
      "grad_norm": 20305.153631529116,
      "learning_rate": 1.430266861846223e-07,
      "loss": 1.6779,
      "step": 489312
    },
    {
      "epoch": 0.0017759264929949066,
      "grad_norm": 24950.807602159894,
      "learning_rate": 1.4302200506336835e-07,
      "loss": 1.6917,
      "step": 489344
    },
    {
      "epoch": 0.0017760426273457433,
      "grad_norm": 19545.71625702164,
      "learning_rate": 1.4301732440170966e-07,
      "loss": 1.6772,
      "step": 489376
    },
    {
      "epoch": 0.00177615876169658,
      "grad_norm": 22117.651954943136,
      "learning_rate": 1.43012644199571e-07,
      "loss": 1.6602,
      "step": 489408
    },
    {
      "epoch": 0.0017762748960474168,
      "grad_norm": 11017.561254651593,
      "learning_rate": 1.4300811069188267e-07,
      "loss": 1.6415,
      "step": 489440
    },
    {
      "epoch": 0.0017763910303982534,
      "grad_norm": 10280.366335885117,
      "learning_rate": 1.4300343139420442e-07,
      "loss": 1.6609,
      "step": 489472
    },
    {
      "epoch": 0.00177650716474909,
      "grad_norm": 10180.706851687657,
      "learning_rate": 1.429987525558231e-07,
      "loss": 1.6628,
      "step": 489504
    },
    {
      "epoch": 0.0017766232990999269,
      "grad_norm": 8423.89565462441,
      "learning_rate": 1.4299407417666354e-07,
      "loss": 1.6464,
      "step": 489536
    },
    {
      "epoch": 0.0017767394334507636,
      "grad_norm": 10848.392138930083,
      "learning_rate": 1.429893962566506e-07,
      "loss": 1.6649,
      "step": 489568
    },
    {
      "epoch": 0.0017768555678016002,
      "grad_norm": 13616.544054935524,
      "learning_rate": 1.4298471879570925e-07,
      "loss": 1.6694,
      "step": 489600
    },
    {
      "epoch": 0.001776971702152437,
      "grad_norm": 10779.183920872674,
      "learning_rate": 1.4298004179376436e-07,
      "loss": 1.6608,
      "step": 489632
    },
    {
      "epoch": 0.0017770878365032737,
      "grad_norm": 9957.03198749507,
      "learning_rate": 1.4297536525074089e-07,
      "loss": 1.6728,
      "step": 489664
    },
    {
      "epoch": 0.0017772039708541104,
      "grad_norm": 8985.188924001543,
      "learning_rate": 1.4297068916656376e-07,
      "loss": 1.6825,
      "step": 489696
    },
    {
      "epoch": 0.0017773201052049472,
      "grad_norm": 8990.670942705,
      "learning_rate": 1.4296601354115797e-07,
      "loss": 1.6611,
      "step": 489728
    },
    {
      "epoch": 0.0017774362395557837,
      "grad_norm": 13103.844321419574,
      "learning_rate": 1.4296133837444853e-07,
      "loss": 1.6676,
      "step": 489760
    },
    {
      "epoch": 0.0017775523739066205,
      "grad_norm": 8937.956142206114,
      "learning_rate": 1.429566636663604e-07,
      "loss": 1.6685,
      "step": 489792
    },
    {
      "epoch": 0.0017776685082574572,
      "grad_norm": 11093.439863270543,
      "learning_rate": 1.429519894168186e-07,
      "loss": 1.6609,
      "step": 489824
    },
    {
      "epoch": 0.001777784642608294,
      "grad_norm": 10907.31424320396,
      "learning_rate": 1.4294731562574823e-07,
      "loss": 1.6653,
      "step": 489856
    },
    {
      "epoch": 0.0017779007769591305,
      "grad_norm": 10224.982542772384,
      "learning_rate": 1.4294264229307428e-07,
      "loss": 1.6472,
      "step": 489888
    },
    {
      "epoch": 0.0017780169113099673,
      "grad_norm": 9390.149732565504,
      "learning_rate": 1.4293796941872187e-07,
      "loss": 1.6543,
      "step": 489920
    },
    {
      "epoch": 0.001778133045660804,
      "grad_norm": 12564.190224602618,
      "learning_rate": 1.4293329700261605e-07,
      "loss": 1.6544,
      "step": 489952
    },
    {
      "epoch": 0.0017782491800116408,
      "grad_norm": 9580.354899480499,
      "learning_rate": 1.4292862504468195e-07,
      "loss": 1.6802,
      "step": 489984
    },
    {
      "epoch": 0.0017783653143624775,
      "grad_norm": 10398.303419308362,
      "learning_rate": 1.429239535448447e-07,
      "loss": 1.6659,
      "step": 490016
    },
    {
      "epoch": 0.001778481448713314,
      "grad_norm": 13004.900768556443,
      "learning_rate": 1.4291928250302947e-07,
      "loss": 1.6491,
      "step": 490048
    },
    {
      "epoch": 0.0017785975830641508,
      "grad_norm": 9271.180075912667,
      "learning_rate": 1.4291461191916135e-07,
      "loss": 1.6597,
      "step": 490080
    },
    {
      "epoch": 0.0017787137174149876,
      "grad_norm": 8206.36435944688,
      "learning_rate": 1.4290994179316554e-07,
      "loss": 1.6779,
      "step": 490112
    },
    {
      "epoch": 0.0017788298517658243,
      "grad_norm": 9296.0773447729,
      "learning_rate": 1.429052721249673e-07,
      "loss": 1.6857,
      "step": 490144
    },
    {
      "epoch": 0.0017789459861166609,
      "grad_norm": 11612.619514993161,
      "learning_rate": 1.4290060291449177e-07,
      "loss": 1.6883,
      "step": 490176
    },
    {
      "epoch": 0.0017790621204674976,
      "grad_norm": 8189.395948420128,
      "learning_rate": 1.428959341616642e-07,
      "loss": 1.6978,
      "step": 490208
    },
    {
      "epoch": 0.0017791782548183344,
      "grad_norm": 8595.718236424458,
      "learning_rate": 1.428912658664098e-07,
      "loss": 1.6865,
      "step": 490240
    },
    {
      "epoch": 0.0017792943891691711,
      "grad_norm": 10919.377454781934,
      "learning_rate": 1.4288659802865385e-07,
      "loss": 1.6879,
      "step": 490272
    },
    {
      "epoch": 0.0017794105235200079,
      "grad_norm": 9126.678694903201,
      "learning_rate": 1.428819306483217e-07,
      "loss": 1.6803,
      "step": 490304
    },
    {
      "epoch": 0.0017795266578708444,
      "grad_norm": 8556.66336839308,
      "learning_rate": 1.4287726372533855e-07,
      "loss": 1.6646,
      "step": 490336
    },
    {
      "epoch": 0.0017796427922216812,
      "grad_norm": 11730.696143025783,
      "learning_rate": 1.4287259725962974e-07,
      "loss": 1.6665,
      "step": 490368
    },
    {
      "epoch": 0.001779758926572518,
      "grad_norm": 11668.522957084157,
      "learning_rate": 1.4286793125112065e-07,
      "loss": 1.6381,
      "step": 490400
    },
    {
      "epoch": 0.0017798750609233547,
      "grad_norm": 18326.27970975015,
      "learning_rate": 1.4286326569973657e-07,
      "loss": 1.6464,
      "step": 490432
    },
    {
      "epoch": 0.0017799911952741912,
      "grad_norm": 23245.28614579739,
      "learning_rate": 1.428586006054029e-07,
      "loss": 1.6526,
      "step": 490464
    },
    {
      "epoch": 0.001780107329625028,
      "grad_norm": 19564.118993708867,
      "learning_rate": 1.42853935968045e-07,
      "loss": 1.6725,
      "step": 490496
    },
    {
      "epoch": 0.0017802234639758647,
      "grad_norm": 21900.936235695495,
      "learning_rate": 1.4284927178758827e-07,
      "loss": 1.661,
      "step": 490528
    },
    {
      "epoch": 0.0017803395983267015,
      "grad_norm": 17300.20277337812,
      "learning_rate": 1.4284460806395812e-07,
      "loss": 1.6665,
      "step": 490560
    },
    {
      "epoch": 0.0017804557326775382,
      "grad_norm": 26538.309667346937,
      "learning_rate": 1.4283994479708e-07,
      "loss": 1.6583,
      "step": 490592
    },
    {
      "epoch": 0.0017805718670283748,
      "grad_norm": 16092.16331013329,
      "learning_rate": 1.4283528198687937e-07,
      "loss": 1.6671,
      "step": 490624
    },
    {
      "epoch": 0.0017806880013792115,
      "grad_norm": 30133.404719679453,
      "learning_rate": 1.4283061963328168e-07,
      "loss": 1.6907,
      "step": 490656
    },
    {
      "epoch": 0.0017808041357300483,
      "grad_norm": 26468.21157539738,
      "learning_rate": 1.4282595773621238e-07,
      "loss": 1.697,
      "step": 490688
    },
    {
      "epoch": 0.001780920270080885,
      "grad_norm": 10589.28288412393,
      "learning_rate": 1.428214419587078e-07,
      "loss": 1.6941,
      "step": 490720
    },
    {
      "epoch": 0.0017810364044317216,
      "grad_norm": 8885.178445028552,
      "learning_rate": 1.4281678096021116e-07,
      "loss": 1.6765,
      "step": 490752
    },
    {
      "epoch": 0.0017811525387825583,
      "grad_norm": 10418.514289475252,
      "learning_rate": 1.4281212041802183e-07,
      "loss": 1.6756,
      "step": 490784
    },
    {
      "epoch": 0.001781268673133395,
      "grad_norm": 11727.238208546802,
      "learning_rate": 1.4280746033206536e-07,
      "loss": 1.6772,
      "step": 490816
    },
    {
      "epoch": 0.0017813848074842318,
      "grad_norm": 12785.523610709106,
      "learning_rate": 1.428028007022673e-07,
      "loss": 1.6916,
      "step": 490848
    },
    {
      "epoch": 0.0017815009418350686,
      "grad_norm": 12417.859396852584,
      "learning_rate": 1.4279814152855326e-07,
      "loss": 1.6702,
      "step": 490880
    },
    {
      "epoch": 0.0017816170761859051,
      "grad_norm": 10218.160989140855,
      "learning_rate": 1.4279348281084884e-07,
      "loss": 1.6616,
      "step": 490912
    },
    {
      "epoch": 0.0017817332105367419,
      "grad_norm": 8729.565281272602,
      "learning_rate": 1.4278882454907962e-07,
      "loss": 1.6614,
      "step": 490944
    },
    {
      "epoch": 0.0017818493448875786,
      "grad_norm": 11067.282954727416,
      "learning_rate": 1.427841667431713e-07,
      "loss": 1.6757,
      "step": 490976
    },
    {
      "epoch": 0.0017819654792384154,
      "grad_norm": 12524.132225427837,
      "learning_rate": 1.4277950939304952e-07,
      "loss": 1.6957,
      "step": 491008
    },
    {
      "epoch": 0.001782081613589252,
      "grad_norm": 9975.510613497436,
      "learning_rate": 1.4277485249863988e-07,
      "loss": 1.6721,
      "step": 491040
    },
    {
      "epoch": 0.0017821977479400887,
      "grad_norm": 10412.1927565715,
      "learning_rate": 1.4277019605986818e-07,
      "loss": 1.6866,
      "step": 491072
    },
    {
      "epoch": 0.0017823138822909254,
      "grad_norm": 9752.081213771757,
      "learning_rate": 1.4276554007666e-07,
      "loss": 1.6907,
      "step": 491104
    },
    {
      "epoch": 0.0017824300166417622,
      "grad_norm": 10367.380961457913,
      "learning_rate": 1.4276088454894116e-07,
      "loss": 1.6939,
      "step": 491136
    },
    {
      "epoch": 0.001782546150992599,
      "grad_norm": 8608.569683751186,
      "learning_rate": 1.4275622947663737e-07,
      "loss": 1.6872,
      "step": 491168
    },
    {
      "epoch": 0.0017826622853434355,
      "grad_norm": 9305.98409626838,
      "learning_rate": 1.4275157485967435e-07,
      "loss": 1.6796,
      "step": 491200
    },
    {
      "epoch": 0.0017827784196942722,
      "grad_norm": 9397.371121755275,
      "learning_rate": 1.4274692069797794e-07,
      "loss": 1.6836,
      "step": 491232
    },
    {
      "epoch": 0.001782894554045109,
      "grad_norm": 14730.348400496168,
      "learning_rate": 1.4274226699147386e-07,
      "loss": 1.6682,
      "step": 491264
    },
    {
      "epoch": 0.0017830106883959457,
      "grad_norm": 11873.244291262603,
      "learning_rate": 1.4273761374008796e-07,
      "loss": 1.6555,
      "step": 491296
    },
    {
      "epoch": 0.0017831268227467823,
      "grad_norm": 11922.19694519429,
      "learning_rate": 1.4273296094374603e-07,
      "loss": 1.6538,
      "step": 491328
    },
    {
      "epoch": 0.001783242957097619,
      "grad_norm": 13028.399287709906,
      "learning_rate": 1.4272830860237398e-07,
      "loss": 1.6679,
      "step": 491360
    },
    {
      "epoch": 0.0017833590914484558,
      "grad_norm": 8677.590103248713,
      "learning_rate": 1.4272365671589755e-07,
      "loss": 1.6631,
      "step": 491392
    },
    {
      "epoch": 0.0017834752257992925,
      "grad_norm": 9021.609501635503,
      "learning_rate": 1.427190052842427e-07,
      "loss": 1.6549,
      "step": 491424
    },
    {
      "epoch": 0.0017835913601501293,
      "grad_norm": 9540.5391881172,
      "learning_rate": 1.4271435430733532e-07,
      "loss": 1.6481,
      "step": 491456
    },
    {
      "epoch": 0.0017837074945009658,
      "grad_norm": 11980.510840527628,
      "learning_rate": 1.4270970378510128e-07,
      "loss": 1.6562,
      "step": 491488
    },
    {
      "epoch": 0.0017838236288518026,
      "grad_norm": 9488.85493618698,
      "learning_rate": 1.4270505371746654e-07,
      "loss": 1.6591,
      "step": 491520
    },
    {
      "epoch": 0.0017839397632026393,
      "grad_norm": 9911.516130239612,
      "learning_rate": 1.4270040410435698e-07,
      "loss": 1.6325,
      "step": 491552
    },
    {
      "epoch": 0.001784055897553476,
      "grad_norm": 10881.122460481733,
      "learning_rate": 1.4269575494569863e-07,
      "loss": 1.6469,
      "step": 491584
    },
    {
      "epoch": 0.0017841720319043126,
      "grad_norm": 13057.871189439726,
      "learning_rate": 1.4269110624141744e-07,
      "loss": 1.6593,
      "step": 491616
    },
    {
      "epoch": 0.0017842881662551494,
      "grad_norm": 7533.673871359178,
      "learning_rate": 1.4268645799143941e-07,
      "loss": 1.652,
      "step": 491648
    },
    {
      "epoch": 0.0017844043006059861,
      "grad_norm": 12540.394890114107,
      "learning_rate": 1.426818101956905e-07,
      "loss": 1.6671,
      "step": 491680
    },
    {
      "epoch": 0.0017845204349568229,
      "grad_norm": 19889.568321107425,
      "learning_rate": 1.4267716285409678e-07,
      "loss": 1.686,
      "step": 491712
    },
    {
      "epoch": 0.0017846365693076596,
      "grad_norm": 9080.036013144441,
      "learning_rate": 1.4267266117494651e-07,
      "loss": 1.689,
      "step": 491744
    },
    {
      "epoch": 0.0017847527036584962,
      "grad_norm": 9403.760099024219,
      "learning_rate": 1.426680147272547e-07,
      "loss": 1.681,
      "step": 491776
    },
    {
      "epoch": 0.001784868838009333,
      "grad_norm": 10451.275137513125,
      "learning_rate": 1.4266336873349855e-07,
      "loss": 1.6637,
      "step": 491808
    },
    {
      "epoch": 0.0017849849723601697,
      "grad_norm": 12273.284645929141,
      "learning_rate": 1.4265872319360418e-07,
      "loss": 1.661,
      "step": 491840
    },
    {
      "epoch": 0.0017851011067110064,
      "grad_norm": 10092.572219211514,
      "learning_rate": 1.4265407810749769e-07,
      "loss": 1.6762,
      "step": 491872
    },
    {
      "epoch": 0.001785217241061843,
      "grad_norm": 15373.91934413603,
      "learning_rate": 1.4264943347510518e-07,
      "loss": 1.6598,
      "step": 491904
    },
    {
      "epoch": 0.0017853333754126797,
      "grad_norm": 10112.522731742065,
      "learning_rate": 1.4264478929635281e-07,
      "loss": 1.676,
      "step": 491936
    },
    {
      "epoch": 0.0017854495097635165,
      "grad_norm": 8823.992973705272,
      "learning_rate": 1.4264014557116674e-07,
      "loss": 1.6666,
      "step": 491968
    },
    {
      "epoch": 0.0017855656441143532,
      "grad_norm": 13457.447454848188,
      "learning_rate": 1.4263550229947316e-07,
      "loss": 1.6918,
      "step": 492000
    },
    {
      "epoch": 0.00178568177846519,
      "grad_norm": 8769.576842698854,
      "learning_rate": 1.4263085948119825e-07,
      "loss": 1.6724,
      "step": 492032
    },
    {
      "epoch": 0.0017857979128160265,
      "grad_norm": 12399.831450467382,
      "learning_rate": 1.4262621711626823e-07,
      "loss": 1.6418,
      "step": 492064
    },
    {
      "epoch": 0.0017859140471668633,
      "grad_norm": 11592.296062471836,
      "learning_rate": 1.4262157520460932e-07,
      "loss": 1.6449,
      "step": 492096
    },
    {
      "epoch": 0.0017860301815177,
      "grad_norm": 15753.393158300849,
      "learning_rate": 1.4261693374614773e-07,
      "loss": 1.6499,
      "step": 492128
    },
    {
      "epoch": 0.0017861463158685368,
      "grad_norm": 11880.754521493995,
      "learning_rate": 1.4261229274080978e-07,
      "loss": 1.6465,
      "step": 492160
    },
    {
      "epoch": 0.0017862624502193733,
      "grad_norm": 10947.056590700535,
      "learning_rate": 1.4260765218852172e-07,
      "loss": 1.6594,
      "step": 492192
    },
    {
      "epoch": 0.00178637858457021,
      "grad_norm": 13255.075254407271,
      "learning_rate": 1.4260301208920984e-07,
      "loss": 1.6764,
      "step": 492224
    },
    {
      "epoch": 0.0017864947189210468,
      "grad_norm": 10289.394734385498,
      "learning_rate": 1.425983724428005e-07,
      "loss": 1.6826,
      "step": 492256
    },
    {
      "epoch": 0.0017866108532718836,
      "grad_norm": 8993.9513007354,
      "learning_rate": 1.4259373324921993e-07,
      "loss": 1.6929,
      "step": 492288
    },
    {
      "epoch": 0.0017867269876227204,
      "grad_norm": 9409.836980522032,
      "learning_rate": 1.4258909450839455e-07,
      "loss": 1.6857,
      "step": 492320
    },
    {
      "epoch": 0.0017868431219735569,
      "grad_norm": 8063.90525737995,
      "learning_rate": 1.425844562202507e-07,
      "loss": 1.6467,
      "step": 492352
    },
    {
      "epoch": 0.0017869592563243936,
      "grad_norm": 12097.902462823877,
      "learning_rate": 1.4257981838471477e-07,
      "loss": 1.6575,
      "step": 492384
    },
    {
      "epoch": 0.0017870753906752304,
      "grad_norm": 9987.146939942359,
      "learning_rate": 1.4257518100171314e-07,
      "loss": 1.6413,
      "step": 492416
    },
    {
      "epoch": 0.0017871915250260671,
      "grad_norm": 10559.760129851435,
      "learning_rate": 1.425705440711722e-07,
      "loss": 1.638,
      "step": 492448
    },
    {
      "epoch": 0.0017873076593769037,
      "grad_norm": 7698.987076232821,
      "learning_rate": 1.4256590759301845e-07,
      "loss": 1.66,
      "step": 492480
    },
    {
      "epoch": 0.0017874237937277404,
      "grad_norm": 9832.522362039153,
      "learning_rate": 1.4256127156717827e-07,
      "loss": 1.6627,
      "step": 492512
    },
    {
      "epoch": 0.0017875399280785772,
      "grad_norm": 10270.164653013117,
      "learning_rate": 1.4255663599357814e-07,
      "loss": 1.6443,
      "step": 492544
    },
    {
      "epoch": 0.001787656062429414,
      "grad_norm": 9963.670408037391,
      "learning_rate": 1.4255200087214455e-07,
      "loss": 1.6423,
      "step": 492576
    },
    {
      "epoch": 0.0017877721967802507,
      "grad_norm": 10226.823553772696,
      "learning_rate": 1.4254736620280398e-07,
      "loss": 1.6466,
      "step": 492608
    },
    {
      "epoch": 0.0017878883311310872,
      "grad_norm": 14919.384169596276,
      "learning_rate": 1.4254273198548296e-07,
      "loss": 1.6569,
      "step": 492640
    },
    {
      "epoch": 0.001788004465481924,
      "grad_norm": 14778.16510937674,
      "learning_rate": 1.42538098220108e-07,
      "loss": 1.6597,
      "step": 492672
    },
    {
      "epoch": 0.0017881205998327607,
      "grad_norm": 10254.95889801612,
      "learning_rate": 1.4253346490660566e-07,
      "loss": 1.6743,
      "step": 492704
    },
    {
      "epoch": 0.0017882367341835975,
      "grad_norm": 12386.375579643951,
      "learning_rate": 1.425288320449025e-07,
      "loss": 1.6808,
      "step": 492736
    },
    {
      "epoch": 0.001788352868534434,
      "grad_norm": 19071.30997073877,
      "learning_rate": 1.4252419963492506e-07,
      "loss": 1.6839,
      "step": 492768
    },
    {
      "epoch": 0.0017884690028852708,
      "grad_norm": 18044.024384820586,
      "learning_rate": 1.4251956767660002e-07,
      "loss": 1.6907,
      "step": 492800
    },
    {
      "epoch": 0.0017885851372361075,
      "grad_norm": 23114.30448877924,
      "learning_rate": 1.4251493616985394e-07,
      "loss": 1.6987,
      "step": 492832
    },
    {
      "epoch": 0.0017887012715869443,
      "grad_norm": 11069.270798024594,
      "learning_rate": 1.425104498282561e-07,
      "loss": 1.7012,
      "step": 492864
    },
    {
      "epoch": 0.001788817405937781,
      "grad_norm": 13448.386371606075,
      "learning_rate": 1.4250581921034168e-07,
      "loss": 1.6744,
      "step": 492896
    },
    {
      "epoch": 0.0017889335402886176,
      "grad_norm": 11757.713553238147,
      "learning_rate": 1.4250118904378846e-07,
      "loss": 1.6568,
      "step": 492928
    },
    {
      "epoch": 0.0017890496746394543,
      "grad_norm": 9741.440345246692,
      "learning_rate": 1.424965593285231e-07,
      "loss": 1.6397,
      "step": 492960
    },
    {
      "epoch": 0.001789165808990291,
      "grad_norm": 9105.149092683765,
      "learning_rate": 1.4249193006447232e-07,
      "loss": 1.658,
      "step": 492992
    },
    {
      "epoch": 0.0017892819433411279,
      "grad_norm": 8315.213647285318,
      "learning_rate": 1.4248730125156283e-07,
      "loss": 1.6795,
      "step": 493024
    },
    {
      "epoch": 0.0017893980776919644,
      "grad_norm": 10092.89690822214,
      "learning_rate": 1.4248267288972134e-07,
      "loss": 1.6443,
      "step": 493056
    },
    {
      "epoch": 0.0017895142120428011,
      "grad_norm": 10700.922203249587,
      "learning_rate": 1.424780449788746e-07,
      "loss": 1.6445,
      "step": 493088
    },
    {
      "epoch": 0.001789630346393638,
      "grad_norm": 12730.780337434151,
      "learning_rate": 1.4247341751894938e-07,
      "loss": 1.6561,
      "step": 493120
    },
    {
      "epoch": 0.0017897464807444747,
      "grad_norm": 8876.3830471651,
      "learning_rate": 1.4246879050987242e-07,
      "loss": 1.6592,
      "step": 493152
    },
    {
      "epoch": 0.0017898626150953114,
      "grad_norm": 13484.283444069248,
      "learning_rate": 1.4246416395157058e-07,
      "loss": 1.6795,
      "step": 493184
    },
    {
      "epoch": 0.001789978749446148,
      "grad_norm": 10666.820894718352,
      "learning_rate": 1.4245953784397067e-07,
      "loss": 1.6746,
      "step": 493216
    },
    {
      "epoch": 0.0017900948837969847,
      "grad_norm": 10331.916182393274,
      "learning_rate": 1.4245491218699945e-07,
      "loss": 1.6793,
      "step": 493248
    },
    {
      "epoch": 0.0017902110181478215,
      "grad_norm": 11073.80991348506,
      "learning_rate": 1.424502869805838e-07,
      "loss": 1.6644,
      "step": 493280
    },
    {
      "epoch": 0.0017903271524986582,
      "grad_norm": 13941.917586903173,
      "learning_rate": 1.424456622246506e-07,
      "loss": 1.6675,
      "step": 493312
    },
    {
      "epoch": 0.0017904432868494947,
      "grad_norm": 9724.6914604012,
      "learning_rate": 1.4244103791912673e-07,
      "loss": 1.6738,
      "step": 493344
    },
    {
      "epoch": 0.0017905594212003315,
      "grad_norm": 7145.572195422841,
      "learning_rate": 1.4243641406393906e-07,
      "loss": 1.692,
      "step": 493376
    },
    {
      "epoch": 0.0017906755555511683,
      "grad_norm": 11175.425361032127,
      "learning_rate": 1.4243179065901452e-07,
      "loss": 1.666,
      "step": 493408
    },
    {
      "epoch": 0.001790791689902005,
      "grad_norm": 9154.3633312208,
      "learning_rate": 1.4242716770428003e-07,
      "loss": 1.6437,
      "step": 493440
    },
    {
      "epoch": 0.0017909078242528418,
      "grad_norm": 10537.705822426435,
      "learning_rate": 1.4242254519966252e-07,
      "loss": 1.6413,
      "step": 493472
    },
    {
      "epoch": 0.0017910239586036783,
      "grad_norm": 11791.434348712628,
      "learning_rate": 1.4241792314508897e-07,
      "loss": 1.6657,
      "step": 493504
    },
    {
      "epoch": 0.001791140092954515,
      "grad_norm": 10858.035365571435,
      "learning_rate": 1.4241330154048638e-07,
      "loss": 1.6645,
      "step": 493536
    },
    {
      "epoch": 0.0017912562273053518,
      "grad_norm": 11667.102725184175,
      "learning_rate": 1.4240868038578168e-07,
      "loss": 1.6549,
      "step": 493568
    },
    {
      "epoch": 0.0017913723616561886,
      "grad_norm": 15936.043925642274,
      "learning_rate": 1.4240405968090195e-07,
      "loss": 1.651,
      "step": 493600
    },
    {
      "epoch": 0.001791488496007025,
      "grad_norm": 8681.902095739159,
      "learning_rate": 1.4239943942577417e-07,
      "loss": 1.6594,
      "step": 493632
    },
    {
      "epoch": 0.0017916046303578619,
      "grad_norm": 12018.650340200433,
      "learning_rate": 1.423948196203254e-07,
      "loss": 1.6623,
      "step": 493664
    },
    {
      "epoch": 0.0017917207647086986,
      "grad_norm": 9371.78808979375,
      "learning_rate": 1.4239020026448273e-07,
      "loss": 1.6788,
      "step": 493696
    },
    {
      "epoch": 0.0017918368990595354,
      "grad_norm": 11423.240345891354,
      "learning_rate": 1.423855813581732e-07,
      "loss": 1.6949,
      "step": 493728
    },
    {
      "epoch": 0.0017919530334103721,
      "grad_norm": 10423.069413565278,
      "learning_rate": 1.4238096290132393e-07,
      "loss": 1.6922,
      "step": 493760
    },
    {
      "epoch": 0.0017920691677612087,
      "grad_norm": 8594.655897707597,
      "learning_rate": 1.42376344893862e-07,
      "loss": 1.6869,
      "step": 493792
    },
    {
      "epoch": 0.0017921853021120454,
      "grad_norm": 10838.132865027997,
      "learning_rate": 1.4237172733571454e-07,
      "loss": 1.6612,
      "step": 493824
    },
    {
      "epoch": 0.0017923014364628822,
      "grad_norm": 16361.075514769804,
      "learning_rate": 1.4236711022680874e-07,
      "loss": 1.6689,
      "step": 493856
    },
    {
      "epoch": 0.001792417570813719,
      "grad_norm": 12056.882184047416,
      "learning_rate": 1.4236263783089027e-07,
      "loss": 1.6895,
      "step": 493888
    },
    {
      "epoch": 0.0017925337051645555,
      "grad_norm": 8909.128801403647,
      "learning_rate": 1.4235802160621606e-07,
      "loss": 1.6622,
      "step": 493920
    },
    {
      "epoch": 0.0017926498395153922,
      "grad_norm": 11334.765634983372,
      "learning_rate": 1.4235340583056728e-07,
      "loss": 1.6613,
      "step": 493952
    },
    {
      "epoch": 0.001792765973866229,
      "grad_norm": 8661.48924839141,
      "learning_rate": 1.4234879050387116e-07,
      "loss": 1.6472,
      "step": 493984
    },
    {
      "epoch": 0.0017928821082170657,
      "grad_norm": 9989.910910513667,
      "learning_rate": 1.423441756260549e-07,
      "loss": 1.6835,
      "step": 494016
    },
    {
      "epoch": 0.0017929982425679025,
      "grad_norm": 8215.0471696759,
      "learning_rate": 1.4233956119704574e-07,
      "loss": 1.6673,
      "step": 494048
    },
    {
      "epoch": 0.001793114376918739,
      "grad_norm": 9207.392030319987,
      "learning_rate": 1.4233494721677095e-07,
      "loss": 1.6643,
      "step": 494080
    },
    {
      "epoch": 0.0017932305112695758,
      "grad_norm": 7813.891348105629,
      "learning_rate": 1.4233033368515783e-07,
      "loss": 1.6574,
      "step": 494112
    },
    {
      "epoch": 0.0017933466456204125,
      "grad_norm": 8881.502800765194,
      "learning_rate": 1.423257206021336e-07,
      "loss": 1.6664,
      "step": 494144
    },
    {
      "epoch": 0.0017934627799712493,
      "grad_norm": 10008.131493940315,
      "learning_rate": 1.4232110796762568e-07,
      "loss": 1.6597,
      "step": 494176
    },
    {
      "epoch": 0.0017935789143220858,
      "grad_norm": 8425.571909372087,
      "learning_rate": 1.423164957815613e-07,
      "loss": 1.6572,
      "step": 494208
    },
    {
      "epoch": 0.0017936950486729226,
      "grad_norm": 11239.233247868824,
      "learning_rate": 1.4231188404386782e-07,
      "loss": 1.6747,
      "step": 494240
    },
    {
      "epoch": 0.0017938111830237593,
      "grad_norm": 11481.165968663636,
      "learning_rate": 1.4230727275447261e-07,
      "loss": 1.6623,
      "step": 494272
    },
    {
      "epoch": 0.001793927317374596,
      "grad_norm": 10377.415285127603,
      "learning_rate": 1.4230266191330305e-07,
      "loss": 1.6681,
      "step": 494304
    },
    {
      "epoch": 0.0017940434517254328,
      "grad_norm": 10803.203043542226,
      "learning_rate": 1.4229805152028654e-07,
      "loss": 1.6608,
      "step": 494336
    },
    {
      "epoch": 0.0017941595860762694,
      "grad_norm": 10674.474319609373,
      "learning_rate": 1.4229344157535047e-07,
      "loss": 1.6688,
      "step": 494368
    },
    {
      "epoch": 0.0017942757204271061,
      "grad_norm": 13993.998284979172,
      "learning_rate": 1.4228883207842224e-07,
      "loss": 1.6625,
      "step": 494400
    },
    {
      "epoch": 0.0017943918547779429,
      "grad_norm": 10245.884442057699,
      "learning_rate": 1.4228422302942933e-07,
      "loss": 1.6589,
      "step": 494432
    },
    {
      "epoch": 0.0017945079891287796,
      "grad_norm": 17360.92566656513,
      "learning_rate": 1.422796144282992e-07,
      "loss": 1.6699,
      "step": 494464
    },
    {
      "epoch": 0.0017946241234796162,
      "grad_norm": 10015.001547678363,
      "learning_rate": 1.4227500627495927e-07,
      "loss": 1.6757,
      "step": 494496
    },
    {
      "epoch": 0.001794740257830453,
      "grad_norm": 12201.242559673994,
      "learning_rate": 1.4227039856933706e-07,
      "loss": 1.6781,
      "step": 494528
    },
    {
      "epoch": 0.0017948563921812897,
      "grad_norm": 11437.843852754766,
      "learning_rate": 1.4226579131136012e-07,
      "loss": 1.662,
      "step": 494560
    },
    {
      "epoch": 0.0017949725265321264,
      "grad_norm": 8962.585564445117,
      "learning_rate": 1.4226118450095592e-07,
      "loss": 1.6647,
      "step": 494592
    },
    {
      "epoch": 0.0017950886608829632,
      "grad_norm": 8404.504863464594,
      "learning_rate": 1.42256578138052e-07,
      "loss": 1.6775,
      "step": 494624
    },
    {
      "epoch": 0.0017952047952337997,
      "grad_norm": 8918.406584138223,
      "learning_rate": 1.4225197222257596e-07,
      "loss": 1.6737,
      "step": 494656
    },
    {
      "epoch": 0.0017953209295846365,
      "grad_norm": 12278.345735480818,
      "learning_rate": 1.422473667544553e-07,
      "loss": 1.6636,
      "step": 494688
    },
    {
      "epoch": 0.0017954370639354732,
      "grad_norm": 8863.279979781752,
      "learning_rate": 1.422427617336177e-07,
      "loss": 1.6694,
      "step": 494720
    },
    {
      "epoch": 0.00179555319828631,
      "grad_norm": 9521.681469152389,
      "learning_rate": 1.4223815715999067e-07,
      "loss": 1.6751,
      "step": 494752
    },
    {
      "epoch": 0.0017956693326371465,
      "grad_norm": 9446.340878880033,
      "learning_rate": 1.4223355303350187e-07,
      "loss": 1.6624,
      "step": 494784
    },
    {
      "epoch": 0.0017957854669879833,
      "grad_norm": 14448.167496260556,
      "learning_rate": 1.4222894935407893e-07,
      "loss": 1.6663,
      "step": 494816
    },
    {
      "epoch": 0.00179590160133882,
      "grad_norm": 9952.910629559576,
      "learning_rate": 1.4222434612164955e-07,
      "loss": 1.6619,
      "step": 494848
    },
    {
      "epoch": 0.0017960177356896568,
      "grad_norm": 12741.480604702108,
      "learning_rate": 1.4221974333614135e-07,
      "loss": 1.677,
      "step": 494880
    },
    {
      "epoch": 0.0017961338700404935,
      "grad_norm": 23458.34930254045,
      "learning_rate": 1.4221514099748202e-07,
      "loss": 1.6762,
      "step": 494912
    },
    {
      "epoch": 0.00179625000439133,
      "grad_norm": 24690.75810905773,
      "learning_rate": 1.4221053910559928e-07,
      "loss": 1.6777,
      "step": 494944
    },
    {
      "epoch": 0.0017963661387421668,
      "grad_norm": 14384.660301863232,
      "learning_rate": 1.4220608144882177e-07,
      "loss": 1.6616,
      "step": 494976
    },
    {
      "epoch": 0.0017964822730930036,
      "grad_norm": 9224.354503161725,
      "learning_rate": 1.4220148043631918e-07,
      "loss": 1.68,
      "step": 495008
    },
    {
      "epoch": 0.0017965984074438403,
      "grad_norm": 10536.328962214497,
      "learning_rate": 1.4219687987037868e-07,
      "loss": 1.672,
      "step": 495040
    },
    {
      "epoch": 0.0017967145417946769,
      "grad_norm": 7836.709641169564,
      "learning_rate": 1.42192279750928e-07,
      "loss": 1.6361,
      "step": 495072
    },
    {
      "epoch": 0.0017968306761455136,
      "grad_norm": 9681.32666528715,
      "learning_rate": 1.421876800778949e-07,
      "loss": 1.6434,
      "step": 495104
    },
    {
      "epoch": 0.0017969468104963504,
      "grad_norm": 11432.537426135983,
      "learning_rate": 1.421830808512072e-07,
      "loss": 1.6505,
      "step": 495136
    },
    {
      "epoch": 0.0017970629448471871,
      "grad_norm": 7736.028438417222,
      "learning_rate": 1.4217848207079273e-07,
      "loss": 1.6456,
      "step": 495168
    },
    {
      "epoch": 0.0017971790791980239,
      "grad_norm": 8372.795112744609,
      "learning_rate": 1.421738837365793e-07,
      "loss": 1.6761,
      "step": 495200
    },
    {
      "epoch": 0.0017972952135488604,
      "grad_norm": 8302.568879569744,
      "learning_rate": 1.4216928584849478e-07,
      "loss": 1.6755,
      "step": 495232
    },
    {
      "epoch": 0.0017974113478996972,
      "grad_norm": 10502.395441041057,
      "learning_rate": 1.42164688406467e-07,
      "loss": 1.6666,
      "step": 495264
    },
    {
      "epoch": 0.001797527482250534,
      "grad_norm": 16583.458505390245,
      "learning_rate": 1.421600914104239e-07,
      "loss": 1.6662,
      "step": 495296
    },
    {
      "epoch": 0.0017976436166013707,
      "grad_norm": 11717.3517485821,
      "learning_rate": 1.421554948602933e-07,
      "loss": 1.6634,
      "step": 495328
    },
    {
      "epoch": 0.0017977597509522072,
      "grad_norm": 10159.403328936203,
      "learning_rate": 1.421508987560032e-07,
      "loss": 1.6667,
      "step": 495360
    },
    {
      "epoch": 0.001797875885303044,
      "grad_norm": 9766.858246130123,
      "learning_rate": 1.421463030974815e-07,
      "loss": 1.6835,
      "step": 495392
    },
    {
      "epoch": 0.0017979920196538807,
      "grad_norm": 12786.904081911305,
      "learning_rate": 1.421417078846561e-07,
      "loss": 1.6694,
      "step": 495424
    },
    {
      "epoch": 0.0017981081540047175,
      "grad_norm": 10815.76479034192,
      "learning_rate": 1.4213711311745505e-07,
      "loss": 1.674,
      "step": 495456
    },
    {
      "epoch": 0.0017982242883555542,
      "grad_norm": 12417.399244608348,
      "learning_rate": 1.4213251879580627e-07,
      "loss": 1.6831,
      "step": 495488
    },
    {
      "epoch": 0.0017983404227063908,
      "grad_norm": 8621.876593874444,
      "learning_rate": 1.4212792491963773e-07,
      "loss": 1.7192,
      "step": 495520
    },
    {
      "epoch": 0.0017984565570572275,
      "grad_norm": 8534.053315980631,
      "learning_rate": 1.4212333148887752e-07,
      "loss": 1.7017,
      "step": 495552
    },
    {
      "epoch": 0.0017985726914080643,
      "grad_norm": 8702.050562942048,
      "learning_rate": 1.4211873850345363e-07,
      "loss": 1.6653,
      "step": 495584
    },
    {
      "epoch": 0.001798688825758901,
      "grad_norm": 11675.156187392098,
      "learning_rate": 1.421141459632941e-07,
      "loss": 1.6551,
      "step": 495616
    },
    {
      "epoch": 0.0017988049601097376,
      "grad_norm": 10063.422181345668,
      "learning_rate": 1.4210955386832697e-07,
      "loss": 1.6667,
      "step": 495648
    },
    {
      "epoch": 0.0017989210944605743,
      "grad_norm": 15487.575019995867,
      "learning_rate": 1.4210496221848038e-07,
      "loss": 1.6614,
      "step": 495680
    },
    {
      "epoch": 0.001799037228811411,
      "grad_norm": 8225.880986253083,
      "learning_rate": 1.421003710136824e-07,
      "loss": 1.6803,
      "step": 495712
    },
    {
      "epoch": 0.0017991533631622478,
      "grad_norm": 10161.508943065493,
      "learning_rate": 1.420957802538611e-07,
      "loss": 1.6902,
      "step": 495744
    },
    {
      "epoch": 0.0017992694975130846,
      "grad_norm": 9337.755190622636,
      "learning_rate": 1.4209118993894465e-07,
      "loss": 1.688,
      "step": 495776
    },
    {
      "epoch": 0.0017993856318639211,
      "grad_norm": 10918.230351114598,
      "learning_rate": 1.4208660006886118e-07,
      "loss": 1.6901,
      "step": 495808
    },
    {
      "epoch": 0.0017995017662147579,
      "grad_norm": 8141.760743229932,
      "learning_rate": 1.4208201064353884e-07,
      "loss": 1.6825,
      "step": 495840
    },
    {
      "epoch": 0.0017996179005655946,
      "grad_norm": 8617.552552784346,
      "learning_rate": 1.420774216629058e-07,
      "loss": 1.6717,
      "step": 495872
    },
    {
      "epoch": 0.0017997340349164314,
      "grad_norm": 12859.946656187964,
      "learning_rate": 1.4207283312689028e-07,
      "loss": 1.6788,
      "step": 495904
    },
    {
      "epoch": 0.001799850169267268,
      "grad_norm": 8436.607137943547,
      "learning_rate": 1.420682450354205e-07,
      "loss": 1.652,
      "step": 495936
    },
    {
      "epoch": 0.0017999663036181047,
      "grad_norm": 22243.660849779204,
      "learning_rate": 1.4206365738842463e-07,
      "loss": 1.6593,
      "step": 495968
    },
    {
      "epoch": 0.0018000824379689414,
      "grad_norm": 15884.827477816685,
      "learning_rate": 1.4205907018583093e-07,
      "loss": 1.6699,
      "step": 496000
    },
    {
      "epoch": 0.0018001985723197782,
      "grad_norm": 20051.21921480088,
      "learning_rate": 1.420544834275677e-07,
      "loss": 1.6862,
      "step": 496032
    },
    {
      "epoch": 0.001800314706670615,
      "grad_norm": 30652.411454892095,
      "learning_rate": 1.420498971135631e-07,
      "loss": 1.681,
      "step": 496064
    },
    {
      "epoch": 0.0018004308410214515,
      "grad_norm": 24118.63213368453,
      "learning_rate": 1.4204531124374558e-07,
      "loss": 1.6703,
      "step": 496096
    },
    {
      "epoch": 0.0018005469753722882,
      "grad_norm": 26270.027940601813,
      "learning_rate": 1.4204072581804333e-07,
      "loss": 1.6448,
      "step": 496128
    },
    {
      "epoch": 0.001800663109723125,
      "grad_norm": 23879.240858955294,
      "learning_rate": 1.4203614083638468e-07,
      "loss": 1.6451,
      "step": 496160
    },
    {
      "epoch": 0.0018007792440739617,
      "grad_norm": 18648.951927655344,
      "learning_rate": 1.42031556298698e-07,
      "loss": 1.6568,
      "step": 496192
    },
    {
      "epoch": 0.0018008953784247983,
      "grad_norm": 16446.32116918553,
      "learning_rate": 1.4202697220491163e-07,
      "loss": 1.6633,
      "step": 496224
    },
    {
      "epoch": 0.001801011512775635,
      "grad_norm": 18520.36219948195,
      "learning_rate": 1.4202238855495397e-07,
      "loss": 1.6841,
      "step": 496256
    },
    {
      "epoch": 0.0018011276471264718,
      "grad_norm": 16707.451032398687,
      "learning_rate": 1.4201780534875335e-07,
      "loss": 1.6766,
      "step": 496288
    },
    {
      "epoch": 0.0018012437814773085,
      "grad_norm": 17162.103833737867,
      "learning_rate": 1.4201336579085157e-07,
      "loss": 1.6769,
      "step": 496320
    },
    {
      "epoch": 0.0018013599158281453,
      "grad_norm": 8724.498495615664,
      "learning_rate": 1.4200878345808846e-07,
      "loss": 1.6785,
      "step": 496352
    },
    {
      "epoch": 0.0018014760501789818,
      "grad_norm": 10301.710149290748,
      "learning_rate": 1.4200420156886991e-07,
      "loss": 1.6936,
      "step": 496384
    },
    {
      "epoch": 0.0018015921845298186,
      "grad_norm": 8745.917676264738,
      "learning_rate": 1.419996201231244e-07,
      "loss": 1.6678,
      "step": 496416
    },
    {
      "epoch": 0.0018017083188806553,
      "grad_norm": 11189.077620608412,
      "learning_rate": 1.419950391207803e-07,
      "loss": 1.6458,
      "step": 496448
    },
    {
      "epoch": 0.001801824453231492,
      "grad_norm": 12884.027475909852,
      "learning_rate": 1.419904585617662e-07,
      "loss": 1.6412,
      "step": 496480
    },
    {
      "epoch": 0.0018019405875823286,
      "grad_norm": 7957.317387160072,
      "learning_rate": 1.4198587844601056e-07,
      "loss": 1.6619,
      "step": 496512
    },
    {
      "epoch": 0.0018020567219331654,
      "grad_norm": 8112.0468440462055,
      "learning_rate": 1.4198129877344188e-07,
      "loss": 1.6744,
      "step": 496544
    },
    {
      "epoch": 0.0018021728562840021,
      "grad_norm": 7712.487925436254,
      "learning_rate": 1.4197671954398871e-07,
      "loss": 1.6517,
      "step": 496576
    },
    {
      "epoch": 0.001802288990634839,
      "grad_norm": 11634.318114956286,
      "learning_rate": 1.4197214075757957e-07,
      "loss": 1.6627,
      "step": 496608
    },
    {
      "epoch": 0.0018024051249856757,
      "grad_norm": 9931.45346865201,
      "learning_rate": 1.4196756241414307e-07,
      "loss": 1.6752,
      "step": 496640
    },
    {
      "epoch": 0.0018025212593365122,
      "grad_norm": 11134.592314045449,
      "learning_rate": 1.4196298451360778e-07,
      "loss": 1.6663,
      "step": 496672
    },
    {
      "epoch": 0.001802637393687349,
      "grad_norm": 17938.442072822265,
      "learning_rate": 1.4195840705590222e-07,
      "loss": 1.6668,
      "step": 496704
    },
    {
      "epoch": 0.0018027535280381857,
      "grad_norm": 10780.042578765633,
      "learning_rate": 1.419538300409551e-07,
      "loss": 1.6666,
      "step": 496736
    },
    {
      "epoch": 0.0018028696623890225,
      "grad_norm": 8990.87726531733,
      "learning_rate": 1.41949253468695e-07,
      "loss": 1.6632,
      "step": 496768
    },
    {
      "epoch": 0.001802985796739859,
      "grad_norm": 10168.269764320772,
      "learning_rate": 1.4194467733905054e-07,
      "loss": 1.6653,
      "step": 496800
    },
    {
      "epoch": 0.0018031019310906957,
      "grad_norm": 12739.443001952637,
      "learning_rate": 1.4194010165195044e-07,
      "loss": 1.6631,
      "step": 496832
    },
    {
      "epoch": 0.0018032180654415325,
      "grad_norm": 11175.046308628882,
      "learning_rate": 1.4193552640732333e-07,
      "loss": 1.6547,
      "step": 496864
    },
    {
      "epoch": 0.0018033341997923693,
      "grad_norm": 11829.759929939406,
      "learning_rate": 1.4193095160509795e-07,
      "loss": 1.6614,
      "step": 496896
    },
    {
      "epoch": 0.001803450334143206,
      "grad_norm": 8245.584394086329,
      "learning_rate": 1.4192637724520295e-07,
      "loss": 1.6525,
      "step": 496928
    },
    {
      "epoch": 0.0018035664684940425,
      "grad_norm": 9049.484626209385,
      "learning_rate": 1.419218033275671e-07,
      "loss": 1.6431,
      "step": 496960
    },
    {
      "epoch": 0.0018036826028448793,
      "grad_norm": 11035.257314625698,
      "learning_rate": 1.4191722985211909e-07,
      "loss": 1.6377,
      "step": 496992
    },
    {
      "epoch": 0.001803798737195716,
      "grad_norm": 13024.382518952672,
      "learning_rate": 1.4191265681878774e-07,
      "loss": 1.6626,
      "step": 497024
    },
    {
      "epoch": 0.0018039148715465528,
      "grad_norm": 10277.133647082732,
      "learning_rate": 1.4190808422750176e-07,
      "loss": 1.6638,
      "step": 497056
    },
    {
      "epoch": 0.0018040310058973893,
      "grad_norm": 9683.574133552136,
      "learning_rate": 1.4190351207818998e-07,
      "loss": 1.6486,
      "step": 497088
    },
    {
      "epoch": 0.001804147140248226,
      "grad_norm": 8110.8332494263495,
      "learning_rate": 1.418989403707812e-07,
      "loss": 1.655,
      "step": 497120
    },
    {
      "epoch": 0.0018042632745990628,
      "grad_norm": 10656.839869304597,
      "learning_rate": 1.4189436910520422e-07,
      "loss": 1.6787,
      "step": 497152
    },
    {
      "epoch": 0.0018043794089498996,
      "grad_norm": 12321.260325145313,
      "learning_rate": 1.418897982813879e-07,
      "loss": 1.684,
      "step": 497184
    },
    {
      "epoch": 0.0018044955433007364,
      "grad_norm": 9498.95889032056,
      "learning_rate": 1.4188522789926108e-07,
      "loss": 1.6825,
      "step": 497216
    },
    {
      "epoch": 0.001804611677651573,
      "grad_norm": 8525.454357393512,
      "learning_rate": 1.4188065795875262e-07,
      "loss": 1.6938,
      "step": 497248
    },
    {
      "epoch": 0.0018047278120024096,
      "grad_norm": 8844.696489987658,
      "learning_rate": 1.4187608845979145e-07,
      "loss": 1.6841,
      "step": 497280
    },
    {
      "epoch": 0.0018048439463532464,
      "grad_norm": 11179.513764023908,
      "learning_rate": 1.4187151940230641e-07,
      "loss": 1.6744,
      "step": 497312
    },
    {
      "epoch": 0.0018049600807040832,
      "grad_norm": 22087.3073053281,
      "learning_rate": 1.4186695078622645e-07,
      "loss": 1.6688,
      "step": 497344
    },
    {
      "epoch": 0.0018050762150549197,
      "grad_norm": 22953.441223485424,
      "learning_rate": 1.4186238261148048e-07,
      "loss": 1.6562,
      "step": 497376
    },
    {
      "epoch": 0.0018051923494057564,
      "grad_norm": 17684.336346043638,
      "learning_rate": 1.4185795761299025e-07,
      "loss": 1.6677,
      "step": 497408
    },
    {
      "epoch": 0.0018053084837565932,
      "grad_norm": 10517.796156990304,
      "learning_rate": 1.4185339030691302e-07,
      "loss": 1.637,
      "step": 497440
    },
    {
      "epoch": 0.00180542461810743,
      "grad_norm": 13228.049667278998,
      "learning_rate": 1.4184882344195888e-07,
      "loss": 1.6438,
      "step": 497472
    },
    {
      "epoch": 0.0018055407524582667,
      "grad_norm": 8751.765307639367,
      "learning_rate": 1.4184425701805688e-07,
      "loss": 1.6439,
      "step": 497504
    },
    {
      "epoch": 0.0018056568868091032,
      "grad_norm": 9849.4479033091,
      "learning_rate": 1.4183969103513598e-07,
      "loss": 1.6726,
      "step": 497536
    },
    {
      "epoch": 0.00180577302115994,
      "grad_norm": 11370.199646444209,
      "learning_rate": 1.4183512549312525e-07,
      "loss": 1.6657,
      "step": 497568
    },
    {
      "epoch": 0.0018058891555107768,
      "grad_norm": 10157.242539193401,
      "learning_rate": 1.418305603919537e-07,
      "loss": 1.6505,
      "step": 497600
    },
    {
      "epoch": 0.0018060052898616135,
      "grad_norm": 9292.711014553288,
      "learning_rate": 1.4182599573155046e-07,
      "loss": 1.6485,
      "step": 497632
    },
    {
      "epoch": 0.00180612142421245,
      "grad_norm": 12000.587152302176,
      "learning_rate": 1.4182143151184455e-07,
      "loss": 1.6541,
      "step": 497664
    },
    {
      "epoch": 0.0018062375585632868,
      "grad_norm": 10677.066263726194,
      "learning_rate": 1.4181686773276505e-07,
      "loss": 1.6672,
      "step": 497696
    },
    {
      "epoch": 0.0018063536929141236,
      "grad_norm": 9469.050216362779,
      "learning_rate": 1.418123043942411e-07,
      "loss": 1.6774,
      "step": 497728
    },
    {
      "epoch": 0.0018064698272649603,
      "grad_norm": 10045.991041206438,
      "learning_rate": 1.418077414962018e-07,
      "loss": 1.6793,
      "step": 497760
    },
    {
      "epoch": 0.001806585961615797,
      "grad_norm": 11178.114331138326,
      "learning_rate": 1.4180317903857632e-07,
      "loss": 1.6602,
      "step": 497792
    },
    {
      "epoch": 0.0018067020959666336,
      "grad_norm": 12047.24632436807,
      "learning_rate": 1.417986170212938e-07,
      "loss": 1.6599,
      "step": 497824
    },
    {
      "epoch": 0.0018068182303174704,
      "grad_norm": 8692.873863113395,
      "learning_rate": 1.417940554442834e-07,
      "loss": 1.6562,
      "step": 497856
    },
    {
      "epoch": 0.0018069343646683071,
      "grad_norm": 11953.742761160624,
      "learning_rate": 1.4178949430747433e-07,
      "loss": 1.6715,
      "step": 497888
    },
    {
      "epoch": 0.0018070504990191439,
      "grad_norm": 10330.202514955841,
      "learning_rate": 1.417849336107958e-07,
      "loss": 1.6652,
      "step": 497920
    },
    {
      "epoch": 0.0018071666333699804,
      "grad_norm": 9364.509917769323,
      "learning_rate": 1.4178037335417697e-07,
      "loss": 1.6428,
      "step": 497952
    },
    {
      "epoch": 0.0018072827677208172,
      "grad_norm": 8619.302523986496,
      "learning_rate": 1.4177581353754717e-07,
      "loss": 1.6455,
      "step": 497984
    },
    {
      "epoch": 0.001807398902071654,
      "grad_norm": 10283.960423883398,
      "learning_rate": 1.4177125416083557e-07,
      "loss": 1.6678,
      "step": 498016
    },
    {
      "epoch": 0.0018075150364224907,
      "grad_norm": 9271.31004766856,
      "learning_rate": 1.4176669522397148e-07,
      "loss": 1.6908,
      "step": 498048
    },
    {
      "epoch": 0.0018076311707733274,
      "grad_norm": 10038.698321993743,
      "learning_rate": 1.4176213672688417e-07,
      "loss": 1.6641,
      "step": 498080
    },
    {
      "epoch": 0.001807747305124164,
      "grad_norm": 10715.344138197335,
      "learning_rate": 1.4175757866950294e-07,
      "loss": 1.6718,
      "step": 498112
    },
    {
      "epoch": 0.0018078634394750007,
      "grad_norm": 10020.31356794786,
      "learning_rate": 1.4175302105175714e-07,
      "loss": 1.6722,
      "step": 498144
    },
    {
      "epoch": 0.0018079795738258375,
      "grad_norm": 8825.387583556883,
      "learning_rate": 1.4174846387357604e-07,
      "loss": 1.6819,
      "step": 498176
    },
    {
      "epoch": 0.0018080957081766742,
      "grad_norm": 9302.566312582781,
      "learning_rate": 1.4174390713488903e-07,
      "loss": 1.6661,
      "step": 498208
    },
    {
      "epoch": 0.0018082118425275108,
      "grad_norm": 13580.819857431288,
      "learning_rate": 1.4173935083562545e-07,
      "loss": 1.685,
      "step": 498240
    },
    {
      "epoch": 0.0018083279768783475,
      "grad_norm": 7891.436497875402,
      "learning_rate": 1.4173479497571468e-07,
      "loss": 1.6959,
      "step": 498272
    },
    {
      "epoch": 0.0018084441112291843,
      "grad_norm": 11209.19229918017,
      "learning_rate": 1.4173023955508613e-07,
      "loss": 1.6617,
      "step": 498304
    },
    {
      "epoch": 0.001808560245580021,
      "grad_norm": 9781.303185158918,
      "learning_rate": 1.4172568457366924e-07,
      "loss": 1.6648,
      "step": 498336
    },
    {
      "epoch": 0.0018086763799308578,
      "grad_norm": 9161.79971402999,
      "learning_rate": 1.417211300313934e-07,
      "loss": 1.6541,
      "step": 498368
    },
    {
      "epoch": 0.0018087925142816943,
      "grad_norm": 8414.297475131243,
      "learning_rate": 1.4171657592818799e-07,
      "loss": 1.6738,
      "step": 498400
    },
    {
      "epoch": 0.001808908648632531,
      "grad_norm": 17228.79519873633,
      "learning_rate": 1.417120222639826e-07,
      "loss": 1.6732,
      "step": 498432
    },
    {
      "epoch": 0.0018090247829833678,
      "grad_norm": 29906.043268877947,
      "learning_rate": 1.4170746903870662e-07,
      "loss": 1.6482,
      "step": 498464
    },
    {
      "epoch": 0.0018091409173342046,
      "grad_norm": 8882.45180116391,
      "learning_rate": 1.417030585202229e-07,
      "loss": 1.6439,
      "step": 498496
    },
    {
      "epoch": 0.0018092570516850411,
      "grad_norm": 10791.047956523964,
      "learning_rate": 1.4169850615888317e-07,
      "loss": 1.657,
      "step": 498528
    },
    {
      "epoch": 0.0018093731860358779,
      "grad_norm": 12774.063253327033,
      "learning_rate": 1.416939542362636e-07,
      "loss": 1.6597,
      "step": 498560
    },
    {
      "epoch": 0.0018094893203867146,
      "grad_norm": 11917.938915768951,
      "learning_rate": 1.4168940275229374e-07,
      "loss": 1.6329,
      "step": 498592
    },
    {
      "epoch": 0.0018096054547375514,
      "grad_norm": 9872.789474105077,
      "learning_rate": 1.416848517069031e-07,
      "loss": 1.6416,
      "step": 498624
    },
    {
      "epoch": 0.0018097215890883881,
      "grad_norm": 9774.031307500503,
      "learning_rate": 1.4168030110002127e-07,
      "loss": 1.6461,
      "step": 498656
    },
    {
      "epoch": 0.0018098377234392247,
      "grad_norm": 12284.01302506636,
      "learning_rate": 1.4167575093157782e-07,
      "loss": 1.6481,
      "step": 498688
    },
    {
      "epoch": 0.0018099538577900614,
      "grad_norm": 8824.119672805895,
      "learning_rate": 1.4167120120150237e-07,
      "loss": 1.6631,
      "step": 498720
    },
    {
      "epoch": 0.0018100699921408982,
      "grad_norm": 9888.174148951868,
      "learning_rate": 1.4166665190972453e-07,
      "loss": 1.6786,
      "step": 498752
    },
    {
      "epoch": 0.001810186126491735,
      "grad_norm": 11305.019062345715,
      "learning_rate": 1.4166210305617395e-07,
      "loss": 1.6919,
      "step": 498784
    },
    {
      "epoch": 0.0018103022608425715,
      "grad_norm": 11689.025451251271,
      "learning_rate": 1.4165755464078025e-07,
      "loss": 1.6847,
      "step": 498816
    },
    {
      "epoch": 0.0018104183951934082,
      "grad_norm": 12198.939216177772,
      "learning_rate": 1.416530066634731e-07,
      "loss": 1.674,
      "step": 498848
    },
    {
      "epoch": 0.001810534529544245,
      "grad_norm": 10211.845278890589,
      "learning_rate": 1.4164845912418217e-07,
      "loss": 1.6673,
      "step": 498880
    },
    {
      "epoch": 0.0018106506638950817,
      "grad_norm": 9889.448922968357,
      "learning_rate": 1.416439120228372e-07,
      "loss": 1.6857,
      "step": 498912
    },
    {
      "epoch": 0.0018107667982459185,
      "grad_norm": 9356.596175960573,
      "learning_rate": 1.4163936535936785e-07,
      "loss": 1.6731,
      "step": 498944
    },
    {
      "epoch": 0.001810882932596755,
      "grad_norm": 8568.227354593248,
      "learning_rate": 1.4163481913370385e-07,
      "loss": 1.6848,
      "step": 498976
    },
    {
      "epoch": 0.0018109990669475918,
      "grad_norm": 7324.071135645803,
      "learning_rate": 1.41630273345775e-07,
      "loss": 1.6737,
      "step": 499008
    },
    {
      "epoch": 0.0018111152012984285,
      "grad_norm": 9259.224373563911,
      "learning_rate": 1.4162572799551098e-07,
      "loss": 1.6958,
      "step": 499040
    },
    {
      "epoch": 0.0018112313356492653,
      "grad_norm": 10773.901800183627,
      "learning_rate": 1.4162118308284165e-07,
      "loss": 1.6742,
      "step": 499072
    },
    {
      "epoch": 0.0018113474700001018,
      "grad_norm": 9959.705618139524,
      "learning_rate": 1.4161663860769672e-07,
      "loss": 1.6473,
      "step": 499104
    },
    {
      "epoch": 0.0018114636043509386,
      "grad_norm": 11336.251585069907,
      "learning_rate": 1.4161209457000604e-07,
      "loss": 1.6421,
      "step": 499136
    },
    {
      "epoch": 0.0018115797387017753,
      "grad_norm": 9791.137625424331,
      "learning_rate": 1.4160755096969943e-07,
      "loss": 1.6532,
      "step": 499168
    },
    {
      "epoch": 0.001811695873052612,
      "grad_norm": 11307.842411353282,
      "learning_rate": 1.4160300780670671e-07,
      "loss": 1.6474,
      "step": 499200
    },
    {
      "epoch": 0.0018118120074034488,
      "grad_norm": 8933.524500442141,
      "learning_rate": 1.4159846508095773e-07,
      "loss": 1.666,
      "step": 499232
    },
    {
      "epoch": 0.0018119281417542854,
      "grad_norm": 10737.351256245649,
      "learning_rate": 1.415939227923824e-07,
      "loss": 1.6824,
      "step": 499264
    },
    {
      "epoch": 0.0018120442761051221,
      "grad_norm": 8852.89421601772,
      "learning_rate": 1.4158938094091058e-07,
      "loss": 1.6891,
      "step": 499296
    },
    {
      "epoch": 0.0018121604104559589,
      "grad_norm": 8149.998527607229,
      "learning_rate": 1.4158483952647214e-07,
      "loss": 1.6963,
      "step": 499328
    },
    {
      "epoch": 0.0018122765448067956,
      "grad_norm": 8913.987435485873,
      "learning_rate": 1.4158029854899704e-07,
      "loss": 1.669,
      "step": 499360
    },
    {
      "epoch": 0.0018123926791576322,
      "grad_norm": 9735.931388418881,
      "learning_rate": 1.4157575800841516e-07,
      "loss": 1.6582,
      "step": 499392
    },
    {
      "epoch": 0.001812508813508469,
      "grad_norm": 14107.043347207806,
      "learning_rate": 1.4157121790465653e-07,
      "loss": 1.6665,
      "step": 499424
    },
    {
      "epoch": 0.0018126249478593057,
      "grad_norm": 13441.37552484864,
      "learning_rate": 1.4156667823765104e-07,
      "loss": 1.6376,
      "step": 499456
    },
    {
      "epoch": 0.0018127410822101424,
      "grad_norm": 22105.77987767,
      "learning_rate": 1.4156213900732872e-07,
      "loss": 1.6439,
      "step": 499488
    },
    {
      "epoch": 0.0018128572165609792,
      "grad_norm": 20329.82872529919,
      "learning_rate": 1.4155760021361952e-07,
      "loss": 1.6581,
      "step": 499520
    },
    {
      "epoch": 0.0018129733509118157,
      "grad_norm": 9092.223050497605,
      "learning_rate": 1.4155320367350778e-07,
      "loss": 1.6647,
      "step": 499552
    },
    {
      "epoch": 0.0018130894852626525,
      "grad_norm": 10773.74800150811,
      "learning_rate": 1.415486657391762e-07,
      "loss": 1.6436,
      "step": 499584
    },
    {
      "epoch": 0.0018132056196134892,
      "grad_norm": 8026.569130082915,
      "learning_rate": 1.4154412824125002e-07,
      "loss": 1.6356,
      "step": 499616
    },
    {
      "epoch": 0.001813321753964326,
      "grad_norm": 9837.302272472876,
      "learning_rate": 1.4153959117965927e-07,
      "loss": 1.6411,
      "step": 499648
    },
    {
      "epoch": 0.0018134378883151625,
      "grad_norm": 10816.178807693594,
      "learning_rate": 1.4153505455433408e-07,
      "loss": 1.6622,
      "step": 499680
    },
    {
      "epoch": 0.0018135540226659993,
      "grad_norm": 9387.04383711933,
      "learning_rate": 1.4153051836520453e-07,
      "loss": 1.6592,
      "step": 499712
    },
    {
      "epoch": 0.001813670157016836,
      "grad_norm": 10119.781420564379,
      "learning_rate": 1.415259826122007e-07,
      "loss": 1.6731,
      "step": 499744
    },
    {
      "epoch": 0.0018137862913676728,
      "grad_norm": 9830.55705440948,
      "learning_rate": 1.4152144729525268e-07,
      "loss": 1.6956,
      "step": 499776
    },
    {
      "epoch": 0.0018139024257185095,
      "grad_norm": 8871.065888606623,
      "learning_rate": 1.415169124142907e-07,
      "loss": 1.7016,
      "step": 499808
    },
    {
      "epoch": 0.001814018560069346,
      "grad_norm": 10630.037817430379,
      "learning_rate": 1.4151237796924481e-07,
      "loss": 1.7069,
      "step": 499840
    },
    {
      "epoch": 0.0018141346944201828,
      "grad_norm": 11855.17912137982,
      "learning_rate": 1.4150784396004526e-07,
      "loss": 1.7109,
      "step": 499872
    },
    {
      "epoch": 0.0018142508287710196,
      "grad_norm": 11732.927341460869,
      "learning_rate": 1.4150331038662216e-07,
      "loss": 1.7169,
      "step": 499904
    },
    {
      "epoch": 0.0018143669631218563,
      "grad_norm": 9671.584565106175,
      "learning_rate": 1.4149877724890577e-07,
      "loss": 1.6676,
      "step": 499936
    },
    {
      "epoch": 0.0018144830974726929,
      "grad_norm": 8539.900233609289,
      "learning_rate": 1.4149424454682628e-07,
      "loss": 1.6416,
      "step": 499968
    },
    {
      "epoch": 0.0018145992318235296,
      "grad_norm": 12277.795078921949,
      "learning_rate": 1.414897122803139e-07,
      "loss": 1.6442,
      "step": 500000
    },
    {
      "epoch": 0.0018147153661743664,
      "grad_norm": 8355.936811632793,
      "learning_rate": 1.4148518044929888e-07,
      "loss": 1.6551,
      "step": 500032
    },
    {
      "epoch": 0.0018148315005252031,
      "grad_norm": 8665.008251582914,
      "learning_rate": 1.4148064905371152e-07,
      "loss": 1.6787,
      "step": 500064
    },
    {
      "epoch": 0.00181494763487604,
      "grad_norm": 15497.601362791596,
      "learning_rate": 1.4147611809348207e-07,
      "loss": 1.6303,
      "step": 500096
    },
    {
      "epoch": 0.0018150637692268764,
      "grad_norm": 9837.216882838356,
      "learning_rate": 1.414715875685408e-07,
      "loss": 1.6472,
      "step": 500128
    },
    {
      "epoch": 0.0018151799035777132,
      "grad_norm": 8013.311425372161,
      "learning_rate": 1.4146705747881804e-07,
      "loss": 1.6591,
      "step": 500160
    },
    {
      "epoch": 0.00181529603792855,
      "grad_norm": 15062.682762376695,
      "learning_rate": 1.4146252782424411e-07,
      "loss": 1.6634,
      "step": 500192
    },
    {
      "epoch": 0.0018154121722793867,
      "grad_norm": 11362.0667134109,
      "learning_rate": 1.4145799860474938e-07,
      "loss": 1.6557,
      "step": 500224
    },
    {
      "epoch": 0.0018155283066302232,
      "grad_norm": 8911.968020588944,
      "learning_rate": 1.4145346982026416e-07,
      "loss": 1.6681,
      "step": 500256
    },
    {
      "epoch": 0.00181564444098106,
      "grad_norm": 8493.061285543628,
      "learning_rate": 1.4144894147071882e-07,
      "loss": 1.6741,
      "step": 500288
    },
    {
      "epoch": 0.0018157605753318967,
      "grad_norm": 9898.372795565945,
      "learning_rate": 1.414444135560438e-07,
      "loss": 1.6631,
      "step": 500320
    },
    {
      "epoch": 0.0018158767096827335,
      "grad_norm": 10096.304472429503,
      "learning_rate": 1.4143988607616945e-07,
      "loss": 1.6756,
      "step": 500352
    },
    {
      "epoch": 0.0018159928440335702,
      "grad_norm": 10271.77822969324,
      "learning_rate": 1.4143535903102618e-07,
      "loss": 1.6744,
      "step": 500384
    },
    {
      "epoch": 0.0018161089783844068,
      "grad_norm": 10422.77314345851,
      "learning_rate": 1.4143083242054446e-07,
      "loss": 1.6877,
      "step": 500416
    },
    {
      "epoch": 0.0018162251127352435,
      "grad_norm": 12518.866721872231,
      "learning_rate": 1.414263062446547e-07,
      "loss": 1.6764,
      "step": 500448
    },
    {
      "epoch": 0.0018163412470860803,
      "grad_norm": 12188.656201567095,
      "learning_rate": 1.4142178050328742e-07,
      "loss": 1.6492,
      "step": 500480
    },
    {
      "epoch": 0.001816457381436917,
      "grad_norm": 11200.455883579025,
      "learning_rate": 1.4141725519637305e-07,
      "loss": 1.6595,
      "step": 500512
    },
    {
      "epoch": 0.0018165735157877536,
      "grad_norm": 18576.68474190161,
      "learning_rate": 1.414127303238421e-07,
      "loss": 1.6755,
      "step": 500544
    },
    {
      "epoch": 0.0018166896501385903,
      "grad_norm": 24561.253062496628,
      "learning_rate": 1.4140820588562506e-07,
      "loss": 1.6822,
      "step": 500576
    },
    {
      "epoch": 0.001816805784489427,
      "grad_norm": 16877.367804251942,
      "learning_rate": 1.414036818816525e-07,
      "loss": 1.6639,
      "step": 500608
    },
    {
      "epoch": 0.0018169219188402638,
      "grad_norm": 17272.538666912864,
      "learning_rate": 1.4139915831185492e-07,
      "loss": 1.6665,
      "step": 500640
    },
    {
      "epoch": 0.0018170380531911006,
      "grad_norm": 19258.76963879053,
      "learning_rate": 1.413946351761629e-07,
      "loss": 1.677,
      "step": 500672
    },
    {
      "epoch": 0.0018171541875419371,
      "grad_norm": 24484.277077340877,
      "learning_rate": 1.4139011247450702e-07,
      "loss": 1.6805,
      "step": 500704
    },
    {
      "epoch": 0.0018172703218927739,
      "grad_norm": 13965.630669611737,
      "learning_rate": 1.4138559020681785e-07,
      "loss": 1.6948,
      "step": 500736
    },
    {
      "epoch": 0.0018173864562436106,
      "grad_norm": 22002.977798470827,
      "learning_rate": 1.41381068373026e-07,
      "loss": 1.7109,
      "step": 500768
    },
    {
      "epoch": 0.0018175025905944474,
      "grad_norm": 21092.685746485677,
      "learning_rate": 1.4137654697306208e-07,
      "loss": 1.7053,
      "step": 500800
    },
    {
      "epoch": 0.001817618724945284,
      "grad_norm": 16600.26650388481,
      "learning_rate": 1.4137202600685675e-07,
      "loss": 1.6785,
      "step": 500832
    },
    {
      "epoch": 0.0018177348592961207,
      "grad_norm": 12723.575519483507,
      "learning_rate": 1.413676467344178e-07,
      "loss": 1.6778,
      "step": 500864
    },
    {
      "epoch": 0.0018178509936469574,
      "grad_norm": 12676.555762508993,
      "learning_rate": 1.41363126621972e-07,
      "loss": 1.6835,
      "step": 500896
    },
    {
      "epoch": 0.0018179671279977942,
      "grad_norm": 9123.958461106671,
      "learning_rate": 1.4135860694307893e-07,
      "loss": 1.6933,
      "step": 500928
    },
    {
      "epoch": 0.001818083262348631,
      "grad_norm": 10245.899472471903,
      "learning_rate": 1.413540876976693e-07,
      "loss": 1.6776,
      "step": 500960
    },
    {
      "epoch": 0.0018181993966994675,
      "grad_norm": 11436.304473036735,
      "learning_rate": 1.4134956888567382e-07,
      "loss": 1.6658,
      "step": 500992
    },
    {
      "epoch": 0.0018183155310503042,
      "grad_norm": 9301.343343840179,
      "learning_rate": 1.4134505050702322e-07,
      "loss": 1.6592,
      "step": 501024
    },
    {
      "epoch": 0.001818431665401141,
      "grad_norm": 9148.720347677045,
      "learning_rate": 1.4134053256164822e-07,
      "loss": 1.6727,
      "step": 501056
    },
    {
      "epoch": 0.0018185477997519778,
      "grad_norm": 11066.06163004707,
      "learning_rate": 1.413360150494796e-07,
      "loss": 1.6529,
      "step": 501088
    },
    {
      "epoch": 0.0018186639341028143,
      "grad_norm": 10337.73485827529,
      "learning_rate": 1.4133149797044812e-07,
      "loss": 1.6401,
      "step": 501120
    },
    {
      "epoch": 0.001818780068453651,
      "grad_norm": 10104.501769013652,
      "learning_rate": 1.4132698132448457e-07,
      "loss": 1.6433,
      "step": 501152
    },
    {
      "epoch": 0.0018188962028044878,
      "grad_norm": 10899.519805936407,
      "learning_rate": 1.4132246511151977e-07,
      "loss": 1.6438,
      "step": 501184
    },
    {
      "epoch": 0.0018190123371553246,
      "grad_norm": 12976.760381543614,
      "learning_rate": 1.4131794933148457e-07,
      "loss": 1.637,
      "step": 501216
    },
    {
      "epoch": 0.0018191284715061613,
      "grad_norm": 13268.891136790593,
      "learning_rate": 1.4131343398430972e-07,
      "loss": 1.6589,
      "step": 501248
    },
    {
      "epoch": 0.0018192446058569978,
      "grad_norm": 11795.671748569473,
      "learning_rate": 1.4130891906992613e-07,
      "loss": 1.674,
      "step": 501280
    },
    {
      "epoch": 0.0018193607402078346,
      "grad_norm": 11298.83887839808,
      "learning_rate": 1.4130440458826466e-07,
      "loss": 1.6707,
      "step": 501312
    },
    {
      "epoch": 0.0018194768745586714,
      "grad_norm": 9568.807449207032,
      "learning_rate": 1.412998905392562e-07,
      "loss": 1.6592,
      "step": 501344
    },
    {
      "epoch": 0.001819593008909508,
      "grad_norm": 12363.344369546616,
      "learning_rate": 1.4129537692283163e-07,
      "loss": 1.6563,
      "step": 501376
    },
    {
      "epoch": 0.0018197091432603446,
      "grad_norm": 9573.134282981724,
      "learning_rate": 1.4129086373892187e-07,
      "loss": 1.6639,
      "step": 501408
    },
    {
      "epoch": 0.0018198252776111814,
      "grad_norm": 9826.054243693141,
      "learning_rate": 1.4128635098745784e-07,
      "loss": 1.6745,
      "step": 501440
    },
    {
      "epoch": 0.0018199414119620182,
      "grad_norm": 8210.8159156079,
      "learning_rate": 1.412818386683705e-07,
      "loss": 1.6671,
      "step": 501472
    },
    {
      "epoch": 0.001820057546312855,
      "grad_norm": 13681.49085443542,
      "learning_rate": 1.412773267815908e-07,
      "loss": 1.6777,
      "step": 501504
    },
    {
      "epoch": 0.0018201736806636917,
      "grad_norm": 11117.533719310231,
      "learning_rate": 1.4127281532704973e-07,
      "loss": 1.6692,
      "step": 501536
    },
    {
      "epoch": 0.0018202898150145282,
      "grad_norm": 10234.997997068685,
      "learning_rate": 1.4126830430467824e-07,
      "loss": 1.6882,
      "step": 501568
    },
    {
      "epoch": 0.001820405949365365,
      "grad_norm": 9251.178033093947,
      "learning_rate": 1.412637937144074e-07,
      "loss": 1.6642,
      "step": 501600
    },
    {
      "epoch": 0.0018205220837162017,
      "grad_norm": 10028.651255278548,
      "learning_rate": 1.4125928355616816e-07,
      "loss": 1.6611,
      "step": 501632
    },
    {
      "epoch": 0.0018206382180670385,
      "grad_norm": 7145.056612791811,
      "learning_rate": 1.412547738298916e-07,
      "loss": 1.6766,
      "step": 501664
    },
    {
      "epoch": 0.001820754352417875,
      "grad_norm": 10718.3200176147,
      "learning_rate": 1.4125026453550876e-07,
      "loss": 1.6581,
      "step": 501696
    },
    {
      "epoch": 0.0018208704867687118,
      "grad_norm": 8387.87100520746,
      "learning_rate": 1.4124575567295071e-07,
      "loss": 1.6453,
      "step": 501728
    },
    {
      "epoch": 0.0018209866211195485,
      "grad_norm": 11155.81211745698,
      "learning_rate": 1.4124124724214857e-07,
      "loss": 1.6595,
      "step": 501760
    },
    {
      "epoch": 0.0018211027554703853,
      "grad_norm": 8910.73105867302,
      "learning_rate": 1.4123673924303336e-07,
      "loss": 1.6717,
      "step": 501792
    },
    {
      "epoch": 0.001821218889821222,
      "grad_norm": 9359.849144083466,
      "learning_rate": 1.4123223167553625e-07,
      "loss": 1.6641,
      "step": 501824
    },
    {
      "epoch": 0.0018213350241720586,
      "grad_norm": 19824.987868848748,
      "learning_rate": 1.4122772453958834e-07,
      "loss": 1.6583,
      "step": 501856
    },
    {
      "epoch": 0.0018214511585228953,
      "grad_norm": 10724.319838572514,
      "learning_rate": 1.4122335866310487e-07,
      "loss": 1.6597,
      "step": 501888
    },
    {
      "epoch": 0.001821567292873732,
      "grad_norm": 11864.539771942273,
      "learning_rate": 1.412188523765683e-07,
      "loss": 1.6729,
      "step": 501920
    },
    {
      "epoch": 0.0018216834272245688,
      "grad_norm": 10312.2490272491,
      "learning_rate": 1.4121434652137654e-07,
      "loss": 1.6755,
      "step": 501952
    },
    {
      "epoch": 0.0018217995615754053,
      "grad_norm": 9877.546760203162,
      "learning_rate": 1.412098410974608e-07,
      "loss": 1.6445,
      "step": 501984
    },
    {
      "epoch": 0.001821915695926242,
      "grad_norm": 9470.178456607879,
      "learning_rate": 1.4120533610475235e-07,
      "loss": 1.6572,
      "step": 502016
    },
    {
      "epoch": 0.0018220318302770789,
      "grad_norm": 10926.829366289197,
      "learning_rate": 1.4120083154318234e-07,
      "loss": 1.6702,
      "step": 502048
    },
    {
      "epoch": 0.0018221479646279156,
      "grad_norm": 14562.316985974448,
      "learning_rate": 1.41196327412682e-07,
      "loss": 1.6649,
      "step": 502080
    },
    {
      "epoch": 0.0018222640989787524,
      "grad_norm": 11756.743086416407,
      "learning_rate": 1.4119182371318263e-07,
      "loss": 1.6308,
      "step": 502112
    },
    {
      "epoch": 0.001822380233329589,
      "grad_norm": 8196.834266959411,
      "learning_rate": 1.4118732044461548e-07,
      "loss": 1.6397,
      "step": 502144
    },
    {
      "epoch": 0.0018224963676804257,
      "grad_norm": 13165.044777743826,
      "learning_rate": 1.411828176069118e-07,
      "loss": 1.6436,
      "step": 502176
    },
    {
      "epoch": 0.0018226125020312624,
      "grad_norm": 9657.526805554307,
      "learning_rate": 1.4117831520000293e-07,
      "loss": 1.6465,
      "step": 502208
    },
    {
      "epoch": 0.0018227286363820992,
      "grad_norm": 8253.744483566232,
      "learning_rate": 1.4117381322382017e-07,
      "loss": 1.6568,
      "step": 502240
    },
    {
      "epoch": 0.0018228447707329357,
      "grad_norm": 8675.585513381791,
      "learning_rate": 1.4116931167829483e-07,
      "loss": 1.6664,
      "step": 502272
    },
    {
      "epoch": 0.0018229609050837725,
      "grad_norm": 11567.639603652942,
      "learning_rate": 1.4116481056335828e-07,
      "loss": 1.6677,
      "step": 502304
    },
    {
      "epoch": 0.0018230770394346092,
      "grad_norm": 8874.870590605815,
      "learning_rate": 1.4116030987894185e-07,
      "loss": 1.6633,
      "step": 502336
    },
    {
      "epoch": 0.001823193173785446,
      "grad_norm": 11269.373185763261,
      "learning_rate": 1.4115580962497695e-07,
      "loss": 1.6627,
      "step": 502368
    },
    {
      "epoch": 0.0018233093081362827,
      "grad_norm": 9419.078086522057,
      "learning_rate": 1.4115130980139494e-07,
      "loss": 1.6719,
      "step": 502400
    },
    {
      "epoch": 0.0018234254424871193,
      "grad_norm": 12024.614754743705,
      "learning_rate": 1.4114681040812725e-07,
      "loss": 1.683,
      "step": 502432
    },
    {
      "epoch": 0.001823541576837956,
      "grad_norm": 13891.437866542108,
      "learning_rate": 1.4114231144510528e-07,
      "loss": 1.6717,
      "step": 502464
    },
    {
      "epoch": 0.0018236577111887928,
      "grad_norm": 13133.185752131889,
      "learning_rate": 1.4113781291226048e-07,
      "loss": 1.6718,
      "step": 502496
    },
    {
      "epoch": 0.0018237738455396295,
      "grad_norm": 8241.408010770974,
      "learning_rate": 1.4113331480952426e-07,
      "loss": 1.6751,
      "step": 502528
    },
    {
      "epoch": 0.001823889979890466,
      "grad_norm": 11372.143509470852,
      "learning_rate": 1.4112881713682813e-07,
      "loss": 1.7054,
      "step": 502560
    },
    {
      "epoch": 0.0018240061142413028,
      "grad_norm": 8948.441316788081,
      "learning_rate": 1.4112431989410355e-07,
      "loss": 1.6701,
      "step": 502592
    },
    {
      "epoch": 0.0018241222485921396,
      "grad_norm": 13209.174538933157,
      "learning_rate": 1.4111982308128205e-07,
      "loss": 1.6392,
      "step": 502624
    },
    {
      "epoch": 0.0018242383829429763,
      "grad_norm": 9168.539469293895,
      "learning_rate": 1.4111532669829512e-07,
      "loss": 1.6392,
      "step": 502656
    },
    {
      "epoch": 0.001824354517293813,
      "grad_norm": 12794.084648774213,
      "learning_rate": 1.4111083074507426e-07,
      "loss": 1.6445,
      "step": 502688
    },
    {
      "epoch": 0.0018244706516446496,
      "grad_norm": 10929.43822893016,
      "learning_rate": 1.4110633522155104e-07,
      "loss": 1.6394,
      "step": 502720
    },
    {
      "epoch": 0.0018245867859954864,
      "grad_norm": 8862.572651324219,
      "learning_rate": 1.4110184012765704e-07,
      "loss": 1.6606,
      "step": 502752
    },
    {
      "epoch": 0.0018247029203463231,
      "grad_norm": 11544.538795465152,
      "learning_rate": 1.4109734546332376e-07,
      "loss": 1.6815,
      "step": 502784
    },
    {
      "epoch": 0.0018248190546971599,
      "grad_norm": 13966.516960215957,
      "learning_rate": 1.4109285122848287e-07,
      "loss": 1.6853,
      "step": 502816
    },
    {
      "epoch": 0.0018249351890479964,
      "grad_norm": 10612.200525809903,
      "learning_rate": 1.4108835742306593e-07,
      "loss": 1.6609,
      "step": 502848
    },
    {
      "epoch": 0.0018250513233988332,
      "grad_norm": 16929.277598291075,
      "learning_rate": 1.4108386404700454e-07,
      "loss": 1.658,
      "step": 502880
    },
    {
      "epoch": 0.00182516745774967,
      "grad_norm": 19969.384967995386,
      "learning_rate": 1.410793711002304e-07,
      "loss": 1.6602,
      "step": 502912
    },
    {
      "epoch": 0.0018252835921005067,
      "grad_norm": 16273.747447960473,
      "learning_rate": 1.410748785826751e-07,
      "loss": 1.6647,
      "step": 502944
    },
    {
      "epoch": 0.0018253997264513434,
      "grad_norm": 25034.603252298606,
      "learning_rate": 1.4107038649427033e-07,
      "loss": 1.6469,
      "step": 502976
    },
    {
      "epoch": 0.00182551586080218,
      "grad_norm": 11630.439200649304,
      "learning_rate": 1.4106603519280738e-07,
      "loss": 1.6493,
      "step": 503008
    },
    {
      "epoch": 0.0018256319951530167,
      "grad_norm": 17669.257596175343,
      "learning_rate": 1.4106154394909304e-07,
      "loss": 1.649,
      "step": 503040
    },
    {
      "epoch": 0.0018257481295038535,
      "grad_norm": 9912.487881455392,
      "learning_rate": 1.4105705313432648e-07,
      "loss": 1.68,
      "step": 503072
    },
    {
      "epoch": 0.0018258642638546902,
      "grad_norm": 7735.882625790026,
      "learning_rate": 1.4105256274843938e-07,
      "loss": 1.6777,
      "step": 503104
    },
    {
      "epoch": 0.0018259803982055268,
      "grad_norm": 12873.884883748184,
      "learning_rate": 1.4104807279136346e-07,
      "loss": 1.6587,
      "step": 503136
    },
    {
      "epoch": 0.0018260965325563635,
      "grad_norm": 15677.253394647929,
      "learning_rate": 1.4104358326303052e-07,
      "loss": 1.6501,
      "step": 503168
    },
    {
      "epoch": 0.0018262126669072003,
      "grad_norm": 13968.536143776842,
      "learning_rate": 1.4103909416337232e-07,
      "loss": 1.6518,
      "step": 503200
    },
    {
      "epoch": 0.001826328801258037,
      "grad_norm": 8873.853278029786,
      "learning_rate": 1.4103460549232063e-07,
      "loss": 1.655,
      "step": 503232
    },
    {
      "epoch": 0.0018264449356088738,
      "grad_norm": 10713.786352172607,
      "learning_rate": 1.4103011724980724e-07,
      "loss": 1.6702,
      "step": 503264
    },
    {
      "epoch": 0.0018265610699597103,
      "grad_norm": 12083.220100618875,
      "learning_rate": 1.41025629435764e-07,
      "loss": 1.6935,
      "step": 503296
    },
    {
      "epoch": 0.001826677204310547,
      "grad_norm": 9939.563370691894,
      "learning_rate": 1.4102114205012274e-07,
      "loss": 1.6885,
      "step": 503328
    },
    {
      "epoch": 0.0018267933386613838,
      "grad_norm": 9239.101687934817,
      "learning_rate": 1.4101665509281527e-07,
      "loss": 1.6835,
      "step": 503360
    },
    {
      "epoch": 0.0018269094730122206,
      "grad_norm": 9044.41297155321,
      "learning_rate": 1.4101216856377348e-07,
      "loss": 1.6782,
      "step": 503392
    },
    {
      "epoch": 0.0018270256073630571,
      "grad_norm": 12976.209924319197,
      "learning_rate": 1.4100768246292926e-07,
      "loss": 1.6861,
      "step": 503424
    },
    {
      "epoch": 0.0018271417417138939,
      "grad_norm": 10065.3996443261,
      "learning_rate": 1.4100319679021446e-07,
      "loss": 1.6685,
      "step": 503456
    },
    {
      "epoch": 0.0018272578760647306,
      "grad_norm": 9074.305042260812,
      "learning_rate": 1.4099871154556102e-07,
      "loss": 1.6348,
      "step": 503488
    },
    {
      "epoch": 0.0018273740104155674,
      "grad_norm": 7422.344777763965,
      "learning_rate": 1.4099422672890086e-07,
      "loss": 1.6444,
      "step": 503520
    },
    {
      "epoch": 0.0018274901447664041,
      "grad_norm": 9565.577452511689,
      "learning_rate": 1.409897423401659e-07,
      "loss": 1.65,
      "step": 503552
    },
    {
      "epoch": 0.0018276062791172407,
      "grad_norm": 8911.18488193349,
      "learning_rate": 1.409852583792881e-07,
      "loss": 1.6688,
      "step": 503584
    },
    {
      "epoch": 0.0018277224134680774,
      "grad_norm": 11543.70321863829,
      "learning_rate": 1.409807748461994e-07,
      "loss": 1.6497,
      "step": 503616
    },
    {
      "epoch": 0.0018278385478189142,
      "grad_norm": 10118.633899889846,
      "learning_rate": 1.4097629174083185e-07,
      "loss": 1.6634,
      "step": 503648
    },
    {
      "epoch": 0.001827954682169751,
      "grad_norm": 10027.066171119048,
      "learning_rate": 1.409718090631174e-07,
      "loss": 1.6768,
      "step": 503680
    },
    {
      "epoch": 0.0018280708165205875,
      "grad_norm": 8723.035366201377,
      "learning_rate": 1.4096732681298808e-07,
      "loss": 1.6677,
      "step": 503712
    },
    {
      "epoch": 0.0018281869508714242,
      "grad_norm": 8995.37125415066,
      "learning_rate": 1.409628449903759e-07,
      "loss": 1.6494,
      "step": 503744
    },
    {
      "epoch": 0.001828303085222261,
      "grad_norm": 8922.215756189715,
      "learning_rate": 1.4095836359521291e-07,
      "loss": 1.6683,
      "step": 503776
    },
    {
      "epoch": 0.0018284192195730977,
      "grad_norm": 7331.470520980085,
      "learning_rate": 1.409538826274312e-07,
      "loss": 1.6748,
      "step": 503808
    },
    {
      "epoch": 0.0018285353539239345,
      "grad_norm": 9348.82409717928,
      "learning_rate": 1.4094940208696278e-07,
      "loss": 1.6742,
      "step": 503840
    },
    {
      "epoch": 0.001828651488274771,
      "grad_norm": 11330.036893143817,
      "learning_rate": 1.4094492197373979e-07,
      "loss": 1.6627,
      "step": 503872
    },
    {
      "epoch": 0.0018287676226256078,
      "grad_norm": 11525.969807352438,
      "learning_rate": 1.4094044228769432e-07,
      "loss": 1.6634,
      "step": 503904
    },
    {
      "epoch": 0.0018288837569764445,
      "grad_norm": 9170.045256158772,
      "learning_rate": 1.409359630287585e-07,
      "loss": 1.67,
      "step": 503936
    },
    {
      "epoch": 0.0018289998913272813,
      "grad_norm": 12748.555682899925,
      "learning_rate": 1.4093148419686445e-07,
      "loss": 1.6581,
      "step": 503968
    },
    {
      "epoch": 0.0018291160256781178,
      "grad_norm": 9693.274369375913,
      "learning_rate": 1.409270057919443e-07,
      "loss": 1.6551,
      "step": 504000
    },
    {
      "epoch": 0.0018292321600289546,
      "grad_norm": 19601.532593141794,
      "learning_rate": 1.4092252781393024e-07,
      "loss": 1.6532,
      "step": 504032
    },
    {
      "epoch": 0.0018293482943797913,
      "grad_norm": 20828.301899098737,
      "learning_rate": 1.4091805026275445e-07,
      "loss": 1.6584,
      "step": 504064
    },
    {
      "epoch": 0.001829464428730628,
      "grad_norm": 12403.501279880613,
      "learning_rate": 1.4091371304202758e-07,
      "loss": 1.6782,
      "step": 504096
    },
    {
      "epoch": 0.0018295805630814648,
      "grad_norm": 10022.274791682776,
      "learning_rate": 1.4090923633099146e-07,
      "loss": 1.6438,
      "step": 504128
    },
    {
      "epoch": 0.0018296966974323014,
      "grad_norm": 11513.550277824821,
      "learning_rate": 1.409047600465924e-07,
      "loss": 1.6728,
      "step": 504160
    },
    {
      "epoch": 0.0018298128317831381,
      "grad_norm": 8166.257404711169,
      "learning_rate": 1.4090028418876256e-07,
      "loss": 1.6886,
      "step": 504192
    },
    {
      "epoch": 0.0018299289661339749,
      "grad_norm": 10201.198949143183,
      "learning_rate": 1.408958087574342e-07,
      "loss": 1.6815,
      "step": 504224
    },
    {
      "epoch": 0.0018300451004848116,
      "grad_norm": 7946.9414242210205,
      "learning_rate": 1.4089133375253968e-07,
      "loss": 1.6776,
      "step": 504256
    },
    {
      "epoch": 0.0018301612348356482,
      "grad_norm": 9790.290291916783,
      "learning_rate": 1.4088685917401117e-07,
      "loss": 1.6862,
      "step": 504288
    },
    {
      "epoch": 0.001830277369186485,
      "grad_norm": 8602.364674901895,
      "learning_rate": 1.40882385021781e-07,
      "loss": 1.6827,
      "step": 504320
    },
    {
      "epoch": 0.0018303935035373217,
      "grad_norm": 14912.935056520564,
      "learning_rate": 1.4087791129578154e-07,
      "loss": 1.6597,
      "step": 504352
    },
    {
      "epoch": 0.0018305096378881584,
      "grad_norm": 12130.035119487495,
      "learning_rate": 1.4087343799594506e-07,
      "loss": 1.6715,
      "step": 504384
    },
    {
      "epoch": 0.0018306257722389952,
      "grad_norm": 10211.272300746856,
      "learning_rate": 1.4086896512220395e-07,
      "loss": 1.6528,
      "step": 504416
    },
    {
      "epoch": 0.0018307419065898317,
      "grad_norm": 8380.208350631863,
      "learning_rate": 1.4086449267449053e-07,
      "loss": 1.6646,
      "step": 504448
    },
    {
      "epoch": 0.0018308580409406685,
      "grad_norm": 9371.44919422818,
      "learning_rate": 1.4086002065273717e-07,
      "loss": 1.6445,
      "step": 504480
    },
    {
      "epoch": 0.0018309741752915052,
      "grad_norm": 11964.05332652776,
      "learning_rate": 1.4085554905687628e-07,
      "loss": 1.6385,
      "step": 504512
    },
    {
      "epoch": 0.001831090309642342,
      "grad_norm": 9588.117854928567,
      "learning_rate": 1.408510778868403e-07,
      "loss": 1.6491,
      "step": 504544
    },
    {
      "epoch": 0.0018312064439931785,
      "grad_norm": 8351.345520333834,
      "learning_rate": 1.408466071425616e-07,
      "loss": 1.6765,
      "step": 504576
    },
    {
      "epoch": 0.0018313225783440153,
      "grad_norm": 9764.599326137248,
      "learning_rate": 1.408421368239726e-07,
      "loss": 1.669,
      "step": 504608
    },
    {
      "epoch": 0.001831438712694852,
      "grad_norm": 11435.537591211007,
      "learning_rate": 1.408376669310058e-07,
      "loss": 1.6415,
      "step": 504640
    },
    {
      "epoch": 0.0018315548470456888,
      "grad_norm": 10130.52575141093,
      "learning_rate": 1.4083319746359363e-07,
      "loss": 1.6492,
      "step": 504672
    },
    {
      "epoch": 0.0018316709813965255,
      "grad_norm": 10702.28340121864,
      "learning_rate": 1.4082872842166858e-07,
      "loss": 1.6635,
      "step": 504704
    },
    {
      "epoch": 0.001831787115747362,
      "grad_norm": 10966.874486379425,
      "learning_rate": 1.4082425980516316e-07,
      "loss": 1.6676,
      "step": 504736
    },
    {
      "epoch": 0.0018319032500981988,
      "grad_norm": 9152.386246220161,
      "learning_rate": 1.4081979161400984e-07,
      "loss": 1.6874,
      "step": 504768
    },
    {
      "epoch": 0.0018320193844490356,
      "grad_norm": 11063.641172778516,
      "learning_rate": 1.408153238481412e-07,
      "loss": 1.6767,
      "step": 504800
    },
    {
      "epoch": 0.0018321355187998723,
      "grad_norm": 9238.92558688509,
      "learning_rate": 1.4081085650748974e-07,
      "loss": 1.6816,
      "step": 504832
    },
    {
      "epoch": 0.0018322516531507089,
      "grad_norm": 9758.077064668018,
      "learning_rate": 1.4080638959198802e-07,
      "loss": 1.6596,
      "step": 504864
    },
    {
      "epoch": 0.0018323677875015456,
      "grad_norm": 8618.971284323901,
      "learning_rate": 1.408019231015686e-07,
      "loss": 1.6653,
      "step": 504896
    },
    {
      "epoch": 0.0018324839218523824,
      "grad_norm": 10544.0952196004,
      "learning_rate": 1.407974570361641e-07,
      "loss": 1.6789,
      "step": 504928
    },
    {
      "epoch": 0.0018326000562032191,
      "grad_norm": 11864.846901667126,
      "learning_rate": 1.407929913957071e-07,
      "loss": 1.6624,
      "step": 504960
    },
    {
      "epoch": 0.001832716190554056,
      "grad_norm": 10543.754359809413,
      "learning_rate": 1.407885261801302e-07,
      "loss": 1.6437,
      "step": 504992
    },
    {
      "epoch": 0.0018328323249048924,
      "grad_norm": 9514.884550009001,
      "learning_rate": 1.4078406138936605e-07,
      "loss": 1.6486,
      "step": 505024
    },
    {
      "epoch": 0.0018329484592557292,
      "grad_norm": 7893.16311753406,
      "learning_rate": 1.4077959702334726e-07,
      "loss": 1.6608,
      "step": 505056
    },
    {
      "epoch": 0.001833064593606566,
      "grad_norm": 24412.772722490987,
      "learning_rate": 1.4077513308200655e-07,
      "loss": 1.6942,
      "step": 505088
    },
    {
      "epoch": 0.0018331807279574027,
      "grad_norm": 9344.64145914652,
      "learning_rate": 1.407708090437478e-07,
      "loss": 1.6721,
      "step": 505120
    },
    {
      "epoch": 0.0018332968623082392,
      "grad_norm": 13283.70957225428,
      "learning_rate": 1.4076634593829525e-07,
      "loss": 1.6651,
      "step": 505152
    },
    {
      "epoch": 0.001833412996659076,
      "grad_norm": 9697.421719199387,
      "learning_rate": 1.407618832573209e-07,
      "loss": 1.6692,
      "step": 505184
    },
    {
      "epoch": 0.0018335291310099127,
      "grad_norm": 13315.054787720552,
      "learning_rate": 1.407574210007575e-07,
      "loss": 1.6532,
      "step": 505216
    },
    {
      "epoch": 0.0018336452653607495,
      "grad_norm": 9743.308113777373,
      "learning_rate": 1.4075295916853777e-07,
      "loss": 1.6631,
      "step": 505248
    },
    {
      "epoch": 0.0018337613997115863,
      "grad_norm": 9828.478417333987,
      "learning_rate": 1.4074849776059444e-07,
      "loss": 1.6907,
      "step": 505280
    },
    {
      "epoch": 0.0018338775340624228,
      "grad_norm": 7520.295073998094,
      "learning_rate": 1.407440367768603e-07,
      "loss": 1.6933,
      "step": 505312
    },
    {
      "epoch": 0.0018339936684132595,
      "grad_norm": 9891.123899739605,
      "learning_rate": 1.407395762172681e-07,
      "loss": 1.665,
      "step": 505344
    },
    {
      "epoch": 0.0018341098027640963,
      "grad_norm": 9355.367870907055,
      "learning_rate": 1.4073511608175066e-07,
      "loss": 1.6664,
      "step": 505376
    },
    {
      "epoch": 0.001834225937114933,
      "grad_norm": 12973.861106085575,
      "learning_rate": 1.4073065637024082e-07,
      "loss": 1.6751,
      "step": 505408
    },
    {
      "epoch": 0.0018343420714657696,
      "grad_norm": 8885.61511658028,
      "learning_rate": 1.407261970826713e-07,
      "loss": 1.6977,
      "step": 505440
    },
    {
      "epoch": 0.0018344582058166063,
      "grad_norm": 12002.964633789437,
      "learning_rate": 1.4072173821897502e-07,
      "loss": 1.6906,
      "step": 505472
    },
    {
      "epoch": 0.001834574340167443,
      "grad_norm": 10839.099778118107,
      "learning_rate": 1.4071727977908485e-07,
      "loss": 1.6494,
      "step": 505504
    },
    {
      "epoch": 0.0018346904745182799,
      "grad_norm": 8607.275991857121,
      "learning_rate": 1.407128217629336e-07,
      "loss": 1.6543,
      "step": 505536
    },
    {
      "epoch": 0.0018348066088691166,
      "grad_norm": 9278.238302608961,
      "learning_rate": 1.4070836417045415e-07,
      "loss": 1.6629,
      "step": 505568
    },
    {
      "epoch": 0.0018349227432199531,
      "grad_norm": 9682.923525464817,
      "learning_rate": 1.4070390700157945e-07,
      "loss": 1.672,
      "step": 505600
    },
    {
      "epoch": 0.00183503887757079,
      "grad_norm": 9220.50063716716,
      "learning_rate": 1.4069945025624237e-07,
      "loss": 1.6555,
      "step": 505632
    },
    {
      "epoch": 0.0018351550119216267,
      "grad_norm": 14727.744565954421,
      "learning_rate": 1.4069499393437587e-07,
      "loss": 1.6525,
      "step": 505664
    },
    {
      "epoch": 0.0018352711462724634,
      "grad_norm": 17456.054422463283,
      "learning_rate": 1.4069053803591284e-07,
      "loss": 1.6644,
      "step": 505696
    },
    {
      "epoch": 0.0018353872806233,
      "grad_norm": 12433.031167016352,
      "learning_rate": 1.4068608256078626e-07,
      "loss": 1.6657,
      "step": 505728
    },
    {
      "epoch": 0.0018355034149741367,
      "grad_norm": 10811.852385229831,
      "learning_rate": 1.4068162750892917e-07,
      "loss": 1.6719,
      "step": 505760
    },
    {
      "epoch": 0.0018356195493249735,
      "grad_norm": 8720.546542505235,
      "learning_rate": 1.4067717288027446e-07,
      "loss": 1.7004,
      "step": 505792
    },
    {
      "epoch": 0.0018357356836758102,
      "grad_norm": 9977.591492940568,
      "learning_rate": 1.4067271867475517e-07,
      "loss": 1.7077,
      "step": 505824
    },
    {
      "epoch": 0.001835851818026647,
      "grad_norm": 10236.217660835471,
      "learning_rate": 1.4066826489230432e-07,
      "loss": 1.6952,
      "step": 505856
    },
    {
      "epoch": 0.0018359679523774835,
      "grad_norm": 16092.803360508697,
      "learning_rate": 1.4066381153285495e-07,
      "loss": 1.6796,
      "step": 505888
    },
    {
      "epoch": 0.0018360840867283203,
      "grad_norm": 9916.318066701975,
      "learning_rate": 1.406593585963401e-07,
      "loss": 1.6852,
      "step": 505920
    },
    {
      "epoch": 0.001836200221079157,
      "grad_norm": 9388.105879249551,
      "learning_rate": 1.406549060826928e-07,
      "loss": 1.706,
      "step": 505952
    },
    {
      "epoch": 0.0018363163554299938,
      "grad_norm": 10230.413676875438,
      "learning_rate": 1.4065045399184615e-07,
      "loss": 1.6735,
      "step": 505984
    },
    {
      "epoch": 0.0018364324897808303,
      "grad_norm": 9666.00082764325,
      "learning_rate": 1.4064600232373325e-07,
      "loss": 1.664,
      "step": 506016
    },
    {
      "epoch": 0.001836548624131667,
      "grad_norm": 10628.512972189477,
      "learning_rate": 1.406415510782872e-07,
      "loss": 1.6617,
      "step": 506048
    },
    {
      "epoch": 0.0018366647584825038,
      "grad_norm": 9670.648892396,
      "learning_rate": 1.406371002554411e-07,
      "loss": 1.662,
      "step": 506080
    },
    {
      "epoch": 0.0018367808928333406,
      "grad_norm": 20675.328098968585,
      "learning_rate": 1.406326498551281e-07,
      "loss": 1.6539,
      "step": 506112
    },
    {
      "epoch": 0.0018368970271841773,
      "grad_norm": 20933.33571125252,
      "learning_rate": 1.4062819987728138e-07,
      "loss": 1.6275,
      "step": 506144
    },
    {
      "epoch": 0.0018370131615350139,
      "grad_norm": 9132.223387543692,
      "learning_rate": 1.4062388936404872e-07,
      "loss": 1.6394,
      "step": 506176
    },
    {
      "epoch": 0.0018371292958858506,
      "grad_norm": 7805.963361430798,
      "learning_rate": 1.4061944021773713e-07,
      "loss": 1.6465,
      "step": 506208
    },
    {
      "epoch": 0.0018372454302366874,
      "grad_norm": 13993.671140912238,
      "learning_rate": 1.406149914936934e-07,
      "loss": 1.6452,
      "step": 506240
    },
    {
      "epoch": 0.0018373615645875241,
      "grad_norm": 8180.320042638918,
      "learning_rate": 1.4061054319185074e-07,
      "loss": 1.66,
      "step": 506272
    },
    {
      "epoch": 0.0018374776989383607,
      "grad_norm": 9322.36944129549,
      "learning_rate": 1.4060609531214242e-07,
      "loss": 1.6856,
      "step": 506304
    },
    {
      "epoch": 0.0018375938332891974,
      "grad_norm": 8130.0255842155875,
      "learning_rate": 1.4060164785450165e-07,
      "loss": 1.6948,
      "step": 506336
    },
    {
      "epoch": 0.0018377099676400342,
      "grad_norm": 9930.779929089154,
      "learning_rate": 1.4059720081886165e-07,
      "loss": 1.6761,
      "step": 506368
    },
    {
      "epoch": 0.001837826101990871,
      "grad_norm": 13055.270200191186,
      "learning_rate": 1.4059275420515572e-07,
      "loss": 1.6744,
      "step": 506400
    },
    {
      "epoch": 0.0018379422363417075,
      "grad_norm": 9044.519445498472,
      "learning_rate": 1.4058830801331718e-07,
      "loss": 1.6561,
      "step": 506432
    },
    {
      "epoch": 0.0018380583706925442,
      "grad_norm": 10145.872954063638,
      "learning_rate": 1.4058386224327926e-07,
      "loss": 1.6639,
      "step": 506464
    },
    {
      "epoch": 0.001838174505043381,
      "grad_norm": 9816.600022411018,
      "learning_rate": 1.4057941689497528e-07,
      "loss": 1.6484,
      "step": 506496
    },
    {
      "epoch": 0.0018382906393942177,
      "grad_norm": 9660.251860070732,
      "learning_rate": 1.4057497196833863e-07,
      "loss": 1.6506,
      "step": 506528
    },
    {
      "epoch": 0.0018384067737450545,
      "grad_norm": 9798.861770634383,
      "learning_rate": 1.4057052746330256e-07,
      "loss": 1.6415,
      "step": 506560
    },
    {
      "epoch": 0.001838522908095891,
      "grad_norm": 11286.919065892162,
      "learning_rate": 1.4056608337980052e-07,
      "loss": 1.6635,
      "step": 506592
    },
    {
      "epoch": 0.0018386390424467278,
      "grad_norm": 10453.855461024894,
      "learning_rate": 1.4056163971776582e-07,
      "loss": 1.6503,
      "step": 506624
    },
    {
      "epoch": 0.0018387551767975645,
      "grad_norm": 8425.511853887574,
      "learning_rate": 1.4055719647713183e-07,
      "loss": 1.6334,
      "step": 506656
    },
    {
      "epoch": 0.0018388713111484013,
      "grad_norm": 10042.720946038478,
      "learning_rate": 1.4055275365783202e-07,
      "loss": 1.6432,
      "step": 506688
    },
    {
      "epoch": 0.0018389874454992378,
      "grad_norm": 10440.844793406328,
      "learning_rate": 1.4054831125979976e-07,
      "loss": 1.6535,
      "step": 506720
    },
    {
      "epoch": 0.0018391035798500746,
      "grad_norm": 10449.65501822907,
      "learning_rate": 1.4054386928296847e-07,
      "loss": 1.6521,
      "step": 506752
    },
    {
      "epoch": 0.0018392197142009113,
      "grad_norm": 9412.36335890195,
      "learning_rate": 1.4053942772727165e-07,
      "loss": 1.6703,
      "step": 506784
    },
    {
      "epoch": 0.001839335848551748,
      "grad_norm": 8506.026099184037,
      "learning_rate": 1.4053498659264268e-07,
      "loss": 1.7005,
      "step": 506816
    },
    {
      "epoch": 0.0018394519829025848,
      "grad_norm": 10959.191941014627,
      "learning_rate": 1.4053054587901508e-07,
      "loss": 1.6914,
      "step": 506848
    },
    {
      "epoch": 0.0018395681172534214,
      "grad_norm": 8927.85842181651,
      "learning_rate": 1.4052610558632235e-07,
      "loss": 1.704,
      "step": 506880
    },
    {
      "epoch": 0.0018396842516042581,
      "grad_norm": 10153.440993082098,
      "learning_rate": 1.40521665714498e-07,
      "loss": 1.7012,
      "step": 506912
    },
    {
      "epoch": 0.0018398003859550949,
      "grad_norm": 9130.351143302212,
      "learning_rate": 1.4051722626347546e-07,
      "loss": 1.6772,
      "step": 506944
    },
    {
      "epoch": 0.0018399165203059316,
      "grad_norm": 12601.87081349432,
      "learning_rate": 1.4051278723318839e-07,
      "loss": 1.6602,
      "step": 506976
    },
    {
      "epoch": 0.0018400326546567682,
      "grad_norm": 13940.410037011106,
      "learning_rate": 1.4050834862357025e-07,
      "loss": 1.6263,
      "step": 507008
    },
    {
      "epoch": 0.001840148789007605,
      "grad_norm": 11896.263278861981,
      "learning_rate": 1.4050391043455465e-07,
      "loss": 1.6384,
      "step": 507040
    },
    {
      "epoch": 0.0018402649233584417,
      "grad_norm": 9045.68781243306,
      "learning_rate": 1.4049947266607512e-07,
      "loss": 1.6529,
      "step": 507072
    },
    {
      "epoch": 0.0018403810577092784,
      "grad_norm": 9335.944301461957,
      "learning_rate": 1.4049503531806528e-07,
      "loss": 1.6663,
      "step": 507104
    },
    {
      "epoch": 0.0018404971920601152,
      "grad_norm": 11308.551100826313,
      "learning_rate": 1.4049059839045874e-07,
      "loss": 1.6394,
      "step": 507136
    },
    {
      "epoch": 0.0018406133264109517,
      "grad_norm": 20664.861383517673,
      "learning_rate": 1.404861618831891e-07,
      "loss": 1.6408,
      "step": 507168
    },
    {
      "epoch": 0.0018407294607617885,
      "grad_norm": 30454.839352720282,
      "learning_rate": 1.4048172579619004e-07,
      "loss": 1.6551,
      "step": 507200
    },
    {
      "epoch": 0.0018408455951126252,
      "grad_norm": 22959.24458687611,
      "learning_rate": 1.4047729012939518e-07,
      "loss": 1.6517,
      "step": 507232
    },
    {
      "epoch": 0.001840961729463462,
      "grad_norm": 23772.33753756664,
      "learning_rate": 1.4047285488273818e-07,
      "loss": 1.6408,
      "step": 507264
    },
    {
      "epoch": 0.0018410778638142985,
      "grad_norm": 17188.812175365696,
      "learning_rate": 1.404684200561527e-07,
      "loss": 1.6646,
      "step": 507296
    },
    {
      "epoch": 0.0018411939981651353,
      "grad_norm": 24110.542092619984,
      "learning_rate": 1.404639856495725e-07,
      "loss": 1.6697,
      "step": 507328
    },
    {
      "epoch": 0.001841310132515972,
      "grad_norm": 22163.66467892889,
      "learning_rate": 1.4045955166293122e-07,
      "loss": 1.6625,
      "step": 507360
    },
    {
      "epoch": 0.0018414262668668088,
      "grad_norm": 23374.142978941494,
      "learning_rate": 1.4045511809616262e-07,
      "loss": 1.6655,
      "step": 507392
    },
    {
      "epoch": 0.0018415424012176455,
      "grad_norm": 20197.73729901446,
      "learning_rate": 1.4045068494920046e-07,
      "loss": 1.6755,
      "step": 507424
    },
    {
      "epoch": 0.001841658535568482,
      "grad_norm": 9607.996044961717,
      "learning_rate": 1.4044639073835129e-07,
      "loss": 1.6944,
      "step": 507456
    },
    {
      "epoch": 0.0018417746699193188,
      "grad_norm": 12163.968431396062,
      "learning_rate": 1.404419584176894e-07,
      "loss": 1.6598,
      "step": 507488
    },
    {
      "epoch": 0.0018418908042701556,
      "grad_norm": 11589.766693078856,
      "learning_rate": 1.4043752651663727e-07,
      "loss": 1.6371,
      "step": 507520
    },
    {
      "epoch": 0.0018420069386209923,
      "grad_norm": 12140.502790247197,
      "learning_rate": 1.4043309503512876e-07,
      "loss": 1.6417,
      "step": 507552
    },
    {
      "epoch": 0.0018421230729718289,
      "grad_norm": 8018.338980113026,
      "learning_rate": 1.4042866397309762e-07,
      "loss": 1.6527,
      "step": 507584
    },
    {
      "epoch": 0.0018422392073226656,
      "grad_norm": 10995.477433927097,
      "learning_rate": 1.404242333304777e-07,
      "loss": 1.6764,
      "step": 507616
    },
    {
      "epoch": 0.0018423553416735024,
      "grad_norm": 9391.1738350432,
      "learning_rate": 1.4041980310720283e-07,
      "loss": 1.6494,
      "step": 507648
    },
    {
      "epoch": 0.0018424714760243391,
      "grad_norm": 13470.157088913254,
      "learning_rate": 1.4041537330320689e-07,
      "loss": 1.6558,
      "step": 507680
    },
    {
      "epoch": 0.0018425876103751759,
      "grad_norm": 10885.432283561366,
      "learning_rate": 1.4041094391842372e-07,
      "loss": 1.6662,
      "step": 507712
    },
    {
      "epoch": 0.0018427037447260124,
      "grad_norm": 8030.9937118640555,
      "learning_rate": 1.404065149527872e-07,
      "loss": 1.6712,
      "step": 507744
    },
    {
      "epoch": 0.0018428198790768492,
      "grad_norm": 10207.987852657348,
      "learning_rate": 1.4040208640623127e-07,
      "loss": 1.6751,
      "step": 507776
    },
    {
      "epoch": 0.001842936013427686,
      "grad_norm": 11825.61118927897,
      "learning_rate": 1.4039765827868982e-07,
      "loss": 1.683,
      "step": 507808
    },
    {
      "epoch": 0.0018430521477785227,
      "grad_norm": 12039.888371575544,
      "learning_rate": 1.4039323057009676e-07,
      "loss": 1.6808,
      "step": 507840
    },
    {
      "epoch": 0.0018431682821293592,
      "grad_norm": 8660.916233286176,
      "learning_rate": 1.4038880328038604e-07,
      "loss": 1.6616,
      "step": 507872
    },
    {
      "epoch": 0.001843284416480196,
      "grad_norm": 9472.869786923075,
      "learning_rate": 1.4038437640949166e-07,
      "loss": 1.6718,
      "step": 507904
    },
    {
      "epoch": 0.0018434005508310327,
      "grad_norm": 10354.215276881197,
      "learning_rate": 1.4037994995734752e-07,
      "loss": 1.6652,
      "step": 507936
    },
    {
      "epoch": 0.0018435166851818695,
      "grad_norm": 9903.501703942904,
      "learning_rate": 1.4037552392388764e-07,
      "loss": 1.6896,
      "step": 507968
    },
    {
      "epoch": 0.0018436328195327062,
      "grad_norm": 9055.021148512024,
      "learning_rate": 1.4037109830904605e-07,
      "loss": 1.6629,
      "step": 508000
    },
    {
      "epoch": 0.0018437489538835428,
      "grad_norm": 9788.257454726046,
      "learning_rate": 1.4036667311275674e-07,
      "loss": 1.6568,
      "step": 508032
    },
    {
      "epoch": 0.0018438650882343795,
      "grad_norm": 9037.830602528464,
      "learning_rate": 1.4036224833495372e-07,
      "loss": 1.6574,
      "step": 508064
    },
    {
      "epoch": 0.0018439812225852163,
      "grad_norm": 7605.12596871347,
      "learning_rate": 1.4035782397557108e-07,
      "loss": 1.6699,
      "step": 508096
    },
    {
      "epoch": 0.001844097356936053,
      "grad_norm": 10325.53204440333,
      "learning_rate": 1.4035340003454283e-07,
      "loss": 1.6568,
      "step": 508128
    },
    {
      "epoch": 0.0018442134912868896,
      "grad_norm": 8944.121533163556,
      "learning_rate": 1.4034897651180306e-07,
      "loss": 1.6398,
      "step": 508160
    },
    {
      "epoch": 0.0018443296256377263,
      "grad_norm": 8405.36162220282,
      "learning_rate": 1.403445534072859e-07,
      "loss": 1.6492,
      "step": 508192
    },
    {
      "epoch": 0.001844445759988563,
      "grad_norm": 9428.264103216456,
      "learning_rate": 1.4034013072092538e-07,
      "loss": 1.6484,
      "step": 508224
    },
    {
      "epoch": 0.0018445618943393998,
      "grad_norm": 9117.962930391854,
      "learning_rate": 1.4033570845265567e-07,
      "loss": 1.6404,
      "step": 508256
    },
    {
      "epoch": 0.0018446780286902366,
      "grad_norm": 8660.07240154492,
      "learning_rate": 1.4033128660241088e-07,
      "loss": 1.6554,
      "step": 508288
    },
    {
      "epoch": 0.0018447941630410731,
      "grad_norm": 10974.173317384777,
      "learning_rate": 1.403268651701252e-07,
      "loss": 1.6737,
      "step": 508320
    },
    {
      "epoch": 0.0018449102973919099,
      "grad_norm": 11686.708347520273,
      "learning_rate": 1.403224441557327e-07,
      "loss": 1.6721,
      "step": 508352
    },
    {
      "epoch": 0.0018450264317427466,
      "grad_norm": 11784.155718590959,
      "learning_rate": 1.4031802355916764e-07,
      "loss": 1.6645,
      "step": 508384
    },
    {
      "epoch": 0.0018451425660935834,
      "grad_norm": 9351.608845540964,
      "learning_rate": 1.4031360338036415e-07,
      "loss": 1.661,
      "step": 508416
    },
    {
      "epoch": 0.00184525870044442,
      "grad_norm": 18337.310162616544,
      "learning_rate": 1.403091836192565e-07,
      "loss": 1.6668,
      "step": 508448
    },
    {
      "epoch": 0.0018453748347952567,
      "grad_norm": 21613.265926277778,
      "learning_rate": 1.4030476427577882e-07,
      "loss": 1.6825,
      "step": 508480
    },
    {
      "epoch": 0.0018454909691460934,
      "grad_norm": 23331.774386017023,
      "learning_rate": 1.4030034534986544e-07,
      "loss": 1.6611,
      "step": 508512
    },
    {
      "epoch": 0.0018456071034969302,
      "grad_norm": 22923.4733842845,
      "learning_rate": 1.4029592684145055e-07,
      "loss": 1.6676,
      "step": 508544
    },
    {
      "epoch": 0.001845723237847767,
      "grad_norm": 27296.93198877852,
      "learning_rate": 1.402915087504684e-07,
      "loss": 1.6665,
      "step": 508576
    },
    {
      "epoch": 0.0018458393721986035,
      "grad_norm": 19770.49700943302,
      "learning_rate": 1.402870910768533e-07,
      "loss": 1.6867,
      "step": 508608
    },
    {
      "epoch": 0.0018459555065494402,
      "grad_norm": 24615.398757688246,
      "learning_rate": 1.4028267382053955e-07,
      "loss": 1.6734,
      "step": 508640
    },
    {
      "epoch": 0.001846071640900277,
      "grad_norm": 18284.883373978624,
      "learning_rate": 1.4027825698146143e-07,
      "loss": 1.6649,
      "step": 508672
    },
    {
      "epoch": 0.0018461877752511137,
      "grad_norm": 21122.192689207244,
      "learning_rate": 1.4027384055955325e-07,
      "loss": 1.6675,
      "step": 508704
    },
    {
      "epoch": 0.0018463039096019503,
      "grad_norm": 19602.373937867833,
      "learning_rate": 1.4026942455474938e-07,
      "loss": 1.6452,
      "step": 508736
    },
    {
      "epoch": 0.001846420043952787,
      "grad_norm": 18003.325026227794,
      "learning_rate": 1.4026500896698416e-07,
      "loss": 1.6533,
      "step": 508768
    },
    {
      "epoch": 0.0018465361783036238,
      "grad_norm": 16149.250385079798,
      "learning_rate": 1.4026059379619196e-07,
      "loss": 1.6608,
      "step": 508800
    },
    {
      "epoch": 0.0018466523126544605,
      "grad_norm": 19395.703029279448,
      "learning_rate": 1.402561790423071e-07,
      "loss": 1.6795,
      "step": 508832
    },
    {
      "epoch": 0.0018467684470052973,
      "grad_norm": 21736.148876928495,
      "learning_rate": 1.4025176470526406e-07,
      "loss": 1.665,
      "step": 508864
    },
    {
      "epoch": 0.0018468845813561338,
      "grad_norm": 30701.55487919138,
      "learning_rate": 1.4024735078499715e-07,
      "loss": 1.6714,
      "step": 508896
    },
    {
      "epoch": 0.0018470007157069706,
      "grad_norm": 22646.73504061899,
      "learning_rate": 1.4024293728144091e-07,
      "loss": 1.68,
      "step": 508928
    },
    {
      "epoch": 0.0018471168500578073,
      "grad_norm": 8523.260643673875,
      "learning_rate": 1.4023866209718965e-07,
      "loss": 1.6993,
      "step": 508960
    },
    {
      "epoch": 0.001847232984408644,
      "grad_norm": 9051.016738466458,
      "learning_rate": 1.402342494138408e-07,
      "loss": 1.6627,
      "step": 508992
    },
    {
      "epoch": 0.0018473491187594806,
      "grad_norm": 8157.568510285402,
      "learning_rate": 1.4022983714700794e-07,
      "loss": 1.6476,
      "step": 509024
    },
    {
      "epoch": 0.0018474652531103174,
      "grad_norm": 10090.63328042398,
      "learning_rate": 1.402254252966256e-07,
      "loss": 1.6634,
      "step": 509056
    },
    {
      "epoch": 0.0018475813874611541,
      "grad_norm": 8409.780496540918,
      "learning_rate": 1.402210138626282e-07,
      "loss": 1.6727,
      "step": 509088
    },
    {
      "epoch": 0.001847697521811991,
      "grad_norm": 9008.15663718166,
      "learning_rate": 1.4021660284495028e-07,
      "loss": 1.6683,
      "step": 509120
    },
    {
      "epoch": 0.0018478136561628276,
      "grad_norm": 17820.294385896097,
      "learning_rate": 1.4021219224352636e-07,
      "loss": 1.6333,
      "step": 509152
    },
    {
      "epoch": 0.0018479297905136642,
      "grad_norm": 10586.455119632823,
      "learning_rate": 1.40207782058291e-07,
      "loss": 1.6374,
      "step": 509184
    },
    {
      "epoch": 0.001848045924864501,
      "grad_norm": 10080.04166658055,
      "learning_rate": 1.4020337228917872e-07,
      "loss": 1.6454,
      "step": 509216
    },
    {
      "epoch": 0.0018481620592153377,
      "grad_norm": 8700.670089136813,
      "learning_rate": 1.401989629361241e-07,
      "loss": 1.656,
      "step": 509248
    },
    {
      "epoch": 0.0018482781935661744,
      "grad_norm": 8885.332407963137,
      "learning_rate": 1.4019455399906174e-07,
      "loss": 1.6518,
      "step": 509280
    },
    {
      "epoch": 0.001848394327917011,
      "grad_norm": 8413.463852659022,
      "learning_rate": 1.4019014547792617e-07,
      "loss": 1.669,
      "step": 509312
    },
    {
      "epoch": 0.0018485104622678477,
      "grad_norm": 9352.106072965596,
      "learning_rate": 1.4018573737265207e-07,
      "loss": 1.6732,
      "step": 509344
    },
    {
      "epoch": 0.0018486265966186845,
      "grad_norm": 9871.72264602283,
      "learning_rate": 1.4018132968317406e-07,
      "loss": 1.6635,
      "step": 509376
    },
    {
      "epoch": 0.0018487427309695212,
      "grad_norm": 10066.414058640743,
      "learning_rate": 1.4017692240942672e-07,
      "loss": 1.6739,
      "step": 509408
    },
    {
      "epoch": 0.001848858865320358,
      "grad_norm": 12693.50117185956,
      "learning_rate": 1.401725155513447e-07,
      "loss": 1.6695,
      "step": 509440
    },
    {
      "epoch": 0.0018489749996711945,
      "grad_norm": 14313.627283117303,
      "learning_rate": 1.4016810910886275e-07,
      "loss": 1.6883,
      "step": 509472
    },
    {
      "epoch": 0.0018490911340220313,
      "grad_norm": 11492.908074112487,
      "learning_rate": 1.401637030819155e-07,
      "loss": 1.6738,
      "step": 509504
    },
    {
      "epoch": 0.001849207268372868,
      "grad_norm": 8863.795349623095,
      "learning_rate": 1.401592974704376e-07,
      "loss": 1.6735,
      "step": 509536
    },
    {
      "epoch": 0.0018493234027237048,
      "grad_norm": 9982.502592035728,
      "learning_rate": 1.4015489227436382e-07,
      "loss": 1.6783,
      "step": 509568
    },
    {
      "epoch": 0.0018494395370745413,
      "grad_norm": 8402.966857009493,
      "learning_rate": 1.4015048749362887e-07,
      "loss": 1.6748,
      "step": 509600
    },
    {
      "epoch": 0.001849555671425378,
      "grad_norm": 12827.85250928619,
      "learning_rate": 1.401460831281675e-07,
      "loss": 1.6861,
      "step": 509632
    },
    {
      "epoch": 0.0018496718057762148,
      "grad_norm": 10244.282991015038,
      "learning_rate": 1.4014167917791444e-07,
      "loss": 1.6392,
      "step": 509664
    },
    {
      "epoch": 0.0018497879401270516,
      "grad_norm": 8378.161850907394,
      "learning_rate": 1.4013727564280443e-07,
      "loss": 1.6482,
      "step": 509696
    },
    {
      "epoch": 0.0018499040744778884,
      "grad_norm": 10571.64717534595,
      "learning_rate": 1.401328725227723e-07,
      "loss": 1.6588,
      "step": 509728
    },
    {
      "epoch": 0.001850020208828725,
      "grad_norm": 15675.303633422862,
      "learning_rate": 1.4012846981775285e-07,
      "loss": 1.655,
      "step": 509760
    },
    {
      "epoch": 0.0018501363431795616,
      "grad_norm": 11396.50964111381,
      "learning_rate": 1.4012406752768085e-07,
      "loss": 1.6746,
      "step": 509792
    },
    {
      "epoch": 0.0018502524775303984,
      "grad_norm": 8753.01479491495,
      "learning_rate": 1.4011966565249115e-07,
      "loss": 1.6803,
      "step": 509824
    },
    {
      "epoch": 0.0018503686118812352,
      "grad_norm": 8733.476684574134,
      "learning_rate": 1.4011526419211857e-07,
      "loss": 1.6785,
      "step": 509856
    },
    {
      "epoch": 0.0018504847462320717,
      "grad_norm": 14918.35513721268,
      "learning_rate": 1.4011086314649798e-07,
      "loss": 1.656,
      "step": 509888
    },
    {
      "epoch": 0.0018506008805829084,
      "grad_norm": 8976.674105703069,
      "learning_rate": 1.4010646251556424e-07,
      "loss": 1.6575,
      "step": 509920
    },
    {
      "epoch": 0.0018507170149337452,
      "grad_norm": 23405.453381637366,
      "learning_rate": 1.4010206229925225e-07,
      "loss": 1.6541,
      "step": 509952
    },
    {
      "epoch": 0.001850833149284582,
      "grad_norm": 20571.942834841826,
      "learning_rate": 1.4009766249749687e-07,
      "loss": 1.6611,
      "step": 509984
    },
    {
      "epoch": 0.0018509492836354187,
      "grad_norm": 23670.42931592074,
      "learning_rate": 1.40093263110233e-07,
      "loss": 1.6542,
      "step": 510016
    },
    {
      "epoch": 0.0018510654179862552,
      "grad_norm": 18475.44922322594,
      "learning_rate": 1.4008886413739564e-07,
      "loss": 1.6454,
      "step": 510048
    },
    {
      "epoch": 0.001851181552337092,
      "grad_norm": 17419.880826228404,
      "learning_rate": 1.4008446557891964e-07,
      "loss": 1.6475,
      "step": 510080
    },
    {
      "epoch": 0.0018512976866879288,
      "grad_norm": 16796.29125729844,
      "learning_rate": 1.4008006743474e-07,
      "loss": 1.6784,
      "step": 510112
    },
    {
      "epoch": 0.0018514138210387655,
      "grad_norm": 18052.384219265885,
      "learning_rate": 1.400756697047917e-07,
      "loss": 1.6746,
      "step": 510144
    },
    {
      "epoch": 0.001851529955389602,
      "grad_norm": 16958.769530835663,
      "learning_rate": 1.4007127238900967e-07,
      "loss": 1.656,
      "step": 510176
    },
    {
      "epoch": 0.0018516460897404388,
      "grad_norm": 10263.435292337552,
      "learning_rate": 1.40067012884239e-07,
      "loss": 1.6498,
      "step": 510208
    },
    {
      "epoch": 0.0018517622240912756,
      "grad_norm": 13149.858706465253,
      "learning_rate": 1.4006261638365694e-07,
      "loss": 1.6562,
      "step": 510240
    },
    {
      "epoch": 0.0018518783584421123,
      "grad_norm": 10208.234323329378,
      "learning_rate": 1.4005822029704823e-07,
      "loss": 1.656,
      "step": 510272
    },
    {
      "epoch": 0.001851994492792949,
      "grad_norm": 7833.499728729172,
      "learning_rate": 1.4005382462434792e-07,
      "loss": 1.6754,
      "step": 510304
    },
    {
      "epoch": 0.0018521106271437856,
      "grad_norm": 10947.431479575473,
      "learning_rate": 1.400494293654911e-07,
      "loss": 1.7137,
      "step": 510336
    },
    {
      "epoch": 0.0018522267614946224,
      "grad_norm": 11153.812263078486,
      "learning_rate": 1.4004503452041273e-07,
      "loss": 1.6978,
      "step": 510368
    },
    {
      "epoch": 0.001852342895845459,
      "grad_norm": 8720.272243456622,
      "learning_rate": 1.40040640089048e-07,
      "loss": 1.7002,
      "step": 510400
    },
    {
      "epoch": 0.0018524590301962959,
      "grad_norm": 8451.13696492963,
      "learning_rate": 1.4003624607133197e-07,
      "loss": 1.6986,
      "step": 510432
    },
    {
      "epoch": 0.0018525751645471324,
      "grad_norm": 14069.547825001342,
      "learning_rate": 1.4003185246719975e-07,
      "loss": 1.6775,
      "step": 510464
    },
    {
      "epoch": 0.0018526912988979692,
      "grad_norm": 11004.814219240596,
      "learning_rate": 1.4002745927658642e-07,
      "loss": 1.6776,
      "step": 510496
    },
    {
      "epoch": 0.001852807433248806,
      "grad_norm": 11532.715031595986,
      "learning_rate": 1.4002306649942718e-07,
      "loss": 1.6565,
      "step": 510528
    },
    {
      "epoch": 0.0018529235675996427,
      "grad_norm": 9304.29750169243,
      "learning_rate": 1.4001867413565716e-07,
      "loss": 1.6583,
      "step": 510560
    },
    {
      "epoch": 0.0018530397019504794,
      "grad_norm": 9180.855515691335,
      "learning_rate": 1.400142821852115e-07,
      "loss": 1.6598,
      "step": 510592
    },
    {
      "epoch": 0.001853155836301316,
      "grad_norm": 8565.557541689857,
      "learning_rate": 1.400098906480254e-07,
      "loss": 1.6836,
      "step": 510624
    },
    {
      "epoch": 0.0018532719706521527,
      "grad_norm": 12674.698102913537,
      "learning_rate": 1.4000549952403408e-07,
      "loss": 1.6815,
      "step": 510656
    },
    {
      "epoch": 0.0018533881050029895,
      "grad_norm": 9422.476319949019,
      "learning_rate": 1.4000110881317273e-07,
      "loss": 1.6859,
      "step": 510688
    },
    {
      "epoch": 0.0018535042393538262,
      "grad_norm": 10452.308070469411,
      "learning_rate": 1.3999671851537658e-07,
      "loss": 1.6845,
      "step": 510720
    },
    {
      "epoch": 0.0018536203737046628,
      "grad_norm": 9790.647373897193,
      "learning_rate": 1.399923286305808e-07,
      "loss": 1.6538,
      "step": 510752
    },
    {
      "epoch": 0.0018537365080554995,
      "grad_norm": 9720.84214458809,
      "learning_rate": 1.3998793915872075e-07,
      "loss": 1.6573,
      "step": 510784
    },
    {
      "epoch": 0.0018538526424063363,
      "grad_norm": 9595.191399862744,
      "learning_rate": 1.3998355009973164e-07,
      "loss": 1.672,
      "step": 510816
    },
    {
      "epoch": 0.001853968776757173,
      "grad_norm": 11451.144222303725,
      "learning_rate": 1.3997916145354876e-07,
      "loss": 1.69,
      "step": 510848
    },
    {
      "epoch": 0.0018540849111080098,
      "grad_norm": 12362.359321747608,
      "learning_rate": 1.3997477322010737e-07,
      "loss": 1.6915,
      "step": 510880
    },
    {
      "epoch": 0.0018542010454588463,
      "grad_norm": 8596.814526323107,
      "learning_rate": 1.3997038539934284e-07,
      "loss": 1.6541,
      "step": 510912
    },
    {
      "epoch": 0.001854317179809683,
      "grad_norm": 9512.437542501923,
      "learning_rate": 1.3996599799119045e-07,
      "loss": 1.6464,
      "step": 510944
    },
    {
      "epoch": 0.0018544333141605198,
      "grad_norm": 12808.542305820753,
      "learning_rate": 1.3996161099558554e-07,
      "loss": 1.6583,
      "step": 510976
    },
    {
      "epoch": 0.0018545494485113566,
      "grad_norm": 13688.580203951029,
      "learning_rate": 1.3995722441246347e-07,
      "loss": 1.6552,
      "step": 511008
    },
    {
      "epoch": 0.001854665582862193,
      "grad_norm": 8498.47386299446,
      "learning_rate": 1.399528382417596e-07,
      "loss": 1.6312,
      "step": 511040
    },
    {
      "epoch": 0.0018547817172130299,
      "grad_norm": 9656.676861115318,
      "learning_rate": 1.3994845248340933e-07,
      "loss": 1.638,
      "step": 511072
    },
    {
      "epoch": 0.0018548978515638666,
      "grad_norm": 10302.51619751214,
      "learning_rate": 1.39944067137348e-07,
      "loss": 1.6574,
      "step": 511104
    },
    {
      "epoch": 0.0018550139859147034,
      "grad_norm": 9999.713395892904,
      "learning_rate": 1.399396822035111e-07,
      "loss": 1.6702,
      "step": 511136
    },
    {
      "epoch": 0.0018551301202655401,
      "grad_norm": 10321.832007933475,
      "learning_rate": 1.39935297681834e-07,
      "loss": 1.658,
      "step": 511168
    },
    {
      "epoch": 0.0018552462546163767,
      "grad_norm": 18362.97753633653,
      "learning_rate": 1.3993091357225214e-07,
      "loss": 1.675,
      "step": 511200
    },
    {
      "epoch": 0.0018553623889672134,
      "grad_norm": 16418.7025065929,
      "learning_rate": 1.3992652987470098e-07,
      "loss": 1.682,
      "step": 511232
    },
    {
      "epoch": 0.0018554785233180502,
      "grad_norm": 24970.837390844543,
      "learning_rate": 1.3992214658911597e-07,
      "loss": 1.6771,
      "step": 511264
    },
    {
      "epoch": 0.001855594657668887,
      "grad_norm": 20470.289690182697,
      "learning_rate": 1.3991776371543257e-07,
      "loss": 1.6704,
      "step": 511296
    },
    {
      "epoch": 0.0018557107920197235,
      "grad_norm": 21608.523873693917,
      "learning_rate": 1.3991338125358633e-07,
      "loss": 1.6862,
      "step": 511328
    },
    {
      "epoch": 0.0018558269263705602,
      "grad_norm": 24030.23761846728,
      "learning_rate": 1.399089992035127e-07,
      "loss": 1.6761,
      "step": 511360
    },
    {
      "epoch": 0.001855943060721397,
      "grad_norm": 18872.104281187087,
      "learning_rate": 1.3990461756514726e-07,
      "loss": 1.6651,
      "step": 511392
    },
    {
      "epoch": 0.0018560591950722337,
      "grad_norm": 8585.395972230985,
      "learning_rate": 1.3990037324553024e-07,
      "loss": 1.6662,
      "step": 511424
    },
    {
      "epoch": 0.0018561753294230705,
      "grad_norm": 10112.648515596693,
      "learning_rate": 1.3989599241752686e-07,
      "loss": 1.6531,
      "step": 511456
    },
    {
      "epoch": 0.001856291463773907,
      "grad_norm": 11383.018316773456,
      "learning_rate": 1.3989161200104028e-07,
      "loss": 1.6603,
      "step": 511488
    },
    {
      "epoch": 0.0018564075981247438,
      "grad_norm": 11057.002667992805,
      "learning_rate": 1.398872319960061e-07,
      "loss": 1.6528,
      "step": 511520
    },
    {
      "epoch": 0.0018565237324755805,
      "grad_norm": 9567.538450406144,
      "learning_rate": 1.3988285240235987e-07,
      "loss": 1.6446,
      "step": 511552
    },
    {
      "epoch": 0.0018566398668264173,
      "grad_norm": 11861.868486878448,
      "learning_rate": 1.3987847322003727e-07,
      "loss": 1.6472,
      "step": 511584
    },
    {
      "epoch": 0.0018567560011772538,
      "grad_norm": 9168.49246059569,
      "learning_rate": 1.3987409444897389e-07,
      "loss": 1.6572,
      "step": 511616
    },
    {
      "epoch": 0.0018568721355280906,
      "grad_norm": 9718.332161435932,
      "learning_rate": 1.3986971608910532e-07,
      "loss": 1.6633,
      "step": 511648
    },
    {
      "epoch": 0.0018569882698789273,
      "grad_norm": 10397.335043173323,
      "learning_rate": 1.3986533814036725e-07,
      "loss": 1.6377,
      "step": 511680
    },
    {
      "epoch": 0.001857104404229764,
      "grad_norm": 16210.371494817755,
      "learning_rate": 1.3986096060269536e-07,
      "loss": 1.6441,
      "step": 511712
    },
    {
      "epoch": 0.0018572205385806008,
      "grad_norm": 10163.304777482568,
      "learning_rate": 1.3985658347602527e-07,
      "loss": 1.6641,
      "step": 511744
    },
    {
      "epoch": 0.0018573366729314374,
      "grad_norm": 9814.498458912713,
      "learning_rate": 1.3985220676029273e-07,
      "loss": 1.6645,
      "step": 511776
    },
    {
      "epoch": 0.0018574528072822741,
      "grad_norm": 9680.974124539327,
      "learning_rate": 1.3984783045543338e-07,
      "loss": 1.6598,
      "step": 511808
    },
    {
      "epoch": 0.0018575689416331109,
      "grad_norm": 8609.523796354824,
      "learning_rate": 1.3984345456138297e-07,
      "loss": 1.6693,
      "step": 511840
    },
    {
      "epoch": 0.0018576850759839476,
      "grad_norm": 12303.551682339536,
      "learning_rate": 1.3983907907807725e-07,
      "loss": 1.6658,
      "step": 511872
    },
    {
      "epoch": 0.0018578012103347842,
      "grad_norm": 9478.571622349013,
      "learning_rate": 1.3983470400545195e-07,
      "loss": 1.6553,
      "step": 511904
    },
    {
      "epoch": 0.001857917344685621,
      "grad_norm": 10778.874338260002,
      "learning_rate": 1.3983032934344285e-07,
      "loss": 1.6633,
      "step": 511936
    },
    {
      "epoch": 0.0018580334790364577,
      "grad_norm": 8824.52831600647,
      "learning_rate": 1.3982595509198568e-07,
      "loss": 1.6608,
      "step": 511968
    },
    {
      "epoch": 0.0018581496133872944,
      "grad_norm": 8269.63046332785,
      "learning_rate": 1.3982158125101624e-07,
      "loss": 1.6663,
      "step": 512000
    },
    {
      "epoch": 0.0018582657477381312,
      "grad_norm": 8636.788523519608,
      "learning_rate": 1.3981720782047036e-07,
      "loss": 1.652,
      "step": 512032
    },
    {
      "epoch": 0.0018583818820889677,
      "grad_norm": 13193.799528566438,
      "learning_rate": 1.3981283480028386e-07,
      "loss": 1.6463,
      "step": 512064
    },
    {
      "epoch": 0.0018584980164398045,
      "grad_norm": 8200.637292308445,
      "learning_rate": 1.3980846219039253e-07,
      "loss": 1.6552,
      "step": 512096
    },
    {
      "epoch": 0.0018586141507906412,
      "grad_norm": 8137.017389682782,
      "learning_rate": 1.3980408999073225e-07,
      "loss": 1.6798,
      "step": 512128
    },
    {
      "epoch": 0.001858730285141478,
      "grad_norm": 10619.559689553987,
      "learning_rate": 1.3979971820123886e-07,
      "loss": 1.6823,
      "step": 512160
    },
    {
      "epoch": 0.0018588464194923145,
      "grad_norm": 9996.282709087414,
      "learning_rate": 1.3979534682184822e-07,
      "loss": 1.6509,
      "step": 512192
    },
    {
      "epoch": 0.0018589625538431513,
      "grad_norm": 8987.866265137684,
      "learning_rate": 1.3979097585249626e-07,
      "loss": 1.6507,
      "step": 512224
    },
    {
      "epoch": 0.001859078688193988,
      "grad_norm": 9561.356598307586,
      "learning_rate": 1.3978660529311885e-07,
      "loss": 1.6535,
      "step": 512256
    },
    {
      "epoch": 0.0018591948225448248,
      "grad_norm": 8733.789441015853,
      "learning_rate": 1.397822351436519e-07,
      "loss": 1.6601,
      "step": 512288
    },
    {
      "epoch": 0.0018593109568956615,
      "grad_norm": 9562.341345089078,
      "learning_rate": 1.3977786540403136e-07,
      "loss": 1.6754,
      "step": 512320
    },
    {
      "epoch": 0.001859427091246498,
      "grad_norm": 10859.88029399956,
      "learning_rate": 1.3977349607419317e-07,
      "loss": 1.6824,
      "step": 512352
    },
    {
      "epoch": 0.0018595432255973348,
      "grad_norm": 13619.97562406042,
      "learning_rate": 1.3976912715407328e-07,
      "loss": 1.6682,
      "step": 512384
    },
    {
      "epoch": 0.0018596593599481716,
      "grad_norm": 9350.472929215934,
      "learning_rate": 1.3976475864360767e-07,
      "loss": 1.6635,
      "step": 512416
    },
    {
      "epoch": 0.0018597754942990083,
      "grad_norm": 22519.527526127185,
      "learning_rate": 1.3976039054273227e-07,
      "loss": 1.6687,
      "step": 512448
    },
    {
      "epoch": 0.0018598916286498449,
      "grad_norm": 17613.89951146537,
      "learning_rate": 1.3975602285138317e-07,
      "loss": 1.6726,
      "step": 512480
    },
    {
      "epoch": 0.0018600077630006816,
      "grad_norm": 18628.34013002769,
      "learning_rate": 1.3975165556949631e-07,
      "loss": 1.6659,
      "step": 512512
    },
    {
      "epoch": 0.0018601238973515184,
      "grad_norm": 19473.77806179376,
      "learning_rate": 1.3974728869700777e-07,
      "loss": 1.6331,
      "step": 512544
    },
    {
      "epoch": 0.0018602400317023551,
      "grad_norm": 22146.73646386754,
      "learning_rate": 1.3974292223385353e-07,
      "loss": 1.6368,
      "step": 512576
    },
    {
      "epoch": 0.0018603561660531919,
      "grad_norm": 18757.264192840063,
      "learning_rate": 1.397385561799697e-07,
      "loss": 1.6321,
      "step": 512608
    },
    {
      "epoch": 0.0018604723004040284,
      "grad_norm": 26401.277847861835,
      "learning_rate": 1.3973419053529234e-07,
      "loss": 1.6619,
      "step": 512640
    },
    {
      "epoch": 0.0018605884347548652,
      "grad_norm": 9324.014800503053,
      "learning_rate": 1.397299617071755e-07,
      "loss": 1.6509,
      "step": 512672
    },
    {
      "epoch": 0.001860704569105702,
      "grad_norm": 10336.841683996132,
      "learning_rate": 1.397255968679366e-07,
      "loss": 1.6376,
      "step": 512704
    },
    {
      "epoch": 0.0018608207034565387,
      "grad_norm": 16908.84277530547,
      "learning_rate": 1.3972123243771442e-07,
      "loss": 1.6431,
      "step": 512736
    },
    {
      "epoch": 0.0018609368378073752,
      "grad_norm": 8890.47580279031,
      "learning_rate": 1.397168684164451e-07,
      "loss": 1.6564,
      "step": 512768
    },
    {
      "epoch": 0.001861052972158212,
      "grad_norm": 11607.68676352011,
      "learning_rate": 1.397125048040648e-07,
      "loss": 1.6567,
      "step": 512800
    },
    {
      "epoch": 0.0018611691065090487,
      "grad_norm": 10782.282875161456,
      "learning_rate": 1.3970814160050962e-07,
      "loss": 1.6836,
      "step": 512832
    },
    {
      "epoch": 0.0018612852408598855,
      "grad_norm": 8062.696323191144,
      "learning_rate": 1.397037788057158e-07,
      "loss": 1.7019,
      "step": 512864
    },
    {
      "epoch": 0.0018614013752107222,
      "grad_norm": 10775.145474656016,
      "learning_rate": 1.396994164196195e-07,
      "loss": 1.6831,
      "step": 512896
    },
    {
      "epoch": 0.0018615175095615588,
      "grad_norm": 9551.461144767329,
      "learning_rate": 1.3969505444215685e-07,
      "loss": 1.6701,
      "step": 512928
    },
    {
      "epoch": 0.0018616336439123955,
      "grad_norm": 9358.412472209162,
      "learning_rate": 1.3969069287326415e-07,
      "loss": 1.6834,
      "step": 512960
    },
    {
      "epoch": 0.0018617497782632323,
      "grad_norm": 12062.983627610542,
      "learning_rate": 1.396863317128776e-07,
      "loss": 1.6863,
      "step": 512992
    },
    {
      "epoch": 0.001861865912614069,
      "grad_norm": 10218.587084328245,
      "learning_rate": 1.3968197096093338e-07,
      "loss": 1.6926,
      "step": 513024
    },
    {
      "epoch": 0.0018619820469649056,
      "grad_norm": 10328.700208641938,
      "learning_rate": 1.3967761061736778e-07,
      "loss": 1.6636,
      "step": 513056
    },
    {
      "epoch": 0.0018620981813157423,
      "grad_norm": 8389.160148668041,
      "learning_rate": 1.3967325068211708e-07,
      "loss": 1.6478,
      "step": 513088
    },
    {
      "epoch": 0.001862214315666579,
      "grad_norm": 8840.635723747473,
      "learning_rate": 1.3966889115511752e-07,
      "loss": 1.6472,
      "step": 513120
    },
    {
      "epoch": 0.0018623304500174158,
      "grad_norm": 10125.967805597646,
      "learning_rate": 1.396645320363054e-07,
      "loss": 1.6641,
      "step": 513152
    },
    {
      "epoch": 0.0018624465843682526,
      "grad_norm": 12055.626072502417,
      "learning_rate": 1.3966017332561705e-07,
      "loss": 1.6416,
      "step": 513184
    },
    {
      "epoch": 0.0018625627187190891,
      "grad_norm": 9467.324754121408,
      "learning_rate": 1.396558150229888e-07,
      "loss": 1.6424,
      "step": 513216
    },
    {
      "epoch": 0.0018626788530699259,
      "grad_norm": 10408.955182918216,
      "learning_rate": 1.396514571283569e-07,
      "loss": 1.6413,
      "step": 513248
    },
    {
      "epoch": 0.0018627949874207626,
      "grad_norm": 9270.239155491081,
      "learning_rate": 1.396470996416578e-07,
      "loss": 1.6508,
      "step": 513280
    },
    {
      "epoch": 0.0018629111217715994,
      "grad_norm": 10519.114411394146,
      "learning_rate": 1.3964274256282782e-07,
      "loss": 1.6698,
      "step": 513312
    },
    {
      "epoch": 0.001863027256122436,
      "grad_norm": 11165.470343877145,
      "learning_rate": 1.396383858918033e-07,
      "loss": 1.6872,
      "step": 513344
    },
    {
      "epoch": 0.0018631433904732727,
      "grad_norm": 9901.368996255012,
      "learning_rate": 1.3963402962852067e-07,
      "loss": 1.6887,
      "step": 513376
    },
    {
      "epoch": 0.0018632595248241094,
      "grad_norm": 9298.629791533804,
      "learning_rate": 1.3962967377291634e-07,
      "loss": 1.677,
      "step": 513408
    },
    {
      "epoch": 0.0018633756591749462,
      "grad_norm": 10359.862547350713,
      "learning_rate": 1.396253183249267e-07,
      "loss": 1.6615,
      "step": 513440
    },
    {
      "epoch": 0.001863491793525783,
      "grad_norm": 10086.076541450595,
      "learning_rate": 1.3962096328448817e-07,
      "loss": 1.6488,
      "step": 513472
    },
    {
      "epoch": 0.0018636079278766195,
      "grad_norm": 17680.30678466864,
      "learning_rate": 1.3961660865153725e-07,
      "loss": 1.6613,
      "step": 513504
    },
    {
      "epoch": 0.0018637240622274562,
      "grad_norm": 10258.119125843685,
      "learning_rate": 1.3961225442601034e-07,
      "loss": 1.6513,
      "step": 513536
    },
    {
      "epoch": 0.001863840196578293,
      "grad_norm": 10530.271411506923,
      "learning_rate": 1.3960790060784394e-07,
      "loss": 1.6484,
      "step": 513568
    },
    {
      "epoch": 0.0018639563309291297,
      "grad_norm": 8993.577041422395,
      "learning_rate": 1.3960354719697456e-07,
      "loss": 1.6429,
      "step": 513600
    },
    {
      "epoch": 0.0018640724652799663,
      "grad_norm": 8340.444232773216,
      "learning_rate": 1.3959919419333863e-07,
      "loss": 1.6587,
      "step": 513632
    },
    {
      "epoch": 0.001864188599630803,
      "grad_norm": 16687.090579247182,
      "learning_rate": 1.3959484159687276e-07,
      "loss": 1.6624,
      "step": 513664
    },
    {
      "epoch": 0.0018643047339816398,
      "grad_norm": 20220.636191772006,
      "learning_rate": 1.3959048940751337e-07,
      "loss": 1.6378,
      "step": 513696
    },
    {
      "epoch": 0.0018644208683324765,
      "grad_norm": 20028.948249970592,
      "learning_rate": 1.3958613762519707e-07,
      "loss": 1.646,
      "step": 513728
    },
    {
      "epoch": 0.0018645370026833133,
      "grad_norm": 19210.557514033786,
      "learning_rate": 1.395817862498604e-07,
      "loss": 1.6534,
      "step": 513760
    },
    {
      "epoch": 0.0018646531370341498,
      "grad_norm": 19328.786407842577,
      "learning_rate": 1.3957743528143995e-07,
      "loss": 1.6648,
      "step": 513792
    },
    {
      "epoch": 0.0018647692713849866,
      "grad_norm": 16557.946007883947,
      "learning_rate": 1.3957308471987227e-07,
      "loss": 1.6818,
      "step": 513824
    },
    {
      "epoch": 0.0018648854057358233,
      "grad_norm": 17213.840942683302,
      "learning_rate": 1.3956873456509397e-07,
      "loss": 1.7079,
      "step": 513856
    },
    {
      "epoch": 0.00186500154008666,
      "grad_norm": 21080.19999905124,
      "learning_rate": 1.3956438481704166e-07,
      "loss": 1.7221,
      "step": 513888
    },
    {
      "epoch": 0.0018651176744374966,
      "grad_norm": 20705.06836501633,
      "learning_rate": 1.3956003547565194e-07,
      "loss": 1.6993,
      "step": 513920
    },
    {
      "epoch": 0.0018652338087883334,
      "grad_norm": 12725.642616386805,
      "learning_rate": 1.3955582243891977e-07,
      "loss": 1.6943,
      "step": 513952
    },
    {
      "epoch": 0.0018653499431391701,
      "grad_norm": 12390.63146090626,
      "learning_rate": 1.3955147389796194e-07,
      "loss": 1.6566,
      "step": 513984
    },
    {
      "epoch": 0.001865466077490007,
      "grad_norm": 20310.059970369366,
      "learning_rate": 1.3954712576347868e-07,
      "loss": 1.6629,
      "step": 514016
    },
    {
      "epoch": 0.0018655822118408437,
      "grad_norm": 14461.362868001064,
      "learning_rate": 1.3954277803540662e-07,
      "loss": 1.6494,
      "step": 514048
    },
    {
      "epoch": 0.0018656983461916802,
      "grad_norm": 10728.064503907497,
      "learning_rate": 1.395384307136825e-07,
      "loss": 1.6363,
      "step": 514080
    },
    {
      "epoch": 0.001865814480542517,
      "grad_norm": 8369.321477873818,
      "learning_rate": 1.3953408379824302e-07,
      "loss": 1.6491,
      "step": 514112
    },
    {
      "epoch": 0.0018659306148933537,
      "grad_norm": 7597.16763011058,
      "learning_rate": 1.3952973728902488e-07,
      "loss": 1.6563,
      "step": 514144
    },
    {
      "epoch": 0.0018660467492441905,
      "grad_norm": 13037.311992891788,
      "learning_rate": 1.3952539118596483e-07,
      "loss": 1.6628,
      "step": 514176
    },
    {
      "epoch": 0.001866162883595027,
      "grad_norm": 12328.537788399726,
      "learning_rate": 1.395210454889996e-07,
      "loss": 1.6507,
      "step": 514208
    },
    {
      "epoch": 0.0018662790179458637,
      "grad_norm": 8637.426931673575,
      "learning_rate": 1.39516700198066e-07,
      "loss": 1.6485,
      "step": 514240
    },
    {
      "epoch": 0.0018663951522967005,
      "grad_norm": 14711.672440616667,
      "learning_rate": 1.3951235531310073e-07,
      "loss": 1.6456,
      "step": 514272
    },
    {
      "epoch": 0.0018665112866475373,
      "grad_norm": 8294.68010233065,
      "learning_rate": 1.3950801083404064e-07,
      "loss": 1.6397,
      "step": 514304
    },
    {
      "epoch": 0.001866627420998374,
      "grad_norm": 9057.196034093553,
      "learning_rate": 1.3950366676082253e-07,
      "loss": 1.6588,
      "step": 514336
    },
    {
      "epoch": 0.0018667435553492105,
      "grad_norm": 9711.045772727053,
      "learning_rate": 1.394993230933832e-07,
      "loss": 1.6708,
      "step": 514368
    },
    {
      "epoch": 0.0018668596897000473,
      "grad_norm": 13942.769595743881,
      "learning_rate": 1.3949497983165948e-07,
      "loss": 1.6716,
      "step": 514400
    },
    {
      "epoch": 0.001866975824050884,
      "grad_norm": 9846.09282913786,
      "learning_rate": 1.3949063697558821e-07,
      "loss": 1.6668,
      "step": 514432
    },
    {
      "epoch": 0.0018670919584017208,
      "grad_norm": 10462.373153352924,
      "learning_rate": 1.3948629452510626e-07,
      "loss": 1.6867,
      "step": 514464
    },
    {
      "epoch": 0.0018672080927525573,
      "grad_norm": 11937.097804742994,
      "learning_rate": 1.3948195248015052e-07,
      "loss": 1.6811,
      "step": 514496
    },
    {
      "epoch": 0.001867324227103394,
      "grad_norm": 13056.938768333104,
      "learning_rate": 1.3947761084065785e-07,
      "loss": 1.6632,
      "step": 514528
    },
    {
      "epoch": 0.0018674403614542309,
      "grad_norm": 9026.107245097412,
      "learning_rate": 1.3947326960656513e-07,
      "loss": 1.6449,
      "step": 514560
    },
    {
      "epoch": 0.0018675564958050676,
      "grad_norm": 9345.238359720955,
      "learning_rate": 1.3946892877780933e-07,
      "loss": 1.65,
      "step": 514592
    },
    {
      "epoch": 0.0018676726301559044,
      "grad_norm": 8813.086179086189,
      "learning_rate": 1.3946458835432733e-07,
      "loss": 1.6592,
      "step": 514624
    },
    {
      "epoch": 0.001867788764506741,
      "grad_norm": 9966.350585846356,
      "learning_rate": 1.394602483360561e-07,
      "loss": 1.6965,
      "step": 514656
    },
    {
      "epoch": 0.0018679048988575777,
      "grad_norm": 12030.738131968463,
      "learning_rate": 1.394559087229326e-07,
      "loss": 1.6725,
      "step": 514688
    },
    {
      "epoch": 0.0018680210332084144,
      "grad_norm": 8993.409809410445,
      "learning_rate": 1.3945156951489375e-07,
      "loss": 1.659,
      "step": 514720
    },
    {
      "epoch": 0.0018681371675592512,
      "grad_norm": 8993.661101019985,
      "learning_rate": 1.3944723071187656e-07,
      "loss": 1.6649,
      "step": 514752
    },
    {
      "epoch": 0.0018682533019100877,
      "grad_norm": 8545.861103481615,
      "learning_rate": 1.3944289231381808e-07,
      "loss": 1.6747,
      "step": 514784
    },
    {
      "epoch": 0.0018683694362609245,
      "grad_norm": 14060.797416931942,
      "learning_rate": 1.394385543206552e-07,
      "loss": 1.6721,
      "step": 514816
    },
    {
      "epoch": 0.0018684855706117612,
      "grad_norm": 8965.704657192317,
      "learning_rate": 1.3943421673232506e-07,
      "loss": 1.6649,
      "step": 514848
    },
    {
      "epoch": 0.001868601704962598,
      "grad_norm": 11369.975549665884,
      "learning_rate": 1.3942987954876465e-07,
      "loss": 1.6688,
      "step": 514880
    },
    {
      "epoch": 0.0018687178393134347,
      "grad_norm": 10594.685460172945,
      "learning_rate": 1.3942554276991102e-07,
      "loss": 1.6657,
      "step": 514912
    },
    {
      "epoch": 0.0018688339736642713,
      "grad_norm": 10572.41353712576,
      "learning_rate": 1.3942120639570123e-07,
      "loss": 1.6729,
      "step": 514944
    },
    {
      "epoch": 0.001868950108015108,
      "grad_norm": 12854.666078899132,
      "learning_rate": 1.3941700591899986e-07,
      "loss": 1.675,
      "step": 514976
    },
    {
      "epoch": 0.0018690662423659448,
      "grad_norm": 10074.99022332032,
      "learning_rate": 1.394126703412488e-07,
      "loss": 1.6732,
      "step": 515008
    },
    {
      "epoch": 0.0018691823767167815,
      "grad_norm": 8978.747685507149,
      "learning_rate": 1.394083351679548e-07,
      "loss": 1.6858,
      "step": 515040
    },
    {
      "epoch": 0.001869298511067618,
      "grad_norm": 10983.452007451939,
      "learning_rate": 1.39404000399055e-07,
      "loss": 1.6452,
      "step": 515072
    },
    {
      "epoch": 0.0018694146454184548,
      "grad_norm": 10163.639997559929,
      "learning_rate": 1.3939966603448655e-07,
      "loss": 1.6578,
      "step": 515104
    },
    {
      "epoch": 0.0018695307797692916,
      "grad_norm": 9222.716085839354,
      "learning_rate": 1.3939533207418659e-07,
      "loss": 1.6482,
      "step": 515136
    },
    {
      "epoch": 0.0018696469141201283,
      "grad_norm": 9111.960381827832,
      "learning_rate": 1.3939099851809228e-07,
      "loss": 1.6663,
      "step": 515168
    },
    {
      "epoch": 0.001869763048470965,
      "grad_norm": 8427.12584455697,
      "learning_rate": 1.3938666536614077e-07,
      "loss": 1.656,
      "step": 515200
    },
    {
      "epoch": 0.0018698791828218016,
      "grad_norm": 10230.727149132656,
      "learning_rate": 1.393823326182693e-07,
      "loss": 1.6405,
      "step": 515232
    },
    {
      "epoch": 0.0018699953171726384,
      "grad_norm": 10552.139119628779,
      "learning_rate": 1.3937800027441498e-07,
      "loss": 1.6516,
      "step": 515264
    },
    {
      "epoch": 0.0018701114515234751,
      "grad_norm": 8731.708767475013,
      "learning_rate": 1.3937366833451512e-07,
      "loss": 1.6576,
      "step": 515296
    },
    {
      "epoch": 0.0018702275858743119,
      "grad_norm": 9787.966898186773,
      "learning_rate": 1.3936933679850694e-07,
      "loss": 1.6627,
      "step": 515328
    },
    {
      "epoch": 0.0018703437202251484,
      "grad_norm": 9412.364102604615,
      "learning_rate": 1.3936500566632762e-07,
      "loss": 1.6775,
      "step": 515360
    },
    {
      "epoch": 0.0018704598545759852,
      "grad_norm": 13052.31941074076,
      "learning_rate": 1.3936067493791443e-07,
      "loss": 1.6869,
      "step": 515392
    },
    {
      "epoch": 0.001870575988926822,
      "grad_norm": 10218.82556852792,
      "learning_rate": 1.3935634461320468e-07,
      "loss": 1.6929,
      "step": 515424
    },
    {
      "epoch": 0.0018706921232776587,
      "grad_norm": 9598.845868123939,
      "learning_rate": 1.3935201469213565e-07,
      "loss": 1.6978,
      "step": 515456
    },
    {
      "epoch": 0.0018708082576284954,
      "grad_norm": 8454.123017794336,
      "learning_rate": 1.393476851746446e-07,
      "loss": 1.6945,
      "step": 515488
    },
    {
      "epoch": 0.001870924391979332,
      "grad_norm": 8504.104185627079,
      "learning_rate": 1.3934335606066884e-07,
      "loss": 1.7032,
      "step": 515520
    },
    {
      "epoch": 0.0018710405263301687,
      "grad_norm": 8519.639194238216,
      "learning_rate": 1.3933902735014573e-07,
      "loss": 1.718,
      "step": 515552
    },
    {
      "epoch": 0.0018711566606810055,
      "grad_norm": 9689.329388559354,
      "learning_rate": 1.3933469904301256e-07,
      "loss": 1.7158,
      "step": 515584
    },
    {
      "epoch": 0.0018712727950318422,
      "grad_norm": 10068.637445056804,
      "learning_rate": 1.3933037113920675e-07,
      "loss": 1.6958,
      "step": 515616
    },
    {
      "epoch": 0.0018713889293826788,
      "grad_norm": 9850.107004494927,
      "learning_rate": 1.393260436386656e-07,
      "loss": 1.7057,
      "step": 515648
    },
    {
      "epoch": 0.0018715050637335155,
      "grad_norm": 10192.975620494733,
      "learning_rate": 1.3932171654132654e-07,
      "loss": 1.7011,
      "step": 515680
    },
    {
      "epoch": 0.0018716211980843523,
      "grad_norm": 10053.86691775856,
      "learning_rate": 1.3931738984712693e-07,
      "loss": 1.697,
      "step": 515712
    },
    {
      "epoch": 0.001871737332435189,
      "grad_norm": 8287.599169844063,
      "learning_rate": 1.3931306355600417e-07,
      "loss": 1.6858,
      "step": 515744
    },
    {
      "epoch": 0.0018718534667860258,
      "grad_norm": 10353.329126421126,
      "learning_rate": 1.3930873766789566e-07,
      "loss": 1.6736,
      "step": 515776
    },
    {
      "epoch": 0.0018719696011368623,
      "grad_norm": 12311.932585910305,
      "learning_rate": 1.3930441218273891e-07,
      "loss": 1.6803,
      "step": 515808
    },
    {
      "epoch": 0.001872085735487699,
      "grad_norm": 13329.105071234153,
      "learning_rate": 1.393000871004713e-07,
      "loss": 1.6526,
      "step": 515840
    },
    {
      "epoch": 0.0018722018698385358,
      "grad_norm": 9308.39470585557,
      "learning_rate": 1.3929576242103032e-07,
      "loss": 1.652,
      "step": 515872
    },
    {
      "epoch": 0.0018723180041893726,
      "grad_norm": 8881.67439168989,
      "learning_rate": 1.3929143814435342e-07,
      "loss": 1.6592,
      "step": 515904
    },
    {
      "epoch": 0.0018724341385402091,
      "grad_norm": 8141.985752873805,
      "learning_rate": 1.392871142703781e-07,
      "loss": 1.6628,
      "step": 515936
    },
    {
      "epoch": 0.0018725502728910459,
      "grad_norm": 22506.785110272856,
      "learning_rate": 1.3928279079904186e-07,
      "loss": 1.6725,
      "step": 515968
    },
    {
      "epoch": 0.0018726664072418826,
      "grad_norm": 8183.578801477994,
      "learning_rate": 1.3927860282008787e-07,
      "loss": 1.6604,
      "step": 516000
    },
    {
      "epoch": 0.0018727825415927194,
      "grad_norm": 9867.245005572731,
      "learning_rate": 1.3927428014126473e-07,
      "loss": 1.6629,
      "step": 516032
    },
    {
      "epoch": 0.0018728986759435561,
      "grad_norm": 10615.440452472993,
      "learning_rate": 1.392699578648952e-07,
      "loss": 1.6698,
      "step": 516064
    },
    {
      "epoch": 0.0018730148102943927,
      "grad_norm": 8824.34178848485,
      "learning_rate": 1.3926563599091684e-07,
      "loss": 1.6751,
      "step": 516096
    },
    {
      "epoch": 0.0018731309446452294,
      "grad_norm": 11757.459249344647,
      "learning_rate": 1.3926131451926724e-07,
      "loss": 1.6724,
      "step": 516128
    },
    {
      "epoch": 0.0018732470789960662,
      "grad_norm": 11394.45988188997,
      "learning_rate": 1.3925699344988394e-07,
      "loss": 1.6546,
      "step": 516160
    },
    {
      "epoch": 0.001873363213346903,
      "grad_norm": 11190.954740324883,
      "learning_rate": 1.3925267278270456e-07,
      "loss": 1.6525,
      "step": 516192
    },
    {
      "epoch": 0.0018734793476977395,
      "grad_norm": 11905.006341871473,
      "learning_rate": 1.3924835251766668e-07,
      "loss": 1.6579,
      "step": 516224
    },
    {
      "epoch": 0.0018735954820485762,
      "grad_norm": 10589.354654557566,
      "learning_rate": 1.3924403265470799e-07,
      "loss": 1.6629,
      "step": 516256
    },
    {
      "epoch": 0.001873711616399413,
      "grad_norm": 11747.150633238683,
      "learning_rate": 1.3923971319376605e-07,
      "loss": 1.6736,
      "step": 516288
    },
    {
      "epoch": 0.0018738277507502497,
      "grad_norm": 10956.103686986537,
      "learning_rate": 1.3923539413477857e-07,
      "loss": 1.6566,
      "step": 516320
    },
    {
      "epoch": 0.0018739438851010865,
      "grad_norm": 8833.439420746598,
      "learning_rate": 1.3923107547768316e-07,
      "loss": 1.6618,
      "step": 516352
    },
    {
      "epoch": 0.001874060019451923,
      "grad_norm": 9856.095981675504,
      "learning_rate": 1.392267572224175e-07,
      "loss": 1.6659,
      "step": 516384
    },
    {
      "epoch": 0.0018741761538027598,
      "grad_norm": 9810.6870299689,
      "learning_rate": 1.3922243936891936e-07,
      "loss": 1.6597,
      "step": 516416
    },
    {
      "epoch": 0.0018742922881535965,
      "grad_norm": 9193.472249373464,
      "learning_rate": 1.3921812191712634e-07,
      "loss": 1.669,
      "step": 516448
    },
    {
      "epoch": 0.0018744084225044333,
      "grad_norm": 12368.085057922264,
      "learning_rate": 1.3921380486697623e-07,
      "loss": 1.6741,
      "step": 516480
    },
    {
      "epoch": 0.0018745245568552698,
      "grad_norm": 13636.989183833799,
      "learning_rate": 1.3920948821840671e-07,
      "loss": 1.6742,
      "step": 516512
    },
    {
      "epoch": 0.0018746406912061066,
      "grad_norm": 9098.20355894503,
      "learning_rate": 1.3920517197135557e-07,
      "loss": 1.6817,
      "step": 516544
    },
    {
      "epoch": 0.0018747568255569433,
      "grad_norm": 10892.386331745676,
      "learning_rate": 1.3920085612576053e-07,
      "loss": 1.685,
      "step": 516576
    },
    {
      "epoch": 0.00187487295990778,
      "grad_norm": 10859.765743329826,
      "learning_rate": 1.3919654068155937e-07,
      "loss": 1.6788,
      "step": 516608
    },
    {
      "epoch": 0.0018749890942586168,
      "grad_norm": 9304.011822864371,
      "learning_rate": 1.3919222563868988e-07,
      "loss": 1.6772,
      "step": 516640
    },
    {
      "epoch": 0.0018751052286094534,
      "grad_norm": 13331.928892699661,
      "learning_rate": 1.3918791099708987e-07,
      "loss": 1.6678,
      "step": 516672
    },
    {
      "epoch": 0.0018752213629602901,
      "grad_norm": 9790.088865786664,
      "learning_rate": 1.3918359675669715e-07,
      "loss": 1.648,
      "step": 516704
    },
    {
      "epoch": 0.0018753374973111269,
      "grad_norm": 10363.72577792369,
      "learning_rate": 1.3917928291744952e-07,
      "loss": 1.6735,
      "step": 516736
    },
    {
      "epoch": 0.0018754536316619636,
      "grad_norm": 10357.01974508111,
      "learning_rate": 1.3917496947928485e-07,
      "loss": 1.6628,
      "step": 516768
    },
    {
      "epoch": 0.0018755697660128002,
      "grad_norm": 9662.47111250533,
      "learning_rate": 1.3917065644214095e-07,
      "loss": 1.6494,
      "step": 516800
    },
    {
      "epoch": 0.001875685900363637,
      "grad_norm": 10973.603419114434,
      "learning_rate": 1.3916634380595573e-07,
      "loss": 1.6742,
      "step": 516832
    },
    {
      "epoch": 0.0018758020347144737,
      "grad_norm": 9178.434180185637,
      "learning_rate": 1.3916203157066703e-07,
      "loss": 1.6843,
      "step": 516864
    },
    {
      "epoch": 0.0018759181690653104,
      "grad_norm": 9246.048453258289,
      "learning_rate": 1.391577197362128e-07,
      "loss": 1.6443,
      "step": 516896
    },
    {
      "epoch": 0.0018760343034161472,
      "grad_norm": 10157.42299995427,
      "learning_rate": 1.391534083025309e-07,
      "loss": 1.642,
      "step": 516928
    },
    {
      "epoch": 0.0018761504377669837,
      "grad_norm": 10333.905360511098,
      "learning_rate": 1.3914909726955924e-07,
      "loss": 1.6633,
      "step": 516960
    },
    {
      "epoch": 0.0018762665721178205,
      "grad_norm": 18396.10850152825,
      "learning_rate": 1.3914478663723576e-07,
      "loss": 1.6638,
      "step": 516992
    },
    {
      "epoch": 0.0018763827064686572,
      "grad_norm": 18260.807430122033,
      "learning_rate": 1.3914047640549846e-07,
      "loss": 1.6394,
      "step": 517024
    },
    {
      "epoch": 0.001876498840819494,
      "grad_norm": 20103.064443014653,
      "learning_rate": 1.3913616657428522e-07,
      "loss": 1.6532,
      "step": 517056
    },
    {
      "epoch": 0.0018766149751703305,
      "grad_norm": 8814.13433072131,
      "learning_rate": 1.3913199180718396e-07,
      "loss": 1.6783,
      "step": 517088
    },
    {
      "epoch": 0.0018767311095211673,
      "grad_norm": 10928.430445402486,
      "learning_rate": 1.391276827643213e-07,
      "loss": 1.6654,
      "step": 517120
    },
    {
      "epoch": 0.001876847243872004,
      "grad_norm": 9999.16816540256,
      "learning_rate": 1.3912337412179865e-07,
      "loss": 1.6603,
      "step": 517152
    },
    {
      "epoch": 0.0018769633782228408,
      "grad_norm": 8822.441952203482,
      "learning_rate": 1.39119065879554e-07,
      "loss": 1.6795,
      "step": 517184
    },
    {
      "epoch": 0.0018770795125736775,
      "grad_norm": 10096.489885103634,
      "learning_rate": 1.391147580375254e-07,
      "loss": 1.6894,
      "step": 517216
    },
    {
      "epoch": 0.001877195646924514,
      "grad_norm": 10481.299728564201,
      "learning_rate": 1.3911045059565085e-07,
      "loss": 1.6527,
      "step": 517248
    },
    {
      "epoch": 0.0018773117812753508,
      "grad_norm": 9684.971450654875,
      "learning_rate": 1.3910614355386843e-07,
      "loss": 1.6605,
      "step": 517280
    },
    {
      "epoch": 0.0018774279156261876,
      "grad_norm": 8422.302060600772,
      "learning_rate": 1.3910183691211622e-07,
      "loss": 1.6883,
      "step": 517312
    },
    {
      "epoch": 0.0018775440499770243,
      "grad_norm": 9139.988621437118,
      "learning_rate": 1.3909753067033227e-07,
      "loss": 1.6931,
      "step": 517344
    },
    {
      "epoch": 0.0018776601843278609,
      "grad_norm": 8435.654805644906,
      "learning_rate": 1.3909322482845471e-07,
      "loss": 1.6697,
      "step": 517376
    },
    {
      "epoch": 0.0018777763186786976,
      "grad_norm": 9472.568817380004,
      "learning_rate": 1.3908891938642162e-07,
      "loss": 1.676,
      "step": 517408
    },
    {
      "epoch": 0.0018778924530295344,
      "grad_norm": 12115.0368550822,
      "learning_rate": 1.3908461434417112e-07,
      "loss": 1.6983,
      "step": 517440
    },
    {
      "epoch": 0.0018780085873803711,
      "grad_norm": 10778.130264568155,
      "learning_rate": 1.3908030970164136e-07,
      "loss": 1.6614,
      "step": 517472
    },
    {
      "epoch": 0.001878124721731208,
      "grad_norm": 13079.988532105064,
      "learning_rate": 1.390760054587705e-07,
      "loss": 1.6416,
      "step": 517504
    },
    {
      "epoch": 0.0018782408560820444,
      "grad_norm": 13798.919377980294,
      "learning_rate": 1.3907170161549667e-07,
      "loss": 1.6617,
      "step": 517536
    },
    {
      "epoch": 0.0018783569904328812,
      "grad_norm": 10463.288010945698,
      "learning_rate": 1.3906739817175803e-07,
      "loss": 1.6697,
      "step": 517568
    },
    {
      "epoch": 0.001878473124783718,
      "grad_norm": 9043.646388487334,
      "learning_rate": 1.390630951274928e-07,
      "loss": 1.6499,
      "step": 517600
    },
    {
      "epoch": 0.0018785892591345547,
      "grad_norm": 10229.944672382155,
      "learning_rate": 1.390587924826392e-07,
      "loss": 1.6557,
      "step": 517632
    },
    {
      "epoch": 0.0018787053934853912,
      "grad_norm": 7478.031826623901,
      "learning_rate": 1.3905449023713537e-07,
      "loss": 1.6807,
      "step": 517664
    },
    {
      "epoch": 0.001878821527836228,
      "grad_norm": 10531.721891504732,
      "learning_rate": 1.390501883909196e-07,
      "loss": 1.6856,
      "step": 517696
    },
    {
      "epoch": 0.0018789376621870647,
      "grad_norm": 8761.09308248691,
      "learning_rate": 1.3904588694393015e-07,
      "loss": 1.6853,
      "step": 517728
    },
    {
      "epoch": 0.0018790537965379015,
      "grad_norm": 10046.528156532484,
      "learning_rate": 1.3904158589610523e-07,
      "loss": 1.6812,
      "step": 517760
    },
    {
      "epoch": 0.0018791699308887383,
      "grad_norm": 9257.757233801283,
      "learning_rate": 1.3903728524738312e-07,
      "loss": 1.6702,
      "step": 517792
    },
    {
      "epoch": 0.0018792860652395748,
      "grad_norm": 12256.099705860752,
      "learning_rate": 1.3903298499770208e-07,
      "loss": 1.6561,
      "step": 517824
    },
    {
      "epoch": 0.0018794021995904115,
      "grad_norm": 11794.243511137118,
      "learning_rate": 1.3902868514700044e-07,
      "loss": 1.6464,
      "step": 517856
    },
    {
      "epoch": 0.0018795183339412483,
      "grad_norm": 8802.081571991934,
      "learning_rate": 1.390243856952165e-07,
      "loss": 1.6768,
      "step": 517888
    },
    {
      "epoch": 0.001879634468292085,
      "grad_norm": 11585.420665646976,
      "learning_rate": 1.3902008664228857e-07,
      "loss": 1.6779,
      "step": 517920
    },
    {
      "epoch": 0.0018797506026429216,
      "grad_norm": 10178.160541080102,
      "learning_rate": 1.39015787988155e-07,
      "loss": 1.6394,
      "step": 517952
    },
    {
      "epoch": 0.0018798667369937583,
      "grad_norm": 9798.788496543846,
      "learning_rate": 1.390114897327541e-07,
      "loss": 1.6638,
      "step": 517984
    },
    {
      "epoch": 0.001879982871344595,
      "grad_norm": 9588.793146168084,
      "learning_rate": 1.3900719187602424e-07,
      "loss": 1.6758,
      "step": 518016
    },
    {
      "epoch": 0.0018800990056954318,
      "grad_norm": 12245.924873197615,
      "learning_rate": 1.3900289441790385e-07,
      "loss": 1.6551,
      "step": 518048
    },
    {
      "epoch": 0.0018802151400462686,
      "grad_norm": 17753.721863316434,
      "learning_rate": 1.3899859735833126e-07,
      "loss": 1.6521,
      "step": 518080
    },
    {
      "epoch": 0.0018803312743971051,
      "grad_norm": 23044.177399074153,
      "learning_rate": 1.3899430069724492e-07,
      "loss": 1.6682,
      "step": 518112
    },
    {
      "epoch": 0.001880447408747942,
      "grad_norm": 16338.274817128031,
      "learning_rate": 1.389900044345832e-07,
      "loss": 1.6728,
      "step": 518144
    },
    {
      "epoch": 0.0018805635430987786,
      "grad_norm": 22986.332634850653,
      "learning_rate": 1.3898570857028454e-07,
      "loss": 1.6599,
      "step": 518176
    },
    {
      "epoch": 0.0018806796774496154,
      "grad_norm": 21624.88048521887,
      "learning_rate": 1.389814131042874e-07,
      "loss": 1.6809,
      "step": 518208
    },
    {
      "epoch": 0.001880795811800452,
      "grad_norm": 20893.162900815187,
      "learning_rate": 1.3897711803653022e-07,
      "loss": 1.7226,
      "step": 518240
    },
    {
      "epoch": 0.0018809119461512887,
      "grad_norm": 12843.038425544011,
      "learning_rate": 1.3897295756934928e-07,
      "loss": 1.6982,
      "step": 518272
    },
    {
      "epoch": 0.0018810280805021254,
      "grad_norm": 10930.074473671257,
      "learning_rate": 1.3896866328544722e-07,
      "loss": 1.6784,
      "step": 518304
    },
    {
      "epoch": 0.0018811442148529622,
      "grad_norm": 8596.156583031745,
      "learning_rate": 1.3896436939960245e-07,
      "loss": 1.682,
      "step": 518336
    },
    {
      "epoch": 0.001881260349203799,
      "grad_norm": 9032.130867076716,
      "learning_rate": 1.3896007591175355e-07,
      "loss": 1.6739,
      "step": 518368
    },
    {
      "epoch": 0.0018813764835546355,
      "grad_norm": 12934.481821858963,
      "learning_rate": 1.38955782821839e-07,
      "loss": 1.6414,
      "step": 518400
    },
    {
      "epoch": 0.0018814926179054722,
      "grad_norm": 14032.341857295238,
      "learning_rate": 1.3895149012979733e-07,
      "loss": 1.6559,
      "step": 518432
    },
    {
      "epoch": 0.001881608752256309,
      "grad_norm": 9507.504614776686,
      "learning_rate": 1.389471978355671e-07,
      "loss": 1.6787,
      "step": 518464
    },
    {
      "epoch": 0.0018817248866071458,
      "grad_norm": 8000.900074366634,
      "learning_rate": 1.3894290593908687e-07,
      "loss": 1.6593,
      "step": 518496
    },
    {
      "epoch": 0.0018818410209579823,
      "grad_norm": 11310.857969225854,
      "learning_rate": 1.389386144402952e-07,
      "loss": 1.6436,
      "step": 518528
    },
    {
      "epoch": 0.001881957155308819,
      "grad_norm": 10675.020093657902,
      "learning_rate": 1.389343233391307e-07,
      "loss": 1.6686,
      "step": 518560
    },
    {
      "epoch": 0.0018820732896596558,
      "grad_norm": 9595.772819319975,
      "learning_rate": 1.3893003263553197e-07,
      "loss": 1.6907,
      "step": 518592
    },
    {
      "epoch": 0.0018821894240104926,
      "grad_norm": 11249.256153186307,
      "learning_rate": 1.3892574232943758e-07,
      "loss": 1.6637,
      "step": 518624
    },
    {
      "epoch": 0.0018823055583613293,
      "grad_norm": 9049.611704377156,
      "learning_rate": 1.3892145242078622e-07,
      "loss": 1.6493,
      "step": 518656
    },
    {
      "epoch": 0.0018824216927121658,
      "grad_norm": 8943.47762338566,
      "learning_rate": 1.3891716290951648e-07,
      "loss": 1.6731,
      "step": 518688
    },
    {
      "epoch": 0.0018825378270630026,
      "grad_norm": 8705.197413040098,
      "learning_rate": 1.3891287379556705e-07,
      "loss": 1.6771,
      "step": 518720
    },
    {
      "epoch": 0.0018826539614138394,
      "grad_norm": 10772.766589878387,
      "learning_rate": 1.3890858507887655e-07,
      "loss": 1.6578,
      "step": 518752
    },
    {
      "epoch": 0.0018827700957646761,
      "grad_norm": 8951.469153161395,
      "learning_rate": 1.389042967593837e-07,
      "loss": 1.6848,
      "step": 518784
    },
    {
      "epoch": 0.0018828862301155126,
      "grad_norm": 9753.406994481467,
      "learning_rate": 1.389000088370272e-07,
      "loss": 1.7121,
      "step": 518816
    },
    {
      "epoch": 0.0018830023644663494,
      "grad_norm": 8707.303141616238,
      "learning_rate": 1.3889572131174573e-07,
      "loss": 1.6654,
      "step": 518848
    },
    {
      "epoch": 0.0018831184988171862,
      "grad_norm": 9814.52780321091,
      "learning_rate": 1.3889143418347804e-07,
      "loss": 1.6424,
      "step": 518880
    },
    {
      "epoch": 0.001883234633168023,
      "grad_norm": 8618.262701960297,
      "learning_rate": 1.388871474521628e-07,
      "loss": 1.6665,
      "step": 518912
    },
    {
      "epoch": 0.0018833507675188597,
      "grad_norm": 10646.472655297623,
      "learning_rate": 1.388828611177388e-07,
      "loss": 1.6672,
      "step": 518944
    },
    {
      "epoch": 0.0018834669018696962,
      "grad_norm": 15703.660210282189,
      "learning_rate": 1.3887857518014485e-07,
      "loss": 1.6619,
      "step": 518976
    },
    {
      "epoch": 0.001883583036220533,
      "grad_norm": 10317.969567700808,
      "learning_rate": 1.3887428963931963e-07,
      "loss": 1.6611,
      "step": 519008
    },
    {
      "epoch": 0.0018836991705713697,
      "grad_norm": 9345.566435481585,
      "learning_rate": 1.38870004495202e-07,
      "loss": 1.6777,
      "step": 519040
    },
    {
      "epoch": 0.0018838153049222065,
      "grad_norm": 10064.717780444715,
      "learning_rate": 1.388657197477307e-07,
      "loss": 1.6643,
      "step": 519072
    },
    {
      "epoch": 0.001883931439273043,
      "grad_norm": 9181.357633814294,
      "learning_rate": 1.3886143539684458e-07,
      "loss": 1.6666,
      "step": 519104
    },
    {
      "epoch": 0.0018840475736238798,
      "grad_norm": 12245.903151666684,
      "learning_rate": 1.3885715144248244e-07,
      "loss": 1.6866,
      "step": 519136
    },
    {
      "epoch": 0.0018841637079747165,
      "grad_norm": 10489.838511626382,
      "learning_rate": 1.3885286788458314e-07,
      "loss": 1.6988,
      "step": 519168
    },
    {
      "epoch": 0.0018842798423255533,
      "grad_norm": 10406.3176964765,
      "learning_rate": 1.388485847230855e-07,
      "loss": 1.6692,
      "step": 519200
    },
    {
      "epoch": 0.00188439597667639,
      "grad_norm": 12326.386331768123,
      "learning_rate": 1.3884430195792846e-07,
      "loss": 1.6582,
      "step": 519232
    },
    {
      "epoch": 0.0018845121110272266,
      "grad_norm": 21167.242994778513,
      "learning_rate": 1.388400195890508e-07,
      "loss": 1.6684,
      "step": 519264
    },
    {
      "epoch": 0.0018846282453780633,
      "grad_norm": 24968.898894424638,
      "learning_rate": 1.388357376163915e-07,
      "loss": 1.6696,
      "step": 519296
    },
    {
      "epoch": 0.0018847443797289,
      "grad_norm": 19140.185997006403,
      "learning_rate": 1.3883145603988942e-07,
      "loss": 1.6587,
      "step": 519328
    },
    {
      "epoch": 0.0018848605140797368,
      "grad_norm": 9566.729430688421,
      "learning_rate": 1.3882730864037619e-07,
      "loss": 1.676,
      "step": 519360
    },
    {
      "epoch": 0.0018849766484305734,
      "grad_norm": 8735.550240253902,
      "learning_rate": 1.3882302784363012e-07,
      "loss": 1.688,
      "step": 519392
    },
    {
      "epoch": 0.00188509278278141,
      "grad_norm": 9029.317249936454,
      "learning_rate": 1.3881874744286e-07,
      "loss": 1.6514,
      "step": 519424
    },
    {
      "epoch": 0.0018852089171322469,
      "grad_norm": 10120.078557007353,
      "learning_rate": 1.3881446743800478e-07,
      "loss": 1.6571,
      "step": 519456
    },
    {
      "epoch": 0.0018853250514830836,
      "grad_norm": 10024.559242181174,
      "learning_rate": 1.388101878290034e-07,
      "loss": 1.6853,
      "step": 519488
    },
    {
      "epoch": 0.0018854411858339204,
      "grad_norm": 12566.606065282702,
      "learning_rate": 1.3880590861579485e-07,
      "loss": 1.6771,
      "step": 519520
    },
    {
      "epoch": 0.001885557320184757,
      "grad_norm": 9969.347019740058,
      "learning_rate": 1.3880162979831816e-07,
      "loss": 1.6573,
      "step": 519552
    },
    {
      "epoch": 0.0018856734545355937,
      "grad_norm": 8787.543797899387,
      "learning_rate": 1.3879735137651228e-07,
      "loss": 1.662,
      "step": 519584
    },
    {
      "epoch": 0.0018857895888864304,
      "grad_norm": 11754.71803149697,
      "learning_rate": 1.3879307335031627e-07,
      "loss": 1.6852,
      "step": 519616
    },
    {
      "epoch": 0.0018859057232372672,
      "grad_norm": 8708.445900388886,
      "learning_rate": 1.387887957196692e-07,
      "loss": 1.6564,
      "step": 519648
    },
    {
      "epoch": 0.0018860218575881037,
      "grad_norm": 12745.19564384949,
      "learning_rate": 1.3878451848451005e-07,
      "loss": 1.649,
      "step": 519680
    },
    {
      "epoch": 0.0018861379919389405,
      "grad_norm": 8821.175545243388,
      "learning_rate": 1.3878024164477792e-07,
      "loss": 1.6627,
      "step": 519712
    },
    {
      "epoch": 0.0018862541262897772,
      "grad_norm": 10916.984840147028,
      "learning_rate": 1.387759652004119e-07,
      "loss": 1.6795,
      "step": 519744
    },
    {
      "epoch": 0.001886370260640614,
      "grad_norm": 10360.123165291037,
      "learning_rate": 1.3877168915135103e-07,
      "loss": 1.6458,
      "step": 519776
    },
    {
      "epoch": 0.0018864863949914507,
      "grad_norm": 12999.039656836192,
      "learning_rate": 1.3876741349753445e-07,
      "loss": 1.6566,
      "step": 519808
    },
    {
      "epoch": 0.0018866025293422873,
      "grad_norm": 10502.24728331989,
      "learning_rate": 1.3876313823890128e-07,
      "loss": 1.6759,
      "step": 519840
    },
    {
      "epoch": 0.001886718663693124,
      "grad_norm": 9440.491936334674,
      "learning_rate": 1.3875886337539061e-07,
      "loss": 1.6886,
      "step": 519872
    },
    {
      "epoch": 0.0018868347980439608,
      "grad_norm": 9011.232102215545,
      "learning_rate": 1.3875458890694162e-07,
      "loss": 1.6729,
      "step": 519904
    },
    {
      "epoch": 0.0018869509323947975,
      "grad_norm": 10835.219425558487,
      "learning_rate": 1.3875031483349344e-07,
      "loss": 1.677,
      "step": 519936
    },
    {
      "epoch": 0.001887067066745634,
      "grad_norm": 11264.120205324516,
      "learning_rate": 1.3874604115498528e-07,
      "loss": 1.6937,
      "step": 519968
    },
    {
      "epoch": 0.0018871832010964708,
      "grad_norm": 9427.245621070875,
      "learning_rate": 1.3874176787135623e-07,
      "loss": 1.67,
      "step": 520000
    },
    {
      "epoch": 0.0018872993354473076,
      "grad_norm": 8366.969582829855,
      "learning_rate": 1.387374949825456e-07,
      "loss": 1.6681,
      "step": 520032
    },
    {
      "epoch": 0.0018874154697981443,
      "grad_norm": 9159.711567511282,
      "learning_rate": 1.387332224884925e-07,
      "loss": 1.7059,
      "step": 520064
    },
    {
      "epoch": 0.001887531604148981,
      "grad_norm": 8420.518155078107,
      "learning_rate": 1.387289503891362e-07,
      "loss": 1.6946,
      "step": 520096
    },
    {
      "epoch": 0.0018876477384998176,
      "grad_norm": 10726.222261355579,
      "learning_rate": 1.3872467868441593e-07,
      "loss": 1.6453,
      "step": 520128
    },
    {
      "epoch": 0.0018877638728506544,
      "grad_norm": 9999.309976193357,
      "learning_rate": 1.3872040737427089e-07,
      "loss": 1.6542,
      "step": 520160
    },
    {
      "epoch": 0.0018878800072014911,
      "grad_norm": 10485.131568082492,
      "learning_rate": 1.3871613645864042e-07,
      "loss": 1.6839,
      "step": 520192
    },
    {
      "epoch": 0.0018879961415523279,
      "grad_norm": 10896.33681564589,
      "learning_rate": 1.387118659374637e-07,
      "loss": 1.6661,
      "step": 520224
    },
    {
      "epoch": 0.0018881122759031644,
      "grad_norm": 10691.888701253862,
      "learning_rate": 1.3870759581068013e-07,
      "loss": 1.6629,
      "step": 520256
    },
    {
      "epoch": 0.0018882284102540012,
      "grad_norm": 9914.805494814309,
      "learning_rate": 1.3870332607822888e-07,
      "loss": 1.6732,
      "step": 520288
    },
    {
      "epoch": 0.001888344544604838,
      "grad_norm": 10519.325833911602,
      "learning_rate": 1.3869905674004936e-07,
      "loss": 1.6991,
      "step": 520320
    },
    {
      "epoch": 0.0018884606789556747,
      "grad_norm": 15694.240217353627,
      "learning_rate": 1.3869478779608085e-07,
      "loss": 1.682,
      "step": 520352
    },
    {
      "epoch": 0.0018885768133065114,
      "grad_norm": 21778.07594807218,
      "learning_rate": 1.386905192462627e-07,
      "loss": 1.6808,
      "step": 520384
    },
    {
      "epoch": 0.001888692947657348,
      "grad_norm": 22787.005419756235,
      "learning_rate": 1.3868625109053426e-07,
      "loss": 1.7002,
      "step": 520416
    },
    {
      "epoch": 0.0018888090820081847,
      "grad_norm": 19330.62192481142,
      "learning_rate": 1.3868198332883488e-07,
      "loss": 1.6957,
      "step": 520448
    },
    {
      "epoch": 0.0018889252163590215,
      "grad_norm": 9522.708858302873,
      "learning_rate": 1.3867784931038273e-07,
      "loss": 1.6595,
      "step": 520480
    },
    {
      "epoch": 0.0018890413507098582,
      "grad_norm": 10452.238420548969,
      "learning_rate": 1.3867358232425097e-07,
      "loss": 1.6699,
      "step": 520512
    },
    {
      "epoch": 0.0018891574850606948,
      "grad_norm": 10473.490153716668,
      "learning_rate": 1.386693157319683e-07,
      "loss": 1.6919,
      "step": 520544
    },
    {
      "epoch": 0.0018892736194115315,
      "grad_norm": 8636.279291454162,
      "learning_rate": 1.3866504953347419e-07,
      "loss": 1.6579,
      "step": 520576
    },
    {
      "epoch": 0.0018893897537623683,
      "grad_norm": 9211.275156024816,
      "learning_rate": 1.3866078372870806e-07,
      "loss": 1.6706,
      "step": 520608
    },
    {
      "epoch": 0.001889505888113205,
      "grad_norm": 8119.751720342193,
      "learning_rate": 1.3865651831760934e-07,
      "loss": 1.6797,
      "step": 520640
    },
    {
      "epoch": 0.0018896220224640418,
      "grad_norm": 8194.156088335149,
      "learning_rate": 1.3865225330011752e-07,
      "loss": 1.68,
      "step": 520672
    },
    {
      "epoch": 0.0018897381568148783,
      "grad_norm": 10712.599964527752,
      "learning_rate": 1.3864798867617202e-07,
      "loss": 1.6559,
      "step": 520704
    },
    {
      "epoch": 0.001889854291165715,
      "grad_norm": 12913.698927882746,
      "learning_rate": 1.3864372444571232e-07,
      "loss": 1.6704,
      "step": 520736
    },
    {
      "epoch": 0.0018899704255165518,
      "grad_norm": 9898.711431292459,
      "learning_rate": 1.3863946060867798e-07,
      "loss": 1.6852,
      "step": 520768
    },
    {
      "epoch": 0.0018900865598673886,
      "grad_norm": 15306.284591630982,
      "learning_rate": 1.3863519716500847e-07,
      "loss": 1.6542,
      "step": 520800
    },
    {
      "epoch": 0.0018902026942182251,
      "grad_norm": 9700.046288549349,
      "learning_rate": 1.3863093411464325e-07,
      "loss": 1.6553,
      "step": 520832
    },
    {
      "epoch": 0.0018903188285690619,
      "grad_norm": 9284.916585516534,
      "learning_rate": 1.3862667145752195e-07,
      "loss": 1.6914,
      "step": 520864
    },
    {
      "epoch": 0.0018904349629198986,
      "grad_norm": 8558.573128740562,
      "learning_rate": 1.3862240919358407e-07,
      "loss": 1.6937,
      "step": 520896
    },
    {
      "epoch": 0.0018905510972707354,
      "grad_norm": 10335.696009461579,
      "learning_rate": 1.3861814732276917e-07,
      "loss": 1.6768,
      "step": 520928
    },
    {
      "epoch": 0.0018906672316215721,
      "grad_norm": 10534.39091737154,
      "learning_rate": 1.3861388584501683e-07,
      "loss": 1.6866,
      "step": 520960
    },
    {
      "epoch": 0.0018907833659724087,
      "grad_norm": 9343.748712374494,
      "learning_rate": 1.386096247602666e-07,
      "loss": 1.6898,
      "step": 520992
    },
    {
      "epoch": 0.0018908995003232454,
      "grad_norm": 12441.67094887178,
      "learning_rate": 1.3860536406845814e-07,
      "loss": 1.6586,
      "step": 521024
    },
    {
      "epoch": 0.0018910156346740822,
      "grad_norm": 11961.161314855677,
      "learning_rate": 1.38601103769531e-07,
      "loss": 1.645,
      "step": 521056
    },
    {
      "epoch": 0.001891131769024919,
      "grad_norm": 8675.426329581735,
      "learning_rate": 1.3859684386342485e-07,
      "loss": 1.6514,
      "step": 521088
    },
    {
      "epoch": 0.0018912479033757555,
      "grad_norm": 10159.27310391841,
      "learning_rate": 1.385925843500793e-07,
      "loss": 1.6777,
      "step": 521120
    },
    {
      "epoch": 0.0018913640377265922,
      "grad_norm": 10224.316505273104,
      "learning_rate": 1.38588325229434e-07,
      "loss": 1.6507,
      "step": 521152
    },
    {
      "epoch": 0.001891480172077429,
      "grad_norm": 12745.124401119041,
      "learning_rate": 1.3858406650142865e-07,
      "loss": 1.652,
      "step": 521184
    },
    {
      "epoch": 0.0018915963064282657,
      "grad_norm": 8722.164983534765,
      "learning_rate": 1.385798081660029e-07,
      "loss": 1.6821,
      "step": 521216
    },
    {
      "epoch": 0.0018917124407791025,
      "grad_norm": 13693.801225372012,
      "learning_rate": 1.385755502230964e-07,
      "loss": 1.6832,
      "step": 521248
    },
    {
      "epoch": 0.001891828575129939,
      "grad_norm": 11252.470128820605,
      "learning_rate": 1.385712926726489e-07,
      "loss": 1.638,
      "step": 521280
    },
    {
      "epoch": 0.0018919447094807758,
      "grad_norm": 9461.517425867798,
      "learning_rate": 1.3856703551460012e-07,
      "loss": 1.6539,
      "step": 521312
    },
    {
      "epoch": 0.0018920608438316125,
      "grad_norm": 9611.933000182638,
      "learning_rate": 1.3856277874888974e-07,
      "loss": 1.6702,
      "step": 521344
    },
    {
      "epoch": 0.0018921769781824493,
      "grad_norm": 10215.035193282498,
      "learning_rate": 1.3855852237545755e-07,
      "loss": 1.65,
      "step": 521376
    },
    {
      "epoch": 0.0018922931125332858,
      "grad_norm": 12152.430045056833,
      "learning_rate": 1.3855426639424328e-07,
      "loss": 1.647,
      "step": 521408
    },
    {
      "epoch": 0.0018924092468841226,
      "grad_norm": 12879.584698273466,
      "learning_rate": 1.385500108051867e-07,
      "loss": 1.6655,
      "step": 521440
    },
    {
      "epoch": 0.0018925253812349593,
      "grad_norm": 17570.505285847645,
      "learning_rate": 1.385457556082276e-07,
      "loss": 1.6772,
      "step": 521472
    },
    {
      "epoch": 0.001892641515585796,
      "grad_norm": 19206.081536846603,
      "learning_rate": 1.3854150080330575e-07,
      "loss": 1.6664,
      "step": 521504
    },
    {
      "epoch": 0.0018927576499366328,
      "grad_norm": 20649.474957005565,
      "learning_rate": 1.38537246390361e-07,
      "loss": 1.6659,
      "step": 521536
    },
    {
      "epoch": 0.0018928737842874694,
      "grad_norm": 20316.244731741146,
      "learning_rate": 1.385329923693331e-07,
      "loss": 1.6737,
      "step": 521568
    },
    {
      "epoch": 0.0018929899186383061,
      "grad_norm": 25986.1005924321,
      "learning_rate": 1.3852873874016194e-07,
      "loss": 1.6628,
      "step": 521600
    },
    {
      "epoch": 0.0018931060529891429,
      "grad_norm": 17579.218640201278,
      "learning_rate": 1.3852448550278736e-07,
      "loss": 1.6423,
      "step": 521632
    },
    {
      "epoch": 0.0018932221873399796,
      "grad_norm": 17717.964442903703,
      "learning_rate": 1.3852023265714917e-07,
      "loss": 1.6631,
      "step": 521664
    },
    {
      "epoch": 0.0018933383216908162,
      "grad_norm": 20632.17719970435,
      "learning_rate": 1.3851598020318727e-07,
      "loss": 1.698,
      "step": 521696
    },
    {
      "epoch": 0.001893454456041653,
      "grad_norm": 10585.724160396398,
      "learning_rate": 1.3851186101186266e-07,
      "loss": 1.6633,
      "step": 521728
    },
    {
      "epoch": 0.0018935705903924897,
      "grad_norm": 10189.717366050936,
      "learning_rate": 1.385076093288378e-07,
      "loss": 1.6675,
      "step": 521760
    },
    {
      "epoch": 0.0018936867247433264,
      "grad_norm": 9521.095525200868,
      "learning_rate": 1.385033580373108e-07,
      "loss": 1.6798,
      "step": 521792
    },
    {
      "epoch": 0.0018938028590941632,
      "grad_norm": 13986.715697403733,
      "learning_rate": 1.3849910713722155e-07,
      "loss": 1.6876,
      "step": 521824
    },
    {
      "epoch": 0.0018939189934449997,
      "grad_norm": 11658.994810874563,
      "learning_rate": 1.3849485662851002e-07,
      "loss": 1.6546,
      "step": 521856
    },
    {
      "epoch": 0.0018940351277958365,
      "grad_norm": 8032.5959689256115,
      "learning_rate": 1.3849060651111616e-07,
      "loss": 1.6478,
      "step": 521888
    },
    {
      "epoch": 0.0018941512621466732,
      "grad_norm": 12802.384777845104,
      "learning_rate": 1.3848635678497993e-07,
      "loss": 1.6713,
      "step": 521920
    },
    {
      "epoch": 0.00189426739649751,
      "grad_norm": 12774.36777300544,
      "learning_rate": 1.3848210745004128e-07,
      "loss": 1.648,
      "step": 521952
    },
    {
      "epoch": 0.0018943835308483465,
      "grad_norm": 10072.409046499253,
      "learning_rate": 1.384778585062402e-07,
      "loss": 1.6495,
      "step": 521984
    },
    {
      "epoch": 0.0018944996651991833,
      "grad_norm": 9662.200784500392,
      "learning_rate": 1.384736099535167e-07,
      "loss": 1.6752,
      "step": 522016
    },
    {
      "epoch": 0.00189461579955002,
      "grad_norm": 10601.629686043556,
      "learning_rate": 1.384693617918108e-07,
      "loss": 1.6833,
      "step": 522048
    },
    {
      "epoch": 0.0018947319339008568,
      "grad_norm": 10088.108048588696,
      "learning_rate": 1.3846511402106249e-07,
      "loss": 1.6713,
      "step": 522080
    },
    {
      "epoch": 0.0018948480682516936,
      "grad_norm": 9198.065666214827,
      "learning_rate": 1.3846086664121183e-07,
      "loss": 1.665,
      "step": 522112
    },
    {
      "epoch": 0.00189496420260253,
      "grad_norm": 10524.071645518192,
      "learning_rate": 1.3845661965219888e-07,
      "loss": 1.677,
      "step": 522144
    },
    {
      "epoch": 0.0018950803369533668,
      "grad_norm": 12186.61716802493,
      "learning_rate": 1.384523730539637e-07,
      "loss": 1.6496,
      "step": 522176
    },
    {
      "epoch": 0.0018951964713042036,
      "grad_norm": 8569.612476652605,
      "learning_rate": 1.3844812684644635e-07,
      "loss": 1.6473,
      "step": 522208
    },
    {
      "epoch": 0.0018953126056550404,
      "grad_norm": 9144.85363469531,
      "learning_rate": 1.3844388102958695e-07,
      "loss": 1.6744,
      "step": 522240
    },
    {
      "epoch": 0.0018954287400058769,
      "grad_norm": 8513.33530409792,
      "learning_rate": 1.3843963560332554e-07,
      "loss": 1.6755,
      "step": 522272
    },
    {
      "epoch": 0.0018955448743567136,
      "grad_norm": 11951.67586575205,
      "learning_rate": 1.384353905676023e-07,
      "loss": 1.6453,
      "step": 522304
    },
    {
      "epoch": 0.0018956610087075504,
      "grad_norm": 10034.43212145062,
      "learning_rate": 1.3843114592235736e-07,
      "loss": 1.6415,
      "step": 522336
    },
    {
      "epoch": 0.0018957771430583872,
      "grad_norm": 11296.919491613633,
      "learning_rate": 1.384269016675308e-07,
      "loss": 1.6689,
      "step": 522368
    },
    {
      "epoch": 0.001895893277409224,
      "grad_norm": 10209.064795562814,
      "learning_rate": 1.3842265780306277e-07,
      "loss": 1.6604,
      "step": 522400
    },
    {
      "epoch": 0.0018960094117600604,
      "grad_norm": 10133.214593602564,
      "learning_rate": 1.3841841432889353e-07,
      "loss": 1.639,
      "step": 522432
    },
    {
      "epoch": 0.0018961255461108972,
      "grad_norm": 8181.0583667395995,
      "learning_rate": 1.3841417124496317e-07,
      "loss": 1.6529,
      "step": 522464
    },
    {
      "epoch": 0.001896241680461734,
      "grad_norm": 11624.950580540117,
      "learning_rate": 1.3840992855121194e-07,
      "loss": 1.6817,
      "step": 522496
    },
    {
      "epoch": 0.0018963578148125707,
      "grad_norm": 11315.498751712184,
      "learning_rate": 1.3840568624757998e-07,
      "loss": 1.6638,
      "step": 522528
    },
    {
      "epoch": 0.0018964739491634072,
      "grad_norm": 13696.562926515542,
      "learning_rate": 1.3840144433400756e-07,
      "loss": 1.6712,
      "step": 522560
    },
    {
      "epoch": 0.001896590083514244,
      "grad_norm": 8630.914667635176,
      "learning_rate": 1.3839720281043491e-07,
      "loss": 1.6991,
      "step": 522592
    },
    {
      "epoch": 0.0018967062178650807,
      "grad_norm": 9461.566360809398,
      "learning_rate": 1.3839296167680225e-07,
      "loss": 1.7106,
      "step": 522624
    },
    {
      "epoch": 0.0018968223522159175,
      "grad_norm": 11406.509983338463,
      "learning_rate": 1.3838872093304983e-07,
      "loss": 1.6707,
      "step": 522656
    },
    {
      "epoch": 0.0018969384865667543,
      "grad_norm": 12374.603023935757,
      "learning_rate": 1.383844805791179e-07,
      "loss": 1.6763,
      "step": 522688
    },
    {
      "epoch": 0.0018970546209175908,
      "grad_norm": 8202.949835272675,
      "learning_rate": 1.3838024061494684e-07,
      "loss": 1.694,
      "step": 522720
    },
    {
      "epoch": 0.0018971707552684275,
      "grad_norm": 23269.356673530965,
      "learning_rate": 1.3837600104047684e-07,
      "loss": 1.6683,
      "step": 522752
    },
    {
      "epoch": 0.0018972868896192643,
      "grad_norm": 10314.930925604882,
      "learning_rate": 1.3837189432427685e-07,
      "loss": 1.6545,
      "step": 522784
    },
    {
      "epoch": 0.001897403023970101,
      "grad_norm": 10852.148911621145,
      "learning_rate": 1.3836765551685644e-07,
      "loss": 1.6586,
      "step": 522816
    },
    {
      "epoch": 0.0018975191583209376,
      "grad_norm": 10308.824569270737,
      "learning_rate": 1.3836341709895998e-07,
      "loss": 1.6772,
      "step": 522848
    },
    {
      "epoch": 0.0018976352926717743,
      "grad_norm": 8762.604863851844,
      "learning_rate": 1.3835917907052776e-07,
      "loss": 1.6428,
      "step": 522880
    },
    {
      "epoch": 0.001897751427022611,
      "grad_norm": 11411.129041422677,
      "learning_rate": 1.3835494143150018e-07,
      "loss": 1.6419,
      "step": 522912
    },
    {
      "epoch": 0.0018978675613734479,
      "grad_norm": 7479.599053425257,
      "learning_rate": 1.3835070418181759e-07,
      "loss": 1.6719,
      "step": 522944
    },
    {
      "epoch": 0.0018979836957242846,
      "grad_norm": 10648.914404764459,
      "learning_rate": 1.383464673214204e-07,
      "loss": 1.6792,
      "step": 522976
    },
    {
      "epoch": 0.0018980998300751211,
      "grad_norm": 10925.647440769815,
      "learning_rate": 1.3834223085024897e-07,
      "loss": 1.6547,
      "step": 523008
    },
    {
      "epoch": 0.001898215964425958,
      "grad_norm": 13978.880570346111,
      "learning_rate": 1.383379947682437e-07,
      "loss": 1.6525,
      "step": 523040
    },
    {
      "epoch": 0.0018983320987767947,
      "grad_norm": 8528.291739850367,
      "learning_rate": 1.3833375907534506e-07,
      "loss": 1.6845,
      "step": 523072
    },
    {
      "epoch": 0.0018984482331276314,
      "grad_norm": 8963.855085843366,
      "learning_rate": 1.3832952377149345e-07,
      "loss": 1.6569,
      "step": 523104
    },
    {
      "epoch": 0.001898564367478468,
      "grad_norm": 10762.363495069287,
      "learning_rate": 1.3832528885662933e-07,
      "loss": 1.6598,
      "step": 523136
    },
    {
      "epoch": 0.0018986805018293047,
      "grad_norm": 9593.677084413463,
      "learning_rate": 1.3832105433069315e-07,
      "loss": 1.6865,
      "step": 523168
    },
    {
      "epoch": 0.0018987966361801415,
      "grad_norm": 11568.412596376393,
      "learning_rate": 1.3831682019362537e-07,
      "loss": 1.6619,
      "step": 523200
    },
    {
      "epoch": 0.0018989127705309782,
      "grad_norm": 8544.737678829,
      "learning_rate": 1.3831258644536654e-07,
      "loss": 1.6437,
      "step": 523232
    },
    {
      "epoch": 0.001899028904881815,
      "grad_norm": 11125.9124569628,
      "learning_rate": 1.383083530858571e-07,
      "loss": 1.6469,
      "step": 523264
    },
    {
      "epoch": 0.0018991450392326515,
      "grad_norm": 9497.852178255882,
      "learning_rate": 1.3830412011503753e-07,
      "loss": 1.6728,
      "step": 523296
    },
    {
      "epoch": 0.0018992611735834883,
      "grad_norm": 10981.541421858772,
      "learning_rate": 1.3829988753284844e-07,
      "loss": 1.6619,
      "step": 523328
    },
    {
      "epoch": 0.001899377307934325,
      "grad_norm": 11647.899724843102,
      "learning_rate": 1.3829565533923032e-07,
      "loss": 1.6453,
      "step": 523360
    },
    {
      "epoch": 0.0018994934422851618,
      "grad_norm": 8853.078108770982,
      "learning_rate": 1.3829142353412373e-07,
      "loss": 1.6633,
      "step": 523392
    },
    {
      "epoch": 0.0018996095766359983,
      "grad_norm": 8495.23348707968,
      "learning_rate": 1.3828719211746918e-07,
      "loss": 1.6767,
      "step": 523424
    },
    {
      "epoch": 0.001899725710986835,
      "grad_norm": 9874.92339210791,
      "learning_rate": 1.3828296108920733e-07,
      "loss": 1.6583,
      "step": 523456
    },
    {
      "epoch": 0.0018998418453376718,
      "grad_norm": 9752.033634068332,
      "learning_rate": 1.3827873044927873e-07,
      "loss": 1.6781,
      "step": 523488
    },
    {
      "epoch": 0.0018999579796885086,
      "grad_norm": 13631.373958629409,
      "learning_rate": 1.3827450019762396e-07,
      "loss": 1.6939,
      "step": 523520
    },
    {
      "epoch": 0.0019000741140393453,
      "grad_norm": 11842.227155396065,
      "learning_rate": 1.3827027033418365e-07,
      "loss": 1.691,
      "step": 523552
    },
    {
      "epoch": 0.0019001902483901819,
      "grad_norm": 11149.731655963744,
      "learning_rate": 1.3826604085889844e-07,
      "loss": 1.6705,
      "step": 523584
    },
    {
      "epoch": 0.0019003063827410186,
      "grad_norm": 11400.742081110335,
      "learning_rate": 1.3826181177170895e-07,
      "loss": 1.6778,
      "step": 523616
    },
    {
      "epoch": 0.0019004225170918554,
      "grad_norm": 11165.063367486993,
      "learning_rate": 1.3825758307255583e-07,
      "loss": 1.694,
      "step": 523648
    },
    {
      "epoch": 0.0019005386514426921,
      "grad_norm": 10843.991700476352,
      "learning_rate": 1.3825335476137976e-07,
      "loss": 1.6772,
      "step": 523680
    },
    {
      "epoch": 0.0019006547857935287,
      "grad_norm": 9099.974505458793,
      "learning_rate": 1.382491268381214e-07,
      "loss": 1.6675,
      "step": 523712
    },
    {
      "epoch": 0.0019007709201443654,
      "grad_norm": 10118.044475094977,
      "learning_rate": 1.3824489930272143e-07,
      "loss": 1.6816,
      "step": 523744
    },
    {
      "epoch": 0.0019008870544952022,
      "grad_norm": 18373.608028909293,
      "learning_rate": 1.3824067215512059e-07,
      "loss": 1.6727,
      "step": 523776
    },
    {
      "epoch": 0.001901003188846039,
      "grad_norm": 19913.767298027764,
      "learning_rate": 1.3823657747563668e-07,
      "loss": 1.6423,
      "step": 523808
    },
    {
      "epoch": 0.0019011193231968757,
      "grad_norm": 8465.865460778361,
      "learning_rate": 1.3823235109134207e-07,
      "loss": 1.6686,
      "step": 523840
    },
    {
      "epoch": 0.0019012354575477122,
      "grad_norm": 11145.504923510643,
      "learning_rate": 1.382281250946706e-07,
      "loss": 1.6999,
      "step": 523872
    },
    {
      "epoch": 0.001901351591898549,
      "grad_norm": 9445.65445059261,
      "learning_rate": 1.3822389948556303e-07,
      "loss": 1.6544,
      "step": 523904
    },
    {
      "epoch": 0.0019014677262493857,
      "grad_norm": 14171.584385664151,
      "learning_rate": 1.3821967426396012e-07,
      "loss": 1.6381,
      "step": 523936
    },
    {
      "epoch": 0.0019015838606002225,
      "grad_norm": 8176.281917840162,
      "learning_rate": 1.3821544942980263e-07,
      "loss": 1.6547,
      "step": 523968
    },
    {
      "epoch": 0.001901699994951059,
      "grad_norm": 9574.528709027927,
      "learning_rate": 1.3821122498303138e-07,
      "loss": 1.6646,
      "step": 524000
    },
    {
      "epoch": 0.0019018161293018958,
      "grad_norm": 8041.839590541457,
      "learning_rate": 1.3820700092358714e-07,
      "loss": 1.6434,
      "step": 524032
    },
    {
      "epoch": 0.0019019322636527325,
      "grad_norm": 9183.289606671457,
      "learning_rate": 1.3820277725141076e-07,
      "loss": 1.6554,
      "step": 524064
    },
    {
      "epoch": 0.0019020483980035693,
      "grad_norm": 10545.517151851776,
      "learning_rate": 1.3819855396644305e-07,
      "loss": 1.6686,
      "step": 524096
    },
    {
      "epoch": 0.001902164532354406,
      "grad_norm": 10800.657757747906,
      "learning_rate": 1.3819433106862482e-07,
      "loss": 1.6661,
      "step": 524128
    },
    {
      "epoch": 0.0019022806667052426,
      "grad_norm": 9525.041522219208,
      "learning_rate": 1.3819010855789697e-07,
      "loss": 1.6487,
      "step": 524160
    },
    {
      "epoch": 0.0019023968010560793,
      "grad_norm": 8521.653830096597,
      "learning_rate": 1.3818588643420037e-07,
      "loss": 1.6696,
      "step": 524192
    },
    {
      "epoch": 0.001902512935406916,
      "grad_norm": 10666.80289496342,
      "learning_rate": 1.3818166469747587e-07,
      "loss": 1.6969,
      "step": 524224
    },
    {
      "epoch": 0.0019026290697577528,
      "grad_norm": 11064.045914582965,
      "learning_rate": 1.3817744334766435e-07,
      "loss": 1.6616,
      "step": 524256
    },
    {
      "epoch": 0.0019027452041085894,
      "grad_norm": 11713.755503680279,
      "learning_rate": 1.3817322238470676e-07,
      "loss": 1.6469,
      "step": 524288
    },
    {
      "epoch": 0.0019027452041085894,
      "eval_loss": 1.71875,
      "eval_runtime": 24882.5315,
      "eval_samples_per_second": 214.175,
      "eval_steps_per_second": 0.418,
      "step": 524288
    }
  ],
  "logging_steps": 32,
  "max_steps": 275542936,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 262144,
  "total_flos": 4.092632087624103e+19,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
